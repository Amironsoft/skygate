<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Aneural%20networks%26id_list%3D%26start%3D0%26max_results%3D2000" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:neural networks&amp;id_list=&amp;start=0&amp;max_results=2000</title>
  <id>http://arxiv.org/api/TfcOcUOuideOGiKyrdODbHq950w</id>
  <updated>2018-03-06T00:00:00-05:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">90225</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">2000</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/cs/0504056v1</id>
    <updated>2005-04-13T13:59:55Z</updated>
    <published>2005-04-13T13:59:55Z</published>
    <title>Self-Organizing Multilayered Neural Networks of Optimal Complexity</title>
    <summary>  The principles of self-organizing the neural networks of optimal complexity
is considered under the unrepresentative learning set. The method of
self-organizing the multi-layered neural networks is offered and used to train
the logical neural networks which were applied to the medical diagnostics.
</summary>
    <author>
      <name>V. Schetinin</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0504056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0608073v1</id>
    <updated>2006-08-18T08:28:23Z</updated>
    <published>2006-08-18T08:28:23Z</published>
    <title>Parametrical Neural Networks and Some Other Similar Architectures</title>
    <summary>  A review of works on associative neural networks accomplished during last
four years in the Institute of Optical Neural Technologies RAS is given. The
presentation is based on description of parametrical neural networks (PNN). For
today PNN have record recognizing characteristics (storage capacity, noise
immunity and speed of operation). Presentation of basic ideas and principles is
accentuated.
</summary>
    <author>
      <name>Leonid B. Litinskii</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 2 figures, accepted for publication in "Optical Memory &amp;
  Neural Networks" (2006)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0608073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0608073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.06246v1</id>
    <updated>2017-01-23T01:26:24Z</updated>
    <published>2017-01-23T01:26:24Z</published>
    <title>Neural network representation of tensor network and chiral states</title>
    <summary>  We study the representational power of a Boltzmann machine (a type of neural
network) in quantum many-body systems. We prove that any (local) tensor network
state has a (local) neural network representation. The construction is almost
optimal in the sense that the number of parameters in the neural network
representation is almost linear in the number of nonzero parameters in the
tensor network representation. Despite the difficulty of representing (gapped)
chiral topological states with local tensor networks, we construct a
quasi-local neural network representation for a chiral p-wave superconductor.
This demonstrates the power of Boltzmann machines.
</summary>
    <author>
      <name>Yichen Huang</name>
    </author>
    <author>
      <name>Joel E. Moore</name>
    </author>
    <link href="http://arxiv.org/abs/1701.06246v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.06246v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07333v1</id>
    <updated>2016-05-24T08:20:12Z</updated>
    <published>2016-05-24T08:20:12Z</published>
    <title>Combining Recurrent and Convolutional Neural Networks for Relation
  Classification</title>
    <summary>  This paper investigates two different neural architectures for the task of
relation classification: convolutional neural networks and recurrent neural
networks. For both models, we demonstrate the effect of different architectural
choices. We present a new context representation for convolutional neural
networks for relation classification (extended middle context). Furthermore, we
propose connectionist bi-directional recurrent neural networks and introduce
ranking loss for their optimization. Finally, we show that combining
convolutional and recurrent neural networks using a simple voting scheme is
accurate enough to improve results. Our neural models achieve state-of-the-art
results on the SemEval 2010 relation classification task.
</summary>
    <author>
      <name>Ngoc Thang Vu</name>
    </author>
    <author>
      <name>Heike Adel</name>
    </author>
    <author>
      <name>Pankaj Gupta</name>
    </author>
    <author>
      <name>Hinrich Sch√ºtze</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NAACL 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07333v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07333v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.5997v2</id>
    <updated>2014-04-26T23:10:51Z</updated>
    <published>2014-04-23T22:37:56Z</published>
    <title>One weird trick for parallelizing convolutional neural networks</title>
    <summary>  I present a new way to parallelize the training of convolutional neural
networks across multiple GPUs. The method scales significantly better than all
alternatives when applied to modern convolutional neural networks.
</summary>
    <author>
      <name>Alex Krizhevsky</name>
    </author>
    <link href="http://arxiv.org/abs/1404.5997v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.5997v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.01439v1</id>
    <updated>2016-10-05T14:26:27Z</updated>
    <published>2016-10-05T14:26:27Z</published>
    <title>Nonlinear Systems Identification Using Deep Dynamic Neural Networks</title>
    <summary>  Neural networks are known to be effective function approximators. Recently,
deep neural networks have proven to be very effective in pattern recognition,
classification tasks and human-level control to model highly nonlinear
realworld systems. This paper investigates the effectiveness of deep neural
networks in the modeling of dynamical systems with complex behavior. Three deep
neural network structures are trained on sequential data, and we investigate
the effectiveness of these networks in modeling associated characteristics of
the underlying dynamical systems. We carry out similar evaluations on select
publicly available system identification datasets. We demonstrate that deep
neural networks are effective model estimators from input-output data
</summary>
    <author>
      <name>Olalekan Ogunmolu</name>
    </author>
    <author>
      <name>Xuejun Gu</name>
    </author>
    <author>
      <name>Steve Jiang</name>
    </author>
    <author>
      <name>Nicholas Gans</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">American Control Conference, 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.01439v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.01439v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.02522v1</id>
    <updated>2016-12-08T03:28:10Z</updated>
    <published>2016-12-08T03:28:10Z</published>
    <title>Geometric Decomposition of Feed Forward Neural Networks</title>
    <summary>  There have been several attempts to mathematically understand neural networks
and many more from biological and computational perspectives. The field has
exploded in the last decade, yet neural networks are still treated much like a
black box. In this work we describe a structure that is inherent to a feed
forward neural network. This will provide a framework for future work on neural
networks to improve training algorithms, compute the homology of the network,
and other applications. Our approach takes a more geometric point of view and
is unlike other attempts to mathematically understand neural networks that rely
on a functional perspective.
</summary>
    <author>
      <name>Sven Cattell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.02522v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.02522v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92B20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.04759v1</id>
    <updated>2017-11-13T18:50:04Z</updated>
    <published>2017-11-13T18:50:04Z</published>
    <title>Neural Networks Architecture Evaluation in a Quantum Computer</title>
    <summary>  In this work, we propose a quantum algorithm to evaluate neural networks
architectures named Quantum Neural Network Architecture Evaluation (QNNAE). The
proposed algorithm is based on a quantum associative memory and the learning
algorithm for artificial neural networks. Unlike conventional algorithms for
evaluating neural network architectures, QNNAE does not depend on
initialization of weights. The proposed algorithm has a binary output and
results in 0 with probability proportional to the performance of the network.
And its computational cost is equal to the computational cost to train a neural
network.
</summary>
    <author>
      <name>Adenilton Jos√© da Silva</name>
    </author>
    <author>
      <name>Rodolfo Luan F. de Oliveira</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/BRACIS.2017.33</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/BRACIS.2017.33" rel="related"/>
    <link href="http://arxiv.org/abs/1711.04759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.04759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.2853v1</id>
    <updated>2013-12-10T16:15:48Z</updated>
    <published>2013-12-10T16:15:48Z</published>
    <title>Performance Analysis Of Neural Network Models For Oxazolines And
  Oxazoles Derivatives Descriptor Dataset</title>
    <summary>  Neural networks have been used successfully to a broad range of areas such as
business, data mining, drug discovery and biology. In medicine, neural networks
have been applied widely in medical diagnosis, detection and evaluation of new
drugs and treatment cost estimation. In addition, neural networks have begin
practice in data mining strategies for the aim of prediction, knowledge
discovery. This paper will present the application of neural networks for the
prediction and analysis of antitubercular activity of Oxazolines and Oxazoles
derivatives. This study presents techniques based on the development of Single
hidden layer neural network (SHLFFNN), Gradient Descent Back propagation neural
network (GDBPNN), Gradient Descent Back propagation with momentum neural
network (GDBPMNN), Back propagation with Weight decay neural network (BPWDNN)
and Quantile regression neural network (QRNN) of artificial neural network
(ANN) models Here, we comparatively evaluate the performance of five neural
network techniques. The evaluation of the efficiency of each model by ways of
benchmark experiments is an accepted application. Cross-validation and
resampling techniques are commonly used to derive point estimates of the
performances which are compared to identify methods with good properties.
Predictive accuracy was evaluated using the root mean squared error (RMSE),
Coefficient determination(???), mean absolute error(MAE), mean percentage
error(MPE) and relative square error(RSE). We found that all five neural
network models were able to produce feasible models. QRNN model is outperforms
with all statistical tests amongst other four models.
</summary>
    <author>
      <name> Doreswamy</name>
    </author>
    <author>
      <name>Chanabasayya . M. Vastrad</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijist.2013.3601</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijist.2013.3601" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">published International Journal of Information Sciences and
  Techniques (IJIST) Vol.3, No.6, November 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1312.2853v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.2853v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.4495v1</id>
    <updated>2010-09-22T22:32:37Z</updated>
    <published>2010-09-22T22:32:37Z</published>
    <title>Unary Coding for Neural Network Learning</title>
    <summary>  This paper presents some properties of unary coding of significance for
biological learning and instantaneously trained neural networks.
</summary>
    <author>
      <name>Subhash Kak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1009.4495v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.4495v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05549v1</id>
    <updated>2017-01-19T18:43:56Z</updated>
    <published>2017-01-19T18:43:56Z</published>
    <title>Deep Neural Networks - A Brief History</title>
    <summary>  Introduction to deep neural networks and their history.
</summary>
    <author>
      <name>Krzysztof J. Cios</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.05549v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05549v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08551v1</id>
    <updated>2016-03-28T20:28:09Z</updated>
    <published>2016-03-28T20:28:09Z</published>
    <title>Genetic cellular neural networks for generating three-dimensional
  geometry</title>
    <summary>  There are a number of ways to procedurally generate interesting
three-dimensional shapes, and a method where a cellular neural network is
combined with a mesh growth algorithm is presented here. The aim is to create a
shape from a genetic code in such a way that a crude search can find
interesting shapes. Identical neural networks are placed at each vertex of a
mesh which can communicate with neural networks on neighboring vertices. The
output of the neural networks determine how the mesh grows, allowing
interesting shapes to be produced emergently, mimicking some of the complexity
of biological organism development. Since the neural networks' parameters can
be freely mutated, the approach is amenable for use in a genetic algorithm.
</summary>
    <author>
      <name>Hugo Martay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.08551v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08551v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92B20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.0324v1</id>
    <updated>2013-02-02T00:43:42Z</updated>
    <published>2013-02-02T00:43:42Z</published>
    <title>A New Constructive Method to Optimize Neural Network Architecture and
  Generalization</title>
    <summary>  In this paper, after analyzing the reasons of poor generalization and
overfitting in neural networks, we consider some noise data as a singular value
of a continuous function - jump discontinuity point. The continuous part can be
approximated with the simplest neural networks, which have good generalization
performance and optimal network architecture, by traditional algorithms such as
constructive algorithm for feed-forward neural networks with incremental
training, BP algorithm, ELM algorithm, various constructive algorithm, RBF
approximation and SVM. At the same time, we will construct RBF neural networks
to fit the singular value with every error in, and we prove that a function
with jumping discontinuity points can be approximated by the simplest neural
networks with a decay RBF neural networks in by each error, and a function with
jumping discontinuity point can be constructively approximated by a decay RBF
neural networks in by each error and the constructive part have no
generalization influence to the whole machine learning system which will
optimize neural network architecture and generalization performance, reduce the
overfitting phenomenon by avoid fitting the noisy data.
</summary>
    <author>
      <name>Hou Muzhou</name>
    </author>
    <author>
      <name>Moon Ho Lee</name>
    </author>
    <link href="http://arxiv.org/abs/1302.0324v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.0324v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="41A99, 65D15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.0930v1</id>
    <updated>2007-07-06T14:18:02Z</updated>
    <published>2007-07-06T14:18:02Z</published>
    <title>Bayesian Learning of Neural Networks for Signal/Background
  Discrimination in Particle Physics</title>
    <summary>  Neural networks are used extensively in classification problems in particle
physics research. Since the training of neural networks can be viewed as a
problem of inference, Bayesian learning of neural networks can provide more
optimal and robust results than conventional learning methods. We have
investigated the use of Bayesian neural networks for signal/background
discrimination in the search for second generation leptoquarks at the Tevatron,
as an example. We present a comparison of the results obtained from the
conventional training of feedforward neural networks and networks trained with
Bayesian methods.
</summary>
    <author>
      <name>Michael Pogwizd</name>
    </author>
    <author>
      <name>Laura Jane Elgass</name>
    </author>
    <author>
      <name>Pushpalatha C. Bhat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 4 figures, conference proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/0707.0930v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.0930v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0308503v1</id>
    <updated>2003-08-25T15:11:24Z</updated>
    <published>2003-08-25T15:11:24Z</published>
    <title>Neural network learning dynamics in a path integral framework</title>
    <summary>  A path-integral formalism is proposed for studying the dynamical evolution in
time of patterns in an artificial neural network in the presence of noise. An
effective cost function is constructed which determines the unique global
minimum of the neural network system. The perturbative method discussed also
provides a way for determining the storage capacity of the network.
</summary>
    <author>
      <name>J. Balakrishnan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s100510051172</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s100510051172" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Eur.Phys.J.B15, 679 (2000)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0308503v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0308503v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.3714v2</id>
    <updated>2014-12-13T00:57:57Z</updated>
    <published>2014-12-11T16:35:27Z</published>
    <title>Feature Weight Tuning for Recursive Neural Networks</title>
    <summary>  This paper addresses how a recursive neural network model can automatically
leave out useless information and emphasize important evidence, in other words,
to perform "weight tuning" for higher-level representation acquisition. We
propose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural
Network (BENN), which automatically control how much one specific unit
contributes to the higher-level representation. The proposed model can be
viewed as incorporating a more powerful compositional function for embedding
acquisition in recursive neural networks. Experimental results demonstrate the
significant improvement over standard neural models.
</summary>
    <author>
      <name>Jiwei Li</name>
    </author>
    <link href="http://arxiv.org/abs/1412.3714v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3714v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0407436v1</id>
    <updated>2004-07-16T12:50:21Z</updated>
    <published>2004-07-16T12:50:21Z</published>
    <title>Neural Networks Processing Mean Values of Random Variables</title>
    <summary>  We introduce a class of neural networks derived from probabilistic models in
the form of Bayesian belief networks. By imposing additional assumptions about
the nature of the probabilistic models represented in the belief networks, we
derive neural networks with standard dynamics that require no training to
determine the synaptic weights, that can pool multiple sources of evidence, and
that deal cleanly and consistently with inconsistent or contradictory evidence.
The presented neural networks capture many properties of Bayesian belief
networks, providing distributed versions of probabilistic models.
</summary>
    <author>
      <name>M. J. Barber</name>
    </author>
    <author>
      <name>J. W. Clark</name>
    </author>
    <author>
      <name>C. H. Anderson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures, 1 table, submitted to Phys Rev E</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0407436v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0407436v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0410492v1</id>
    <updated>2004-10-19T23:08:45Z</updated>
    <published>2004-10-19T23:08:45Z</published>
    <title>Stability of a neural network model with small-world connections</title>
    <summary>  Small-world networks are highly clustered networks with small distances among
the nodes. There are many biological neural networks that present this kind of
connections. There are no special weightings in the connections of most
existing small-world network models. However, this kind of simply-connected
models cannot characterize biological neural networks, in which there are
different weights in synaptic connections. In this paper, we present a neural
network model with weighted small-world connections, and further investigate
the stability of this model.
</summary>
    <author>
      <name>Chunguang Li</name>
    </author>
    <author>
      <name>Guanrong Chen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.68.052901</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.68.052901" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physical Review E, Vol. 68, 052901, 2003</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0410492v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0410492v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.4610v1</id>
    <updated>2010-04-26T19:18:48Z</updated>
    <published>2010-04-26T19:18:48Z</published>
    <title>Mobility Prediction in Wireless Ad Hoc Networks using Neural Networks</title>
    <summary>  Mobility prediction allows estimating the stability of paths in a mobile
wireless Ad Hoc networks. Identifying stable paths helps to improve routing by
reducing the overhead and the number of connection interruptions. In this
paper, we introduce a neural network based method for mobility prediction in Ad
Hoc networks. This method consists of a multi-layer and recurrent neural
network using back propagation through time algorithm for training.
</summary>
    <author>
      <name>Heni Kaaniche</name>
    </author>
    <author>
      <name>Farouk Kamoun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Heni Kaaniche and Farouk Kamoun, "Mobility Prediction in Wireless Ad
  Hoc Networks using Neural Networks", Journal of Telecommunications, Volume 2,
  Issue 1, p95-101, April 2010</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Telecommunications, Volume 2, Issue 1, p95-101, April
  2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.4610v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.4610v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.5326v1</id>
    <updated>2010-04-29T15:35:32Z</updated>
    <published>2010-04-29T15:35:32Z</published>
    <title>Designing neural networks that process mean values of random variables</title>
    <summary>  We introduce a class of neural networks derived from probabilistic models in
the form of Bayesian networks. By imposing additional assumptions about the
nature of the probabilistic models represented in the networks, we derive
neural networks with standard dynamics that require no training to determine
the synaptic weights, that perform accurate calculation of the mean values of
the random variables, that can pool multiple sources of evidence, and that deal
cleanly and consistently with inconsistent or contradictory evidence. The
presented neural networks capture many properties of Bayesian networks,
providing distributed versions of probabilistic models.
</summary>
    <author>
      <name>Michael J. Barber</name>
    </author>
    <author>
      <name>John W. Clark</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, elsarticle</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.5326v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.5326v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.00726v1</id>
    <updated>2015-10-02T20:17:33Z</updated>
    <published>2015-10-02T20:17:33Z</published>
    <title>A Primer on Neural Network Models for Natural Language Processing</title>
    <summary>  Over the past few years, neural networks have re-emerged as powerful
machine-learning models, yielding state-of-the-art results in fields such as
image recognition and speech processing. More recently, neural network models
started to be applied also to textual natural language signals, again with very
promising results. This tutorial surveys neural network models from the
perspective of natural language processing research, in an attempt to bring
natural-language researchers up to speed with the neural techniques. The
tutorial covers input encoding for natural language tasks, feed-forward
networks, convolutional networks, recurrent networks and recursive networks, as
well as the computation graph abstraction for automatic gradient computation.
</summary>
    <author>
      <name>Yoav Goldberg</name>
    </author>
    <link href="http://arxiv.org/abs/1510.00726v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.00726v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.08985v1</id>
    <updated>2015-10-30T06:42:03Z</updated>
    <published>2015-10-30T06:42:03Z</published>
    <title>Prediction-Adaptation-Correction Recurrent Neural Networks for
  Low-Resource Language Speech Recognition</title>
    <summary>  In this paper, we investigate the use of prediction-adaptation-correction
recurrent neural networks (PAC-RNNs) for low-resource speech recognition. A
PAC-RNN is comprised of a pair of neural networks in which a {\it correction}
network uses auxiliary information given by a {\it prediction} network to help
estimate the state probability. The information from the correction network is
also used by the prediction network in a recurrent loop. Our model outperforms
other state-of-the-art neural networks (DNNs, LSTMs) on IARPA-Babel tasks.
Moreover, transfer learning from a language that is similar to the target
language can help improve performance further.
</summary>
    <author>
      <name>Yu Zhang</name>
    </author>
    <author>
      <name>Ekapol Chuangsuwanich</name>
    </author>
    <author>
      <name>James Glass</name>
    </author>
    <author>
      <name>Dong Yu</name>
    </author>
    <link href="http://arxiv.org/abs/1510.08985v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.08985v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00095v1</id>
    <updated>2017-07-01T04:56:08Z</updated>
    <published>2017-07-01T04:56:08Z</published>
    <title>Exploring the Imposition of Synaptic Precision Restrictions For
  Evolutionary Synthesis of Deep Neural Networks</title>
    <summary>  A key contributing factor to incredible success of deep neural networks has
been the significant rise on massively parallel computing devices allowing
researchers to greatly increase the size and depth of deep neural networks,
leading to significant improvements in modeling accuracy. Although deeper,
larger, or complex deep neural networks have shown considerable promise, the
computational complexity of such networks is a major barrier to utilization in
resource-starved scenarios. We explore the synaptogenesis of deep neural
networks in the formation of efficient deep neural network architectures within
an evolutionary deep intelligence framework, where a probabilistic generative
modeling strategy is introduced to stochastically synthesize increasingly
efficient yet effective offspring deep neural networks over generations,
mimicking evolutionary processes such as heredity, random mutation, and natural
selection in a probabilistic manner. In this study, we primarily explore the
imposition of synaptic precision restrictions and its impact on the
evolutionary synthesis of deep neural networks to synthesize more efficient
network architectures tailored for resource-starved scenarios. Experimental
results show significant improvements in synaptic efficiency (~10X decrease for
GoogLeNet-based DetectNet) and inference speed (&gt;5X increase for
GoogLeNet-based DetectNet) while preserving modeling accuracy.
</summary>
    <author>
      <name>Mohammad Javad Shafiee</name>
    </author>
    <author>
      <name>Francis Li</name>
    </author>
    <author>
      <name>Alexander Wong</name>
    </author>
    <link href="http://arxiv.org/abs/1707.00095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.02693v1</id>
    <updated>2015-10-09T15:04:11Z</updated>
    <published>2015-10-09T15:04:11Z</published>
    <title>Feedforward Sequential Memory Neural Networks without Recurrent Feedback</title>
    <summary>  We introduce a new structure for memory neural networks, called feedforward
sequential memory networks (FSMN), which can learn long-term dependency without
using recurrent feedback. The proposed FSMN is a standard feedforward neural
networks equipped with learnable sequential memory blocks in the hidden layers.
In this work, we have applied FSMN to several language modeling (LM) tasks.
Experimental results have shown that the memory blocks in FSMN can learn
effective representations of long history. Experiments have shown that FSMN
based language models can significantly outperform not only feedforward neural
network (FNN) based LMs but also the popular recurrent neural network (RNN)
LMs.
</summary>
    <author>
      <name>ShiLiang Zhang</name>
    </author>
    <author>
      <name>Hui Jiang</name>
    </author>
    <author>
      <name>Si Wei</name>
    </author>
    <author>
      <name>LiRong Dai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.02693v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.02693v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06220v1</id>
    <updated>2016-03-20T14:39:27Z</updated>
    <published>2016-03-20T14:39:27Z</published>
    <title>Flow of Information in Feed-Forward Deep Neural Networks</title>
    <summary>  Feed-forward deep neural networks have been used extensively in various
machine learning applications. Developing a precise understanding of the
underling behavior of neural networks is crucial for their efficient
deployment. In this paper, we use an information theoretic approach to study
the flow of information in a neural network and to determine how entropy of
information changes between consecutive layers. Moreover, using the Information
Bottleneck principle, we develop a constrained optimization problem that can be
used in the training process of a deep neural network. Furthermore, we
determine a lower bound for the level of data representation that can be
achieved in a deep neural network with an acceptable level of distortion.
</summary>
    <author>
      <name>Pejman Khadivi</name>
    </author>
    <author>
      <name>Ravi Tandon</name>
    </author>
    <author>
      <name>Naren Ramakrishnan</name>
    </author>
    <link href="http://arxiv.org/abs/1603.06220v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06220v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.08171v1</id>
    <updated>2017-02-27T08:00:58Z</updated>
    <published>2017-02-27T08:00:58Z</published>
    <title>Fixed-point optimization of deep neural networks with adaptive step size
  retraining</title>
    <summary>  Fixed-point optimization of deep neural networks plays an important role in
hardware based design and low-power implementations. Many deep neural networks
show fairly good performance even with 2- or 3-bit precision when quantized
weights are fine-tuned by retraining. We propose an improved fixedpoint
optimization algorithm that estimates the quantization step size dynamically
during the retraining. In addition, a gradual quantization scheme is also
tested, which sequentially applies fixed-point optimizations from high- to
low-precision. The experiments are conducted for feed-forward deep neural
networks (FFDNNs), convolutional neural networks (CNNs), and recurrent neural
networks (RNNs).
</summary>
    <author>
      <name>Sungho Shin</name>
    </author>
    <author>
      <name>Yoonho Boo</name>
    </author>
    <author>
      <name>Wonyong Sung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is accepted in ICASSP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.08171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.08171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505016v1</id>
    <updated>2005-05-07T20:56:58Z</updated>
    <published>2005-05-07T20:56:58Z</published>
    <title>Visual Character Recognition using Artificial Neural Networks</title>
    <summary>  The recognition of optical characters is known to be one of the earliest
applications of Artificial Neural Networks, which partially emulate human
thinking in the domain of artificial intelligence. In this paper, a simplified
neural approach to recognition of optical or visual characters is portrayed and
discussed. The document is expected to serve as a resource for learners and
amateur investigators in pattern recognition, neural networking and related
disciplines.
</summary>
    <author>
      <name>Shashank Araokar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, tutorial resource</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0406047v1</id>
    <updated>2004-06-24T13:14:58Z</updated>
    <published>2004-06-24T13:14:58Z</published>
    <title>Self-organizing neural networks in classification and image recognition</title>
    <summary>  Self-organizing neural networks are used for brick finding in OPERA
experiment. Self-organizing neural networks and wavelet analysis used for
recognition and extraction of car numbers from images.
</summary>
    <author>
      <name>G. A. Ososkov</name>
    </author>
    <author>
      <name>S. G. Dmitrievskiy</name>
    </author>
    <author>
      <name>A. V. Stadnik</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0406047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0406047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.5081v2</id>
    <updated>2011-06-28T21:32:26Z</updated>
    <published>2011-03-25T20:59:13Z</published>
    <title>Using Variable Threshold to Increase Capacity in a Feedback Neural
  Network</title>
    <summary>  The article presents new results on the use of variable thresholds to
increase the capacity of a feedback neural network. Non-binary networks are
also considered in this analysis.
</summary>
    <author>
      <name>Praveen Kuruvada</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1103.5081v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.5081v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.00712v1</id>
    <updated>2016-12-02T15:46:09Z</updated>
    <published>2016-12-02T15:46:09Z</published>
    <title>Probabilistic Neural Programs</title>
    <summary>  We present probabilistic neural programs, a framework for program induction
that permits flexible specification of both a computational model and inference
algorithm while simultaneously enabling the use of deep neural networks.
Probabilistic neural programs combine a computation graph for specifying a
neural network with an operator for weighted nondeterministic choice. Thus, a
program describes both a collection of decisions as well as the neural network
architecture used to make each one. We evaluate our approach on a challenging
diagram question answering task where probabilistic neural programs correctly
execute nearly twice as many programs as a baseline model.
</summary>
    <author>
      <name>Kenton W. Murray</name>
    </author>
    <author>
      <name>Jayant Krishnamurthy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in NAMPI workshop at NIPS 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.00712v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.00712v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.3115v1</id>
    <updated>2014-03-12T21:19:26Z</updated>
    <published>2014-03-12T21:19:26Z</published>
    <title>Memory Capacity of Neural Networks using a Circulant Weight Matrix</title>
    <summary>  This paper presents results on the memory capacity of a generalized feedback
neural network using a circulant matrix. Children are capable of learning soon
after birth which indicates that the neural networks of the brain have prior
learnt capacity that is a consequence of the regular structures in the brain's
organization. Motivated by this idea, we consider the capacity of circulant
matrices as weight matrices in a feedback network.
</summary>
    <author>
      <name>Vamsi Sashank Kotagiri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.3115v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.3115v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04682v1</id>
    <updated>2016-08-16T17:38:45Z</updated>
    <published>2016-08-16T17:38:45Z</published>
    <title>Extent of error control in neural networks</title>
    <summary>  The article sets and solves the task to control an error of the artificial
neural network with variable signal conductivity. This kind of neural networks
was especially developed to construct timetables. Behavior of such a neural
network can be described as dynamic system control problem. The authors gave as
the results of the solving the ANN feedback control problem.
</summary>
    <author>
      <name>Alexander Ignatenkov</name>
    </author>
    <author>
      <name>Alexey Olshansky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure, 11 references</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.04682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="49L20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.01430v2</id>
    <updated>2016-10-06T13:28:41Z</updated>
    <published>2016-10-05T14:14:51Z</published>
    <title>LAYERS: Yet another Neural Network toolkit</title>
    <summary>  Layers is an open source neural network toolkit aim at providing an easy way
to implement modern neural networks. The main user target are students and to
this end layers provides an easy scriptting language that can be early adopted.
The user has to focus only on design details as network totpology and parameter
tunning.
</summary>
    <author>
      <name>Roberto Paredes</name>
    </author>
    <author>
      <name>Jos√©-Miguel Bened√≠</name>
    </author>
    <link href="http://arxiv.org/abs/1610.01430v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.01430v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.06511v2</id>
    <updated>2017-10-02T22:45:53Z</updated>
    <published>2017-06-20T15:17:37Z</published>
    <title>Optimal modularity and memory capacity of neural networks</title>
    <summary>  The neural network is a powerful computing framework that has been exploited
by biological evolution and by humans for solving diverse problems. Although
the computational capabilities of neural networks are determined by their
structure, the current understanding of the relationships between a neural
network's architecture and function is still primitive. Here we reveal that
neural network's modular architecture plays a vital role in determining the
neural dynamics and memory performance of the network. In particular, we
demonstrate that there exists an optimal modularity for memory performance,
where a balance between local cohesion and global connectivity is established,
allowing optimally modular networks to remember longer. Our results suggest
that insights from dynamical analysis of neural networks and information
spreading processes can be leveraged to better design neural networks and may
shed light on the brain's modular organization.
</summary>
    <author>
      <name>Nathaniel Rodriguez</name>
    </author>
    <author>
      <name>Eduardo Izquierdo</name>
    </author>
    <author>
      <name>Yong-Yeol Ahn</name>
    </author>
    <link href="http://arxiv.org/abs/1706.06511v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.06511v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.07252v1</id>
    <updated>2017-08-24T02:14:50Z</updated>
    <published>2017-08-24T02:14:50Z</published>
    <title>A Study on Neural Network Language Modeling</title>
    <summary>  An exhaustive study on neural network language modeling (NNLM) is performed
in this paper. Different architectures of basic neural network language models
are described and examined. A number of different improvements over basic
neural network language models, including importance sampling, word classes,
caching and bidirectional recurrent neural network (BiRNN), are studied
separately, and the advantages and disadvantages of every technique are
evaluated. Then, the limits of neural network language modeling are explored
from the aspects of model architecture and knowledge representation. Part of
the statistical information from a word sequence will loss when it is processed
word by word in a certain order, and the mechanism of training neural network
by updating weight matrixes and vectors imposes severe restrictions on any
significant enhancement of NNLM. For knowledge representation, the knowledge
represented by neural network language models is the approximate probabilistic
distribution of word sequences from a certain training data set rather than the
knowledge of a language itself or the information conveyed by word sequences in
a natural language. Finally, some directions for improving neural network
language modeling further is discussed.
</summary>
    <author>
      <name>Dengliang Shi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.07252v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.07252v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.04035v1</id>
    <updated>2016-12-13T05:40:20Z</updated>
    <published>2016-12-13T05:40:20Z</published>
    <title>DizzyRNN: Reparameterizing Recurrent Neural Networks for Norm-Preserving
  Backpropagation</title>
    <summary>  The vanishing and exploding gradient problems are well-studied obstacles that
make it difficult for recurrent neural networks to learn long-term time
dependencies. We propose a reparameterization of standard recurrent neural
networks to update linear transformations in a provably norm-preserving way
through Givens rotations. Additionally, we use the absolute value function as
an element-wise non-linearity to preserve the norm of backpropagated signals
over the entire network. We show that this reparameterization reduces the
number of parameters and maintains the same algorithmic complexity as a
standard recurrent neural network, while outperforming standard recurrent
neural networks with orthogonal initializations and Long Short-Term Memory
networks on the copy problem.
</summary>
    <author>
      <name>Victor Dorobantu</name>
    </author>
    <author>
      <name>Per Andre Stromhaug</name>
    </author>
    <author>
      <name>Jess Renteria</name>
    </author>
    <link href="http://arxiv.org/abs/1612.04035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.04035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.06729v1</id>
    <updated>2017-07-21T00:50:46Z</updated>
    <published>2017-07-21T00:50:46Z</published>
    <title>Predictive networking and optimization for flow-based networks</title>
    <summary>  Artificial Neural Networks (ANNs) were used to classify neural network flows
by flow size. After training the neural network was able to predict the size of
a flows with 87% accuracy with a Feed Forward Neural Network. This demonstrates
that flow based routers can prioritize candidate flows with a predicted large
number of packets for priority insertion into hardware content-addressable
memory.
</summary>
    <author>
      <name>Michael Arnold</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A thesis submitted for the Master of Science Degree at The University
  of Alabama in Huntsville</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.06729v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.06729v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.01427v3</id>
    <updated>2017-03-30T19:51:47Z</updated>
    <published>2016-11-04T15:47:32Z</published>
    <title>Sparsely-Connected Neural Networks: Towards Efficient VLSI
  Implementation of Deep Neural Networks</title>
    <summary>  Recently deep neural networks have received considerable attention due to
their ability to extract and represent high-level abstractions in data sets.
Deep neural networks such as fully-connected and convolutional neural networks
have shown excellent performance on a wide range of recognition and
classification tasks. However, their hardware implementations currently suffer
from large silicon area and high power consumption due to the their high degree
of complexity. The power/energy consumption of neural networks is dominated by
memory accesses, the majority of which occur in fully-connected networks. In
fact, they contain most of the deep neural network parameters. In this paper,
we propose sparsely-connected networks, by showing that the number of
connections in fully-connected networks can be reduced by up to 90% while
improving the accuracy performance on three popular datasets (MNIST, CIFAR10
and SVHN). We then propose an efficient hardware architecture based on
linear-feedback shift registers to reduce the memory requirements of the
proposed sparsely-connected networks. The proposed architecture can save up to
90% of memory compared to the conventional implementations of fully-connected
neural networks. Moreover, implementation results show up to 84% reduction in
the energy consumption of a single neuron of the proposed sparsely-connected
networks compared to a single neuron of fully-connected neural networks.
</summary>
    <author>
      <name>Arash Ardakani</name>
    </author>
    <author>
      <name>Carlo Condo</name>
    </author>
    <author>
      <name>Warren J. Gross</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at ICLR 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.01427v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.01427v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9811031v1</id>
    <updated>1998-11-24T23:33:12Z</updated>
    <published>1998-11-24T23:33:12Z</published>
    <title>Speech Synthesis with Neural Networks</title>
    <summary>  Text-to-speech conversion has traditionally been performed either by
concatenating short samples of speech or by using rule-based systems to convert
a phonetic representation of speech into an acoustic representation, which is
then converted into speech. This paper describes a system that uses a
time-delay neural network (TDNN) to perform this phonetic-to-acoustic mapping,
with another neural network to control the timing of the generated speech. The
neural network system requires less memory than a concatenation system, and
performed well in tests comparing it to commercial systems using other
technologies.
</summary>
    <author>
      <name>Orhan Karaali</name>
    </author>
    <author>
      <name>Gerald Corrigan</name>
    </author>
    <author>
      <name>Ira Gerson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, PostScript</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">World Congress on Neural Networks (1996) 45-50. San Diego</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9811031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9811031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/nlin/0202038v1</id>
    <updated>2002-02-19T12:39:23Z</updated>
    <published>2002-02-19T12:39:23Z</published>
    <title>On model selection and the disability of neural networks to decompose
  tasks</title>
    <summary>  A neural network with fixed topology can be regarded as a parametrization of
functions, which decides on the correlations between functional variations when
parameters are adapted. We propose an analysis, based on a differential
geometry point of view, that allows to calculate these correlations. In
practise, this describes how one response is unlearned while another is
trained. Concerning conventional feed-forward neural networks we find that they
generically introduce strong correlations, are predisposed to forgetting, and
inappropriate for task decomposition. Perspectives to solve these problems are
discussed.
</summary>
    <author>
      <name>Marc Toussaint</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LaTeX, 7 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Joint Conference on Neural
  Networks (IJCNN 2002), 245-250.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/nlin/0202038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/nlin/0202038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.4170v1</id>
    <updated>2012-02-19T16:56:45Z</updated>
    <published>2012-02-19T16:56:45Z</published>
    <title>Classification by Ensembles of Neural Networks</title>
    <summary>  We introduce a new procedure for training of artificial neural networks by
using the approximation of an objective function by arithmetic mean of an
ensemble of selected randomly generated neural networks, and apply this
procedure to the classification (or pattern recognition) problem. This approach
differs from the standard one based on the optimization theory. In particular,
any neural network from the mentioned ensemble may not be an approximation of
the objective function.
</summary>
    <author>
      <name>S. V. Kozyrev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, LaTeX</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">p-Adic Numbers, Ultrametric Analysis and Applications, 2012, Vol.
  4, No. 1, pp. 27-33</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1202.4170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.4170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.2468v1</id>
    <updated>2014-01-10T21:09:36Z</updated>
    <published>2014-01-10T21:09:36Z</published>
    <title>N2Sky - Neural Networks as Services in the Clouds</title>
    <summary>  We present the N2Sky system, which provides a framework for the exchange of
neural network specific knowledge, as neural network paradigms and objects, by
a virtual organization environment. It follows the sky computing paradigm
delivering ample resources by the usage of federated Clouds. N2Sky is a novel
Cloud-based neural network simulation environment, which follows a pure service
oriented approach. The system implements a transparent environment aiming to
enable both novice and experienced users to do neural network research easily
and comfortably. N2Sky is built using the RAVO reference architecture of
virtual organizations which allows itself naturally integrating into the Cloud
service stack (SaaS, PaaS, and IaaS) of service oriented architectures.
</summary>
    <author>
      <name>Erich Schikuta</name>
    </author>
    <author>
      <name>Erwin Mann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">extended version of paper published at IJCNN 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.2468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.2468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.5; I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.06662v1</id>
    <updated>2017-09-19T22:21:49Z</updated>
    <published>2017-09-19T22:21:49Z</published>
    <title>Verifying Properties of Binarized Deep Neural Networks</title>
    <summary>  Understanding properties of deep neural networks is an important challenge in
deep learning. In this paper, we take a step in this direction by proposing a
rigorous way of verifying properties of a popular class of neural networks,
Binarized Neural Networks, using the well-developed means of Boolean
satisfiability. Our main contribution is a construction that creates a
representation of a binarized neural network as a Boolean formula. Our encoding
is the first exact Boolean representation of a deep neural network. Using this
encoding, we leverage the power of modern SAT solvers along with a proposed
counterexample-guided search procedure to verify various properties of these
networks. A particular focus will be on the critical property of robustness to
adversarial perturbations. For this property, our experimental results
demonstrate that our approach scales to medium-size deep neural networks used
in image classification tasks. To the best of our knowledge, this is the first
work on verifying properties of deep neural networks using an exact Boolean
encoding of the network.
</summary>
    <author>
      <name>Nina Narodytska</name>
    </author>
    <author>
      <name>Shiva Prasad Kasiviswanathan</name>
    </author>
    <author>
      <name>Leonid Ryzhyk</name>
    </author>
    <author>
      <name>Mooly Sagiv</name>
    </author>
    <author>
      <name>Toby Walsh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.06662v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.06662v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.1164v1</id>
    <updated>2010-02-05T09:20:51Z</updated>
    <published>2010-02-05T09:20:51Z</published>
    <title>Existence and Global Logarithmic Stability of Impulsive Neural Networks
  with Time Delay</title>
    <summary>  The stability and convergence of the neural networks are the fundamental
characteristics in the Hopfield type networks. Since time delay is ubiquitous
in most physical and biological systems, more attention is being made for the
delayed neural networks. The inclusion of time delay into a neural model is
natural due to the finite transmission time of the interactions. The stability
analysis of the neural networks depends on the Lyapunov function and hence it
must be constructed for the given system. In this paper we have made an attempt
to establish the logarithmic stability of the impulsive delayed neural networks
by constructing suitable Lyapunov function.
</summary>
    <author>
      <name>A. K. Ojha</name>
    </author>
    <author>
      <name>Dushmanta Mallick</name>
    </author>
    <author>
      <name>C. Mallick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues, IJCSI, Vol. 7,
  Issue 1, No. 2, January 2010,
  http://ijcsi.org/articles/Existence-and-Global-Logarithmic-Stability-of-Impulsive-Neural-Networks-with-Time-Delay.php</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues, IJCSI, Vol. 7,
  Issue 1, No. 2, January 2010,
  http://ijcsi.org/articles/Existence-and-Global-Logarithmic-Stability-of-Impulsive-Neural-Networks-with-Time-Delay.php</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1002.1164v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.1164v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.0215v1</id>
    <updated>2012-12-02T15:07:56Z</updated>
    <published>2012-12-02T15:07:56Z</published>
    <title>Artificial Neural Network for Performance Modeling and Optimization of
  CMOS Analog Circuits</title>
    <summary>  This paper presents an implementation of multilayer feed forward neural
networks (NN) to optimize CMOS analog circuits. For modeling and design
recently neural network computational modules have got acceptance as an
unorthodox and useful tool. To achieve high performance of active or passive
circuit component neural network can be trained accordingly. A well trained
neural network can produce more accurate outcome depending on its learning
capability. Neural network model can replace empirical modeling solutions
limited by range and accuracy.[2] Neural network models are easy to obtain for
new circuits or devices which can replace analytical methods. Numerical
modeling methods can also be replaced by neural network model due to their
computationally expansive behavior.[2][10][20]. The pro- posed implementation
is aimed at reducing resource requirement, without much compromise on the
speed. The NN ensures proper functioning by assigning the appropriate inputs,
weights, biases, and excitation function of the layer that is currently being
computed. The concept used is shown to be very effective in reducing resource
requirements and enhancing speed.
</summary>
    <author>
      <name>Mriganka Chakraborty</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/9380-3731</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/9380-3731" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications November 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.0215v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.0215v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.4855v1</id>
    <updated>2012-09-20T14:14:59Z</updated>
    <published>2012-09-20T14:14:59Z</published>
    <title>The Future of Neural Networks</title>
    <summary>  The paper describes some recent developments in neural networks and discusses
the applicability of neural networks in the development of a machine that
mimics the human brain. The paper mentions a new architecture, the pulsed
neural network that is being considered as the next generation of neural
networks. The paper also explores the use of memristors in the development of a
brain-like computer called the MoNETA. A new model, multi/infinite dimensional
neural networks, are a recent development in the area of advanced neural
networks. The paper concludes that the need of neural networks in the
development of human-like technology is essential and may be non-expendable for
it.
</summary>
    <author>
      <name>Sachin Lakra</name>
    </author>
    <author>
      <name>T. V. Prasad</name>
    </author>
    <author>
      <name>G. Ramakrishna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Published in proceedings of 6th National Conference on Computing
  for Nation Development, INDIACom 2012, New Delhi, India, 23-24 February,
  2012, pp. 481-486</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1209.4855v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.4855v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.05683v1</id>
    <updated>2017-06-18T16:30:25Z</updated>
    <published>2017-06-18T16:30:25Z</published>
    <title>Sparse Neural Networks Topologies</title>
    <summary>  We propose Sparse Neural Network architectures that are based on random or
structured bipartite graph topologies. Sparse architectures provide compression
of the models learned and speed-ups of computations, they can also surpass
their unstructured or fully connected counterparts. As we show, even more
compact topologies of the so-called SNN (Sparse Neural Network) can be achieved
with the use of structured graphs of connections between consecutive layers of
neurons. In this paper, we investigate how the accuracy and training speed of
the models depend on the topology and sparsity of the neural network. Previous
approaches using sparcity are all based on fully connected neural network
models and create sparcity during training phase, instead we explicitly define
a sparse architectures of connections before the training. Building compact
neural network models is coherent with empirical observations showing that
there is much redundancy in learned neural network models. We show
experimentally that the accuracy of the models learned with neural networks
depends on expander-like properties of the underlying topologies such as the
spectral gap and algebraic connectivity rather than the density of the graphs
of connections.
</summary>
    <author>
      <name>Alfred Bourely</name>
    </author>
    <author>
      <name>John Patrick Boueri</name>
    </author>
    <author>
      <name>Krzysztof Choromonski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.05683v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.05683v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.07373v1</id>
    <updated>2016-02-24T02:39:47Z</updated>
    <published>2016-02-24T02:39:47Z</published>
    <title>On Study of the Binarized Deep Neural Network for Image Classification</title>
    <summary>  Recently, the deep neural network (derived from the artificial neural
network) has attracted many researchers' attention by its outstanding
performance. However, since this network requires high-performance GPUs and
large storage, it is very hard to use it on individual devices. In order to
improve the deep neural network, many trials have been made by refining the
network structure or training strategy. Unlike those trials, in this paper, we
focused on the basic propagation function of the artificial neural network and
proposed the binarized deep neural network. This network is a pure binary
system, in which all the values and calculations are binarized. As a result,
our network can save a lot of computational resource and storage. Therefore, it
is possible to use it on various devices. Moreover, the experimental results
proved the feasibility of the proposed network.
</summary>
    <author>
      <name>Song Wang</name>
    </author>
    <author>
      <name>Dongchun Ren</name>
    </author>
    <author>
      <name>Li Chen</name>
    </author>
    <author>
      <name>Wei Fan</name>
    </author>
    <author>
      <name>Jun Sun</name>
    </author>
    <author>
      <name>Satoshi Naoi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures. Rejected conference (CVPR 2015) submission.
  Submission date: November, 2014. This work is patented in China (NO.
  201410647710.3)</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.07373v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.07373v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09444v1</id>
    <updated>2016-11-29T00:39:45Z</updated>
    <published>2016-11-29T00:39:45Z</published>
    <title>The empirical size of trained neural networks</title>
    <summary>  ReLU neural networks define piecewise linear functions of their inputs.
However, initializing and training a neural network is very different from
fitting a linear spline. In this paper, we expand empirically upon previous
theoretical work to demonstrate features of trained neural networks. Standard
network initialization and training produce networks vastly simpler than a
naive parameter count would suggest and can impart odd features to the trained
network. However, we also show the forced simplicity is beneficial and, indeed,
critical for the wide success of these networks.
</summary>
    <author>
      <name>Kevin K. Chen</name>
    </author>
    <author>
      <name>Anthony Gamst</name>
    </author>
    <author>
      <name>Alden Walker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.09444v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09444v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.04747v1</id>
    <updated>2017-08-16T02:17:59Z</updated>
    <published>2017-08-16T02:17:59Z</published>
    <title>An Improved Neural Segmentation Method Based on U-NET</title>
    <summary>  Neural segmentation has a great impact on the smooth implementation of local
anesthesia surgery. At present, the network for the segmentation includes U-NET
[1] and SegNet [2]. U-NET network has short training time and less training
parameters, but the depth is not deep enough. SegNet network has deeper
structure, but it needs longer training time, and more training samples. In
this paper, we propose an improved U-NET neural network for the segmentation.
This network deepens the original structure through importing residual network.
Compared with U-NET and SegNet, the improved U-NET network has fewer training
parameters, shorter training time and get a great improvement in segmentation
effect. The improved U-NET network structure has a good application scene in
neural segmentation.
</summary>
    <author>
      <name>Chenyang Xu</name>
    </author>
    <author>
      <name>Mengxin Li</name>
    </author>
    <link href="http://arxiv.org/abs/1708.04747v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.04747v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/chao-dyn/9701021v1</id>
    <updated>1997-01-22T08:41:37Z</updated>
    <published>1997-01-22T08:41:37Z</published>
    <title>Chaotic Simulated Annealing by A Neural Network Model with Transient
  Chaos</title>
    <summary>  We propose a neural network model with transient chaos, or a transiently
chaotic neural network (TCNN) as an approximation method for combinatorial
optimization problem, by introducing transiently chaotic dynamics into neural
networks. Unlike conventional neural networks only with point attractors, the
proposed neural network has richer and more flexible dynamics, so that it can
be expected to have higher ability of searching for globally optimal or
near-optimal solutions. A significant property of this model is that the
chaotic neurodynamics is temporarily generated for searching and
self-organizing, and eventually vanishes with autonomous decreasing of a
bifurcation parameter corresponding to the "temperature" in usual annealing
process. Therefore, the neural network gradually approaches, through the
transient chaos, to dynamical structure similar to such conventional models as
the Hopfield neural network which converges to a stable equilibrium point.
Since the optimization process of the transiently chaotic neural network is
similar to simulated annealing, not in a stochastic way but in a
deterministically chaotic way, the new method is regarded as chaotic simulated
annealing (CSA). Fundamental characteristics of the transiently chaotic
neurodynamics are numerically investigated with examples of a single neuron
model and the Traveling Salesman Problem (TSP). Moreover, a maintenance
scheduling problem for generators in a practical power system is also analysed
to verify practical efficiency of this new method.
</summary>
    <author>
      <name>Luonan Chen</name>
    </author>
    <author>
      <name>Kazuyuki Aihara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">the theoretical results related to this paper should be referred to
  "Chaos and Asymptotical Stability in Discrete-time Neural Networks" by L.Chen
  and K.Aihara, Physica D (in press). Journal ref.: Neural Networks, Vol.8,
  No.6, pp.915-930, 1995</arxiv:comment>
    <link href="http://arxiv.org/abs/chao-dyn/9701021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/chao-dyn/9701021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="chao-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="chao-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.0213v1</id>
    <updated>2007-10-01T06:51:42Z</updated>
    <published>2007-10-01T06:51:42Z</published>
    <title>Optimising the topology of complex neural networks</title>
    <summary>  In this paper, we study instances of complex neural networks, i.e. neural
netwo rks with complex topologies. We use Self-Organizing Map neural networks
whose n eighbourhood relationships are defined by a complex network, to
classify handwr itten digits. We show that topology has a small impact on
performance and robus tness to neuron failures, at least at long learning
times. Performance may howe ver be increased (by almost 10%) by artificial
evolution of the network topo logy. In our experimental conditions, the evolved
networks are more random than their parents, but display a more heterogeneous
degree distribution.
</summary>
    <author>
      <name>Fei Jiang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs, INRIA Futurs</arxiv:affiliation>
    </author>
    <author>
      <name>Hugues Berry</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs</arxiv:affiliation>
    </author>
    <author>
      <name>Marc Schoenauer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans ECCS'07 (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.0213v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.0213v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.1141v2</id>
    <updated>2014-10-28T19:14:37Z</updated>
    <published>2014-10-05T10:54:07Z</published>
    <title>On the Computational Efficiency of Training Neural Networks</title>
    <summary>  It is well-known that neural networks are computationally hard to train. On
the other hand, in practice, modern day neural networks are trained efficiently
using SGD and a variety of tricks that include different activation functions
(e.g. ReLU), over-specification (i.e., train networks which are larger than
needed), and regularization. In this paper we revisit the computational
complexity of training neural networks from a modern perspective. We provide
both positive and negative results, some of them yield new provably efficient
and practical algorithms for training certain types of neural networks.
</summary>
    <author>
      <name>Roi Livni</name>
    </author>
    <author>
      <name>Shai Shalev-Shwartz</name>
    </author>
    <author>
      <name>Ohad Shamir</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Section 2 is revised due to a mistake</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.1141v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.1141v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.01000v1</id>
    <updated>2016-09-04T23:57:43Z</updated>
    <published>2016-09-04T23:57:43Z</published>
    <title>Convexified Convolutional Neural Networks</title>
    <summary>  We describe the class of convexified convolutional neural networks (CCNNs),
which capture the parameter sharing of convolutional neural networks in a
convex manner. By representing the nonlinear convolutional filters as vectors
in a reproducing kernel Hilbert space, the CNN parameters can be represented as
a low-rank matrix, which can be relaxed to obtain a convex optimization
problem. For learning two-layer convolutional neural networks, we prove that
the generalization error obtained by a convexified CNN converges to that of the
best possible CNN. For learning deeper networks, we train CCNNs in a layer-wise
manner. Empirically, CCNNs achieve performance competitive with CNNs trained by
backpropagation, SVMs, fully-connected neural networks, stacked denoising
auto-encoders, and other baseline methods.
</summary>
    <author>
      <name>Yuchen Zhang</name>
    </author>
    <author>
      <name>Percy Liang</name>
    </author>
    <author>
      <name>Martin J. Wainwright</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.01000v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.01000v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.06570v1</id>
    <updated>2017-10-18T03:05:47Z</updated>
    <published>2017-10-18T03:05:47Z</published>
    <title>A Correspondence Between Random Neural Networks and Statistical Field
  Theory</title>
    <summary>  A number of recent papers have provided evidence that practical design
questions about neural networks may be tackled theoretically by studying the
behavior of random networks. However, until now the tools available for
analyzing random neural networks have been relatively ad-hoc. In this work, we
show that the distribution of pre-activations in random neural networks can be
exactly mapped onto lattice models in statistical physics. We argue that
several previous investigations of stochastic networks actually studied a
particular factorial approximation to the full lattice model. For random linear
networks and random rectified linear networks we show that the corresponding
lattice models in the wide network limit may be systematically approximated by
a Gaussian distribution with covariance between the layers of the network. In
each case, the approximate distribution can be diagonalized by Fourier
transformation. We show that this approximation accurately describes the
results of numerical simulations of wide random neural networks. Finally, we
demonstrate that in each case the large scale behavior of the random networks
can be approximated by an effective field theory.
</summary>
    <author>
      <name>Samuel S. Schoenholz</name>
    </author>
    <author>
      <name>Jeffrey Pennington</name>
    </author>
    <author>
      <name>Jascha Sohl-Dickstein</name>
    </author>
    <link href="http://arxiv.org/abs/1710.06570v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.06570v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.05267v1</id>
    <updated>2016-09-16T18:10:18Z</updated>
    <published>2016-09-16T18:10:18Z</published>
    <title>Rule Extraction Algorithm for Deep Neural Networks: A Review</title>
    <summary>  Despite the highest classification accuracy in wide varieties of application
areas, artificial neural network has one disadvantage. The way this Network
comes to a decision is not easily comprehensible. The lack of explanation
ability reduces the acceptability of neural network in data mining and decision
system. This drawback is the reason why researchers have proposed many rule
extraction algorithms to solve the problem. Recently, Deep Neural Network (DNN)
is achieving a profound result over the standard neural network for
classification and recognition problems. It is a hot machine learning area
proven both useful and innovative. This paper has thoroughly reviewed various
rule extraction algorithms, considering the classification scheme:
decompositional, pedagogical, and eclectics. It also presents the evaluation of
these algorithms based on the neural network structure with which the algorithm
is intended to work. The main contribution of this review is to show that there
is a limited study of rule extraction algorithm from DNN.
</summary>
    <author>
      <name>Tameru Hailesilassie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages,2 figures,IEEE Publication format, Keywords- Artificial
  neural network; Deep neural network; Rule extraction; Decompositional;
  Pedagogical; Eclectic</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">(IJCSIS) International Journal of Computer Science and Information
  Security,Vol. 14, No. 7, July 2016, page 371-381</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.05267v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.05267v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.0936v1</id>
    <updated>2009-12-04T21:33:10Z</updated>
    <published>2009-12-04T21:33:10Z</published>
    <title>Neural-estimator for the surface emission rate of atmospheric gases</title>
    <summary>  The emission rate of minority atmospheric gases is inferred by a new approach
based on neural networks. The neural network applied is the multi-layer
perceptron with backpropagation algorithm for learning. The identification of
these surface fluxes is an inverse problem. A comparison between the new
neural-inversion and regularized inverse solution id performed. The results
obtained from the neural networks are significantly better. In addition, the
inversion with the neural netwroks is fster than regularized approaches, after
training.
</summary>
    <author>
      <name>F. F. Paes</name>
    </author>
    <author>
      <name>H. F. Campos Velho</name>
    </author>
    <link href="http://arxiv.org/abs/0912.0936v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.0936v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.05955v1</id>
    <updated>2017-03-17T10:46:23Z</updated>
    <published>2017-03-17T10:46:23Z</published>
    <title>Implicit Gradient Neural Networks with a Positive-Definite Mass Matrix
  for Online Linear Equations Solving</title>
    <summary>  Motivated by the advantages achieved by implicit analogue net for solving
online linear equations, a novel implicit neural model is designed based on
conventional explicit gradient neural networks in this letter by introducing a
positive-definite mass matrix. In addition to taking the advantages of the
implicit neural dynamics, the proposed implicit gradient neural networks can
still achieve globally exponential convergence to the unique theoretical
solution of linear equations and also global stability even under no-solution
and multi-solution situations. Simulative results verify theoretical
convergence analysis on the proposed neural dynamics.
</summary>
    <author>
      <name>Ke Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Information Processing Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.05955v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.05955v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.1231v1</id>
    <updated>2014-06-04T23:00:05Z</updated>
    <published>2014-06-04T23:00:05Z</published>
    <title>Multi-task Neural Networks for QSAR Predictions</title>
    <summary>  Although artificial neural networks have occasionally been used for
Quantitative Structure-Activity/Property Relationship (QSAR/QSPR) studies in
the past, the literature has of late been dominated by other machine learning
techniques such as random forests. However, a variety of new neural net
techniques along with successful applications in other domains have renewed
interest in network approaches. In this work, inspired by the winning team's
use of neural networks in a recent QSAR competition, we used an artificial
neural network to learn a function that predicts activities of compounds for
multiple assays at the same time. We conducted experiments leveraging recent
methods for dealing with overfitting in neural networks as well as other tricks
from the neural networks literature. We compared our methods to alternative
methods reported to perform well on these tasks and found that our neural net
methods provided superior performance.
</summary>
    <author>
      <name>George E. Dahl</name>
    </author>
    <author>
      <name>Navdeep Jaitly</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <link href="http://arxiv.org/abs/1406.1231v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.1231v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04153v1</id>
    <updated>2016-04-14T13:48:26Z</updated>
    <published>2016-04-14T13:48:26Z</published>
    <title>Learning to Generate Genotypes with Neural Networks</title>
    <summary>  Neural networks and evolutionary computation have a rich intertwined history.
They most commonly appear together when an evolutionary algorithm optimises the
parameters and topology of a neural network for reinforcement learning
problems, or when a neural network is applied as a surrogate fitness function
to aid the evolutionary optimisation of expensive fitness functions. In this
paper we take a different approach, asking the question of whether a neural
network can be used to provide a mutation distribution for an evolutionary
algorithm, and what advantages this approach may offer? Two modern neural
network models are investigated, a Denoising Autoencoder modified to produce
stochastic outputs and the Neural Autoregressive Distribution Estimator.
Results show that the neural network approach to learning genotypes is able to
solve many difficult discrete problems, such as MaxSat and HIFF, and regularly
outperforms other evolutionary techniques.
</summary>
    <author>
      <name>Alexander W. Churchill</name>
    </author>
    <author>
      <name>Siddharth Sigtia</name>
    </author>
    <author>
      <name>Chrisantha Fernando</name>
    </author>
    <link href="http://arxiv.org/abs/1604.04153v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04153v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.04781v2</id>
    <updated>2015-11-06T19:41:13Z</updated>
    <published>2015-10-16T05:37:06Z</published>
    <title>A Survey: Time Travel in Deep Learning Space: An Introduction to Deep
  Learning Models and How Deep Learning Models Evolved from the Initial Ideas</title>
    <summary>  This report will show the history of deep learning evolves. It will trace
back as far as the initial belief of connectionism modelling of brain, and come
back to look at its early stage realization: neural networks. With the
background of neural network, we will gradually introduce how convolutional
neural network, as a representative of deep discriminative models, is developed
from neural networks, together with many practical techniques that can help in
optimization of neural networks. On the other hand, we will also trace back to
see the evolution history of deep generative models, to see how researchers
balance the representation power and computation complexity to reach Restricted
Boltzmann Machine and eventually reach Deep Belief Nets. Further, we will also
look into the development history of modelling time series data with neural
networks. We start with Time Delay Neural Networks and move further to
currently famous model named Recurrent Neural Network and its extension Long
Short Term Memory. We will also briefly look into how to construct deep
recurrent neural networks. Finally, we will conclude this report with some
interesting open-ended questions of deep neural networks.
</summary>
    <author>
      <name>Haohan Wang</name>
    </author>
    <author>
      <name>Bhiksha Raj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">43 pages, 31 figures. Fix typos in abstract</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.04781v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.04781v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00168v2</id>
    <updated>2017-10-04T06:55:48Z</updated>
    <published>2017-03-01T07:58:29Z</published>
    <title>Modular Representation of Layered Neural Networks</title>
    <summary>  Layered neural networks have greatly improved the performance of various
applications including image processing, speech recognition, natural language
processing, and bioinformatics. However, it is still difficult to discover or
interpret knowledge from the inference provided by a layered neural network,
since its internal representation has many nonlinear and complex parameters
embedded in hierarchical layers. Therefore, it becomes important to establish a
new methodology by which layered neural networks can be understood.
  In this paper, we propose a new method for extracting a global and simplified
structure from a layered neural network. Based on network analysis, the
proposed method detects communities or clusters of units with similar
connection patterns. We show its effectiveness by applying it to three use
cases. (1) Network decomposition: it can decompose a trained neural network
into multiple small independent networks thus dividing the problem and reducing
the computation time. (2) Training assessment: the appropriateness of a trained
result with a given hyperparameter or randomly chosen initial parameters can be
evaluated by using a modularity index. And (3) data analysis: in practical data
it reveals the community structure in the input, hidden, and output layers,
which serves as a clue for discovering knowledge from a trained neural network.
</summary>
    <author>
      <name>Chihiro Watanabe</name>
    </author>
    <author>
      <name>Kaoru Hiramatsu</name>
    </author>
    <author>
      <name>Kunio Kashino</name>
    </author>
    <link href="http://arxiv.org/abs/1703.00168v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00168v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0208453v1</id>
    <updated>2002-08-23T11:35:30Z</updated>
    <published>2002-08-23T11:35:30Z</published>
    <title>Neural Cryptography</title>
    <summary>  Two neural networks which are trained on their mutual output bits show a
novel phenomenon: The networks synchronize to a state with identical time
dependent weights. It is shown how synchronization by mutual learning can be
applied to cryptography: secret key exchange over a public channel.
</summary>
    <author>
      <name>Wolfgang Kinzel</name>
    </author>
    <author>
      <name>Ido Kanter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9th International Conference on Neural Information Processing,
  Singapore, Nov. 2002</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0208453v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0208453v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.0417v1</id>
    <updated>2010-07-02T18:05:29Z</updated>
    <published>2010-07-02T18:05:29Z</published>
    <title>Delta Learning Rule for the Active Sites Model</title>
    <summary>  This paper reports the results on methods of comparing the memory retrieval
capacity of the Hebbian neural network which implements the B-Matrix approach,
by using the Widrow-Hoff rule of learning. We then, extend the recently
proposed Active Sites model by developing a delta rule to increase memory
capacity. Also, this paper extends the binary neural network to a multi-level
(non-binary) neural network.
</summary>
    <author>
      <name>Krishna Chaithanya Lingashetty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 Pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1007.0417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.0417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.01809v1</id>
    <updated>2017-05-04T12:20:56Z</updated>
    <published>2017-05-04T12:20:56Z</published>
    <title>Pixel Normalization from Numeric Data as Input to Neural Networks</title>
    <summary>  Text to image transformation for input to neural networks requires
intermediate steps. This paper attempts to present a new approach to pixel
normalization so as to convert textual data into image, suitable as input for
neural networks. This method can be further improved by its Graphics Processing
Unit (GPU) implementation to provide significant speedup in computational time.
</summary>
    <author>
      <name>Parth Sane</name>
    </author>
    <author>
      <name>Ravindra Agrawal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE WiSPNET 2017 conference in Chennai</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.01809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504054v1</id>
    <updated>2005-04-13T13:40:38Z</updated>
    <published>2005-04-13T13:40:38Z</published>
    <title>Learning from Web: Review of Approaches</title>
    <summary>  Knowledge discovery is defined as non-trivial extraction of implicit,
previously unknown and potentially useful information from given data.
Knowledge extraction from web documents deals with unstructured, free-format
documents whose number is enormous and rapidly growing. The artificial neural
networks are well suitable to solve a problem of knowledge discovery from web
documents because trained networks are able more accurately and easily to
classify the learning and testing examples those represent the text mining
domain. However, the neural networks that consist of large number of weighted
connections and activation units often generate the incomprehensible and
hard-to-understand models of text classification. This problem may be also
addressed to most powerful recurrent neural networks that employ the feedback
links from hidden or output units to their input units. Due to feedback links,
recurrent neural networks are able take into account of a context in document.
To be useful for data mining, self-organizing neural network techniques of
knowledge extraction have been explored and developed. Self-organization
principles were used to create an adequate neural-network structure and reduce
a dimensionality of features used to describe text documents. The use of these
principles seems interesting because ones are able to reduce a neural-network
redundancy and considerably facilitate the knowledge representation.
</summary>
    <author>
      <name>Vitaly Schetinin</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0504054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.07588v2</id>
    <updated>2016-11-25T18:02:21Z</updated>
    <published>2016-11-23T00:11:41Z</published>
    <title>A Neural Network Model to Classify Liver Cancer Patients Using Data
  Expansion and Compression</title>
    <summary>  We develop a neural network model to classify liver cancer patients into
high-risk and low-risk groups using genomic data. Our approach provides a novel
technique to classify big data sets using neural network models. We preprocess
the data before training the neural network models. We first expand the data
using wavelet analysis. We then compress the wavelet coefficients by mapping
them onto a new scaled orthonormal coordinate system. Then the data is used to
train a neural network model that enables us to classify cancer patients into
two different classes of high-risk and low-risk patients. We use the
leave-one-out approach to build a neural network model. This neural network
model enables us to classify a patient using genomic data as a high-risk or
low-risk patient without any information about the survival time of the
patient. The results from genomic data analysis are compared with survival time
analysis. It is shown that the expansion and compression of data using wavelet
analysis and singular value decomposition (SVD) is essential to train the
neural network model.
</summary>
    <author>
      <name>Ashkan Zeinalzadeh</name>
    </author>
    <author>
      <name>Tom Wenska</name>
    </author>
    <author>
      <name>Gordon Okimoto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.07588v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.07588v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00144v4</id>
    <updated>2017-09-22T01:53:39Z</updated>
    <published>2017-03-01T05:38:16Z</published>
    <title>Theoretical Properties for Neural Networks with Weight Matrices of Low
  Displacement Rank</title>
    <summary>  Recently low displacement rank (LDR) matrices, or so-called structured
matrices, have been proposed to compress large-scale neural networks. Empirical
results have shown that neural networks with weight matrices of LDR matrices,
referred as LDR neural networks, can achieve significant reduction in space and
computational complexity while retaining high accuracy. We formally study LDR
matrices in deep learning. First, we prove the universal approximation property
of LDR neural networks with a mild condition on the displacement operators. We
then show that the error bounds of LDR neural networks are as efficient as
general neural networks with both single-layer and multiple-layer structure.
Finally, we propose back-propagation based training algorithm for general LDR
neural networks.
</summary>
    <author>
      <name>Liang Zhao</name>
    </author>
    <author>
      <name>Siyu Liao</name>
    </author>
    <author>
      <name>Yanzhi Wang</name>
    </author>
    <author>
      <name>Zhe Li</name>
    </author>
    <author>
      <name>Jian Tang</name>
    </author>
    <author>
      <name>Victor Pan</name>
    </author>
    <author>
      <name>Bo Yuan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.00144v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00144v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.07592v1</id>
    <updated>2017-11-21T01:11:00Z</updated>
    <published>2017-11-21T01:11:00Z</published>
    <title>Sparse-Input Neural Networks for High-dimensional Nonparametric
  Regression and Classification</title>
    <summary>  Neural networks are usually not the tool of choice for nonparametric
high-dimensional problems where the number of input features is much larger
than the number of observations. Though neural networks can approximate complex
multivariate functions, they generally require a large number of training
observations to obtain reasonable fits, unless one can learn the appropriate
network structure. In this manuscript, we show that neural networks can be
applied successfully to high-dimensional settings if the true function falls in
a low dimensional subspace, and proper regularization is used. We propose
fitting a neural network with a sparse group lasso penalty on the first-layer
input weights, which results in a neural net that only uses a small subset of
the original features. In addition, we characterize the statistical convergence
of the penalized empirical risk minimizer to the optimal neural network: we
show that the excess risk of this penalized estimator only grows with the
logarithm of the number of input features; and we show that the weights of
irrelevant features converge to zero. Via simulation studies and data analyses,
we show that these sparse-input neural networks outperform existing
nonparametric high-dimensional estimation methods when the data has complex
higher-order interactions.
</summary>
    <author>
      <name>Jean Feng</name>
    </author>
    <author>
      <name>Noah Simon</name>
    </author>
    <link href="http://arxiv.org/abs/1711.07592v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.07592v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.08934v1</id>
    <updated>2017-12-24T14:54:41Z</updated>
    <published>2017-12-24T14:54:41Z</published>
    <title>A Survey of FPGA Based Neural Network Accelerator</title>
    <summary>  Recent researches on neural network have shown great advantage in computer
vision over traditional algorithms based on handcrafted features and models.
Neural network is now widely adopted in regions like image, speech and video
recognition. But the great computation and storage complexity of neural network
based algorithms poses great difficulty on its application. CPU platforms are
hard to offer enough computation capacity. GPU platforms are the first choice
for neural network process because of its high computation capacity and easy to
use development frameworks.
  On the other hand, FPGA based neural network accelerator is becoming a
research topic. Because specific designed hardware is the next possible
solution to surpass GPU in speed and energy efficiency. Various FPGA based
accelerator designs have been proposed with software and hardware optimization
techniques to achieve high speed and energy efficiency. In this paper, we give
an overview of previous work on neural network accelerators based on FPGA and
summarize the main techniques used. Investigation from software to hardware,
from circuit level to system level is carried out to complete analysis of FPGA
based neural network accelerator design and serves as a guide to future work.
</summary>
    <author>
      <name>Kaiyuan Guo</name>
    </author>
    <author>
      <name>Shulin Zeng</name>
    </author>
    <author>
      <name>Jincheng Yu</name>
    </author>
    <author>
      <name>Yu Wang</name>
    </author>
    <author>
      <name>Huazhong Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1712.08934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.08934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04393v3</id>
    <updated>2017-02-06T22:51:33Z</updated>
    <published>2016-06-14T14:36:55Z</published>
    <title>Deep Learning with Darwin: Evolutionary Synthesis of Deep Neural
  Networks</title>
    <summary>  Taking inspiration from biological evolution, we explore the idea of "Can
deep neural networks evolve naturally over successive generations into highly
efficient deep neural networks?" by introducing the notion of synthesizing new
highly efficient, yet powerful deep neural networks over successive generations
via an evolutionary process from ancestor deep neural networks. The
architectural traits of ancestor deep neural networks are encoded using
synaptic probability models, which can be viewed as the `DNA' of these
networks. New descendant networks with differing network architectures are
synthesized based on these synaptic probability models from the ancestor
networks and computational environmental factor models, in a random manner to
mimic heredity, natural selection, and random mutation. These offspring
networks are then trained into fully functional networks, like one would train
a newborn, and have more efficient, more diverse network architectures than
their ancestor networks, while achieving powerful modeling capabilities.
Experimental results for the task of visual saliency demonstrated that the
synthesized `evolved' offspring networks can achieve state-of-the-art
performance while having network architectures that are significantly more
efficient (with a staggering $\sim$48-fold decrease in synapses by the fourth
generation) compared to the original ancestor network.
</summary>
    <author>
      <name>Mohammad Javad Shafiee</name>
    </author>
    <author>
      <name>Akshaya Mishra</name>
    </author>
    <author>
      <name>Alexander Wong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.04393v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04393v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09598v1</id>
    <updated>2017-06-29T07:13:42Z</updated>
    <published>2017-06-29T07:13:42Z</published>
    <title>CS591 Report: Application of siamesa network in 2D transformation</title>
    <summary>  Deep learning has been extensively used various aspects of computer vision
area. Deep learning separate itself from traditional neural network by having a
much deeper and complicated network layers in its network structures.
Traditionally, deep neural network is abundantly used in computer vision tasks
including classification and detection and has achieve remarkable success and
set up a new state of the art results in these fields. Instead of using neural
network for vision recognition and detection. I will show the ability of neural
network to do image registration, synthesis of images and image retrieval in
this report.
</summary>
    <author>
      <name>Dorothy Chang</name>
    </author>
    <link href="http://arxiv.org/abs/1706.09598v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09598v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0212486v1</id>
    <updated>2002-12-19T14:46:01Z</updated>
    <published>2002-12-19T14:46:01Z</published>
    <title>Neural Networks, Game Theory and Time Series Generation</title>
    <summary>  This dissertation highlights connections between the fields of neural
networks, game theory and time series generation. The concept of
antipredictability is explained, and the properties of time series that are
antipredictable for several prototypical prediction algorithms (neural
networks, Boolean funtions etc.) are studied. The Minority Game provides a
framework in which antipredictability arises naturally. Several variations of
the MG are introduced and compared, including extensions to more than two
choices, and the properties of the generated time series are analysed. A
learning algorithm is presented by which a neural network can find a good mixed
strategy in zero-sum matrix games. In a certain limit, this algorithm is a
stochastic variation of the "fictitious play" learning algorithm.
</summary>
    <author>
      <name>Richard Metzler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Dissertation. 130 pages, quite a few figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0212486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0212486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0603396v1</id>
    <updated>2006-03-15T00:19:58Z</updated>
    <published>2006-03-15T00:19:58Z</published>
    <title>Periodic Neural Activity Induced by Network Complexity</title>
    <summary>  We study a model for neural activity on the small-world topology of Watts and
Strogatz and on the scale-free topology of Barab\'asi and Albert. We find that
the topology of the network connections may spontaneously induce periodic
neural activity, contrasting with chaotic neural activities exhibited by
regular topologies. Periodic activity exists only for relatively small networks
and occurs with higher probability when the rewiring probability is larger. The
average length of the periods increases with the square root of the network
size.
</summary>
    <author>
      <name>D. R. Paula</name>
    </author>
    <author>
      <name>A. D. Araujo</name>
    </author>
    <author>
      <name>J. S. Andrade Jr</name>
    </author>
    <author>
      <name>H. J. Herrmann</name>
    </author>
    <author>
      <name>J. A. C. Gallas</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.74.017102</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.74.017102" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0603396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0603396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504053v1</id>
    <updated>2005-04-13T13:28:15Z</updated>
    <published>2005-04-13T13:28:15Z</published>
    <title>A Neural-Network Technique for Recognition of Filaments in Solar Images</title>
    <summary>  We describe a new neural-network technique developed for an automated
recognition of solar filaments visible in the hydrogen H-alpha line full disk
spectroheliograms. This technique allows neural networks learn from a few image
fragments labelled manually to recognize the single filaments depicted on a
local background. The trained network is able to recognize filaments depicted
on the backgrounds with variations in brightness caused by atmospherics
distortions. Despite the difference in backgrounds in our experiments the
neural network has properly recognized filaments in the testing image
fragments. Using a parabolic activation function we extend this technique to
recognize multiple solar filaments which may appear in one fragment.
</summary>
    <author>
      <name>V. V. Zharkova</name>
    </author>
    <author>
      <name>V. Schetinin</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0504053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.4351v1</id>
    <updated>2011-01-23T09:16:35Z</updated>
    <published>2011-01-23T09:16:35Z</published>
    <title>Building a Chaotic Proved Neural Network</title>
    <summary>  Chaotic neural networks have received a great deal of attention these last
years. In this paper we establish a precise correspondence between the
so-called chaotic iterations and a particular class of artificial neural
networks: global recurrent multi-layer perceptrons. We show formally that it is
possible to make these iterations behave chaotically, as defined by Devaney,
and thus we obtain the first neural networks proven chaotic. Several neural
networks with different architectures are trained to exhibit a chaotical
behavior.
</summary>
    <author>
      <name>Jacques M. Bahi</name>
    </author>
    <author>
      <name>Christophe Guyeux</name>
    </author>
    <author>
      <name>Michel Salomon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, submitted to ICCANS 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1101.4351v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.4351v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.06602v2</id>
    <updated>2016-03-07T16:36:33Z</updated>
    <published>2015-06-22T13:52:01Z</published>
    <title>Context-dependent representation in recurrent neural networks</title>
    <summary>  In order to assess the short-term memory performance of non-linear random
neural networks, we introduce a measure to quantify the dependence of a neural
representation upon the past context. We study this measure both numerically
and theoretically using the mean-field theory for random neural networks,
showing the existence of an optimal level of synaptic weights heterogeneity. We
further investigate the influence of the network topology, in particular the
symmetry of reciprocal synaptic connections, on this measure of context
dependence, revealing the importance of considering the interplay between
non-linearities and connectivity structure.
</summary>
    <author>
      <name>Gilles Wainrib</name>
    </author>
    <link href="http://arxiv.org/abs/1506.06602v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.06602v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.01321v1</id>
    <updated>2016-02-03T14:46:35Z</updated>
    <published>2016-02-03T14:46:35Z</published>
    <title>A continuum among logarithmic, linear, and exponential functions, and
  its potential to improve generalization in neural networks</title>
    <summary>  We present the soft exponential activation function for artificial neural
networks that continuously interpolates between logarithmic, linear, and
exponential functions. This activation function is simple, differentiable, and
parameterized so that it can be trained as the rest of the network is trained.
We hypothesize that soft exponential has the potential to improve neural
network learning, as it can exactly calculate many natural operations that
typical neural networks can only approximate, including addition,
multiplication, inner product, distance, polynomials, and sinusoids.
</summary>
    <author>
      <name>Luke B. Godfrey</name>
    </author>
    <author>
      <name>Michael S. Gashler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 8 figures, conference, In Proceedings of Knowledge Discovery
  and Information Retrieval (KDIR) 2015, Lisbon, Portugal, December 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.01321v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.01321v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.00857v1</id>
    <updated>2017-05-02T08:45:54Z</updated>
    <published>2017-05-02T08:45:54Z</published>
    <title>Decoding Small Surface Codes with Feedforward Neural Networks</title>
    <summary>  Surface codes reach high error thresholds when decoded with known algorithms,
but the decoding time will likely exceed the available time budget, especially
for near-term implementations. To decrease the decoding time, we reduce the
decoding problem to a classification problem that a feedforward neural network
can solve. We investigate quantum error correction and fault tolerance at small
code distances using neural network-based decoders, demonstrating that the
neural network can generalize to inputs that were not provided during training
and that they can reach similar or better decoding performance compared to
previous algorithms. We conclude by discussing the time required by a
feedforward neural network decoder in hardware.
</summary>
    <author>
      <name>Savvas Varsamopoulos</name>
    </author>
    <author>
      <name>Ben Criger</name>
    </author>
    <author>
      <name>Koen Bertels</name>
    </author>
    <link href="http://arxiv.org/abs/1705.00857v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.00857v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.07790v1</id>
    <updated>2017-12-21T04:50:03Z</updated>
    <published>2017-12-21T04:50:03Z</published>
    <title>The use of adversaries for optimal neural network training</title>
    <summary>  B-decay data from the Belle experiment at the KEKB collider have a
substantial background from $e^{+}e^{-}\to q \bar{q}$ events. To suppress this
we employ deep neural network algorithms. These provide improved signal from
background discrimination. However, the deep neural network develops a
substantial correlation with the $\Delta E$ kinematic variable used to
distinguish signal from background in the final fit due to its relationship
with input variables. The effect of this correlation is counter-acted by
deploying an adversarial neural network. Overall the adversarial deep neural
network performs better than an unoptimised commercial package, NeuroBayes.
</summary>
    <author>
      <name>Anton Hawthorne-Gonzalvez</name>
    </author>
    <author>
      <name>Martin Sevior</name>
    </author>
    <link href="http://arxiv.org/abs/1712.07790v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.07790v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.09685v1</id>
    <updated>2017-12-27T21:05:42Z</updated>
    <published>2017-12-27T21:05:42Z</published>
    <title>Neural network augmented inverse problems for PDEs</title>
    <summary>  In this paper we show how to augment classical methods for inverse problems
with artificial neural networks. The neural network acts as a parametric
container for the coefficient to be estimated from noisy data. Neural networks
are global, smooth function approximators and as such they do not require
regularization of the error functional to recover smooth solutions and
coefficients. We give detailed examples using the Poisson equation in 1, 2, and
3 space dimensions and show that the neural network augmentation is robust with
respect to noisy data, mesh, and geometry.
</summary>
    <author>
      <name>Jens Berg</name>
    </author>
    <author>
      <name>Kaj Nystr√∂m</name>
    </author>
    <link href="http://arxiv.org/abs/1712.09685v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.09685v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.00905v1</id>
    <updated>2018-01-03T05:52:52Z</updated>
    <published>2018-01-03T05:52:52Z</published>
    <title>Neural Networks in Adversarial Setting and Ill-Conditioned Weight Space</title>
    <summary>  Recently, Neural networks have seen a huge surge in its adoption due to their
ability to provide high accuracy on various tasks. On the other hand, the
existence of adversarial examples have raised suspicions regarding the
generalization capabilities of neural networks. In this work, we focus on the
weight matrix learnt by the neural networks and hypothesize that ill
conditioned weight matrix is one of the contributing factors in neural
network's susceptibility towards adversarial examples. For ensuring that the
learnt weight matrix's condition number remains sufficiently low, we suggest
using orthogonal regularizer. We show that this indeed helps in increasing the
adversarial accuracy on MNIST and F-MNIST datasets.
</summary>
    <author>
      <name>Mayank Singh</name>
    </author>
    <author>
      <name>Abhishek Sinha</name>
    </author>
    <author>
      <name>Balaji Krishnamurthy</name>
    </author>
    <link href="http://arxiv.org/abs/1801.00905v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.00905v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.01117v1</id>
    <updated>2018-01-05T02:10:32Z</updated>
    <published>2018-01-05T02:10:32Z</published>
    <title>Learning from Pseudo-Randomness With an Artificial Neural Network - Does
  God Play Pseudo-Dice?</title>
    <summary>  Inspired by the fact that the neural network, as the mainstream for machine
learning, has brought successes in many application areas, here we propose to
use this approach for decoding hidden correlation among pseudo-random data and
predicting events accordingly. With a simple neural network structure and a
typical training procedure, we demonstrate the learning and prediction power of
the neural network in extremely random environment. Finally, we postulate that
the high sensitivity and efficiency of the neural network may allow to
critically test if there could be any fundamental difference between quantum
randomness and pseudo randomness, which is equivalent to the question: Does God
play dice?
</summary>
    <author>
      <name>Fenglei Fan</name>
    </author>
    <author>
      <name>Ge Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures, 22 references</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.01117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.01117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.04435v1</id>
    <updated>2018-01-13T13:17:31Z</updated>
    <published>2018-01-13T13:17:31Z</published>
    <title>Neural network forecast of the sunspot diagram</title>
    <summary>  We attempt to forecast the Sun's sunspot butterfly diagram using neural
networks as a prediction method. We use this approach to forecast in both
latitude (space) and time, using a full spatial-temporal series of the sunspot
diagram from 1874 to 2015 that trains the neural network. The analysis of the
results show that it is indeed possible to reconstruct the overall shape and
amplitude of the spatial-temporal pattern of sunspots using these feed-forward
neural networks. However, we conclude that more data and/or improved neural
network techniques are probably necessary for this approach to have real
predictive power.
</summary>
    <author>
      <name>Eurico Covas</name>
    </author>
    <author>
      <name>Nuno Peixinho</name>
    </author>
    <author>
      <name>Joao Fernandes</name>
    </author>
    <link href="http://arxiv.org/abs/1801.04435v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.04435v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.09608v1</id>
    <updated>2016-10-30T06:34:19Z</updated>
    <published>2016-10-30T06:34:19Z</published>
    <title>A Theoretical Study of The Relationship Between Whole An ELM Network and
  Its Subnetworks</title>
    <summary>  A biological neural network is constituted by numerous subnetworks and
modules with different functionalities. For an artificial neural network, the
relationship between a network and its subnetworks is also important and useful
for both theoretical and algorithmic research, i.e. it can be exploited to
develop incremental network training algorithm or parallel network training
algorithm. In this paper we explore the relationship between an ELM neural
network and its subnetworks. To the best of our knowledge, we are the first to
prove a theorem that shows an ELM neural network can be scattered into
subnetworks and its optimal solution can be constructed recursively by the
optimal solutions of these subnetworks. Based on the theorem we also present
two algorithms to train a large ELM neural network efficiently: one is a
parallel network training algorithm and the other is an incremental network
training algorithm. The experimental results demonstrate the usefulness of the
theorem and the validity of the developed algorithms.
</summary>
    <author>
      <name>Enmei Tu</name>
    </author>
    <author>
      <name>Guanghao Zhang</name>
    </author>
    <author>
      <name>Lily Rachmawati</name>
    </author>
    <author>
      <name>Eshan Rajabally</name>
    </author>
    <author>
      <name>Guang-Bin Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.09608v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.09608v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.03918v1</id>
    <updated>2018-01-11T18:43:10Z</updated>
    <published>2018-01-11T18:43:10Z</published>
    <title>Black Holes as Brains: Neural Networks with Area Law Entropy</title>
    <summary>  Motivated by the potential similarities between the underlying mechanisms of
the enhanced memory storage capacity in black holes and in brain networks, we
construct an artificial quantum neural network based on gravity-like synaptic
connections and a symmetry structure that allows to describe the network in
terms of geometry of a d-dimensional space. We show that the network possesses
a critical state in which the gapless neurons emerge that appear to inhabit a
(d-1)-dimensional surface, with their number given by the surface area. In the
excitations of these neurons, the network can store and retrieve an
exponentially large number of patterns within an arbitrarily narrow energy gap.
The corresponding micro-state entropy of the brain network exhibits an area
law. The neural network can be described in terms of a quantum field, via
identifying the different neurons with the different momentum modes of the
field, while identifying the synaptic connections among the neurons with the
interactions among the corresponding momentum modes. Such a mapping allows to
attribute a well-defined sense of geometry to an intrinsically non-local
system, such as the neural network, and vice versa, it allows to represent the
quantum field model as a neural network.
</summary>
    <author>
      <name>Gia Dvali</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.03918v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.03918v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.06186v2</id>
    <updated>2017-03-02T09:36:24Z</updated>
    <published>2017-02-14T17:24:04Z</published>
    <title>Survey of reasoning using Neural networks</title>
    <summary>  Reason and inference require process as well as memory skills by humans.
Neural networks are able to process tasks like image recognition (better than
humans) but in memory aspects are still limited (by attention mechanism, size).
Recurrent Neural Network (RNN) and it's modified version LSTM are able to solve
small memory contexts, but as context becomes larger than a threshold, it is
difficult to use them. The Solution is to use large external memory. Still, it
poses many challenges like, how to train neural networks for discrete memory
representation, how to describe long term dependencies in sequential data etc.
Most prominent neural architectures for such tasks are Memory networks:
inference components combined with long term memory and Neural Turing Machines:
neural networks using external memory resources. Also, additional techniques
like attention mechanism, end to end gradient descent on discrete memory
representation are needed to support these solutions. Preliminary results of
above neural architectures on simple algorithms (sorting, copying) and Question
Answering (based on story, dialogs) application are comparable with the state
of the art. In this paper, I explain these architectures (in general), the
additional techniques used and the results of their application.
</summary>
    <author>
      <name>Amit Sahu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.06186v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.06186v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.0213v1</id>
    <updated>2011-12-01T15:37:09Z</updated>
    <published>2011-12-01T15:37:09Z</published>
    <title>Supervised Learning of Logical Operations in Layered Spiking Neural
  Networks with Spike Train Encoding</title>
    <summary>  Few algorithms for supervised training of spiking neural networks exist that
can deal with patterns of multiple spikes, and their computational properties
are largely unexplored. We demonstrate in a set of simulations that the ReSuMe
learning algorithm can be successfully applied to layered neural networks.
Input and output patterns are encoded as spike trains of multiple precisely
timed spikes, and the network learns to transform the input trains into target
output trains. This is done by combining the ReSuMe learning algorithm with
multiplicative scaling of the connections of downstream neurons.
  We show in particular that layered networks with one hidden layer can learn
the basic logical operations, including Exclusive-Or, while networks without
hidden layer cannot, mirroring an analogous result for layered networks of rate
neurons.
  While supervised learning in spiking neural networks is not yet fit for
technical purposes, exploring computational properties of spiking neural
networks advances our understanding of how computations can be done with spike
trains.
</summary>
    <author>
      <name>Andr√© Gr√ºning</name>
    </author>
    <author>
      <name>Ioana Sporea</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11063-012-9225-1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11063-012-9225-1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Processing Letters October 2012, Volume 36, Issue 2, pp
  117-134</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1112.0213v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.0213v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.06539v1</id>
    <updated>2016-11-20T16:05:07Z</updated>
    <published>2016-11-20T16:05:07Z</published>
    <title>Efficient Stochastic Inference of Bitwise Deep Neural Networks</title>
    <summary>  Recently published methods enable training of bitwise neural networks which
allow reduced representation of down to a single bit per weight. We present a
method that exploits ensemble decisions based on multiple stochastically
sampled network models to increase performance figures of bitwise neural
networks in terms of classification accuracy at inference. Our experiments with
the CIFAR-10 and GTSRB datasets show that the performance of such network
ensembles surpasses the performance of the high-precision base model. With this
technique we achieve 5.81% best classification error on CIFAR-10 test set using
bitwise networks. Concerning inference on embedded systems we evaluate these
bitwise networks using a hardware efficient stochastic rounding procedure. Our
work contributes to efficient embedded bitwise neural networks.
</summary>
    <author>
      <name>Sebastian Vogel</name>
    </author>
    <author>
      <name>Christoph Schorn</name>
    </author>
    <author>
      <name>Andre Guntoro</name>
    </author>
    <author>
      <name>Gerd Ascheid</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, Workshop on Efficient Methods for Deep Neural
  Networks at Neural Information Processing Systems Conference 2016, NIPS 2016,
  EMDNN 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.06539v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.06539v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.1; C.1.3; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.00891v1</id>
    <updated>2017-06-03T03:08:34Z</updated>
    <published>2017-06-03T03:08:34Z</published>
    <title>Spectrum-based deep neural networks for fraud detection</title>
    <summary>  In this paper, we focus on fraud detection on a signed graph with only a
small set of labeled training data. We propose a novel framework that combines
deep neural networks and spectral graph analysis. In particular, we use the
node projection (called as spectral coordinate) in the low dimensional spectral
space of the graph's adjacency matrix as input of deep neural networks.
Spectral coordinates in the spectral space capture the most useful topology
information of the network. Due to the small dimension of spectral coordinates
(compared with the dimension of the adjacency matrix derived from a graph),
training deep neural networks becomes feasible. We develop and evaluate two
neural networks, deep autoencoder and convolutional neural network, in our
fraud detection framework. Experimental results on a real signed graph show
that our spectrum based deep neural networks are effective in fraud detection.
</summary>
    <author>
      <name>Shuhan Yuan</name>
    </author>
    <author>
      <name>Xintao Wu</name>
    </author>
    <author>
      <name>Jun Li</name>
    </author>
    <author>
      <name>Aidong Lu</name>
    </author>
    <link href="http://arxiv.org/abs/1706.00891v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.00891v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.01911v1</id>
    <updated>2017-08-06T17:10:38Z</updated>
    <published>2017-08-06T17:10:38Z</published>
    <title>Training of Deep Neural Networks based on Distance Measures using
  RMSProp</title>
    <summary>  The vanishing gradient problem was a major obstacle for the success of deep
learning. In recent years it was gradually alleviated through multiple
different techniques. However the problem was not really overcome in a
fundamental way, since it is inherent to neural networks with activation
functions based on dot products. In a series of papers, we are going to analyze
alternative neural network structures which are not based on dot products. In
this first paper, we revisit neural networks built up of layers based on
distance measures and Gaussian activation functions. These kinds of networks
were only sparsely used in the past since they are hard to train when using
plain stochastic gradient descent methods. We show that by using Root Mean
Square Propagation (RMSProp) it is possible to efficiently learn multi-layer
neural networks. Furthermore we show that when appropriately initialized these
kinds of neural networks suffer much less from the vanishing and exploding
gradient problem than traditional neural networks even for deep networks.
</summary>
    <author>
      <name>Thomas Kurbiel</name>
    </author>
    <author>
      <name>Shahrzad Khaleghian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 14 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.01911v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.01911v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.05792v1</id>
    <updated>2018-02-15T23:24:39Z</updated>
    <published>2018-02-15T23:24:39Z</published>
    <title>Masked Conditional Neural Networks for Automatic Sound Events
  Recognition</title>
    <summary>  Deep neural network architectures designed for application domains other than
sound, especially image recognition, may not optimally harness the
time-frequency representation when adapted to the sound recognition problem. In
this work, we explore the ConditionaL Neural Network (CLNN) and the Masked
ConditionaL Neural Network (MCLNN) for multi-dimensional temporal signal
recognition. The CLNN considers the inter-frame relationship, and the MCLNN
enforces a systematic sparseness over the network's links to enable learning in
frequency bands rather than bins allowing the network to be frequency shift
invariant mimicking a filterbank. The mask also allows considering several
combinations of features concurrently, which is usually handcrafted through
exhaustive manual search. We applied the MCLNN to the environmental sound
recognition problem using the ESC-10 and ESC-50 datasets. MCLNN achieved
competitive performance, using 12% of the parameters and without augmentation,
compared to state-of-the-art Convolutional Neural Networks.
</summary>
    <author>
      <name>Fady Medhat</name>
    </author>
    <author>
      <name>David Chesmore</name>
    </author>
    <author>
      <name>John Robinson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DSAA.2017.43</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DSAA.2017.43" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Restricted Boltzmann Machine, RBM, Conditional RBM, CRBM, Deep Belief
  Net, DBN, Conditional Neural Network, CLNN, Masked Conditional Neural
  Network, MCLNN, Environmental Sound Recognition, ESR</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Data Science and Advanced
  Analytics (DSAA) Year: 2017, Pages: 389 - 394</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1802.05792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.05792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.06432v1</id>
    <updated>2018-02-18T19:55:09Z</updated>
    <published>2018-02-18T19:55:09Z</published>
    <title>Music Genre Classification using Masked Conditional Neural Networks</title>
    <summary>  The ConditionaL Neural Networks (CLNN) and the Masked ConditionaL Neural
Networks (MCLNN) exploit the nature of multi-dimensional temporal signals. The
CLNN captures the conditional temporal influence between the frames in a window
and the mask in the MCLNN enforces a systematic sparseness that follows a
filterbank-like pattern over the network links. The mask induces the network to
learn about time-frequency representations in bands, allowing the network to
sustain frequency shifts. Additionally, the mask in the MCLNN automates the
exploration of a range of feature combinations, usually done through an
exhaustive manual search. We have evaluated the MCLNN performance using the
Ballroom and Homburg datasets of music genres. MCLNN has achieved accuracies
that are competitive to state-of-the-art handcrafted attempts in addition to
models based on Convolutional Neural Networks.
</summary>
    <author>
      <name>Fady Medhat</name>
    </author>
    <author>
      <name>David Chesmore</name>
    </author>
    <author>
      <name>John Robinson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-70096-0_49</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-70096-0_49" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Conditional Neural Networks (CLNN), Masked Conditional Neural
  Networks (MCLNN), Conditional Restricted Boltzmann Machine (CRBM), Deep
  Belief Nets (DBN), Music Information Retrieval (MIR)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Neural Information Processing (ICONIP)
  Year: 2017, Pages: 470-481</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1802.06432v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06432v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06216v3</id>
    <updated>2016-08-06T23:45:51Z</updated>
    <published>2016-06-20T17:29:01Z</published>
    <title>Neural networks with differentiable structure</title>
    <summary>  While gradient descent has proven highly successful in learning connection
weights for neural networks, the actual structure of these networks is usually
determined by hand, or by other optimization algorithms. Here we describe a
simple method to make network structure differentiable, and therefore
accessible to gradient descent. We test this method on recurrent neural
networks applied to simple sequence prediction problems. Starting with initial
networks containing only one node, the method automatically builds networks
that successfully solve the tasks. The number of nodes in the final network
correlates with task difficulty. The method can dynamically increase network
size in response to an abrupt complexification in the task; however, reduction
in network size in response to task simplification is not evident for
reasonable meta-parameters. The method does not penalize network performance
for these test tasks: variable-size networks actually reach better performance
than fixed-size networks of higher, lower or identical size. We conclude by
discussing how this method could be applied to more complex networks, such as
feedforward layered networks, or multiple-area networks of arbitrary shape.
</summary>
    <author>
      <name>Thomas Miconi</name>
    </author>
    <link href="http://arxiv.org/abs/1606.06216v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06216v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0406268v2</id>
    <updated>2004-10-05T15:16:48Z</updated>
    <published>2004-06-11T14:26:53Z</published>
    <title>Networking genetic regulation and neural computation: Directed network
  topology and its effect on the dynamics</title>
    <summary>  Two different types of directed networks are investigated, transcriptional
regulation networks and neural networks. The directed network structure are
studied and also shown to reflect the different processes taking place on the
networks. The distribution of influence, identified as the the number of
downstream vertices, are used as a tool for investigating random vertex
removal. In the transcriptional regulation networks we observe that only a
small number of vertices have a large influence. The small influences of most
vertices limit the effect of a random removal to in most cases only a small
fraction of vertices in the network. The neural network has a rather different
topology with respect to the influence, which are large for most vertices. To
further investigate the effect of vertex removal we simulate the biological
processes taking place on the networks. Opposed to the presumpted large effect
of random vertex removal in the neural network, the high density of edges in
conjunction with the dynamics used makes the change in the state of the system
to be highly localized around the removed vertex.
</summary>
    <author>
      <name>Andreas Gr√∂nlund</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.70.061908</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.70.061908" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0406268v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0406268v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.4524v1</id>
    <updated>2007-07-31T02:27:10Z</updated>
    <published>2007-07-31T02:27:10Z</published>
    <title>Image Authentication Based on Neural Networks</title>
    <summary>  Neural network has been attracting more and more researchers since the past
decades. The properties, such as parameter sensitivity, random similarity,
learning ability, etc., make it suitable for information protection, such as
data encryption, data authentication, intrusion detection, etc. In this paper,
by investigating neural networks' properties, the low-cost authentication
method based on neural networks is proposed and used to authenticate images or
videos. The authentication method can detect whether the images or videos are
modified maliciously. Firstly, this chapter introduces neural networks'
properties, such as parameter sensitivity, random similarity, diffusion
property, confusion property, one-way property, etc. Secondly, the chapter
gives an introduction to neural network based protection methods. Thirdly, an
image or video authentication scheme based on neural networks is presented, and
its performances, including security, robustness and efficiency, are analyzed.
Finally, conclusions are drawn, and some open issues in this field are
presented.
</summary>
    <author>
      <name>Shiguo Lian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages,10 figures, submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/0707.4524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.4524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; E.3.x; F.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.10642v1</id>
    <updated>2017-03-30T19:04:55Z</updated>
    <published>2017-03-30T19:04:55Z</published>
    <title>Deep Neural Network Optimized to Resistive Memory with Nonlinear
  Current-Voltage Characteristics</title>
    <summary>  Artificial Neural Network computation relies on intensive vector-matrix
multiplications. Recently, the emerging nonvolatile memory (NVM) crossbar array
showed a feasibility of implementing such operations with high energy
efficiency, thus there are many works on efficiently utilizing emerging NVM
crossbar array as analog vector-matrix multiplier. However, its nonlinear I-V
characteristics restrain critical design parameters, such as the read voltage
and weight range, resulting in substantial accuracy loss. In this paper,
instead of optimizing hardware parameters to a given neural network, we propose
a methodology of reconstructing a neural network itself optimized to resistive
memory crossbar arrays. To verify the validity of the proposed method, we
simulated various neural network with MNIST and CIFAR-10 dataset using two
different specific Resistive Random Access Memory (RRAM) model. Simulation
results show that our proposed neural network produces significantly higher
inference accuracies than conventional neural network when the synapse devices
have nonlinear I-V characteristics.
</summary>
    <author>
      <name>Hyungjun Kim</name>
    </author>
    <author>
      <name>Taesu Kim</name>
    </author>
    <author>
      <name>Jinseok Kim</name>
    </author>
    <author>
      <name>Jae-Joon Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.10642v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.10642v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9412030v1</id>
    <updated>1994-12-06T09:26:29Z</updated>
    <published>1994-12-06T09:26:29Z</published>
    <title>Attractor Neural Networks</title>
    <summary>  In this lecture I will present some models of neural networks that have been
developed in the recent years. The aim is to construct neural networks which
work as associative memories. Different attractors of the network will be
identified as different internal representations of different objects. At the
end of the lecture I will present a comparison among the theoretical results
and some of the experiments done on real mammal brains.
</summary>
    <author>
      <name>Giorgio Parisi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dipart. Fisica, Universita Roma I</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/cond-mat/9412030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9412030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9808043v1</id>
    <updated>1998-08-04T19:04:49Z</updated>
    <published>1998-08-04T19:04:49Z</published>
    <title>Dynamics of networks and applications</title>
    <summary>  A survey is made of several aspects of the dynamics of networks, with special
emphasis on unsupervised learning processes, non-Gaussian data analysis and
pattern recognition in networks with complex nodes.
</summary>
    <author>
      <name>R. Vilela Mendes</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Grupo de Fisica-Matematica, Lisboa, Portugal</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Talk at the Heraeus Seminar "Scientific Applications of Neural Nets",
  to appear in the proceedings (Springer LNP)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">in "Scientific Applic. of Neural Nets", Springer Lect. Notes in
  Physics 522, 1999</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/9808043v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9808043v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.1612v1</id>
    <updated>2014-11-06T13:48:41Z</updated>
    <published>2014-11-06T13:48:41Z</published>
    <title>Effect of Activity and Inter-Cluster Correlations on
  Information-Theoretic Properties of Neural Networks</title>
    <summary>  On the basis of solutions of the master equation for networks with a small
number of neurons it is shown that the conditional entropy and integrated
information of neural networks depend on their average activity and
inter-cluster correlations.
</summary>
    <author>
      <name>Andrey Demichev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.1612v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.1612v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.00404v1</id>
    <updated>2016-05-02T09:33:46Z</updated>
    <published>2016-05-02T09:33:46Z</published>
    <title>Simple2Complex: Global Optimization by Gradient Descent</title>
    <summary>  A method named simple2complex for modeling and training deep neural networks
is proposed. Simple2complex train deep neural networks by smoothly adding more
and more layers to the shallow networks, as the learning procedure going on,
the network is just like growing. Compared with learning by end2end,
simple2complex is with less possibility trapping into local minimal, namely,
owning ability for global optimization. Cifar10 is used for verifying the
superiority of simple2complex.
</summary>
    <author>
      <name>Ming Li</name>
    </author>
    <link href="http://arxiv.org/abs/1605.00404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.00404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.04515v1</id>
    <updated>2017-10-12T13:40:43Z</updated>
    <published>2017-10-12T13:40:43Z</published>
    <title>Convolutional Attention-based Seq2Seq Neural Network for End-to-End ASR</title>
    <summary>  This thesis introduces the sequence to sequence model with Luong's attention
mechanism for end-to-end ASR. It also describes various neural network
algorithms including Batch normalization, Dropout and Residual network which
constitute the convolutional attention-based seq2seq neural network. Finally
the proposed model proved its effectiveness for speech recognition achieving
15.8% phoneme error rate on TIMIT dataset.
</summary>
    <author>
      <name>Dan Lim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Masters thesis, Korea Univ</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.04515v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.04515v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.05029v3</id>
    <updated>2018-02-18T18:42:40Z</updated>
    <published>2017-08-16T18:28:22Z</published>
    <title>Deep Neural Network Capacity</title>
    <summary>  In recent years, deep neural network exhibits its powerful superiority on
information discrimination in many computer vision applications. However, the
capacity of deep neural network architecture is still a mystery to the
researchers. Intuitively, larger capacity of neural network can always deposit
more information to improve the discrimination ability of the model. But, the
learnable parameter scale is not feasible to estimate the capacity of deep
neural network. Due to the overfitting, directly increasing hidden nodes number
and hidden layer number are already demonstrated not necessary to effectively
increase the network discrimination ability.
  In this paper, we propose a novel measurement, named "total valid bits", to
evaluate the capacity of deep neural networks for exploring how to
quantitatively understand the deep learning and the insights behind its super
performance. Specifically, our scheme to retrieve the total valid bits
incorporates the skilled techniques in both training phase and inference phase.
In the network training, we design decimal weight regularization and 8-bit
forward quantization to obtain the integer-oriented network representations.
Moreover, we develop adaptive-bitwidth and non-uniform quantization strategy in
the inference phase to find the neural network capacity, total valid bits. By
allowing zero bitwidth, our adaptive-bitwidth quantization can execute the
model reduction and valid bits finding simultaneously. In our extensive
experiments, we first demonstrate that our total valid bits is a good indicator
of neural network capacity. We also analyze the impact on network capacity from
the network architecture and advanced training skills, such as dropout and
batch normalization.
</summary>
    <author>
      <name>Aosen Wang</name>
    </author>
    <author>
      <name>Hua Zhou</name>
    </author>
    <author>
      <name>Wenyao Xu</name>
    </author>
    <author>
      <name>Xin Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">There is an error in Average Valid Bits computation in figure 1 in
  page 2</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.05029v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.05029v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.00165v3</id>
    <updated>2018-03-03T00:45:00Z</updated>
    <published>2017-11-01T02:13:25Z</published>
    <title>Deep Neural Networks as Gaussian Processes</title>
    <summary>  It has long been known that a single-layer fully-connected neural network
with an i.i.d. prior over its parameters is equivalent to a Gaussian process
(GP), in the limit of infinite network width. This correspondence enables exact
Bayesian inference for infinite width neural networks on regression tasks by
means of evaluating the corresponding GP. Recently, kernel functions which
mimic multi-layer random neural networks have been developed, but only outside
of a Bayesian framework. As such, previous work has not identified that these
kernels can be used as covariance functions for GPs and allow fully Bayesian
prediction with a deep neural network.
  In this work, we derive the exact equivalence between infinitely wide deep
networks and GPs. We further develop a computationally efficient pipeline to
compute the covariance function for these GPs. We then use the resulting GPs to
perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10.
We observe that trained neural network accuracy approaches that of the
corresponding GP with increasing layer width, and that the GP uncertainty is
strongly correlated with trained network prediction error. We further find that
test performance increases as finite-width trained networks are made wider and
more similar to a GP, and thus that GP predictions typically outperform those
of finite-width networks. Finally we connect the performance of these GPs to
the recent theory of signal propagation in random neural networks.
</summary>
    <author>
      <name>Jaehoon Lee</name>
    </author>
    <author>
      <name>Yasaman Bahri</name>
    </author>
    <author>
      <name>Roman Novak</name>
    </author>
    <author>
      <name>Samuel S. Schoenholz</name>
    </author>
    <author>
      <name>Jeffrey Pennington</name>
    </author>
    <author>
      <name>Jascha Sohl-Dickstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published version in ICLR 2018. 10 pages + appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.00165v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.00165v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02053v1</id>
    <updated>2016-09-07T16:30:01Z</updated>
    <published>2016-09-07T16:30:01Z</published>
    <title>Fast and Efficient Asynchronous Neural Computation with Adapting Spiking
  Neural Networks</title>
    <summary>  Biological neurons communicate with a sparing exchange of pulses - spikes. It
is an open question how real spiking neurons produce the kind of powerful
neural computation that is possible with deep artificial neural networks, using
only so very few spikes to communicate. Building on recent insights in
neuroscience, we present an Adapting Spiking Neural Network (ASNN) based on
adaptive spiking neurons. These spiking neurons efficiently encode information
in spike-trains using a form of Asynchronous Pulsed Sigma-Delta coding while
homeostatically optimizing their firing rate. In the proposed paradigm of
spiking neuron computation, neural adaptation is tightly coupled to synaptic
plasticity, to ensure that downstream neurons can correctly decode upstream
spiking neurons. We show that this type of network is inherently able to carry
out asynchronous and event-driven neural computation, while performing
identical to corresponding artificial neural networks (ANNs). In particular, we
show that these adaptive spiking neurons can be drop in replacements for ReLU
neurons in standard feedforward ANNs comprised of such units. We demonstrate
that this can also be successfully applied to a ReLU based deep convolutional
neural network for classifying the MNIST dataset. The ASNN thus outperforms
current Spiking Neural Networks (SNNs) implementations, while responding (up
to) an order of magnitude faster and using an order of magnitude fewer spikes.
Additionally, in a streaming setting where frames are continuously classified,
we show that the ASNN requires substantially fewer network updates as compared
to the corresponding ANN.
</summary>
    <author>
      <name>Davide Zambrano</name>
    </author>
    <author>
      <name>Sander M. Bohte</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.02053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.0573v1</id>
    <updated>2014-05-03T11:58:27Z</updated>
    <published>2014-05-03T11:58:27Z</published>
    <title>Spatial Neural Networks and their Functional Samples: Similarities and
  Differences</title>
    <summary>  Models of neural networks have proven their utility in the development of
learning algorithms in computer science and in the theoretical study of brain
dynamics in computational neuroscience. We propose in this paper a spatial
neural network model to analyze the important class of functional networks,
which are commonly employed in computational studies of clinical brain imaging
time series. We developed a simulation framework inspired by multichannel brain
surface recordings (more specifically, EEG -- electroencephalogram) in order to
link the mesoscopic network dynamics (represented by sampled functional
networks) and the microscopic network structure (represented by an
integrate-and-fire neural network located in a 3D space -- hence the term
spatial neural network). Functional networks are obtained by computing pairwise
correlations between time-series of mesoscopic electric potential dynamics,
which allows the construction of a graph where each node represents one
time-series. The spatial neural network model is central in this study in the
sense that it allowed us to characterize sampled functional networks in terms
of what features they are able to reproduce from the underlying spatial
network. Our modeling approach shows that, in specific conditions of sample
size and edge density, it is possible to precisely estimate several network
measurements of spatial networks by just observing functional samples.
</summary>
    <author>
      <name>Lucas Antiqueira</name>
    </author>
    <author>
      <name>Liang Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures (submitted)</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.0573v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.0573v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.10761v1</id>
    <updated>2017-11-29T10:28:02Z</updated>
    <published>2017-11-29T10:28:02Z</published>
    <title>Transfer Learning with Binary Neural Networks</title>
    <summary>  Previous work has shown that it is possible to train deep neural networks
with low precision weights and activations. In the extreme case it is even
possible to constrain the network to binary values. The costly floating point
multiplications are then reduced to fast logical operations. High end smart
phones such as Google's Pixel 2 and Apple's iPhone X are already equipped with
specialised hardware for image processing and it is very likely that other
future consumer hardware will also have dedicated accelerators for deep neural
networks. Binary neural networks are attractive in this case because the
logical operations are very fast and efficient when implemented in hardware. We
propose a transfer learning based architecture where we first train a binary
network on Imagenet and then retrain part of the network for different tasks
while keeping most of the network fixed. The fixed binary part could be
implemented in a hardware accelerator while the last layers of the network are
evaluated in software. We show that a single binary neural network trained on
the Imagenet dataset can indeed be used as a feature extractor for other
datasets.
</summary>
    <author>
      <name>Sam Leroux</name>
    </author>
    <author>
      <name>Steven Bohez</name>
    </author>
    <author>
      <name>Tim Verbelen</name>
    </author>
    <author>
      <name>Bert Vankeirsbilck</name>
    </author>
    <author>
      <name>Pieter Simoens</name>
    </author>
    <author>
      <name>Bart Dhoedt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Learning on the Phone and other Consumer Devices, NIPS2017
  Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.10761v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.10761v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04374v2</id>
    <updated>2016-10-05T17:45:06Z</updated>
    <published>2016-08-15T19:38:35Z</published>
    <title>A Geometric Framework for Convolutional Neural Networks</title>
    <summary>  In this paper, a geometric framework for neural networks is proposed. This
framework uses the inner product space structure underlying the parameter set
to perform gradient descent not in a component-based form, but in a
coordinate-free manner. Convolutional neural networks are described in this
framework in a compact form, with the gradients of standard --- and
higher-order --- loss functions calculated for each layer of the network. This
approach can be applied to other network structures and provides a basis on
which to create new networks.
</summary>
    <author>
      <name>Anthony L. Caterini</name>
    </author>
    <author>
      <name>Dong Eui Chang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Added proofs and algorithms that were missing from previous version</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.04374v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04374v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.1; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.09876v1</id>
    <updated>2018-02-27T13:35:44Z</updated>
    <published>2018-02-27T13:35:44Z</published>
    <title>Parameter diagnostics of phases and phase transition learning by neural
  networks</title>
    <summary>  We present an analysis of neural network-based machine learning schemes for
phases and phase transitions in theoretical condensed matter research, focusing
on neural networks with a single hidden layer. Such shallow neural networks
were previously found to be efficient in classifying phases and locating phase
transitions of various basic model systems. In order to rationalize the
emergence of the classification process and for identifying any underlying
physical quantities, it is feasible to examine the weight matrices and the
convolutional filter kernels that result from the learning process of such
shallow networks. Furthermore, we demonstrate how the learning-by-confusing
scheme can be used, in combination with a simple threshold-value classification
method, to diagnose the learning parameters of neural networks. In particular,
we study the classification process of both fully-connected and convolutional
neural networks for the two-dimensional Ising model with extended domain wall
configurations included in the low-temperature regime. Moreover, we consider
the two-dimensional XY model and contrast the performance of the
learning-by-confusing scheme and convolutional neural networks trained on bare
spin configurations to the case of preprocessed samples with respect to vortex
configurations. We discuss these findings in relation to similar recent
investigations and possible further applications.
</summary>
    <author>
      <name>Philippe Suchsland</name>
    </author>
    <author>
      <name>Stefan Wessel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 23 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.09876v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.09876v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.08254v3</id>
    <updated>2017-05-23T11:45:31Z</updated>
    <published>2016-05-26T12:19:09Z</published>
    <title>Robust Large Margin Deep Neural Networks</title>
    <summary>  The generalization error of deep neural networks via their classification
margin is studied in this work. Our approach is based on the Jacobian matrix of
a deep neural network and can be applied to networks with arbitrary
non-linearities and pooling layers, and to networks with different
architectures such as feed forward networks and residual networks. Our analysis
leads to the conclusion that a bounded spectral norm of the network's Jacobian
matrix in the neighbourhood of the training samples is crucial for a deep
neural network of arbitrary depth and width to generalize well. This is a
significant improvement over the current bounds in the literature, which imply
that the generalization error grows with either the width or the depth of the
network. Moreover, it shows that the recently proposed batch normalization and
weight normalization re-parametrizations enjoy good generalization properties,
and leads to a novel network regularizer based on the network's Jacobian
matrix. The analysis is supported with experimental results on the MNIST,
CIFAR-10, LaRED and ImageNet datasets.
</summary>
    <author>
      <name>Jure Sokolic</name>
    </author>
    <author>
      <name>Raja Giryes</name>
    </author>
    <author>
      <name>Guillermo Sapiro</name>
    </author>
    <author>
      <name>Miguel R. D. Rodrigues</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TSP.2017.2708039</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TSP.2017.2708039" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted to IEEE Transactions on Signal Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.08254v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08254v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9812006v1</id>
    <updated>1998-12-05T00:00:57Z</updated>
    <published>1998-12-05T00:00:57Z</published>
    <title>A High Quality Text-To-Speech System Composed of Multiple Neural
  Networks</title>
    <summary>  While neural networks have been employed to handle several different
text-to-speech tasks, ours is the first system to use neural networks
throughout, for both linguistic and acoustic processing. We divide the
text-to-speech task into three subtasks, a linguistic module mapping from text
to a linguistic representation, an acoustic module mapping from the linguistic
representation to speech, and a video module mapping from the linguistic
representation to animated images. The linguistic module employs a
letter-to-sound neural network and a postlexical neural network. The acoustic
module employs a duration neural network and a phonetic neural network. The
visual neural network is employed in parallel to the acoustic module to drive a
talking head. The use of neural networks that can be retrained on the
characteristics of different voices and languages affords our system a degree
of adaptability and naturalness heretofore unavailable.
</summary>
    <author>
      <name>Orhan Karaali</name>
    </author>
    <author>
      <name>Gerald Corrigan</name>
    </author>
    <author>
      <name>Noel Massey</name>
    </author>
    <author>
      <name>Corey Miller</name>
    </author>
    <author>
      <name>Otto Schnurr</name>
    </author>
    <author>
      <name>Andrew Mackie</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.1998.675495</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.1998.675495" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Source link (9812006.tar.gz) contains: 1 PostScript file (4 pages)
  and 3 WAV audio files. If your system does not support Windows WAV files, try
  a tool like "sox" to translate the audio into a format of your choice</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the IEEE International Conference on Acoustics,
  Speech and Signal Processing (1998) 2:1237-1240. Seattle, Washington</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9812006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9812006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.02676v1</id>
    <updated>2017-02-09T02:02:27Z</updated>
    <published>2017-02-09T02:02:27Z</published>
    <title>Energy Saving Additive Neural Network</title>
    <summary>  In recent years, machine learning techniques based on neural networks for
mobile computing become increasingly popular. Classical multi-layer neural
networks require matrix multiplications at each stage. Multiplication operation
is not an energy efficient operation and consequently it drains the battery of
the mobile device. In this paper, we propose a new energy efficient neural
network with the universal approximation property over space of Lebesgue
integrable functions. This network, called, additive neural network, is very
suitable for mobile computing. The neural structure is based on a novel vector
product definition, called ef-operator, that permits a multiplier-free
implementation. In ef-operation, the "product" of two real numbers is defined
as the sum of their absolute values, with the sign determined by the sign of
the product of the numbers. This "product" is used to construct a vector
product in $R^N$. The vector product induces the $l_1$ norm. The proposed
additive neural network successfully solves the XOR problem. The experiments on
MNIST dataset show that the classification performances of the proposed
additive neural networks are very similar to the corresponding multi-layer
perceptron and convolutional neural networks (LeNet).
</summary>
    <author>
      <name>Arman Afrasiyabi</name>
    </author>
    <author>
      <name>Ozan Yildiz</name>
    </author>
    <author>
      <name>Baris Nasir</name>
    </author>
    <author>
      <name>Fatos T. Yarman Vural</name>
    </author>
    <author>
      <name>A. Enis Cetin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages (double column), 2 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.02676v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.02676v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0603015v1</id>
    <updated>2006-03-02T23:59:19Z</updated>
    <published>2006-03-02T23:59:19Z</published>
    <title>The Basic Kak Neural Network with Complex Inputs</title>
    <summary>  The Kak family of neural networks is able to learn patterns quickly, and this
speed of learning can be a decisive advantage over other competing models in
many applications. Amongst the implementations of these networks are those
using reconfigurable networks, FPGAs and optical networks. In some
applications, it is useful to use complex data, and it is with that in mind
that this introduction to the basic Kak network with complex inputs is being
presented. The training algorithm is prescriptive and the network weights are
assigned simply upon examining the inputs. The input is mapped using quaternary
encoding for purpose of efficienty. This network family is part of a larger
hierarchy of learning schemes that include quantum models.
</summary>
    <author>
      <name>Pritam Rajagopal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 9 figures, 7 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0603015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0603015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.05440v1</id>
    <updated>2017-12-14T20:31:29Z</updated>
    <published>2017-12-14T20:31:29Z</published>
    <title>Nonparametric Neural Networks</title>
    <summary>  Automatically determining the optimal size of a neural network for a given
task without prior information currently requires an expensive global search
and training many networks from scratch. In this paper, we address the problem
of automatically finding a good network size during a single training cycle. We
introduce *nonparametric neural networks*, a non-probabilistic framework for
conducting optimization over all possible network sizes and prove its soundness
when network growth is limited via an L_p penalty. We train networks under this
framework by continuously adding new units while eliminating redundant units
via an L_2 penalty. We employ a novel optimization algorithm, which we term
*adaptive radial-angular gradient descent* or *AdaRad*, and obtain promising
results.
</summary>
    <author>
      <name>George Philipp</name>
    </author>
    <author>
      <name>Jaime G. Carbonell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.05440v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.05440v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.4071v1</id>
    <updated>2008-04-25T09:30:28Z</updated>
    <published>2008-04-25T09:30:28Z</published>
    <title>Logic Mining Using Neural Networks</title>
    <summary>  Knowledge could be gained from experts, specialists in the area of interest,
or it can be gained by induction from sets of data. Automatic induction of
knowledge from data sets, usually stored in large databases, is called data
mining. Data mining methods are important in the management of complex systems.
There are many technologies available to data mining practitioners, including
Artificial Neural Networks, Regression, and Decision Trees. Neural networks
have been successfully applied in wide range of supervised and unsupervised
learning applications. Neural network methods are not commonly used for data
mining tasks, because they often produce incomprehensible models, and require
long training times. One way in which the collective properties of a neural
network may be used to implement a computational task is by way of the concept
of energy minimization. The Hopfield network is well-known example of such an
approach. The Hopfield network is useful as content addressable memory or an
analog computer for solving combinatorial-type optimization problems. Wan
Abdullah [1] proposed a method of doing logic programming on a Hopfield neural
network. Optimization of logical inconsistency is carried out by the network
after the connection strengths are defined from the logic program; the network
relaxes to neural states corresponding to a valid interpretation. In this
article, we describe how Hopfield network is able to induce logical rules from
large database by using reverse analysis method: given the values of the
connections of a network, we can hope to know what logical rules are entrenched
in the database.
</summary>
    <author>
      <name>Saratha Sathasivam</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">USM</arxiv:affiliation>
    </author>
    <author>
      <name>Wan Ahmad Tajuddin Wan Abdullah</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Univ Malaya</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on Intelligent Systems
  2005 (ICIS 2005), Kuala Lumpur, 1-3 December 2005</arxiv:comment>
    <link href="http://arxiv.org/abs/0804.4071v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.4071v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.0798v1</id>
    <updated>2009-06-03T23:10:25Z</updated>
    <published>2009-06-03T23:10:25Z</published>
    <title>Single Neuron Memories and the Network's Proximity Matrix</title>
    <summary>  This paper extends the treatment of single-neuron memories obtained by the
B-matrix approach. The spreading of the activity within the network is
determined by the network's proximity matrix which represents the separations
amongst the neurons through the neural pathways.
</summary>
    <author>
      <name>Subhash Kak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0906.0798v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.0798v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06154v1</id>
    <updated>2016-04-21T01:47:33Z</updated>
    <published>2016-04-21T01:47:33Z</published>
    <title>Deep Adaptive Network: An Efficient Deep Neural Network with Sparse
  Binary Connections</title>
    <summary>  Deep neural networks are state-of-the-art models for understanding the
content of images, video and raw input data. However, implementing a deep
neural network in embedded systems is a challenging task, because a typical
deep neural network, such as a Deep Belief Network using 128x128 images as
input, could exhaust Giga bytes of memory and result in bandwidth and computing
bottleneck. To address this challenge, this paper presents a hardware-oriented
deep learning algorithm, named as the Deep Adaptive Network, which attempts to
exploit the sparsity in the neural connections. The proposed method adaptively
reduces the weights associated with negligible features to zero, leading to
sparse feedforward network architecture. Furthermore, since the small
proportion of important weights are significantly larger than zero, they can be
robustly thresholded and represented using single-bit integers (-1 and +1),
leading to implementations of deep neural networks with sparse and binary
connections. Our experiments showed that, for the application of recognizing
MNIST handwritten digits, the features extracted by a two-layer Deep Adaptive
Network with about 25% reserved important connections achieved 97.2%
classification accuracy, which was almost the same with the standard Deep
Belief Network (97.3%). Furthermore, for efficient hardware implementations,
the sparse-and-binary-weighted deep neural network could save about 99.3%
memory and 99.9% computation units without significant loss of classification
accuracy for pattern recognition applications.
</summary>
    <author>
      <name>Xichuan Zhou</name>
    </author>
    <author>
      <name>Shengli Li</name>
    </author>
    <author>
      <name>Kai Qin</name>
    </author>
    <author>
      <name>Kunping Li</name>
    </author>
    <author>
      <name>Fang Tang</name>
    </author>
    <author>
      <name>Shengdong Hu</name>
    </author>
    <author>
      <name>Shujun Liu</name>
    </author>
    <author>
      <name>Zhi Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, extended and submitted to IEEE Transactions of Systems,
  Man, and Cybernetics</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.06154v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06154v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.00630v2</id>
    <updated>2017-08-09T10:05:09Z</updated>
    <published>2017-08-02T07:58:45Z</published>
    <title>ProjectionNet: Learning Efficient On-Device Deep Networks Using Neural
  Projections</title>
    <summary>  Deep neural networks have become ubiquitous for applications related to
visual recognition and language understanding tasks. However, it is often
prohibitive to use typical neural networks on devices like mobile phones or
smart watches since the model sizes are huge and cannot fit in the limited
memory available on such devices. While these devices could make use of machine
learning models running on high-performance data centers with CPUs or GPUs,
this is not feasible for many applications because data can be privacy
sensitive and inference needs to be performed directly "on" device.
  We introduce a new architecture for training compact neural networks using a
joint optimization framework. At its core lies a novel objective that jointly
trains using two different types of networks--a full trainer neural network
(using existing architectures like Feed-forward NNs or LSTM RNNs) combined with
a simpler "projection" network that leverages random projections to transform
inputs or intermediate representations into bits. The simpler network encodes
lightweight and efficient-to-compute operations in bit space with a low memory
footprint. The two networks are trained jointly using backpropagation, where
the projection network learns from the full network similar to apprenticeship
learning. Once trained, the smaller network can be used directly for inference
at low memory and computation cost. We demonstrate the effectiveness of the new
approach at significantly shrinking the memory requirements of different types
of neural networks while preserving good accuracy on visual recognition and
text classification tasks. We also study the question "how many neural bits are
required to solve a given task?" using the new framework and show empirical
results contrasting model predictive capacity (in bits) versus accuracy on
several datasets.
</summary>
    <author>
      <name>Sujith Ravi</name>
    </author>
    <link href="http://arxiv.org/abs/1708.00630v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00630v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.09856v1</id>
    <updated>2018-01-30T05:47:01Z</updated>
    <published>2018-01-30T05:47:01Z</published>
    <title>ReNN: Rule-embedded Neural Networks</title>
    <summary>  The artificial neural network shows powerful ability of inference, but it is
still criticized for lack of interpretability and prerequisite needs of big
dataset. This paper proposes the Rule-embedded Neural Network (ReNN) to
overcome the shortages. ReNN first makes local-based inferences to detect local
patterns, and then uses rules based on domain knowledge about the local
patterns to generate rule-modulated map. After that, ReNN makes global-based
inferences that synthesizes the local patterns and the rule-modulated map. To
solve the optimization problem caused by rules, we use a two-stage optimization
strategy to train the ReNN model. By introducing rules into ReNN, we can
strengthen traditional neural networks with long-term dependencies which are
difficult to learn with limited empirical dataset, thus improving inference
accuracy. The complexity of neural networks can be reduced since long-term
dependencies are not modeled with neural connections, and thus the amount of
data needed to optimize the neural networks can be reduced. Besides, inferences
from ReNN can be analyzed with both local patterns and rules, and thus have
better interpretability. In this paper, ReNN has been validated with a
time-series detection problem.
</summary>
    <author>
      <name>Hu Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">conference paper, 6 pages, 4 figures, and 1 Table</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.09856v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.09856v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504058v1</id>
    <updated>2005-04-13T14:06:32Z</updated>
    <published>2005-04-13T14:06:32Z</published>
    <title>Polynomial Neural Networks Learnt to Classify EEG Signals</title>
    <summary>  A neural network based technique is presented, which is able to successfully
extract polynomial classification rules from labeled electroencephalogram (EEG)
signals. To represent the classification rules in an analytical form, we use
the polynomial neural networks trained by a modified Group Method of Data
Handling (GMDH). The classification rules were extracted from clinical EEG data
that were recorded from an Alzheimer patient and the sudden death risk
patients. The third data is EEG recordings that include the normal and artifact
segments. These EEG data were visually identified by medical experts. The
extracted polynomial rules verified on the testing EEG data allow to correctly
classify 72% of the risk group patients and 96.5% of the segments. These rules
performs slightly better than standard feedforward neural networks.
</summary>
    <author>
      <name>Vitaly Schetinin</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0504058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0607090v1</id>
    <updated>2006-07-18T21:01:43Z</updated>
    <published>2006-07-18T21:01:43Z</published>
    <title>Neural Networks with Complex and Quaternion Inputs</title>
    <summary>  This article investigates Kak neural networks, which can be instantaneously
trained, for complex and quaternion inputs. The performance of the basic
algorithm has been analyzed and shown how it provides a plausible model of
human perception and understanding of images. The motivation for studying
quaternion inputs is their use in representing spatial rotations that find
applications in computer graphics, robotics, global navigation, computer vision
and the spatial orientation of instruments. The problem of efficient mapping of
data in quaternion neural networks is examined. Some problems that need to be
addressed before quaternion neural networks find applications are identified.
</summary>
    <author>
      <name>Adityan Rishiyur</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0607090v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0607090v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.6921v1</id>
    <updated>2013-07-26T04:42:49Z</updated>
    <published>2013-07-26T04:42:49Z</published>
    <title>Memcapacitive neural networks</title>
    <summary>  We show that memcapacitive (memory capacitive) systems can be used as
synapses in artificial neural networks. As an example of our approach, we
discuss the architecture of an integrate-and-fire neural network based on
memcapacitive synapses. Moreover, we demonstrate that the
spike-timing-dependent plasticity can be simply realized with some of these
devices. Memcapacitive synapses are a low-energy alternative to memristive
synapses for neuromorphic computation.
</summary>
    <author>
      <name>Y. V. Pershin</name>
    </author>
    <author>
      <name>M. Di Ventra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1049/el.2013.2463</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1049/el.2013.2463" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Electronics Letters 50, 141 (2014)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1307.6921v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.6921v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.6413v1</id>
    <updated>2014-10-23T16:54:39Z</updated>
    <published>2014-10-23T16:54:39Z</published>
    <title>Initialization of multilayer forecasting artifical neural networks</title>
    <summary>  In this paper, a new method was developed for initialising artificial neural
networks predicting dynamics of time series. Initial weighting coefficients
were determined for neurons analogously to the case of a linear prediction
filter. Moreover, to improve the accuracy of the initialization method for a
multilayer neural network, some variants of decomposition of the transformation
matrix corresponding to the linear prediction filter were suggested. The
efficiency of the proposed neural network prediction method by forecasting
solutions of the Lorentz chaotic system is shown in this paper.
</summary>
    <author>
      <name>Vladimir V. Bochkarev</name>
    </author>
    <author>
      <name>Yulia S. Maslennikova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Uchenye Zapiski Kazanskogo Universiteta. Seriya
  Fiziko-Matematicheskie Nauki, 2010, vol. 152, no. 1, pp. 7-14. (In Russian)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1410.6413v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.6413v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M45, 62M10, 68T05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.3684v1</id>
    <updated>2014-12-10T18:23:13Z</updated>
    <published>2014-12-10T18:23:13Z</published>
    <title>Object Recognition Using Deep Neural Networks: A Survey</title>
    <summary>  Recognition of objects using Deep Neural Networks is an active area of
research and many breakthroughs have been made in the last few years. The paper
attempts to indicate how far this field has progressed. The paper briefly
describes the history of research in Neural Networks and describe several of
the recent advances in this field. The performances of recently developed
Neural Network Algorithm over benchmark datasets have been tabulated. Finally,
some the applications of this field have been provided.
</summary>
    <author>
      <name>Soren Goyal</name>
    </author>
    <author>
      <name>Paul Benjamin</name>
    </author>
    <link href="http://arxiv.org/abs/1412.3684v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3684v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.00193v1</id>
    <updated>2015-02-01T04:39:30Z</updated>
    <published>2015-02-01T04:39:30Z</published>
    <title>Evolutionary Artificial Neural Network Based on Chemical Reaction
  Optimization</title>
    <summary>  Evolutionary algorithms (EAs) are very popular tools to design and evolve
artificial neural networks (ANNs), especially to train them. These methods have
advantages over the conventional backpropagation (BP) method because of their
low computational requirement when searching in a large solution space. In this
paper, we employ Chemical Reaction Optimization (CRO), a newly developed global
optimization method, to replace BP in training neural networks. CRO is a
population-based metaheuristics mimicking the transition of molecules and their
interactions in a chemical reaction. Simulation results show that CRO
outperforms many EA strategies commonly used to train neural networks.
</summary>
    <author>
      <name>James J. Q. Yu</name>
    </author>
    <author>
      <name>Albert Y. S. Lam</name>
    </author>
    <author>
      <name>Victor O. K. Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CEC.2011.5949872</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CEC.2011.5949872" rel="related"/>
    <link href="http://arxiv.org/abs/1502.00193v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.00193v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.03827v1</id>
    <updated>2016-03-12T00:02:51Z</updated>
    <published>2016-03-12T00:02:51Z</published>
    <title>Sequential Short-Text Classification with Recurrent and Convolutional
  Neural Networks</title>
    <summary>  Recent approaches based on artificial neural networks (ANNs) have shown
promising results for short-text classification. However, many short texts
occur in sequences (e.g., sentences in a document or utterances in a dialog),
and most existing ANN-based systems do not leverage the preceding short texts
when classifying a subsequent one. In this work, we present a model based on
recurrent neural networks and convolutional neural networks that incorporates
the preceding short texts. Our model achieves state-of-the-art results on three
different datasets for dialog act prediction.
</summary>
    <author>
      <name>Ji Young Lee</name>
    </author>
    <author>
      <name>Franck Dernoncourt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as a conference paper at NAACL 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.03827v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.03827v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07724v1</id>
    <updated>2016-09-25T10:18:19Z</updated>
    <published>2016-09-25T10:18:19Z</published>
    <title>The RNN-ELM Classifier</title>
    <summary>  In this paper we examine learning methods combining the Random Neural
Network, a biologically inspired neural network and the Extreme Learning
Machine that achieve state of the art classification performance while
requiring much shorter training time. The Random Neural Network is a integrate
and fire computational model of a neural network whose mathematical structure
permits the efficient analysis of large ensembles of neurons. An activation
function is derived from the RNN and used in an Extreme Learning Machine. We
compare the performance of this combination against the ELM with various
activation functions, we reduce the input dimensionality via PCA and compare
its performance vs. autoencoder based versions of the RNN-ELM.
</summary>
    <author>
      <name>Athanasios Vlontzos</name>
    </author>
    <link href="http://arxiv.org/abs/1609.07724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.01589v1</id>
    <updated>2016-12-05T23:28:54Z</updated>
    <published>2016-12-05T23:28:54Z</published>
    <title>Improving the Performance of Neural Networks in Regression Tasks Using
  Drawering</title>
    <summary>  The method presented extends a given regression neural network to make its
performance improve. The modification affects the learning procedure only,
hence the extension may be easily omitted during evaluation without any change
in prediction. It means that the modified model may be evaluated as quickly as
the original one but tends to perform better.
  This improvement is possible because the modification gives better expressive
power, provides better behaved gradients and works as a regularization. The
knowledge gained by the temporarily extended neural network is contained in the
parameters shared with the original neural network.
  The only cost is an increase in learning time.
</summary>
    <author>
      <name>Konrad Zolna</name>
    </author>
    <link href="http://arxiv.org/abs/1612.01589v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.01589v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.07655v1</id>
    <updated>2017-11-21T07:23:32Z</updated>
    <published>2017-11-21T07:23:32Z</published>
    <title>Genetic Algorithms for Evolving Deep Neural Networks</title>
    <summary>  In recent years, deep learning methods applying unsupervised learning to
train deep layers of neural networks have achieved remarkable results in
numerous fields. In the past, many genetic algorithms based methods have been
successfully applied to training neural networks. In this paper, we extend
previous work and propose a GA-assisted method for deep learning. Our
experimental results indicate that this GA-assisted approach improves the
performance of a deep autoencoder, producing a sparser neural network.
</summary>
    <author>
      <name>Eli David</name>
    </author>
    <author>
      <name>Iddo Greental</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2598394.2602287</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2598394.2602287" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Genetic and Evolutionary Computation Conference (GECCO), pages
  1451-1452, Vancouver, Canada, July 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1711.07655v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.07655v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.06601v1</id>
    <updated>2018-01-19T23:39:15Z</updated>
    <published>2018-01-19T23:39:15Z</published>
    <title>CMSIS-NN: Efficient Neural Network Kernels for Arm Cortex-M CPUs</title>
    <summary>  Deep Neural Networks are becoming increasingly popular in always-on IoT edge
devices performing data analytics right at the source, reducing latency as well
as energy consumption for data communication. This paper presents CMSIS-NN,
efficient kernels developed to maximize the performance and minimize the memory
footprint of neural network (NN) applications on Arm Cortex-M processors
targeted for intelligent IoT edge devices. Neural network inference based on
CMSIS-NN kernels achieves 4.6X improvement in runtime/throughput and 4.9X
improvement in energy efficiency.
</summary>
    <author>
      <name>Liangzhen Lai</name>
    </author>
    <author>
      <name>Naveen Suda</name>
    </author>
    <author>
      <name>Vikas Chandra</name>
    </author>
    <link href="http://arxiv.org/abs/1801.06601v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.06601v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.01670v2</id>
    <updated>2016-03-08T16:36:00Z</updated>
    <published>2016-03-05T02:06:43Z</published>
    <title>Network Morphism</title>
    <summary>  We present in this paper a systematic study on how to morph a well-trained
neural network to a new one so that its network function can be completely
preserved. We define this as \emph{network morphism} in this research. After
morphing a parent network, the child network is expected to inherit the
knowledge from its parent network and also has the potential to continue
growing into a more powerful one with much shortened training time. The first
requirement for this network morphism is its ability to handle diverse morphing
types of networks, including changes of depth, width, kernel size, and even
subnet. To meet this requirement, we first introduce the network morphism
equations, and then develop novel morphing algorithms for all these morphing
types for both classic and convolutional neural networks. The second
requirement for this network morphism is its ability to deal with non-linearity
in a network. We propose a family of parametric-activation functions to
facilitate the morphing of any continuous non-linear activation neurons.
Experimental results on benchmark datasets and typical neural networks
demonstrate the effectiveness of the proposed network morphism scheme.
</summary>
    <author>
      <name>Tao Wei</name>
    </author>
    <author>
      <name>Changhu Wang</name>
    </author>
    <author>
      <name>Yong Rui</name>
    </author>
    <author>
      <name>Chang Wen Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review for ICML 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.01670v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.01670v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.1127v3</id>
    <updated>2009-05-20T11:23:05Z</updated>
    <published>2008-09-06T03:44:09Z</published>
    <title>Self-organization of feedforward structure and entrainment in excitatory
  neural networks with spike-timing-dependent plasticity</title>
    <summary>  Spike-timing dependent plasticity (STDP) is an organizing principle of
biological neural networks. While synchronous firing of neurons is considered
to be an important functional block in the brain, how STDP shapes neural
networks possibly toward synchrony is not entirely clear. We examine relations
between STDP and synchronous firing in spontaneously firing neural populations.
Using coupled heterogeneous phase oscillators placed on initial networks, we
show numerically that STDP prunes some synapses and promotes formation of a
feedforward network. Eventually a pacemaker, which is the neuron with the
fastest inherent frequency in our numerical simulations, emerges at the root of
the feedforward network. In each oscillatory cycle, a packet of neural activity
is propagated from the pacemaker to downstream neurons along layers of the
feedforward network. This event occurs above a clear-cut threshold value of the
initial synaptic weight. Below the threshold, neurons are self-organized into
separate clusters each of which is a feedforward network.
</summary>
    <author>
      <name>Yuko K. Takahashi</name>
    </author>
    <author>
      <name>Hiroshi Kori</name>
    </author>
    <author>
      <name>Naoki Masuda</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.79.051904</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.79.051904" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 figures, 1 table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physical Review E, 79, 051904 (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0809.1127v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.1127v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05916v1</id>
    <updated>2016-08-21T10:48:36Z</updated>
    <published>2016-08-21T10:48:36Z</published>
    <title>Neural Networks and Chaos: Construction, Evaluation of Chaotic Networks,
  and Prediction of Chaos with Multilayer Feedforward Networks</title>
    <summary>  Many research works deal with chaotic neural networks for various fields of
application. Unfortunately, up to now these networks are usually claimed to be
chaotic without any mathematical proof. The purpose of this paper is to
establish, based on a rigorous theoretical framework, an equivalence between
chaotic iterations according to Devaney and a particular class of neural
networks. On the one hand we show how to build such a network, on the other
hand we provide a method to check if a neural network is a chaotic one.
Finally, the ability of classical feedforward multilayer perceptrons to learn
sets of data obtained from a dynamical system is regarded. Various Boolean
functions are iterated on finite states. Iterations of some of them are proven
to be chaotic as it is defined by Devaney. In that context, important
differences occur in the training process, establishing with various neural
networks that chaotic behaviors are far more difficult to learn.
</summary>
    <author>
      <name>Jacques M. Bahi</name>
    </author>
    <author>
      <name>Jean-Fran√ßois Couchot</name>
    </author>
    <author>
      <name>Christophe Guyeux</name>
    </author>
    <author>
      <name>Michel Salomon</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">AIP Chaos, An Interdisciplinary Journal of Nonlinear Science.
  22(1), 013122 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1608.05916v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05916v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.06464v1</id>
    <updated>2017-11-17T09:29:52Z</updated>
    <published>2017-11-17T09:29:52Z</published>
    <title>A unified deep artificial neural network approach to partial
  differential equations in complex geometries</title>
    <summary>  We use deep feedforward artificial neural networks to approximate solutions
of partial differential equations of advection and diffusion type in complex
geometries. We derive analytical expressions of the gradients of the cost
function with respect to the network parameters, as well as the gradient of the
network itself with respect to the input, for arbitrarily deep networks. The
method is based on an ansatz for the solution, which requires nothing but
feedforward neural networks, and an unconstrained gradient based optimization
method such as gradient descent or quasi-Newton methods.
  We provide detailed examples on how to use deep feedforward neural networks
as a basis for further work on deep neural network approximations to partial
differential equations. We highlight the benefits of deep compared to shallow
neural networks and other convergence enhancing techniques.
</summary>
    <author>
      <name>Jens Berg</name>
    </author>
    <author>
      <name>Kaj Nystr√∂m</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.06464v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.06464v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.04161v2</id>
    <updated>2017-03-03T20:43:04Z</updated>
    <published>2016-10-13T16:34:30Z</published>
    <title>Why Deep Neural Networks for Function Approximation?</title>
    <summary>  Recently there has been much interest in understanding why deep neural
networks are preferred to shallow networks. We show that, for a large class of
piecewise smooth functions, the number of neurons needed by a shallow network
to approximate a function is exponentially larger than the corresponding number
of neurons needed by a deep network for a given degree of function
approximation. First, we consider univariate functions on a bounded interval
and require a neural network to achieve an approximation error of $\varepsilon$
uniformly over the interval. We show that shallow networks (i.e., networks
whose depth does not depend on $\varepsilon$) require
$\Omega(\text{poly}(1/\varepsilon))$ neurons while deep networks (i.e.,
networks whose depth grows with $1/\varepsilon$) require
$\mathcal{O}(\text{polylog}(1/\varepsilon))$ neurons. We then extend these
results to certain classes of important multivariate functions. Our results are
derived for neural networks which use a combination of rectifier linear units
(ReLUs) and binary step units, two of the most popular type of activation
functions. Our analysis builds on a simple observation: the multiplication of
two bits can be represented by a ReLU.
</summary>
    <author>
      <name>Shiyu Liang</name>
    </author>
    <author>
      <name>R. Srikant</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper is published at the 5th International Conference on
  Learning Representations (ICLR)</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.04161v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.04161v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0470v1</id>
    <updated>2014-09-01T16:20:41Z</updated>
    <published>2014-09-01T16:20:41Z</published>
    <title>Neural coordination can be enhanced by occasional interruption of normal
  firing patterns: A self-optimizing spiking neural network model</title>
    <summary>  The state space of a conventional Hopfield network typically exhibits many
different attractors of which only a small subset satisfy constraints between
neurons in a globally optimal fashion. It has recently been demonstrated that
combining Hebbian learning with occasional alterations of normal neural states
avoids this problem by means of self-organized enlargement of the best basins
of attraction. However, so far it is not clear to what extent this process of
self-optimization is also operative in real brains. Here we demonstrate that it
can be transferred to more biologically plausible neural networks by
implementing a self-optimizing spiking neural network model. In addition, by
using this spiking neural network to emulate a Hopfield network with Hebbian
learning, we attempt to make a connection between rate-based and temporal
coding based neural systems. Although further work is required to make this
model more realistic, it already suggests that the efficacy of the
self-optimizing process is independent from the simplifying assumptions of a
conventional Hopfield network. We also discuss natural and cultural processes
that could be responsible for occasional alteration of neural firing patterns
in actual brains
</summary>
    <author>
      <name>Alexander Woodward</name>
    </author>
    <author>
      <name>Tom Froese</name>
    </author>
    <author>
      <name>Takashi Ikegami</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 6 figures; Neural Networks, in press</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.0470v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0470v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92B20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; F.1.1; C.1.3; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.04818v1</id>
    <updated>2017-03-14T23:10:57Z</updated>
    <published>2017-03-14T23:10:57Z</published>
    <title>Neural Graph Machines: Learning Neural Networks Using Graphs</title>
    <summary>  Label propagation is a powerful and flexible semi-supervised learning
technique on graphs. Neural networks, on the other hand, have proven track
records in many supervised learning tasks. In this work, we propose a training
framework with a graph-regularised objective, namely "Neural Graph Machines",
that can combine the power of neural networks and label propagation. This work
generalises previous literature on graph-augmented training of neural networks,
enabling it to be applied to multiple neural architectures (Feed-forward NNs,
CNNs and LSTM RNNs) and a wide range of graphs. The new objective allows the
neural networks to harness both labeled and unlabeled data by: (a) allowing the
network to train using labeled data as in the supervised setting, (b) biasing
the network to learn similar hidden representations for neighboring nodes on a
graph, in the same vein as label propagation. Such architectures with the
proposed objective can be trained efficiently using stochastic gradient descent
and scaled to large graphs, with a runtime that is linear in the number of
edges. The proposed joint training approach convincingly outperforms many
existing methods on a wide range of tasks (multi-label classification on social
graphs, news categorization, document classification and semantic intent
classification), with multiple forms of graph inputs (including graphs with and
without node-level features) and using different types of neural networks.
</summary>
    <author>
      <name>Thang D. Bui</name>
    </author>
    <author>
      <name>Sujith Ravi</name>
    </author>
    <author>
      <name>Vivek Ramavajjala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.04818v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.04818v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0405024v1</id>
    <updated>2004-05-06T13:44:20Z</updated>
    <published>2004-05-06T13:44:20Z</published>
    <title>Meta-Learning Evolutionary Artificial Neural Networks</title>
    <summary>  In this paper, we present MLEANN (Meta-Learning Evolutionary Artificial
Neural Network), an automatic computational framework for the adaptive
optimization of artificial neural networks wherein the neural network
architecture, activation function, connection weights; learning algorithm and
its parameters are adapted according to the problem. We explored the
performance of MLEANN and conventionally designed artificial neural networks
for function approximation problems. To evaluate the comparative performance,
we used three different well-known chaotic time series. We also present the
state of the art popular neural network learning algorithms and some
experimentation results related to convergence speed and generalization
performance. We explored the performance of backpropagation algorithm;
conjugate gradient algorithm, quasi-Newton algorithm and Levenberg-Marquardt
algorithm for the three chaotic time series. Performances of the different
learning algorithms were evaluated when the activation functions and
architecture were changed. We further present the theoretical background,
algorithm, design strategy and further demonstrate how effective and inevitable
is the proposed MLEANN framework to design a neural network, which is smaller,
faster and with a better generalization performance.
</summary>
    <author>
      <name>Ajith Abraham</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neurocomputing Journal, Elsevier Science, Netherlands, Vol. 56c,
  pp. 1-38, 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0405024v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0405024v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/hep-ph/0606257v1</id>
    <updated>2006-06-24T00:42:19Z</updated>
    <published>2006-06-24T00:42:19Z</published>
    <title>An Empirical Study of Boosted Neural Network for Particle Classification
  in High Energy Collisions</title>
    <summary>  The possible application of boosted neural network to particle classification
in high energy physics is discussed. A two-dimensional toy model, where the
boundary between signal and background is irregular but not overlapping, is
constructed to show how boosting technique works with neural network. It is
found that boosted neural network not only decreases the error rate of
classification significantly but also increases the efficiency and
signal-background ratio. Besides, boosted neural network can avoid the
disadvantage aspects of single neural network design. The boosted neural
network is also applied to the classification of quark- and gluon- jet samples
from Monte Carlo \EE collisions, where the two samples show significant
overlapping. The performance of boosting technique for the two different
boundary cases -- with and without overlapping is discussed.
</summary>
    <author>
      <name>Yu Meiling</name>
    </author>
    <author>
      <name>Xu Mingmei</name>
    </author>
    <author>
      <name>Liu Lianshou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/hep-ph/0606257v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/hep-ph/0606257v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06905v2</id>
    <updated>2016-08-02T16:17:05Z</updated>
    <published>2016-06-22T11:30:47Z</published>
    <title>Learning text representation using recurrent convolutional neural
  network with highway layers</title>
    <summary>  Recently, the rapid development of word embedding and neural networks has
brought new inspiration to various NLP and IR tasks. In this paper, we describe
a staged hybrid model combining Recurrent Convolutional Neural Networks (RCNN)
with highway layers. The highway network module is incorporated in the middle
takes the output of the bi-directional Recurrent Neural Network (Bi-RNN) module
in the first stage and provides the Convolutional Neural Network (CNN) module
in the last stage with the input. The experiment shows that our model
outperforms common neural network models (CNN, RNN, Bi-RNN) on a sentiment
analysis task. Besides, the analysis of how sequence length influences the RCNN
with highway layers shows that our model could learn good representation for
the long text.
</summary>
    <author>
      <name>Ying Wen</name>
    </author>
    <author>
      <name>Weinan Zhang</name>
    </author>
    <author>
      <name>Rui Luo</name>
    </author>
    <author>
      <name>Jun Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Neu-IR '16 SIGIR Workshop on Neural Information Retrieval</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.06905v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06905v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09448v2</id>
    <updated>2016-11-30T02:00:48Z</updated>
    <published>2016-11-29T01:07:58Z</published>
    <title>The Upper Bound on Knots in Neural Networks</title>
    <summary>  Neural networks with rectified linear unit activations are essentially
multivariate linear splines. As such, one of many ways to measure the
"complexity" or "expressivity" of a neural network is to count the number of
knots in the spline model. We study the number of knots in fully-connected
feedforward neural networks with rectified linear unit activation functions. We
intentionally keep the neural networks very simple, so as to make theoretical
analyses more approachable. An induction on the number of layers $l$ reveals a
tight upper bound on the number of knots in $\mathbb{R} \to \mathbb{R}^p$ deep
neural networks. With $n_i \gg 1$ neurons in layer $i = 1, \dots, l$, the upper
bound is approximately $n_1 \dots n_l$. We then show that the exact upper bound
is tight, and we demonstrate the upper bound with an example. The purpose of
these analyses is to pave a path for understanding the behavior of general
$\mathbb{R}^q \to \mathbb{R}^p$ neural networks.
</summary>
    <author>
      <name>Kevin K. Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.09448v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09448v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07199v1</id>
    <updated>2017-05-19T21:33:00Z</updated>
    <published>2017-05-19T21:33:00Z</published>
    <title>The High-Dimensional Geometry of Binary Neural Networks</title>
    <summary>  Recent research has shown that one can train a neural network with binary
weights and activations at train time by augmenting the weights with a
high-precision continuous latent variable that accumulates small changes from
stochastic gradient descent. However, there is a dearth of theoretical analysis
to explain why we can effectively capture the features in our data with binary
weights and activations. Our main result is that the neural networks with
binary weights and activations trained using the method of Courbariaux, Hubara
et al. (2016) work because of the high-dimensional geometry of binary vectors.
In particular, the ideal continuous vectors that extract out features in the
intermediate representations of these BNNs are well-approximated by binary
vectors in the sense that dot products are approximately preserved. Compared to
previous research that demonstrated the viability of such BNNs, our work
explains why these BNNs work in terms of the HD geometry. Our theory serves as
a foundation for understanding not only BNNs but a variety of methods that seek
to compress traditional neural networks. Furthermore, a better understanding of
multilayer binary neural networks serves as a starting point for generalizing
BNNs to other neural network architectures such as recurrent neural networks.
</summary>
    <author>
      <name>Alexander G. Anderson</name>
    </author>
    <author>
      <name>Cory P. Berg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.07199v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07199v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09779v1</id>
    <updated>2017-06-29T14:38:13Z</updated>
    <published>2017-06-29T14:38:13Z</published>
    <title>Deep neural networks for direct, featureless learning through
  observation: the case of 2d spin models</title>
    <summary>  We train a deep convolutional neural network to accurately predict the
energies and magnetizations of Ising model configurations, using both the
traditional nearest-neighbour Hamiltonian, as well as a long-range screened
Coulomb Hamiltonian. We demonstrate the capability of a convolutional deep
neural network in predicting the nearest-neighbour energy of the 4x4 Ising
model. Using its success at this task, we motivate the study of the larger 8x8
Ising model, showing that the deep neural network can learn the
nearest-neighbour Ising Hamiltonian after only seeing a vanishingly small
fraction of configuration space. Additionally, we show that the neural network
has learned both the energy and magnetization operators with sufficient
accuracy to replicate the low-temperature Ising phase transition. Finally, we
teach the convolutional deep neural network to accurately predict a long-range
interaction through a screened Coulomb Hamiltonian. In this case, the benefits
of the neural network become apparent; it is able to make predictions with a
high degree of accuracy, 1600 times faster than a CUDA-optimized "exact"
calculation.
</summary>
    <author>
      <name>K. Mills</name>
    </author>
    <author>
      <name>I. Tamblyn</name>
    </author>
    <link href="http://arxiv.org/abs/1706.09779v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09779v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.02196v1</id>
    <updated>2017-10-05T20:04:10Z</updated>
    <published>2017-10-05T20:04:10Z</published>
    <title>Porcupine Neural Networks: (Almost) All Local Optima are Global</title>
    <summary>  Neural networks have been used prominently in several machine learning and
statistics applications. In general, the underlying optimization of neural
networks is non-convex which makes their performance analysis challenging. In
this paper, we take a novel approach to this problem by asking whether one can
constrain neural network weights to make its optimization landscape have good
theoretical properties while at the same time, be a good approximation for the
unconstrained one. For two-layer neural networks, we provide affirmative
answers to these questions by introducing Porcupine Neural Networks (PNNs)
whose weight vectors are constrained to lie over a finite set of lines. We show
that most local optima of PNN optimizations are global while we have a
characterization of regions where bad local optimizers may exist. Moreover, our
theoretical and empirical results suggest that an unconstrained neural network
can be approximated using a polynomially-large PNN.
</summary>
    <author>
      <name>Soheil Feizi</name>
    </author>
    <author>
      <name>Hamid Javadi</name>
    </author>
    <author>
      <name>Jesse Zhang</name>
    </author>
    <author>
      <name>David Tse</name>
    </author>
    <link href="http://arxiv.org/abs/1710.02196v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.02196v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.04481v1</id>
    <updated>2017-11-13T09:16:09Z</updated>
    <published>2017-11-13T09:16:09Z</published>
    <title>An Automatic Diagnosis Method of Facial Acne Vulgaris Based on
  Convolutional Neural Network</title>
    <summary>  In this paper, we present a new automatic diagnosis method of facial acne
vulgaris based on convolutional neural network. This method is proposed to
overcome the shortcoming of classification types in previous methods. The core
of our method is to extract features of images based on convolutional neural
network and achieve classification by classifier. We design a binary classifier
of skin-and-non-skin to detect skin area and a seven-classifier to achieve the
classification of facial acne vulgaris and healthy skin. In the experiment, we
compared the effectiveness of our convolutional neural network and the
pre-trained VGG16 neural network on the ImageNet dataset. And we use the ROC
curve and normal confusion matrix to evaluate the performance of the binary
classifier and the seven-classifier. The results of our experiment show that
the pre-trained VGG16 neural network is more effective in extracting image
features. The classifiers based on the pre-trained VGG16 neural network achieve
the skin detection and acne classification and have good robustness.
</summary>
    <author>
      <name>Xiaolei Shen</name>
    </author>
    <author>
      <name>Jiachi Zhang</name>
    </author>
    <author>
      <name>Chenjun Yan</name>
    </author>
    <author>
      <name>Hong Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 7 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.04481v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.04481v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.08163v1</id>
    <updated>2017-12-21T08:57:06Z</updated>
    <published>2017-12-21T08:57:06Z</published>
    <title>Reachable Set Computation and Safety Verification for Neural Networks
  with ReLU Activations</title>
    <summary>  Neural networks have been widely used to solve complex real-world problems.
Due to the complicate, nonlinear, non-convex nature of neural networks, formal
safety guarantees for the output behaviors of neural networks will be crucial
for their applications in safety-critical systems.In this paper, the output
reachable set computation and safety verification problems for a class of
neural networks consisting of Rectified Linear Unit (ReLU) activation functions
are addressed. A layer-by-layer approach is developed to compute output
reachable set. The computation is formulated in the form of a set of
manipulations for a union of polyhedra, which can be efficiently applied with
the aid of polyhedron computation tools. Based on the output reachable set
computation results, the safety verification for a ReLU neural network can be
performed by checking the intersections of unsafe regions and output reachable
set described by a union of polyhedra. A numerical example of a randomly
generated ReLU neural network is provided to show the effectiveness of the
approach developed in this paper.
</summary>
    <author>
      <name>Weiming Xiang</name>
    </author>
    <author>
      <name>Hoang-Dung Tran</name>
    </author>
    <author>
      <name>Taylor T. Johnson</name>
    </author>
    <link href="http://arxiv.org/abs/1712.08163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.08163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03557v1</id>
    <updated>2018-02-10T09:26:56Z</updated>
    <published>2018-02-10T09:26:56Z</published>
    <title>Reachable Set Estimation and Verification for Neural Network Models of
  Nonlinear Dynamic Systems</title>
    <summary>  Neural networks have been widely used to solve complex real-world problems.
Due to the complicate, nonlinear, non-convex nature of neural networks, formal
safety guarantees for the behaviors of neural network systems will be crucial
for their applications in safety-critical systems. In this paper, the reachable
set estimation and verification problems for Nonlinear Autoregressive-Moving
Average (NARMA) models in the forms of neural networks are addressed. The
neural network involved in the model is a class of feed-forward neural networks
called Multi-Layer Perceptron (MLP). By partitioning the input set of an MLP
into a finite number of cells, a layer-by-layer computation algorithm is
developed for reachable set estimation for each individual cell. The union of
estimated reachable sets of all cells forms an over-approximation of reachable
set of the MLP. Furthermore, an iterative reachable set estimation algorithm
based on reachable set estimation for MLPs is developed for NARMA models. The
safety verification can be performed by checking the existence of intersections
of unsafe regions and estimated reachable set. Several numerical examples are
provided to illustrate our approach.
</summary>
    <author>
      <name>Weiming Xiang</name>
    </author>
    <author>
      <name>Diego Manzanas Lopez</name>
    </author>
    <author>
      <name>Patrick Musau</name>
    </author>
    <author>
      <name>Taylor T. Johnson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.03557v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03557v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03930v1</id>
    <updated>2018-02-12T08:25:55Z</updated>
    <published>2018-02-12T08:25:55Z</published>
    <title>Visualizing Neural Network Developing Perturbation Theory</title>
    <summary>  In this letter, motivated by the question that whether the empirical fitting
of data by neural network can yield the same structure of physical laws, we
apply the neural network to a simple quantum mechanical two-body scattering
problem with short-range potentials, which by itself also plays an important
role in many branches of physics. We train a neural network to accurately
predict $ s $-wave scattering length, which governs the low-energy scattering
physics, directly from the scattering potential without solving Schr\"odinger
equation or obtaining the wavefunction. After analyzing the neural network, it
is shown that the neural network develops perturbation theory order by order
when the potential increases. This provides an important benchmark to the
machine-assisted physics research or even automated machine learning physics
laws.
</summary>
    <author>
      <name>Yadong Wu</name>
    </author>
    <author>
      <name>Pengfei Zhang</name>
    </author>
    <author>
      <name>Huitao Shen</name>
    </author>
    <author>
      <name>Hui Zhai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.03930v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03930v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.02501v1</id>
    <updated>2016-10-08T08:57:36Z</updated>
    <published>2016-10-08T08:57:36Z</published>
    <title>Revisiting Multiple Instance Neural Networks</title>
    <summary>  Recently neural networks and multiple instance learning are both attractive
topics in Artificial Intelligence related research fields. Deep neural networks
have achieved great success in supervised learning problems, and multiple
instance learning as a typical weakly-supervised learning method is effective
for many applications in computer vision, biometrics, nature language
processing, etc. In this paper, we revisit the problem of solving multiple
instance learning problems using neural networks. Neural networks are appealing
for solving multiple instance learning problem. The multiple instance neural
networks perform multiple instance learning in an end-to-end way, which take a
bag with various number of instances as input and directly output bag label.
All of the parameters in a multiple instance network are able to be optimized
via back-propagation. We propose a new multiple instance neural network to
learn bag representations, which is different from the existing multiple
instance neural networks that focus on estimating instance label. In addition,
recent tricks developed in deep learning have been studied in multiple instance
networks, we find deep supervision is effective for boosting bag classification
accuracy. In the experiments, the proposed multiple instance networks achieve
state-of-the-art or competitive performance on several MIL benchmarks.
Moreover, it is extremely fast for both testing and training, e.g., it takes
only 0.0003 second to predict a bag and a few seconds to train on a MIL
datasets on a moderate CPU.
</summary>
    <author>
      <name>Xinggang Wang</name>
    </author>
    <author>
      <name>Yongluan Yan</name>
    </author>
    <author>
      <name>Peng Tang</name>
    </author>
    <author>
      <name>Xiang Bai</name>
    </author>
    <author>
      <name>Wenyu Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1610.02501v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.02501v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9704098v1</id>
    <updated>1997-04-11T09:26:38Z</updated>
    <published>1997-04-11T09:26:38Z</published>
    <title>Phase Transitions of Neural Networks</title>
    <summary>  The cooperative behaviour of interacting neurons and synapses is studied
using models and methods from statistical physics. The competition between
training error and entropy may lead to discontinuous properties of the neural
network. This is demonstrated for a few examples: Perceptron, associative
memory, learning from examples, generalization, multilayer networks, structure
recognition, Bayesian estimate, on-line training, noise estimation and time
series generation.
</summary>
    <author>
      <name>Wolfgang Kinzel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/13642819808205038</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/13642819808205038" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Plenary talk for MINERVA workshop on mesoscopics, fractals and neural
  networks, Eilat, March 1997 Postscript File</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9704098v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9704098v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9808041v1</id>
    <updated>1998-08-04T18:38:03Z</updated>
    <published>1998-08-04T18:38:03Z</published>
    <title>Neural networks and logical reasoning systems. A translation table</title>
    <summary>  A correspondence is established between the elements of logic reasoning
systems (knowledge bases, rules, inference and queries) and the hardware and
dynamical operations of neural networks. The correspondence is framed as a
general translation dictionary which, hopefully, will allow to go back and
forth between symbolic and network formulations, a desirable step in
learning-oriented systems and multicomputer networks. In the framework of Horn
clause logics it is found that atomic propositions with n arguments correspond
to nodes with n-th order synapses, rules to synaptic intensity constraints,
forward chaining to synaptic dynamics and queries either to simple node
activation or to a query tensor dynamics.
</summary>
    <author>
      <name>Joao Martins</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Laboratorio de Mecatronica, DEEC, IST, Portugal</arxiv:affiliation>
    </author>
    <author>
      <name>R. Vilela Mendes</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Laboratorio de Mecatronica, DEEC, IST, Portugal</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Neural Systems 11 (2001) 179-186</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/9808041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9808041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9909443v1</id>
    <updated>1999-09-30T08:17:21Z</updated>
    <published>1999-09-30T08:17:21Z</published>
    <title>Thresholds in layered neural networks with variable activity</title>
    <summary>  The inclusion of a threshold in the dynamics of layered neural networks with
variable activity is studied at arbitrary temperature. In particular, the
effects on the retrieval quality of a self-controlled threshold obtained by
forcing the neural activity to stay equal to the activity of the stored paterns
during the whole retrieval process, are compared with those of a threshold
chosen externally for every loading and every temperature through optimisation
of the mutual information content of the network. Numerical results, mostly
concerning low activity networks are discussed.
</summary>
    <author>
      <name>D. Bolle'</name>
    </author>
    <author>
      <name>G. Massolo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/0305-4470/33/14/301</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/0305-4470/33/14/301" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, Latex2e, 6 eps figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J. Phys. A 33, 2597-2609 (2000).</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/9909443v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9909443v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0405505v1</id>
    <updated>2004-05-21T04:49:41Z</updated>
    <published>2004-05-21T04:49:41Z</published>
    <title>Analyzing Stability of Equilibrium Points in Neural Networks: A General
  Approach</title>
    <summary>  Networks of coupled neural systems represent an important class of models in
computational neuroscience. In some applications it is required that
equilibrium points in these networks remain stable under parameter variations.
Here we present a general methodology to yield explicit constraints on the
coupling strengths to ensure the stability of the equilibrium point. Two models
of coupled excitatory-inhibitory oscillators are used to illustrate the
approach.
</summary>
    <author>
      <name>Wilson A. Truccolo</name>
    </author>
    <author>
      <name>Govindan Rangarajan</name>
    </author>
    <author>
      <name>Yonghong Chen</name>
    </author>
    <author>
      <name>Mingzhou Ding</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Networks, vol. 16, 1453-1460 (2003)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0405505v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0405505v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0412680v1</id>
    <updated>2004-12-24T11:38:34Z</updated>
    <published>2004-12-24T11:38:34Z</published>
    <title>Vector-Neuron Models of Associative Memory</title>
    <summary>  We consider two models of Hopfield-like associative memory with $q$-valued
neurons: Potts-glass neural network (PGNN) and parametrical neural network
(PNN). In these models neurons can be in more than two different states. The
models have the record characteristics of its storage capacity and noise
immunity, and significantly exceed the Hopfield model. We present a uniform
formalism allowing us to describe both PNN and PGNN. This networks inherent
mechanisms, responsible for outstanding recognizing properties, are clarified.
</summary>
    <author>
      <name>B. V. Kryzhanovsky</name>
    </author>
    <author>
      <name>L. B. Litinskii</name>
    </author>
    <author>
      <name>A. L. Mikaelian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, Lecture on International Joint Conference on Neural Networks
  IJCNN-2004</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0412680v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0412680v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/q-bio/0601003v2</id>
    <updated>2006-10-24T14:14:07Z</updated>
    <published>2006-01-03T18:54:23Z</published>
    <title>Designing the Dynamics of Spiking Neural Networks</title>
    <summary>  Precise timing of spikes and temporal locking are key elements of neural
computation. Here we demonstrate how even strongly heterogeneous, deterministic
neural networks with delayed interactions and complex topology can exhibit
periodic patterns of spikes that are precisely timed. We develop an analytical
method to find the set of all networks exhibiting a predefined pattern
dynamics. Such patterns may be arbitrarily long and of complicated temporal
structure. We point out that the same pattern can exist in very different
networks and have different stability properties.
</summary>
    <author>
      <name>Raoul-Martin Memmesheimer</name>
    </author>
    <author>
      <name>Marc Timme</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.97.188101</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.97.188101" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/q-bio/0601003v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/q-bio/0601003v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.3451v1</id>
    <updated>2012-11-14T22:28:56Z</updated>
    <published>2012-11-14T22:28:56Z</published>
    <title>Memory Capacity of a Random Neural Network</title>
    <summary>  This paper considers the problem of information capacity of a random neural
network. The network is represented by matrices that are square and
symmetrical. The matrices have a weight which determines the highest and lowest
possible value found in the matrix. The examined matrices are randomly
generated and analyzed by a computer program. We find the surprising result
that the capacity of the network is a maximum for the binary random neural
network and it does not change as the number of quantization levels associated
with the weights increases.
</summary>
    <author>
      <name>Matt Stowe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.3451v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.3451v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.05133v2</id>
    <updated>2015-09-02T18:27:36Z</updated>
    <published>2015-08-20T21:35:52Z</published>
    <title>Steps Toward Deep Kernel Methods from Infinite Neural Networks</title>
    <summary>  Contemporary deep neural networks exhibit impressive results on practical
problems. These networks generalize well although their inherent capacity may
extend significantly beyond the number of training examples. We analyze this
behavior in the context of deep, infinite neural networks. We show that deep
infinite layers are naturally aligned with Gaussian processes and kernel
methods, and devise stochastic kernels that encode the information of these
networks. We show that stability results apply despite the size, offering an
explanation for their empirical success.
</summary>
    <author>
      <name>Tamir Hazan</name>
    </author>
    <author>
      <name>Tommi Jaakkola</name>
    </author>
    <link href="http://arxiv.org/abs/1508.05133v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.05133v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.00062v1</id>
    <updated>2015-08-28T16:28:27Z</updated>
    <published>2015-08-28T16:28:27Z</published>
    <title>The Kinetic Energy of Hydrocarbons as a Function of Electron Density and
  Convolutional Neural Networks</title>
    <summary>  We demonstrate a convolutional neural network trained to reproduce the
Kohn-Sham kinetic energy of hydrocarbons from electron density. The output of
the network is used as a non-local correction to the conventional local and
semi-local kinetic functionals. We show that this approximation qualitatively
reproduces Kohn-Sham potential energy surfaces when used with conventional
exchange correlation functionals. Numerical noise inherited from the
non-linearity of the neural network is identified as the major challenge for
the model. Finally we examine the features in the density learned by the neural
network to anticipate the prospects of generalizing these models.
</summary>
    <author>
      <name>Kun Yao</name>
    </author>
    <author>
      <name>John Parkhill</name>
    </author>
    <link href="http://arxiv.org/abs/1509.00062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.00062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.09887v2</id>
    <updated>2017-03-09T18:07:37Z</updated>
    <published>2016-10-31T12:08:46Z</published>
    <title>Depth-Width Tradeoffs in Approximating Natural Functions with Neural
  Networks</title>
    <summary>  We provide several new depth-based separation results for feed-forward neural
networks, proving that various types of simple and natural functions can be
better approximated using deeper networks than shallower ones, even if the
shallower networks are much larger. This includes indicators of balls and
ellipses; non-linear functions which are radial with respect to the $L_1$ norm;
and smooth non-linear functions. We also show that these gaps can be observed
experimentally: Increasing the depth indeed allows better learning than
increasing width, when training neural networks to learn an indicator of a unit
ball.
</summary>
    <author>
      <name>Itay Safran</name>
    </author>
    <author>
      <name>Ohad Shamir</name>
    </author>
    <link href="http://arxiv.org/abs/1610.09887v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.09887v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.06694v1</id>
    <updated>2016-11-21T09:24:24Z</updated>
    <published>2016-11-21T09:24:24Z</published>
    <title>Training Sparse Neural Networks</title>
    <summary>  Deep neural networks with lots of parameters are typically used for
large-scale computer vision tasks such as image classification. This is a
result of using dense matrix multiplications and convolutions. However, sparse
computations are known to be much more efficient. In this work, we train and
build neural networks which implicitly use sparse computations. We introduce
additional gate variables to perform parameter selection and show that this is
equivalent to using a spike-and-slab prior. We experimentally validate our
method on both small and large networks and achieve state-of-the-art
compression results for sparse neural network models.
</summary>
    <author>
      <name>Suraj Srinivas</name>
    </author>
    <author>
      <name>Akshayvarun Subramanya</name>
    </author>
    <author>
      <name>R. Venkatesh Babu</name>
    </author>
    <link href="http://arxiv.org/abs/1611.06694v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.06694v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00534v1</id>
    <updated>2017-03-01T22:21:21Z</updated>
    <published>2017-03-01T22:21:21Z</published>
    <title>Skin cancer reorganization and classification with deep neural network</title>
    <summary>  As one kind of skin cancer, melanoma is very dangerous. Dermoscopy based
early detection and recarbonization strategy is critical for melanoma therapy.
However, well-trained dermatologists dominant the diagnostic accuracy. In order
to solve this problem, many effort focus on developing automatic image analysis
systems. Here we report a novel strategy based on deep learning technique, and
achieve very high skin lesion segmentation and melanoma diagnosis accuracy: 1)
we build a segmentation neural network (skin_segnn), which achieved very high
lesion boundary detection accuracy; 2) We build another very deep neural
network based on Google inception v3 network (skin_recnn) and its well-trained
weight. The novel designed transfer learning based deep neural network
skin_inceptions_v3_nn helps to achieve a high prediction accuracy.
</summary>
    <author>
      <name>Hao Chang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures. ISIC2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.00534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00784v1</id>
    <updated>2017-07-03T23:45:54Z</updated>
    <published>2017-07-03T23:45:54Z</published>
    <title>Deep Jointly-Informed Neural Networks</title>
    <summary>  In this work a novel, automated process for determining an appropriate deep
neural network architecture and weight initialization based on decision trees
is presented. The method maps a collection of decision trees trained on the
data into a collection of initialized neural networks, with the structure of
the network determined by the structure of the tree. These models, referred to
as "deep jointly-informed neural networks", demonstrate high predictive
performance for a variety of datasets. Furthermore, the algorithm is readily
cast into a Bayesian framework, resulting in accurate and scalable models that
provide quantified uncertainties on predictions.
</summary>
    <author>
      <name>K. D. Humbird</name>
    </author>
    <author>
      <name>J. L. Peterson</name>
    </author>
    <author>
      <name>R. G. McClarren</name>
    </author>
    <link href="http://arxiv.org/abs/1707.00784v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00784v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.03414v1</id>
    <updated>2017-10-10T06:14:58Z</updated>
    <published>2017-10-10T06:14:58Z</published>
    <title>Network of Recurrent Neural Networks</title>
    <summary>  We describe a class of systems theory based neural networks called "Network
Of Recurrent neural networks" (NOR), which introduces a new structure level to
RNN related models. In NOR, RNNs are viewed as the high-level neurons and are
used to build the high-level layers. More specifically, we propose several
methodologies to design different NOR topologies according to the theory of
system evolution. Then we carry experiments on three different tasks to
evaluate our implementations. Experimental results show our models outperform
simple RNN remarkably under the same number of parameters, and sometimes
achieve even better results than GRU and LSTM.
</summary>
    <author>
      <name>Chao-Ming Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review as a conference paper at AAAI 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.03414v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.03414v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03646v1</id>
    <updated>2018-02-10T19:43:42Z</updated>
    <published>2018-02-10T19:43:42Z</published>
    <title>On the Universal Approximability of Quantized ReLU Neural Networks</title>
    <summary>  Compression is a key step to deploy large neural networks on
resource-constrained platforms. As a popular compression technique,
quantization constrains the number of distinct weight values and thus reducing
the number of bits required to represent and store each weight. In this paper,
we study the representation power of quantized neural networks. First, we prove
the universal approximability of quantized ReLU networks. Then we provide upper
bounds of storage size given the approximation error bound and the bit-width of
weights for function-independent and function-dependent structures. To the best
of the authors' knowledge, this is the first work on the universal
approximability as well as the associated storage size bound of quantized
neural networks.
</summary>
    <author>
      <name>Yukun Ding</name>
    </author>
    <author>
      <name>Jinglan Liu</name>
    </author>
    <author>
      <name>Yiyu Shi</name>
    </author>
    <link href="http://arxiv.org/abs/1802.03646v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03646v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08265v1</id>
    <updated>2016-08-29T22:02:39Z</updated>
    <published>2016-08-29T22:02:39Z</published>
    <title>About Learning in Recurrent Bistable Gradient Networks</title>
    <summary>  Recurrent Bistable Gradient Networks are attractor based neural networks
characterized by bistable dynamics of each single neuron. Coupled together
using linear interaction determined by the interconnection weights, these
networks do not suffer from spurious states or very limited capacity anymore.
Vladimir Chinarov and Michael Menzinger, who invented these networks, trained
them using Hebb's learning rule. We show, that this way of computing the
weights leads to unwanted behaviour and limitations of the networks
capabilities. Furthermore we evince, that using the first order of Hintons
Contrastive Divergence algorithm leads to a quite promising recurrent neural
network. These findings are tested by learning images of the MNIST database for
handwritten numbers.
</summary>
    <author>
      <name>J. Fischer</name>
    </author>
    <author>
      <name>S. Lackner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.08265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.01145v3</id>
    <updated>2017-05-01T14:01:32Z</updated>
    <published>2016-10-03T23:08:22Z</published>
    <title>Error bounds for approximations with deep ReLU networks</title>
    <summary>  We study expressive power of shallow and deep neural networks with piece-wise
linear activation functions. We establish new rigorous upper and lower bounds
for the network complexity in the setting of approximations in Sobolev spaces.
In particular, we prove that deep ReLU networks more efficiently approximate
smooth functions than shallow networks. In the case of approximations of 1D
Lipschitz functions we describe adaptive depth-6 network architectures more
efficient than the standard shallow architecture.
</summary>
    <author>
      <name>Dmitry Yarotsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages; major revision in v3; submitted to Neural Networks</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.01145v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.01145v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.05731v1</id>
    <updated>2018-01-17T16:17:28Z</updated>
    <published>2018-01-17T16:17:28Z</published>
    <title>In-network Neural Networks</title>
    <summary>  We present N2Net, a system that implements binary neural networks using
commodity switching chips deployed in network switches and routers. Our system
shows that these devices can run simple neural network models, whose input is
encoded in the network packets' header, at packet processing speeds (billions
of packets per second). Furthermore, our experience highlights that switching
chips could support even more complex models, provided that some minor and
cheap modifications to the chip's design are applied. We believe N2Net provides
an interesting building block for future end-to-end networked systems.
</summary>
    <author>
      <name>Giuseppe Siracusano</name>
    </author>
    <author>
      <name>Roberto Bifulco</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at SysML 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.05731v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.05731v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.01963v1</id>
    <updated>2017-11-06T15:36:32Z</updated>
    <published>2017-11-06T15:36:32Z</published>
    <title>Semi-Parallel Deep Neural Networks (SPDNN), Convergence and
  Generalization</title>
    <summary>  The Semi-Parallel Deep Neural Network (SPDNN) idea is explained in this
article and it has been shown that the convergence of the mixed network is very
close to the best network in the set and the generalization of SPDNN is better
than all the parent networks.
</summary>
    <author>
      <name>Shabab Bazrafkan</name>
    </author>
    <author>
      <name>Peter Corcoran</name>
    </author>
    <link href="http://arxiv.org/abs/1711.01963v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.01963v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0401633v2</id>
    <updated>2005-07-15T16:07:12Z</updated>
    <published>2004-01-30T12:53:47Z</published>
    <title>Self-organized annealing in laterally inhibited neural networks shows
  power law decay</title>
    <summary>  In this paper we present a method which assigns to each layer of a multilayer
neural network, whose network dynamics is governed by a noisy winner-take-all
mechanism, a neural temperature. This neural temperature is obtained by a least
mean square fit of the probability distribution of the noisy winner-take-all
mechanism to the distribution of a softmax mechanism, which has a well defined
temperature as free parameter. We call this approximated temperature resulting
from the optimization step the neural temperature. We apply this method to a
multilayer neural network during learning the XOR-problem with a Hebb-like
learning rule and show that after a transient the neural temperature decreases
in each layer according to a power law. This indicates a self-organized
annealing behavior induced by the learning rule itself instead of an external
adjustment of a control parameter as in physically motivated optimization
methods e.g. simulated annealing.
</summary>
    <author>
      <name>Frank Emmert-Streib</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0401633v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0401633v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0706.1051v1</id>
    <updated>2007-06-07T18:13:59Z</updated>
    <published>2007-06-07T18:13:59Z</published>
    <title>Improved Neural Modeling of Real-World Systems Using Genetic Algorithm
  Based Variable Selection</title>
    <summary>  Neural network models of real-world systems, such as industrial processes,
made from sensor data must often rely on incomplete data. System states may not
all be known, sensor data may be biased or noisy, and it is not often known
which sensor data may be useful for predictive modelling. Genetic algorithms
may be used to help to address this problem by determining the near optimal
subset of sensor variables most appropriate to produce good models. This paper
describes the use of genetic search to optimize variable selection to determine
inputs into the neural network model. We discuss genetic algorithm
implementation issues including data representation types and genetic operators
such as crossover and mutation. We present the use of this technique for neural
network modelling of a typical industrial application, a liquid fed ceramic
melter, and detail the results of the genetic search to optimize the neural
network model for this application.
</summary>
    <author>
      <name>Donald A. Sofge</name>
    </author>
    <author>
      <name>David L. Elliott</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">D. Sofge and D. Elliott, "Improved Neural Modeling of Real-World
  Systems Using Genetic Algorithm Based Variable Selection," In Int'l Conf. on
  Neural Networks and Brain (ICNN&amp;B'98-Beijing), 1998</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0706.1051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0706.1051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.06594v3</id>
    <updated>2015-09-28T10:15:35Z</updated>
    <published>2015-07-23T18:18:49Z</published>
    <title>Neural NILM: Deep Neural Networks Applied to Energy Disaggregation</title>
    <summary>  Energy disaggregation estimates appliance-by-appliance electricity
consumption from a single meter that measures the whole home's electricity
demand. Recently, deep neural networks have driven remarkable improvements in
classification performance in neighbouring machine learning fields such as
image classification and automatic speech recognition. In this paper, we adapt
three deep neural network architectures to energy disaggregation: 1) a form of
recurrent neural network called `long short-term memory' (LSTM); 2) denoising
autoencoders; and 3) a network which regresses the start time, end time and
average power demand of each appliance activation. We use seven metrics to test
the performance of these algorithms on real aggregate power data from five
appliances. Tests are performed against a house not seen during training and
against houses seen during training. We find that all three neural nets achieve
better F1 scores (averaged over all five appliances) than either combinatorial
optimisation or factorial hidden Markov models and that our neural net
algorithms generalise well to an unseen house.
</summary>
    <author>
      <name>Jack Kelly</name>
    </author>
    <author>
      <name>William Knottenbelt</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2821650.2821672</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2821650.2821672" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ACM BuildSys'15, November 4--5, 2015, Seoul</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.06594v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.06594v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9705270v1</id>
    <updated>1997-05-27T12:16:43Z</updated>
    <published>1997-05-27T12:16:43Z</published>
    <title>Neural Networks</title>
    <summary>  We review the theory of neural networks, as it has emerged in the last ten
years or so within the physics community, emphasizing questions of biological
relevance over those of importance in mathematical statistics and machine
learning theory.
</summary>
    <author>
      <name>Heinz Horner</name>
    </author>
    <author>
      <name>Reimer Kuehn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, LaTeX, 1 eps figure included, uses epsf and Springer's
  lamuphys style-file, review paper to appear in: "Intelligence and Artificial
  Intelligence" (provisional title), edited by U. Ratsch, M. Richter and I.O.
  Stamatescu (Springer, Heidelberg, 1997)</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9705270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9705270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0006010v1</id>
    <updated>2000-06-01T09:42:45Z</updated>
    <published>2000-06-01T09:42:45Z</published>
    <title>Statistical Mechanics of Recurrent Neural Networks I. Statics</title>
    <summary>  A lecture notes style review of the equilibrium statistical mechanics of
recurrent neural networks with discrete and continuous neurons (e.g. Ising,
coupled-oscillators). To be published in the Handbook of Biological Physics
(North-Holland). Accompanied by a similar review (part II) dealing with the
dynamics.
</summary>
    <author>
      <name>A. C. C. Coolen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">49 pages, LaTeX</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0006010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0006010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0006011v1</id>
    <updated>2000-06-01T09:43:46Z</updated>
    <published>2000-06-01T09:43:46Z</published>
    <title>Statistical Mechanics of Recurrent Neural Networks II. Dynamics</title>
    <summary>  A lecture notes style review of the non-equilibrium statistical mechanics of
recurrent neural networks with discrete and continuous neurons (e.g. Ising,
graded-response, coupled-oscillators). To be published in the Handbook of
Biological Physics (North-Holland). Accompanied by a similar review (part I)
dealing with the statics.
</summary>
    <author>
      <name>A. C. C. Coolen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">49 pages, LaTeX</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0006011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0006011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0010343v1</id>
    <updated>2000-10-23T13:01:14Z</updated>
    <published>2000-10-23T13:01:14Z</published>
    <title>Predicting and generating time series by neural networks: An
  investigation using statistical physics</title>
    <summary>  An overview is given about the statistical physics of neural networks
generating and analysing time series. Storage capacity, bit and sequence
generation, prediction error, antipredictable sequences, interacting
perceptrons and the application on the minority game are discussed. Finally, as
a demonstration a perceptron predicts bit sequences produced by human beings.
</summary>
    <author>
      <name>Wolfgang Kinzel</name>
    </author>
    <link href="http://arxiv.org/abs/cond-mat/0010343v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0010343v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0106242v1</id>
    <updated>2001-06-13T13:48:35Z</updated>
    <published>2001-06-13T13:48:35Z</published>
    <title>Local field dynamics in symmetric Q-Ising neural networks</title>
    <summary>  The time evolution of the local field in symmetric Q-Ising neural networks is
studied for arbitrary Q. In particular, the structure of the noise and the
appearance of gaps in the probability distribution are discussed. Results are
presented for several values of Q and compared with numerical simulations.
</summary>
    <author>
      <name>D. Bolle'</name>
    </author>
    <author>
      <name>G. M. Shim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages Latex, 6 eps figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0106242v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0106242v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0601129v1</id>
    <updated>2006-01-30T22:02:47Z</updated>
    <published>2006-01-30T22:02:47Z</published>
    <title>Instantaneously Trained Neural Networks</title>
    <summary>  This paper presents a review of instantaneously trained neural networks
(ITNNs). These networks trade learning time for size and, in the basic model, a
new hidden node is created for each training sample. Various versions of the
corner-classification family of ITNNs, which have found applications in
artificial intelligence (AI), are described. Implementation issues are also
considered.
</summary>
    <author>
      <name>Abhilash Ponnath</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0601129v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0601129v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/q-bio/0410004v1</id>
    <updated>2004-10-04T10:29:49Z</updated>
    <published>2004-10-04T10:29:49Z</published>
    <title>Can Neural Networks Recognize Parts?</title>
    <summary>  We have demonstrated neural networks can recognize parts by visual images.
Input signals are gray scale photographs of objects consisting of some parts
and output signals are their shapes. By training neural networks by a few set
of images, without any supervision they become to be able to recognize the
boundary between parts.
</summary>
    <author>
      <name>Koji Matsumura</name>
    </author>
    <author>
      <name>Y-h. Taguchi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to J. Phys. Soc. Jpn</arxiv:comment>
    <link href="http://arxiv.org/abs/q-bio/0410004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/q-bio/0410004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/quant-ph/0107012v2</id>
    <updated>2001-07-05T07:58:54Z</updated>
    <published>2001-07-03T13:16:38Z</published>
    <title>Quantum neural network</title>
    <summary>  It is suggested that a quantum neural network (QNN), a type of artificial
neural network, can be built using the principles of quantum information
processing. The input and output qubits in the QNN can be implemented by
optical modes with different polarization, the weights of the QNN can be
implemented by optical beam splitters and phase shifters
</summary>
    <author>
      <name>M. V. Altaisky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LaTeX, 4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/quant-ph/0107012v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/quant-ph/0107012v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/quant-ph/0305072v1</id>
    <updated>2003-05-13T12:28:52Z</updated>
    <published>2003-05-13T12:28:52Z</published>
    <title>A neural-network-like quantum information processing system</title>
    <summary>  The Hopfield neural networks and the holographic neural networks are models
which were successfully simulated on conventional computers. Starting with
these models, an analogous fundamental quantum information processing system is
developed in this article. Neuro-quantum interaction can regulate the
"collapse"-readout of quantum computation results. This paper is a
comprehensive introduction into associative processing and memory-storage in
quantum-physical framework.
</summary>
    <author>
      <name>Mitja Perus</name>
    </author>
    <author>
      <name>Horst Bischof</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, an introductory summary of essentials</arxiv:comment>
    <link href="http://arxiv.org/abs/quant-ph/0305072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/quant-ph/0305072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.1198v1</id>
    <updated>2008-12-05T18:18:02Z</updated>
    <published>2008-12-05T18:18:02Z</published>
    <title>A chilean seismic regionalization through a Kohonen neural network</title>
    <summary>  A study of seismic regionalization for central Chile based on a neural
network is presented. A scenario with six seismic regions is obtained,
independently of the size of the neighborhood or the reach of the correlation
between the cells of the grid. The high correlation between the spatial
distribution of the seismic zones and geographical data confirm our election of
the training vectors of the neural network.
</summary>
    <author>
      <name>Jorge Reyes</name>
    </author>
    <author>
      <name>Victor H. Cardenas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0812.1198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.1198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.geo-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.geo-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.1931v1</id>
    <updated>2012-07-09T01:26:53Z</updated>
    <published>2012-07-09T01:26:53Z</published>
    <title>An Augmented Smoothing Method of L1 -norm Minimization and Its
  Implementation by Neural Network Model</title>
    <summary>  In this paper we propose an augmented smoothing function for nonlinear L1
-norm minimization problem and consider a global stability of a gradient-based
neural network model to minimize the smoothing function. The numerical
simulations show that our smoothing neural network finds successfully the
global solution of the L1 -norm minimization problems considered in the
simulation.
</summary>
    <author>
      <name>Yunchol Jong</name>
    </author>
    <link href="http://arxiv.org/abs/1207.1931v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.1931v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90C30, 90C59, 65K10, 92B20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.6290v1</id>
    <updated>2013-07-24T03:48:25Z</updated>
    <published>2013-07-24T03:48:25Z</published>
    <title>Neural Network Model of Pricing Health Care Insurance</title>
    <summary>  To pricing health insurance plan, statisticians use mathematical models to
predict customers' future health condition. General Addictive Model (GAM) is a
wide accepted method for this problem. However, it have several limitations. To
solve this problem, a new method named neural network model is implemented.
Compare with GAM model, neural network provide a more accurate predicting
result.
</summary>
    <author>
      <name>Guanxi Zhuang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.6290v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.6290v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.4009v1</id>
    <updated>2012-08-20T13:54:26Z</updated>
    <published>2012-08-20T13:54:26Z</published>
    <title>Learning sparse messages in networks of neural cliques</title>
    <summary>  An extension to a recently introduced binary neural network is proposed in
order to allow the learning of sparse messages, in large numbers and with high
memory efficiency. This new network is justified both in biological and
informational terms. The learning and retrieval rules are detailed and
illustrated by various simulation results.
</summary>
    <author>
      <name>Behrooz Kamary Aliabadi</name>
    </author>
    <author>
      <name>Claude Berrou</name>
    </author>
    <author>
      <name>Vincent Gripon</name>
    </author>
    <author>
      <name>Xiaoran Jiang</name>
    </author>
    <link href="http://arxiv.org/abs/1208.4009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.4009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.6082v1</id>
    <updated>2012-10-22T22:59:58Z</updated>
    <published>2012-10-22T22:59:58Z</published>
    <title>Interplay: Dispersed Activation in Neural Networks</title>
    <summary>  This paper presents a multi-point stimulation of a Hebbian neural network
with investigation of the interplay between the stimulus waves through the
neurons of the network. Equilibrium of the resulting memory is achieved for
recall of specific memory data at a rate faster than single point stimulus. The
interplay of the intersecting stimuli appears to parallel the clarification
process of recall in biological systems.
</summary>
    <author>
      <name>Richard L. Churchill</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.6082v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.6082v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.3191v2</id>
    <updated>2014-12-14T03:18:33Z</updated>
    <published>2014-12-10T04:06:38Z</published>
    <title>Bach in 2014: Music Composition with Recurrent Neural Network</title>
    <summary>  We propose a framework for computer music composition that uses resilient
propagation (RProp) and long short term memory (LSTM) recurrent neural network.
In this paper, we show that LSTM network learns the structure and
characteristics of music pieces properly by demonstrating its ability to
recreate music. We also show that predicting existing music using RProp
outperforms Back propagation through time (BPTT).
</summary>
    <author>
      <name>I-Ting Liu</name>
    </author>
    <author>
      <name>Bhiksha Ramakrishnan</name>
    </author>
    <link href="http://arxiv.org/abs/1412.3191v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3191v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.04110v1</id>
    <updated>2016-07-14T12:45:07Z</updated>
    <published>2016-07-14T12:45:07Z</published>
    <title>Using Recurrent Neural Network for Learning Expressive Ontologies</title>
    <summary>  Recently, Neural Networks have been proven extremely effective in many
natural language processing tasks such as sentiment analysis, question
answering, or machine translation. Aiming to exploit such advantages in the
Ontology Learning process, in this technical report we present a detailed
description of a Recurrent Neural Network based system to be used to pursue
such goal.
</summary>
    <author>
      <name>Giulio Petrucci</name>
    </author>
    <author>
      <name>Chiara Ghidini</name>
    </author>
    <author>
      <name>Marco Rospocher</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.04110v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.04110v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.04968v1</id>
    <updated>2017-01-18T06:49:03Z</updated>
    <published>2017-01-18T06:49:03Z</published>
    <title>Multilayer Perceptron Algebra</title>
    <summary>  Artificial Neural Networks(ANN) has been phenomenally successful on various
pattern recognition tasks. However, the design of neural networks rely heavily
on the experience and intuitions of individual developers. In this article, the
author introduces a mathematical structure called MLP algebra on the set of all
Multilayer Perceptron Neural Networks(MLP), which can serve as a guiding
principle to build MLPs accommodating to the particular data sets, and to build
complex MLPs from simpler ones.
</summary>
    <author>
      <name>Zhao Peng</name>
    </author>
    <link href="http://arxiv.org/abs/1701.04968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.04968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.06749v2</id>
    <updated>2017-04-22T13:03:20Z</updated>
    <published>2017-03-20T14:02:11Z</published>
    <title>Efficient variational Bayesian neural network ensembles for outlier
  detection</title>
    <summary>  In this work we perform outlier detection using ensembles of neural networks
obtained by variational approximation of the posterior in a Bayesian neural
network setting. The variational parameters are obtained by sampling from the
true posterior by gradient descent. We show our outlier detection results are
comparable to those obtained using other efficient ensembling methods.
</summary>
    <author>
      <name>Nick Pawlowski</name>
    </author>
    <author>
      <name>Miguel Jaques</name>
    </author>
    <author>
      <name>Ben Glocker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at Workshop track - ICLR 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.06749v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.06749v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.08531v1</id>
    <updated>2017-04-27T12:27:25Z</updated>
    <published>2017-04-27T12:27:25Z</published>
    <title>A Survey of Neural Network Techniques for Feature Extraction from Text</title>
    <summary>  This paper aims to catalyze the discussions about text feature extraction
techniques using neural network architectures. The research questions discussed
in the paper focus on the state-of-the-art neural network techniques that have
proven to be useful tools for language processing, language generation, text
classification and other computational linguistics tasks.
</summary>
    <author>
      <name>Vineet John</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.08531v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.08531v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07101v1</id>
    <updated>2017-06-21T19:51:43Z</updated>
    <published>2017-06-21T19:51:43Z</published>
    <title>The energy landscape of a simple neural network</title>
    <summary>  We explore the energy landscape of a simple neural network. In particular, we
expand upon previous work demonstrating that the empirical complexity of fitted
neural networks is vastly less than a naive parameter count would suggest and
that this implicit regularization is actually beneficial for generalization
from fitted models.
</summary>
    <author>
      <name>Anthony Collins Gamst</name>
    </author>
    <author>
      <name>Alden Walker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.07101v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07101v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.05969v7</id>
    <updated>2018-01-16T10:02:55Z</updated>
    <published>2017-12-16T14:55:13Z</published>
    <title>Learning a Virtual Codec Based on Deep Convolutional Neural Network to
  Compress Image</title>
    <summary>  Although deep convolutional neural network has been proved to efficiently
eliminate coding artifacts caused by the coarse quantization of traditional
codec, it's difficult to train any neural network in front of the encoder for
gradient's back-propagation. In this paper, we propose an end-to-end image
compression framework based on convolutional neural network to resolve the
problem of non-differentiability of the quantization function in the standard
codec. First, the feature description neural network is used to get a valid
description in the low-dimension space with respect to the ground-truth image
so that the amount of image data is greatly reduced for storage or
transmission. After image's valid description, standard image codec such as
JPEG is leveraged to further compress image, which leads to image's great
distortion and compression artifacts, especially blocking artifacts, detail
missing, blurring, and ringing artifacts. Then, we use a post-processing neural
network to remove these artifacts. Due to the challenge of directly learning a
non-linear function for a standard codec based on convolutional neural network,
we propose to learn a virtual codec neural network to approximate the
projection from the valid description image to the post-processed compressed
image, so that the gradient could be efficiently back-propagated from the
post-processing neural network to the feature description neural network during
training. Meanwhile, an advanced learning algorithm is proposed to train our
deep neural networks for compression. Obviously, the priority of the proposed
method is compatible with standard existing codecs and our learning strategy
can be easily extended into these codecs based on convolutional neural network.
Experimental results have demonstrated the advances of the proposed method as
compared to several state-of-the-art approaches, especially at very low
bit-rate.
</summary>
    <author>
      <name>Lijun Zhao</name>
    </author>
    <author>
      <name>Huihui Bai</name>
    </author>
    <author>
      <name>Anhong Wang</name>
    </author>
    <author>
      <name>Yao Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.05969v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.05969v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0510169v1</id>
    <updated>2005-10-07T05:44:37Z</updated>
    <published>2005-10-07T05:44:37Z</published>
    <title>Theory of Recurrent Neural Network with Common Synaptic Inputs</title>
    <summary>  We discuss the effects of common synaptic inputs in a recurrent neural
network. Because of the effects of these common synaptic inputs, the
correlation between neural inputs cannot be ignored, and thus the network
exhibits sample dependence. Networks of this type do not have well-defined
thermodynamic limits, and self-averaging breaks down. We therefore need to
develop a suitable theory without relying on these common properties. While the
effects of the common synaptic inputs have been analyzed in layered neural
networks, it was apparently difficult to analyze these effects in recurrent
neural networks due to feedback connections. We investigated a sequential
associative memory model as an example of recurrent networks and succeeded in
deriving a macroscopic dynamical description as a recurrence relation form of a
probability density function.
</summary>
    <author>
      <name>Masaki Kawamura</name>
    </author>
    <author>
      <name>Michiko Yamana</name>
    </author>
    <author>
      <name>Masato Okada</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1143/JPSJ.74.2961</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1143/JPSJ.74.2961" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of the Physical Society of Japan, vol.74, no.11, Nov.
  2005, pp.2961-2965</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0510169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0510169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504064v1</id>
    <updated>2005-04-14T10:27:55Z</updated>
    <published>2005-04-14T10:27:55Z</published>
    <title>Neural-Network Techniques for Visual Mining Clinical
  Electroencephalograms</title>
    <summary>  In this chapter we describe new neural-network techniques developed for
visual mining clinical electroencephalograms (EEGs), the weak electrical
potentials invoked by brain activity. These techniques exploit fruitful ideas
of Group Method of Data Handling (GMDH). Section 2 briefly describes the
standard neural-network techniques which are able to learn well-suited
classification modes from data presented by relevant features. Section 3
introduces an evolving cascade neural network technique which adds new input
nodes as well as new neurons to the network while the training error decreases.
This algorithm is applied to recognize artifacts in the clinical EEGs. Section
4 presents the GMDH-type polynomial networks learnt from data. We applied this
technique to distinguish the EEGs recorded from an Alzheimer and a healthy
patient as well as recognize EEG artifacts. Section 5 describes the new
neural-network technique developed to induce multi-class concepts from data. We
used this technique for inducing a 16-class concept from the large-scale
clinical EEG data. Finally we discuss perspectives of applying the
neural-network techniques to clinical EEGs.
</summary>
    <author>
      <name>Vitaly Schetinin</name>
    </author>
    <author>
      <name>Joachim Schult</name>
    </author>
    <author>
      <name>Anatoly Brazhnikov</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0504064v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504064v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.5317v1</id>
    <updated>2012-06-22T20:36:56Z</updated>
    <published>2012-06-22T20:36:56Z</published>
    <title>Eigenvalue spectra of asymmetric random matrices for multi-component
  neural networks</title>
    <summary>  This paper focuses on large neural networks whose synaptic connectivity
matrices are randomly chosen from certain random matrix ensembles. The dynamics
of these networks can be characterized by the eigenvalue spectra of their
connectivity matrices. In reality, neurons in a network do not necessarily
behave in a similar way, but may belong to several different categories. The
first study of the spectra of two-component neural networks was carried out by
Rajan and Abbott. In their model, neurons are either 'excitatory' or
'inhibitory', and strengths of synapses from different types of neurons have
Gaussian distributions with different means and variances. A surprising finding
by Rajan and Abbott is that the eigenvalue spectra of these types of random
synaptic matrices do not depend on the mean values of their elements. In this
paper we prove that this is true even for a much more general type of random
neural network, where there is a finite number of types of neurons, and their
synaptic strengths have correlated distributions. Furthermore, using the
diagrammatic techniques, we calculate the explicit formula for the spectra of
synaptic matrices of multi-component neural networks.
</summary>
    <author>
      <name>Yi Wei</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.85.066116</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.85.066116" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 85, 066116 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1206.5317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.5317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.2173v1</id>
    <updated>2014-08-07T08:30:34Z</updated>
    <published>2014-08-07T08:30:34Z</published>
    <title>Face Detection Using Radial Basis Functions Neural Networks With Fixed
  Spread</title>
    <summary>  This paper presented a face detection system using Radial Basis Function
Neural Networks With Fixed Spread Value. Face detection is the first step in
face recognition system. The purpose is to localize and extract the face region
from the background that will be fed into the face recognition system for
identification. General preprocessing approach was used for normalizing the
image and Radial Basis Function (RBF) Neural Network was used to distinguish
between face and non-face. RBF Neural Networks offer several advantages
compared to other neural network architecture such as they can be trained using
fast two stages training algorithm and the network possesses the property of
best approximation. The output of the network can be optimized by setting
suitable value of center and spread of the RBF. In this paper, fixed spread
value will be used. The Radial Basis Function Neural Network (RBFNN) used to
distinguish faces and non-faces and the evaluation of the system will be the
performance of detection, False Acceptance Rate (FAR), False Rejection Rate
(FRR) and the discriminative properties.
</summary>
    <author>
      <name>K. A. A. Aziz</name>
    </author>
    <author>
      <name>S. S. Abdullah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, The Second International Conference on Control,
  Instrumentation and Mechatronic Engineering (CIM09) Malacca, Malaysia, June
  2-3, 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.2173v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.2173v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.00393v3</id>
    <updated>2015-07-23T17:11:04Z</updated>
    <published>2015-05-03T04:58:53Z</published>
    <title>ReNet: A Recurrent Neural Network Based Alternative to Convolutional
  Networks</title>
    <summary>  In this paper, we propose a deep neural network architecture for object
recognition based on recurrent neural networks. The proposed network, called
ReNet, replaces the ubiquitous convolution+pooling layer of the deep
convolutional neural network with four recurrent neural networks that sweep
horizontally and vertically in both directions across the image. We evaluate
the proposed ReNet on three widely-used benchmark datasets; MNIST, CIFAR-10 and
SVHN. The result suggests that ReNet is a viable alternative to the deep
convolutional neural network, and that further investigation is needed.
</summary>
    <author>
      <name>Francesco Visin</name>
    </author>
    <author>
      <name>Kyle Kastner</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <author>
      <name>Matteo Matteucci</name>
    </author>
    <author>
      <name>Aaron Courville</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <link href="http://arxiv.org/abs/1505.00393v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.00393v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.06342v1</id>
    <updated>2016-11-19T11:21:25Z</updated>
    <published>2016-11-19T11:21:25Z</published>
    <title>Quantized neural network design under weight capacity constraint</title>
    <summary>  The complexity of deep neural network algorithms for hardware implementation
can be lowered either by scaling the number of units or reducing the
word-length of weights. Both approaches, however, can accompany the performance
degradation although many types of research are conducted to relieve this
problem. Thus, it is an important question which one, between the network size
scaling and the weight quantization, is more effective for hardware
optimization. For this study, the performances of fully-connected deep neural
networks (FCDNNs) and convolutional neural networks (CNNs) are evaluated while
changing the network complexity and the word-length of weights. Based on these
experiments, we present the effective compression ratio (ECR) to guide the
trade-off between the network size and the precision of weights when the
hardware resource is limited.
</summary>
    <author>
      <name>Sungho Shin</name>
    </author>
    <author>
      <name>Kyuyeon Hwang</name>
    </author>
    <author>
      <name>Wonyong Sung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is accepted at NIPS 2016 workshop on Efficient Methods for
  Deep Neural Networks (EMDNN). arXiv admin note: text overlap with
  arXiv:1511.06488</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.06342v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.06342v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.01135v2</id>
    <updated>2017-05-19T04:50:29Z</updated>
    <published>2017-02-03T19:26:01Z</published>
    <title>Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks</title>
    <summary>  Deep neural networks have emerged as a widely used and effective means for
tackling complex, real-world problems. However, a major obstacle in applying
them to safety-critical systems is the great difficulty in providing formal
guarantees about their behavior. We present a novel, scalable, and efficient
technique for verifying properties of deep neural networks (or providing
counter-examples). The technique is based on the simplex method, extended to
handle the non-convex Rectified Linear Unit (ReLU) activation function, which
is a crucial ingredient in many modern neural networks. The verification
procedure tackles neural networks as a whole, without making any simplifying
assumptions. We evaluated our technique on a prototype deep neural network
implementation of the next-generation airborne collision avoidance system for
unmanned aircraft (ACAS Xu). Results show that our technique can successfully
prove properties of networks that are an order of magnitude larger than the
largest networks verified using existing methods.
</summary>
    <author>
      <name>Guy Katz</name>
    </author>
    <author>
      <name>Clark Barrett</name>
    </author>
    <author>
      <name>David Dill</name>
    </author>
    <author>
      <name>Kyle Julian</name>
    </author>
    <author>
      <name>Mykel Kochenderfer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the extended version of a paper with the same title that
  appeared at CAV 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.01135v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.01135v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0703405v3</id>
    <updated>2008-05-11T02:15:16Z</updated>
    <published>2007-03-15T10:42:09Z</published>
    <title>Topology and Dynamics of Attractor Neural Networks: The Role of
  Loopiness</title>
    <summary>  We derive an exact representation of the topological effect on the dynamics
of sequence processing neural networks within signal-to-noise analysis. A new
network structure parameter, loopiness coefficient, is introduced to
quantitatively study the loop effect on network dynamics. The large loopiness
coefficient means the large probability of finding loops in the networks. We
develop the recursive equations for the overlap parameters of neural networks
in the term of the loopiness. It was found that the large loopiness increases
the correlations among the network states at different times, and eventually it
reduces the performance of neural networks. The theory is applied to several
network topological structures, including fully-connected, densely-connected
random, densely-connected regular, and densely-connected small-world, where
encouraging results are obtained.
</summary>
    <author>
      <name>Pan Zhang</name>
    </author>
    <author>
      <name>Yong Chen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physa.2008.02.073</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physa.2008.02.073" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures, comments are favored</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physica A 387, (2008) 4411-4416</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0703405v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0703405v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.08512v1</id>
    <updated>2016-05-27T06:02:48Z</updated>
    <published>2016-05-27T06:02:48Z</published>
    <title>SNN: Stacked Neural Networks</title>
    <summary>  It has been proven that transfer learning provides an easy way to achieve
state-of-the-art accuracies on several vision tasks by training a simple
classifier on top of features obtained from pre-trained neural networks. The
goal of this work is to generate better features for transfer learning from
multiple publicly available pre-trained neural networks. To this end, we
propose a novel architecture called Stacked Neural Networks which leverages the
fast training time of transfer learning while simultaneously being much more
accurate. We show that using a stacked NN architecture can result in up to 8%
improvements in accuracy over state-of-the-art techniques using only one
pre-trained network for transfer learning. A second aim of this work is to make
network fine- tuning retain the generalizability of the base network to unseen
tasks. To this end, we propose a new technique called "joint fine-tuning" that
is able to give accuracies comparable to finetuning the same network
individually over two datasets. We also show that a jointly finetuned network
generalizes better to unseen tasks when compared to a network finetuned over a
single task.
</summary>
    <author>
      <name>Milad Mohammadi</name>
    </author>
    <author>
      <name>Subhasis Das</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.08512v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08512v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.07553v1</id>
    <updated>2017-11-20T21:28:40Z</updated>
    <published>2017-11-20T21:28:40Z</published>
    <title>Residual Gated Graph ConvNets</title>
    <summary>  Graph-structured data such as functional brain networks, social networks,
gene regulatory networks, communications networks have brought the interest in
generalizing neural networks to graph domains. In this paper, we are interested
to de- sign efficient neural network architectures for graphs with variable
length. Several existing works such as Scarselli et al. (2009); Li et al.
(2016) have focused on recurrent neural networks (RNNs) to solve this task. A
recent different approach was proposed in Sukhbaatar et al. (2016), where a
vanilla graph convolutional neural network (ConvNets) was introduced. We
believe the latter approach to be a better paradigm to solve graph learning
problems because ConvNets are more pruned to deep networks than RNNs. For this
reason, we propose the most generic class of residual multi-layer graph
ConvNets that make use of an edge gating mechanism, as proposed in Marcheggiani
&amp; Titov (2017). Gated edges appear to be a natural property in the context of
graph learning tasks, as the system has the ability to learn which edges are
important or not for the task to solve. We apply several graph neural models to
two basic network science tasks; subgraph matching and semi-supervised
clustering for graphs with variable length. Numerical results show the
performances of the new model.
</summary>
    <author>
      <name>Xavier Bresson</name>
    </author>
    <author>
      <name>Thomas Laurent</name>
    </author>
    <link href="http://arxiv.org/abs/1711.07553v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.07553v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/q-bio/0607034v1</id>
    <updated>2006-07-21T17:05:25Z</updated>
    <published>2006-07-21T17:05:25Z</published>
    <title>Nonoptimal Component Placement, but Short Processing Paths, due to
  Long-Distance Projections in Neural Systems</title>
    <summary>  It has been suggested that neural systems across several scales of
organization show optimal component placement, in which any spatial
rearrangement of the components would lead to an increase of total wiring.
Using extensive connectivity datasets for diverse neural networks combined with
spatial coordinates for network nodes, we applied an optimization algorithm to
the network layouts, in order to search for wire-saving component
rearrangements. We found that optimized component rearrangements could
substantially reduce total wiring length in all tested neural networks.
Specifically, total wiring among 95 primate (Macaque) cortical areas could be
decreased by 32%, and wiring of neuronal networks in the nematode
Caenorhabditis elegans could be reduced by 48% on the global level, and by 49%
for neurons within frontal ganglia. Wiring length reductions were possible due
to the existence of long-distance projections in neural networks. We explored
the role of these projections by comparing the original networks with minimally
rewired networks of the same size, which possessed only the shortest possible
connections. In the minimally rewired networks, the number of processing steps
along the shortest paths between components was significantly increased
compared to the original networks. Additional benchmark comparisons also
indicated that neural networks are more similar to network layouts that
minimize the length of processing paths, rather than wiring length. These
findings suggest that neural systems are not exclusively optimized for minimal
global wiring, but for a variety of factors including the minimization of
processing steps.
</summary>
    <author>
      <name>Marcus Kaiser</name>
    </author>
    <author>
      <name>Claus C. Hilgetag</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pcbi.0020095</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pcbi.0020095" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">PLoS Comput Biol 2(7): e95 (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/q-bio/0607034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/q-bio/0607034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.05463v4</id>
    <updated>2015-11-10T20:30:05Z</updated>
    <published>2015-08-22T03:36:43Z</published>
    <title>StochasticNet: Forming Deep Neural Networks via Stochastic Connectivity</title>
    <summary>  Deep neural networks is a branch in machine learning that has seen a meteoric
rise in popularity due to its powerful abilities to represent and model
high-level abstractions in highly complex data. One area in deep neural
networks that is ripe for exploration is neural connectivity formation. A
pivotal study on the brain tissue of rats found that synaptic formation for
specific functional connectivity in neocortical neural microcircuits can be
surprisingly well modeled and predicted as a random formation. Motivated by
this intriguing finding, we introduce the concept of StochasticNet, where deep
neural networks are formed via stochastic connectivity between neurons. As a
result, any type of deep neural networks can be formed as a StochasticNet by
allowing the neuron connectivity to be stochastic. Stochastic synaptic
formations, in a deep neural network architecture, can allow for efficient
utilization of neurons for performing specific tasks. To evaluate the
feasibility of such a deep neural network architecture, we train a
StochasticNet using four different image datasets (CIFAR-10, MNIST, SVHN, and
STL-10). Experimental results show that a StochasticNet, using less than half
the number of neural connections as a conventional deep neural network,
achieves comparable accuracy and reduces overfitting on the CIFAR-10, MNIST and
SVHN dataset. Interestingly, StochasticNet with less than half the number of
neural connections, achieved a higher accuracy (relative improvement in test
error rate of ~6% compared to ConvNet) on the STL-10 dataset than a
conventional deep neural network. Finally, StochasticNets have faster
operational speeds while achieving better or similar accuracy performances.
</summary>
    <author>
      <name>Mohammad Javad Shafiee</name>
    </author>
    <author>
      <name>Parthipan Siva</name>
    </author>
    <author>
      <name>Alexander Wong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.05463v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.05463v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.04082v2</id>
    <updated>2017-04-20T17:54:13Z</updated>
    <published>2017-01-15T17:32:02Z</published>
    <title>Embedding Watermarks into Deep Neural Networks</title>
    <summary>  Deep neural networks have recently achieved significant progress. Sharing
trained models of these deep neural networks is very important in the rapid
progress of researching or developing deep neural network systems. At the same
time, it is necessary to protect the rights of shared trained models. To this
end, we propose to use a digital watermarking technology to protect
intellectual property or detect intellectual property infringement of trained
models. Firstly, we formulate a new problem: embedding watermarks into deep
neural networks. We also define requirements, embedding situations, and attack
types for watermarking to deep neural networks. Secondly, we propose a general
framework to embed a watermark into model parameters using a parameter
regularizer. Our approach does not hurt the performance of networks into which
a watermark is embedded. Finally, we perform comprehensive experiments to
reveal the potential of watermarking to deep neural networks as a basis of this
new problem. We show that our framework can embed a watermark in the situations
of training a network from scratch, fine-tuning, and distilling without hurting
the performance of a deep neural network. The embedded watermark does not
disappear even after fine-tuning or parameter pruning; the watermark completely
remains even after removing 65% of parameters were pruned. The implementation
of this research is: https://github.com/yu4u/dnn-watermark
</summary>
    <author>
      <name>Yusuke Uchida</name>
    </author>
    <author>
      <name>Yuki Nagai</name>
    </author>
    <author>
      <name>Shigeyuki Sakazawa</name>
    </author>
    <author>
      <name>Shin'ichi Satoh</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3078971.3078974</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3078971.3078974" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICMR '17 Proceedings of the 2017 ACM on International Conference
  on Multimedia Retrieval Pages 269-277</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1701.04082v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.04082v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.02617v1</id>
    <updated>2018-02-07T19:58:50Z</updated>
    <published>2018-02-07T19:58:50Z</published>
    <title>Recognition of Acoustic Events Using Masked Conditional Neural Networks</title>
    <summary>  Automatic feature extraction using neural networks has accomplished
remarkable success for images, but for sound recognition, these models are
usually modified to fit the nature of the multi-dimensional temporal
representation of the audio signal in spectrograms. This may not efficiently
harness the time-frequency representation of the signal. The ConditionaL Neural
Network (CLNN) takes into consideration the interrelation between the temporal
frames, and the Masked ConditionaL Neural Network (MCLNN) extends upon the CLNN
by forcing a systematic sparseness over the network's weights using a binary
mask. The masking allows the network to learn about frequency bands rather than
bins, mimicking a filterbank used in signal transformations such as MFCC.
Additionally, the Mask is designed to consider various combinations of
features, which automates the feature hand-crafting process. We applied the
MCLNN for the Environmental Sound Recognition problem using the Urbansound8k,
YorNoise, ESC-10 and ESC-50 datasets. The MCLNN have achieved competitive
performance compared to state-of-the-art Convolutional Neural Networks and
hand-crafted attempts.
</summary>
    <author>
      <name>Fady Medhat</name>
    </author>
    <author>
      <name>David Chesmore</name>
    </author>
    <author>
      <name>John Robinson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICMLA.2017.0-158</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICMLA.2017.0-158" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Restricted Boltzmann Machine, RBM, Conditional Restricted Boltzmann
  Machine, CRBM, Conditional Neural Networks, CLNN, Masked Conditional Neural
  Networks, MCLNN, Deep Neural Network, Environmental Sound Recognition, ESR</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Machine Learning and Applications
  (ICMLA) Year: 2017 Pages: 199 - 206</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1802.02617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.02617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.01509v1</id>
    <updated>2017-05-03T17:08:05Z</updated>
    <published>2017-05-03T17:08:05Z</published>
    <title>Neural Models for Information Retrieval</title>
    <summary>  Neural ranking models for information retrieval (IR) use shallow or deep
neural networks to rank search results in response to a query. Traditional
learning to rank models employ machine learning techniques over hand-crafted IR
features. By contrast, neural models learn representations of language from raw
text that can bridge the gap between query and document vocabulary. Unlike
classical IR models, these new machine learning based approaches are
data-hungry, requiring large scale training data before they can be deployed.
This tutorial introduces basic concepts and intuitions behind neural IR models,
and places them in the context of traditional retrieval models. We begin by
introducing fundamental concepts of IR and different neural and non-neural
approaches to learning vector representations of text. We then review shallow
neural IR methods that employ pre-trained neural term embeddings without
learning the IR task end-to-end. We introduce deep neural networks next,
discussing popular deep architectures. Finally, we review the current DNN
models for information retrieval. We conclude with a discussion on potential
future directions for neural IR.
</summary>
    <author>
      <name>Bhaskar Mitra</name>
    </author>
    <author>
      <name>Nick Craswell</name>
    </author>
    <link href="http://arxiv.org/abs/1705.01509v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01509v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.4572v1</id>
    <updated>2010-09-23T10:44:24Z</updated>
    <published>2010-09-23T10:44:24Z</published>
    <title>Medical diagnosis using neural network</title>
    <summary>  This research is to search for alternatives to the resolution of complex
medical diagnosis where human knowledge should be apprehended in a general
fashion. Successful application examples show that human diagnostic
capabilities are significantly worse than the neural diagnostic system. This
paper describes a modified feedforward neural network constructive algorithm
(MFNNCA), a new algorithm for medical diagnosis. The new constructive algorithm
with backpropagation; offer an approach for the incremental construction of
near-minimal neural network architectures for pattern classification. The
algorithm starts with minimal number of hidden units in the single hidden
layer; additional units are added to the hidden layer one at a time to improve
the accuracy of the network and to get an optimal size of a neural network. The
MFNNCA was tested on several benchmarking classification problems including the
cancer, heart disease and diabetes. Experimental results show that the MFNNCA
can produce optimal neural network architecture with good generalization
ability.
</summary>
    <author>
      <name>S. M. Kamruzzaman</name>
    </author>
    <author>
      <name>Ahmed Ryadh Hasan</name>
    </author>
    <author>
      <name>Abu Bakar Siddiquee</name>
    </author>
    <author>
      <name>Md. Ehsanul Hoque Mazumder</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, International Conference</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 3rd International Conference on Electrical &amp; Computer
  Engineering (ICECE 2004), Dhaka Bangladesh, pp. 537-540, Dec. 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.4572v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.4572v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.01847v1</id>
    <updated>2015-03-06T04:48:00Z</updated>
    <published>2015-03-06T04:48:00Z</published>
    <title>Estimation of the parameters of an infectious disease model using neural
  networks</title>
    <summary>  In this paper, we propose a realistic mathematical model taking into account
the mutual interference among the interacting populations. This model attempts
to describe the control (vaccination) function as a function of the number of
infective individuals, which is an improvement over the existing susceptible
infective epidemic models. Regarding the growth of the epidemic as a nonlinear
phenomenon we have developed a neural network architecture to estimate the
vital parameters associated with this model. This architecture is based on a
recently developed new class of neural networks known as co-operative and
supportive neural networks. The application of this architecture to the present
study involves preprocessing of the input data, and this renders an efficient
estimation of the rate of spread of the epidemic. It is observed that the
proposed new neural network outperforms a simple feed-forward neural network
and polynomial regression.
</summary>
    <author>
      <name>V. Sree Hari Rao</name>
    </author>
    <author>
      <name>M. Naresh Kumar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.nonrwa.2009.04.006</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.nonrwa.2009.04.006" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 11 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Nonlinear Analysis: Real World Applications 11(2010) 1810-1818</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1503.01847v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.01847v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.00709v1</id>
    <updated>2016-01-29T13:53:42Z</updated>
    <published>2016-01-29T13:53:42Z</published>
    <title>Quantum perceptron over a field and neural network architecture
  selection in a quantum computer</title>
    <summary>  In this work, we propose a quantum neural network named quantum perceptron
over a field (QPF). Quantum computers are not yet a reality and the models and
algorithms proposed in this work cannot be simulated in actual (or classical)
computers. QPF is a direct generalization of a classical perceptron and solves
some drawbacks found in previous models of quantum perceptrons. We also present
a learning algorithm named Superposition based Architecture Learning algorithm
(SAL) that optimizes the neural network weights and architectures. SAL searches
for the best architecture in a finite set of neural network architectures with
linear time over the number of patterns in the training set. SAL is the first
learning algorithm to determine neural network architectures in polynomial
time. This speedup is obtained by the use of quantum parallelism and a
non-linear quantum operator.
</summary>
    <author>
      <name>Adenilton J. da Silva</name>
    </author>
    <author>
      <name>Teresa B. Ludermir</name>
    </author>
    <author>
      <name>Wilson R. de Oliveira</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neunet.2016.01.002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neunet.2016.01.002" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Networks, Volume 76, April 2016, Pages 55-64</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.00709v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.00709v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08275v1</id>
    <updated>2016-04-28T00:35:32Z</updated>
    <published>2016-04-28T00:35:32Z</published>
    <title>Crafting Adversarial Input Sequences for Recurrent Neural Networks</title>
    <summary>  Machine learning models are frequently used to solve complex security
problems, as well as to make decisions in sensitive situations like guiding
autonomous vehicles or predicting financial market behaviors. Previous efforts
have shown that numerous machine learning models were vulnerable to adversarial
manipulations of their inputs taking the form of adversarial samples. Such
inputs are crafted by adding carefully selected perturbations to legitimate
inputs so as to force the machine learning model to misbehave, for instance by
outputting a wrong class if the machine learning task of interest is
classification. In fact, to the best of our knowledge, all previous work on
adversarial samples crafting for neural network considered models used to solve
classification tasks, most frequently in computer vision applications. In this
paper, we contribute to the field of adversarial machine learning by
investigating adversarial input sequences for recurrent neural networks
processing sequential data. We show that the classes of algorithms introduced
previously to craft adversarial samples misclassified by feed-forward neural
networks can be adapted to recurrent neural networks. In a experiment, we show
that adversaries can craft adversarial sequences misleading both categorical
and sequential recurrent neural networks.
</summary>
    <author>
      <name>Nicolas Papernot</name>
    </author>
    <author>
      <name>Patrick McDaniel</name>
    </author>
    <author>
      <name>Ananthram Swami</name>
    </author>
    <author>
      <name>Richard Harang</name>
    </author>
    <link href="http://arxiv.org/abs/1604.08275v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08275v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07373v1</id>
    <updated>2016-08-26T07:14:37Z</updated>
    <published>2016-08-26T07:14:37Z</published>
    <title>Applying Topological Persistence in Convolutional Neural Network for
  Music Audio Signals</title>
    <summary>  Recent years have witnessed an increased interest in the application of
persistent homology, a topological tool for data analysis, to machine learning
problems. Persistent homology is known for its ability to numerically
characterize the shapes of spaces induced by features or functions. On the
other hand, deep neural networks have been shown effective in various tasks. To
our best knowledge, however, existing neural network models seldom exploit
shape information. In this paper, we investigate a way to use persistent
homology in the framework of deep neural networks. Specifically, we propose to
embed the so-called "persistence landscape," a rather new topological summary
for data, into a convolutional neural network (CNN) for dealing with audio
signals. Our evaluation on automatic music tagging, a multi-label
classification task, shows that the resulting persistent convolutional neural
network (PCNN) model can perform significantly better than state-of-the-art
models in prediction accuracy. We also discuss the intuition behind the design
of the proposed model, and offer insights into the features that it learns.
</summary>
    <author>
      <name>Jen-Yu Liu</name>
    </author>
    <author>
      <name>Shyh-Kang Jeng</name>
    </author>
    <author>
      <name>Yi-Hsuan Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1608.07373v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07373v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.06918v1</id>
    <updated>2016-10-21T19:58:29Z</updated>
    <published>2016-10-21T19:58:29Z</published>
    <title>Learning to Protect Communications with Adversarial Neural Cryptography</title>
    <summary>  We ask whether neural networks can learn to use secret keys to protect
information from other neural networks. Specifically, we focus on ensuring
confidentiality properties in a multiagent system, and we specify those
properties in terms of an adversary. Thus, a system may consist of neural
networks named Alice and Bob, and we aim to limit what a third neural network
named Eve learns from eavesdropping on the communication between Alice and Bob.
We do not prescribe specific cryptographic algorithms to these neural networks;
instead, we train end-to-end, adversarially. We demonstrate that the neural
networks can learn how to perform forms of encryption and decryption, and also
how to apply these operations selectively in order to meet confidentiality
goals.
</summary>
    <author>
      <name>Mart√≠n Abadi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Google Brain</arxiv:affiliation>
    </author>
    <author>
      <name>David G. Andersen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Google Brain</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.06918v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.06918v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.10458v1</id>
    <updated>2017-03-30T13:18:35Z</updated>
    <published>2017-03-30T13:18:35Z</published>
    <title>Application of a Shallow Neural Network to Short-Term Stock Trading</title>
    <summary>  Machine learning is increasingly prevalent in stock market trading. Though
neural networks have seen success in computer vision and natural language
processing, they have not been as useful in stock market trading. To
demonstrate the applicability of a neural network in stock trading, we made a
single-layer neural network that recommends buying or selling shares of a stock
by comparing the highest high of 10 consecutive days with that of the next 10
days, a process repeated for the stock's year-long historical data. A
chi-squared analysis found that the neural network can accurately and
appropriately decide whether to buy or sell shares for a given stock, showing
that a neural network can make simple decisions about the stock market.
</summary>
    <author>
      <name>Abhinav Madahar</name>
    </author>
    <author>
      <name>Yuze Ma</name>
    </author>
    <author>
      <name>Kunal Patel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.10458v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.10458v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.00727v2</id>
    <updated>2018-01-31T08:43:37Z</updated>
    <published>2017-11-01T10:21:02Z</published>
    <title>Performance Evaluation of Channel Decoding With Deep Neural Networks</title>
    <summary>  With the demand of high data rate and low latency in fifth generation (5G),
deep neural network decoder (NND) has become a promising candidate due to its
capability of one-shot decoding and parallel computing. In this paper, three
types of NND, i.e., multi-layer perceptron (MLP), convolution neural network
(CNN) and recurrent neural network (RNN), are proposed with the same parameter
magnitude. The performance of these deep neural networks are evaluated through
extensive simulation. Numerical results show that RNN has the best decoding
performance, yet at the price of the highest computational overhead. Moreover,
we find there exists a saturation length for each type of neural network, which
is caused by their restricted learning abilities.
</summary>
    <author>
      <name>Wei Lyu</name>
    </author>
    <author>
      <name>Zhaoyang Zhang</name>
    </author>
    <author>
      <name>Chunxu Jiao</name>
    </author>
    <author>
      <name>Kangjian Qin</name>
    </author>
    <author>
      <name>Huazi Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 11 figures, Latex; typos corrected; IEEE ICC 2018 to appear</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.00727v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.00727v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.05860v1</id>
    <updated>2017-11-06T19:17:58Z</updated>
    <published>2017-11-06T19:17:58Z</published>
    <title>A General Neural Network Hardware Architecture on FPGA</title>
    <summary>  Field Programmable Gate Arrays (FPGAs) plays an increasingly important role
in data sampling and processing industries due to its highly parallel
architecture, low power consumption, and flexibility in custom algorithms.
Especially, in the artificial intelligence field, for training and implement
the neural networks and machine learning algorithms, high energy efficiency
hardware implement and massively parallel computing capacity are heavily
demanded. Therefore, many global companies have applied FPGAs into AI and
Machine learning fields such as autonomous driving and Automatic Spoken
Language Recognition (Baidu) [1] [2] and Bing search (Microsoft) [3].
Considering the FPGAs great potential in these fields, we tend to implement a
general neural network hardware architecture on XILINX ZU9CG System On Chip
(SOC) platform [4], which contains abundant hardware resource and powerful
processing capacity. The general neural network architecture on the FPGA SOC
platform can perform forward and backward algorithms in deep neural networks
(DNN) with high performance and easily be adjusted according to the type and
scale of the neural networks.
</summary>
    <author>
      <name>Yufeng Hao</name>
    </author>
    <link href="http://arxiv.org/abs/1711.05860v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.05860v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.08831v1</id>
    <updated>2018-01-26T14:45:24Z</updated>
    <published>2018-01-26T14:45:24Z</published>
    <title>A Multilayer Convolutional Encoder-Decoder Neural Network for
  Grammatical Error Correction</title>
    <summary>  We improve automatic correction of grammatical, orthographic, and collocation
errors in text using a multilayer convolutional encoder-decoder neural network.
The network is initialized with embeddings that make use of character N-gram
information to better suit this task. When evaluated on common benchmark test
data sets (CoNLL-2014 and JFLEG), our model substantially outperforms all prior
neural approaches on this task as well as strong statistical machine
translation-based systems with neural and task-specific features trained on the
same data. Our analysis shows the superiority of convolutional neural networks
over recurrent neural networks such as long short-term memory (LSTM) networks
in capturing the local context via attention, and thereby improving the
coverage in correcting grammatical errors. By ensembling multiple models, and
incorporating an N-gram language model and edit features via rescoring, our
novel method becomes the first neural approach to outperform the current
state-of-the-art statistical machine translation-based approach, both in terms
of grammaticality and fluency.
</summary>
    <author>
      <name>Shamil Chollampatt</name>
    </author>
    <author>
      <name>Hwee Tou Ng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures, In Proceedings of AAAI 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.08831v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.08831v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.07850v1</id>
    <updated>2017-10-21T20:14:00Z</updated>
    <published>2017-10-21T20:14:00Z</published>
    <title>Deep Neural Network Approximation using Tensor Sketching</title>
    <summary>  Deep neural networks are powerful learning models that achieve
state-of-the-art performance on many computer vision, speech, and language
processing tasks. In this paper, we study a fundamental question that arises
when designing deep network architectures: Given a target network architecture
can we design a smaller network architecture that approximates the operation of
the target network? The question is, in part, motivated by the challenge of
parameter reduction (compression) in modern deep neural networks, as the ever
increasing storage and memory requirements of these networks pose a problem in
resource constrained environments.
  In this work, we focus on deep convolutional neural network architectures,
and propose a novel randomized tensor sketching technique that we utilize to
develop a unified framework for approximating the operation of both the
convolutional and fully connected layers. By applying the sketching technique
along different tensor dimensions, we design changes to the convolutional and
fully connected layers that substantially reduce the number of effective
parameters in a network. We show that the resulting smaller network can be
trained directly, and has a classification accuracy that is comparable to the
original network.
</summary>
    <author>
      <name>Shiva Prasad Kasiviswanathan</name>
    </author>
    <author>
      <name>Nina Narodytska</name>
    </author>
    <author>
      <name>Hongxia Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.07850v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.07850v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.0929v1</id>
    <updated>2007-09-06T18:00:15Z</updated>
    <published>2007-09-06T18:00:15Z</published>
    <title>Analysis of network by generalized mutual entropies</title>
    <summary>  Generalized mutual entropy is defined for networks and applied for analysis
of complex network structures. The method is tested for the case of computer
simulated scale free networks, random networks, and their mixtures. The
possible applications for real network analysis are discussed.
</summary>
    <author>
      <name>V. Gudkov</name>
    </author>
    <author>
      <name>V. Montealegre</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physa.2008.01.005</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physa.2008.01.005" rel="related"/>
    <link href="http://arxiv.org/abs/0709.0929v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.0929v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0511042v1</id>
    <updated>2005-11-10T10:00:46Z</updated>
    <published>2005-11-10T10:00:46Z</published>
    <title>Dimensions of Neural-symbolic Integration - A Structured Survey</title>
    <summary>  Research on integrated neural-symbolic systems has made significant progress
in the recent past. In particular the understanding of ways to deal with
symbolic knowledge within connectionist systems (also called artificial neural
networks) has reached a critical mass which enables the community to strive for
applicable implementations and use cases. Recent work has covered a great
variety of logics used in artificial intelligence and provides a multitude of
techniques for dealing with them within the context of artificial neural
networks. We present a comprehensive survey of the field of neural-symbolic
integration, including a new classification of system according to their
architectures and abilities.
</summary>
    <author>
      <name>Sebastian Bader</name>
    </author>
    <author>
      <name>Pascal Hitzler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0511042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0511042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08571v1</id>
    <updated>2016-02-27T08:45:35Z</updated>
    <published>2016-02-27T08:45:35Z</published>
    <title>Towards Neural Knowledge DNA</title>
    <summary>  In this paper, we propose the Neural Knowledge DNA, a framework that tailors
the ideas underlying the success of neural networks to the scope of knowledge
representation. Knowledge representation is a fundamental field that dedicate
to representing information about the world in a form that computer systems can
utilize to solve complex tasks. The proposed Neural Knowledge DNA is designed
to support discovering, storing, reusing, improving, and sharing knowledge
among machines and organisation. It is constructed in a similar fashion of how
DNA formed: built up by four essential elements. As the DNA produces
phenotypes, the Neural Knowledge DNA carries information and knowledge via its
four essential elements, namely, Networks, Experiences, States, and Actions.
</summary>
    <author>
      <name>Haoxi Zhang</name>
    </author>
    <author>
      <name>Cesar Sanin</name>
    </author>
    <author>
      <name>Edward Szczerbicki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.08571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.02540v3</id>
    <updated>2017-11-01T08:50:32Z</updated>
    <published>2017-09-08T05:00:20Z</published>
    <title>The Expressive Power of Neural Networks: A View from the Width</title>
    <summary>  The expressive power of neural networks is important for understanding deep
learning. Most existing works consider this problem from the view of the depth
of a network. In this paper, we study how width affects the expressiveness of
neural networks. Classical results state that depth-bounded (e.g. depth-$2$)
networks with suitable activation functions are universal approximators. We
show a universal approximation theorem for width-bounded ReLU networks:
width-$(n+4)$ ReLU networks, where $n$ is the input dimension, are universal
approximators. Moreover, except for a measure zero set, all functions cannot be
approximated by width-$n$ ReLU networks, which exhibits a phase transition.
Several recent works demonstrate the benefits of depth by proving the
depth-efficiency of neural networks. That is, there are classes of deep
networks which cannot be realized by any shallow network whose size is no more
than an exponential bound. Here we pose the dual question on the
width-efficiency of ReLU networks: Are there wide networks that cannot be
realized by narrow networks whose size is not substantially larger? We show
that there exist classes of wide networks which cannot be realized by any
narrow network whose depth is no more than a polynomial bound. On the other
hand, we demonstrate by extensive experiments that narrow networks whose size
exceed the polynomial bound by a constant factor can approximate wide and
shallow network with high accuracy. Our results provide more comprehensive
evidence that depth is more effective than width for the expressiveness of ReLU
networks.
</summary>
    <author>
      <name>Zhou Lu</name>
    </author>
    <author>
      <name>Hongming Pu</name>
    </author>
    <author>
      <name>Feicheng Wang</name>
    </author>
    <author>
      <name>Zhiqiang Hu</name>
    </author>
    <author>
      <name>Liwei Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted by NIPS 2017 ( with some typos fixed)</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.02540v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.02540v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504055v1</id>
    <updated>2005-04-13T13:57:56Z</updated>
    <published>2005-04-13T13:57:56Z</published>
    <title>A Learning Algorithm for Evolving Cascade Neural Networks</title>
    <summary>  A new learning algorithm for Evolving Cascade Neural Networks (ECNNs) is
described. An ECNN starts to learn with one input node and then adding new
inputs as well as new hidden neurons evolves it. The trained ECNN has a nearly
minimal number of input and hidden neurons as well as connections. The
algorithm was successfully applied to classify artifacts and normal segments in
clinical electroencephalograms (EEGs). The EEG segments were visually labeled
by EEG-viewer. The trained ECNN has correctly classified 96.69% of the testing
segments. It is slightly better than a standard fully connected neural network.
</summary>
    <author>
      <name>Vitaly Schetinin</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Processing Letter 17:21-31, 2003. Kluwer</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0504055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/nlin/0202039v1</id>
    <updated>2002-02-19T13:11:42Z</updated>
    <published>2002-02-19T13:11:42Z</published>
    <title>A neural model for multi-expert architectures</title>
    <summary>  We present a generalization of conventional artificial neural networks that
allows for a functional equivalence to multi-expert systems. The new model
provides an architectural freedom going beyond existing multi-expert models and
an integrative formalism to compare and combine various techniques of learning.
(We consider gradient, EM, reinforcement, and unsupervised learning.) Its
uniform representation aims at a simple genetic encoding and evolutionary
structure optimization of multi-expert systems. This paper contains a detailed
description of the model and learning rules, empirically validates its
functionality, and discusses future perspectives.
</summary>
    <author>
      <name>Marc Toussaint</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LaTeX, 8 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Joint Conference on Neural
  Networks (IJCNN 2002), 2755-2760.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/nlin/0202039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/nlin/0202039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.03592v1</id>
    <updated>2017-02-12T23:12:01Z</updated>
    <published>2017-02-12T23:12:01Z</published>
    <title>Graph Neural Networks and Boolean Satisfiability</title>
    <summary>  In this paper we explore whether or not deep neural architectures can learn
to classify Boolean satisfiability (SAT). We devote considerable time to
discussing the theoretical properties of SAT. Then, we define a graph
representation for Boolean formulas in conjunctive normal form, and train
neural classifiers over general graph structures called Graph Neural Networks,
or GNNs, to recognize features of satisfiability. To the best of our knowledge
this has never been tried before. Our preliminary findings are potentially
profound. In a weakly-supervised setting, that is, without problem specific
feature engineering, Graph Neural Networks can learn features of
satisfiability.
</summary>
    <author>
      <name>Benedikt B√ºnz</name>
    </author>
    <author>
      <name>Matthew Lamm</name>
    </author>
    <link href="http://arxiv.org/abs/1702.03592v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.03592v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.01991v2</id>
    <updated>2017-06-22T04:11:21Z</updated>
    <published>2017-06-06T21:58:50Z</published>
    <title>Unsupervised Neural-Symbolic Integration</title>
    <summary>  Symbolic has been long considered as a language of human intelligence while
neural networks have advantages of robust computation and dealing with noisy
data. The integration of neural-symbolic can offer better learning and
reasoning while providing a means for interpretability through the
representation of symbolic knowledge. Although previous works focus intensively
on supervised feedforward neural networks, little has been done for the
unsupervised counterparts. In this paper we show how to integrate symbolic
knowledge into unsupervised neural networks. We exemplify our approach with
knowledge in different forms, including propositional logic for DNA promoter
prediction and first-order logic for understanding family relationship.
</summary>
    <author>
      <name>Son N. Tran</name>
    </author>
    <link href="http://arxiv.org/abs/1706.01991v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.01991v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.4984v1</id>
    <updated>2010-09-25T07:05:29Z</updated>
    <published>2010-09-25T07:05:29Z</published>
    <title>Rule Extraction using Artificial Neural Networks</title>
    <summary>  Artificial neural networks have been successfully applied to a variety of
business application problems involving classification and regression. Although
backpropagation neural networks generally predict better than decision trees do
for pattern classification problems, they are often regarded as black boxes,
i.e., their predictions are not as interpretable as those of decision trees. In
many applications, it is desirable to extract knowledge from trained neural
networks so that the users can gain a better understanding of the solution.
This paper presents an efficient algorithm to extract rules from artificial
neural networks. We use two-phase training algorithm for backpropagation
learning. In the first phase, the number of hidden nodes of the network is
determined automatically in a constructive fashion by adding nodes one after
another based on the performance of the network on training data. In the second
phase, the number of relevant input units of the network is determined using
pruning algorithm. The pruning process attempts to eliminate as many
connections as possible from the network. Relevant and irrelevant attributes of
the data are distinguished during the training process. Those that are relevant
will be kept and others will be automatically discarded. From the simplified
networks having small number of connections and nodes we may easily able to
extract symbolic rules using the proposed algorithm. Extensive experimental
results on several benchmarks problems in neural networks demonstrate the
effectiveness of the proposed approach with good generalization ability.
</summary>
    <author>
      <name>S. M. Kamruzzaman</name>
    </author>
    <author>
      <name>Ahmed Ryadh Hasan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 Pages, International Conference</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. International Conference on Information and Communication
  Technology in Management (ICTM 2005), Multimedia University, Malaysia, May
  2005</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.4984v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.4984v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.6310v1</id>
    <updated>2013-02-26T04:22:44Z</updated>
    <published>2013-02-26T04:22:44Z</published>
    <title>Estimating Sectoral Pollution Load in Lagos, Nigeria Using Data Mining
  Techniques</title>
    <summary>  Industrial pollution is often considered to be one of the prime factors
contributing to air, water and soil pollution. Sectoral pollution loads
(ton/yr) into different media (i.e. air, water and land) in Lagos were
estimated using Industrial Pollution Projected System (IPPS). These were
further studied using Artificial neural Networks (ANNs), a data mining
technique that has the ability of detecting and describing patterns in large
data sets with variables that are non- linearly related. Time Lagged Recurrent
Network (TLRN) appeared as the best Neural Network model among all the neural
networks considered which includes Multilayer Perceptron (MLP) Network,
Generalized Feed Forward Neural Network (GFNN), Radial Basis Function (RBF)
Network and Recurrent Network (RN). TLRN modelled the data-sets better than the
others in terms of the mean average error (MAE) (0.14), time (39 s) and linear
correlation coefficient (0.84). The results showed that Artificial Neural
Networks (ANNs) technique (i.e., Time Lagged Recurrent Network) is also
applicable and effective in environmental assessment study. Keywords:
Artificial Neural Networks (ANNs), Data Mining Techniques, Industrial Pollution
Projection System (IPPS), Pollution load, Pollution Intensity.
</summary>
    <author>
      <name>Adesesan . B Adeyemo</name>
    </author>
    <author>
      <name>Adebola A. Oketola</name>
    </author>
    <author>
      <name>Emmanuel O. Adetula</name>
    </author>
    <author>
      <name>O. Osibanjo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 11 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">A.B ADEYEMO, A.A OKETOLA, E.O ADETULA, O.OSIBANJO (2012):
  Estimating Sectoral Pollution Load in Lagos, Nigeria Using Data Mining
  Techniques,International-Journal-of-Computer-Science-Issues, Volume 9, Issue
  6, November 2012 pages 465-475</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1302.6310v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.6310v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.03176v2</id>
    <updated>2016-11-21T16:22:35Z</updated>
    <published>2015-05-12T21:49:36Z</published>
    <title>Firing Rate Dynamics in Recurrent Spiking Neural Networks with Intrinsic
  and Network Heterogeneity</title>
    <summary>  Heterogeneity of neural attributes has recently gained a lot of attention and
is increasing recognized as a crucial feature in neural processing. Despite its
importance, this physiological feature has traditionally been neglected in
theoretical studies of cortical neural networks. Thus, there is still a lot
unknown about the consequences of cellular and circuit heterogeneity in spiking
neural networks. In particular, combining network or synaptic heterogeneity and
intrinsic heterogeneity has yet to be considered systematically despite the
fact that both are known to exist and likely have significant roles in neural
network dynamics. In a canonical recurrent spiking neural network model, we
study how these two forms of heterogeneity lead to different distributions of
excitatory firing rates. To analytically characterize how these types of
heterogeneities affect the network, we employ a dimension reduction method that
relies on a combination of Monte Carlo simulations and probability density
function equations. We find that the relationship between intrinsic and network
heterogeneity has a strong effect on the overall level of heterogeneity of the
firing rates. Specifically, this relationship can lead to amplification or
attenuation of firing rate heterogeneity, and these effects depend on whether
the recurrent network is firing asynchronously or rhythmically firing. These
observations are captured with the aforementioned reduction method, and
furthermore simpler analytic descriptions based on this dimension reduction
method are developed. The final analytic descriptions provide compact and
descriptive formulas for how the relationship between intrinsic and network
heterogeneity determines the firing rate heterogeneity dynamics in various
settings.
</summary>
    <author>
      <name>Cheng Ly</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10827-015-0578-0</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10827-015-0578-0" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.03176v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.03176v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.5472v2</id>
    <updated>2012-06-18T15:48:31Z</updated>
    <published>2011-07-27T13:48:22Z</published>
    <title>Neural Relax</title>
    <summary>  We present an algorithm for data preprocessing of an associative memory
inspired to an electrostatic problem that turns out to have intimate relations
with information maximization.
</summary>
    <author>
      <name>Elisa Benedetti</name>
    </author>
    <author>
      <name>Marco Budinich</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/NECO_a_00359</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/NECO_a_00359" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Computation, November 2012, Vol. 24, No. 11, pp. 3091 3110</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1107.5472v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.5472v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.04503v1</id>
    <updated>2017-01-17T01:15:14Z</updated>
    <published>2017-01-17T01:15:14Z</published>
    <title>Deep Learning for Computational Chemistry</title>
    <summary>  The rise and fall of artificial neural networks is well documented in the
scientific literature of both computer science and computational chemistry. Yet
almost two decades later, we are now seeing a resurgence of interest in deep
learning, a machine learning algorithm based on multilayer neural networks.
Within the last few years, we have seen the transformative impact of deep
learning in many domains, particularly in speech recognition and computer
vision, to the extent that the majority of expert practitioners in those field
are now regularly eschewing prior established models in favor of deep learning
models. In this review, we provide an introductory overview into the theory of
deep neural networks and their unique properties that distinguish them from
traditional machine learning algorithms used in cheminformatics. By providing
an overview of the variety of emerging applications of deep neural networks, we
highlight its ubiquity and broad applicability to a wide range of challenges in
the field, including QSAR, virtual screening, protein structure prediction,
quantum chemistry, materials design and property prediction. In reviewing the
performance of deep neural networks, we observed a consistent outperformance
against non-neural networks state-of-the-art models across disparate research
topics, and deep neural network based models often exceeded the "glass ceiling"
expectations of their respective tasks. Coupled with the maturity of
GPU-accelerated computing for training deep neural networks and the exponential
growth of chemical data on which to train these networks on, we anticipate that
deep learning algorithms will be a valuable tool for computational chemistry.
</summary>
    <author>
      <name>Garrett B. Goh</name>
    </author>
    <author>
      <name>Nathan O. Hodas</name>
    </author>
    <author>
      <name>Abhinav Vishnu</name>
    </author>
    <link href="http://arxiv.org/abs/1701.04503v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.04503v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.3563v1</id>
    <updated>2010-04-20T20:24:51Z</updated>
    <published>2010-04-20T20:24:51Z</published>
    <title>A QoS Provisioning Recurrent Neural Network based Call Admission Control
  for beyond 3G Networks</title>
    <summary>  The Call admission control (CAC) is one of the Radio Resource Management
(RRM) techniques that plays influential role in ensuring the desired Quality of
Service (QoS) to the users and applications in next generation networks. This
paper proposes a fuzzy neural approach for making the call admission control
decision in multi class traffic based Next Generation Wireless Networks (NGWN).
The proposed Fuzzy Neural call admission control (FNCAC) scheme is an
integrated CAC module that combines the linguistic control capabilities of the
fuzzy logic controller and the learning capabilities of the neural networks.
The model is based on recurrent radial basis function networks which have
better learning and adaptability that can be used to develop intelligent system
to handle the incoming traffic in an heterogeneous network environment. The
simulation results are optimistic and indicates that the proposed FNCAC
algorithm performs better than the other two methods and the call blocking
probability is minimal when compared to other two methods.
</summary>
    <author>
      <name>Ramesh Babu H. S.</name>
    </author>
    <author>
      <name> Gowrishankar</name>
    </author>
    <author>
      <name>Satyanarayana P. S</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues online at
  http://ijcsi.org/articles/A-QoS-Provisioning-Recurrent-Neural-Network-based-Call-Admission-Control-for-beyond-3G-Networks.php</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI, Volume 7, Issue 2, March 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.3563v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.3563v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.06910v2</id>
    <updated>2017-10-20T14:49:30Z</updated>
    <published>2017-10-18T19:53:57Z</published>
    <title>Characterization of Gradient Dominance and Regularity Conditions for
  Neural Networks</title>
    <summary>  The past decade has witnessed a successful application of deep learning to
solving many challenging problems in machine learning and artificial
intelligence. However, the loss functions of deep neural networks (especially
nonlinear networks) are still far from being well understood from a theoretical
aspect. In this paper, we enrich the current understanding of the landscape of
the square loss functions for three types of neural networks. Specifically,
when the parameter matrices are square, we provide an explicit characterization
of the global minimizers for linear networks, linear residual networks, and
nonlinear networks with one hidden layer. Then, we establish two quadratic
types of landscape properties for the square loss of these neural networks,
i.e., the gradient dominance condition within the neighborhood of their full
rank global minimizers, and the regularity condition along certain directions
and within the neighborhood of their global minimizers. These two landscape
properties are desirable for the optimization around the global minimizers of
the loss function for these neural networks.
</summary>
    <author>
      <name>Yi Zhou</name>
    </author>
    <author>
      <name>Yingbin Liang</name>
    </author>
    <link href="http://arxiv.org/abs/1710.06910v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.06910v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03308v1</id>
    <updated>2018-02-09T15:35:41Z</updated>
    <published>2018-02-09T15:35:41Z</published>
    <title>Predictive Neural Networks</title>
    <summary>  Recurrent neural networks are a powerful means to cope with time series. We
show that already linearly activated recurrent neural networks can approximate
any time-dependent function f(t) given by a number of function values. The
approximation can effectively be learned by simply solving a linear equation
system; no backpropagation or similar methods are needed. Furthermore the
network size can be reduced by taking only the most relevant components of the
network. Thus, in contrast to others, our approach not only learns network
weights but also the network architecture. The networks have interesting
properties: In the stationary case they end up in ellipse trajectories in the
long run, and they allow the prediction of further values and compact
representations of functions. We demonstrate this by several experiments, among
them multiple superimposed oscillators (MSO) and robotic soccer. Predictive
neural networks outperform the previous state-of-the-art for the MSO task with
a minimal number of units.
</summary>
    <author>
      <name>Frieder Stolzenburg</name>
    </author>
    <author>
      <name>Olivia Michael</name>
    </author>
    <author>
      <name>Oliver Obst</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 8 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.03308v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03308v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.7085v1</id>
    <updated>2012-05-31T19:38:08Z</updated>
    <published>2012-05-31T19:38:08Z</published>
    <title>Long-term memory stabilized by noise-induced rehearsal</title>
    <summary>  Cortical networks can maintain memories for decades despite the short
lifetime of synaptic strength. Can a neural network store long-lasting memories
in unstable synapses? Here, we study the effects of random noise on the
stability of memory stored in synapses of an attractor neural network. The
model includes ongoing spike timing dependent plasticity (STDP). We show that
certain classes of STDP rules can lead to the stabilization of memory patterns
stored in the network. The stabilization results from rehearsals induced by
noise. We show that unstructured neural noise, after passing through the
recurrent network weights, carries the imprint of all memory patterns in
temporal correlations. Under certain conditions, STDP combined with these
correlations, can lead to reinforcement of all existing patterns, even those
that are never explicitly visited. Thus, unstructured neural noise can
stabilize the existing structure of synaptic connectivity. Our findings may
provide the functional reason for highly irregular spiking displayed by
cortical neurons and provide justification for models of system memory
consolidation. Therefore, we propose that irregular neural activity is the
feature that helps cortical networks maintain stable connections.
</summary>
    <author>
      <name>Yi Wei</name>
    </author>
    <author>
      <name>Alexei A. Koulakov</name>
    </author>
    <link href="http://arxiv.org/abs/1205.7085v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.7085v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.3817v1</id>
    <updated>2011-07-19T19:38:51Z</updated>
    <published>2011-07-19T19:38:51Z</published>
    <title>Multiscale approach for bone remodeling simulation based on finite
  element and neural network computation</title>
    <summary>  The aim of this paper is to develop a multiscale hierarchical hybrid model
based on finite element analysis and neural network computation to link
mesoscopic scale (trabecular network level) and macroscopic (whole bone level)
to simulate bone remodelling process. Because whole bone simulation considering
the 3D trabecular level is time consuming, the finite element calculation is
performed at macroscopic level and a trained neural network are employed as
numerical devices for substituting the finite element code needed for the
mesoscale prediction. The bone mechanical properties are updated at macroscopic
scale depending on the morphological organization at the mesoscopic computed by
the trained neural network. The digital image-based modeling technique using
m-CT and voxel finite element mesh is used to capture 2 mm3 Representative
Volume Elements at mesoscale level in a femur head. The input data for the
artificial neural network are a set of bone material parameters, boundary
conditions and the applied stress. The output data is the updated bone
properties and some trabecular bone factors. The presented approach, to our
knowledge, is the first model incorporating both FE analysis and neural network
computation to simulate the multilevel bone adaptation in rapid way.
</summary>
    <author>
      <name>Ridha Hambli</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Prisme</arxiv:affiliation>
    </author>
    <author>
      <name>Abdelwahed Barkaoui</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Prisme</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference Multiscale Materials Modeling - MMM2010,
  Freiburg : Germany (2010)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1107.3817v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.3817v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.TO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.03626v1</id>
    <updated>2015-06-11T11:10:25Z</updated>
    <published>2015-06-11T11:10:25Z</published>
    <title>Margin-Based Feed-Forward Neural Network Classifiers</title>
    <summary>  Margin-Based Principle has been proposed for a long time, it has been proved
that this principle could reduce the structural risk and improve the
performance in both theoretical and practical aspects. Meanwhile, feed-forward
neural network is a traditional classifier, which is very hot at present with a
deeper architecture. However, the training algorithm of feed-forward neural
network is developed and generated from Widrow-Hoff Principle that means to
minimize the squared error. In this paper, we propose a new training algorithm
for feed-forward neural networks based on Margin-Based Principle, which could
effectively promote the accuracy and generalization ability of neural network
classifiers with less labelled samples and flexible network. We have conducted
experiments on four UCI open datasets and achieved good results as expected. In
conclusion, our model could handle more sparse labelled and more high-dimension
dataset in a high accuracy while modification from old ANN method to our method
is easy and almost free of work.
</summary>
    <author>
      <name>Han Xiao</name>
    </author>
    <author>
      <name>Xiaoyan Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been published in ICANN 2015: International Conference
  on Artificial Neural Networks, Amsterdam, The Netherlands, (May 14-15, 2015)</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.03626v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.03626v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04859v1</id>
    <updated>2016-05-16T18:12:45Z</updated>
    <published>2016-05-16T18:12:45Z</published>
    <title>Reducing the Model Order of Deep Neural Networks Using Information
  Theory</title>
    <summary>  Deep neural networks are typically represented by a much larger number of
parameters than shallow models, making them prohibitive for small footprint
devices. Recent research shows that there is considerable redundancy in the
parameter space of deep neural networks. In this paper, we propose a method to
compress deep neural networks by using the Fisher Information metric, which we
estimate through a stochastic optimization method that keeps track of
second-order information in the network. We first remove unimportant parameters
and then use non-uniform fixed point quantization to assign more bits to
parameters with higher Fisher Information estimates. We evaluate our method on
a classification task with a convolutional neural network trained on the MNIST
data set. Experimental results show that our method outperforms existing
methods for both network pruning and quantization.
</summary>
    <author>
      <name>Ming Tu</name>
    </author>
    <author>
      <name>Visar Berisha</name>
    </author>
    <author>
      <name>Yu Cao</name>
    </author>
    <author>
      <name>Jae-sun Seo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ISVLSI 2016 special session</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.04859v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04859v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07326v3</id>
    <updated>2016-07-03T09:39:30Z</updated>
    <published>2016-06-23T14:30:36Z</published>
    <title>DropNeuron: Simplifying the Structure of Deep Neural Networks</title>
    <summary>  Deep learning using multi-layer neural networks (NNs) architecture manifests
superb power in modern machine learning systems. The trained Deep Neural
Networks (DNNs) are typically large. The question we would like to address is
whether it is possible to simplify the NN during training process to achieve a
reasonable performance within an acceptable computational time. We presented a
novel approach of optimising a deep neural network through regularisation of
net- work architecture. We proposed regularisers which support a simple
mechanism of dropping neurons during a network training process. The method
supports the construction of a simpler deep neural networks with compatible
performance with its simplified version. As a proof of concept, we evaluate the
proposed method with examples including sparse linear regression, deep
autoencoder and convolutional neural network. The valuations demonstrate
excellent performance.
  The code for this work can be found in
http://www.github.com/panweihit/DropNeuron
</summary>
    <author>
      <name>Wei Pan</name>
    </author>
    <author>
      <name>Hao Dong</name>
    </author>
    <author>
      <name>Yike Guo</name>
    </author>
    <link href="http://arxiv.org/abs/1606.07326v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07326v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.03165v1</id>
    <updated>2016-10-11T02:48:13Z</updated>
    <published>2016-10-11T02:48:13Z</published>
    <title>Long Short-Term Memory based Convolutional Recurrent Neural Networks for
  Large Vocabulary Speech Recognition</title>
    <summary>  Long short-term memory (LSTM) recurrent neural networks (RNNs) have been
shown to give state-of-the-art performance on many speech recognition tasks, as
they are able to provide the learned dynamically changing contextual window of
all sequence history. On the other hand, the convolutional neural networks
(CNNs) have brought significant improvements to deep feed-forward neural
networks (FFNNs), as they are able to better reduce spectral variation in the
input signal. In this paper, a network architecture called as convolutional
recurrent neural network (CRNN) is proposed by combining the CNN and LSTM RNN.
In the proposed CRNNs, each speech frame, without adjacent context frames, is
organized as a number of local feature patches along the frequency axis, and
then a LSTM network is performed on each feature patch along the time axis. We
train and compare FFNNs, LSTM RNNs and the proposed LSTM CRNNs at various
number of configurations. Experimental results show that the LSTM CRNNs can
exceed state-of-the-art speech recognition performance.
</summary>
    <author>
      <name>Xiangang Li</name>
    </author>
    <author>
      <name>Xihong Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in INTERSPEECH 2015, September 6-10, 2015, Dresden, Germany</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.03165v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.03165v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.09463v1</id>
    <updated>2016-10-29T06:12:03Z</updated>
    <published>2016-10-29T06:12:03Z</published>
    <title>Sparse Signal Recovery for Binary Compressed Sensing by Majority Voting
  Neural Networks</title>
    <summary>  In this paper, we propose majority voting neural networks for sparse signal
recovery in binary compressed sensing. The majority voting neural network is
composed of several independently trained feedforward neural networks employing
the sigmoid function as an activation function. Our empirical study shows that
a choice of a loss function used in training processes for the network is of
prime importance. We found a loss function suitable for sparse signal recovery,
which includes a cross entropy-like term and an $L_1$ regularized term. From
the experimental results, we observed that the majority voting neural network
achieves excellent recovery performance, which is approaching the optimal
performance as the number of component nets grows. The simple architecture of
the majority voting neural networks would be beneficial for both software and
hardware implementations.
</summary>
    <author>
      <name>Daisuke Ito</name>
    </author>
    <author>
      <name>Tadashi Wadayama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.09463v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.09463v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.04959v1</id>
    <updated>2017-04-17T13:23:36Z</updated>
    <published>2017-04-17T13:23:36Z</published>
    <title>Introspection: Accelerating Neural Network Training By Learning Weight
  Evolution</title>
    <summary>  Neural Networks are function approximators that have achieved
state-of-the-art accuracy in numerous machine learning tasks. In spite of their
great success in terms of accuracy, their large training time makes it
difficult to use them for various tasks. In this paper, we explore the idea of
learning weight evolution pattern from a simple network for accelerating
training of novel neural networks. We use a neural network to learn the
training pattern from MNIST classification and utilize it to accelerate
training of neural networks used for CIFAR-10 and ImageNet classification. Our
method has a low memory footprint and is computationally efficient. This method
can also be used with other optimizers to give faster convergence. The results
indicate a general trend in the weight evolution during training of neural
networks.
</summary>
    <author>
      <name>Abhishek Sinha</name>
    </author>
    <author>
      <name>Mausoom Sarkar</name>
    </author>
    <author>
      <name>Aahitagni Mukherjee</name>
    </author>
    <author>
      <name>Balaji Krishnamurthy</name>
    </author>
    <link href="http://arxiv.org/abs/1704.04959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.04959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.09401v3</id>
    <updated>2018-01-19T23:13:36Z</updated>
    <published>2017-08-30T18:00:51Z</published>
    <title>Machine Learning Topological Invariants with Neural Networks</title>
    <summary>  In this Letter we supervisedly train neural networks to distinguish different
topological phases in the context of topological band insulators. After
training with Hamiltonians of one-dimensional insulators with chiral symmetry,
the neural network can predict their topological winding numbers with nearly
100% accuracy, even for Hamiltonians with larger winding numbers that are not
included in the training data. These results show a remarkable success that the
neural network can capture the global and nonlinear topological features of
quantum phases from local inputs. By opening up the neural network, we confirm
that the network does learn the discrete version of the winding number formula.
We also make a couple of remarks regarding the role of the symmetry and the
opposite effect of regularization techniques when applying machine learning to
physical systems.
</summary>
    <author>
      <name>Pengfei Zhang</name>
    </author>
    <author>
      <name>Huitao Shen</name>
    </author>
    <author>
      <name>Hui Zhai</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.120.066401</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.120.066401" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures and 1 table + 2 pages of supplemental material</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. Lett. 120, 066401 (2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1708.09401v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.09401v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.mes-hall" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mes-hall" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.str-el" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.08862v1</id>
    <updated>2017-12-24T00:27:09Z</updated>
    <published>2017-12-24T00:27:09Z</published>
    <title>Neural Network Multitask Learning for Traffic Flow Forecasting</title>
    <summary>  Traditional neural network approaches for traffic flow forecasting are
usually single task learning (STL) models, which do not take advantage of the
information provided by related tasks. In contrast to STL, multitask learning
(MTL) has the potential to improve generalization by transferring information
in training signals of extra tasks. In this paper, MTL based neural networks
are used for traffic flow forecasting. For neural network MTL, a
backpropagation (BP) network is constructed by incorporating traffic flows at
several contiguous time instants into an output layer. Nodes in the output
layer can be seen as outputs of different but closely related STL tasks.
Comprehensive experiments on urban vehicular traffic flow data and comparisons
with STL show that MTL in BP neural networks is a promising and effective
approach for traffic flow forecasting.
</summary>
    <author>
      <name>Feng Jin</name>
    </author>
    <author>
      <name>Shiliang Sun</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Joint Conference on Neural
  Networks (IJCNN), 2008. pp. 1898-1902</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1712.08862v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.08862v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.09327v1</id>
    <updated>2017-12-26T18:52:41Z</updated>
    <published>2017-12-26T18:52:41Z</published>
    <title>Building Robust Deep Neural Networks for Road Sign Detection</title>
    <summary>  Deep Neural Networks are built to generalize outside of training set in mind
by using techniques such as regularization, early stopping and dropout. But
considerations to make them more resilient to adversarial examples are rarely
taken. As deep neural networks become more prevalent in mission-critical and
real-time systems, miscreants start to attack them by intentionally making deep
neural networks to misclassify an object of one type to be seen as another
type. This can be catastrophic in some scenarios where the classification of a
deep neural network can lead to a fatal decision by a machine. In this work, we
used GTSRB dataset to craft adversarial samples by Fast Gradient Sign Method
and Jacobian Saliency Method, used those crafted adversarial samples to attack
another Deep Convolutional Neural Network and built the attacked network to be
more resilient against adversarial attacks by making it more robust by
Defensive Distillation and Adversarial Training
</summary>
    <author>
      <name>Arkar Min Aung</name>
    </author>
    <author>
      <name>Yousef Fadila</name>
    </author>
    <author>
      <name>Radian Gondokaryono</name>
    </author>
    <author>
      <name>Luis Gonzalez</name>
    </author>
    <link href="http://arxiv.org/abs/1712.09327v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.09327v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00614v2</id>
    <updated>2018-02-07T08:02:59Z</updated>
    <published>2018-02-02T09:39:40Z</published>
    <title>Visual Interpretability for Deep Learning: a Survey</title>
    <summary>  This paper reviews recent studies in understanding neural-network
representations and learning neural networks with interpretable/disentangled
middle-layer representations. Although deep neural networks have exhibited
superior performance in various tasks, the interpretability is always the
Achilles' heel of deep neural networks. At present, deep neural networks obtain
high discrimination power at the cost of low interpretability of their
black-box representations. We believe that high model interpretability may help
people to break several bottlenecks of deep learning, e.g., learning from very
few annotations, learning via human-computer communications at the semantic
level, and semantically debugging network representations. We focus on
convolutional neural networks (CNNs), and we revisit the visualization of CNN
representations, methods of diagnosing representations of pre-trained CNNs,
approaches for disentangling pre-trained CNN representations, learning of CNNs
with disentangled representations, and middle-to-end learning based on model
interpretability. Finally, we discuss prospective trends in explainable
artificial intelligence.
</summary>
    <author>
      <name>Quanshi Zhang</name>
    </author>
    <author>
      <name>Song-Chun Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/1802.00614v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00614v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00891v1</id>
    <updated>2018-02-03T01:42:32Z</updated>
    <published>2018-02-03T01:42:32Z</published>
    <title>Joint Binary Neural Network for Multi-label Learning with Applications
  to Emotion Classification</title>
    <summary>  Recently the deep learning techniques have achieved success in multi-label
classification due to its automatic representation learning ability and the
end-to-end learning framework. Existing deep neural networks in multi-label
classification can be divided into two kinds: binary relevance neural network
(BRNN) and threshold dependent neural network (TDNN). However, the former needs
to train a set of isolate binary networks which ignore dependencies between
labels and have heavy computational load, while the latter needs an additional
threshold function mechanism to transform the multi-class probabilities to
multi-label outputs. In this paper, we propose a joint binary neural network
(JBNN), to address these shortcomings. In JBNN, the representation of the text
is fed to a set of logistic functions instead of a softmax function, and the
multiple binary classifications are carried out synchronously in one neural
network framework. Moreover, the relations between labels are captured via
training on a joint binary cross entropy (JBCE) loss. To better meet
multi-label emotion classification, we further proposed to incorporate the
prior label relations into the JBCE loss. The experimental results on the
benchmark dataset show that our model performs significantly better than the
state-of-the-art multi-label emotion classification methods, in both
classification performance and computational efficiency.
</summary>
    <author>
      <name>Huihui He</name>
    </author>
    <author>
      <name>Rui Xia</name>
    </author>
    <link href="http://arxiv.org/abs/1802.00891v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00891v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0511235v1</id>
    <updated>2005-11-09T17:02:37Z</updated>
    <published>2005-11-09T17:02:37Z</published>
    <title>Feed-forward chains of recurrent attractor neural networks with finite
  dilution near saturation</title>
    <summary>  A stationary state replica analysis for a dual neural network model that
interpolates between a fully recurrent symmetric attractor network and a
strictly feed-forward layered network, studied by Coolen and Viana, is extended
in this work to account for finite dilution of the recurrent Hebbian
interactions between binary Ising units within each layer. Gradual dilution is
found to suppress part of the phase transitions that arise from the competition
between recurrent and feed-forward operation modes of the network. Despite
that, a long chain of layers still exhibits a relatively good performance under
finite dilution for a balanced ratio between inter-layer and intra-layer
interactions.
</summary>
    <author>
      <name>F. L. Metz</name>
    </author>
    <author>
      <name>W. K. Theumann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physa.2005.11.049</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physa.2005.11.049" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Physica A</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0511235v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0511235v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/physics/0402076v2</id>
    <updated>2005-01-06T19:37:23Z</updated>
    <published>2004-02-16T21:31:09Z</published>
    <title>Fastest learning in small world neural networks</title>
    <summary>  We investigate supervised learning in neural networks. We consider a
multi-layered feed-forward network with back propagation. We find that the
network of small-world connectivity reduces the learning error and learning
time when compared to the networks of regular or random connectivity. Our study
has potential applications in the domain of data-mining, image processing,
speech recognition, and pattern recognition.
</summary>
    <author>
      <name>D. Simard</name>
    </author>
    <author>
      <name>L. Nadeau</name>
    </author>
    <author>
      <name>H. Kr√∂ger</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physleta.2004.12.078</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physleta.2004.12.078" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Text completely revised (14 pages), all new figures (7 figs)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Lett. A336 (2005) 8-15.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/physics/0402076v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/physics/0402076v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.3679v2</id>
    <updated>2008-01-31T04:18:42Z</updated>
    <published>2007-05-24T23:55:34Z</published>
    <title>Transient dynamics for sequence processing neural networks: effect of
  degree distributions</title>
    <summary>  We derive a analytic evolution equation for overlap parameters including the
effect of degree distribution on the transient dynamics of sequence processing
neural networks. In the special case of globally coupled networks, the
precisely retrieved critical loading ratio $\alpha_c = N ^{-1/2}$ is obtained,
where $N$ is the network size. In the presence of random networks, our
theoretical predictions agree quantitatively with the numerical experiments for
delta, binomial, and power-law degree distributions.
</summary>
    <author>
      <name>Yong Chen</name>
    </author>
    <author>
      <name>Pan Zhang</name>
    </author>
    <author>
      <name>Lianchun Yu</name>
    </author>
    <author>
      <name>Shengli Zhang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.77.016110</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.77.016110" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 77, 016110 (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0705.3679v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.3679v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.0060v1</id>
    <updated>2010-02-27T04:38:43Z</updated>
    <published>2010-02-27T04:38:43Z</published>
    <title>Comment on "Fastest learning in small-world neural networks"</title>
    <summary>  This comment reexamines Simard et al.'s work in [D. Simard, L. Nadeau, H.
Kroger, Phys. Lett. A 336 (2005) 8-15]. We found that Simard et al. calculated
mistakenly the local connectivity lengths Dlocal of networks. The right results
of Dlocal are presented and the supervised learning performance of feedforward
neural networks (FNNs) with different rewirings are re-investigated in this
comment. This comment discredits Simard et al's work by two conclusions: 1)
Rewiring connections of FNNs cannot generate networks with small-world
connectivity; 2) For different training sets, there do not exist networks with
a certain number of rewirings generating reduced learning errors than networks
with other numbers of rewiring.
</summary>
    <author>
      <name>Z. X. Guo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1003.0060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.0060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.8104v1</id>
    <updated>2013-07-30T19:51:12Z</updated>
    <published>2013-07-30T19:51:12Z</published>
    <title>Neural Network Capacity for Multilevel Inputs</title>
    <summary>  This paper examines the memory capacity of generalized neural networks.
Hopfield networks trained with a variety of learning techniques are
investigated for their capacity both for binary and non-binary alphabets. It is
shown that the capacity can be much increased when multilevel inputs are used.
New learning strategies are proposed to increase Hopfield network capacity, and
the scalability of these methods is also examined in respect to size of the
network. The ability to recall entire patterns from stimulation of a single
neuron is examined for the increased capacity networks.
</summary>
    <author>
      <name>Matt Stowe</name>
    </author>
    <author>
      <name>Subhash Kak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages,17 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.8104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.8104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.2297v1</id>
    <updated>2014-07-08T23:30:19Z</updated>
    <published>2014-07-08T23:30:19Z</published>
    <title>Transition to chaos in random networks with cell-type-specific
  connectivity</title>
    <summary>  In neural circuits, statistical connectivity rules strongly depend on
neuronal type. Here we study dynamics of neural networks with cell-type
specific connectivity by extending the dynamic mean field method, and find that
these networks exhibit a phase transition between silent and chaotic activity.
By analyzing the locus of this transition, we derive a new result in random
matrix theory: the spectral radius of a random connectivity matrix with
block-structured variances. We apply our results to show how a small group of
hyper-excitable neurons within the network can significantly increase the
network's computational capacity.
</summary>
    <author>
      <name>Johnatan Aljadeff</name>
    </author>
    <author>
      <name>Merav Stern</name>
    </author>
    <author>
      <name>Tatyana O. Sharpee</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.114.088101</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.114.088101" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. Lett. 114, 088101 (2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1407.2297v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.2297v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07866v1</id>
    <updated>2016-03-25T10:27:00Z</updated>
    <published>2016-03-25T10:27:00Z</published>
    <title>The Asymptotic Performance of Linear Echo State Neural Networks</title>
    <summary>  In this article, a study of the mean-square error (MSE) performance of linear
echo-state neural networks is performed, both for training and testing tasks.
Considering the realistic setting of noise present at the network nodes, we
derive deterministic equivalents for the aforementioned MSE in the limit where
the number of input data $T$ and network size $n$ both grow large. Specializing
then the network connectivity matrix to specific random settings, we further
obtain simple formulas that provide new insights on the performance of such
networks.
</summary>
    <author>
      <name>Romain Couillet</name>
    </author>
    <author>
      <name>Gilles Wainrib</name>
    </author>
    <author>
      <name>Harry Sevi</name>
    </author>
    <author>
      <name>Hafiz Tiomoko Ali</name>
    </author>
    <link href="http://arxiv.org/abs/1603.07866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04428v2</id>
    <updated>2016-07-14T15:18:56Z</updated>
    <published>2016-04-15T11:07:45Z</published>
    <title>The Artificial Mind's Eye: Resisting Adversarials for Convolutional
  Neural Networks using Internal Projection</title>
    <summary>  We introduce a novel artificial neural network architecture that integrates
robustness to adversarial input in the network structure. The main idea of our
approach is to force the network to make predictions on what the given instance
of the class under consideration would look like and subsequently test those
predictions. By forcing the network to redraw the relevant parts of the image
and subsequently comparing this new image to the original, we are having the
network give a "proof" of the presence of the object.
</summary>
    <author>
      <name>Harm Berntsen</name>
    </author>
    <author>
      <name>Wouter Kuijper</name>
    </author>
    <author>
      <name>Tom Heskes</name>
    </author>
    <link href="http://arxiv.org/abs/1604.04428v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04428v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.06217v2</id>
    <updated>2017-09-12T22:14:36Z</updated>
    <published>2017-03-17T23:52:14Z</published>
    <title>Deciding How to Decide: Dynamic Routing in Artificial Neural Networks</title>
    <summary>  We propose and systematically evaluate three strategies for training
dynamically-routed artificial neural networks: graphs of learned
transformations through which different input signals may take different paths.
Though some approaches have advantages over others, the resulting networks are
often qualitatively similar. We find that, in dynamically-routed networks
trained to classify images, layers and branches become specialized to process
distinct categories of images. Additionally, given a fixed computational
budget, dynamically-routed networks tend to perform better than comparable
statically-routed networks.
</summary>
    <author>
      <name>Mason McGill</name>
    </author>
    <author>
      <name>Pietro Perona</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2017. Code at https://github.com/MasonMcGill/multipath-nn Video
  abstract at https://youtu.be/NHQsDaycwyQ</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.06217v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.06217v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0204054v1</id>
    <updated>2002-04-02T13:17:58Z</updated>
    <published>2002-04-02T13:17:58Z</published>
    <title>Theory of Interacting Neural Networks</title>
    <summary>  In this contribution we give an overview over recent work on the theory of
interacting neural networks. The model is defined in Section 2. The typical
teacher/student scenario is considered in Section 3. A static teacher network
is presenting training examples for an adaptive student network. In the case of
multilayer networks, the student shows a transition from a symmetric state to
specialisation. Neural networks can also generate a time series. Training on
time series and predicting it are studied in Section 4. When a network is
trained on its own output, it is interacting with itself. Such a scenario has
implications on the theory of prediction algorithms, as discussed in Section 5.
When a system of networks is trained on its minority decisions, it may be
considered as a model for competition in closed markets, see Section 6. In
Section 7 we consider two mutually interacting networks. A novel phenomenon is
observed: synchronisation by mutual learning. In Section 8 it is shown, how
this phenomenon can be applied to cryptography: Generation of a secret key over
a public channel.
</summary>
    <author>
      <name>Wolfgang Kinzel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Contribution to Networks, ed. by H.G. Schuster and S. Bornholdt, to
  be published by Wiley VCH</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0204054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0204054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0702055v1</id>
    <updated>2007-02-09T13:16:14Z</updated>
    <published>2007-02-09T13:16:14Z</published>
    <title>On the possibility of making the complete computer model of a human
  brain</title>
    <summary>  The development of the algorithm of a neural network building by the
corresponding parts of a DNA code is discussed.
</summary>
    <author>
      <name>A. V. Paraskevov</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0702055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0702055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.3161v1</id>
    <updated>2011-10-14T09:42:06Z</updated>
    <published>2011-10-14T09:42:06Z</published>
    <title>Intrinsic adaptation in autonomous recurrent neural networks</title>
    <summary>  A massively recurrent neural network responds on one side to input stimuli
and is autonomously active, on the other side, in the absence of sensory
inputs. Stimuli and information processing depends crucially on the qualia of
the autonomous-state dynamics of the ongoing neural activity. This default
neural activity may be dynamically structured in time and space, showing
regular, synchronized, bursting or chaotic activity patterns.
  We study the influence of non-synaptic plasticity on the default dynamical
state of recurrent neural networks. The non-synaptic adaption considered acts
on intrinsic neural parameters, such as the threshold and the gain, and is
driven by the optimization of the information entropy. We observe, in the
presence of the intrinsic adaptation processes, three distinct and globally
attracting dynamical regimes, a regular synchronized, an overall chaotic and an
intermittent bursting regime. The intermittent bursting regime is characterized
by intervals of regular flows, which are quite insensitive to external stimuli,
interseeded by chaotic bursts which respond sensitively to input signals. We
discuss these finding in the context of self-organized information processing
and critical brain dynamics.
</summary>
    <author>
      <name>Dimitrije Markovic</name>
    </author>
    <author>
      <name>Claudius Gros</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/NECO_a_00232</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/NECO_a_00232" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 8 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Computation February 2012, Vol. 24, No. 2: 523-540</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1110.3161v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.3161v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.7136v1</id>
    <updated>2014-02-28T05:34:36Z</updated>
    <published>2014-02-28T05:34:36Z</published>
    <title>Neural Network Approach to Railway Stand Lateral Skew Control</title>
    <summary>  The paper presents a study of an adaptive approach to lateral skew control
for an experimental railway stand. The preliminary experiments with the real
experimental railway stand and simulations with its 3-D mechanical model,
indicates difficulties of model-based control of the device. Thus, use of
neural networks for identification and control of lateral skew shall be
investigated. This paper focuses on real-data based modeling of the railway
stand by various neural network models, i.e; linear neural unit and quadratic
neural unit architectures. Furthermore, training methods of these neural
architectures as such, real-time-recurrent-learning and a variation of
back-propagation-through-time are examined, accompanied by a discussion of the
produced experimental results.
</summary>
    <author>
      <name>Peter Mark Benes</name>
    </author>
    <author>
      <name>Ivo Bukovsky</name>
    </author>
    <author>
      <name>Matous Cejnek</name>
    </author>
    <author>
      <name>Jan Kalivoda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">P. M. Benes et al., "Neural Network Approach to Railway Stand Lateral
  Skew Control" in Computer Science &amp; Information Technology (CS&amp; IT), Sydney,
  NSW, Australia, AIRCC, 2014, pp. 327-339</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.7136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.7136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.06046v1</id>
    <updated>2018-01-18T14:40:27Z</updated>
    <published>2018-01-18T14:40:27Z</published>
    <title>Conditions for traveling waves in spiking neural networks</title>
    <summary>  Spatiotemporal patterns such as traveling waves are frequently observed in
recordings of neural activity. The mechanisms underlying the generation of such
patterns are largely unknown. Previous studies have investigated the existence
and uniqueness of different types of waves or bumps of activity using
neural-field models, phenomenological coarse-grained descriptions of
neural-network dynamics. But it remains unclear how these insights can be
transferred to more biologically realistic networks of spiking neurons, where
individual neurons fire irregularly. Here, we employ mean-field theory to
reduce a microscopic model of leaky integrate-and-fire (LIF) neurons with
distance-dependent connectivity to an effective neural-field model. In contrast
to existing phenomenological descriptions, the dynamics in this neural-field
model depends on the mean and the variance in the synaptic input, both
determining the amplitude and the temporal structure of the resulting effective
coupling kernel. For the neural-field model we derive conditions for the
existence of spatial and temporal oscillations and periodic traveling waves
using linear stability analysis. We first prove that periodic traveling waves
cannot occur in a single homogeneous population of neurons, irrespective of the
form of distance dependence of the connection probability. Compatible with the
architecture of cortical neural networks, traveling waves emerge in
two-population networks of excitatory and inhibitory neurons as a combination
of delay-induced temporal oscillations and spatial oscillations due to
distance-dependent connectivity profiles. Finally, we demonstrate quantitative
agreement between predictions of the analytically tractable neural-field model
and numerical simulations of both networks of nonlinear rate-based units and
networks of LIF neurons.
</summary>
    <author>
      <name>Johanna Senk</name>
    </author>
    <author>
      <name>Karol√≠na Korvasov√°</name>
    </author>
    <author>
      <name>Jannis Schuecker</name>
    </author>
    <author>
      <name>Espen Hagen</name>
    </author>
    <author>
      <name>Tom Tetzlaff</name>
    </author>
    <author>
      <name>Markus Diesmann</name>
    </author>
    <author>
      <name>Moritz Helias</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.06046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.06046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/adap-org/9411003v1</id>
    <updated>1994-11-30T11:22:32Z</updated>
    <published>1994-11-30T11:22:32Z</published>
    <title>An Evolutionary Approach to Associative Memory in Recurrent Neural
  Networks</title>
    <summary>  In this paper, we investigate the associative memory in recurrent neural
networks, based on the model of evolving neural networks proposed by Nolfi,
Miglino and Parisi. Experimentally developed network has highly asymmetric
synaptic weights and dilute connections, quite different from those of the
Hopfield model. Some results on the effect of learning efficiency on the
evolution are also presented.
</summary>
    <author>
      <name>Sh. Fujita</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Kobe University</arxiv:affiliation>
    </author>
    <author>
      <name>H. Nishimura</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Hyogo University of Education</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, compressed and uuencoded postscript file</arxiv:comment>
    <link href="http://arxiv.org/abs/adap-org/9411003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/adap-org/9411003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="adap-org" scheme="http://arxiv.org/schemas/atom"/>
    <category term="adap-org" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9712132v1</id>
    <updated>1997-12-12T08:05:55Z</updated>
    <published>1997-12-12T08:05:55Z</published>
    <title>Threshold Noise as a Source of Volatility in Random Synchronous
  Asymmetric Neural Networks</title>
    <summary>  We study the diversity of complex spatio-temporal patterns of random
synchronous asymmetric neural networks (RSANNs). Specifically, we investigate
the impact of noisy thresholds on network performance and find that there is a
narrow and interesting region of noise parameters where RSANNs display specific
features of behavior desired for rapidly `thinking' systems: accessibility to a
large set of distinct, complex patterns.
</summary>
    <author>
      <name>Henrik Bohr</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Arizona</arxiv:affiliation>
    </author>
    <author>
      <name>Patrick McGuire</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Arizona</arxiv:affiliation>
    </author>
    <author>
      <name>Chris Pershing</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Arizona</arxiv:affiliation>
    </author>
    <author>
      <name>Johann Rafelski</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Arizona</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 11 figures, submitted to Neural Computation</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9712132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9712132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="adap-org" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9803386v1</id>
    <updated>1998-03-31T15:19:11Z</updated>
    <published>1998-03-31T15:19:11Z</published>
    <title>Finite Size Effects in Separable Recurrent Neural Networks</title>
    <summary>  We perform a systematic analytical study of finite size effects in separable
recurrent neural network models with sequential dynamics, away from saturation.
We find two types of finite size effects: thermal fluctuations, and
disorder-induced `frozen' corrections to the mean-field laws. The finite size
effects are described by equations that correspond to a time-dependent
Ornstein-Uhlenbeck process. We show how the theory can be used to understand
and quantify various finite size phenomena in recurrent neural networks, with
and without detailed balance.
</summary>
    <author>
      <name>A. Castellanos</name>
    </author>
    <author>
      <name>A. C. C. Coolen</name>
    </author>
    <author>
      <name>L. Viana</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/0305-4470/31/31/009</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/0305-4470/31/31/009" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages LaTex, with 4 postscript figures included</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9803386v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9803386v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0103092v1</id>
    <updated>2001-03-05T04:34:04Z</updated>
    <published>2001-03-05T04:34:04Z</published>
    <title>Reconstructing signal from fiber-optic measuring system with non-linear
  perceptron</title>
    <summary>  A computer model of the feed-forward neural network with the hidden layer is
developed to reconstruct physical field investigated by the fiber-optic
measuring system. The Gaussian distributions of some physical quantity are
selected as learning patterns. Neural network is learned by error
back-propagation using the conjugate gradient and coordinate descent
minimization of deviation. Learned neural network reconstructs the
two-dimensional scalar physical field with distribution having one or two
Gaussian peaks.
</summary>
    <author>
      <name>A. V. Panov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, Latex, 1 postscript figure</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0103092v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0103092v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0103585v2</id>
    <updated>2001-04-10T16:25:45Z</updated>
    <published>2001-03-28T15:33:40Z</published>
    <title>On the equivalence of the Ashkin-Teller and the four-state Potts-glass
  models of neural networks</title>
    <summary>  We show that for a particular choice of the coupling parameters the
Ashkin-Teller spin-glass neural network model with the Hebb learning rule and
one condensed pattern yields the same thermodynamic properties as the
four-state anisotropic Potts-glass neural network model. This equivalence is
not seen at the level of the Hamiltonians.
</summary>
    <author>
      <name>D. Bolle'</name>
    </author>
    <author>
      <name>P. Kozlowski</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.64.067102</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.64.067102" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, revtex, additional arguments presented</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 64, 067102 (2001)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0103585v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0103585v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0412023v1</id>
    <updated>2004-12-06T20:23:15Z</updated>
    <published>2004-12-06T20:23:15Z</published>
    <title>Multidimensional data classification with artificial neural networks</title>
    <summary>  Multi-dimensional data classification is an important and challenging problem
in many astro-particle experiments. Neural networks have proved to be versatile
and robust in multi-dimensional data classification. In this article we shall
study the classification of gamma from the hadrons for the MAGIC Experiment.
Two neural networks have been used for the classification task. One is
Multi-Layer Perceptron based on supervised learning and other is
Self-Organising Map (SOM), which is based on unsupervised learning technique.
The results have been shown and the possible ways of combining these networks
have been proposed to yield better and faster classification results.
</summary>
    <author>
      <name>P. Boinee</name>
    </author>
    <author>
      <name>F. Barbarino</name>
    </author>
    <author>
      <name>A. De Angelis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures, Submitted to EURASIP Journal on Applied Signal
  Processing, 2004</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0412023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0412023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; K.3.2; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/hep-ph/9607460v1</id>
    <updated>1996-07-29T16:07:20Z</updated>
    <published>1996-07-29T16:07:20Z</published>
    <title>Neural network analysis for gamma gamma -&gt; 3pi at Daphne</title>
    <summary>  We consider the possibility of using neural networks in experimental data
analysis in Daphne. We analyze the process $\gamma\gamma\to \pi^+ \pi^- \pi^0$
and its backgrounds using neural networks and we compare their performances
with traditional methods of applying cuts on several kinematical variables. We
find that the neural networks are more efficient and can be of great help for
processes with small number of produced events.
</summary>
    <author>
      <name>Ll. Ametller</name>
    </author>
    <author>
      <name>Ll. Garrido</name>
    </author>
    <author>
      <name>P. Talavera</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/S0370-2693(97)00124-X</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/S0370-2693(97)00124-X" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, latex, 2 figures.</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys.Lett. B396 (1997) 280-286</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/hep-ph/9607460v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/hep-ph/9607460v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/hep-ph/0607122v1</id>
    <updated>2006-07-11T17:16:35Z</updated>
    <published>2006-07-11T17:16:35Z</published>
    <title>The neural network approach to parton distribution functions</title>
    <summary>  We introduce the neural network approach to the parametrization of parton
distributions. After a general introduction, we present in detail our approach
to parametrize experimental data, based on a combination of Monte Carlo methods
and neural networks. We apply this strategy first in three different cases: the
proton structure function, hadronic tau decays and B meson decay spectra.
Finally we describe the neural network approach applied to the parametrization
of parton distribution functions, and present results on the nonsinglet parton
distribution.
</summary>
    <author>
      <name>Joan Rojo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Ph. D. Thesis, 163 pages, version with higher resolution figures
  available from the following website:
  http://www.ecm.ub.es/~joanrojo/thesis.pdf</arxiv:comment>
    <link href="http://arxiv.org/abs/hep-ph/0607122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/hep-ph/0607122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/nlin/0003003v1</id>
    <updated>2000-03-01T11:38:02Z</updated>
    <published>2000-03-01T11:38:02Z</published>
    <title>Coherent Response in a Chaotic Neural Network</title>
    <summary>  We set up a signal-driven scheme of the chaotic neural network with the
coupling constants corresponding to certain information, and investigate the
stochastic resonance-like effects under its deterministic dynamics, comparing
with the conventional case of Hopfield network with stochastic noise. It is
shown that the chaotic neural network can enhance weak subthreshold signals and
have higher coherence abilities between stimulus and response than those
attained by the conventional stochastic model.
</summary>
    <author>
      <name>Haruhiko Nishimura</name>
    </author>
    <author>
      <name>Naofumi Katada</name>
    </author>
    <author>
      <name>Kazuyuki Aihara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures, to be published in Neural Processing Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/nlin/0003003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/nlin/0003003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/nlin/0609014v1</id>
    <updated>2006-09-06T10:39:30Z</updated>
    <published>2006-09-06T10:39:30Z</published>
    <title>Multilayered feed forward Artificial Neural Network model to predict the
  average summer-monsoon rainfall in India</title>
    <summary>  In the present research, possibility of predicting average summer-monsoon
rainfall over India has been analyzed through Artificial Neural Network models.
In formulating the Artificial Neural Network based predictive model, three
layered networks have been constructed with sigmoid non-linearity. The models
under study are different in the number of hidden neurons. After a thorough
training and test procedure, neural net with three nodes in the hidden layer is
found to be the best predictive model.
</summary>
    <author>
      <name>Surajit Chattopadhyay</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2478/s11600-007-0020-8</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2478/s11600-007-0020-8" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 1 table, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/nlin/0609014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/nlin/0609014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/physics/9712035v1</id>
    <updated>1997-12-17T17:12:29Z</updated>
    <published>1997-12-17T17:12:29Z</published>
    <title>Characteristic functions and process identification by neural networks</title>
    <summary>  Principal component analysis (PCA) algorithms use neural networks to extract
the eigenvectors of the correlation matrix from the data. However, if the
process is non-Gaussian, PCA algorithms or their higher order generalisations
provide only incomplete or misleading information on the statistical properties
of the data. To handle such situations we propose neural network algorithms,
with an hybrid (supervised and unsupervised) learning scheme, which constructs
the characteristic function of the probability distribution and the transition
functions of the stochastic process. Illustrative examples are presented, which
include Cauchy and Levy-type processes
</summary>
    <author>
      <name>Joaquim A. Dente</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Laboratorio de Mecatronica, DEEC, IST, Lisboa, Portugal</arxiv:affiliation>
    </author>
    <author>
      <name>R. Vilela Mendes</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Laboratorio de Mecatronica, DEEC, IST, Lisboa, Portugal</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages Latex, 12 figures in a combined ps-file</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Networks,10 (1997) 1465-1471</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/physics/9712035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/physics/9712035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/physics/0703041v1</id>
    <updated>2007-03-05T07:18:44Z</updated>
    <published>2007-03-05T07:18:44Z</published>
    <title>A Hardware Implementation of Artificial Neural Network Using Field
  Programmable Gate Arrays</title>
    <summary>  An artificial neural network algorithm is implemented using a field
programmable gate array hardware. One hidden layer is used in the feed-forward
neural network structure in order to discriminate one class of patterns from
the other class in real time. With five 8-bit input patterns, six hidden nodes,
and one 8-bit output, the implemented hardware neural network makes decision on
a set of input patterns in 11 clocks and the result is identical to what to
expect from off-line computation. This implementation may be used in level 1
hardware triggers in high energy physics experiments
</summary>
    <author>
      <name>E. Won</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.nima.2007.08.163</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.nima.2007.08.163" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 4 figures, submitted to Nucl. Instr. Meth. A</arxiv:comment>
    <link href="http://arxiv.org/abs/physics/0703041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/physics/0703041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/quant-ph/0408148v2</id>
    <updated>2004-08-26T01:33:47Z</updated>
    <published>2004-08-23T22:55:59Z</published>
    <title>Indispensable Role of Quantum Theory in the Brain Dynamics</title>
    <summary>  Recently, Tegmark pointed out that the superposition of ion states involved
in the superposition of firing and resting states of a neuron quickly decohere.
It undoubtedly indicates that neural networks cannot work as quantum computers,
or computers taking advantage of coherent states. Does it also mean that the
brain can be modeled as a neural network obeying classical physics? Here we
show that it does not mean that the brain can be modeled as a neural network
obeying classical physics. A brand new perspective in research of neural
networks from quantum theoretical aspect is presented.
</summary>
    <author>
      <name>Yukinari Kurita</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15pages</arxiv:comment>
    <link href="http://arxiv.org/abs/quant-ph/0408148v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/quant-ph/0408148v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.1214v1</id>
    <updated>2007-05-09T07:08:58Z</updated>
    <published>2007-05-09T07:08:58Z</published>
    <title>Control of Complex Systems Using Bayesian Networks and Genetic Algorithm</title>
    <summary>  A method based on Bayesian neural networks and genetic algorithm is proposed
to control the fermentation process. The relationship between input and output
variables is modelled using Bayesian neural network that is trained using
hybrid Monte Carlo method. A feedback loop based on genetic algorithm is used
to change input variables so that the output variables are as close to the
desired target as possible without the loss of confidence level on the
prediction that the neural network gives. The proposed procedure is found to
reduce the distance between the desired target and measured outputs
significantly.
</summary>
    <author>
      <name>Tshilidzi Marwala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0705.1214v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.1214v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.2265v1</id>
    <updated>2010-02-11T03:35:38Z</updated>
    <published>2010-02-11T03:35:38Z</published>
    <title>Sequential optimizing investing strategy with neural networks</title>
    <summary>  In this paper we propose an investing strategy based on neural network models
combined with ideas from game-theoretic probability of Shafer and Vovk. Our
proposed strategy uses parameter values of a neural network with the best
performance until the previous round (trading day) for deciding the investment
in the current round. We compare performance of our proposed strategy with
various strategies including a strategy based on supervised neural network
models and show that our procedure is competitive with other strategies.
</summary>
    <author>
      <name>Ryo Adachi</name>
    </author>
    <author>
      <name>Akimichi Takemura</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Expert Systems with Applications 38 (2011) 12991-12998</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1002.2265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.2265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.2406v1</id>
    <updated>2010-09-13T14:35:41Z</updated>
    <published>2010-09-13T14:35:41Z</published>
    <title>Adaptation of the neural network-based IDS to new attacks detection</title>
    <summary>  In this paper we report our experiment concerning new attacks detection by a
neural network-based Intrusion Detection System. What is crucial for this topic
is the adaptation of the neural network that is already in use to correct
classification of a new "normal traffic" and of an attack representation not
presented during the network training process. When it comes to the new attack
it should also be easy to obtain vectors to test and to retrain the neural
classifier. We describe the proposal of an algorithm and a distributed IDS
architecture that could achieve the goals mentioned above.
</summary>
    <author>
      <name>Przemyslaw Kukielka</name>
    </author>
    <author>
      <name>Zbigniew Kotulski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1009.2406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.2406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.0265v1</id>
    <updated>2014-07-01T14:50:36Z</updated>
    <published>2014-07-01T14:50:36Z</published>
    <title>Supervised learning in Spiking Neural Networks with Limited Precision:
  SNN/LP</title>
    <summary>  A new supervised learning algorithm, SNN/LP, is proposed for Spiking Neural
Networks. This novel algorithm uses limited precision for both synaptic weights
and synaptic delays; 3 bits in each case. Also a genetic algorithm is used for
the supervised training. The results are comparable or better than previously
published work. The results are applicable to the realization of large scale
hardware neural networks. One of the trained networks is implemented in
programmable hardware.
</summary>
    <author>
      <name>Evangelos Stromatias</name>
    </author>
    <author>
      <name>John Marsland</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, originally submitted to IJCNN 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.0265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.0265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.2626v1</id>
    <updated>2011-10-12T10:56:29Z</updated>
    <published>2011-10-12T10:56:29Z</published>
    <title>Analysis of Heart Diseases Dataset using Neural Network Approach</title>
    <summary>  One of the important techniques of Data mining is Classification. Many real
world problems in various fields such as business, science, industry and
medicine can be solved by using classification approach. Neural Networks have
emerged as an important tool for classification. The advantages of Neural
Networks helps for efficient classification of given data. In this study a
Heart diseases dataset is analyzed using Neural Network approach. To increase
the efficiency of the classification process parallel approach is also adopted
in the training phase.
</summary>
    <author>
      <name>K. Usha Rani</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijdkp.2011.1501</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijdkp.2011.1501" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures, 1 table; International Journal of Data Mining &amp;
  Knowledge Management Process (IJDKP) Vol.1, No.5, September 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.2626v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.2626v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.6185v1</id>
    <updated>2011-06-30T11:08:05Z</updated>
    <published>2011-06-30T11:08:05Z</published>
    <title>Effects of Compensation, Connectivity and Tau in a Computational Model
  of Alzheimer's Disease</title>
    <summary>  This work updates an existing, simplistic computational model of Alzheimer's
Disease (AD) to investigate the behaviour of synaptic compensatory mechanisms
in neural networks with small-world connectivity, and varying methods of
calculating compensation. It additionally introduces a method for simulating
tau neurofibrillary pathology, resulting in a more dramatic damage profile.
Small-world connectivity is shown to have contrasting effects on capacity,
retrieval time, and robustness to damage, whilst the use of more
easily-obtained remote memories rather than recent memories for synaptic
compensation is found to lead to rapid network damage.
</summary>
    <author>
      <name>Mark Rowan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, submitted to International Joint Conference on Neural
  Networks 2011</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The 2011 International Joint Conference on Neural Networks
  (IJCNN), (2011) 543--550</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1106.6185v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.6185v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.2013v1</id>
    <updated>2013-11-08T16:14:14Z</updated>
    <published>2013-11-08T16:14:14Z</published>
    <title>Google matrix analysis of C.elegans neural network</title>
    <summary>  We study the structural properties of the neural network of the C.elegans
(worm) from a directed graph point of view. The Google matrix analysis is used
to characterize the neuron connectivity structure and node classifications are
discussed and compared with physiological properties of the cells. Our results
are obtained by a proper definition of neural directed network and subsequent
eigenvector analysis which recovers some results of previous studies. Our
analysis highlights particular sets of important neurons constituting the core
of the neural system. The applications of PageRank, CheiRank and ImpactRank to
characterization of interdependency of neurons are discussed.
</summary>
    <author>
      <name>Vivek Kandiah</name>
    </author>
    <author>
      <name>Dima L. Shepelyansky</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physleta.2014.04.045</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physleta.2014.04.045" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Additional information on the webpage
  http://www.quantware.ups-tlse.fr/QWLIB/wormgooglematrix/index.html</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.2013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.2013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.04899v2</id>
    <updated>2015-08-21T01:09:51Z</updated>
    <published>2015-08-20T07:17:25Z</published>
    <title>Sparse selection of bases in neural-network potential for crystalline
  and liquid Si</title>
    <summary>  The neural-network interatomic potential for crystalline and liquid Si has
been developed using the forward stepwise regression technique to reduce the
number of bases with keeping the accuracy of the potential. This approach of
making the neural-network potential enables us to construct the accurate
interatomic potentials with less and important bases selected systematically
and less heuristically. The evaluation of bulk crystalline properties, and
dynamic properties of liquid Si show good agreements between the neural-network
potential and ab-initio results.
</summary>
    <author>
      <name>Ryo Kobayashi</name>
    </author>
    <author>
      <name>Tomoyuki Tamura</name>
    </author>
    <author>
      <name>Ichiro Takeuchi</name>
    </author>
    <author>
      <name>Shuji Ogata</name>
    </author>
    <link href="http://arxiv.org/abs/1508.04899v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.04899v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.07385v3</id>
    <updated>2016-03-28T13:46:06Z</updated>
    <published>2015-09-24T14:20:29Z</published>
    <title>Provable approximation properties for deep neural networks</title>
    <summary>  We discuss approximation of functions using deep neural nets. Given a
function $f$ on a $d$-dimensional manifold $\Gamma \subset \mathbb{R}^m$, we
construct a sparsely-connected depth-4 neural network and bound its error in
approximating $f$. The size of the network depends on dimension and curvature
of the manifold $\Gamma$, the complexity of $f$, in terms of its wavelet
description, and only weakly on the ambient dimension $m$. Essentially, our
network computes wavelet functions, which are computed from Rectified Linear
Units (ReLU)
</summary>
    <author>
      <name>Uri Shaham</name>
    </author>
    <author>
      <name>Alexander Cloninger</name>
    </author>
    <author>
      <name>Ronald R. Coifman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.acha.2016.04.003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.acha.2016.04.003" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted for publication in Applied and Computational Harmonic
  Analysis</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.07385v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.07385v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.00513v1</id>
    <updated>2015-11-02T14:23:22Z</updated>
    <published>2015-11-02T14:23:22Z</published>
    <title>Pixel-wise Segmentation of Street with Neural Networks</title>
    <summary>  Pixel-wise street segmentation of photographs taken from a drivers
perspective is important for self-driving cars and can also support other
object recognition tasks. A framework called SST was developed to examine the
accuracy and execution time of different neural networks. The best neural
network achieved an $F_1$-score of 89.5% with a simple feedforward neural
network which trained to solve a regression task.
</summary>
    <author>
      <name>Sebastian Bittel</name>
    </author>
    <author>
      <name>Vitali Kaiser</name>
    </author>
    <author>
      <name>Marvin Teichmann</name>
    </author>
    <author>
      <name>Martin Thoma</name>
    </author>
    <link href="http://arxiv.org/abs/1511.00513v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.00513v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.01705v4</id>
    <updated>2016-06-07T23:25:51Z</updated>
    <published>2016-01-07T21:21:59Z</published>
    <title>Learning to Compose Neural Networks for Question Answering</title>
    <summary>  We describe a question answering model that applies to both images and
structured knowledge bases. The model uses natural language strings to
automatically assemble neural networks from a collection of composable modules.
Parameters for these modules are learned jointly with network-assembly
parameters via reinforcement learning, with only (world, question, answer)
triples as supervision. Our approach, which we term a dynamic neural model
network, achieves state-of-the-art results on benchmark datasets in both visual
and structured domains.
</summary>
    <author>
      <name>Jacob Andreas</name>
    </author>
    <author>
      <name>Marcus Rohrbach</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <author>
      <name>Dan Klein</name>
    </author>
    <link href="http://arxiv.org/abs/1601.01705v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.01705v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.05744v1</id>
    <updated>2016-10-18T19:03:02Z</updated>
    <published>2016-10-18T19:03:02Z</published>
    <title>A neural network approach to predicting and computing knot invariants</title>
    <summary>  In this paper we use artificial neural networks to predict and help compute
the values of certain knot invariants. In particular, we show that neural
networks are able to predict when a knot is quasipositive with a high degree of
accuracy. Given a knot with unknown quasipositivity we use these predictions to
identify braid representatives that are likely to be quasipositive, which we
then subject to further testing to verify. Using these techniques we identify
84 new quasipositive 11 and 12-crossing knots. Furthermore, we show that neural
networks are also able to predict and help compute the slice genus and
Ozsv\'{a}th-Szab\'{o} $\tau$-invariant of knots.
</summary>
    <author>
      <name>Mark C. Hughes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 1 figure, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.05744v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.05744v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="57M25, 57M27" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.06151v1</id>
    <updated>2016-10-19T18:49:50Z</updated>
    <published>2016-10-19T18:49:50Z</published>
    <title>Neural Networks for Modeling and Control of Particle Accelerators</title>
    <summary>  We describe some of the challenges of particle accelerator control, highlight
recent advances in neural network techniques, discuss some promising avenues
for incorporating neural networks into particle accelerator control systems,
and describe a neural network-based control system that is being developed for
resonance control of an RF electron gun at the Fermilab Accelerator Science and
Technology (FAST) facility, including initial experimental results from a
benchmark controller.
</summary>
    <author>
      <name>A. L. Edelen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Colorado State U.</arxiv:affiliation>
    </author>
    <author>
      <name>S. G. Biedron</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Colorado State U. and Ljubljana U.</arxiv:affiliation>
    </author>
    <author>
      <name>B. E. Chase</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Fermilab</arxiv:affiliation>
    </author>
    <author>
      <name>D. Edstrom</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Fermilab</arxiv:affiliation>
    </author>
    <author>
      <name>S. V. Milton</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Colorado State U.</arxiv:affiliation>
    </author>
    <author>
      <name>P. Stabile</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Geneva, ADAM</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TNS.2016.2543203</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TNS.2016.2543203" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pp</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Nuclear Science, Volume: 63, Issue: 2, 20
  April 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.06151v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.06151v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.acc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.acc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.06665v4</id>
    <updated>2017-04-21T00:00:05Z</updated>
    <published>2016-11-21T06:23:50Z</published>
    <title>A New System of Global Fractional-order Interval Implicit Projection
  Neural Networks</title>
    <summary>  The purpose of this paper is to introduce and investigate a new system of
global fractional-order interval implicit projection neural networks. An
existence and uniqueness theorem of the equilibrium point for such kind of
global fractional-order interval implicit projection neural networks is
obtained under some suitable assumptions. Moreover, Mittag-Leffler stability of
the global fractional-order interval implicit projection neural networks is
also proved. Finally, two numerical examples are given to illustrate the
validity of our results.
</summary>
    <author>
      <name>Zeng-bao Wu</name>
    </author>
    <author>
      <name>Jin-dong Li</name>
    </author>
    <author>
      <name>Nan-jing Huang</name>
    </author>
    <link href="http://arxiv.org/abs/1611.06665v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.06665v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.07136v1</id>
    <updated>2016-11-22T03:21:05Z</updated>
    <published>2016-11-22T03:21:05Z</published>
    <title>Cascaded Neural Networks with Selective Classifiers and its evaluation
  using Lung X-ray CT Images</title>
    <summary>  Lung nodule detection is a class imbalanced problem because nodules are found
with much lower frequency than non-nodules. In the class imbalanced problem,
conventional classifiers tend to be overwhelmed by the majority class and
ignore the minority class. We therefore propose cascaded convolutional neural
networks to cope with the class imbalanced problem. In the proposed approach,
cascaded convolutional neural networks that perform as selective classifiers
filter out obvious non-nodules. Successively, a convolutional neural network
trained with a balanced data set calculates nodule probabilities. The proposed
method achieved the detection sensitivity of 85.3% and 90.7% at 1 and 4 false
positives per scan in FROC curve, respectively.
</summary>
    <author>
      <name>Masaharu Sakamoto</name>
    </author>
    <author>
      <name>Hiroki Nakano</name>
    </author>
    <link href="http://arxiv.org/abs/1611.07136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.07136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.07324v1</id>
    <updated>2017-02-23T18:19:35Z</updated>
    <published>2017-02-23T18:19:35Z</published>
    <title>Inherent Biases of Recurrent Neural Networks for Phonological
  Assimilation and Dissimilation</title>
    <summary>  A recurrent neural network model of phonological pattern learning is
proposed. The model is a relatively simple neural network with one recurrent
layer, and displays biases in learning that mimic observed biases in human
learning. Single-feature patterns are learned faster than two-feature patterns,
and vowel or consonant-only patterns are learned faster than patterns involving
vowels and consonants, mimicking the results of laboratory learning
experiments. In non-recurrent models, capturing these biases requires the use
of alpha features or some other representation of repeated features, but with a
recurrent neural network, these elaborations are not necessary.
</summary>
    <author>
      <name>Amanda Doucette</name>
    </author>
    <link href="http://arxiv.org/abs/1702.07324v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.07324v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.01961v2</id>
    <updated>2017-06-12T21:05:58Z</updated>
    <published>2017-03-06T16:39:16Z</published>
    <title>Multiplicative Normalizing Flows for Variational Bayesian Neural
  Networks</title>
    <summary>  We reinterpret multiplicative noise in neural networks as auxiliary random
variables that augment the approximate posterior in a variational setting for
Bayesian neural networks. We show that through this interpretation it is both
efficient and straightforward to improve the approximation by employing
normalizing flows while still allowing for local reparametrizations and a
tractable lower bound. In experiments we show that with this new approximation
we can significantly improve upon classical mean field for Bayesian neural
networks on both predictive accuracy as well as predictive uncertainty.
</summary>
    <author>
      <name>Christos Louizos</name>
    </author>
    <author>
      <name>Max Welling</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appearing at the International Conference on Machine Learning (ICML)
  2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.01961v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.01961v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.03360v1</id>
    <updated>2017-05-09T14:43:52Z</updated>
    <published>2017-05-09T14:43:52Z</published>
    <title>Skin lesion detection based on an ensemble of deep convolutional neural
  network</title>
    <summary>  Skin cancer is a major public health problem, with over 5 million newly
diagnosed cases in the United States each year. Melanoma is the deadliest form
of skin cancer, responsible for over 9,000 deaths each year. In this paper, we
propose an ensemble of deep convolutional neural networks to classify
dermoscopy images into three classes. To achieve the highest classification
accuracy, we fuse the outputs of the softmax layers of four different neural
architectures. For aggregation, we consider the individual accuracies of the
networks weighted by the confidence values provided by their final softmax
layers. This fusion-based approach outperformed all the individual neural
networks regarding classification accuracy.
</summary>
    <author>
      <name>Balazs Harangi</name>
    </author>
    <link href="http://arxiv.org/abs/1705.03360v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.03360v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09723v1</id>
    <updated>2017-07-31T05:00:32Z</updated>
    <published>2017-07-31T05:00:32Z</published>
    <title>Solving the Bose-Hubbard model with machine learning</title>
    <summary>  Motivated by the recent successful application of artificial neural networks
to quantum many-body problems [G. Carleo and M. Troyer, Science {\bf 355}, 602
(2017)], a method to calculate the ground state of the Bose-Hubbard model using
a feedforward neural network is proposed. The results are in good agreement
with those obtained by exact diagonalization and the Gutzwiller approximation.
The method of neural-network quantum states is promising for solving quantum
many-body problems of ultracold atoms in optical lattices.
</summary>
    <author>
      <name>Hiroki Saito</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.7566/JPSJ.86.093001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.7566/JPSJ.86.093001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J. Phys. Soc. Jpn. 86, 093001 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1707.09723v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09723v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.quant-gas" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09775v1</id>
    <updated>2017-07-31T09:14:14Z</updated>
    <published>2017-07-31T09:14:14Z</published>
    <title>Capacity limitations of visual search in deep convolutional neural
  network</title>
    <summary>  Deep convolutional neural networks follow roughly the architecture of
biological visual systems, and have shown a performance comparable to human
observers in object recognition tasks. In this study, I test a pre-trained deep
neural network in some classic visual search tasks. The results reveal a
qualitative difference from human performance. It appears that there is no
difference between searches for simple features that pop out in experiments
with humans, and for feature configurations that exhibit strict capacity
limitations in human vision. Both types of stimuli reveal moderate capacity
limitations in the neural network tested here.
</summary>
    <author>
      <name>Endel Poder</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.09775v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09775v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.00077v1</id>
    <updated>2017-07-31T21:33:42Z</updated>
    <published>2017-07-31T21:33:42Z</published>
    <title>Bayesian Sparsification of Recurrent Neural Networks</title>
    <summary>  Recurrent neural networks show state-of-the-art results in many text analysis
tasks but often require a lot of memory to store their weights. Recently
proposed Sparse Variational Dropout eliminates the majority of the weights in a
feed-forward neural network without significant loss of quality. We apply this
technique to sparsify recurrent neural networks. To account for recurrent
specifics we also rely on Binary Variational Dropout for RNN. We report 99.5%
sparsity level on sentiment analysis task without a quality drop and up to 87%
sparsity level on language modeling task with slight loss of accuracy.
</summary>
    <author>
      <name>Ekaterina Lobacheva</name>
    </author>
    <author>
      <name>Nadezhda Chirkova</name>
    </author>
    <author>
      <name>Dmitry Vetrov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Workshop on Learning to Generate Natural Language, ICML,
  2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.00077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.00129v1</id>
    <updated>2017-08-01T02:09:12Z</updated>
    <published>2017-08-01T02:09:12Z</published>
    <title>Deep Generative Adversarial Neural Networks for Realistic Prostate
  Lesion MRI Synthesis</title>
    <summary>  Generative Adversarial Neural Networks (GANs) are applied to the synthetic
generation of prostate lesion MRI images. GANs have been applied to a variety
of natural images, is shown show that the same techniques can be used in the
medical domain to create realistic looking synthetic lesion images. 16mm x 16mm
patches are extracted from 330 MRI scans from the SPIE ProstateX Challenge 2016
and used to train a Deep Convolutional Generative Adversarial Neural Network
(DCGAN) utilizing cutting edge techniques. Synthetic outputs are compared to
real images and the implicit latent representations induced by the GAN are
explored. Training techniques and successful neural network architectures are
explained in detail.
</summary>
    <author>
      <name>Andy Kitchen</name>
    </author>
    <author>
      <name>Jarrel Seah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.00129v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00129v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.10; I.4.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.07908v1</id>
    <updated>2017-09-20T20:45:53Z</updated>
    <published>2017-09-20T20:45:53Z</published>
    <title>Neural Network Alternatives to Convolutive Audio Models for Source
  Separation</title>
    <summary>  Convolutive Non-Negative Matrix Factorization model factorizes a given audio
spectrogram using frequency templates with a temporal dimension. In this paper,
we present a convolutional auto-encoder model that acts as a neural network
alternative to convolutive NMF. Using the modeling flexibility granted by
neural networks, we also explore the idea of using a Recurrent Neural Network
in the encoder. Experimental results on speech mixtures from TIMIT dataset
indicate that the convolutive architecture provides a significant improvement
in separation performance in terms of BSSeval metrics.
</summary>
    <author>
      <name>Shrikant Venkataramani</name>
    </author>
    <author>
      <name>Y. Cem Subakan</name>
    </author>
    <author>
      <name>Paris Smaragdis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in MLSP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.07908v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.07908v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.04724v2</id>
    <updated>2017-11-14T23:27:16Z</updated>
    <published>2017-10-12T21:27:34Z</published>
    <title>Training deep neural networks for the inverse design of nanophotonic
  structures</title>
    <summary>  Data inconsistency leads to a slow training process when deep neural networks
are used for the inverse design of photonic devices, an issue that arises from
the fundamental property of non-uniqueness in all inverse scattering problems.
Here we show that by combining forward modeling and inverse design in a tandem
architecture, one can overcome this fundamental issue, allowing deep neural
networks to be effectively trained by data sets that contain non-unique
electromagnetic scattering instances. This paves the way for using deep neural
networks to design complex photonic structures that requires large training
sets.
</summary>
    <author>
      <name>Dianjing Liu</name>
    </author>
    <author>
      <name>Yixuan Tan</name>
    </author>
    <author>
      <name>Zongfu Yu</name>
    </author>
    <link href="http://arxiv.org/abs/1710.04724v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.04724v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.app-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.04252v1</id>
    <updated>2017-11-12T09:01:35Z</updated>
    <published>2017-11-12T09:01:35Z</published>
    <title>Extracting Critical Exponent by Finite-Size Scaling with Convolutional
  Neural Networks</title>
    <summary>  Machine learning has been successfully applied to identify phases and phase
transitions in condensed matter systems. However, quantitative characterization
of the critical fluctuations near phase transitions is lacking. In this study
we extract the critical behavior of a quantum Hall plateau transition with a
convolutional neural network. We introduce a finite-size scaling approach and
show that the localization length critical exponent learned by the neural
network is consistent with the value obtained by conventional approaches. We
illustrate the physics behind the approach by a cross-examination of the
inverse participation ratios.
</summary>
    <author>
      <name>Zhenyu Li</name>
    </author>
    <author>
      <name>Mingxing Luo</name>
    </author>
    <author>
      <name>Xin Wan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.04252v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.04252v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.04606v1</id>
    <updated>2017-11-13T14:55:32Z</updated>
    <published>2017-11-13T14:55:32Z</published>
    <title>Provably efficient neural network representation for image
  classification</title>
    <summary>  The state-of-the-art approaches for image classification are based on neural
networks. Mathematically, the task of classifying images is equivalent to
finding the function that maps an image to the label it is associated with. To
rigorously establish the success of neural network methods, we should first
prove that the function has an efficient neural network representation, and
then design provably efficient training algorithms to find such a
representation. Here, we achieve the first goal based on a set of assumptions
about the patterns in the images. The validity of these assumptions is very
intuitive in many image classification problems, including but not limited to,
recognizing handwritten digits.
</summary>
    <author>
      <name>Yichen Huang</name>
    </author>
    <link href="http://arxiv.org/abs/1711.04606v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.04606v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.00720v1</id>
    <updated>2017-12-03T06:37:15Z</updated>
    <published>2017-12-03T06:37:15Z</published>
    <title>Automatic Recognition of Coal and Gangue based on Convolution Neural
  Network</title>
    <summary>  We designed a gangue sorting system,and built a convolutional neural network
model based on AlexNet. Data enhancement and transfer learning are used to
solve the problem which the convolution neural network has insufficient
training data in the training stage. An object detection and region clipping
algorithm is proposed to adjust the training image data to the optimum size.
Compared with traditional neural network and SVM algorithm, this algorithm has
higher recognition rate for coal and coal gangue, and provides important
reference for identification and separation of coal and gangue.
</summary>
    <author>
      <name>Huichao Hong</name>
    </author>
    <author>
      <name>Lixin Zheng</name>
    </author>
    <author>
      <name>Jianqing Zhu</name>
    </author>
    <author>
      <name>Shuwan Pan</name>
    </author>
    <author>
      <name>Kaiting Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/1712.00720v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.00720v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.05417v1</id>
    <updated>2018-01-16T22:00:40Z</updated>
    <published>2018-01-16T22:00:40Z</published>
    <title>Quantum Walk Inspired Neural Networks for Graph-Structured Data</title>
    <summary>  In recent years, along with the overwhelming advances in the field of neural
information processing, quantum information processing (QIP) has shown
significant progress in solving problems that are intractable on classical
computers. Quantum machine learning (QML) explores the ways in which these
fields can learn from one another. We propose quantum walk neural networks
(QWNN), a new graph neural network architecture based on quantum random walks,
the quantum parallel to classical random walks. A QWNN learns a quantum walk on
a graph to construct a diffusion operator which can be applied to a signal on a
graph. We demonstrate the use of the network for prediction tasks for graph
structured signals.
</summary>
    <author>
      <name>Stefan Dernbach</name>
    </author>
    <author>
      <name>Arman Mohseni-Kabir</name>
    </author>
    <author>
      <name>Don Towsley</name>
    </author>
    <author>
      <name>Siddharth Pal</name>
    </author>
    <link href="http://arxiv.org/abs/1801.05417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.05417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.02091v1</id>
    <updated>2018-02-06T17:46:32Z</updated>
    <published>2018-02-06T17:46:32Z</published>
    <title>Structural Recurrent Neural Network (SRNN) for Group Activity Analysis</title>
    <summary>  A group of persons can be analyzed at various semantic levels such as
individual actions, their interactions, and the activity of the entire group.
In this paper, we propose a structural recurrent neural network (SRNN) that
uses a series of interconnected RNNs to jointly capture the actions of
individuals, their interactions, as well as the group activity. While previous
structural recurrent neural networks assumed that the number of nodes and edges
is constant, we use a grid pooling layer to address the fact that the number of
individuals in a group can vary. We evaluate two variants of the structural
recurrent neural network on the Volleyball Dataset.
</summary>
    <author>
      <name>Sovan Biswas</name>
    </author>
    <author>
      <name>Juergen Gall</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in WACV 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.02091v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.02091v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.07384v1</id>
    <updated>2018-02-21T00:47:32Z</updated>
    <published>2018-02-21T00:47:32Z</published>
    <title>Interpreting Neural Network Judgments via Minimal, Stable, and Symbolic
  Corrections</title>
    <summary>  The paper describes a new algorithm to generate minimal, stable, and symbolic
corrections to an input that will cause a neural network with ReLU neurons to
change its output. We argue that such a correction is a useful way to provide
feedback to a user when the neural network produces an output that is different
from a desired output. Our algorithm generates such a correction by solving a
series of linear constraint satisfaction problems. The technique is evaluated
on a neural network that has been trained to predict whether an applicant will
pay a mortgage.
</summary>
    <author>
      <name>Xin Zhang</name>
    </author>
    <author>
      <name>Armando Solar-Lezama</name>
    </author>
    <author>
      <name>Rishabh Singh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.07384v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.07384v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.10275v1</id>
    <updated>2018-02-28T06:16:33Z</updated>
    <published>2018-02-28T06:16:33Z</published>
    <title>Solving for high dimensional committor functions using artificial neural
  networks</title>
    <summary>  In this note we propose a method based on artificial neural network to study
the transition between states governed by stochastic processes. In particular,
we aim for numerical schemes for the committor function, the central object of
transition path theory, which satisfies a high-dimensional Fokker-Planck
equation. By working with the variational formulation of such partial
differential equation and parameterizing the committor function in terms of a
neural network, approximations can be obtained via optimizing the neural
network weights using stochastic algorithms. The numerical examples show that
moderate accuracy can be achieved for high-dimensional problems.
</summary>
    <author>
      <name>Yuehaw Khoo</name>
    </author>
    <author>
      <name>Jianfeng Lu</name>
    </author>
    <author>
      <name>Lexing Ying</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.10275v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.10275v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65Nxx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.00885v1</id>
    <updated>2018-03-02T15:22:10Z</updated>
    <published>2018-03-02T15:22:10Z</published>
    <title>Essentially No Barriers in Neural Network Energy Landscape</title>
    <summary>  Training neural networks involves finding minima of a high-dimensional
non-convex loss function. Knowledge of the structure of this energy landscape
is sparse. Relaxing from linear interpolations, we construct continuous paths
between minima of recent neural network architectures on CIFAR10 and CIFAR100.
Surprisingly, the paths are essentially flat in both the training and test
landscapes. This implies that neural networks have enough capacity for
structural changes, or that these changes are small between minima. Also, each
minimum has at least one vanishing Hessian eigenvalue in addition to those
resulting from trivial invariance.
</summary>
    <author>
      <name>Felix Draxler</name>
    </author>
    <author>
      <name>Kambis Veschgini</name>
    </author>
    <author>
      <name>Manfred Salmhofer</name>
    </author>
    <author>
      <name>Fred A. Hamprecht</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to 35th International Conference on Machine Learning (ICML
  2018) on February 9th, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.00885v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.00885v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0808.2341v3</id>
    <updated>2009-04-04T10:38:29Z</updated>
    <published>2008-08-18T06:27:12Z</published>
    <title>A Moving Bump in a Continuous Manifold: A Comprehensive Study of the
  Tracking Dynamics of Continuous Attractor Neural Networks</title>
    <summary>  Understanding how the dynamics of a neural network is shaped by the network
structure, and consequently how the network structure facilitates the functions
implemented by the neural system, is at the core of using mathematical models
to elucidate brain functions. This study investigates the tracking dynamics of
continuous attractor neural networks (CANNs). Due to the translational
invariance of neuronal recurrent interactions, CANNs can hold a continuous
family of stationary states. They form a continuous manifold in which the
neural system is neutrally stable. We systematically explore how this property
facilitates the tracking performance of a CANN, which is believed to have clear
correspondence with brain functions. By using the wave functions of the quantum
harmonic oscillator as the basis, we demonstrate how the dynamics of a CANN is
decomposed into different motion modes, corresponding to distortions in the
amplitude, position, width or skewness of the network state. We then develop a
perturbative approach that utilizes the dominating movement of the network's
stationary states in the state space. This method allows us to approximate the
network dynamics up to an arbitrary accuracy depending on the order of
perturbation used. We quantify the distortions of a Gaussian bump during
tracking, and study their effects on the tracking performance. Results are
obtained on the maximum speed for a moving stimulus to be trackable and the
reaction time for the network to catch up with an abrupt change in the
stimulus.
</summary>
    <author>
      <name>C. C. Alan Fung</name>
    </author>
    <author>
      <name>K. Y. Michael Wong</name>
    </author>
    <author>
      <name>Si Wu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/neco.2009.07-08-824</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/neco.2009.07-08-824" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">43 pages, 10 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Comput. 2010 22(3): 752-92</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0808.2341v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0808.2341v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.05504v1</id>
    <updated>2018-01-16T23:43:34Z</updated>
    <published>2018-01-16T23:43:34Z</published>
    <title>Automatic Classification of Music Genre using Masked Conditional Neural
  Networks</title>
    <summary>  Neural network based architectures used for sound recognition are usually
adapted from other application domains such as image recognition, which may not
harness the time-frequency representation of a signal. The ConditionaL Neural
Networks (CLNN) and its extension the Masked ConditionaL Neural Networks
(MCLNN) are designed for multidimensional temporal signal recognition. The CLNN
is trained over a window of frames to preserve the inter-frame relation, and
the MCLNN enforces a systematic sparseness over the network's links that mimics
a filterbank-like behavior. The masking operation induces the network to learn
in frequency bands, which decreases the network susceptibility to
frequency-shifts in time-frequency representations. Additionally, the mask
allows an exploration of a range of feature combinations concurrently analogous
to the manual handcrafting of the optimum collection of features for a
recognition task. MCLNN have achieved competitive performance on the Ballroom
music dataset compared to several hand-crafted attempts and outperformed models
based on state-of-the-art Convolutional Neural Networks.
</summary>
    <author>
      <name>Fady Medhat</name>
    </author>
    <author>
      <name>David Chesmore</name>
    </author>
    <author>
      <name>John Robinson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICDM.2017.125</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICDM.2017.125" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Restricted Boltzmann Machine; RBM; Conditional RBM; CRBM; Deep Belief
  Net; DBN; Conditional Neural Network; CLNN; Masked Conditional Neural
  Network; MCLNN; Music Information Retrieval; MIR. IEEE International
  Conference on Data Mining (ICDM), 2017</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Data Mining (ICDM) Year: 2017
  Pages: 979 - 984</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1801.05504v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.05504v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.06488v3</id>
    <updated>2016-01-07T13:50:22Z</updated>
    <published>2015-11-20T04:55:46Z</published>
    <title>Resiliency of Deep Neural Networks under Quantization</title>
    <summary>  The complexity of deep neural network algorithms for hardware implementation
can be much lowered by optimizing the word-length of weights and signals.
Direct quantization of floating-point weights, however, does not show good
performance when the number of bits assigned is small. Retraining of quantized
networks has been developed to relieve this problem. In this work, the effects
of retraining are analyzed for a feedforward deep neural network (FFDNN) and a
convolutional neural network (CNN). The network complexity is controlled to
know their effects on the resiliency of quantized networks by retraining. The
complexity of the FFDNN is controlled by varying the unit size in each hidden
layer and the number of layers, while that of the CNN is done by modifying the
feature map configuration. We find that the performance gap between the
floating-point and the retrain-based ternary (+1, 0, -1) weight neural networks
exists with a fair amount in 'complexity limited' networks, but the discrepancy
almost vanishes in fully complex networks whose capability is limited by the
training data, rather than by the number of connections. This research shows
that highly complex DNNs have the capability of absorbing the effects of severe
weight quantization through retraining, but connection limited networks are
less resilient. This paper also presents the effective compression ratio to
guide the trade-off between the network size and the precision when the
hardware resource is limited.
</summary>
    <author>
      <name>Wonyong Sung</name>
    </author>
    <author>
      <name>Sungho Shin</name>
    </author>
    <author>
      <name>Kyuyeon Hwang</name>
    </author>
    <link href="http://arxiv.org/abs/1511.06488v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.06488v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.04199v2</id>
    <updated>2017-04-19T00:53:16Z</updated>
    <published>2017-04-13T16:23:19Z</published>
    <title>Evolution and Analysis of Embodied Spiking Neural Networks Reveals
  Task-Specific Clusters of Effective Networks</title>
    <summary>  Elucidating principles that underlie computation in neural networks is
currently a major research topic of interest in neuroscience. Transfer Entropy
(TE) is increasingly used as a tool to bridge the gap between network
structure, function, and behavior in fMRI studies. Computational models allow
us to bridge the gap even further by directly associating individual neuron
activity with behavior. However, most computational models that have analyzed
embodied behaviors have employed non-spiking neurons. On the other hand,
computational models that employ spiking neural networks tend to be restricted
to disembodied tasks. We show for the first time the artificial evolution and
TE-analysis of embodied spiking neural networks to perform a
cognitively-interesting behavior. Specifically, we evolved an agent controlled
by an Izhikevich neural network to perform a visual categorization task. The
smallest networks capable of performing the task were found by repeating
evolutionary runs with different network sizes. Informational analysis of the
best solution revealed task-specific TE-network clusters, suggesting that
within-task homogeneity and across-task heterogeneity were key to behavioral
success. Moreover, analysis of the ensemble of solutions revealed that
task-specificity of TE-network clusters correlated with fitness. This provides
an empirically testable hypothesis that links network structure to behavior.
</summary>
    <author>
      <name>Madhavun Candadai Vasu</name>
    </author>
    <author>
      <name>Eduardo J. Izquierdo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3071178.3071336</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3071178.3071336" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Camera ready version of accepted for GECCO'17</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.04199v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.04199v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.10203v1</id>
    <updated>2017-11-28T09:41:34Z</updated>
    <published>2017-11-28T09:41:34Z</published>
    <title>Visualisation and 'diagnostic classifiers' reveal how recurrent and
  recursive neural networks process hierarchical structure</title>
    <summary>  We investigate how neural networks can learn and process languages with
hierarchical, compositional semantics. To this end, we define the artificial
task of processing nested arithmetic expressions, and study whether different
types of neural networks can learn to compute their meaning. We find that
recursive neural networks can find a generalising solution to this problem, and
we visualise this solution by breaking it up in three steps: project, sum and
squash. As a next step, we investigate recurrent neural networks, and show that
a gated recurrent unit, that processes its input incrementally, also performs
very well on this task. To develop an understanding of what the recurrent
network encodes, visualisation techniques alone do not suffice. Therefore, we
develop an approach where we formulate and test multiple hypotheses on the
information encoded and processed by the network. For each hypothesis, we
derive predictions about features of the hidden state representations at each
time step, and train 'diagnostic classifiers' to test those predictions. Our
results indicate that the networks follow a strategy similar to our
hypothesised 'cumulative strategy', which explains the high accuracy of the
network on novel expressions, the generalisation to longer expressions than
seen in training, and the mild deterioration with increasing length. This is
turn shows that diagnostic classifiers can be a useful technique for opening up
the black box of neural networks. We argue that diagnostic classification,
unlike most visualisation techniques, does scale up from small networks in a
toy domain, to larger and deeper recurrent networks dealing with real-life
data, and may therefore contribute to a better understanding of the internal
dynamics of current state-of-the-art models in natural language processing.
</summary>
    <author>
      <name>Dieuwke Hupkes</name>
    </author>
    <author>
      <name>Sara Veldhoen</name>
    </author>
    <author>
      <name>Willem Zuidema</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.10203v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.10203v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.7005v1</id>
    <updated>2014-08-29T13:00:17Z</updated>
    <published>2014-08-29T13:00:17Z</published>
    <title>The quest for a Quantum Neural Network</title>
    <summary>  With the overwhelming success in the field of quantum information in the last
decades, the "quest" for a Quantum Neural Network (QNN) model began in order to
combine quantum computing with the striking properties of neural computing.
This article presents a systematic approach to QNN research, which so far
consists of a conglomeration of ideas and proposals. It outlines the challenge
of combining the nonlinear, dissipative dynamics of neural computing and the
linear, unitary dynamics of quantum computing. It establishes requirements for
a meaningful QNN and reviews existing literature against these requirements. It
is found that none of the proposals for a potential QNN model fully exploits
both the advantages of quantum physics and computing in neural networks. An
outlook on possible ways forward is given, emphasizing the idea of Open Quantum
Neural Networks based on dissipative quantum computing.
</summary>
    <author>
      <name>M. Schuld</name>
    </author>
    <author>
      <name>I. Sinayskiy</name>
    </author>
    <author>
      <name>F. Petruccione</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11128-014-0809-8</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11128-014-0809-8" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Review of Quantum Neural Networks research; 21 pages, 5 figs, 71 Refs</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.7005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.7005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04466v1</id>
    <updated>2016-06-14T17:26:27Z</updated>
    <published>2016-06-14T17:26:27Z</published>
    <title>Neural Networks and Continuous Time</title>
    <summary>  The fields of neural computation and artificial neural networks have
developed much in the last decades. Most of the works in these fields focus on
implementing and/or learning discrete functions or behavior. However,
technical, physical, and also cognitive processes evolve continuously in time.
This cannot be described directly with standard architectures of artificial
neural networks such as multi-layer feed-forward perceptrons. Therefore, in
this paper, we will argue that neural networks modeling continuous time are
needed explicitly for this purpose, because with them the synthesis and
analysis of continuous and possibly periodic processes in time are possible
(e.g. for robot behavior) besides computing discrete classification functions
(e.g. for logical reasoning). We will relate possible neural network
architectures with (hybrid) automata models that allow to express continuous
processes.
</summary>
    <author>
      <name>Frieder Stolzenburg</name>
    </author>
    <author>
      <name>Florian Ruh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 10 figures. This paper is an extended version of a
  contribution presented at KI 2009 Workshop Complex Cognition</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.04466v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04466v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.05672v1</id>
    <updated>2017-09-17T14:44:07Z</updated>
    <published>2017-09-17T14:44:07Z</published>
    <title>Neural Affine Grayscale Image Denoising</title>
    <summary>  We propose a new grayscale image denoiser, dubbed as Neural Affine Image
Denoiser (Neural AIDE), which utilizes neural network in a novel way. Unlike
other neural network based image denoising methods, which typically apply
simple supervised learning to learn a mapping from a noisy patch to a clean
patch, we formulate to train a neural network to learn an \emph{affine} mapping
that gets applied to a noisy pixel, based on its context. Our formulation
enables both supervised training of the network from the labeled training
dataset and adaptive fine-tuning of the network parameters using the given
noisy image subject to denoising. The key tool for devising Neural AIDE is to
devise an estimated loss function of the MSE of the affine mapping, solely
based on the noisy data. As a result, our algorithm can outperform most of the
recent state-of-the-art methods in the standard benchmark datasets. Moreover,
our fine-tuning method can nicely overcome one of the drawbacks of the
patch-level supervised learning methods in image denoising; namely, a
supervised trained model with a mismatched noise variance can be mostly
corrected as long as we have the matched noise variance during the fine-tuning
step.
</summary>
    <author>
      <name>Sungmin Cha</name>
    </author>
    <author>
      <name>Taesup Moon</name>
    </author>
    <link href="http://arxiv.org/abs/1709.05672v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.05672v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.08314v2</id>
    <updated>2018-01-06T23:16:33Z</updated>
    <published>2017-12-22T06:28:28Z</published>
    <title>Benchmarking Decoupled Neural Interfaces with Synthetic Gradients</title>
    <summary>  Artifical Neural Networks are a particular class of learning systems modeled
after biological neural functions with an interesting penchant for Hebbian
learning, that is "neurons that wire together, fire together". However, unlike
their natural counterparts, artificial neural networks have a close and
stringent coupling between the modules of neurons in the network. This coupling
or locking imposes upon the network a strict and inflexible structure that
prevent layers in the network from updating their weights until a full
feed-forward and backward pass has occurred. Such a constraint though may have
sufficed for a while, is now no longer feasible in the era of very-large-scale
machine learning, coupled with the increased desire for parallelization of the
learning process across multiple computing infrastructures. To solve this
problem, synthetic gradients (SG) with decoupled neural interfaces (DNI) are
introduced as a viable alternative to the backpropagation algorithm. This paper
performs a speed benchmark to compare the speed and accuracy capabilities of
SG-DNI as opposed to a standard neural interface using multilayer perceptron
MLP. SG-DNI shows good promise, in that it not only captures the learning
problem, it is also over 3-fold faster due to it asynchronous learning
capabilities.
</summary>
    <author>
      <name>Ekaba Bisong</name>
    </author>
    <link href="http://arxiv.org/abs/1712.08314v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.08314v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.05508v1</id>
    <updated>2015-08-22T13:15:09Z</updated>
    <published>2015-08-22T13:15:09Z</published>
    <title>Towards Neural Network-based Reasoning</title>
    <summary>  We propose Neural Reasoner, a framework for neural network-based reasoning
over natural language sentences. Given a question, Neural Reasoner can infer
over multiple supporting facts and find an answer to the question in specific
forms. Neural Reasoner has 1) a specific interaction-pooling mechanism,
allowing it to examine multiple facts, and 2) a deep architecture, allowing it
to model the complicated logical relations in reasoning tasks. Assuming no
particular structure exists in the question and facts, Neural Reasoner is able
to accommodate different types of reasoning and different forms of language
expressions. Despite the model complexity, Neural Reasoner can still be trained
effectively in an end-to-end manner. Our empirical studies show that Neural
Reasoner can outperform existing neural reasoning systems with remarkable
margins on two difficult artificial tasks (Positional Reasoning and Path
Finding) proposed in [8]. For example, it improves the accuracy on Path
Finding(10K) from 33.4% [6] to over 98%.
</summary>
    <author>
      <name>Baolin Peng</name>
    </author>
    <author>
      <name>Zhengdong Lu</name>
    </author>
    <author>
      <name>Hang Li</name>
    </author>
    <author>
      <name>Kam-Fai Wong</name>
    </author>
    <link href="http://arxiv.org/abs/1508.05508v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.05508v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0111151v3</id>
    <updated>2011-02-13T06:35:04Z</updated>
    <published>2001-11-09T05:07:18Z</published>
    <title>Neural Networks with Finite Width Action Potentials</title>
    <summary>  The paper was done as an assigned Princeton university project. It is being
withdrawn since it needs to be changed and updated substantially.
</summary>
    <author>
      <name>Fariel Shafee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0111151v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0111151v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.1390v1</id>
    <updated>2007-05-10T05:52:22Z</updated>
    <published>2007-05-10T05:52:22Z</published>
    <title>Machine and Component Residual Life Estimation through the Application
  of Neural Networks</title>
    <summary>  This paper concerns the use of neural networks for predicting the residual
life of machines and components. In addition, the advantage of using
condition-monitoring data to enhance the predictive capability of these neural
networks was also investigated. A number of neural network variations were
trained and tested with the data of two different reliability-related datasets.
The first dataset represents the renewal case where the failed unit is repaired
and restored to a good-as-new condition. Data was collected in the laboratory
by subjecting a series of similar test pieces to fatigue loading with a
hydraulic actuator. The average prediction error of the various neural networks
being compared varied from 431 to 841 seconds on this dataset, where test
pieces had a characteristic life of 8,971 seconds. The second dataset was
collected from a group of pumps used to circulate a water and magnetite
solution within a plant. The data therefore originated from a repaired system
affected by reliability degradation. When optimized, the multi-layer perceptron
neural networks trained with the Levenberg-Marquardt algorithm and the general
regression neural network produced a sum-of-squares error within 11.1% of each
other. The potential for using neural networks for residual life prediction and
the advantage of incorporating condition-based data into the model were proven
for both examples.
</summary>
    <author>
      <name>M. A. Herzog</name>
    </author>
    <author>
      <name>T. Marwala</name>
    </author>
    <author>
      <name>P. S. Heyns</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0705.1390v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.1390v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7948v2</id>
    <updated>2013-06-02T20:32:05Z</updated>
    <published>2013-04-30T10:41:26Z</published>
    <title>Convolutional Neural Networks learn compact local image descriptors</title>
    <summary>  A standard deep convolutional neural network paired with a suitable loss
function learns compact local image descriptors that perform comparably to
state-of-the art approaches.
</summary>
    <author>
      <name>Christian Osendorfer</name>
    </author>
    <author>
      <name>Justin Bayer</name>
    </author>
    <author>
      <name>Patrick van der Smagt</name>
    </author>
    <link href="http://arxiv.org/abs/1304.7948v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7948v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.5598v1</id>
    <updated>2014-04-22T19:24:23Z</updated>
    <published>2014-04-22T19:24:23Z</published>
    <title>Descriptive examples of the limitations of Artificial Neural Networks
  applied to the analysis of independent stochastic data</title>
    <summary>  We show with a few descriptive examples the limitations of Artificial Neural
Networks when they are applied to the analysis of independent stochastic data.
</summary>
    <author>
      <name>Henry Navarro</name>
    </author>
    <author>
      <name>Leonardo Bennun</name>
    </author>
    <link href="http://arxiv.org/abs/1404.5598v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.5598v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.5549v2</id>
    <updated>2016-05-02T16:52:36Z</updated>
    <published>2014-08-24T04:10:33Z</published>
    <title>On the Neuron Response Features of Convolutional Neural Networks for
  Remote Sensing Image</title>
    <summary>  In this paper, some patterns of the Neuron Response of deep Convolutional
Neural Networks were observed.
</summary>
    <author>
      <name>Jie Chen</name>
    </author>
    <author>
      <name>Min Deng</name>
    </author>
    <author>
      <name>Haifeng Li</name>
    </author>
    <link href="http://arxiv.org/abs/1408.5549v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.5549v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.00036v2</id>
    <updated>2015-04-14T22:55:08Z</updated>
    <published>2015-02-27T23:50:22Z</published>
    <title>Norm-Based Capacity Control in Neural Networks</title>
    <summary>  We investigate the capacity, convexity and characterization of a general
family of norm-constrained feed-forward networks.
</summary>
    <author>
      <name>Behnam Neyshabur</name>
    </author>
    <author>
      <name>Ryota Tomioka</name>
    </author>
    <author>
      <name>Nathan Srebro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.00036v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.00036v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.0261v1</id>
    <updated>2013-09-01T20:35:17Z</updated>
    <published>2013-09-01T20:35:17Z</published>
    <title>Multi-Column Deep Neural Networks for Offline Handwritten Chinese
  Character Classification</title>
    <summary>  Our Multi-Column Deep Neural Networks achieve best known recognition rates on
Chinese characters from the ICDAR 2011 and 2013 offline handwriting
competitions, approaching human performance.
</summary>
    <author>
      <name>Dan Cire≈üan</name>
    </author>
    <author>
      <name>J√ºrgen Schmidhuber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure, IDSIA tech report</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.0261v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.0261v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04644v2</id>
    <updated>2017-03-22T17:46:16Z</updated>
    <published>2016-08-16T15:59:35Z</published>
    <title>Towards Evaluating the Robustness of Neural Networks</title>
    <summary>  Neural networks provide state-of-the-art results for most machine learning
tasks. Unfortunately, neural networks are vulnerable to adversarial examples:
given an input $x$ and any target classification $t$, it is possible to find a
new input $x'$ that is similar to $x$ but classified as $t$. This makes it
difficult to apply neural networks in security-critical areas. Defensive
distillation is a recently proposed approach that can take an arbitrary neural
network, and increase its robustness, reducing the success rate of current
attacks' ability to find adversarial examples from $95\%$ to $0.5\%$.
  In this paper, we demonstrate that defensive distillation does not
significantly increase the robustness of neural networks by introducing three
new attack algorithms that are successful on both distilled and undistilled
neural networks with $100\%$ probability. Our attacks are tailored to three
distance metrics used previously in the literature, and when compared to
previous adversarial example generation algorithms, our attacks are often much
more effective (and never worse). Furthermore, we propose using high-confidence
adversarial examples in a simple transferability test we show can also be used
to break defensive distillation. We hope our attacks will be used as a
benchmark in future defense attempts to create neural networks that resist
adversarial examples.
</summary>
    <author>
      <name>Nicholas Carlini</name>
    </author>
    <author>
      <name>David Wagner</name>
    </author>
    <link href="http://arxiv.org/abs/1608.04644v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04644v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03147v1</id>
    <updated>2016-09-11T10:19:36Z</updated>
    <published>2016-09-11T10:19:36Z</published>
    <title>SDSS-DR12 Bulk Stellar Spectral Classification: Artificial Neural
  Networks Approach</title>
    <summary>  This paper explores the application of Probabilistic Neural Network (PNN),
Support Vector Machine (SVM) and Kmeans clustering as tools for automated
classification of massive stellar spectra.
</summary>
    <author>
      <name>S. Kheirdastan</name>
    </author>
    <author>
      <name>M. Bazarghan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10509-016-2880-3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10509-016-2880-3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 6 figures in Astrophysics and Space Science,2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.03147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.09060v2</id>
    <updated>2017-11-22T16:06:06Z</updated>
    <published>2016-09-28T19:59:56Z</published>
    <title>Machine Learning Topological States</title>
    <summary>  Artificial neural networks and machine learning have now reached a new era
after several decades of improvement where applications are to explode in many
fields of science, industry, and technology. Here, we use artificial neural
networks to study an intriguing phenomenon in quantum physics--- the
topological phases of matter. We find that certain topological states, either
symmetry-protected or with intrinsic topological order, can be represented with
classical artificial neural networks. This is demonstrated by using three
concrete spin systems, the one-dimensional (1D) symmetry-protected topological
cluster state and the 2D and 3D toric code states with intrinsic topological
orders. For all three cases we show rigorously that the topological ground
states can be represented by short-range neural networks in an \textit{exact}
and \textit{efficient} fashion---the required number of hidden neurons is as
small as the number of physical spins and the number of parameters scales only
\textit{linearly} with the system size. For the 2D toric-code model, we find
that the proposed short-range neural networks can describe the excited states
with abelain anyons and their nontrivial mutual statistics as well. In
addition, by using reinforcement learning we show that neural networks are
capable of finding the topological ground states of non-integrable Hamiltonians
with strong interactions and studying their topological phase transitions. Our
results demonstrate explicitly the exceptional power of neural networks in
describing topological quantum states, and at the same time provide valuable
guidance to machine learning of topological phases in generic lattice models.
</summary>
    <author>
      <name>Dong-Ling Deng</name>
    </author>
    <author>
      <name>Xiaopeng Li</name>
    </author>
    <author>
      <name>S. Das Sarma</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevB.96.195145</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevB.96.195145" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 7 figures, accepted for publication in Phys. Rev. B</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. B 96, 195145 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1609.09060v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09060v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.str-el" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.05189v1</id>
    <updated>2017-11-14T16:53:39Z</updated>
    <published>2017-11-14T16:53:39Z</published>
    <title>CryptoDL: Deep Neural Networks over Encrypted Data</title>
    <summary>  Machine learning algorithms based on deep neural networks have achieved
remarkable results and are being extensively used in different domains.
However, the machine learning algorithms requires access to raw data which is
often privacy sensitive. To address this issue, we develop new techniques to
provide solutions for running deep neural networks over encrypted data. In this
paper, we develop new techniques to adopt deep neural networks within the
practical limitation of current homomorphic encryption schemes. More
specifically, we focus on classification of the well-known convolutional neural
networks (CNN). First, we design methods for approximation of the activation
functions commonly used in CNNs (i.e. ReLU, Sigmoid, and Tanh) with low degree
polynomials which is essential for efficient homomorphic encryption schemes.
Then, we train convolutional neural networks with the approximation polynomials
instead of original activation functions and analyze the performance of the
models. Finally, we implement convolutional neural networks over encrypted data
and measure performance of the models. Our experimental results validate the
soundness of our approach with several convolutional neural networks with
varying number of layers and structures. When applied to the MNIST optical
character recognition tasks, our approach achieves 99.52\% accuracy which
significantly outperforms the state-of-the-art solutions and is very close to
the accuracy of the best non-private version, 99.77\%. Also, it can make close
to 164000 predictions per hour. We also applied our approach to CIFAR-10, which
is much more complex compared to MNIST, and were able to achieve 91.5\%
accuracy with approximation polynomials used as activation functions. These
results show that CryptoDL provides efficient, accurate and scalable
privacy-preserving predictions.
</summary>
    <author>
      <name>Ehsan Hesamifard</name>
    </author>
    <author>
      <name>Hassan Takabi</name>
    </author>
    <author>
      <name>Mehdi Ghasemi</name>
    </author>
    <link href="http://arxiv.org/abs/1711.05189v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.05189v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.00171v1</id>
    <updated>2017-12-30T18:11:59Z</updated>
    <published>2017-12-30T18:11:59Z</published>
    <title>PAC-Bayesian Margin Bounds for Convolutional Neural Networks - Technical
  Report</title>
    <summary>  Recently the generalisation error of deep neural networks has been analysed
through the PAC-Bayesian framework, for the case of fully connected layers. We
adapt this approach to the convolutional setting.
</summary>
    <author>
      <name>Pitas Konstantinos</name>
    </author>
    <author>
      <name>Mike Davies</name>
    </author>
    <author>
      <name>Pierre Vandergheynst</name>
    </author>
    <link href="http://arxiv.org/abs/1801.00171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.00171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.04928v1</id>
    <updated>2018-01-15T00:51:29Z</updated>
    <published>2018-01-15T00:51:29Z</published>
    <title>Leapfrogging for parallelism in deep neural networks</title>
    <summary>  We present a technique, which we term leapfrogging, to parallelize back-
propagation in deep neural networks. We show that this technique yields a
savings of $1-1/k$ of a dominant term in backpropagation, where k is the number
of threads (or gpus).
</summary>
    <author>
      <name>Yatin Saraiya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.04928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.04928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.05387v1</id>
    <updated>2018-01-16T17:47:13Z</updated>
    <published>2018-01-16T17:47:13Z</published>
    <title>StressedNets: Efficient Feature Representations via Stress-induced
  Evolutionary Synthesis of Deep Neural Networks</title>
    <summary>  The computational complexity of leveraging deep neural networks for
extracting deep feature representations is a significant barrier to its
widespread adoption, particularly for use in embedded devices. One particularly
promising strategy to addressing the complexity issue is the notion of
evolutionary synthesis of deep neural networks, which was demonstrated to
successfully produce highly efficient deep neural networks while retaining
modeling performance. Here, we further extend upon the evolutionary synthesis
strategy for achieving efficient feature extraction via the introduction of a
stress-induced evolutionary synthesis framework, where stress signals are
imposed upon the synapses of a deep neural network during training to induce
stress and steer the synthesis process towards the production of more efficient
deep neural networks over successive generations and improved model fidelity at
a greater efficiency. The proposed stress-induced evolutionary synthesis
approach is evaluated on a variety of different deep neural network
architectures (LeNet5, AlexNet, and YOLOv2) on different tasks (object
classification and object detection) to synthesize efficient StressedNets over
multiple generations. Experimental results demonstrate the efficacy of the
proposed framework to synthesize StressedNets with significant improvement in
network architecture efficiency (e.g., 40x for AlexNet and 33x for YOLOv2) and
speed improvements (e.g., 5.5x inference speed-up for YOLOv2 on an Nvidia Tegra
X1 mobile processor).
</summary>
    <author>
      <name>Mohammad Javad Shafiee</name>
    </author>
    <author>
      <name>Brendan Chwyl</name>
    </author>
    <author>
      <name>Francis Li</name>
    </author>
    <author>
      <name>Rongyan Chen</name>
    </author>
    <author>
      <name>Michelle Karg</name>
    </author>
    <author>
      <name>Christian Scharfenberger</name>
    </author>
    <author>
      <name>Alexander Wong</name>
    </author>
    <link href="http://arxiv.org/abs/1801.05387v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.05387v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.5721v2</id>
    <updated>2012-02-17T20:18:44Z</updated>
    <published>2012-01-27T08:37:14Z</published>
    <title>Short-term synaptic facilitation improves information retrieval in noisy
  neural networks</title>
    <summary>  Short-term synaptic depression and facilitation have been found to greatly
influence the performance of autoassociative neural networks. However, only
partial results, focused for instance on the computation of the maximum storage
capacity at zero temperature, have been obtained to date. In this work, we
extended the study of the effect of these synaptic mechanisms on
autoassociative neural networks to more realistic and general conditions,
including the presence of noise in the system. In particular, we characterized
the behavior of the system by means of its phase diagrams, and we concluded
that synaptic facilitation significantly enlarges the region of good retrieval
performance of the network. We also found that networks with facilitating
synapses may have critical temperatures substantially higher than those of
standard autoassociative networks, thus allowing neural networks to perform
better under high-noise conditions.
</summary>
    <author>
      <name>J. F. Mejias</name>
    </author>
    <author>
      <name>B. Hernandez-Gomez</name>
    </author>
    <author>
      <name>J. J. Torres</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1209/0295-5075/97/48008</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1209/0295-5075/97/48008" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, to appear in EPL</arxiv:comment>
    <link href="http://arxiv.org/abs/1201.5721v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.5721v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.01008v1</id>
    <updated>2015-08-05T09:11:06Z</updated>
    <published>2015-08-05T09:11:06Z</published>
    <title>INsight: A Neuromorphic Computing System for Evaluation of Large Neural
  Networks</title>
    <summary>  Deep neural networks have been demonstrated impressive results in various
cognitive tasks such as object detection and image classification. In order to
execute large networks, Von Neumann computers store the large number of weight
parameters in external memories, and processing elements are timed-shared,
which leads to power-hungry I/O operations and processing bottlenecks. This
paper describes a neuromorphic computing system that is designed from the
ground up for the energy-efficient evaluation of large-scale neural networks.
The computing system consists of a non-conventional compiler, a neuromorphic
architecture, and a space-efficient microarchitecture that leverages existing
integrated circuit design methodologies. The compiler factorizes a trained,
feedforward network into a sparsely connected network, compresses the weights
linearly, and generates a time delay neural network reducing the number of
connections. The connections and units in the simplified network are mapped to
silicon synapses and neurons. We demonstrate an implementation of the
neuromorphic computing system based on a field-programmable gate array that
performs the MNIST hand-written digit classification with 97.64% accuracy.
</summary>
    <author>
      <name>Jaeyong Chung</name>
    </author>
    <author>
      <name>Taehwan Shin</name>
    </author>
    <author>
      <name>Yongshin Kang</name>
    </author>
    <link href="http://arxiv.org/abs/1508.01008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.01008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.06071v1</id>
    <updated>2016-01-22T16:59:01Z</updated>
    <published>2016-01-22T16:59:01Z</published>
    <title>Bitwise Neural Networks</title>
    <summary>  Based on the assumption that there exists a neural network that efficiently
represents a set of Boolean functions between all binary inputs and outputs, we
propose a process for developing and deploying neural networks whose weight
parameters, bias terms, input, and intermediate hidden layer output signals,
are all binary-valued, and require only basic bit logic for the feedforward
pass. The proposed Bitwise Neural Network (BNN) is especially suitable for
resource-constrained environments, since it replaces either floating or
fixed-point arithmetic with significantly more efficient bitwise operations.
Hence, the BNN requires for less spatial complexity, less memory bandwidth, and
less power consumption in hardware. In order to design such networks, we
propose to add a few training schemes, such as weight compression and noisy
backpropagation, which result in a bitwise network that performs almost as well
as its corresponding real-valued network. We test the proposed network on the
MNIST dataset, represented using binary features, and show that BNNs result in
competitive performance while offering dramatic computational savings.
</summary>
    <author>
      <name>Minje Kim</name>
    </author>
    <author>
      <name>Paris Smaragdis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper was presented at the International Conference on Machine
  Learning (ICML) Workshop on Resource-Efficient Machine Learning, Lille,
  France, Jul. 6-11, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.06071v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.06071v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.01664v1</id>
    <updated>2017-04-05T23:04:43Z</updated>
    <published>2017-04-05T23:04:43Z</published>
    <title>The Relative Performance of Ensemble Methods with Deep Convolutional
  Neural Networks for Image Classification</title>
    <summary>  Artificial neural networks have been successfully applied to a variety of
machine learning tasks, including image recognition, semantic segmentation, and
machine translation. However, few studies fully investigated ensembles of
artificial neural networks. In this work, we investigated multiple widely used
ensemble methods, including unweighted averaging, majority voting, the Bayes
Optimal Classifier, and the (discrete) Super Learner, for image recognition
tasks, with deep neural networks as candidate algorithms. We designed several
experiments, with the candidate algorithms being the same network structure
with different model checkpoints within a single training process, networks
with same structure but trained multiple times stochastically, and networks
with different structure. In addition, we further studied the over-confidence
phenomenon of the neural networks, as well as its impact on the ensemble
methods. Across all of our experiments, the Super Learner achieved best
performance among all the ensemble methods in this study.
</summary>
    <author>
      <name>Cheng Ju</name>
    </author>
    <author>
      <name>Aur√©lien Bibaut</name>
    </author>
    <author>
      <name>Mark J. van der Laan</name>
    </author>
    <link href="http://arxiv.org/abs/1704.01664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.01664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.09774v1</id>
    <updated>2018-01-29T21:45:05Z</updated>
    <published>2018-01-29T21:45:05Z</published>
    <title>On Psychoacoustically Weighted Cost Functions Towards Resource-Efficient
  Deep Neural Networks for Speech Denoising</title>
    <summary>  We present a psychoacoustically enhanced cost function to balance network
complexity and perceptual performance of deep neural networks for speech
denoising. While training the network, we utilize perceptual weights added to
the ordinary mean-squared error to emphasize contribution from frequency bins
which are most audible while ignoring error from inaudible bins. To generate
the weights, we employ psychoacoustic models to compute the global masking
threshold from the clean speech spectra. We then evaluate the speech denoising
performance of our perceptually guided neural network by using both objective
and perceptual sound quality metrics, testing on various network structures
ranging from shallow and narrow ones to deep and wide ones. The experimental
results showcase our method as a valid approach for infusing perceptual
significance to deep neural network operations. In particular, the more
perceptually sensible enhancement in performance seen by simple neural network
topologies proves that the proposed method can lead to resource-efficient
speech denoising implementations in small devices without degrading the
perceived signal fidelity.
</summary>
    <author>
      <name>Kai Zhen</name>
    </author>
    <author>
      <name>Aswin Sivaraman</name>
    </author>
    <author>
      <name>Jongmo Sung</name>
    </author>
    <author>
      <name>Minje Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.09774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.09774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.00406v1</id>
    <updated>2018-02-23T06:11:37Z</updated>
    <published>2018-02-23T06:11:37Z</published>
    <title>Left ventricle segmentation By modelling uncertainty in prediction of
  deep convolutional neural networks and adaptive thresholding inference</title>
    <summary>  Deep neural networks have shown great achievements in solving complex
problems. However, there are fundamental problems that limit their real world
applications. Lack of measurable criteria for estimating uncertainty in the
network outputs is one of these problems. In this paper, we address this
limitation by introducing deformation to the network input and measuring the
level of stability in the network's output. We calculate simple random
transformations to estimate the prediction uncertainty of deep convolutional
neural networks. For a real use-case, we apply this method to left ventricle
segmentation in MRI cardiac images. We also propose an adaptive thresholding
method to consider the deep neural network uncertainty. Experimental results
demonstrate state-of-the-art performance and highlight the capabilities of
simple methods in conjunction with deep neural networks.
</summary>
    <author>
      <name>Alireza Norouzi</name>
    </author>
    <author>
      <name>Ali Emami</name>
    </author>
    <author>
      <name>S. M. Reza Soroushmehr</name>
    </author>
    <author>
      <name>Nader Karimi</name>
    </author>
    <author>
      <name>Shadrokh Samavi</name>
    </author>
    <author>
      <name>Kayvan Najarian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.00406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.00406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.02444v1</id>
    <updated>2015-02-09T11:45:02Z</updated>
    <published>2015-02-09T11:45:02Z</published>
    <title>On the Dynamics of a Recurrent Hopfield Network</title>
    <summary>  In this research paper novel real/complex valued recurrent Hopfield Neural
Network (RHNN) is proposed. The method of synthesizing the energy landscape of
such a network and the experimental investigation of dynamics of Recurrent
Hopfield Network is discussed. Parallel modes of operation (other than fully
parallel mode) in layered RHNN is proposed. Also, certain potential
applications are proposed.
</summary>
    <author>
      <name>Rama Garimella</name>
    </author>
    <author>
      <name>Berkay Kicanaoglu</name>
    </author>
    <author>
      <name>Moncef Gabbouj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures, 1 table, submitted to IJCNN-2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.02444v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.02444v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.03130v1</id>
    <updated>2017-11-08T19:30:15Z</updated>
    <published>2017-11-08T19:30:15Z</published>
    <title>EnergyNet: Energy-based Adaptive Structural Learning of Artificial
  Neural Network Architectures</title>
    <summary>  We present E NERGY N ET , a new framework for analyzing and building
artificial neural network architectures. Our approach adaptively learns the
structure of the networks in an unsupervised manner. The methodology is based
upon the theoretical guarantees of the energy function of restricted Boltzmann
machines (RBM) of infinite number of nodes. We present experimental results to
show that the final network adapts to the complexity of a given problem.
</summary>
    <author>
      <name>Gus Kristiansen</name>
    </author>
    <author>
      <name>Xavi Gonzalvo</name>
    </author>
    <link href="http://arxiv.org/abs/1711.03130v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.03130v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0003215v2</id>
    <updated>2000-06-21T14:37:18Z</updated>
    <published>2000-03-13T17:12:50Z</published>
    <title>Topological Evolution of Dynamical Networks: Global Criticality from
  Local Dynamics</title>
    <summary>  We evolve network topology of an asymmetrically connected threshold network
by a simple local rewiring rule: quiet nodes grow links, active nodes lose
links. This leads to convergence of the average connectivity of the network
towards the critical value $K_c =2$ in the limit of large system size $N$. How
this principle could generate self-organization in natural complex systems is
discussed for two examples: neural networks and regulatory networks in the
genome.
</summary>
    <author>
      <name>Stefan Bornholdt</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Kiel University</arxiv:affiliation>
    </author>
    <author>
      <name>Thimo Rohlf</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Kiel University</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.84.6114</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.84.6114" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages RevTeX, 4 figures PostScript, revised version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. Lett. 84 (2000) 6114-6117</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0003215v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0003215v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0608702v1</id>
    <updated>2006-08-31T06:30:16Z</updated>
    <published>2006-08-31T06:30:16Z</published>
    <title>Strong Effects of Network Architecture in the Entrainment of Coupled
  Oscillator Systems</title>
    <summary>  Entrainment of randomly coupled oscillator networks by periodic external
forcing applied to a subset of elements is numerically and analytically
investigated. For a large class of interaction functions, we find that the
entrainment window with a tongue shape becomes exponentially narrow for
networks with higher hierarchical organization. However, the entrainment is
significantly facilitated if the networks are directionally biased, i.e.,
closer to the feedforward networks. Furthermore, we show that the networks with
high entrainment ability can be constructed by evolutionary optimization
processes. The neural network structure of the master clock of the circadian
rhythm in mammals is discussed from the viewpoint of our results.
</summary>
    <author>
      <name>Hiroshi Kori</name>
    </author>
    <author>
      <name>Alexander S. Mikhailov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.74.066115</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.74.066115" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 11 figures, RevTeX</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0608702v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0608702v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.4682v1</id>
    <updated>2010-11-21T17:39:27Z</updated>
    <published>2010-11-21T17:39:27Z</published>
    <title>Analysis of attractor distances in Random Boolean Networks</title>
    <summary>  We study the properties of the distance between attractors in Random Boolean
Networks, a prominent model of genetic regulatory networks. We define three
distance measures, upon which attractor distance matrices are constructed and
their main statistic parameters are computed. The experimental analysis shows
that ordered networks have a very clustered set of attractors, while chaotic
networks' attractors are scattered; critical networks show, instead, a pattern
with characteristics of both ordered and chaotic networks.
</summary>
    <author>
      <name>Andrea Roli</name>
    </author>
    <author>
      <name>Stefano Benedettini</name>
    </author>
    <author>
      <name>Roberto Serra</name>
    </author>
    <author>
      <name>Marco Villani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures. Presented at WIRN 2010 - Italian workshop on
  neural networks, May 2010. To appear in a volume published by IOS Press</arxiv:comment>
    <link href="http://arxiv.org/abs/1011.4682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.4682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.07279v1</id>
    <updated>2017-08-24T05:24:26Z</updated>
    <published>2017-08-24T05:24:26Z</published>
    <title>Combining Discrete and Neural Features for Sequence Labeling</title>
    <summary>  Neural network models have recently received heated research attention in the
natural language processing community. Compared with traditional models with
discrete features, neural models have two main advantages. First, they take
low-dimensional, real-valued embedding vectors as inputs, which can be trained
over large raw data, thereby addressing the issue of feature sparsity in
discrete models. Second, deep neural networks can be used to automatically
combine input features, and including non-local features that capture semantic
patterns that cannot be expressed using discrete indicator features. As a
result, neural network models have achieved competitive accuracies compared
with the best discrete models for a range of NLP tasks.
  On the other hand, manual feature templates have been carefully investigated
for most NLP tasks over decades and typically cover the most useful indicator
pattern for solving the problems. Such information can be complementary the
features automatically induced from neural networks, and therefore combining
discrete and neural features can potentially lead to better accuracy compared
with models that leverage discrete or neural features only.
  In this paper, we systematically investigate the effect of discrete and
neural feature combination for a range of fundamental NLP tasks based on
sequence labeling, including word segmentation, POS tagging and named entity
recognition for Chinese and English, respectively. Our results on standard
benchmarks show that state-of-the-art neural models can give accuracies
comparable to the best discrete models in the literature for most tasks and
combing discrete and neural features unanimously yield better results.
</summary>
    <author>
      <name>Jie Yang</name>
    </author>
    <author>
      <name>Zhiyang Teng</name>
    </author>
    <author>
      <name>Meishan Zhang</name>
    </author>
    <author>
      <name>Yue Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by International Conference on Computational Linguistics and
  Intelligent Text Processing (CICLing) 2016, April</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.07279v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.07279v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.07620v1</id>
    <updated>2016-01-28T01:48:42Z</updated>
    <published>2016-01-28T01:48:42Z</published>
    <title>Using Firing-Rate Dynamics to Train Recurrent Networks of Spiking Model
  Neurons</title>
    <summary>  Recurrent neural networks are powerful tools for understanding and modeling
computation and representation by populations of neurons. Continuous-variable
or "rate" model networks have been analyzed and applied extensively for these
purposes. However, neurons fire action potentials, and the discrete nature of
spiking is an important feature of neural circuit dynamics. Despite significant
advances, training recurrently connected spiking neural networks remains a
challenge. We present a procedure for training recurrently connected spiking
networks to generate dynamical patterns autonomously, to produce complex
temporal outputs based on integrating network input, and to model physiological
data. Our procedure makes use of a continuous-variable network to identify
targets for training the inputs to the spiking model neurons. Surprisingly, we
are able to construct spiking networks that duplicate tasks performed by
continuous-variable networks with only a relatively minor expansion in the
number of neurons. Our approach provides a novel view of the significance and
appropriate use of "firing rate" models, and it is a useful approach for
building model spiking networks that can be used to address important questions
about representation and computation in neural systems.
</summary>
    <author>
      <name>Brian DePasquale</name>
    </author>
    <author>
      <name>Mark M. Churchland</name>
    </author>
    <author>
      <name>L. F. Abbott</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.07620v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.07620v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05571v1</id>
    <updated>2016-12-16T17:57:15Z</updated>
    <published>2016-12-16T17:57:15Z</published>
    <title>Delta Networks for Optimized Recurrent Network Computation</title>
    <summary>  Many neural networks exhibit stability in their activation patterns over time
in response to inputs from sensors operating under real-world conditions. By
capitalizing on this property of natural signals, we propose a Recurrent Neural
Network (RNN) architecture called a delta network in which each neuron
transmits its value only when the change in its activation exceeds a threshold.
The execution of RNNs as delta networks is attractive because their states must
be stored and fetched at every timestep, unlike in convolutional neural
networks (CNNs). We show that a naive run-time delta network implementation
offers modest improvements on the number of memory accesses and computes, but
optimized training techniques confer higher accuracy at higher speedup. With
these optimizations, we demonstrate a 9X reduction in cost with negligible loss
of accuracy for the TIDIGITS audio digit recognition benchmark. Similarly, on
the large Wall Street Journal speech recognition benchmark even existing
networks can be greatly accelerated as delta networks, and a 5.7x improvement
with negligible loss of accuracy can be obtained through training. Finally, on
an end-to-end CNN trained for steering angle prediction in a driving dataset,
the RNN cost can be reduced by a substantial 100X.
</summary>
    <author>
      <name>Daniel Neil</name>
    </author>
    <author>
      <name>Jun Haeng Lee</name>
    </author>
    <author>
      <name>Tobi Delbruck</name>
    </author>
    <author>
      <name>Shih-Chii Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1612.05571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.02615v2</id>
    <updated>2017-11-19T17:34:35Z</updated>
    <published>2017-10-07T01:22:24Z</published>
    <title>A Transfer-Learning Approach for Accelerated MRI using Deep Neural
  Networks</title>
    <summary>  Neural network based architectures have recently been proposed for
reconstruction of undersampled MR acquisitions. A deep network containing many
free parameters is typically trained using a relatively large set of
fully-sampled MRI data, and later used for on-line reconstruction of
undersampled data. Ideally network performance should be optimized by drawing
the training and testing data from the same domain. In practice, however, large
datasets comprising hundreds of subjects scanned under a common protocol are
rare. Here, we propose a transfer-learning approach to address the problem of
data scarcity in training deep networks for accelerated MRI. The proposed
approach trains neural networks using thousands of samples from a public
dataset of natural images (ImageNet). The network is then fine-tuned using only
few tens of MR images acquired in the testing domain (T1- or T2-weighted MRI).
The ImageNet-trained network yields nearly identical reconstructions to
networks trained directly in the testing domain using thousands of MR images,
and it outperforms conventional compressed sensing reconstructions in terms of
image quality. The proposed approach might facilitate the use of neural
networks for MRI reconstruction without the need for collection of extensive
imaging datasets.
</summary>
    <author>
      <name>Salman Ul Hassan Dar</name>
    </author>
    <author>
      <name>Tolga √áukur</name>
    </author>
    <link href="http://arxiv.org/abs/1710.02615v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.02615v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.00282v2</id>
    <updated>2018-01-11T05:36:36Z</updated>
    <published>2017-12-31T13:26:20Z</published>
    <title>Using Deep Neural Network Approximate Bayesian Network</title>
    <summary>  We present a new method to approximate posterior probabilities of Bayesian
Network using Deep Neural Network. Experiment results on several public
Bayesian Network datasets shows that Deep Neural Network is capable of learning
joint probability distri- bution of Bayesian Network by learning from a few
observation and posterior probability distribution pairs with high accuracy.
Compared with traditional approximate method likelihood weighting sampling
algorithm, our method is much faster and gains higher accuracy in medium sized
Bayesian Network. Another advantage of our method is that our method can be
parallelled much easier in GPU without extra effort. We also ex- plored the
connection between the accuracy of our model and the number of training
examples. The result shows that our model saturate as the number of training
examples grow and we don't need many training examples to get reasonably good
result. Another contribution of our work is that we have shown discriminative
model like Deep Neural Network can approximate generative model like Bayesian
Network.
</summary>
    <author>
      <name>Jie Jia</name>
    </author>
    <author>
      <name>Honggang Zhou</name>
    </author>
    <author>
      <name>Yunchun Li</name>
    </author>
    <link href="http://arxiv.org/abs/1801.00282v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.00282v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.3081v1</id>
    <updated>2010-03-16T05:13:41Z</updated>
    <published>2010-03-16T05:13:41Z</published>
    <title>Optimal hierarchical modular topologies for producing limited sustained
  activation of neural networks</title>
    <summary>  An essential requirement for the representation of functional patterns in
complex neural networks, such as the mammalian cerebral cortex, is the
existence of stable regimes of network activation, typically arising from a
limited parameter range. In this range of limited sustained activity (LSA), the
activity of neural populations in the network persists between the extremes of
either quickly dying out or activating the whole network. Hierarchical modular
networks were previously found to show a wider parameter range for LSA than
random or small-world networks not possessing hierarchical organization or
multiple modules. Here we explored how variation in the number of hierarchical
levels and modules per level influenced network dynamics and occurrence of LSA.
We tested hierarchical configurations of different network sizes, approximating
the large-scale networks linking cortical columns in one hemisphere of the rat,
cat, or macaque monkey brain. Scaling of the network size affected the number
of hierarchical levels and modules in the optimal networks, also depending on
whether global edge density or the numbers of connections per node were kept
constant. For constant edge density, only few network configurations,
possessing an intermediate number of levels and a large number of modules, led
to a large range of LSA independent of brain size. For a constant number of
node connections, there was a trend for optimal configurations in larger-size
networks to possess a larger number of hierarchical levels or more modules.
These results may help to explain the trend to greater network complexity
apparent in larger brains and may indicate that this complexity is required for
maintaining stable levels of neural activation.
</summary>
    <author>
      <name>Marcus Kaiser</name>
    </author>
    <author>
      <name>Claus C. Hilgetag</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3389/fninf.2010.00008</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3389/fninf.2010.00008" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Contribution to Frontiers in Neuroinformatics special issue on
  'Hierarchy and dynamics in neural networks'
  (http://frontiersin.org/neuroscience/neuroinformatics/specialtopics/29/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Kaiser M and Hilgetag CC (2010) Optimal hierarchical modular
  topologies for producing limited sustained activation of neural networks.
  Front. Neuroinform. 4:8</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.3081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.3081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0310050v4</id>
    <updated>2005-03-31T12:45:11Z</updated>
    <published>2003-10-27T14:27:14Z</published>
    <title>Feedforward Neural Networks with Diffused Nonlinear Weight Functions</title>
    <summary>  In this paper, feedforward neural networks are presented that have nonlinear
weight functions based on look--up tables, that are specially smoothed in a
regularization called the diffusion. The idea of such a type of networks is
based on the hypothesis that the greater number of adaptive parameters per a
weight function might reduce the total number of the weight functions needed to
solve a given problem. Then, if the computational complexity of a propagation
through a single such a weight function would be kept low, then the introduced
neural networks might possibly be relatively fast.
  A number of tests is performed, showing that the presented neural networks
may indeed perform better in some cases than the classic neural networks and a
number of other learning machines.
</summary>
    <author>
      <name>Artur Rataj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 7 figures. Corrected, some parts rewritten</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0310050v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0310050v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/q-bio/0310025v3</id>
    <updated>2003-10-21T22:07:29Z</updated>
    <published>2003-10-20T19:36:04Z</published>
    <title>Pattern Excitation-Based Processing: The Music of The Brain</title>
    <summary>  An approach to information processing based on the excitation of patterns of
activity by non-linear active resonators in response to their input patterns is
proposed. Arguments are presented to show that any computation performed by a
conventional Turing machine-based computer, called T-machine in this paper,
could also be performed by the pattern excitation-based machine, which will be
called P-machine. A realization of this processing scheme by neural networks is
discussed. In this realization, the role of the resonators is played by neural
pattern excitation networks, which are the neural circuits capable of exciting
different spatio-temporal patterns of activity in response to different inputs.
Learning in the neural pattern excitation networks is also considered. It is
shown that there is a duality between pattern excitation and pattern
recognition neural networks, which allows to create new pattern excitation
modes corresponding to recognizable input patterns, based on Hebbian learning
rules. Hierarchically organized, such networks can produce complex behavior.
Animal behavior, human language and thought are treated as examples produced by
such networks.
</summary>
    <author>
      <name>Lev Koyrakh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 7 figures, a reference corrected</arxiv:comment>
    <link href="http://arxiv.org/abs/q-bio/0310025v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/q-bio/0310025v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.2987v2</id>
    <updated>2010-05-26T08:55:11Z</updated>
    <published>2009-03-17T15:51:05Z</published>
    <title>Adaptive self-organization in a realistic neural network model</title>
    <summary>  Information processing in complex systems is often found to be maximally
efficient close to critical states associated with phase transitions. It is
therefore conceivable that also neural information processing operates close to
criticality. This is further supported by the observation of power-law
distributions, which are a hallmark of phase transitions. An important open
question is how neural networks could remain close to a critical point while
undergoing a continual change in the course of development, adaptation,
learning, and more. An influential contribution was made by Bornholdt and
Rohlf, introducing a generic mechanism of robust self-organized criticality in
adaptive networks. Here, we address the question whether this mechanism is
relevant for real neural networks. We show in a realistic model that
spike-time-dependent synaptic plasticity can self-organize neural networks
robustly toward criticality. Our model reproduces several empirical
observations and makes testable predictions on the distribution of synaptic
strength, relating them to the critical state of the network. These results
suggest that the interplay between dynamics and topology may be essential for
neural information processing.
</summary>
    <author>
      <name>Christian Meisel</name>
    </author>
    <author>
      <name>Thilo Gross</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.80.061917</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.80.061917" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 80, 061917 (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0903.2987v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.2987v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.1815v1</id>
    <updated>2009-12-09T17:47:49Z</updated>
    <published>2009-12-09T17:47:49Z</published>
    <title>Detection of Denial of Service Attacks against Domain Name System Using
  Neural Networks</title>
    <summary>  In this paper we introduce an intrusion detection system for Denial of
Service (DoS) attacks against Domain Name System (DNS). Our system architecture
consists of two most important parts: a statistical preprocessor and a neural
network classifier. The preprocessor extracts required statistical features in
a shorttime frame from traffic received by the target name server. We compared
three different neural networks for detecting and classifying different types
of DoS attacks. The proposed system is evaluated in a simulated network and
showed that the best performed neural network is a feed-forward backpropagation
with an accuracy of 99%.
</summary>
    <author>
      <name>Samaneh Rastegari</name>
    </author>
    <author>
      <name>M. Iqbal Saripan</name>
    </author>
    <author>
      <name>Mohd Fadlee A. Rasid</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues, IJCSI Volume 6,
  Issue 1, pp23-27, November 2009</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">S. Rastegari, M. I. Saripan and M. F. A. Rasid, "Detection of
  Denial of Service Attacks against Domain Name System Using Neural Networks",
  IJCSI, Volume 6, Issue 1, pp23-27, November 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0912.1815v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.1815v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.2770v4</id>
    <updated>2012-06-21T15:42:41Z</updated>
    <published>2012-02-13T15:48:04Z</published>
    <title>Multi-Level Error-Resilient Neural Networks with Learning</title>
    <summary>  The problem of neural network association is to retrieve a previously
memorized pattern from its noisy version using a network of neurons. An ideal
neural network should include three components simultaneously: a learning
algorithm, a large pattern retrieval capacity and resilience against noise.
Prior works in this area usually improve one or two aspects at the cost of the
third.
  Our work takes a step forward in closing this gap. More specifically, we show
that by forcing natural constraints on the set of learning patterns, we can
drastically improve the retrieval capacity of our neural network. Moreover, we
devise a learning algorithm whose role is to learn those patterns satisfying
the above mentioned constraints. Finally we show that our neural network can
cope with a fair amount of noise.
</summary>
    <author>
      <name>Amir Hesam Salavati</name>
    </author>
    <author>
      <name>Amin Karbasi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Part of this draft has been submitted to International Symposium on
  Information Theory (ISIT) 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1202.2770v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.2770v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.3134v1</id>
    <updated>2013-07-11T15:03:22Z</updated>
    <published>2013-07-11T15:03:22Z</published>
    <title>Air quality prediction using optimal neural networks with stochastic
  variables</title>
    <summary>  We apply recent methods in stochastic data analysis for discovering a set of
few stochastic variables that represent the relevant information on a
multivariate stochastic system, used as input for artificial neural networks
models for air quality forecast. We show that using these derived variables as
input variables for training the neural networks it is possible to
significantly reduce the amount of input variables necessary for the neural
network model, without considerably changing the predictive power of the model.
The reduced set of variables including these derived variables is therefore
proposed as optimal variable set for training neural networks models in
forecasting geophysical and weather properties. Finally, we briefly discuss
other possible applications of such optimized neural network models.
</summary>
    <author>
      <name>Ana Russo</name>
    </author>
    <author>
      <name>Frank Raischel</name>
    </author>
    <author>
      <name>Pedro G. Lind</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.atmosenv.2013.07.072</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.atmosenv.2013.07.072" rel="related"/>
    <link href="http://arxiv.org/abs/1307.3134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.3134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.ao-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ao-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.5417v1</id>
    <updated>2014-04-22T08:25:23Z</updated>
    <published>2014-04-22T08:25:23Z</published>
    <title>Attractor Metadynamics in Adapting Neural Networks</title>
    <summary>  Slow adaption processes, like synaptic and intrinsic plasticity, abound in
the brain and shape the landscape for the neural dynamics occurring on
substantially faster timescales. At any given time the network is characterized
by a set of internal parameters, which are adapting continuously, albeit
slowly. This set of parameters defines the number and the location of the
respective adiabatic attractors. The slow evolution of network parameters hence
induces an evolving attractor landscape, a process which we term attractor
metadynamics. We study the nature of the metadynamics of the attractor
landscape for several continuous-time autonomous model networks. We find both
first- and second-order changes in the location of adiabatic attractors and
argue that the study of the continuously evolving attractor landscape
constitutes a powerful tool for understanding the overall development of the
neural dynamics.
</summary>
    <author>
      <name>Claudius Gros</name>
    </author>
    <author>
      <name>Mathias Linkerhand</name>
    </author>
    <author>
      <name>Valentin Walther</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Artificial Neural Networks and Machine Learning-ICANN 2014 , S.
  Wermter et al. (Eds), pp. 65-72. Springer (2014)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.5417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.5417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.0595v1</id>
    <updated>2014-12-01T19:12:54Z</updated>
    <published>2014-12-01T19:12:54Z</published>
    <title>Scalability and Optimization Strategies for GPU Enhanced Neural Networks
  (GeNN)</title>
    <summary>  Simulation of spiking neural networks has been traditionally done on
high-performance supercomputers or large-scale clusters. Utilizing the parallel
nature of neural network computation algorithms, GeNN (GPU Enhanced Neural
Network) provides a simulation environment that performs on General Purpose
NVIDIA GPUs with a code generation based approach. GeNN allows the users to
design and simulate neural networks by specifying the populations of neurons at
different stages, their synapse connection densities and the model of
individual neurons. In this report we describe work on how to scale synaptic
weights based on the configuration of the user-defined network to ensure
sufficient spiking and subsequent effective learning. We also discuss
optimization strategies particular to GPU computing: sparse representation of
synapse connections and occupancy based block-size determination.
</summary>
    <author>
      <name>Naresh Balaji</name>
    </author>
    <author>
      <name>Esin Yavuz</name>
    </author>
    <author>
      <name>Thomas Nowotny</name>
    </author>
    <link href="http://arxiv.org/abs/1412.0595v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.0595v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.09308v2</id>
    <updated>2015-11-10T20:08:41Z</updated>
    <published>2015-09-30T19:39:20Z</published>
    <title>Fast Algorithms for Convolutional Neural Networks</title>
    <summary>  Deep convolutional neural networks take GPU days of compute time to train on
large data sets. Pedestrian detection for self driving cars requires very low
latency. Image recognition for mobile phones is constrained by limited
processing resources. The success of convolutional neural networks in these
situations is limited by how fast we can compute them. Conventional FFT based
convolution is fast for large filters, but state of the art convolutional
neural networks use small, 3x3 filters. We introduce a new class of fast
algorithms for convolutional neural networks using Winograd's minimal filtering
algorithms. The algorithms compute minimal complexity convolution over small
tiles, which makes them fast with small filters and small batch sizes. We
benchmark a GPU implementation of our algorithm with the VGG network and show
state of the art throughput at batch sizes from 1 to 64.
</summary>
    <author>
      <name>Andrew Lavin</name>
    </author>
    <author>
      <name>Scott Gray</name>
    </author>
    <link href="http://arxiv.org/abs/1509.09308v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.09308v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; F.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.03651v2</id>
    <updated>2016-10-13T07:11:46Z</updated>
    <published>2016-01-14T16:30:41Z</published>
    <title>Improved Relation Classification by Deep Recurrent Neural Networks with
  Data Augmentation</title>
    <summary>  Nowadays, neural networks play an important role in the task of relation
classification. By designing different neural architectures, researchers have
improved the performance to a large extent in comparison with traditional
methods. However, existing neural networks for relation classification are
usually of shallow architectures (e.g., one-layer convolutional neural networks
or recurrent networks). They may fail to explore the potential representation
space in different abstraction levels. In this paper, we propose deep recurrent
neural networks (DRNNs) for relation classification to tackle this challenge.
Further, we propose a data augmentation method by leveraging the directionality
of relations. We evaluated our DRNNs on the SemEval-2010 Task~8, and achieve an
F1-score of 86.1%, outperforming previous state-of-the-art recorded results.
</summary>
    <author>
      <name>Yan Xu</name>
    </author>
    <author>
      <name>Ran Jia</name>
    </author>
    <author>
      <name>Lili Mou</name>
    </author>
    <author>
      <name>Ge Li</name>
    </author>
    <author>
      <name>Yunchuan Chen</name>
    </author>
    <author>
      <name>Yangyang Lu</name>
    </author>
    <author>
      <name>Zhi Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by COLING-16</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.03651v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.03651v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.04010v2</id>
    <updated>2017-05-18T09:15:26Z</updated>
    <published>2016-11-12T15:59:22Z</published>
    <title>Multi-Language Identification Using Convolutional Recurrent Neural
  Network</title>
    <summary>  Language Identification, being an important aspect of Automatic Speaker
Recognition has had many changes and new approaches to ameliorate performance
over the last decade. We compare the performance of using audio spectrum in the
log scale and using Polyphonic sound sequences from raw audio samples to train
the neural network and to classify speech as either English or Spanish. To
achieve this, we use the novel approach of using a Convolutional Recurrent
Neural Network using Long Short Term Memory (LSTM) or a Gated Recurrent Unit
(GRU) for forward propagation of the neural network. Our hypothesis is that the
performance of using polyphonic sound sequence as features and both LSTM and
GRU as the gating mechanisms for the neural network outperform the traditional
MFCC features using a unidirectional Deep Neural Network.
</summary>
    <author>
      <name>Vrishabh Ajay Lakhani</name>
    </author>
    <author>
      <name>Rohan Mahadev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Further experiments were performed on the model using LibriVox speech
  dataset and it was found that a Time Distributed CRNN model performed better
  and represented our initial ideas about the speaker recognition task better.
  The dataset contains speech in three languages - English, Spanish and Czech.
  A report on our findings along with experimental results will be published
  soon</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.04010v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.04010v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.08835v2</id>
    <updated>2017-05-31T17:15:34Z</updated>
    <published>2017-02-28T16:10:31Z</published>
    <title>Deep Forest: Towards An Alternative to Deep Neural Networks</title>
    <summary>  In this paper, we propose gcForest, a decision tree ensemble approach with
performance highly competitive to deep neural networks. In contrast to deep
neural networks which require great effort in hyper-parameter tuning, gcForest
is much easier to train. Actually, even when gcForest is applied to different
data from different domains, excellent performance can be achieved by almost
same settings of hyper-parameters. The training process of gcForest is
efficient and scalable. In our experiments its training time running on a PC is
comparable to that of deep neural networks running with GPU facilities, and the
efficiency advantage may be more apparent because gcForest is naturally apt to
parallel implementation. Furthermore, in contrast to deep neural networks which
require large-scale training data, gcForest can work well even when there are
only small-scale training data. Moreover, as a tree-based approach, gcForest
should be easier for theoretical analysis than deep neural networks.
</summary>
    <author>
      <name>Zhi-Hua Zhou</name>
    </author>
    <author>
      <name>Ji Feng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IJCAI 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.08835v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.08835v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.10371v2</id>
    <updated>2017-12-06T19:10:46Z</updated>
    <published>2017-03-30T09:10:09Z</published>
    <title>Born to Learn: the Inspiration, Progress, and Future of Evolved Plastic
  Artificial Neural Networks</title>
    <summary>  Biological plastic neural networks are systems of extraordinary computational
capabilities shaped by evolution, development, and lifetime learning. The
interplay of these elements leads to the emergence of adaptive behavior and
intelligence. Inspired by such intricate natural phenomena, Evolved Plastic
Artificial Neural Networks (EPANNs) use simulated evolution in-silico to breed
plastic neural networks with a large variety of dynamics, architectures, and
plasticity rules: these artificial systems are composed of inputs, outputs, and
plastic components that change in response to experiences in an environment.
These systems may autonomously discover novel adaptive algorithms, and lead to
hypotheses on the emergence of biological adaptation. EPANNs have seen
considerable progress over the last two decades. Current scientific and
technological advances in artificial neural networks are now setting the
conditions for radically new approaches and results. In particular, the
limitations of hand-designed networks could be overcome by more flexible and
innovative solutions. This paper brings together a variety of inspiring ideas
that define the field of EPANNs. The main methods and results are reviewed.
Finally, new opportunities and developments are presented.
</summary>
    <author>
      <name>Andrea Soltoggio</name>
    </author>
    <author>
      <name>Kenneth O. Stanley</name>
    </author>
    <author>
      <name>Sebastian Risi</name>
    </author>
    <link href="http://arxiv.org/abs/1703.10371v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.10371v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.05892v1</id>
    <updated>2017-06-19T11:45:50Z</updated>
    <published>2017-06-19T11:45:50Z</published>
    <title>Identification of an Open-loop Plasma Vertical Position Using Fractional
  Order Dynamic Neural Network</title>
    <summary>  In order to identify complicated systems, more prominent and promising
methods are needed among which we may refer to fractional order differential
equations. The aim of this paper is to propose a fractional order nonlinear
model to predict the vertical position of a plasma column system in a Tokamak
by using real data from Damavand Tokamak. The system is identified based on a
newly introduced fractional order dynamic neural network. The proposed
fractional order dynamic neural network (FODNN) is an extension of the integer
order dynamic neural network that employs the so called fractional-order
operators. FODNN is implemented and comparison of the numerical simulation
results with experimental results shows that performance of the proposed method
by using fractional order neural network is preferred to the integer neural
network.
</summary>
    <author>
      <name>Z. Aslipour</name>
    </author>
    <author>
      <name>A. Yazdizadeh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages,8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.05892v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.05892v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.plasm-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.plasm-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09453v2</id>
    <updated>2017-07-04T17:03:20Z</updated>
    <published>2017-06-28T19:41:25Z</published>
    <title>Toward Computation and Memory Efficient Neural Network Acoustic Models
  with Binary Weights and Activations</title>
    <summary>  Neural network acoustic models have significantly advanced state of the art
speech recognition over the past few years. However, they are usually
computationally expensive due to the large number of matrix-vector
multiplications and nonlinearity operations. Neural network models also require
significant amounts of memory for inference because of the large model size.
For these two reasons, it is challenging to deploy neural network based speech
recognizers on resource-constrained platforms such as embedded devices. This
paper investigates the use of binary weights and activations for computation
and memory efficient neural network acoustic models. Compared to real-valued
weight matrices, binary weights require much fewer bits for storage, thereby
cutting down the memory footprint. Furthermore, with binary weights or
activations, the matrix-vector multiplications are turned into addition and
subtraction operations, which are computationally much faster and more energy
efficient for hardware platforms. In this paper, we study the applications of
binary weights and activations for neural network acoustic modeling, reporting
encouraging results on the WSJ and AMI corpora.
</summary>
    <author>
      <name>Liang Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.09453v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09453v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.05922v1</id>
    <updated>2017-07-19T02:29:49Z</updated>
    <published>2017-07-19T02:29:49Z</published>
    <title>Improving Output Uncertainty Estimation and Generalization in Deep
  Learning via Neural Network Gaussian Processes</title>
    <summary>  We propose a simple method that combines neural networks and Gaussian
processes. The proposed method can estimate the uncertainty of outputs and
flexibly adjust target functions where training data exist, which are
advantages of Gaussian processes. The proposed method can also achieve high
generalization performance for unseen input configurations, which is an
advantage of neural networks. With the proposed method, neural networks are
used for the mean functions of Gaussian processes. We present a scalable
stochastic inference procedure, where sparse Gaussian processes are inferred by
stochastic variational inference, and the parameters of neural networks and
kernels are estimated by stochastic gradient descent methods, simultaneously.
We use two real-world spatio-temporal data sets to demonstrate experimentally
that the proposed method achieves better uncertainty estimation and
generalization performance than neural networks and Gaussian processes.
</summary>
    <author>
      <name>Tomoharu Iwata</name>
    </author>
    <author>
      <name>Zoubin Ghahramani</name>
    </author>
    <link href="http://arxiv.org/abs/1707.05922v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.05922v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.03344v1</id>
    <updated>2017-10-09T22:51:28Z</updated>
    <published>2017-10-09T22:51:28Z</published>
    <title>Iterative PET Image Reconstruction Using Convolutional Neural Network
  Representation</title>
    <summary>  PET image reconstruction is challenging due to the ill-poseness of the
inverse problem and limited number of detected photons. Recently deep neural
networks have been widely and successfully used in computer vision tasks and
attracted growing interests in medical imaging. In this work, we trained a deep
residual convolutional neural network to improve PET image quality by using the
existing inter-patient information. An innovative feature of the proposed
method is that we embed the neural network in the iterative reconstruction
framework for image representation, rather than using it as a post-processing
tool. We formulate the objective function as a constraint optimization problem
and solve it using the alternating direction method of multipliers (ADMM)
algorithm. Both simulation data and hybrid real data are used to evaluate the
proposed method. Quantification results show that our proposed iterative neural
network method can outperform the neural network denoising and conventional
penalized maximum likelihood methods.
</summary>
    <author>
      <name>Kuang Gong</name>
    </author>
    <author>
      <name>Jiahui Guan</name>
    </author>
    <author>
      <name>Kyungsang Kim</name>
    </author>
    <author>
      <name>Xuezhu Zhang</name>
    </author>
    <author>
      <name>Georges El Fakhri</name>
    </author>
    <author>
      <name>Jinyi Qi</name>
    </author>
    <author>
      <name>Quanzheng Li</name>
    </author>
    <link href="http://arxiv.org/abs/1710.03344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.03344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.03425v1</id>
    <updated>2017-10-10T07:16:54Z</updated>
    <published>2017-10-10T07:16:54Z</published>
    <title>AdaDNNs: Adaptive Ensemble of Deep Neural Networks for Scene Text
  Recognition</title>
    <summary>  Recognizing text in the wild is a really challenging task because of complex
backgrounds, various illuminations and diverse distortions, even with deep
neural networks (convolutional neural networks and recurrent neural networks).
In the end-to-end training procedure for scene text recognition, the outputs of
deep neural networks at different iterations are always demonstrated with
diversity and complementarity for the target object (text). Here, a simple but
effective deep learning method, an adaptive ensemble of deep neural networks
(AdaDNNs), is proposed to simply select and adaptively combine classifier
components at different iterations from the whole learning system. Furthermore,
the ensemble is formulated as a Bayesian framework for classifier weighting and
combination. A variety of experiments on several typical acknowledged
benchmarks, i.e., ICDAR Robust Reading Competition (Challenge 1, 2 and 4)
datasets, verify the surprised improvement from the baseline DNNs, and the
effectiveness of AdaDNNs compared with the recent state-of-the-art methods.
</summary>
    <author>
      <name>Chun Yang</name>
    </author>
    <author>
      <name>Xu-Cheng Yin</name>
    </author>
    <author>
      <name>Zejun Li</name>
    </author>
    <author>
      <name>Jianwei Wu</name>
    </author>
    <author>
      <name>Chunchao Guo</name>
    </author>
    <author>
      <name>Hongfa Wang</name>
    </author>
    <author>
      <name>Lei Xiao</name>
    </author>
    <link href="http://arxiv.org/abs/1710.03425v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.03425v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.11431v2</id>
    <updated>2018-02-20T17:33:48Z</updated>
    <published>2017-10-31T12:24:26Z</published>
    <title>Physics-guided Neural Networks (PGNN): An Application in Lake
  Temperature Modeling</title>
    <summary>  This paper introduces a novel framework for combining scientific knowledge of
physics-based models with neural networks to advance scientific discovery. This
framework, termed as physics-guided neural network (PGNN), leverages the output
of physics-based model simulations along with observational features to
generate predictions using a neural network architecture. Further, this paper
presents a novel framework for using physics-based loss functions in the
learning objective of neural networks, to ensure that the model predictions not
only show lower errors on the training set but are also scientifically
consistent with the known physics on the unlabeled set. We illustrate the
effectiveness of PGNN for the problem of lake temperature modeling, where
physical relationships between the temperature, density, and depth of water are
used to design a physics-based loss function. By using scientific knowledge to
guide the construction and learning of neural networks, we are able to show
that the proposed framework ensures better generalizability as well as
scientific consistency of results.
</summary>
    <author>
      <name>Anuj Karpatne</name>
    </author>
    <author>
      <name>William Watkins</name>
    </author>
    <author>
      <name>Jordan Read</name>
    </author>
    <author>
      <name>Vipin Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to ACM SIGKDD 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.11431v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.11431v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.05133v2</id>
    <updated>2017-11-15T07:47:34Z</updated>
    <published>2017-11-14T14:54:23Z</published>
    <title>Reinforcement Learning in a large scale photonic Recurrent Neural
  Network</title>
    <summary>  Photonic Neural Network implementations have been gaining considerable
attention as a potentially disruptive future technology. Demonstrating learning
in large scale neural networks is essential to establish photonic machine
learning substrates as viable information processing systems. Realizing
photonic Neural Networks with numerous nonlinear nodes in a fully parallel and
efficient learning hardware was lacking so far. We demonstrate a network of up
to 2500 diffractively coupled photonic nodes, forming a large scale Recurrent
Neural Network. Using a Digital Micro Mirror Device, we realize reinforcement
learning. Our scheme is fully parallel, and the passive weights maximize energy
efficiency and bandwidth. The computational output efficiently converges and we
achieve very good performance.
</summary>
    <author>
      <name>Julian Bueno</name>
    </author>
    <author>
      <name>Sheler Maktoobi</name>
    </author>
    <author>
      <name>Luc Froehly</name>
    </author>
    <author>
      <name>Ingo Fischer</name>
    </author>
    <author>
      <name>Maxime Jacquot</name>
    </author>
    <author>
      <name>Laurent Larger</name>
    </author>
    <author>
      <name>Daniel Brunner</name>
    </author>
    <link href="http://arxiv.org/abs/1711.05133v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.05133v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.02643v1</id>
    <updated>2018-02-07T21:29:52Z</updated>
    <published>2018-02-07T21:29:52Z</published>
    <title>Gradient conjugate priors and deep neural networks</title>
    <summary>  The paper deals with learning the probability distribution of the observed
data by artificial neural networks. We suggest a so-called gradient conjugate
prior (GCP) update appropriate for neural networks, which is a modification of
the classical Bayesian update for conjugate priors. We establish a connection
between the gradient conjugate prior update and the maximization of the
log-likelihood of the predictive distribution. Unlike for the Bayesian neural
networks, we do not impose a prior on the weights of the neural networks, but
rather assume that the ground truth distribution is normal with unknown mean
and variance and learn by neural networks the parameters of a prior
(normal-gamma distribution) for these unknown mean and variance. The update of
the parameters is done, using the gradient that, at each step, directs towards
minimizing the Kullback--Leibler divergence from the prior to the posterior
distribution (both being normal-gamma). We obtain a corresponding dynamical
system for the prior's parameters and analyze its properties. In particular, we
study the limiting behavior of all the prior's parameters and show how it
differs from the case of the classical full Bayesian update. The results are
validated on synthetic and real world data sets.
</summary>
    <author>
      <name>Pavel Gurevich</name>
    </author>
    <author>
      <name>Hannes Stuke</name>
    </author>
    <link href="http://arxiv.org/abs/1802.02643v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.02643v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.06981v1</id>
    <updated>2018-02-20T06:58:32Z</updated>
    <published>2018-02-20T06:58:32Z</published>
    <title>Reachable Set Estimation and Safety Verification for Piecewise Linear
  Systems with Neural Network Controllers</title>
    <summary>  In this work, the reachable set estimation and safety verification problems
for a class of piecewise linear systems equipped with neural network
controllers are addressed. The neural network is considered to consist of
Rectified Linear Unit (ReLU) activation functions. A layer-by-layer approach is
developed for the output reachable set computation of ReLU neural networks. The
computation is formulated in the form of a set of manipulations for a union of
polytopes. Based on the output reachable set for neural network controllers,
the output reachable set for a piecewise linear feedback control system can be
estimated iteratively for a given finite-time interval. With the estimated
output reachable set, the safety verification for piecewise linear systems with
neural network controllers can be performed by checking the existence of
intersections of unsafe regions and output reach set. A numerical example is
presented to illustrate the effectiveness of our approach.
</summary>
    <author>
      <name>Weiming Xiang</name>
    </author>
    <author>
      <name>Hoang-Dung Tran</name>
    </author>
    <author>
      <name>Joel A. Rosenfeld</name>
    </author>
    <author>
      <name>Taylor T. Johnson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures, ACC 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.06981v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06981v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.2336v1</id>
    <updated>2010-11-10T10:40:46Z</updated>
    <published>2010-11-10T10:40:46Z</published>
    <title>A network model with structured nodes</title>
    <summary>  We present a network model in which words over a specific alphabet, called
{\it structures}, are associated to each node and undirected edges are added
depending on some distance between different structures. It is shown that this
model can generate, without the use of preferential attachment or any other
heuristic, networks with topological features similar to biological networks:
power law degree distribution, clustering coefficient independent from the
network size, etc. Specific biological networks ({\it C. Elegans} neural
network and {\it E. Coli} protein-protein interaction network) are replicated
using this model.
</summary>
    <author>
      <name>P. Frisco</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.84.021931</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.84.021931" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">match MRSA gene network not present because MRSA gene network still
  unpublished</arxiv:comment>
    <link href="http://arxiv.org/abs/1011.2336v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.2336v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07706v1</id>
    <updated>2016-09-25T06:44:42Z</updated>
    <published>2016-09-25T06:44:42Z</published>
    <title>Learning by Stimulation Avoidance: A Principle to Control Spiking Neural
  Networks Dynamics</title>
    <summary>  Learning based on networks of real neurons, and by extension biologically
inspired models of neural networks, has yet to find general learning rules
leading to widespread applications. In this paper, we argue for the existence
of a principle allowing to steer the dynamics of a biologically inspired neural
network. Using carefully timed external stimulation, the network can be driven
towards a desired dynamical state. We term this principle "Learning by
Stimulation Avoidance" (LSA). We demonstrate through simulation that the
minimal sufficient conditions leading to LSA in artificial networks are also
sufficient to reproduce learning results similar to those obtained in
biological neurons by Shahaf and Marom [1]. We examine the mechanism's basic
dynamics in a reduced network, and demonstrate how it scales up to a network of
100 neurons. We show that LSA has a higher explanatory power than existing
hypotheses about the response of biological neural networks to external
simulation, and can be used as a learning rule for an embodied application:
learning of wall avoidance by a simulated robot. The surge in popularity of
artificial neural networks is mostly directed to disembodied models of neurons
with biologically irrelevant dynamics: to the authors' knowledge, this is the
first work demonstrating sensory-motor learning with random spiking networks
through pure Hebbian learning.
</summary>
    <author>
      <name>Lana Sinapayen</name>
    </author>
    <author>
      <name>Atsushi Masumori</name>
    </author>
    <author>
      <name>Takashi Ikegami</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pone.0170388</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pone.0170388" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.07706v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07706v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/q-bio/0505021v1</id>
    <updated>2005-05-10T19:51:16Z</updated>
    <published>2005-05-10T19:51:16Z</published>
    <title>Characterizing Self-Developing Biological Neural Networks: A First Step
  Towards their Application To Computing Systems</title>
    <summary>  Carbon nanotubes are often seen as the only alternative technology to silicon
transistors. While they are the most likely short-term one, other longer-term
alternatives should be studied as well. While contemplating biological neurons
as an alternative component may seem preposterous at first sight, significant
recent progress in CMOS-neuron interface suggests this direction may not be
unrealistic; moreover, biological neurons are known to self-assemble into very
large networks capable of complex information processing tasks, something that
has yet to be achieved with other emerging technologies. The first step to
designing computing systems on top of biological neurons is to build an
abstract model of self-assembled biological neural networks, much like computer
architects manipulate abstract models of transistors and circuits. In this
article, we propose a first model of the structure of biological neural
networks. We provide empirical evidence that this model matches the biological
neural networks found in living organisms, and exhibits the small-world graph
structure properties commonly found in many large and self-organized systems,
including biological neural networks. More importantly, we extract the simple
local rules and characteristics governing the growth of such networks, enabling
the development of potentially large but realistic biological neural networks,
as would be needed for complex information processing/computing tasks. Based on
this model, future work will be targeted to understanding the evolution and
learning properties of such networks, and how they can be used to build
computing systems.
</summary>
    <author>
      <name>Hugues Berry</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs</arxiv:affiliation>
    </author>
    <author>
      <name>Olivier Temam</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/q-bio/0505021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/q-bio/0505021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.07174v2</id>
    <updated>2016-12-27T04:53:56Z</updated>
    <published>2016-11-22T07:36:21Z</published>
    <title>Deep Recurrent Convolutional Neural Network: Improving Performance For
  Speech Recognition</title>
    <summary>  A deep learning approach has been widely applied in sequence modeling
problems. In terms of automatic speech recognition (ASR), its performance has
significantly been improved by increasing large speech corpus and deeper neural
network. Especially, recurrent neural network and deep convolutional neural
network have been applied in ASR successfully. Given the arising problem of
training speed, we build a novel deep recurrent convolutional network for
acoustic modeling and then apply deep residual learning to it. Our experiments
show that it has not only faster convergence speed but better recognition
accuracy over traditional deep convolutional recurrent network. In the
experiments, we compare the convergence speed of our novel deep recurrent
convolutional networks and traditional deep convolutional recurrent networks.
With faster convergence speed, our novel deep recurrent convolutional networks
can reach the comparable performance. We further show that applying deep
residual learning can boost the convergence speed of our novel deep recurret
convolutional networks. Finally, we evaluate all our experimental networks by
phoneme error rate (PER) with our proposed bidirectional statistical n-gram
language model. Our evaluation results show that our newly proposed deep
recurrent convolutional network applied with deep residual learning can reach
the best PER of 17.33\% with the fastest convergence speed on TIMIT database.
The outstanding performance of our novel deep recurrent convolutional neural
network with deep residual learning indicates that it can be potentially
adopted in other sequential problems.
</summary>
    <author>
      <name>Zewang Zhang</name>
    </author>
    <author>
      <name>Zheng Sun</name>
    </author>
    <author>
      <name>Jiaqi Liu</name>
    </author>
    <author>
      <name>Jingwen Chen</name>
    </author>
    <author>
      <name>Zhao Huo</name>
    </author>
    <author>
      <name>Xiao Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.07174v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.07174v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00539v1</id>
    <updated>2018-02-02T02:12:33Z</updated>
    <published>2018-02-02T02:12:33Z</published>
    <title>Complex Network Classification with Convolutional Neural Network</title>
    <summary>  Classifying large scale networks into several categories and distinguishing
them according to their fine structures is of great importance with several
applications in real life. However, most studies of complex networks focus on
properties of a single network but seldom on classification, clustering, and
comparison between different networks, in which the network is treated as a
whole. Due to the non-Euclidean properties of the data, conventional methods
can hardly be applied on networks directly. In this paper, we propose a novel
framework of complex network classifier (CNC) by integrating network embedding
and convolutional neural network to tackle the problem of network
classification. By training the classifiers on synthetic complex network data
and real international trade network data, we show CNC can not only classify
networks in a high accuracy and robustness, it can also extract the features of
the networks automatically.
</summary>
    <author>
      <name>Ruyue Xin</name>
    </author>
    <author>
      <name>Jiang Zhang</name>
    </author>
    <author>
      <name>Yitong Shao</name>
    </author>
    <link href="http://arxiv.org/abs/1802.00539v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00539v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505019v1</id>
    <updated>2005-05-10T06:37:31Z</updated>
    <published>2005-05-10T06:37:31Z</published>
    <title>Artificial Neural Networks and their Applications</title>
    <summary>  The Artificial Neural network is a functional imitation of simplified model
of the biological neurons and their goal is to construct useful computers for
real world problems. The ANN applications have increased dramatically in the
last few years fired by both theoretical and practical applications in a wide
variety of applications. A brief theory of ANN is presented and potential areas
are identified and future trends are discussed.
</summary>
    <author>
      <name>Nitin Malik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0607019v1</id>
    <updated>2006-07-06T18:49:20Z</updated>
    <published>2006-07-06T18:49:20Z</published>
    <title>Modelling the Probability Density of Markov Sources</title>
    <summary>  This paper introduces an objective function that seeks to minimise the
average total number of bits required to encode the joint state of all of the
layers of a Markov source. This type of encoder may be applied to the problem
of optimising the bottom-up (recognition model) and top-down (generative model)
connections in a multilayer neural network, and it unifies several previous
results on the optimisation of multilayer neural networks.
</summary>
    <author>
      <name>Stephen Luttrell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0607019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0607019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.0197v1</id>
    <updated>2008-05-02T09:20:11Z</updated>
    <published>2008-05-02T09:20:11Z</published>
    <title>Flatness of the Energy Landscape for Horn Clauses</title>
    <summary>  The Little-Hopfield neural network programmed with Horn clauses is studied.
We argue that the energy landscape of the system, corresponding to the
inconsistency function for logical interpretations of the sets of Horn clauses,
has minimal ruggedness. This is supported by computer simulations.
</summary>
    <author>
      <name>Saratha Sathasivam</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">USM</arxiv:affiliation>
    </author>
    <author>
      <name>Wan Ahmad Tajuddin Wan Abdullah</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Univ. Malaya</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Matematika 23 (2007) 147-156</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0805.0197v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.0197v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.0968v1</id>
    <updated>2014-06-04T08:25:56Z</updated>
    <published>2014-06-04T08:25:56Z</published>
    <title>Integration of a Predictive, Continuous Time Neural Network into
  Securities Market Trading Operations</title>
    <summary>  This paper describes recent development and test implementation of a
continuous time recurrent neural network that has been configured to predict
rates of change in securities. It presents outcomes in the context of popular
technical analysis indicators and highlights the potential impact of continuous
predictive capability on securities market trading operations.
</summary>
    <author>
      <name>Christopher S Kirk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.0968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.0968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.1434v1</id>
    <updated>2010-06-08T01:17:00Z</updated>
    <published>2010-06-08T01:17:00Z</published>
    <title>Computing by Means of Physics-Based Optical Neural Networks</title>
    <summary>  We report recent research on computing with biology-based neural network
models by means of physics-based opto-electronic hardware. New technology
provides opportunities for very-high-speed computation and uncovers problems
obstructing the wide-spread use of this new capability. The Computation
Modeling community may be able to offer solutions to these cross-boundary
research problems.
</summary>
    <author>
      <name>A. Steven Younger</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Missouri State University</arxiv:affiliation>
    </author>
    <author>
      <name>Emmett Redd</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Missouri State University</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.26.15</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.26.15" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 26, 2010, pp. 159-167</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1006.1434v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.1434v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.00299v1</id>
    <updated>2015-01-01T18:37:36Z</updated>
    <published>2015-01-01T18:37:36Z</published>
    <title>Sequence Modeling using Gated Recurrent Neural Networks</title>
    <summary>  In this paper, we have used Recurrent Neural Networks to capture and model
human motion data and generate motions by prediction of the next immediate data
point at each time-step. Our RNN is armed with recently proposed Gated
Recurrent Units which has shown promising results in some sequence modeling
problems such as Machine Translation and Speech Synthesis. We demonstrate that
this model is able to capture long-term dependencies in data and generate
realistic motions.
</summary>
    <author>
      <name>Mohammad Pezeshki</name>
    </author>
    <link href="http://arxiv.org/abs/1501.00299v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.00299v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.02788v1</id>
    <updated>2015-08-12T01:01:11Z</updated>
    <published>2015-08-12T01:01:11Z</published>
    <title>The Effects of Hyperparameters on SGD Training of Neural Networks</title>
    <summary>  The performance of neural network classifiers is determined by a number of
hyperparameters, including learning rate, batch size, and depth. A number of
attempts have been made to explore these parameters in the literature, and at
times, to develop methods for optimizing them. However, exploration of
parameter spaces has often been limited. In this note, I report the results of
large scale experiments exploring these different parameters and their
interactions.
</summary>
    <author>
      <name>Thomas M. Breuel</name>
    </author>
    <link href="http://arxiv.org/abs/1508.02788v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.02788v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.04568v1</id>
    <updated>2016-01-18T15:22:48Z</updated>
    <published>2016-01-18T15:22:48Z</published>
    <title>Content Aware Neural Style Transfer</title>
    <summary>  This paper presents a content-aware style transfer algorithm for paintings
and photos of similar content using pre-trained neural network, obtaining
better results than the previous work. In addition, the numerical experiments
show that the style pattern and the content information is not completely
separated by neural network.
</summary>
    <author>
      <name>Rujie Yin</name>
    </author>
    <link href="http://arxiv.org/abs/1601.04568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.04568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.10; I.5.2; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09002v1</id>
    <updated>2016-03-29T23:48:27Z</updated>
    <published>2016-03-29T23:48:27Z</published>
    <title>Dataflow Matrix Machines as a Generalization of Recurrent Neural
  Networks</title>
    <summary>  Dataflow matrix machines are a powerful generalization of recurrent neural
networks. They work with multiple types of arbitrary linear streams, multiple
types of powerful neurons, and allow to incorporate higher-order constructions.
We expect them to be useful in machine learning and probabilistic programming,
and in the synthesis of dynamic systems and of deterministic and probabilistic
programs.
</summary>
    <author>
      <name>Michael Bukatin</name>
    </author>
    <author>
      <name>Steve Matthews</name>
    </author>
    <author>
      <name>Andrey Radul</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages position paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.09002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.06212v1</id>
    <updated>2016-12-19T14:59:14Z</updated>
    <published>2016-12-19T14:59:14Z</published>
    <title>A recurrent neural network without chaos</title>
    <summary>  We introduce an exceptionally simple gated recurrent neural network (RNN)
that achieves performance comparable to well-known gated architectures, such as
LSTMs and GRUs, on the word-level language modeling task. We prove that our
model has simple, predicable and non-chaotic dynamics. This stands in stark
contrast to more standard gated architectures, whose underlying dynamical
systems exhibit chaotic behavior.
</summary>
    <author>
      <name>Thomas Laurent</name>
    </author>
    <author>
      <name>James von Brecht</name>
    </author>
    <link href="http://arxiv.org/abs/1612.06212v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.06212v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05923v1</id>
    <updated>2017-01-20T20:53:51Z</updated>
    <published>2017-01-20T20:53:51Z</published>
    <title>Gate-Variants of Gated Recurrent Unit (GRU) Neural Networks</title>
    <summary>  The paper evaluates three variants of the Gated Recurrent Unit (GRU) in
recurrent neural networks (RNN) by reducing parameters in the update and reset
gates. We evaluate the three variant GRU models on MNIST and IMDB datasets and
show that these GRU-RNN variant models perform as well as the original GRU RNN
model while reducing the computational expense.
</summary>
    <author>
      <name>Rahul Dey</name>
    </author>
    <author>
      <name>Fathi M. Salem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 8 Figures, 4 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.05923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.03523v1</id>
    <updated>2018-01-09T03:35:20Z</updated>
    <published>2018-01-09T03:35:20Z</published>
    <title>Generative Models for Stochastic Processes Using Convolutional Neural
  Networks</title>
    <summary>  The present paper aims to demonstrate the usage of Convolutional Neural
Networks as a generative model for stochastic processes, enabling researchers
from a wide range of fields (such as quantitative finance and physics) to
develop a general tool for forecasts and simulations without the need to
identify/assume a specific system structure or estimate its parameters.
</summary>
    <author>
      <name>Fernando Fernandes Neto</name>
    </author>
    <link href="http://arxiv.org/abs/1801.03523v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.03523v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04846v1</id>
    <updated>2016-09-15T20:21:30Z</updated>
    <published>2016-09-15T20:21:30Z</published>
    <title>A Tutorial about Random Neural Networks in Supervised Learning</title>
    <summary>  Random Neural Networks (RNNs) are a class of Neural Networks (NNs) that can
also be seen as a specific type of queuing network. They have been successfully
used in several domains during the last 25 years, as queuing networks to
analyze the performance of resource sharing in many engineering areas, as
learning tools and in combinatorial optimization, where they are seen as neural
systems, and also as models of neurological aspects of living beings. In this
article we focus on their learning capabilities, and more specifically, we
present a practical guide for using the RNN to solve supervised learning
problems. We give a general description of these models using almost
indistinctly the terminology of Queuing Theory and the neural one. We present
the standard learning procedures used by RNNs, adapted from similar
well-established improvements in the standard NN field. We describe in
particular a set of learning algorithms covering techniques based on the use of
first order and, then, of second order derivatives. We also discuss some issues
related to these objects and present new perspectives about their use in
supervised learning problems. The tutorial describes their most relevant
applications, and also provides a large bibliography.
</summary>
    <author>
      <name>Sebasti√°n Basterrech</name>
    </author>
    <author>
      <name>Gerardo Rubino</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14311/NNW.2015.25.024</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14311/NNW.2015.25.024" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is a draft of an article to be published in Neural Network
  World</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Network World, Volume 5, Number 15, pp.:457-499, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1609.04846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09934v2</id>
    <updated>2017-08-22T09:19:11Z</updated>
    <published>2016-11-29T23:06:21Z</published>
    <title>Neural Network Models for Software Development Effort Estimation: A
  Comparative Study</title>
    <summary>  Software development effort estimation (SDEE) is one of the main tasks in
software project management. It is crucial for a project manager to efficiently
predict the effort or cost of a software project in a bidding process, since
overestimation will lead to bidding loss and underestimation will cause the
company to lose money. Several SDEE models exist; machine learning models,
especially neural network models, are among the most prominent in the field. In
this study, four different neural network models: Multilayer Perceptron,
General Regression Neural Network, Radial Basis Function Neural Network, and
Cascade Correlation Neural Network are compared with each other based on: (1)
predictive accuracy centered on the Mean Absolute Error criterion, (2) whether
such a model tends to overestimate or underestimate, and (3) how each model
classifies the importance of its inputs. Industrial datasets from the
International Software Benchmarking Standards Group (ISBSG) are used to train
and validate the four models. The main ISBSG dataset was filtered and then
divided into five datasets based on the productivity value of each project.
Results show that the four models tend to overestimate in 80percent of the
datasets, and the significance of the model inputs varies based on the selected
model. Furthermore, the Cascade Correlation Neural Network outperforms the
other three models in the majority of the datasets constructed on the Mean
Absolute Residual criterion.
</summary>
    <author>
      <name>Ali Bou Nassif</name>
    </author>
    <author>
      <name>Mohammad Azzeh</name>
    </author>
    <author>
      <name>Luiz Fernando Capretz</name>
    </author>
    <author>
      <name>Danny Ho</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00521-015-2127-1,</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00521-015-2127-1," rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Computing &amp; Applications, Volume 27, Issue 8, pp.
  2369-2381, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1611.09934v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09934v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.00580v1</id>
    <updated>2017-08-02T02:35:20Z</updated>
    <published>2017-08-02T02:35:20Z</published>
    <title>A Novel Neural Network Model Specified for Representing Logical
  Relations</title>
    <summary>  With computers to handle more and more complicated things in variable
environments, it becomes an urgent requirement that the artificial intelligence
has the ability of automatic judging and deciding according to numerous
specific conditions so as to deal with the complicated and variable cases. ANNs
inspired by brain is a good candidate. However, most of current numeric ANNs
are not good at representing logical relations because these models still try
to represent logical relations in the form of ratio based on functional
approximation. On the other hand, researchers have been trying to design novel
neural network models to make neural network model represent logical relations.
In this work, a novel neural network model specified for representing logical
relations is proposed and applied. New neurons and multiple kinds of links are
defined. Inhibitory links are introduced besides exciting links. Different from
current numeric ANNs, one end of an inhibitory link connects an exciting link
rather than a neuron. Inhibitory links inhibit the connected exciting links
conditionally to make this neural network model represent logical relations
correctly. This model can simulate the operations of Boolean logic gates, and
construct complex logical relations with the advantages of simpler neural
network structures than recent works in this area. This work provides some
ideas to make neural networks represent logical relations more directly and
efficiently, and the model could be used as the complement to current numeric
ANN to deal with logical issues and expand the application areas of ANN.
</summary>
    <author>
      <name>Gang Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1708.00580v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00580v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.07128v3</id>
    <updated>2018-02-14T19:24:55Z</updated>
    <published>2017-11-20T03:19:03Z</published>
    <title>Hello Edge: Keyword Spotting on Microcontrollers</title>
    <summary>  Keyword spotting (KWS) is a critical component for enabling speech based user
interactions on smart devices. It requires real-time response and high accuracy
for good user experience. Recently, neural networks have become an attractive
choice for KWS architecture because of their superior accuracy compared to
traditional speech processing algorithms. Due to its always-on nature, KWS
application has highly constrained power budget and typically runs on tiny
microcontrollers with limited memory and compute capability. The design of
neural network architecture for KWS must consider these constraints. In this
work, we perform neural network architecture evaluation and exploration for
running KWS on resource-constrained microcontrollers. We train various neural
network architectures for keyword spotting published in literature to compare
their accuracy and memory/compute requirements. We show that it is possible to
optimize these neural network architectures to fit within the memory and
compute constraints of microcontrollers without sacrificing accuracy. We
further explore the depthwise separable convolutional neural network (DS-CNN)
and compare it against other neural network architectures. DS-CNN achieves an
accuracy of 95.4%, which is ~10% higher than the DNN model with similar number
of parameters.
</summary>
    <author>
      <name>Yundong Zhang</name>
    </author>
    <author>
      <name>Naveen Suda</name>
    </author>
    <author>
      <name>Liangzhen Lai</name>
    </author>
    <author>
      <name>Vikas Chandra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code available in github at
  https://github.com/ARM-software/ML-KWS-for-MCU</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.07128v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.07128v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9402096v1</id>
    <updated>1994-02-22T14:02:08Z</updated>
    <published>1994-02-22T14:02:08Z</published>
    <title>Response Functions Improving Performance in Analog Attractor Neural
  Networks</title>
    <summary>  In the context of attractor neural networks, we study how the equilibrium
analog neural activities, reached by the network dynamics during memory
retrieval, may improve storage performance by reducing the interferences
between the recalled pattern and the other stored ones. We determine a simple
dynamics that stabilizes network states which are highly correlated with the
retrieved pattern, for a number of stored memories that does not exceed
$\alpha_{\star} N$, where $\alpha_{\star}\in[0,0.41]$ depends on the global
activity level in the network and $N$ is the number of neurons.
</summary>
    <author>
      <name>Nicolas Brunel</name>
    </author>
    <author>
      <name>Riccardo Zecchina</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.49.R1823</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.49.R1823" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages (with figures), LaTex (RevTex), to appear on Phys.Rev.E (RC)</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9402096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9402096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9901251v1</id>
    <updated>1999-01-23T11:42:56Z</updated>
    <published>1999-01-23T11:42:56Z</published>
    <title>Fixed points of Hopfield type neural networks</title>
    <summary>  The set of the fixed points of the Hopfield type network is under
investigation. The connection matrix of the network is constructed according
the Hebb rule from the set of memorized patterns which are treated as distorted
copies of the standard-vector. It is found that the dependence of the set of
the fixed points on the value of the distortion parameter can be described
analytically. The obtained results are interpreted in the terms of neural
networks and the Ising model at T=0.
</summary>
    <author>
      <name>Leonid B. Litinskii</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Institute for High Pressure Physics Russian Academy of Sciences</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, Revtex, 1 Postscript-file</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9901251v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9901251v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9906197v1</id>
    <updated>1999-06-14T14:17:55Z</updated>
    <published>1999-06-14T14:17:55Z</published>
    <title>Fixed Points of Hopfield Type Neural Networks</title>
    <summary>  The set of the fixed points of the Hopfield type network is under
investigation. The connection matrix of the network is constructed according to
the Hebb rule from the set of memorized patterns which are treated as distorted
copies of the standard-vector. It is found that the dependence of the set of
the fixed points on the value of the distortion parameter can be described
analytically. The obtained results are interpreted in the terms of neural
networks and the Ising model.
</summary>
    <author>
      <name>Leonid B. Litinskii</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">High Pressure Physics Institute of Russian Academy of Sciences</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/BF02557200</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/BF02557200" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">RevTEX, 19 pages, 2 Postscript figures, the full version of the
  earler brief report (cond-mat/9901251)</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9906197v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9906197v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0203011v1</id>
    <updated>2002-03-01T08:47:31Z</updated>
    <published>2002-03-01T08:47:31Z</published>
    <title>Interacting neural networks and cryptography</title>
    <summary>  Two neural networks which are trained on their mutual output bits are
analysed using methods of statistical physics. The exact solution of the
dynamics of the two weight vectors shows a novel phenomenon: The networks
synchronize to a state with identical time dependent weights. Extending the
models to multilayer networks with discrete weights, it is shown how
synchronization by mutual learning can be applied to secret key exchange over a
public channel.
</summary>
    <author>
      <name>Wolfgang Kinzel</name>
    </author>
    <author>
      <name>Ido Kanter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Invited talk for the meeting of the German Physical Society</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0203011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0203011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0208281v2</id>
    <updated>2002-08-14T22:30:50Z</updated>
    <published>2002-08-14T14:57:08Z</published>
    <title>Time evolution of the extremely diluted Blume-Emery-Griffiths neural
  network</title>
    <summary>  The time evolution of the extremely diluted Blume-Emery-Griffiths neural
network model is studied, and a detailed equilibrium phase diagram is obtained
exhibiting pattern retrieval, fluctuation retrieval and self-sustained activity
phases. It is shown that saddle-point solutions associated with fluctuation
overlaps slow down considerably the flow of the network states towards the
retrieval fixed points. A comparison of the performance with other three-state
networks is also presented.
</summary>
    <author>
      <name>D. Bolle'</name>
    </author>
    <author>
      <name>D. R. C. Dominguez</name>
    </author>
    <author>
      <name>R. Erichsen Jr.</name>
    </author>
    <author>
      <name>E. Korutcheva</name>
    </author>
    <author>
      <name>W. K. Theumann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.68.062901</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.68.062901" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0208281v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0208281v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0209293v1</id>
    <updated>2002-09-12T14:59:46Z</updated>
    <published>2002-09-12T14:59:46Z</published>
    <title>Coupling parameter in synchronization of diluted neural networks</title>
    <summary>  We study the critical features of coupling parameter in the synchronization
of neural networks with diluted synapses. Based on simulations, the exponential
decay form is observed in the extreme case of global coupling among subsystems
and fully linking in each network: there exists maximum and minimum of the
critical coupling intensity for synchronization in this spatially extended
system. For the partial coupling, we present the primary result about the
critical coupling fraction for various linking degrees of networks.
</summary>
    <author>
      <name>Qi Li</name>
    </author>
    <author>
      <name>Yong Chen</name>
    </author>
    <author>
      <name>Ying Hai Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.65.041916</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.65.041916" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 8 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Phys. Rev. E65, 041916(2002)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0209293v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0209293v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0303425v1</id>
    <updated>2003-03-20T18:12:40Z</updated>
    <published>2003-03-20T18:12:40Z</published>
    <title>Mean-field dynamics of sequence processing neural networks with finite
  connectivity</title>
    <summary>  A recent dynamic mean-field theory for sequence processing in fully connected
neural networks of Hopfield-type (During, Coolen and Sherrington, 1998) is
extended and analized here for a symmetrically diluted network with finite
connectivity near saturation. Equations for the dynamics and the stationary
states are obtained for the macroscopic observables and the precise equivalence
is established with the single-pattern retrieval problem in a layered
feed-forward network with finite connectivity.
</summary>
    <author>
      <name>W. K. Theumann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/S0378-4371(03)00569-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/S0378-4371(03)00569-7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages Latex</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physica A 328, 1-12 (2003)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0303425v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0303425v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0304021v1</id>
    <updated>2003-04-01T19:14:26Z</updated>
    <published>2003-04-01T19:14:26Z</published>
    <title>Topology and Computational Performance of Attractor Neural Networks</title>
    <summary>  To explore the relation between network structure and function, we studied
the computational performance of Hopfield-type attractor neural nets with
regular lattice, random, small-world and scale-free topologies. The random net
is the most efficient for storage and retrieval of patterns by the entire
network. However, in the scale-free case retrieval errors are not distributed
uniformly: the portion of a pattern encoded by the subset of highly connected
nodes is more robust and efficiently recognized than the rest of the pattern.
The scale-free network thus achieves a very strong partial recognition.
Implications for brain function and social dynamics are suggestive.
</summary>
    <author>
      <name>Patrick N. Mcgraw</name>
    </author>
    <author>
      <name>Michael Menzinger</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.68.047102</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.68.047102" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 figures included. Submitted to Phys. Rev. Letters</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 68, 047102 (2003)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0304021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0304021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0503626v2</id>
    <updated>2005-06-29T21:47:34Z</updated>
    <published>2005-03-27T18:32:27Z</published>
    <title>Conditions for the emergence of spatial asymmetric states in attractor
  neural network</title>
    <summary>  In this paper we show that during the retrieval process in a binary symmetric
Hebb neural network, spatial localized states can be observed when the
connectivity of the network is distance-dependent and when a constraint on the
activity of the network is imposed, which forces different levels of activity
in the retrieval and learning states. This asymmetry in the activity during the
retrieval and learning is found to be sufficient condition in order to observe
spatial localized states. The result is confirmed analytically and by
simulation.
</summary>
    <author>
      <name>Kostadin Koroutchev</name>
    </author>
    <author>
      <name>Elka Korutcheva</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2478/BF02475647</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2478/BF02475647" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 p</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CEJP 3(3) 2005 409-419</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0503626v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0503626v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0404042v2</id>
    <updated>2004-04-22T09:00:52Z</updated>
    <published>2004-04-21T16:05:27Z</published>
    <title>Extraction of topological features from communication network
  topological patterns using self-organizing feature maps</title>
    <summary>  Different classes of communication network topologies and their
representation in the form of adjacency matrix and its eigenvalues are
presented. A self-organizing feature map neural network is used to map
different classes of communication network topological patterns. The neural
network simulation results are reported.
</summary>
    <author>
      <name>W. Ali</name>
    </author>
    <author>
      <name>R. J. Mondragon</name>
    </author>
    <author>
      <name>F. Alavi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 Pages, 5 figures, To be appeared in IEE Electronics Letter Journal</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0404042v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0404042v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2; I.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/nlin/0601005v2</id>
    <updated>2007-03-01T08:28:51Z</updated>
    <published>2006-01-04T07:32:35Z</published>
    <title>Reinforcement learning of recurrent neural network for temporal coding</title>
    <summary>  We study a reinforcement learning for temporal coding with neural network
consisting of stochastic spiking neurons. In neural networks, information can
be coded by characteristics of the timing of each neuronal firing, including
the order of firing or the relative phase differences of firing. We derive the
learning rule for this network and show that the network consisting of
Hodgkin-Huxley neurons with the dynamical synaptic kinetics can learn the
appropriate timing of each neuronal firing. We also investigate the system size
dependence of learning efficiency.
</summary>
    <author>
      <name>Daichi Kimura</name>
    </author>
    <author>
      <name>Yoshinori Hayakawa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/nlin/0601005v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/nlin/0601005v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1102.4240v1</id>
    <updated>2011-02-21T14:48:20Z</updated>
    <published>2011-02-21T14:48:20Z</published>
    <title>Sparse neural networks with large learning diversity</title>
    <summary>  Coded recurrent neural networks with three levels of sparsity are introduced.
The first level is related to the size of messages, much smaller than the
number of available neurons. The second one is provided by a particular coding
rule, acting as a local constraint in the neural activity. The third one is a
characteristic of the low final connection density of the network after the
learning phase. Though the proposed network is very simple since it is based on
binary neurons and binary connections, it is able to learn a large number of
messages and recall them, even in presence of strong erasures. The performance
of the network is assessed as a classifier and as an associative memory.
</summary>
    <author>
      <name>Vincent Gripon</name>
    </author>
    <author>
      <name>Claude Berrou</name>
    </author>
    <link href="http://arxiv.org/abs/1102.4240v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.4240v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.3474v1</id>
    <updated>2014-06-13T10:11:18Z</updated>
    <published>2014-06-13T10:11:18Z</published>
    <title>Heterogeneous Multi-task Learning for Human Pose Estimation with Deep
  Convolutional Neural Network</title>
    <summary>  We propose an heterogeneous multi-task learning framework for human pose
estimation from monocular image with deep convolutional neural network. In
particular, we simultaneously learn a pose-joint regressor and a sliding-window
body-part detector in a deep network architecture. We show that including the
body-part detection task helps to regularize the network, directing it to
converge to a good solution. We report competitive and state-of-art results on
several data sets. We also empirically show that the learned neurons in the
middle layer of our network are tuned to localized body parts.
</summary>
    <author>
      <name>Sijin Li</name>
    </author>
    <author>
      <name>Zhi-Qiang Liu</name>
    </author>
    <author>
      <name>Antoni B. Chan</name>
    </author>
    <link href="http://arxiv.org/abs/1406.3474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.3474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.5917v1</id>
    <updated>2013-02-24T15:53:19Z</updated>
    <published>2013-02-24T15:53:19Z</published>
    <title>Modeling of interstitial branching of axonal networks</title>
    <summary>  A single axon can generate branches connecting with plenty synaptic targets.
Process of branching is very important for making connections in central
nervous system. The interstitial branching along primary axon shaft occurs
during nervous system development. Growing axon makes pause in its movement and
leaves active points behind its terminal. The new branches appear from these
points. We suggest mathematical model to describe and investigate neural
network branching process. The model under consideration describes neural
network growth in which the concentration of axon guidance molecules manages
axon's growth. We model the interstitial branching from axon shaft. Numerical
simulations show that in the model framework axonal networks are similar to
neural network.
</summary>
    <author>
      <name>Y. Suleymanov</name>
    </author>
    <author>
      <name>F. Gafarov</name>
    </author>
    <author>
      <name>N. Khusnutdinov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1142/S0219635213500064</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1142/S0219635213500064" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">1. J. Integr. Neurosci. 12, 1-14, (2013)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1302.5917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.5917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06829v1</id>
    <updated>2016-03-22T15:26:26Z</updated>
    <published>2016-03-22T15:26:26Z</published>
    <title>Multi-velocity neural networks for gesture recognition in videos</title>
    <summary>  We present a new action recognition deep neural network which adaptively
learns the best action velocities in addition to the classification. While deep
neural networks have reached maturity for image understanding tasks, we are
still exploring network topologies and features to handle the richer
environment of video clips. Here, we tackle the problem of multiple velocities
in action recognition, and provide state-of-the-art results for gesture
recognition, on known and new collected datasets. We further provide the
training steps for our semi-supervised network, suited to learn from huge
unlabeled datasets with only a fraction of labeled examples.
</summary>
    <author>
      <name>Otkrist Gupta</name>
    </author>
    <author>
      <name>Dan Raviv</name>
    </author>
    <author>
      <name>Ramesh Raskar</name>
    </author>
    <link href="http://arxiv.org/abs/1603.06829v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06829v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.01097v3</id>
    <updated>2017-02-28T02:58:11Z</updated>
    <published>2016-07-05T02:51:33Z</published>
    <title>AdaNet: Adaptive Structural Learning of Artificial Neural Networks</title>
    <summary>  We present new algorithms for adaptively learning artificial neural networks.
Our algorithms (AdaNet) adaptively learn both the structure of the network and
its weights. They are based on a solid theoretical analysis, including
data-dependent generalization guarantees that we prove and discuss in detail.
We report the results of large-scale experiments with one of our algorithms on
several binary classification tasks extracted from the CIFAR-10 dataset. The
results demonstrate that our algorithm can automatically learn network
structures with very competitive performance accuracies when compared with
those achieved for neural networks found by standard approaches.
</summary>
    <author>
      <name>Corinna Cortes</name>
    </author>
    <author>
      <name>Xavi Gonzalvo</name>
    </author>
    <author>
      <name>Vitaly Kuznetsov</name>
    </author>
    <author>
      <name>Mehryar Mohri</name>
    </author>
    <author>
      <name>Scott Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1607.01097v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.01097v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.04153v1</id>
    <updated>2017-05-11T13:11:23Z</updated>
    <published>2017-05-11T13:11:23Z</published>
    <title>Dynamic Compositional Neural Networks over Tree Structure</title>
    <summary>  Tree-structured neural networks have proven to be effective in learning
semantic representations by exploiting syntactic information. In spite of their
success, most existing models suffer from the underfitting problem: they
recursively use the same shared compositional function throughout the whole
compositional process and lack expressive power due to inability to capture the
richness of compositionality. In this paper, we address this issue by
introducing the dynamic compositional neural networks over tree structure
(DC-TreeNN), in which the compositional function is dynamically generated by a
meta network. The role of meta-network is to capture the metaknowledge across
the different compositional rules and formulate them. Experimental results on
two typical tasks show the effectiveness of the proposed models.
</summary>
    <author>
      <name>Pengfei Liu</name>
    </author>
    <author>
      <name>Xipeng Qiu</name>
    </author>
    <author>
      <name>Xuanjing Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by IJCAI 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.04153v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04153v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.03301v1</id>
    <updated>2017-06-11T03:07:42Z</updated>
    <published>2017-06-11T03:07:42Z</published>
    <title>Neural networks and rational functions</title>
    <summary>  Neural networks and rational functions efficiently approximate each other. In
more detail, it is shown here that for any ReLU network, there exists a
rational function of degree $O(\text{polylog}(1/\epsilon))$ which is
$\epsilon$-close, and similarly for any rational function there exists a ReLU
network of size $O(\text{polylog}(1/\epsilon))$ which is $\epsilon$-close. By
contrast, polynomials need degree $\Omega(\text{poly}(1/\epsilon))$ to
approximate even a single ReLU. When converting a ReLU network to a rational
function as above, the hidden constants depend exponentially on the number of
layers, which is shown to be tight; in other words, a compositional
representation can be beneficial even for rational functions.
</summary>
    <author>
      <name>Matus Telgarsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear, ICML 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.03301v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.03301v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.05853v2</id>
    <updated>2017-08-09T09:58:43Z</updated>
    <published>2017-07-18T20:47:06Z</published>
    <title>Encoding Word Confusion Networks with Recurrent Neural Networks for
  Dialog State Tracking</title>
    <summary>  This paper presents our novel method to encode word confusion networks, which
can represent a rich hypothesis space of automatic speech recognition systems,
via recurrent neural networks. We demonstrate the utility of our approach for
the task of dialog state tracking in spoken dialog systems that relies on
automatic speech recognition output. Encoding confusion networks outperforms
encoding the best hypothesis of the automatic speech recognition in a neural
system for dialog state tracking on the well-known second Dialog State Tracking
Challenge dataset.
</summary>
    <author>
      <name>Glorianna Jagfeld</name>
    </author>
    <author>
      <name>Ngoc Thang Vu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Speech-Centric Natural Language Processing Workshop @EMNLP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.05853v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.05853v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.01255v3</id>
    <updated>2017-10-26T15:36:53Z</updated>
    <published>2017-09-30T11:50:02Z</published>
    <title>Variational Grid Setting Network</title>
    <summary>  We propose a new neural network architecture for automatic generation of
missing characters in a Chinese font set. We call the neural network
architecture the Variational Grid Setting Network which is based on the
variational autoencoder (VAE) with some tweaks. The neural network model is
able to generate missing characters relatively large in size ($256 \times 256$
pixels). Moreover, we show that one can use very few samples for training data
set, and get a satisfied result.
</summary>
    <author>
      <name>Yu-Neng Chuang</name>
    </author>
    <author>
      <name>Zi-Yu Huang</name>
    </author>
    <author>
      <name>Yen-Lung Tsai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.01255v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.01255v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.04323v1</id>
    <updated>2017-12-12T14:50:51Z</updated>
    <published>2017-12-12T14:50:51Z</published>
    <title>Deep Echo State Network (DeepESN): A Brief Survey</title>
    <summary>  The study of deep recurrent neural networks (RNNs) and, in particular, of
deep Reservoir Computing (RC) is gaining an increasing research attention in
the neural networks community. The recently introduced deep Echo State Network
(deepESN) model opened the way to an extremely efficient approach for designing
deep neural networks for temporal data. At the same time, the study of deepESNs
allowed to shed light on the intrinsic properties of state dynamics developed
by hierarchical compositions of recurrent layers, i.e. on the bias of depth in
RNNs architectural design. In this paper, we summarize the advancements in the
development, analysis and applications of deepESNs.
</summary>
    <author>
      <name>Claudio Gallicchio</name>
    </author>
    <author>
      <name>Alessio Micheli</name>
    </author>
    <link href="http://arxiv.org/abs/1712.04323v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.04323v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.05245v1</id>
    <updated>2017-12-14T14:25:52Z</updated>
    <published>2017-12-14T14:25:52Z</published>
    <title>Point-wise Convolutional Neural Network</title>
    <summary>  Deep learning with 3D data such as reconstructed point clouds and CAD models
has received great research interests recently. However, the capability of
using point clouds with convolutional neural network has been so far not fully
explored. In this technical report, we present a convolutional neural network
for semantic segmentation and object recognition with 3D point clouds. At the
core of our network is point-wise convolution, a convolution operator that can
be applied at each point of a point cloud. Our fully convolutional network
design, while being simple to implement, can yield competitive accuracy in both
semantic segmentation and object recognition task.
</summary>
    <author>
      <name>Binh-Son Hua</name>
    </author>
    <author>
      <name>Minh-Khoi Tran</name>
    </author>
    <author>
      <name>Sai-Kit Yeung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures, 10 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.05245v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.05245v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.09097v1</id>
    <updated>2018-01-27T14:41:59Z</updated>
    <published>2018-01-27T14:41:59Z</published>
    <title>Towards an Understanding of Neural Networks in Natural-Image Spaces</title>
    <summary>  Two major uncertainties, dataset bias and perturbation, prevail in
state-of-the-art AI algorithms with deep neural networks. In this paper, we
present an intuitive explanation for these issues as well as an interpretation
of the performance of deep networks in a natural-image space. The explanation
consists of two parts: the philosophy of neural networks and a hypothetic model
of natural-image spaces. Following the explanation, we slightly improve the
accuracy of a CIFAR-10 classifier by introducing an additional "random-noise"
category during training. We hope this paper will stimulate discussion in the
community regarding the topological and geometric properties of natural-image
spaces to which deep networks are applied.
</summary>
    <author>
      <name>Yifei Fan</name>
    </author>
    <author>
      <name>Anthony Yezzi</name>
    </author>
    <link href="http://arxiv.org/abs/1801.09097v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.09097v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00541v1</id>
    <updated>2018-02-02T02:24:24Z</updated>
    <published>2018-02-02T02:24:24Z</published>
    <title>Causal Learning and Explanation of Deep Neural Networks via Autoencoded
  Activations</title>
    <summary>  Deep neural networks are complex and opaque. As they enter application in a
variety of important and safety critical domains, users seek methods to explain
their output predictions. We develop an approach to explaining deep neural
networks by constructing causal models on salient concepts contained in a CNN.
We develop methods to extract salient concepts throughout a target network by
using autoencoders trained to extract human-understandable representations of
network activations. We then build a bayesian causal model using these
extracted concepts as variables in order to explain image classification.
Finally, we use this causal model to identify and visualize features with
significant causal influence on final classification.
</summary>
    <author>
      <name>Michael Harradon</name>
    </author>
    <author>
      <name>Jeff Druce</name>
    </author>
    <author>
      <name>Brian Ruttenberg</name>
    </author>
    <link href="http://arxiv.org/abs/1802.00541v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00541v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7118v1</id>
    <updated>2013-04-26T10:42:23Z</updated>
    <published>2013-04-26T10:42:23Z</published>
    <title>Synthesis of neural networks for spatio-temporal spike pattern
  recognition and processing</title>
    <summary>  The advent of large scale neural computational platforms has highlighted the
lack of algorithms for synthesis of neural structures to perform predefined
cognitive tasks. The Neural Engineering Framework offers one such synthesis,
but it is most effective for a spike rate representation of neural information,
and it requires a large number of neurons to implement simple functions. We
describe a neural network synthesis method that generates synaptic connectivity
for neurons which process time-encoded neural signals, and which makes very
sparse use of neurons. The method allows the user to specify, arbitrarily,
neuronal characteristics such as axonal and dendritic delays, and synaptic
transfer functions, and then solves for the optimal input-output relationship
using computed dendritic weights. The method may be used for batch or online
learning and has an extremely fast optimization process. We demonstrate its use
in generating a network to recognize speech which is sparsely encoded as spike
times.
</summary>
    <author>
      <name>J. Tapson</name>
    </author>
    <author>
      <name>G. Cohen</name>
    </author>
    <author>
      <name>S. Afshar</name>
    </author>
    <author>
      <name>K. Stiefel</name>
    </author>
    <author>
      <name>Y. Buskila</name>
    </author>
    <author>
      <name>R. Wang</name>
    </author>
    <author>
      <name>T. J. Hamilton</name>
    </author>
    <author>
      <name>A. van Schaik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In submission to Frontiers in Neuromorphic Engineering</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.7118v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7118v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.00531v2</id>
    <updated>2017-08-15T16:29:05Z</updated>
    <published>2017-08-01T21:53:56Z</published>
    <title>End-to-End Neural Segmental Models for Speech Recognition</title>
    <summary>  Segmental models are an alternative to frame-based models for sequence
prediction, where hypothesized path weights are based on entire segment scores
rather than a single frame at a time. Neural segmental models are segmental
models that use neural network-based weight functions. Neural segmental models
have achieved competitive results for speech recognition, and their end-to-end
training has been explored in several studies. In this work, we review neural
segmental models, which can be viewed as consisting of a neural network-based
acoustic encoder and a finite-state transducer decoder. We study end-to-end
segmental models with different weight functions, including ones based on
frame-level neural classifiers and on segmental recurrent neural networks. We
study how reducing the search space size impacts performance under different
weight functions. We also compare several loss functions for end-to-end
training. Finally, we explore training approaches, including multi-stage vs.
end-to-end training and multitask training that combines segmental and
frame-level losses.
</summary>
    <author>
      <name>Hao Tang</name>
    </author>
    <author>
      <name>Liang Lu</name>
    </author>
    <author>
      <name>Lingpeng Kong</name>
    </author>
    <author>
      <name>Kevin Gimpel</name>
    </author>
    <author>
      <name>Karen Livescu</name>
    </author>
    <author>
      <name>Chris Dyer</name>
    </author>
    <author>
      <name>Noah A. Smith</name>
    </author>
    <author>
      <name>Steve Renals</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/JSTSP.2017.2752462</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/JSTSP.2017.2752462" rel="related"/>
    <link href="http://arxiv.org/abs/1708.00531v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00531v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.00942v1</id>
    <updated>2017-10-03T00:01:55Z</updated>
    <published>2017-10-03T00:01:55Z</published>
    <title>Neural Trojans</title>
    <summary>  While neural networks demonstrate stronger capabilities in pattern
recognition nowadays, they are also becoming larger and deeper. As a result,
the effort needed to train a network also increases dramatically. In many
cases, it is more practical to use a neural network intellectual property (IP)
that an IP vendor has already trained. As we do not know about the training
process, there can be security threats in the neural IP: the IP vendor
(attacker) may embed hidden malicious functionality, i.e. neural Trojans, into
the neural IP. We show that this is an effective attack and provide three
mitigation techniques: input anomaly detection, re-training, and input
preprocessing. All the techniques are proven effective. The input anomaly
detection approach is able to detect 99.8% of Trojan triggers although with
12.2% false positive. The re-training approach is able to prevent 94.1% of
Trojan triggers from triggering the Trojan although it requires that the neural
IP be reconfigurable. In the input preprocessing approach, 90.2% of Trojan
triggers are rendered ineffective and no assumption about the neural IP is
needed.
</summary>
    <author>
      <name>Yuntao Liu</name>
    </author>
    <author>
      <name>Yang Xie</name>
    </author>
    <author>
      <name>Ankur Srivastava</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The shorth-length version of this paper is to appear in the 2017 IEEE
  International Conference on Computer Design (ICCD)</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.00942v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.00942v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0312068v1</id>
    <updated>2003-12-02T13:36:06Z</updated>
    <published>2003-12-02T13:36:06Z</published>
    <title>Cooperating Attackers in Neural Cryptography</title>
    <summary>  A new and successful attack strategy in neural cryptography is presented. The
neural cryptosystem, based on synchronization of neural networks by mutual
learning, has been recently shown to be secure under different attack
strategies. The advanced attacker presented here, named the ``Majority-Flipping
Attacker'', is the first whose success does not decay with the parameters of
the model. This new attacker's outstanding success is due to its using a group
of attackers which cooperate throughout the synchronization process, unlike any
other attack strategy known. An analytical description of this attack is also
presented, and fits the results of simulations.
</summary>
    <author>
      <name>L. N. Shacham</name>
    </author>
    <author>
      <name>E. Klein</name>
    </author>
    <author>
      <name>R. Mislovaty</name>
    </author>
    <author>
      <name>I. Kanter</name>
    </author>
    <author>
      <name>W. Kinzel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.69.066137</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.69.066137" rel="related"/>
    <link href="http://arxiv.org/abs/cond-mat/0312068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0312068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/q-bio/0404012v1</id>
    <updated>2004-04-08T20:06:45Z</updated>
    <published>2004-04-08T20:06:45Z</published>
    <title>Mathematical Analysis and Simulations of the Neural Circuit for
  Locomotion in Lamprey</title>
    <summary>  We analyze the dynamics of the neural circuit of the lamprey central pattern
generator (CPG). This analysis provides insights into how neural interactions
form oscillators and enable spontaneous oscillations in a network of damped
oscillators, which were not apparent in previous simulations or abstract phase
oscillator models. We also show how the different behaviour regimes
(characterized by phase and amplitude relationships between oscillators) of
forward/backward swimming, and turning, can be controlled using the neural
connection strengths and external inputs.
</summary>
    <author>
      <name>Li Zhaoping</name>
    </author>
    <author>
      <name>Alex Lewis</name>
    </author>
    <author>
      <name>Silvia Scarpetta</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.92.198106</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.92.198106" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, accepted for publication in Physical Review Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/q-bio/0404012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/q-bio/0404012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.3451v1</id>
    <updated>2009-03-20T05:17:39Z</updated>
    <published>2009-03-20T05:17:39Z</published>
    <title>Neural network model with discrete and continuous information
  representation</title>
    <summary>  An associative memory model and a neural network model with a Mexican-hat
type interaction are the two most typical attractor networks used in the
artificial neural network models. The associative memory model has discretely
distributed fixed-point attractors, and achieves a discrete information
representation. On the other hand, a neural network model with a Mexican-hat
type interaction uses a line attractor to achieves a continuous information
representation, which can be seen in the working memory in the prefrontal
cortex and columnar activity in the visual cortex. In the present study, we
propose a neural network model that achieves discrete and continuous
information representation. We use a statistical-mechanical analysis to find
that a localized retrieval phase exists in the proposed model, where the memory
pattern is retrieved in the localized subpopulation of the network. In the
localized retrieval phase, the discrete and continuous information
representation is achieved by using the orthogonality of the memory patterns
and the neutral stability of fixed points along the positions of the localized
retrieval. The obtained phase diagram suggests that the antiferromagnetic
interaction and the external field are important for generating the localized
retrieval phase.
</summary>
    <author>
      <name>Jun Kitazono</name>
    </author>
    <author>
      <name>Toshiaki Omori</name>
    </author>
    <author>
      <name>Masato Okada</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1143/JPSJ.78.114801</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1143/JPSJ.78.114801" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0903.3451v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.3451v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.1445v1</id>
    <updated>2014-05-06T20:18:49Z</updated>
    <published>2014-05-06T20:18:49Z</published>
    <title>Pulling back error to the hidden-node parameter technology:
  Single-hidden-layer feedforward network without output weight</title>
    <summary>  According to conventional neural network theories, the feature of
single-hidden-layer feedforward neural networks(SLFNs) resorts to parameters of
the weighted connections and hidden nodes. SLFNs are universal approximators
when at least the parameters of the networks including hidden-node parameter
and output weight are exist. Unlike above neural network theories, this paper
indicates that in order to let SLFNs work as universal approximators, one may
simply calculate the hidden node parameter only and the output weight is not
needed at all. In other words, this proposed neural network architecture can be
considered as a standard SLFNs with fixing output weight equal to an unit
vector. Further more, this paper presents experiments which show that the
proposed learning method tends to extremely reduce network output error to a
very small number with only 1 hidden node. Simulation results demonstrate that
the proposed method can provide several to thousands of times faster than other
learning algorithm including BP, SVM/SVR and other ELM methods.
</summary>
    <author>
      <name>Yimin Yang</name>
    </author>
    <author>
      <name>Q. M. Jonathan Wu</name>
    </author>
    <author>
      <name>Guangbin Huang</name>
    </author>
    <author>
      <name>Yaonan Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.1445v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.1445v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.7828v2</id>
    <updated>2015-01-04T19:44:17Z</updated>
    <published>2014-12-25T14:27:42Z</published>
    <title>Protein Secondary Structure Prediction with Long Short Term Memory
  Networks</title>
    <summary>  Prediction of protein secondary structure from the amino acid sequence is a
classical bioinformatics problem. Common methods use feed forward neural
networks or SVMs combined with a sliding window, as these models does not
naturally handle sequential data. Recurrent neural networks are an
generalization of the feed forward neural network that naturally handle
sequential data. We use a bidirectional recurrent neural network with long
short term memory cells for prediction of secondary structure and evaluate
using the CB513 dataset. On the secondary structure 8-class problem we report
better performance (0.674) than state of the art (0.664). Our model includes
feed forward networks between the long short term memory cells, a path that can
be further explored.
</summary>
    <author>
      <name>S√∏ren Kaae S√∏nderby</name>
    </author>
    <author>
      <name>Ole Winther</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">v2: adds larger network with slightly better results, update author
  affiliations</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.7828v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.7828v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.05336v2</id>
    <updated>2015-07-15T10:14:49Z</updated>
    <published>2015-02-18T18:45:17Z</published>
    <title>Probabilistic Backpropagation for Scalable Learning of Bayesian Neural
  Networks</title>
    <summary>  Large multilayer neural networks trained with backpropagation have recently
achieved state-of-the-art results in a wide range of problems. However, using
backprop for neural net learning still has some disadvantages, e.g., having to
tune a large number of hyperparameters to the data, lack of calibrated
probabilistic predictions, and a tendency to overfit the training data. In
principle, the Bayesian approach to learning neural networks does not have
these problems. However, existing Bayesian techniques lack scalability to large
dataset and network sizes. In this work we present a novel scalable method for
learning Bayesian neural networks, called probabilistic backpropagation (PBP).
Similar to classical backpropagation, PBP works by computing a forward
propagation of probabilities through the network and then doing a backward
computation of gradients. A series of experiments on ten real-world datasets
show that PBP is significantly faster than other techniques, while offering
competitive predictive abilities. Our experiments also show that PBP provides
accurate estimates of the posterior variance on the network weights.
</summary>
    <author>
      <name>Jos√© Miguel Hern√°ndez-Lobato</name>
    </author>
    <author>
      <name>Ryan P. Adams</name>
    </author>
    <link href="http://arxiv.org/abs/1502.05336v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.05336v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.00698v1</id>
    <updated>2015-06-01T22:52:36Z</updated>
    <published>2015-06-01T22:52:36Z</published>
    <title>Statistical Machine Translation Features with Multitask Tensor Networks</title>
    <summary>  We present a three-pronged approach to improving Statistical Machine
Translation (SMT), building on recent success in the application of neural
networks to SMT. First, we propose new features based on neural networks to
model various non-local translation phenomena. Second, we augment the
architecture of the neural network with tensor layers that capture important
higher-order interaction among the network units. Third, we apply multitask
learning to estimate the neural network parameters jointly. Each of our
proposed methods results in significant improvements that are complementary.
The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English and
Chinese-English translation over a state-of-the-art system that already
includes neural network features.
</summary>
    <author>
      <name>Hendra Setiawan</name>
    </author>
    <author>
      <name>Zhongqiang Huang</name>
    </author>
    <author>
      <name>Jacob Devlin</name>
    </author>
    <author>
      <name>Thomas Lamar</name>
    </author>
    <author>
      <name>Rabih Zbib</name>
    </author>
    <author>
      <name>Richard Schwartz</name>
    </author>
    <author>
      <name>John Makhoul</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages (9 content + 2 references), 2 figures, accepted to ACL 2015
  as a long paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.00698v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.00698v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.01893v1</id>
    <updated>2015-09-07T03:43:47Z</updated>
    <published>2015-09-07T03:43:47Z</published>
    <title>Spectrum density of large sparse random matrices associated to neural
  networks</title>
    <summary>  The eigendecomposition of the coupling matrix of large biological networks is
central to the study of the dynamics of these networks. For neural networks,
this matrix should reflect the topology of the network and conform with Dale's
law which states that a neuron can have only all excitatory or only all
inhibitory output connections, i.e., coefficients of one column of the coupling
matrix must all have the same sign. The eigenspectrum density has been
determined before for dense matrices $J_{ij}$, when several populations are
considered. However, the expressions were derived under the assumption of dense
connectivity, whereas neural circuits have sparse connections. Here, we
followed mean-field approaches in order to come up with exact self-consistent
expressions for the spectrum density in the limit of sparse matrices for both
symmetric and neural network matrices. Furthermore we introduced approximations
that allow for good numerical evaluation of the density. Finally, we studied
the phenomenology of localization properties of the eigenvectors.
</summary>
    <author>
      <name>Herv√© Rouault</name>
    </author>
    <author>
      <name>Shaul Druckmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures and one supplementary information (8 pages, 1
  figure)</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.01893v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.01893v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.05222v5</id>
    <updated>2016-07-13T06:27:41Z</updated>
    <published>2015-11-16T23:38:55Z</published>
    <title>Local Dynamics in Trained Recurrent Neural Networks</title>
    <summary>  Learning a task induces connectivity changes in neural circuits, thereby
changing their dynamics. To elucidate task related neural dynamics we study
trained Recurrent Neural Networks. We develop a Mean Field Theory for Reservoir
Computing networks trained to have multiple fixed point attractors. Our main
result is that the dynamics of the network's output in the vicinity of
attractors is governed by a low order linear Ordinary Differential Equation.
Stability of the resulting ODE can be assessed, predicting training success or
failure. As a consequence, networks of Rectified Linear (RLU) and of sigmoidal
nonlinearities are shown to have diametrically different properties when it
comes to learning attractors. Furthermore, a characteristic time constant,
which remains finite at the edge of chaos, offers an explanation of the
network's output robustness in the presence of variability of the internal
neural dynamics. Finally, the proposed theory predicts state dependent
frequency selectivity in network response.
</summary>
    <author>
      <name>Alexander Rivkind</name>
    </author>
    <author>
      <name>Omri Barak</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.118.258101</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.118.258101" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. Lett. 118, 258101 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1511.05222v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.05222v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.05702v2</id>
    <updated>2016-06-07T20:18:03Z</updated>
    <published>2015-12-17T18:08:33Z</published>
    <title>Synthesis of recurrent neural networks for dynamical system simulation</title>
    <summary>  We review several of the most widely used techniques for training recurrent
neural networks to approximate dynamical systems, then describe a novel
algorithm for this task. The algorithm is based on an earlier theoretical
result that guarantees the quality of the network approximation. We show that a
feedforward neural network can be trained on the vector field representation of
a given dynamical system using backpropagation, then recast, using matrix
manipulations, as a recurrent network that replicates the original system's
dynamics. After detailing this algorithm and its relation to earlier
approaches, we present numerical examples that demonstrate its capabilities.
One of the distinguishing features of our approach is that both the original
dynamical systems and the recurrent networks that simulate them operate in
continuous time.
</summary>
    <author>
      <name>Adam Trischler</name>
    </author>
    <author>
      <name>Gabriele MT D'Eleuterio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in Neural Networks</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.05702v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.05702v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05148v2</id>
    <updated>2017-07-07T16:26:16Z</updated>
    <published>2016-08-18T01:05:09Z</published>
    <title>Full Resolution Image Compression with Recurrent Neural Networks</title>
    <summary>  This paper presents a set of full-resolution lossy image compression methods
based on neural networks. Each of the architectures we describe can provide
variable compression rates during deployment without requiring retraining of
the network: each network need only be trained once. All of our architectures
consist of a recurrent neural network (RNN)-based encoder and decoder, a
binarizer, and a neural network for entropy coding. We compare RNN types (LSTM,
associative LSTM) and introduce a new hybrid of GRU and ResNet. We also study
"one-shot" versus additive reconstruction architectures and introduce a new
scaled-additive framework. We compare to previous work, showing improvements of
4.3%-8.8% AUC (area under the rate-distortion curve), depending on the
perceptual metric used. As far as we know, this is the first neural network
architecture that is able to outperform JPEG at image compression across most
bitrates on the rate-distortion curve on the Kodak dataset images, with and
without the aid of entropy coding.
</summary>
    <author>
      <name>George Toderici</name>
    </author>
    <author>
      <name>Damien Vincent</name>
    </author>
    <author>
      <name>Nick Johnston</name>
    </author>
    <author>
      <name>Sung Jin Hwang</name>
    </author>
    <author>
      <name>David Minnen</name>
    </author>
    <author>
      <name>Joel Shor</name>
    </author>
    <author>
      <name>Michele Covell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Updated with content for CVPR and removed supplemental material to an
  external link for size limitations</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.05148v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05148v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00222v2</id>
    <updated>2017-02-26T09:44:34Z</updated>
    <published>2016-09-01T13:08:47Z</published>
    <title>Ternary Neural Networks for Resource-Efficient AI Applications</title>
    <summary>  The computation and storage requirements for Deep Neural Networks (DNNs) are
usually high. This issue limits their deployability on ubiquitous computing
devices such as smart phones, wearables and autonomous drones. In this paper,
we propose ternary neural networks (TNNs) in order to make deep learning more
resource-efficient. We train these TNNs using a teacher-student approach based
on a novel, layer-wise greedy methodology. Thanks to our two-stage training
procedure, the teacher network is still able to use state-of-the-art methods
such as dropout and batch normalization to increase accuracy and reduce
training time. Using only ternary weights and activations, the student ternary
network learns to mimic the behavior of its teacher network without using any
multiplication. Unlike its -1,1 binary counterparts, a ternary neural network
inherently prunes the smaller weights by setting them to zero during training.
This makes them sparser and thus more energy-efficient. We design a
purpose-built hardware architecture for TNNs and implement it on FPGA and ASIC.
We evaluate TNNs on several benchmark datasets and demonstrate up to 3.1x
better energy efficiency with respect to the state of the art while also
improving accuracy.
</summary>
    <author>
      <name>Hande Alemdar</name>
    </author>
    <author>
      <name>Vincent Leroy</name>
    </author>
    <author>
      <name>Adrien Prost-Boucle</name>
    </author>
    <author>
      <name>Fr√©d√©ric P√©trot</name>
    </author>
    <link href="http://arxiv.org/abs/1609.00222v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00222v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.01360v2</id>
    <updated>2016-11-22T16:00:01Z</updated>
    <published>2016-09-06T01:08:03Z</published>
    <title>Evolutionary Synthesis of Deep Neural Networks via Synaptic
  Cluster-driven Genetic Encoding</title>
    <summary>  There has been significant recent interest towards achieving highly efficient
deep neural network architectures. A promising paradigm for achieving this is
the concept of evolutionary deep intelligence, which attempts to mimic
biological evolution processes to synthesize highly-efficient deep neural
networks over successive generations. An important aspect of evolutionary deep
intelligence is the genetic encoding scheme used to mimic heredity, which can
have a significant impact on the quality of offspring deep neural networks.
Motivated by the neurobiological phenomenon of synaptic clustering, we
introduce a new genetic encoding scheme where synaptic probability is driven
towards the formation of a highly sparse set of synaptic clusters. Experimental
results for the task of image classification demonstrated that the synthesized
offspring networks using this synaptic cluster-driven genetic encoding scheme
can achieve state-of-the-art performance while having network architectures
that are not only significantly more efficient (with a ~125-fold decrease in
synapses for MNIST) compared to the original ancestor network, but also
tailored for GPU-accelerated machine learning applications.
</summary>
    <author>
      <name>Mohammad Javad Shafiee</name>
    </author>
    <author>
      <name>Alexander Wong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.01360v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.01360v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.02802v1</id>
    <updated>2017-09-08T06:34:44Z</updated>
    <published>2017-09-08T06:34:44Z</published>
    <title>Towards Proving the Adversarial Robustness of Deep Neural Networks</title>
    <summary>  Autonomous vehicles are highly complex systems, required to function reliably
in a wide variety of situations. Manually crafting software controllers for
these vehicles is difficult, but there has been some success in using deep
neural networks generated using machine-learning. However, deep neural networks
are opaque to human engineers, rendering their correctness very difficult to
prove manually; and existing automated techniques, which were not designed to
operate on neural networks, fail to scale to large systems. This paper focuses
on proving the adversarial robustness of deep neural networks, i.e. proving
that small perturbations to a correctly-classified input to the network cannot
cause it to be misclassified. We describe some of our recent and ongoing work
on verifying the adversarial robustness of networks, and discuss some of the
open questions we have encountered and how they might be addressed.
</summary>
    <author>
      <name>Guy Katz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Stanford University</arxiv:affiliation>
    </author>
    <author>
      <name>Clark Barrett</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Stanford University</arxiv:affiliation>
    </author>
    <author>
      <name>David L. Dill</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Stanford University</arxiv:affiliation>
    </author>
    <author>
      <name>Kyle Julian</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Stanford University</arxiv:affiliation>
    </author>
    <author>
      <name>Mykel J. Kochenderfer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Stanford University</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.257.3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.257.3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings FVAV 2017, arXiv:1709.02126</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 257, 2017, pp. 19-26</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1709.02802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.02802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.4; I.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.02242v1</id>
    <updated>2017-10-06T00:14:52Z</updated>
    <published>2017-10-06T00:14:52Z</published>
    <title>Solving differential equations with unknown constitutive relations as
  recurrent neural networks</title>
    <summary>  We solve a system of ordinary differential equations with an unknown
functional form of a sink (reaction rate) term. We assume that the measurements
(time series) of state variables are partially available, and we use recurrent
neural network to "learn" the reaction rate from this data. This is achieved by
including a discretized ordinary differential equations as part of a recurrent
neural network training problem. We extend TensorFlow's recurrent neural
network architecture to create a simple but scalable and effective solver for
the unknown functions, and apply it to a fedbatch bioreactor simulation
problem. Use of techniques from recent deep learning literature enables
training of functions with behavior manifesting over thousands of time steps.
Our networks are structurally similar to recurrent neural networks, but
differences in design and function require modifications to the conventional
wisdom about training such networks.
</summary>
    <author>
      <name>Tobias Hagge</name>
    </author>
    <author>
      <name>Panos Stinis</name>
    </author>
    <author>
      <name>Enoch Yeung</name>
    </author>
    <author>
      <name>Alexandre M. Tartakovsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.02242v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.02242v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.06015v1</id>
    <updated>2018-01-18T13:52:06Z</updated>
    <published>2018-01-18T13:52:06Z</published>
    <title>Neural network-based preprocessing to estimate the parameters of the
  X-ray emission of a single-temperature thermal plasma</title>
    <summary>  We present data preprocessing based on an artificial neural network to
estimate the parameters of the X-ray emission spectra of a single-temperature
thermal plasma. The method finds appropriate parameters close to the global
optimum. The neural network is designed to learn the parameters of the thermal
plasma (temperature, abundance, normalisation, and redshift) of the input
spectra. After training using 9000 simulated X-ray spectra, the network has
grown to predict all the unknown parameters with uncertainties of about a few
percent. The performance dependence on the network structure has been studied.
We applied the neural network to an actual high-resolution spectrum obtained
with {\it Hitomi}. The predicted plasma parameters agreed with the known
best-fit parameters of the Perseus cluster within $\lesssim10$\% uncertainties.
The result shows a possibility that neural networks trained by simulated data
can be useful to extract a feature built in the data, which would reduce
human-intensive preprocessing costs before detailed spectral analysis, and help
us make the best use of large quantities of spectral data coming in the next
decades.
</summary>
    <author>
      <name>Y. Ichinohe</name>
    </author>
    <author>
      <name>S. Yamada</name>
    </author>
    <author>
      <name>N. Miyazaki</name>
    </author>
    <author>
      <name>S. Saito</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in MNRAS</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.06015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.06015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.00968v1</id>
    <updated>2018-01-03T11:53:34Z</updated>
    <published>2018-01-03T11:53:34Z</published>
    <title>Joint convolutional neural pyramid for depth map super-resolution</title>
    <summary>  High-resolution depth map can be inferred from a low-resolution one with the
guidance of an additional high-resolution texture map of the same scene.
Recently, deep neural networks with large receptive fields are shown to benefit
applications such as image completion. Our insight is that super resolution is
similar to image completion, where only parts of the depth values are precisely
known. In this paper, we present a joint convolutional neural pyramid model
with large receptive fields for joint depth map super-resolution. Our model
consists of three sub-networks, two convolutional neural pyramids concatenated
by a normal convolutional neural network. The convolutional neural pyramids
extract information from large receptive fields of the depth map and guidance
map, while the convolutional neural network effectively transfers useful
structures of the guidance image to the depth image. Experimental results show
that our model outperforms existing state-of-the-art algorithms not only on
data pairs of RGB/depth images, but also on other data pairs like
color/saliency and color-scribbles/colorized images.
</summary>
    <author>
      <name>Yi Xiao</name>
    </author>
    <author>
      <name>Xiang Cao</name>
    </author>
    <author>
      <name>Xianyi Zhu</name>
    </author>
    <author>
      <name>Renzhi Yang</name>
    </author>
    <author>
      <name>Yan Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/1801.00968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.00968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0109256v2</id>
    <updated>2003-09-25T15:27:24Z</updated>
    <published>2001-09-14T00:50:48Z</published>
    <title>Self-organized critical neural networks</title>
    <summary>  A mechanism for self-organization of the degree of connectivity in model
neural networks is studied. Network connectivity is regulated locally on the
basis of an order parameter of the global dynamics which is estimated from an
observable at the single synapse level. This principle is studied in a
two-dimensional neural network with randomly wired asymmetric weights. In this
class of networks, network connectivity is closely related to a phase
transition between ordered and disordered dynamics. A slow topology change is
imposed on the network through a local rewiring rule motivated by
activity-dependent synaptic development: Neighbor neurons whose activity is
correlated, on average develop a new connection while uncorrelated neighbors
tend to disconnect. As a result, robust self-organization of the network
towards the order disorder transition occurs. Convergence is independent of
initial conditions, robust against thermal noise, and does not require fine
tuning of parameters.
</summary>
    <author>
      <name>Stefan Bornholdt</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Kiel University</arxiv:affiliation>
    </author>
    <author>
      <name>Torsten Roehl</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Kiel University</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.67.066118</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.67.066118" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages RevTeX, 7 figures PostScript</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 67 (2003) 066118</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0109256v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0109256v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0506032v1</id>
    <updated>2005-06-10T05:30:41Z</updated>
    <published>2005-06-10T05:30:41Z</published>
    <title>Framework for Hopfield Network based Adaptive routing - A design level
  approach for adaptive routing phenomena with Artificial Neural Network</title>
    <summary>  Routing, as a basic phenomena, by itself, has got umpteen scopes to analyse,
discuss and arrive at an optimal solution for the technocrats over years.
Routing is analysed based on many factors; few key constraints that decide the
factors are communication medium, time dependency, information source nature.
Parametric routing has become the requirement of the day, with some kind of
adaptation to the underlying network environment. Satellite constellations,
particularly LEO satellite constellations have become a reality in operational
to have a non-breaking voice/data communication around the world.Routing in
these constellations has to be treated in a non conventional way, taking their
network geometry into consideration. One of the efficient methods of
optimization is putting Neural Networks to use. Few Artificial Neural Network
models are very much suitable for the adaptive control mechanism, by their
nature of network arrangement. One such efficient model is Hopfield Network
model.
  This paper is an attempt to design a framework for the Hopfield Network based
adaptive routing phenomena in satellite constellations.
</summary>
    <author>
      <name>R. Shankar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">(13 pages, 7 figures, code)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0506032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0506032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/q-bio/0701042v1</id>
    <updated>2007-01-26T09:15:39Z</updated>
    <published>2007-01-26T09:15:39Z</published>
    <title>Stochastic dynamics of a finite-size spiking neural network</title>
    <summary>  We present a simple Markov model of spiking neural dynamics that can be
analytically solved to characterize the stochastic dynamics of a finite-size
spiking neural network. We give closed-form estimates for the equilibrium
distribution, mean rate, variance and autocorrelation function of the network
activity. The model is applicable to any network where the probability of
firing of a neuron in the network only depends on the number of neurons that
fired in a previous temporal epoch. Networks with statistically homogeneous
connectivity and membrane and synaptic time constants that are not excessively
long could satisfy these conditions. Our model completely accounts for the size
of the network and correlations in the firing activity. It also allows us to
examine how the network dynamics can deviate from mean-field theory. We show
that the model and solutions are applicable to spiking neural networks in
biophysically plausible parameter regimes.
</summary>
    <author>
      <name>H. Soula</name>
    </author>
    <author>
      <name>C. C. Chow</name>
    </author>
    <link href="http://arxiv.org/abs/q-bio/0701042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/q-bio/0701042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0706.2048v1</id>
    <updated>2007-06-14T12:00:16Z</updated>
    <published>2007-06-14T12:00:16Z</published>
    <title>Thouless-Anderson-Palmer equation for analog neural network with
  temporally fluctuating white synaptic noise</title>
    <summary>  Effects of synaptic noise on the retrieval process of associative memory
neural networks are studied from the viewpoint of neurobiological and
biophysical understanding of information processing in the brain. We
investigate the statistical mechanical properties of stochastic analog neural
networks with temporally fluctuating synaptic noise, which is assumed to be
white noise. Such networks, in general, defy the use of the replica method,
since they have no energy concept. The self-consistent signal-to-noise analysis
(SCSNA), which is an alternative to the replica method for deriving a set of
order parameter equations, requires no energy concept and thus becomes
available in studying networks without energy functions. Applying the SCSNA to
stochastic network requires the knowledge of the Thouless-Anderson-Palmer (TAP)
equation which defines the deterministic networks equivalent to the original
stochastic ones. The study of the TAP equation which is of particular interest
for the case without energy concept is very few, while it is closely related to
the SCSNA in the case with energy concept. This paper aims to derive the TAP
equation for networks with synaptic noise together with a set of order
parameter equations by a hybrid use of the cavity method and the SCSNA.
</summary>
    <author>
      <name>Akihisa Ichiki</name>
    </author>
    <author>
      <name>Masatoshi Shiino</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1751-8113/40/31/002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1751-8113/40/31/002" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0706.2048v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0706.2048v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.3934v3</id>
    <updated>2010-06-22T04:05:08Z</updated>
    <published>2009-08-27T16:33:52Z</published>
    <title>A framework for simulating and estimating the state and functional
  topology of complex dynamic geometric networks</title>
    <summary>  We present a framework for simulating signal propagation in geometric
networks (i.e. networks that can be mapped to geometric graphs in some space)
and for developing algorithms that estimate (i.e. map) the state and functional
topology of complex dynamic geometric net- works. Within the framework we
define the key features typically present in such networks and of particular
relevance to biological cellular neural networks: Dynamics, signaling,
observation, and control. The framework is particularly well-suited for
estimating functional connectivity in cellular neural networks from
experimentally observable data, and has been implemented using graphics
processing unit (GPU) high performance computing. Computationally, the
framework can simulate cellular network signaling close to or faster than real
time. We further propose a standard test set of networks to measure performance
and compare different mapping algorithms.
</summary>
    <author>
      <name>Marius Buibas</name>
    </author>
    <author>
      <name>Gabriel A. Silva</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Revised following initial peer review. Current version 28 pages and 7
  figures. A slightly modified version has been accepted to Neural Computation
  and is now in press</arxiv:comment>
    <link href="http://arxiv.org/abs/0908.3934v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.3934v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.06633v2</id>
    <updated>2017-09-23T08:08:07Z</updated>
    <published>2017-08-22T14:25:55Z</published>
    <title>Nonparametric regression using deep neural networks with ReLU activation
  function</title>
    <summary>  Consider the multivariate nonparametric regression model. It is shown that
estimators based on sparsely connected deep neural networks with ReLU
activation function and properly chosen network architecture achieve the
minimax rates of convergence (up to log n-factors) under a general composition
assumption on the regression function. The framework includes many well-studied
structural constraints such as (generalized) additive models. While there is a
lot of flexibility in the network architecture, the tuning parameter is the
sparsity of the network. Specifically, we consider large networks with number
of potential parameters being much bigger than the sample size. The analysis
gives some insights why multilayer feedforward neural networks perform well in
practice. Interestingly, the depth (number of layers) of the neural network
architectures plays an important role and our theory suggests that scaling the
network depth with the logarithm of the sample size is natural.
</summary>
    <author>
      <name>Johannes Schmidt-Hieber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.06633v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.06633v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G08" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.02043v1</id>
    <updated>2017-09-07T01:43:19Z</updated>
    <published>2017-09-07T01:43:19Z</published>
    <title>The Mating Rituals of Deep Neural Networks: Learning Compact Feature
  Representations through Sexual Evolutionary Synthesis</title>
    <summary>  Evolutionary deep intelligence was recently proposed as a method for
achieving highly efficient deep neural network architectures over successive
generations. Drawing inspiration from nature, we propose the incorporation of
sexual evolutionary synthesis. Rather than the current asexual synthesis of
networks, we aim to produce more compact feature representations by
synthesizing more diverse and generalizable offspring networks in subsequent
generations via the combination of two parent networks. Experimental results
were obtained using the MNIST and CIFAR-10 datasets, and showed improved
architectural efficiency and comparable testing accuracy relative to the
baseline asexual evolutionary neural networks. In particular, the network
synthesized via sexual evolutionary synthesis for MNIST had approximately
double the architectural efficiency (cluster efficiency of 34.29X and synaptic
efficiency of 258.37X) in comparison to the network synthesized via asexual
evolutionary synthesis, with both networks achieving a testing accuracy of
~97%.
</summary>
    <author>
      <name>Audrey Chung</name>
    </author>
    <author>
      <name>Mohammad Javad Shafiee</name>
    </author>
    <author>
      <name>Paul Fieguth</name>
    </author>
    <author>
      <name>Alexander Wong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.02043v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.02043v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.10174v1</id>
    <updated>2017-10-27T14:42:09Z</updated>
    <published>2017-10-27T14:42:09Z</published>
    <title>SGD Learns Over-parameterized Networks that Provably Generalize on
  Linearly Separable Data</title>
    <summary>  Neural networks exhibit good generalization behavior in the
over-parameterized regime, where the number of network parameters exceeds the
number of observations. Nonetheless, current generalization bounds for neural
networks fail to explain this phenomenon. In an attempt to bridge this gap, we
study the problem of learning a two-layer over-parameterized neural network,
when the data is generated by a linearly separable function. In the case where
the network has Leaky ReLU activations, we provide both optimization and
generalization guarantees for over-parameterized networks. Specifically, we
prove convergence rates of SGD to a global minimum and provide generalization
guarantees for this global minimum that are independent of the network size.
Therefore, our result clearly shows that the use of SGD for optimization both
finds a global minimum, and avoids overfitting despite the high capacity of the
model. This is the first theoretical demonstration that SGD can avoid
overfitting, when learning over-specified neural network classifiers.
</summary>
    <author>
      <name>Alon Brutzkus</name>
    </author>
    <author>
      <name>Amir Globerson</name>
    </author>
    <author>
      <name>Eran Malach</name>
    </author>
    <author>
      <name>Shai Shalev-Shwartz</name>
    </author>
    <link href="http://arxiv.org/abs/1710.10174v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.10174v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.07650v1</id>
    <updated>2018-01-23T16:43:59Z</updated>
    <published>2018-01-23T16:43:59Z</published>
    <title>Dynamic Optimization of Neural Network Structures Using Probabilistic
  Modeling</title>
    <summary>  Deep neural networks (DNNs) are powerful machine learning models and have
succeeded in various artificial intelligence tasks. Although various
architectures and modules for the DNNs have been proposed, selecting and
designing the appropriate network structure for a target problem is a
challenging task. In this paper, we propose a method to simultaneously optimize
the network structure and weight parameters during neural network training. We
consider a probability distribution that generates network structures, and
optimize the parameters of the distribution instead of directly optimizing the
network structure. The proposed method can apply to the various network
structure optimization problems under the same framework. We apply the proposed
method to several structure optimization problems such as selection of layers,
selection of unit types, and selection of connections using the MNIST,
CIFAR-10, and CIFAR-100 datasets. The experimental results show that the
proposed method can find the appropriate and competitive network structures.
</summary>
    <author>
      <name>Shinichi Shirakawa</name>
    </author>
    <author>
      <name>Yasushi Iwata</name>
    </author>
    <author>
      <name>Youhei Akimoto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the Thirty-Second AAAI Conference on Artificial
  Intelligence (AAAI-18), 9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.07650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.07650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.02081v1</id>
    <updated>2017-04-07T03:28:02Z</updated>
    <published>2017-04-07T03:28:02Z</published>
    <title>Evolution in Groups: A deeper look at synaptic cluster driven evolution
  of deep neural networks</title>
    <summary>  A promising paradigm for achieving highly efficient deep neural networks is
the idea of evolutionary deep intelligence, which mimics biological evolution
processes to progressively synthesize more efficient networks. A crucial design
factor in evolutionary deep intelligence is the genetic encoding scheme used to
simulate heredity and determine the architectures of offspring networks. In
this study, we take a deeper look at the notion of synaptic cluster-driven
evolution of deep neural networks which guides the evolution process towards
the formation of a highly sparse set of synaptic clusters in offspring
networks. Utilizing a synaptic cluster-driven genetic encoding, the
probabilistic encoding of synaptic traits considers not only individual
synaptic properties but also inter-synaptic relationships within a deep neural
network. This process results in highly sparse offspring networks which are
particularly tailored for parallel computational devices such as GPUs and deep
neural network accelerator chips. Comprehensive experimental results using four
well-known deep neural network architectures (LeNet-5, AlexNet, ResNet-56, and
DetectNet) on two different tasks (object categorization and object detection)
demonstrate the efficiency of the proposed method. Cluster-driven genetic
encoding scheme synthesizes networks that can achieve state-of-the-art
performance with significantly smaller number of synapses than that of the
original ancestor network. ($\sim$125-fold decrease in synapses for MNIST).
Furthermore, the improved cluster efficiency in the generated offspring
networks ($\sim$9.71-fold decrease in clusters for MNIST and a $\sim$8.16-fold
decrease in clusters for KITTI) is particularly useful for accelerated
performance on parallel computing hardware architectures such as those in GPUs
and deep neural network accelerator chips.
</summary>
    <author>
      <name>Mohammad Javad Shafiee</name>
    </author>
    <author>
      <name>Elnaz Barshan</name>
    </author>
    <author>
      <name>Alexander Wong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages. arXiv admin note: substantial text overlap with
  arXiv:1609.01360</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.02081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.02081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.09505v1</id>
    <updated>2017-10-26T01:30:00Z</updated>
    <published>2017-10-26T01:30:00Z</published>
    <title>Knowledge Projection for Deep Neural Networks</title>
    <summary>  While deeper and wider neural networks are actively pushing the performance
limits of various computer vision and machine learning tasks, they often
require large sets of labeled data for effective training and suffer from
extremely high computational complexity. In this paper, we will develop a new
framework for training deep neural networks on datasets with limited labeled
samples using cross-network knowledge projection which is able to improve the
network performance while reducing the overall computational complexity
significantly. Specifically, a large pre-trained teacher network is used to
observe samples from the training data. A projection matrix is learned to
project this teacher-level knowledge and its visual representations from an
intermediate layer of the teacher network to an intermediate layer of a thinner
and faster student network to guide and regulate its training process. Both the
intermediate layers from the teacher network and the injection layers from the
student network are adaptively selected during training by evaluating a joint
loss function in an iterative manner. This knowledge projection framework
allows us to use crucial knowledge learned by large networks to guide the
training of thinner student networks, avoiding over-fitting, achieving better
network performance, and significantly reducing the complexity. Extensive
experimental results on benchmark datasets have demonstrated that our proposed
knowledge projection approach outperforms existing methods, improving accuracy
by up to 4% while reducing network complexity by 4 to 10 times, which is very
attractive for practical applications of deep neural networks.
</summary>
    <author>
      <name>Zhi Zhang</name>
    </author>
    <author>
      <name>Guanghan Ning</name>
    </author>
    <author>
      <name>Zhihai He</name>
    </author>
    <link href="http://arxiv.org/abs/1710.09505v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.09505v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06402v1</id>
    <updated>2016-05-20T15:22:29Z</updated>
    <published>2016-05-20T15:22:29Z</published>
    <title>Ristretto: Hardware-Oriented Approximation of Convolutional Neural
  Networks</title>
    <summary>  Convolutional neural networks (CNN) have achieved major breakthroughs in
recent years. Their performance in computer vision have matched and in some
areas even surpassed human capabilities. Deep neural networks can capture
complex non-linear features; however this ability comes at the cost of high
computational and memory requirements. State-of-art networks require billions
of arithmetic operations and millions of parameters. To enable embedded devices
such as smartphones, Google glasses and monitoring cameras with the astonishing
power of deep learning, dedicated hardware accelerators can be used to decrease
both execution time and power consumption. In applications where fast
connection to the cloud is not guaranteed or where privacy is important,
computation needs to be done locally. Many hardware accelerators for deep
neural networks have been proposed recently. A first important step of
accelerator design is hardware-oriented approximation of deep networks, which
enables energy-efficient inference. We present Ristretto, a fast and automated
framework for CNN approximation. Ristretto simulates the hardware arithmetic of
a custom hardware accelerator. The framework reduces the bit-width of network
parameters and outputs of resource-intense layers, which reduces the chip area
for multiplication units significantly. Alternatively, Ristretto can remove the
need for multipliers altogether, resulting in an adder-only arithmetic. The
tool fine-tunes trimmed networks to achieve high classification accuracy. Since
training of deep neural networks can be time-consuming, Ristretto uses highly
optimized routines which run on the GPU. This enables fast compression of any
given network. Given a maximum tolerance of 1%, Ristretto can successfully
condense CaffeNet and SqueezeNet to 8-bit. The code for Ristretto is available.
</summary>
    <author>
      <name>Philipp Gysel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Master's Thesis, University of California, Davis; 73 pages and 29
  figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.06402v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06402v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01071v2</id>
    <updated>2017-10-28T06:57:49Z</updated>
    <published>2016-08-03T04:25:47Z</published>
    <title>Enhanced storage capacity with errors in scale-free Hopfield neural
  networks: an analytical study</title>
    <summary>  The Hopfield model is a pioneering neural network model with associative
memory retrieval. The analytical solution of the model in mean field limit
revealed that memories can be retrieved without any error up to a finite
storage capacity of $O(N)$, where $N$ is the system size. Beyond the threshold,
they are completely lost. Since the introduction of the Hopfield model, the
theory of neural networks has been further developed toward realistic neural
networks using analog neurons, spiking neurons, etc. Nevertheless, those
advances are based on fully connected networks, which are inconsistent with
recent experimental discovery that the number of connections of each neuron
seems to be heterogeneous, following a heavy-tailed distribution. Motivated by
this observation, we consider the Hopfield model on scale-free networks and
obtain a different pattern of associative memory retrieval from that obtained
on the fully connected network: the storage capacity becomes tremendously
enhanced but with some error in the memory retrieval, which appears as the
heterogeneity of the connections is increased. Moreover, the error rates are
also obtained on several real neural networks and are indeed similar to that on
scale-free model networks.
</summary>
    <author>
      <name>Do-Hyun Kim</name>
    </author>
    <author>
      <name>Jinha Park</name>
    </author>
    <author>
      <name>B. Kahng</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pone.0184683</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pone.0184683" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">PLoS ONE 12(10): e0184683 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1608.01071v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01071v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.04045v2</id>
    <updated>2017-10-17T14:42:54Z</updated>
    <published>2017-10-11T13:03:19Z</published>
    <title>Neural Networks Quantum States, String-Bond States and chiral
  topological states</title>
    <summary>  Neural Networks Quantum States have been recently introduced as an Ansatz for
describing the wave function of quantum many-body systems. We show that there
are strong connections between Neural Networks Quantum States in the form of
Restricted Boltzmann Machines and some classes of Tensor Network states in
arbitrary dimension. In particular we demonstrate that short-range Restricted
Boltzmann Machines are Entangled Plaquette States, while fully connected
Restricted Boltzmann Machines are String-Bond States with a non-local geometry
and low bond dimension. These results shed light on the underlying architecture
of Restricted Boltzmann Machines and their efficiency at representing many-body
quantum states. String-Bond States also provide a generic way of enhancing the
power of Neural Networks Quantum States and a natural generalization to systems
with larger local Hilbert space. We compare the advantages and drawbacks of
these different classes of states and present a method to combine them
together. This allows us to benefit from both the entanglement structure of
Tensor Networks and the efficiency of Neural Network Quantum States into a
single Ansatz capable of targeting the wave function of strongly correlated
systems. While it remains a challenge to describe states with chiral
topological order using traditional Tensor Networks, we show that Neural
Networks Quantum States and their String-Bond States extension can describe a
lattice Fractional Quantum Hall state exactly. In addition, we provide
numerical evidence that Neural Networks Quantum States can approximate a chiral
spin liquid with better accuracy than Entangled Plaquette States and local
String-Bond States. Our results demonstrate the efficiency of neural networks
to describe complex quantum wave functions and pave the way towards the use of
String-Bond States as a tool in more traditional machine learning applications.
</summary>
    <author>
      <name>Ivan Glasser</name>
    </author>
    <author>
      <name>Nicola Pancotti</name>
    </author>
    <author>
      <name>Moritz August</name>
    </author>
    <author>
      <name>Ivan D. Rodriguez</name>
    </author>
    <author>
      <name>J. Ignacio Cirac</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevX.8.011006</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevX.8.011006" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 7 figures, Note added</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. X 8, 011006 (2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1710.04045v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.04045v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.str-el" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.7450v1</id>
    <updated>2013-11-29T00:24:52Z</updated>
    <published>2013-11-29T00:24:52Z</published>
    <title>Data-driven modeling of the olfactory neural codes and their dynamics in
  the insect antennal lobe</title>
    <summary>  Recordings from neurons in the insects' olfactory primary processing center,
the antennal lobe (AL), reveal that the AL is able to process the input from
chemical receptors into distinct neural activity patterns, called olfactory
neural codes. These exciting results show the importance of neural codes and
their relation to perception. The next challenge is to \emph{model the
dynamics} of neural codes. In our study, we perform multichannel recordings
from the projection neurons in the AL driven by different odorants. We then
derive a neural network from the electrophysiological data. The network
consists of lateral-inhibitory neurons and excitatory neurons, and is capable
of producing unique olfactory neural codes for the tested odorants.
Specifically, we (i) design a projection, an odor space, for the neural
recording from the AL, which discriminates between distinct odorants
trajectories (ii) characterize scent recognition, i.e., decision-making based
on olfactory signals and (iii) infer the wiring of the neural circuit, the
connectome of the AL. We show that the constructed model is consistent with
biological observations, such as contrast enhancement and robustness to noise.
The study answers a key biological question in identifying how lateral
inhibitory neurons can be wired to excitatory neurons to permit robust activity
patterns.
</summary>
    <author>
      <name>Eli Shlizerman</name>
    </author>
    <author>
      <name>Jeffrey A. Riffell</name>
    </author>
    <author>
      <name>J. Nathan Kutz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3389/fncom.2014.00070</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3389/fncom.2014.00070" rel="related"/>
    <link href="http://arxiv.org/abs/1311.7450v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.7450v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/astro-ph/9708206v1</id>
    <updated>1997-08-22T10:39:32Z</updated>
    <published>1997-08-22T10:39:32Z</published>
    <title>Physical Parameterization of Stellar Spectra: The Neural Network
  Approach</title>
    <summary>  We present a technique which employs artificial neural networks to produce
physical parameters for stellar spectra. A neural network is trained on a set
of synthetic optical stellar spectra to give physical parameters (e.g. T_eff,
log g, [M/H]). The network is then used to produce physical parameters for
real, observed spectra.
  Our neural networks are trained on a set of 155 synthetic spectra, generated
using the SPECTRUM program written by Gray (Gray &amp; Corbally 1994, Gray &amp; Arlt
1996). Once trained, the neural network is used to yield T_eff for over 5000
B-K spectra extracted from a set of photographic objective prism plates
(Bailer-Jones, Irwin &amp; von Hippel 1997a). Using the MK classifications for
these spectra assigned by Houk (1975, 1978, 1982, 1988) we have produced a
temperature calibration of the MK system based on this set of 5000 spectra. It
is demonstrated through the metallicity dependence of the derived temperature
calibration that the neural networks are sensitive to the metallicity signature
in the real spectra. With further work it is likely that neural networks will
be able to yield reliable metallicity measurements for stellar spectra.
</summary>
    <author>
      <name>Coryn A. L. Bailer-Jones</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Institute of Astronomy, Cambridge</arxiv:affiliation>
    </author>
    <author>
      <name>Mike Irwin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Royal Greenwich Observatory, Cambridge</arxiv:affiliation>
    </author>
    <author>
      <name>Gerard Gilmore</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Institute of Astronomy, Cambridge</arxiv:affiliation>
    </author>
    <author>
      <name>Ted von Hippel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Wisconsin, Madison</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/mnras/292.1.157</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/mnras/292.1.157" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures. Accepted to MNRAS</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">MNRAS (1997) 292, 157</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/astro-ph/9708206v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/astro-ph/9708206v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.5488v1</id>
    <updated>2014-03-21T15:11:52Z</updated>
    <published>2014-03-21T15:11:52Z</published>
    <title>Missing Data Prediction and Classification: The Use of Auto-Associative
  Neural Networks and Optimization Algorithms</title>
    <summary>  This paper presents methods which are aimed at finding approximations to
missing data in a dataset by using optimization algorithms to optimize the
network parameters after which prediction and classification tasks can be
performed. The optimization methods that are considered are genetic algorithm
(GA), simulated annealing (SA), particle swarm optimization (PSO), random
forest (RF) and negative selection (NS) and these methods are individually used
in combination with auto-associative neural networks (AANN) for missing data
estimation and the results obtained are compared. The methods suggested use the
optimization algorithms to minimize an error function derived from training the
auto-associative neural network during which the interrelationships between the
inputs and the outputs are obtained and stored in the weights connecting the
different layers of the network. The error function is expressed as the square
of the difference between the actual observations and predicted values from an
auto-associative neural network. In the event of missing data, all the values
of the actual observations are not known hence, the error function is
decomposed to depend on the known and unknown variable values. Multi-layer
perceptron (MLP) neural network is employed to train the neural networks using
the scaled conjugate gradient (SCG) method. Prediction accuracy is determined
by mean squared error (MSE), root mean squared error (RMSE), mean absolute
error (MAE), and correlation coefficient (r) computations. Accuracy in
classification is obtained by plotting ROC curves and calculating the areas
under these. Analysis of results depicts that the approach using RF with AANN
produces the most accurate predictions and classifications while on the other
end of the scale is the approach which entails using NS with AANN.
</summary>
    <author>
      <name>Collins Leke</name>
    </author>
    <author>
      <name>Bhekisipho Twala</name>
    </author>
    <author>
      <name>T. Marwala</name>
    </author>
    <link href="http://arxiv.org/abs/1403.5488v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.5488v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08734v1</id>
    <updated>2017-01-30T18:06:07Z</updated>
    <published>2017-01-30T18:06:07Z</published>
    <title>PathNet: Evolution Channels Gradient Descent in Super Neural Networks</title>
    <summary>  For artificial general intelligence (AGI) it would be efficient if multiple
users trained the same giant neural network, permitting parameter reuse,
without catastrophic forgetting. PathNet is a first step in this direction. It
is a neural network algorithm that uses agents embedded in the neural network
whose task is to discover which parts of the network to re-use for new tasks.
Agents are pathways (views) through the network which determine the subset of
parameters that are used and updated by the forwards and backwards passes of
the backpropogation algorithm. During learning, a tournament selection genetic
algorithm is used to select pathways through the neural network for replication
and mutation. Pathway fitness is the performance of that pathway measured
according to a cost function. We demonstrate successful transfer learning;
fixing the parameters along a path learned on task A and re-evolving a new
population of paths for task B, allows task B to be learned faster than it
could be learned from scratch or after fine-tuning. Paths evolved on task B
re-use parts of the optimal path evolved on task A. Positive transfer was
demonstrated for binary MNIST, CIFAR, and SVHN supervised learning
classification tasks, and a set of Atari and Labyrinth reinforcement learning
tasks, suggesting PathNets have general applicability for neural network
training. Finally, PathNet also significantly improves the robustness to
hyperparameter choices of a parallel asynchronous reinforcement learning
algorithm (A3C).
</summary>
    <author>
      <name>Chrisantha Fernando</name>
    </author>
    <author>
      <name>Dylan Banarse</name>
    </author>
    <author>
      <name>Charles Blundell</name>
    </author>
    <author>
      <name>Yori Zwols</name>
    </author>
    <author>
      <name>David Ha</name>
    </author>
    <author>
      <name>Andrei A. Rusu</name>
    </author>
    <author>
      <name>Alexander Pritzel</name>
    </author>
    <author>
      <name>Daan Wierstra</name>
    </author>
    <link href="http://arxiv.org/abs/1701.08734v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08734v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07038v2</id>
    <updated>2017-08-05T12:30:25Z</updated>
    <published>2017-05-19T15:07:07Z</published>
    <title>The Landscape of Deep Learning Algorithms</title>
    <summary>  This paper studies the landscape of empirical risk of deep neural networks by
theoretically analyzing its convergence behavior to the population risk as well
as its stationary points and properties. For an $l$-layer linear neural
network, we prove its empirical risk uniformly converges to its population risk
at the rate of $\mathcal{O}(r^{2l}\sqrt{d\log(l)}/\sqrt{n})$ with training
sample size of $n$, the total weight dimension of $d$ and the magnitude bound
$r$ of weight of each layer. We then derive the stability and generalization
bounds for the empirical risk based on this result. Besides, we establish the
uniform convergence of gradient of the empirical risk to its population
counterpart. We prove the one-to-one correspondence of the non-degenerate
stationary points between the empirical and population risks with convergence
guarantees, which describes the landscape of deep neural networks. In addition,
we analyze these properties for deep nonlinear neural networks with sigmoid
activation functions. We prove similar results for convergence behavior of
their empirical risks as well as the gradients and analyze properties of their
non-degenerate stationary points.
  To our best knowledge, this work is the first one theoretically
characterizing landscapes of deep learning algorithms. Besides, our results
provide the sample complexity of training a good deep neural network. We also
provide theoretical understanding on how the neural network depth $l$, the
layer width, the network size $d$ and parameter magnitude determine the neural
network landscapes.
</summary>
    <author>
      <name>Pan Zhou</name>
    </author>
    <author>
      <name>Jiashi Feng</name>
    </author>
    <link href="http://arxiv.org/abs/1705.07038v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07038v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.05738v1</id>
    <updated>2017-11-15T18:26:49Z</updated>
    <published>2017-11-15T18:26:49Z</published>
    <title>The Neural Network Pushdown Automaton: Model, Stack and Learning
  Simulations</title>
    <summary>  In order for neural networks to learn complex languages or grammars, they
must have sufficient computational power or resources to recognize or generate
such languages. Though many approaches have been discussed, one ob- vious
approach to enhancing the processing power of a recurrent neural network is to
couple it with an external stack memory - in effect creating a neural network
pushdown automata (NNPDA). This paper discusses in detail this NNPDA - its
construction, how it can be trained and how useful symbolic information can be
extracted from the trained network.
  In order to couple the external stack to the neural network, an optimization
method is developed which uses an error function that connects the learning of
the state automaton of the neural network to the learning of the operation of
the external stack. To minimize the error function using gradient descent
learning, an analog stack is designed such that the action and storage of
information in the stack are continuous. One interpretation of a continuous
stack is the probabilistic storage of and action on data. After training on
sample strings of an unknown source grammar, a quantization procedure extracts
from the analog stack and neural network a discrete pushdown automata (PDA).
Simulations show that in learning deterministic context-free grammars - the
balanced parenthesis language, 1*n0*n, and the deterministic Palindrome - the
extracted PDA is correct in the sense that it can correctly recognize unseen
strings of arbitrary length. In addition, the extracted PDAs can be shown to be
identical or equivalent to the PDAs of the source grammars which were used to
generate the training strings.
</summary>
    <author>
      <name>G. Z. Sun</name>
    </author>
    <author>
      <name>C. L. Giles</name>
    </author>
    <author>
      <name>H. H. Chen</name>
    </author>
    <author>
      <name>Y. C. Lee</name>
    </author>
    <link href="http://arxiv.org/abs/1711.05738v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.05738v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.1864v2</id>
    <updated>2012-07-16T16:10:40Z</updated>
    <published>2010-10-09T18:50:07Z</published>
    <title>Transforming complex network to the acyclic one</title>
    <summary>  Acyclic networks are a class of complex networks in which links are directed
and don't have closed loops. Here we present an algorithm for transforming an
ordinary undirected complex network into an acyclic one. Further analysis of an
acyclic network allows finding structural properties of the network. With our
approach one can find the communities and key nodes in complex networks. Also
we propose a new parameter of complex networks which can mark most vulnerable
nodes of the system. The proposed algorithm can be applied to finding
communities and bottlenecks in general complex networks.
</summary>
    <author>
      <name>Roman Shevchuk</name>
    </author>
    <author>
      <name>Andrew Snarskii</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physa.2012.07.030</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physa.2012.07.030" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1010.1864v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.1864v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.08973v2</id>
    <updated>2015-12-08T00:48:54Z</updated>
    <published>2015-07-31T18:43:28Z</published>
    <title>Training recurrent neural networks with sparse, delayed rewards for
  flexible decision tasks</title>
    <summary>  Recurrent neural networks in the chaotic regime exhibit complex dynamics
reminiscent of high-level cortical activity during behavioral tasks. However,
existing training methods for such networks are either biologically
implausible, or require a real-time continuous error signal to guide the
learning process. This is in contrast with most behavioral tasks, which only
provide time-sparse, delayed rewards. Here we show that a biologically
plausible reward-modulated Hebbian learning algorithm, previously used in
feedforward models of birdsong learning, can train recurrent networks based
solely on delayed, phasic reward signals at the end of each trial. The method
requires no dedicated feedback or readout networks: the whole network
connectivity is subject to learning, and the network output is read from one
arbitrarily chosen network cell. We use this method to successfully train a
network on a delayed nonmatch to sample task (which requires memory, flexible
associations, and non-linear mixed selectivities). Using decoding techniques,
we show that the resulting networks exhibit dynamic coding of task-relevant
information, with neural encodings of various task features fluctuating widely
over the course of a trial. Furthermore, network activity moves from a
stimulus-specific representation to a response-specific representation during
response time, in accordance with neural recordings in behaving animals for
similar tasks. We conclude that recurrent neural networks, trained with
reward-modulated Hebbian learning, offer a plausible model of cortical dynamics
during learning and performance of flexible association.
</summary>
    <author>
      <name>Thomas Miconi</name>
    </author>
    <link href="http://arxiv.org/abs/1507.08973v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.08973v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/astro-ph/9607148v1</id>
    <updated>1996-07-29T09:20:06Z</updated>
    <published>1996-07-29T09:20:06Z</published>
    <title>Neural Networks and the Classification of Active Galactic Nucleus
  Spectra</title>
    <summary>  The use of Artificial Neural Networks (ANNs) as a classifier of digital
spectra is investigated. Using both simulated and real data, it is shown that
neural networks can be trained to discriminate between the spectra of different
classes of active galactic nucleus (AGN) with realistic sample sizes and
signal-to-noise ratios. By working in the Fourier domain, neural nets can
classify objects without knowledge of their redshifts.
</summary>
    <author>
      <name>Daya M. Rawson</name>
    </author>
    <author>
      <name>Jeremy Bailey</name>
    </author>
    <author>
      <name>Paul J. Francis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, LaTeX, including two postscript figures, 41 kb. Accepted
  for publication in Publ. ASA</arxiv:comment>
    <link href="http://arxiv.org/abs/astro-ph/9607148v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/astro-ph/9607148v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9811030v1</id>
    <updated>1998-11-24T22:51:20Z</updated>
    <published>1998-11-24T22:51:20Z</published>
    <title>Generating Segment Durations in a Text-To-Speech System: A Hybrid
  Rule-Based/Neural Network Approach</title>
    <summary>  A combination of a neural network with rule firing information from a
rule-based system is used to generate segment durations for a text-to-speech
system. The system shows a slight improvement in performance over a neural
network system without the rule firing information. Synthesized speech using
segment durations was accepted by listeners as having about the same quality as
speech generated using segment durations extracted from natural speech.
</summary>
    <author>
      <name>Gerald Corrigan</name>
    </author>
    <author>
      <name>Noel Massey</name>
    </author>
    <author>
      <name>Orhan Karaali</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, PostScript</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of Eurospeech (1997) 2675-2678. Rhodes, Greece</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9811030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9811030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9811032v1</id>
    <updated>1998-11-24T23:51:56Z</updated>
    <published>1998-11-24T23:51:56Z</published>
    <title>Text-To-Speech Conversion with Neural Networks: A Recurrent TDNN
  Approach</title>
    <summary>  This paper describes the design of a neural network that performs the
phonetic-to-acoustic mapping in a speech synthesis system. The use of a
time-domain neural network architecture limits discontinuities that occur at
phone boundaries. Recurrent data input also helps smooth the output parameter
tracks. Independent testing has demonstrated that the voice quality produced by
this system compares favorably with speech from existing commercial
text-to-speech systems.
</summary>
    <author>
      <name>Orhan Karaali</name>
    </author>
    <author>
      <name>Gerald Corrigan</name>
    </author>
    <author>
      <name>Ira Gerson</name>
    </author>
    <author>
      <name>Noel Massey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, PostScript</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of Eurospeech (1997) 561-564. Rhodes, Greece</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9811032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9811032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0412110v1</id>
    <updated>2004-12-24T13:48:44Z</updated>
    <published>2004-12-24T13:48:44Z</published>
    <title>Q-valued neural network as a system of fast identification and pattern
  recognition</title>
    <summary>  An effective neural network algorithm of the perceptron type is proposed. The
algorithm allows us to identify strongly distorted input vector reliably. It is
shown that its reliability and processing speed are orders of magnitude higher
than that of full connected neural networks. The processing speed of our
algorithm exceeds the one of the stack fast-access retrieval algorithm that is
modified for working when there are noises in the input channel.
</summary>
    <author>
      <name>D. I. Alieva</name>
    </author>
    <author>
      <name>B. V. Kryzhanovsky</name>
    </author>
    <author>
      <name>V. M. Kryzhanovsky</name>
    </author>
    <author>
      <name>A. B. Fonarev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, Presentation on the 7th International Conference on Pattern
  Recognition and Image Analysis PRIA-07-2004, St. Petersburg, Russia</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0412110v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0412110v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0501005v1</id>
    <updated>2005-01-03T18:55:47Z</updated>
    <published>2005-01-03T18:55:47Z</published>
    <title>Portfolio selection using neural networks</title>
    <summary>  In this paper we apply a heuristic method based on artificial neural networks
in order to trace out the efficient frontier associated to the portfolio
selection problem. We consider a generalization of the standard Markowitz
mean-variance model which includes cardinality and bounding constraints. These
constraints ensure the investment in a given number of different assets and
limit the amount of capital to be invested in each asset. We present some
experimental results obtained with the neural network heuristic and we compare
them to those obtained with three previous heuristic methods.
</summary>
    <author>
      <name>Alberto Fernandez</name>
    </author>
    <author>
      <name>Sergio Gomez</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cor.2005.06.017</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cor.2005.06.017" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages; submitted to "Computers &amp; Operations Research"</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computers &amp; Operations Research 34 (2007) 1177-1191</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0501005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0501005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504052v1</id>
    <updated>2005-04-13T13:22:49Z</updated>
    <published>2005-04-13T13:22:49Z</published>
    <title>Learning Multi-Class Neural-Network Models from Electroencephalograms</title>
    <summary>  We describe a new algorithm for learning multi-class neural-network models
from large-scale clinical electroencephalograms (EEGs). This algorithm trains
hidden neurons separately to classify all the pairs of classes. To find best
pairwise classifiers, our algorithm searches for input variables which are
relevant to the classification problem. Despite patient variability and heavily
overlapping classes, a 16-class model learnt from EEGs of 65 sleeping newborns
correctly classified 80.8% of the training and 80.1% of the testing examples.
Additionally, the neural-network model provides a probabilistic interpretation
of decisions.
</summary>
    <author>
      <name>Vitaly Schetinin</name>
    </author>
    <author>
      <name>Joachim Schult</name>
    </author>
    <author>
      <name>Burkhart Scheidt</name>
    </author>
    <author>
      <name>Valery Kuriakin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">KES-2003</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0504052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504067v1</id>
    <updated>2005-04-14T10:36:54Z</updated>
    <published>2005-04-14T10:36:54Z</published>
    <title>An Evolving Cascade Neural Network Technique for Cleaning Sleep
  Electroencephalograms</title>
    <summary>  Evolving Cascade Neural Networks (ECNNs) and a new training algorithm capable
of selecting informative features are described. The ECNN initially learns with
one input node and then evolves by adding new inputs as well as new hidden
neurons. The resultant ECNN has a near minimal number of hidden neurons and
inputs. The algorithm is successfully used for training ECNN to recognise
artefacts in sleep electroencephalograms (EEGs) which were visually labelled by
EEG-viewers. In our experiments, the ECNN outperforms the standard
neural-network as well as evolutionary techniques.
</summary>
    <author>
      <name>Vitaly Schetinin</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Natural Computing Application, 2005</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0504067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0602036v1</id>
    <updated>2006-02-10T06:32:29Z</updated>
    <published>2006-02-10T06:32:29Z</published>
    <title>R√©seaux d'Automates de Caianiello Revisit√©</title>
    <summary>  We exhibit a family of neural networks of McCulloch and Pitts of size $2nk+2$
which can be simulated by a neural networks of Caianiello of size $2n+2$ and
memory length $k$. This simulation allows us to find again one of the result of
the following article: [Cycles exponentiels des r\'{e}seaux de Caianiello et
compteurs en arithm\'{e}tique redondante, Technique et Science Informatiques
Vol. 19, pages 985-1008] on the existence of neural networks of Caianiello of
size $2n+2$ and memory length $k$ which describes a cycle of length $k \times
2^{nk}$.
</summary>
    <author>
      <name>Ren√© Ndoundam</name>
    </author>
    <author>
      <name>Maurice Tchuente</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">39 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0602036v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0602036v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/hep-ex/9705020v2</id>
    <updated>1997-06-02T17:12:25Z</updated>
    <published>1997-05-30T17:50:52Z</published>
    <title>Role of Neural Networks in the Search of the Higgs Boson at LHC</title>
    <summary>  We show that neural network classifiers can be helpful to discriminate Higgs
production from background at LHC in the Higgs mass range M= 200 GeV. We employ
a common feed-forward neural network trained by the backpropagation algorithm
for off-line analysis and the neural chip Totem, trained by the Reactive Tabu
Search algorithm, which could be used for on-line analysis.
</summary>
    <author>
      <name>T. Maggipinto</name>
    </author>
    <author>
      <name>G. Nardulli</name>
    </author>
    <author>
      <name>S. Dusini</name>
    </author>
    <author>
      <name>F. Ferrari</name>
    </author>
    <author>
      <name>I. Lazzizzera</name>
    </author>
    <author>
      <name>A. Sidoti</name>
    </author>
    <author>
      <name>A. Sartori</name>
    </author>
    <author>
      <name>G. P. Tecchiolli</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/S0370-2693(97)00887-3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/S0370-2693(97)00887-3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Latex, 8 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys.Lett. B409 (1997) 517-522</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/hep-ex/9705020v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/hep-ex/9705020v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/q-bio/0411034v1</id>
    <updated>2004-11-16T00:49:05Z</updated>
    <published>2004-11-16T00:49:05Z</published>
    <title>MultiNeuron - Neural Networks Simulator For Medical, Physiological, and
  Psychological Applications</title>
    <summary>  This work describes neural software applied in medicine and physiology to:
investigate and diagnose immune deficiencies; diagnose and study allergic and
pseudoallergic reactions; forecast emergence or aggravation of stagnant cardiac
insufficiency in patients with cardiac rhythm disorders; forecast development
of cardiac arrhythmia after myocardial infarction; reveal relationships between
the accumulated radiation dose and a set of immunological, hormonal, and
bio-chemical parameters of human blood and find a method to be able to judge by
these parameters the dose value; propose a technique for early diagnosis of
chor-oid melanomas; Neural networks also help to predict human relations within
a group.
</summary>
    <author>
      <name>A. N. Gorban</name>
    </author>
    <author>
      <name>D. A. Rossiyev</name>
    </author>
    <author>
      <name>M. G. Dorrer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The talk for the 1995 World Congress on Neural Networks</arxiv:comment>
    <link href="http://arxiv.org/abs/q-bio/0411034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/q-bio/0411034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.TO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/quant-ph/0202016v2</id>
    <updated>2004-06-25T14:38:00Z</updated>
    <published>2002-02-02T18:05:45Z</published>
    <title>Neural Networks with c-NOT Gated Nodes</title>
    <summary>  We try to design a quantum neural network with qubits instead of classical
neurons with deterministic states, and also with quantum operators replacing
teh classical action potentials. With our choice of gates interconnecting teh
neural lattice, it appears that the state of the system behaves in ways
reflecting both the strengths of coupling between neurons as well as initial
conditions. We find that depending whether there is a threshold for emission
from excited to ground state, the system shows either aperiodic oscillations or
coherent ones with periodicity depending on the strength of coupling.
</summary>
    <author>
      <name>Fariel Shafee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages 6 figures; minor corrections made; clearer explanations
  added; Engineering Applications of Artificial Intelligence, online Nov 1,
  2006</arxiv:comment>
    <link href="http://arxiv.org/abs/quant-ph/0202016v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/quant-ph/0202016v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.0199v2</id>
    <updated>2007-05-08T01:06:10Z</updated>
    <published>2007-05-02T04:04:51Z</published>
    <title>The Parameter-Less Self-Organizing Map algorithm</title>
    <summary>  The Parameter-Less Self-Organizing Map (PLSOM) is a new neural network
algorithm based on the Self-Organizing Map (SOM). It eliminates the need for a
learning rate and annealing schemes for learning rate and neighbourhood size.
We discuss the relative performance of the PLSOM and the SOM and demonstrate
some tasks in which the SOM fails but the PLSOM performs satisfactory. Finally
we discuss some example applications of the PLSOM and present a proof of
ordering under certain limited conditions.
</summary>
    <author>
      <name>Erik Berglund</name>
    </author>
    <author>
      <name>Joaquin Sitte</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 27 figures. Based on publication in IEEE Trans. on Neural
  Networks</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Neural Networks, 2006 v.17, n.2, pp.305-316</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0705.0199v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.0199v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.4032v1</id>
    <updated>2007-07-27T02:38:56Z</updated>
    <published>2007-07-27T02:38:56Z</published>
    <title>One-way Hash Function Based on Neural Network</title>
    <summary>  A hash function is constructed based on a three-layer neural network. The
three neuron-layers are used to realize data confusion, diffusion and
compression respectively, and the multi-block hash mode is presented to support
the plaintext with variable length. Theoretical analysis and experimental
results show that this hash function is one-way, with high key sensitivity and
plaintext sensitivity, and secure against birthday attacks or
meet-in-the-middle attacks. Additionally, the neural network's property makes
it practical to realize in a parallel way. These properties make it a suitable
choice for data signature or authentication.
</summary>
    <author>
      <name>Shiguo Lian</name>
    </author>
    <author>
      <name>Jinsheng Sun</name>
    </author>
    <author>
      <name>Zhiquan Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages,5 figures,submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/0707.4032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.4032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; C.2.0; E.3.x; F.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.2227v1</id>
    <updated>2007-10-11T12:09:33Z</updated>
    <published>2007-10-11T12:09:33Z</published>
    <title>A System for Predicting Subcellular Localization of Yeast Genome Using
  Neural Network</title>
    <summary>  The subcellular location of a protein can provide valuable information about
its function. With the rapid increase of sequenced genomic data, the need for
an automated and accurate tool to predict subcellular localization becomes
increasingly important. Many efforts have been made to predict protein
subcellular localization. This paper aims to merge the artificial neural
networks and bioinformatics to predict the location of protein in yeast genome.
We introduce a new subcellular prediction method based on a backpropagation
neural network. The results show that the prediction within an error limit of 5
to 10 percentage can be achieved with the system.
</summary>
    <author>
      <name>Sabu M. Thampi</name>
    </author>
    <author>
      <name>K. Chandra Sekaran</name>
    </author>
    <link href="http://arxiv.org/abs/0710.2227v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.2227v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.3269v1</id>
    <updated>2008-04-21T15:38:45Z</updated>
    <published>2008-04-21T15:38:45Z</published>
    <title>Phoneme recognition in TIMIT with BLSTM-CTC</title>
    <summary>  We compare the performance of a recurrent neural network with the best
results published so far on phoneme recognition in the TIMIT database. These
published results have been obtained with a combination of classifiers.
However, in this paper we apply a single recurrent neural network to the same
task. Our recurrent neural network attains an error rate of 24.6%. This result
is not significantly different from that obtained by the other best methods,
but they rely on a combination of classifiers for achieving comparable
performance.
</summary>
    <author>
      <name>Santiago Fern√°ndez</name>
    </author>
    <author>
      <name>Alex Graves</name>
    </author>
    <author>
      <name>Juergen Schmidhuber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0804.3269v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.3269v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0904.3650v1</id>
    <updated>2009-04-23T10:44:21Z</updated>
    <published>2009-04-23T10:44:21Z</published>
    <title>The use of invariant moments in hand-written character recognition</title>
    <summary>  The goal of this paper is to present the implementation of a Radial Basis
Function neural network with built-in knowledge to recognize hand-written
characters. The neural network includes in its architecture gates controlled by
an attraction/repulsion system of coefficients. These coefficients are derived
from a preprocessing stage which groups the characters according to their
ascendant, central, or descendent components. The neural network is trained
using data from invariant moment functions. Results are compared with those
obtained using a K nearest neighbor method on the same moment data.
</summary>
    <author>
      <name>Dan L. Lacrama</name>
    </author>
    <author>
      <name>Ioan Snep</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages,exposed on 1st "European Conference on Computer Sciences &amp;
  Applications" - XA2006, Timisoara, Romania</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ann. Univ. Tibiscus Comp. Sci. Series IV (2006), 91-102</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0904.3650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0904.3650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.3557v1</id>
    <updated>2010-04-20T20:17:41Z</updated>
    <published>2010-04-20T20:17:41Z</published>
    <title>Neuroevolutionary optimization</title>
    <summary>  This paper presents an application of evolutionary search procedures to
artificial neural networks. Here, we can distinguish among three kinds of
evolution in artificial neural networks, i.e. the evolution of connection
weights, of architectures, and of learning rules. We review each kind of
evolution in detail and analyse critical issues related to different
evolutions. This article concentrates on finding the suitable way of using
evolutionary algorithms for optimizing the artificial neural network
parameters.
</summary>
    <author>
      <name>Eva Volna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues online at
  http://ijcsi.org/articles/Neuroevolutionary-optimization.php</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI, Volume 7, Issue 2, March 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.3557v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.3557v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.5953v1</id>
    <updated>2012-02-27T14:36:43Z</updated>
    <published>2012-02-27T14:36:43Z</published>
    <title>On an Ethical Use of Neural Networks: A Case Study on a North Indian
  Raga</title>
    <summary>  The paper gives an artificial neural network (ANN) approach to time series
modeling, the data being instance versus notes (characterized by pitch)
depicting the structure of a North Indian raga, namely, Bageshree. Respecting
the sentiments of the artists' community, the paper argues why it is more
ethical to model a structure than try and "manufacture" an artist by training
the neural network to copy performances of artists. Indian Classical Music
centers on the ragas, where emotion and devotion are both important and neither
can be substituted by such "calculated artistry" which the ANN generated copies
are ultimately up to.
</summary>
    <author>
      <name>Ripunjai Kumar Shukla</name>
    </author>
    <author>
      <name>Soubhik Chakraborty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ann. Univ. Tibiscus Comp. Sci. Series VII/2 (2009), 41-56</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1202.5953v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.5953v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.3782v3</id>
    <updated>2016-04-22T18:45:01Z</updated>
    <published>2013-07-14T21:03:39Z</published>
    <title>Handwritten Digits Recognition using Deep Convolutional Neural Network:
  An Experimental Study using EBlearn</title>
    <summary>  In this paper, results of an experimental study of a deep convolution neural
network architecture which can classify different handwritten digits using
EBLearn library are reported. The purpose of this neural network is to classify
input images into 10 different classes or digits (0-9) and to explore new
findings. The input dataset used consists of digits images of size 32X32 in
grayscale (MNIST dataset).
</summary>
    <author>
      <name>Karim M. Mahmoud</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author due to some errors and
  incomplete study</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.3782v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.3782v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.3425v1</id>
    <updated>2014-11-13T02:43:12Z</updated>
    <published>2014-11-13T02:43:12Z</published>
    <title>Novel LDPC Decoder via MLP Neural Networks</title>
    <summary>  In this paper, a new method for decoding Low Density Parity Check (LDPC)
codes, based on Multi-Layer Perceptron (MLP) neural networks is proposed. Due
to the fact that in neural networks all procedures are processed in parallel,
this method can be considered as a viable alternative to Message Passing
Algorithm (MPA), with high computational complexity. Our proposed algorithm
runs with soft criterion and concurrently does not use probabilistic quantities
to decide what the estimated codeword is. Although the neural decoder
performance is close to the error performance of Sum Product Algorithm (SPA),
it is comparatively less complex. Therefore, the proposed decoder emerges as a
new infrastructure for decoding LDPC codes.
</summary>
    <author>
      <name>Alireza Karami</name>
    </author>
    <author>
      <name>Mahmoud Ahmadian Attari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.3425v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.3425v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.06962v1</id>
    <updated>2015-03-24T09:34:51Z</updated>
    <published>2015-03-24T09:34:51Z</published>
    <title>Probabilistic Binary-Mask Cocktail-Party Source Separation in a
  Convolutional Deep Neural Network</title>
    <summary>  Separation of competing speech is a key challenge in signal processing and a
feat routinely performed by the human auditory brain. A long standing benchmark
of the spectrogram approach to source separation is known as the ideal binary
mask. Here, we train a convolutional deep neural network, on a two-speaker
cocktail party problem, to make probabilistic predictions about binary masks.
Our results approach ideal binary mask performance, illustrating that
relatively simple deep neural networks are capable of robust binary mask
prediction. We also illustrate the trade-off between prediction statistics and
separation quality.
</summary>
    <author>
      <name>Andrew J. R. Simpson</name>
    </author>
    <link href="http://arxiv.org/abs/1503.06962v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.06962v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.0365v1</id>
    <updated>2011-03-02T08:48:21Z</updated>
    <published>2011-03-02T08:48:21Z</published>
    <title>Diagonal Based Feature Extraction for Handwritten Alphabets Recognition
  System using Neural Network</title>
    <summary>  An off-line handwritten alphabetical character recognition system using
multilayer feed forward neural network is described in the paper. A new method,
called, diagonal based feature extraction is introduced for extracting the
features of the handwritten alphabets. Fifty data sets, each containing 26
alphabets written by various people, are used for training the neural network
and 570 different handwritten alphabetical characters are used for testing. The
proposed recognition system performs quite well yielding higher levels of
recognition accuracy compared to the systems employing the conventional
horizontal and vertical methods of feature extraction. This system will be
suitable for converting handwritten documents into structural text form and
recognizing handwritten names.
</summary>
    <author>
      <name>J. Pradeep</name>
    </author>
    <author>
      <name>E. Srinivasan</name>
    </author>
    <author>
      <name>S. Himavathi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcsit.2011.3103</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcsit.2011.3103" rel="related"/>
    <link href="http://arxiv.org/abs/1103.0365v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.0365v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.2741v1</id>
    <updated>2011-03-14T18:58:59Z</updated>
    <published>2011-03-14T18:58:59Z</published>
    <title>Memory Retrieval in the B-Matrix Neural Network</title>
    <summary>  This paper is an extension to the memory retrieval procedure of the B-Matrix
approach [6],[17] to neural network learning. The B-Matrix is a part of the
interconnection matrix generated from the Hebbian neural network, and in memory
retrieval, the B-matrix is clamped with a small fragment of the memory. The
fragment gradually enlarges by means of feedback, until the entire vector is
obtained. In this paper, we propose the use of delta learning to enhance the
retrieval rate of the stored memories.
</summary>
    <author>
      <name>Prerana Laddha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 Pages, 4 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1103.2741v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.2741v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.3968v1</id>
    <updated>2012-04-18T03:48:38Z</updated>
    <published>2012-04-18T03:48:38Z</published>
    <title>Convolutional Neural Networks Applied to House Numbers Digit
  Classification</title>
    <summary>  We classify digits of real-world house numbers using convolutional neural
networks (ConvNets). ConvNets are hierarchical feature learning neural networks
whose structure is biologically inspired. Unlike many popular vision approaches
that are hand-designed, ConvNets can automatically learn a unique set of
features optimized for a given task. We augmented the traditional ConvNet
architecture by learning multi-stage features and by using Lp pooling and
establish a new state-of-the-art of 94.85% accuracy on the SVHN dataset (45.2%
error improvement). Furthermore, we analyze the benefits of different pooling
methods and multi-stage features in ConvNets. The source code and a tutorial
are available at eblearn.sf.net.
</summary>
    <author>
      <name>Pierre Sermanet</name>
    </author>
    <author>
      <name>Soumith Chintala</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 6 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.3968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.3968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.6511v1</id>
    <updated>2012-10-24T12:37:53Z</updated>
    <published>2012-10-24T12:37:53Z</published>
    <title>Neural Networks for Complex Data</title>
    <summary>  Artificial neural networks are simple and efficient machine learning tools.
Defined originally in the traditional setting of simple vector data, neural
network models have evolved to address more and more difficulties of complex
real world problems, ranging from time evolving data to sophisticated data
structures such as graphs and functions. This paper summarizes advances on
those themes from the last decade, with a focus on results obtained by members
of the SAMM team of Universit\'e Paris 1
</summary>
    <author>
      <name>Marie Cottrell</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAMM</arxiv:affiliation>
    </author>
    <author>
      <name>Madalina Olteanu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAMM</arxiv:affiliation>
    </author>
    <author>
      <name>Fabrice Rossi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAMM</arxiv:affiliation>
    </author>
    <author>
      <name>Joseph Rynkiewicz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAMM</arxiv:affiliation>
    </author>
    <author>
      <name>Nathalie Villa-Vialaneix</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAMM</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s13218-012-0207-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s13218-012-0207-2" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">K\"unstliche Intelligenz 26, 4 (2012) 373-380</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1210.6511v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.6511v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.6461v3</id>
    <updated>2014-02-19T20:02:05Z</updated>
    <published>2013-12-23T03:23:04Z</published>
    <title>Nonparametric Weight Initialization of Neural Networks via Integral
  Representation</title>
    <summary>  A new initialization method for hidden parameters in a neural network is
proposed. Derived from the integral representation of the neural network, a
nonparametric probability distribution of hidden parameters is introduced. In
this proposal, hidden parameters are initialized by samples drawn from this
distribution, and output parameters are fitted by ordinary linear regression.
Numerical experiments show that backpropagation with proposed initialization
converges faster than uniformly random initialization. Also it is shown that
the proposed method achieves enough accuracy by itself without backpropagation
in some cases.
</summary>
    <author>
      <name>Sho Sonoda</name>
    </author>
    <author>
      <name>Noboru Murata</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">For ICLR2014, revised into 9 pages; revised into 12 pages (with
  supplements)</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.6461v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.6461v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.2329v5</id>
    <updated>2015-02-19T14:46:00Z</updated>
    <published>2014-09-08T13:08:00Z</published>
    <title>Recurrent Neural Network Regularization</title>
    <summary>  We present a simple regularization technique for Recurrent Neural Networks
(RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful
technique for regularizing neural networks, does not work well with RNNs and
LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show
that it substantially reduces overfitting on a variety of tasks. These tasks
include language modeling, speech recognition, image caption generation, and
machine translation.
</summary>
    <author>
      <name>Wojciech Zaremba</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <link href="http://arxiv.org/abs/1409.2329v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.2329v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.8341v1</id>
    <updated>2014-12-29T13:47:06Z</updated>
    <published>2014-12-29T13:47:06Z</published>
    <title>Spectral classification using convolutional neural networks</title>
    <summary>  There is a great need for accurate and autonomous spectral classification
methods in astrophysics. This thesis is about training a convolutional neural
network (ConvNet) to recognize an object class (quasar, star or galaxy) from
one-dimension spectra only. Author developed several scripts and C programs for
datasets preparation, preprocessing and postprocessing of the data. EBLearn
library (developed by Pierre Sermanet and Yann LeCun) was used to create
ConvNets. Application on dataset of more than 60000 spectra yielded success
rate of nearly 95%. This thesis conclusively proved great potential of
convolutional neural networks and deep learning methods in astrophysics.
</summary>
    <author>
      <name>Pavel H√°la</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">71 pages, 50 figures, Master's thesis, Masaryk University</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.8341v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.8341v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.01482v1</id>
    <updated>2015-04-07T06:12:14Z</updated>
    <published>2015-04-07T06:12:14Z</published>
    <title>Deep Recurrent Neural Networks for Acoustic Modelling</title>
    <summary>  We present a novel deep Recurrent Neural Network (RNN) model for acoustic
modelling in Automatic Speech Recognition (ASR). We term our contribution as a
TC-DNN-BLSTM-DNN model, the model combines a Deep Neural Network (DNN) with
Time Convolution (TC), followed by a Bidirectional Long Short-Term Memory
(BLSTM), and a final DNN. The first DNN acts as a feature processor to our
model, the BLSTM then generates a context from the sequence acoustic signal,
and the final DNN takes the context and models the posterior probabilities of
the acoustic states. We achieve a 3.47 WER on the Wall Street Journal (WSJ)
eval92 task or more than 8% relative improvement over the baseline DNN models.
</summary>
    <author>
      <name>William Chan</name>
    </author>
    <author>
      <name>Ian Lane</name>
    </author>
    <link href="http://arxiv.org/abs/1504.01482v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.01482v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.04116v1</id>
    <updated>2015-07-15T08:19:14Z</updated>
    <published>2015-07-15T08:19:14Z</published>
    <title>Language discrimination and clustering via a neural network approach</title>
    <summary>  We classify twenty-one Indo-European languages starting from written text. We
use neural networks in order to define a distance among different languages,
construct a dendrogram and analyze the ultrametric structure that emerges. Four
or five subgroups of languages are identified, according to the "cut" of the
dendrogram, drawn with an entropic criterion. The results and the method are
discussed.
</summary>
    <author>
      <name>Angelo Mariano</name>
    </author>
    <author>
      <name>Giorgio Parisi</name>
    </author>
    <author>
      <name>Saverio Pascazio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.04116v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.04116v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.05520v1</id>
    <updated>2015-11-17T19:43:53Z</updated>
    <published>2015-11-17T19:43:53Z</published>
    <title>Automatic Instrument Recognition in Polyphonic Music Using Convolutional
  Neural Networks</title>
    <summary>  Traditional methods to tackle many music information retrieval tasks
typically follow a two-step architecture: feature engineering followed by a
simple learning algorithm. In these "shallow" architectures, feature
engineering and learning are typically disjoint and unrelated. Additionally,
feature engineering is difficult, and typically depends on extensive domain
expertise.
  In this paper, we present an application of convolutional neural networks for
the task of automatic musical instrument identification. In this model, feature
extraction and learning algorithms are trained together in an end-to-end
fashion. We show that a convolutional neural network trained on raw audio can
achieve performance surpassing traditional methods that rely on hand-crafted
features.
</summary>
    <author>
      <name>Peter Li</name>
    </author>
    <author>
      <name>Jiyuan Qian</name>
    </author>
    <author>
      <name>Tian Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1511.05520v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.05520v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.01322v3</id>
    <updated>2016-09-27T11:42:34Z</updated>
    <published>2015-12-04T06:07:28Z</published>
    <title>Fixed-Point Performance Analysis of Recurrent Neural Networks</title>
    <summary>  Recurrent neural networks have shown excellent performance in many
applications, however they require increased complexity in hardware or software
based implementations. The hardware complexity can be much lowered by
minimizing the word-length of weights and signals. This work analyzes the
fixed-point performance of recurrent neural networks using a retrain based
quantization method. The quantization sensitivity of each layer in RNNs is
studied, and the overall fixed-point optimization results minimizing the
capacity of weights while not sacrificing the performance are presented. A
language model and a phoneme recognition examples are used.
</summary>
    <author>
      <name>Sungho Shin</name>
    </author>
    <author>
      <name>Kyuyeon Hwang</name>
    </author>
    <author>
      <name>Wonyong Sung</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MSP.2015.2411564</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MSP.2015.2411564" rel="related"/>
    <link href="http://arxiv.org/abs/1512.01322v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.01322v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.01712v1</id>
    <updated>2015-12-05T23:41:22Z</updated>
    <published>2015-12-05T23:41:22Z</published>
    <title>Generating News Headlines with Recurrent Neural Networks</title>
    <summary>  We describe an application of an encoder-decoder recurrent neural network
with LSTM units and attention to generating headlines from the text of news
articles. We find that the model is quite effective at concisely paraphrasing
news articles. Furthermore, we study how the neural network decides which input
words to pay attention to, and specifically we identify the function of the
different neurons in a simplified attention mechanism. Interestingly, our
simplified attention mechanism performs better that the more complex attention
mechanism on a held out set of articles.
</summary>
    <author>
      <name>Konstantin Lopyrev</name>
    </author>
    <link href="http://arxiv.org/abs/1512.01712v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.01712v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06042v2</id>
    <updated>2016-06-08T13:43:30Z</updated>
    <published>2016-03-19T03:56:03Z</published>
    <title>Globally Normalized Transition-Based Neural Networks</title>
    <summary>  We introduce a globally normalized transition-based neural network model that
achieves state-of-the-art part-of-speech tagging, dependency parsing and
sentence compression results. Our model is a simple feed-forward neural network
that operates on a task-specific transition system, yet achieves comparable or
better accuracies than recurrent models. We discuss the importance of global as
opposed to local normalization: a key insight is that the label bias problem
implies that globally normalized models can be strictly more expressive than
locally normalized models.
</summary>
    <author>
      <name>Daniel Andor</name>
    </author>
    <author>
      <name>Chris Alberti</name>
    </author>
    <author>
      <name>David Weiss</name>
    </author>
    <author>
      <name>Aliaksei Severyn</name>
    </author>
    <author>
      <name>Alessandro Presta</name>
    </author>
    <author>
      <name>Kuzman Ganchev</name>
    </author>
    <author>
      <name>Slav Petrov</name>
    </author>
    <author>
      <name>Michael Collins</name>
    </author>
    <link href="http://arxiv.org/abs/1603.06042v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06042v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09381v1</id>
    <updated>2016-03-30T20:57:07Z</updated>
    <published>2016-03-30T20:57:07Z</published>
    <title>Clinical Information Extraction via Convolutional Neural Network</title>
    <summary>  We report an implementation of a clinical information extraction tool that
leverages deep neural network to annotate event spans and their attributes from
raw clinical notes and pathology reports. Our approach uses context words and
their part-of-speech tags and shape information as features. Then we hire
temporal (1D) convolutional neural network to learn hidden feature
representations. Finally, we use Multilayer Perceptron (MLP) to predict event
spans. The empirical evaluation demonstrates that our approach significantly
outperforms baselines.
</summary>
    <author>
      <name>Peng Li</name>
    </author>
    <author>
      <name>Heng Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1408.5882 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.09381v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09381v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00462v1</id>
    <updated>2016-04-02T05:04:12Z</updated>
    <published>2016-04-02T05:04:12Z</published>
    <title>Centralized and Decentralized Global Outer-synchronization of Asymmetric
  Recurrent Time-varying Neural Network by Data-sampling</title>
    <summary>  In this paper, we discuss the outer-synchronization of the asymmetrically
connected recurrent time-varying neural networks. By both centralized and
decentralized discretization data sampling principles, we derive several
sufficient conditions based on diverse vector norms that guarantee that any two
trajectories from different initial values of the identical neural network
system converge together. The lower bounds of the common time intervals between
data samples in centralized and decentralized principles are proved to be
positive, which guarantees exclusion of Zeno behavior. A numerical example is
provided to illustrate the efficiency of the theoretical results.
</summary>
    <author>
      <name>Wenlian Lu</name>
    </author>
    <author>
      <name>Ren Zheng</name>
    </author>
    <author>
      <name>Tianping Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.00462v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00462v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07143v1</id>
    <updated>2016-04-25T06:43:47Z</updated>
    <published>2016-04-25T06:43:47Z</published>
    <title>Neural Random Forests</title>
    <summary>  Given an ensemble of randomized regression trees, it is possible to
restructure them as a collection of multilayered neural networks with
particular connection weights. Following this principle, we reformulate the
random forest method of Breiman (2001) into a neural network setting, and in
turn propose two new hybrid procedures that we call neural random forests. Both
predictors exploit prior knowledge of regression trees for their architecture,
have less parameters to tune than standard networks, and less restrictions on
the geometry of the decision boundaries. Consistency results are proved, and
substantial numerical evidence is provided on both synthetic and real data sets
to assess the excellent performance of our methods in a large variety of
prediction problems.
</summary>
    <author>
      <name>G√©rard Biau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LPMA, LSTA</arxiv:affiliation>
    </author>
    <author>
      <name>Erwan Scornet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LSTA</arxiv:affiliation>
    </author>
    <author>
      <name>Johannes Welbl</name>
    </author>
    <link href="http://arxiv.org/abs/1604.07143v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07143v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07269v1</id>
    <updated>2016-04-25T14:17:08Z</updated>
    <published>2016-04-25T14:17:08Z</published>
    <title>CMA-ES for Hyperparameter Optimization of Deep Neural Networks</title>
    <summary>  Hyperparameters of deep neural networks are often optimized by grid search,
random search or Bayesian optimization. As an alternative, we propose to use
the Covariance Matrix Adaptation Evolution Strategy (CMA-ES), which is known
for its state-of-the-art performance in derivative-free optimization. CMA-ES
has some useful invariance properties and is friendly to parallel evaluations
of solutions. We provide a toy example comparing CMA-ES and state-of-the-art
Bayesian optimization algorithms for tuning the hyperparameters of a
convolutional neural network for the MNIST dataset on 30 GPUs in parallel.
</summary>
    <author>
      <name>Ilya Loshchilov</name>
    </author>
    <author>
      <name>Frank Hutter</name>
    </author>
    <link href="http://arxiv.org/abs/1604.07269v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07269v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.03764v1</id>
    <updated>2016-05-12T11:39:14Z</updated>
    <published>2016-05-12T11:39:14Z</published>
    <title>Direct Method for Training Feed-forward Neural Networks using Batch
  Extended Kalman Filter for Multi-Step-Ahead Predictions</title>
    <summary>  This paper is dedicated to the long-term, or multi-step-ahead, time series
prediction problem. We propose a novel method for training feed-forward neural
networks, such as multilayer perceptrons, with tapped delay lines. Special
batch calculation of derivatives called Forecasted Propagation Through Time and
batch modification of the Extended Kalman Filter are introduced. Experiments
were carried out on well-known time series benchmarks, the Mackey-Glass chaotic
process and the Santa Fe Laser Data Series. Recurrent and feed-forward neural
networks were evaluated.
</summary>
    <author>
      <name>Artem Chernodub</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of ICANN'2013-LCNS Series, Volume 8131.
  Springer-Verlag New York, Inc., 2013, P. 138-145</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1605.03764v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.03764v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.00781v2</id>
    <updated>2017-02-26T22:01:12Z</updated>
    <published>2016-08-02T11:57:09Z</published>
    <title>Horn: A System for Parallel Training and Regularizing of Large-Scale
  Neural Networks</title>
    <summary>  I introduce a new distributed system for effective training and regularizing
of Large-Scale Neural Networks on distributed computing architectures. The
experiments demonstrate the effectiveness of flexible model partitioning and
parallelization strategies based on neuron-centric computation model, with an
implementation of the collective and parallel dropout neural networks training.
Experiments are performed on MNIST handwritten digits classification including
results.
</summary>
    <author>
      <name>Edward J. Yoon</name>
    </author>
    <link href="http://arxiv.org/abs/1608.00781v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00781v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04426v4</id>
    <updated>2017-02-17T02:49:12Z</updated>
    <published>2016-08-15T22:28:05Z</published>
    <title>Regularization for Unsupervised Deep Neural Nets</title>
    <summary>  Unsupervised neural networks, such as restricted Boltzmann machines (RBMs)
and deep belief networks (DBNs), are powerful tools for feature selection and
pattern recognition tasks. We demonstrate that overfitting occurs in such
models just as in deep feedforward neural networks, and discuss possible
regularization methods to reduce overfitting. We also propose a "partial"
approach to improve the efficiency of Dropout/DropConnect in this scenario, and
discuss the theoretical justification of these methods from model convergence
and likelihood bounds. Finally, we compare the performance of these methods
based on their likelihood and classification error rates for various pattern
recognition data sets.
</summary>
    <author>
      <name>Baiyang Wang</name>
    </author>
    <author>
      <name>Diego Klabjan</name>
    </author>
    <link href="http://arxiv.org/abs/1608.04426v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04426v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.04238v1</id>
    <updated>2016-10-13T20:00:08Z</updated>
    <published>2016-10-13T20:00:08Z</published>
    <title>A Neural Decoder for Topological Codes</title>
    <summary>  We present an algorithm for error correction in topological codes that
exploits modern machine learning techniques. Our decoder is constructed from a
stochastic neural network called a Boltzmann machine, of the type extensively
used in deep learning. We provide a general prescription for the training of
the network and a decoding strategy that is applicable to a wide variety of
stabilizer codes with very little specialization. We demonstrate the neural
decoder numerically on the well-known two dimensional toric code with
phase-flip errors.
</summary>
    <author>
      <name>Giacomo Torlai</name>
    </author>
    <author>
      <name>Roger G. Melko</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.119.030501</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.119.030501" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. Lett. 119, 030501 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.04238v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.04238v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.01678v1</id>
    <updated>2016-11-05T17:17:14Z</updated>
    <published>2016-11-05T17:17:14Z</published>
    <title>Comparing learning algorithms in neural network for diagnosing
  cardiovascular disease</title>
    <summary>  Today data mining techniques are exploited in medical science for diagnosing,
overcoming and treating diseases. Neural network is one of the techniques which
are widely used for diagnosis in medical field. In this article efficiency of
nine algorithms, which are basis of neural network learning in diagnosing
cardiovascular diseases, will be assessed. Algorithms are assessed in terms of
accuracy, sensitivity, transparency, AROC and convergence rate by means of 10
fold cross validation. The results suggest that in training phase, Lonberg-M
algorithm has the best efficiency in terms of all metrics, algorithm OSS has
maximum accuracy in testing phase, algorithm SCG has the maximum transparency
and algorithm CGB has the maximum sensitivity.
</summary>
    <author>
      <name>Mirmorsal Madani</name>
    </author>
    <link href="http://arxiv.org/abs/1611.01678v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.01678v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.03441v1</id>
    <updated>2017-01-12T18:12:05Z</updated>
    <published>2017-01-12T18:12:05Z</published>
    <title>Simplified Gating in Long Short-term Memory (LSTM) Recurrent Neural
  Networks</title>
    <summary>  The standard LSTM recurrent neural networks while very powerful in long-range
dependency sequence applications have highly complex structure and relatively
large (adaptive) parameters. In this work, we present empirical comparison
between the standard LSTM recurrent neural network architecture and three new
parameter-reduced variants obtained by eliminating combinations of the input
signal, bias, and hidden unit signals from individual gating signals. The
experiments on two sequence datasets show that the three new variants, called
simply as LSTM1, LSTM2, and LSTM3, can achieve comparable performance to the
standard LSTM model with less (adaptive) parameters.
</summary>
    <author>
      <name>Yuzhen Lu</name>
    </author>
    <author>
      <name>Fathi M. Salem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 Figures, 3 Tables. arXiv admin note: substantial text
  overlap with arXiv:1612.03707</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.03441v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.03441v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.01523v1</id>
    <updated>2017-04-05T16:54:20Z</updated>
    <published>2017-04-05T16:54:20Z</published>
    <title>MIT at SemEval-2017 Task 10: Relation Extraction with Convolutional
  Neural Networks</title>
    <summary>  Over 50 million scholarly articles have been published: they constitute a
unique repository of knowledge. In particular, one may infer from them
relations between scientific concepts, such as synonyms and hyponyms.
Artificial neural networks have been recently explored for relation extraction.
In this work, we continue this line of work and present a system based on a
convolutional neural network to extract relations. Our model ranked first in
the SemEval-2017 task 10 (ScienceIE) for relation extraction in scientific
articles (subtask C).
</summary>
    <author>
      <name>Ji Young Lee</name>
    </author>
    <author>
      <name>Franck Dernoncourt</name>
    </author>
    <author>
      <name>Peter Szolovits</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at SemEval 2017. The first two authors contributed equally
  to this work</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.01523v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.01523v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.08557v1</id>
    <updated>2017-05-23T23:17:49Z</updated>
    <published>2017-05-23T23:17:49Z</published>
    <title>Grounded Recurrent Neural Networks</title>
    <summary>  In this work, we present the Grounded Recurrent Neural Network (GRNN), a
recurrent neural network architecture for multi-label prediction which
explicitly ties labels to specific dimensions of the recurrent hidden state (we
call this process "grounding"). The approach is particularly well-suited for
extracting large numbers of concepts from text. We apply the new model to
address an important problem in healthcare of understanding what medical
concepts are discussed in clinical text. Using a publicly available dataset
derived from Intensive Care Units, we learn to label a patient's diagnoses and
procedures from their discharge summary. Our evaluation shows a clear advantage
to using our proposed architecture over a variety of strong baselines.
</summary>
    <author>
      <name>Ankit Vani</name>
    </author>
    <author>
      <name>Yacine Jernite</name>
    </author>
    <author>
      <name>David Sontag</name>
    </author>
    <link href="http://arxiv.org/abs/1705.08557v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.08557v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00081v1</id>
    <updated>2017-07-01T01:30:21Z</updated>
    <published>2017-07-01T01:30:21Z</published>
    <title>Synthesizing Deep Neural Network Architectures using Biological Synaptic
  Strength Distributions</title>
    <summary>  In this work, we perform an exploratory study on synthesizing deep neural
networks using biological synaptic strength distributions, and the potential
influence of different distributions on modelling performance particularly for
the scenario associated with small data sets. Surprisingly, a CNN with
convolutional layer synaptic strengths drawn from biologically-inspired
distributions such as log-normal or correlated center-surround distributions
performed relatively well suggesting a possibility for designing deep neural
network architectures that do not require many data samples to learn, and can
sidestep current training procedures while maintaining or boosting modelling
performance.
</summary>
    <author>
      <name>A. H. Karimi</name>
    </author>
    <author>
      <name>M. J. Shafiee</name>
    </author>
    <author>
      <name>A. Ghodsi</name>
    </author>
    <author>
      <name>A. Wong</name>
    </author>
    <link href="http://arxiv.org/abs/1707.00081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.04988v1</id>
    <updated>2017-08-16T17:27:40Z</updated>
    <published>2017-08-16T17:27:40Z</published>
    <title>Warp: a method for neural network interpretability applied to gene
  expression profiles</title>
    <summary>  We show a proof of principle for warping, a method to interpret the inner
working of neural networks in the context of gene expression analysis. Warping
is an efficient way to gain insight to the inner workings of neural nets and
make them more interpretable. We demonstrate the ability of warping to recover
meaningful information for a given class on a samplespecific individual basis.
We found warping works well in both linearly and nonlinearly separable
datasets. These encouraging results show that warping has a potential to be the
answer to neural networks interpretability in computational biology.
</summary>
    <author>
      <name>Trofimov Assya</name>
    </author>
    <author>
      <name>Lemieux Sebastien</name>
    </author>
    <author>
      <name>Perreault Claude</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures, NIPS2016, Machine Learning in Computational
  Biology workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.04988v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.04988v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.05954v1</id>
    <updated>2017-12-16T13:14:03Z</updated>
    <published>2017-12-16T13:14:03Z</published>
    <title>An Artificial Neural Network Architecture Based on Context
  Transformations in Cortical Minicolumns</title>
    <summary>  Cortical minicolumns are considered a model of cortical organization. Their
function is still a source of research and not reflected properly in modern
architecture of nets in algorithms of Artificial Intelligence. We assume its
function and describe it in this article. Furthermore, we show how this
proposal allows to construct a new architecture, that is not based on
convolutional neural networks, test it on MNIST data and receive close to
Convolutional Neural Network accuracy. We also show that the proposed
architecture possesses an ability to train on a small quantity of samples. To
achieve these results, we enable the minicolumns to remember context
transformations.
</summary>
    <author>
      <name>Vasily Morzhakov</name>
    </author>
    <author>
      <name>Alexey Redozubov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.05954v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.05954v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0304469v1</id>
    <updated>2003-04-21T22:55:07Z</updated>
    <published>2003-04-21T22:55:07Z</published>
    <title>Using Recurrent Neural Networks To Forecasting of Forex</title>
    <summary>  This paper reports empirical evidence that a neural networks model is
applicable to the statistically reliable prediction of foreign exchange rates.
Time series data and technical indicators such as moving average, are fed to
neural nets to capture the underlying "rules" of the movement in currency
exchange rates. The trained recurrent neural networks forecast the exchange
rates between American Dollar and four other major currencies, Japanese Yen,
Swiss Frank, British Pound and EURO. Various statistical estimates of forecast
quality have been carried out. Obtained results show, that neural networks are
able to give forecast with coefficient of multiple determination not worse then
0.65. Linear and nonlinear statistical data preprocessing, such as
Kolmogorov-Smirnov test and Hurst exponents for each currency were calculated
and analyzed.
</summary>
    <author>
      <name>V. V. Kondratenko</name>
    </author>
    <author>
      <name>Yu. A Kuperin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0304469v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0304469v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504057v1</id>
    <updated>2005-04-13T14:03:02Z</updated>
    <published>2005-04-13T14:03:02Z</published>
    <title>Diagnostic Rule Extraction Using Neural Networks</title>
    <summary>  The neural networks have trained on incomplete sets that a doctor could
collect. Trained neural networks have correctly classified all the presented
instances. The number of intervals entered for encoding the quantitative
variables is equal two. The number of features as well as the number of neurons
and layers in trained neural networks was minimal. Trained neural networks are
adequately represented as a set of logical formulas that more comprehensible
and easy-to-understand. These formulas are as the syndrome-complexes, which may
be easily tabulated and represented as a diagnostic table that the doctors
usually use. Decision rules provide the evaluations of their confidence in
which interested a doctor. Conducted clinical researches have shown that
iagnostic decisions produced by symbolic rules have coincided with the doctor's
conclusions.
</summary>
    <author>
      <name>Vitaly Schetinin</name>
    </author>
    <author>
      <name>Anatoly Brazhnikov</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0504057v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504057v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505021v3</id>
    <updated>2007-06-08T09:20:49Z</updated>
    <published>2005-05-10T11:36:35Z</published>
    <title>Distant generalization by feedforward neural networks</title>
    <summary>  This paper discusses the notion of generalization of training samples over
long distances in the input space of a feedforward neural network. Such a
generalization might occur in various ways, that differ in how great the
contribution of different training features should be.
  The structure of a neuron in a feedforward neural network is analyzed and it
is concluded, that the actual performance of the discussed generalization in
such neural networks may be problematic -- while such neural networks might be
capable for such a distant generalization, a random and spurious generalization
may occur as well.
  To illustrate the differences in generalizing of the same function by
different learning machines, results given by the support vector machines are
also presented.
</summary>
    <author>
      <name>Artur Rataj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505021v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505021v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.1457v2</id>
    <updated>2010-03-14T10:07:16Z</updated>
    <published>2010-03-07T12:05:22Z</published>
    <title>The Comparison of Methods Artificial Neural Network with Linear
  Regression Using Specific Variables for Prediction Stock Price in Tehran
  Stock Exchange</title>
    <summary>  In this paper, researchers estimated the stock price of activated companies
in Tehran (Iran) stock exchange. It is used Linear Regression and Artificial
Neural Network methods and compared these two methods. In Artificial Neural
Network, of General Regression Neural Network method (GRNN) for architecture is
used. In this paper, first, researchers considered 10 macro economic variables
and 30 financial variables and then they obtained seven final variables
including 3 macro economic variables and 4 financial variables to estimate the
stock price using Independent components Analysis (ICA). So, we presented an
equation for two methods and compared their results which shown that artificial
neural network method is more efficient than linear regression method.
</summary>
    <author>
      <name>Reza Gharoie Ahangar</name>
    </author>
    <author>
      <name>Mahmood Yahyazadehfar</name>
    </author>
    <author>
      <name>Hassan Pournaghshband</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS February 2010, ISSN 1947 5500,
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 7, No. 2, pp. 038-046, February 2010, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.1457v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.1457v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.5016v1</id>
    <updated>2010-07-28T15:58:03Z</updated>
    <published>2010-07-28T15:58:03Z</published>
    <title>Percolation Approach to Study Connectivity in Living Neural Networks</title>
    <summary>  We study neural connectivity in cultures of rat hippocampal neurons. We
measure the neurons' response to an electric stimulation for gradual lower
connectivity, and characterize the size of the giant cluster in the network.
The connectivity undergoes a percolation transition described by the critical
exponent $\beta \simeq 0.65$. We use a theoretic approach based on
bond.percolation on a graph to describe the process of disintegration of the
network and extract its statistical properties. Together with numerical
simulations we show that the connectivity in the neural culture is local,
characterized by a gaussian degree distribution and not a power law on
</summary>
    <author>
      <name>Jordi Soriano</name>
    </author>
    <author>
      <name>Ilan Breskin</name>
    </author>
    <author>
      <name>Elisha Moses</name>
    </author>
    <author>
      <name>Tsvi Tlusty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: neural networks, graphs, connectivity, percolation, giant
  component PACS: 87.18.Sn, 87.19.La, 64.60.Ak
  http://www.weizmann.ac.il/complex/tlusty/papers/AIP2006.pdf</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">9th Granada Seminar 2006 - Cooperative Behavior in Neural Systems
  AIP, eds Garrido PL, Marro J, &amp; Torres JJ (AIP, Granada, SPAIN), Vol 887, pp
  96-106</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1007.5016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.5016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.4564v1</id>
    <updated>2010-09-23T10:25:04Z</updated>
    <published>2010-09-23T10:25:04Z</published>
    <title>A Constructive Algorithm for Feedforward Neural Networks for Medical
  Diagnostic Reasoning</title>
    <summary>  This research is to search for alternatives to the resolution of complex
medical diagnosis where human knowledge should be apprehended in a general
fashion. Successful application examples show that human diagnostic
capabilities are significantly worse than the neural diagnostic system. Our
research describes a constructive neural network algorithm with
backpropagation; offer an approach for the incremental construction of
nearminimal neural network architectures for pattern classification. The
algorithm starts with minimal number of hidden units in the single hidden
layer; additional units are added to the hidden layer one at a time to improve
the accuracy of the network and to get an optimal size of a neural network. Our
algorithm was tested on several benchmarking classification problems including
Cancer1, Heart, and Diabetes with good generalization ability.
</summary>
    <author>
      <name>Abu Bakar Siddiquee</name>
    </author>
    <author>
      <name>Md. Ehsanul Hoque Mazumder</name>
    </author>
    <author>
      <name>S. M. Kamruzzaman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 Pages, International Symposium</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. MMU International Symposium on Information and
  Communications Technology (M2USIC 2004), Kuala Lumpur, Malaysia, pp. TS4B2:
  5-8, Oct. 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.4564v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.4564v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.1074v1</id>
    <updated>2011-09-06T06:05:12Z</updated>
    <published>2011-09-06T06:05:12Z</published>
    <title>A Framework for Predicting Phishing Websites using Neural Networks</title>
    <summary>  In India many people are now dependent on online banking. This raises
security concerns as the banking websites are forged and fraud can be committed
by identity theft. These forged websites are called as Phishing websites and
created by malicious people to mimic web pages of real websites and it attempts
to defraud people of their personal information. Detecting and identifying
phishing websites is a really complex and dynamic problem involving many
factors and criteria. This paper discusses about the prediction of phishing
websites using neural networks. A neural network is a multilayer system which
reduces the error and increases the performance. This paper describes a
framework to better classify and predict the phishing sites using neural
networks.
</summary>
    <author>
      <name>A. Martin</name>
    </author>
    <author>
      <name>Na. Ba. Anutthamaa</name>
    </author>
    <author>
      <name>M. Sathyavathy</name>
    </author>
    <author>
      <name>Marie Manjari Saint Francois</name>
    </author>
    <author>
      <name>Dr. V. Prasanna Venkatesan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Phishing, Neural Networks, Classification,Learning, Phishing
  Detection</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 2, 2011, 330-336</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1109.1074v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.1074v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.5892v2</id>
    <updated>2012-01-30T19:48:31Z</updated>
    <published>2011-11-25T05:11:14Z</published>
    <title>Evolving Chart Pattern Sensitive Neural Network Based Forex Trading
  Agents</title>
    <summary>  Though machine learning has been applied to the foreign exchange market for
algorithmic trading for quiet some time now, and neural networks(NN) have been
shown to yield positive results, in most modern approaches the NN systems are
optimized through traditional methods like the backpropagation algorithm for
example, and their input signals are price lists, and lists composed of other
technical indicator elements. The aim of this paper is twofold: the
presentation and testing of the application of topology and weight evolving
artificial neural network (TWEANN) systems to automated currency trading, and
to demonstrate the performance when using Forex chart images as input to
geometrical regularity aware indirectly encoded neural network systems,
enabling them to use the patterns &amp; trends within, when trading. This paper
presents the benchmark results of NN based automated currency trading systems
evolved using TWEANNs, and compares the performance and generalization
capabilities of these direct encoded NNs which use the standard sliding-window
based price vector inputs, and the indirect (substrate) encoded NNs which use
charts as input. The TWEANN algorithm I will use in this paper to evolve these
currency trading agents is the memetic algorithm based TWEANN system called
Deus Ex Neural Network (DXNN) platform.
</summary>
    <author>
      <name>Gene I. Sher</name>
    </author>
    <link href="http://arxiv.org/abs/1111.5892v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.5892v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.5651v2</id>
    <updated>2012-07-14T17:47:48Z</updated>
    <published>2012-06-25T11:30:44Z</published>
    <title>Optimization of Real, Hermitian Quadratic Forms: Real, Complex
  Hopfield-Amari Neural Network</title>
    <summary>  In this research paper, the problem of optimization of quadratic forms
associated with the dynamics of Hopfield-Amari neural network is considered. An
elegant (and short) proof of the states at which local/global minima of
quadratic form are attained is provided. A theorem associated with local/global
minimization of quadratic energy function using the Hopfield-Amari neural
network is discussed. The results are generalized to a "Complex Hopfield neural
network" dynamics over the complex hypercube (using a "complex signum
function"). It is also reasoned through two theorems that there is no loss of
generality in assuming the threshold vector to be a zero vector in the case of
real as well as a "Complex Hopfield neural network". Some structured quadratic
forms like Toeplitz form and Complex Toeplitz form are discussed.
</summary>
    <author>
      <name>Garimella Ramamurthy</name>
    </author>
    <author>
      <name>Bondalapati Nischal</name>
    </author>
    <link href="http://arxiv.org/abs/1206.5651v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.5651v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.5594v1</id>
    <updated>2012-12-21T13:53:53Z</updated>
    <published>2012-12-21T13:53:53Z</published>
    <title>Black box modelling of HVAC system : improving the performances of
  neural networks</title>
    <summary>  This paper deals with neural networks modelling of HVAC systems. In order to
increase the neural networks performances, a method based on sensitivity
analysis is applied. The same technique is also used to compute the relevance
of each input. To avoid the prediction errors in dry coil conditions, a
metamodel for each capacity is derived from the neural networks. The regression
coefficients of the polynomial forms are identified through the use of spectral
analysis. These methods based on sensitivity and spectral analysis lead to an
optimized neural network model, as regard to its architecture and predictions.
</summary>
    <author>
      <name>Eric Fock</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PIMENT</arxiv:affiliation>
    </author>
    <author>
      <name>Thierry Alex Mara</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PIMENT</arxiv:affiliation>
    </author>
    <author>
      <name>Alfred Jean Philippe Lauret</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PIMENT</arxiv:affiliation>
    </author>
    <author>
      <name>Harry Boyer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PIMENT</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Eighth International IBPSA Conference, Eindhoven : Netherlands
  (2003); Proceedings available at http://www.ibpsa.org/m_bs2003.asp</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.5594v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.5594v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.6922v1</id>
    <updated>2012-12-31T16:40:50Z</updated>
    <published>2012-12-31T16:40:50Z</published>
    <title>Training a Functional Link Neural Network Using an Artificial Bee Colony
  for Solving a Classification Problems</title>
    <summary>  Artificial Neural Networks have emerged as an important tool for
classification and have been widely used to classify a non-linear separable
pattern. The most popular artificial neural networks model is a Multilayer
Perceptron (MLP) as it is able to perform classification task with significant
success. However due to the complexity of MLP structure and also problems such
as local minima trapping, over fitting and weight interference have made neural
network training difficult. Thus, the easy way to avoid these problems is to
remove the hidden layers. This paper presents the ability of Functional Link
Neural Network (FLNN) to overcome the complexity structure of MLP by using
single layer architecture and propose an Artificial Bee Colony (ABC)
optimization for training the FLNN. The proposed technique is expected to
provide better learning scheme for a classifier in order to get more accurate
classification result
</summary>
    <author>
      <name>Yana Mazwin Mohmad Hassim</name>
    </author>
    <author>
      <name>Rozaida Ghazali</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, 4 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computing, Volume 4, Issue 9 (2012), 110-115</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1212.6922v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.6922v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.4237v3</id>
    <updated>2016-09-01T00:02:46Z</updated>
    <published>2014-10-15T21:35:23Z</published>
    <title>Interference of Neural Waves in Distributed Inhibition-stabilized
  Networks</title>
    <summary>  To gain insight into the neural events responsible for visual perception of
static and dynamic optical patterns, we study how neural activation spreads in
arrays of inhibition-stabilized neural networks with nearest-neighbor coupling.
The activation generated in such networks by local stimuli propagates between
locations, forming spatiotemporal waves that affect the dynamics of activation
generated by stimuli separated spatially and temporally, and by stimuli with
complex spatiotemporal structure. These interactions form characteristic
interference patterns that make the network intrinsically selective for certain
stimuli, such as modulations of luminance at specific spatial and temporal
frequencies and specific velocities of visual motion. Due to the inherent
nonlinearity of the network, its intrinsic tuning depends on stimulus intensity
and contrast. The interference patterns have multiple features of "lateral"
interactions between stimuli, well known in physiological and behavioral
studies of visual systems. The diverse phenomena have been previously
attributed to distinct neural circuits. Our results demonstrate how the
canonical circuit can perform the diverse operations in a manner predicted by
neural-wave interference.
</summary>
    <author>
      <name>Sergey Savel'ev</name>
    </author>
    <author>
      <name>Sergei Gepshtein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.4237v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.4237v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.05272v1</id>
    <updated>2015-03-18T02:54:04Z</updated>
    <published>2015-03-18T02:54:04Z</published>
    <title>Improved Calibration of Near-Infrared Spectra by Using Ensembles of
  Neural Network Models</title>
    <summary>  IR or near-infrared (NIR) spectroscopy is a method used to identify a
compound or to analyze the composition of a material. Calibration of NIR
spectra refers to the use of the spectra as multivariate descriptors to predict
concentrations of the constituents. To build a calibration model,
state-of-the-art software predominantly uses linear regression techniques. For
nonlinear calibration problems, neural network-based models have proved to be
an interesting alternative. In this paper, we propose a novel extension of the
conventional neural network-based approach, the use of an ensemble of neural
network models. The individual neural networks are obtained by resampling the
available training data with bootstrapping or cross-validation techniques. The
results obtained for a realistic calibration example show that the
ensemble-based approach produces a significantly more accurate and robust
calibration model than conventional regression methods.
</summary>
    <author>
      <name>A. Ukil</name>
    </author>
    <author>
      <name>J. Bernasconi</name>
    </author>
    <author>
      <name>H. Braendle</name>
    </author>
    <author>
      <name>H. Buijs</name>
    </author>
    <author>
      <name>S. Bonenfant</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/JSEN.2009.2038124</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/JSEN.2009.2038124" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Sensors Journal, vol. 10, no. 3, pp. 578-584, 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1503.05272v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.05272v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.4429v1</id>
    <updated>2011-07-22T06:48:01Z</updated>
    <published>2011-07-22T06:48:01Z</published>
    <title>High Accuracy Human Activity Monitoring using Neural network</title>
    <summary>  This paper presents the designing of a neural network for the classification
of Human activity. A Triaxial accelerometer sensor, housed in a chest worn
sensor unit, has been used for capturing the acceleration of the movements
associated. All the three axis acceleration data were collected at a base
station PC via a CC2420 2.4GHz ISM band radio (zigbee wireless compliant),
processed and classified using MATLAB. A neural network approach for
classification was used with an eye on theoretical and empirical facts. The
work shows a detailed description of the designing steps for the classification
of human body acceleration data. A 4-layer back propagation neural network,
with Levenberg-marquardt algorithm for training, showed best performance among
the other neural network training algorithms.
</summary>
    <author>
      <name>Annapurna Sharma</name>
    </author>
    <author>
      <name>Young-Dong Lee</name>
    </author>
    <author>
      <name>Wan-Young Chung</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICCIT.2008.394</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICCIT.2008.394" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures, 4 Tables, International Conference on Convergence
  Information Technology, pp. 430-435, 2008 Third International Conference on
  Convergence and Hybrid Information Technology, 2008</arxiv:comment>
    <link href="http://arxiv.org/abs/1107.4429v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.4429v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.4149v1</id>
    <updated>2013-12-15T13:57:16Z</updated>
    <published>2013-12-15T13:57:16Z</published>
    <title>Autonomous Quantum Perceptron Neural Network</title>
    <summary>  Recently, with the rapid development of technology, there are a lot of
applications require to achieve low-cost learning. However the computational
power of classical artificial neural networks, they are not capable to provide
low-cost learning. In contrast, quantum neural networks may be representing a
good computational alternate to classical neural network approaches, based on
the computational power of quantum bit (qubit) over the classical bit. In this
paper we present a new computational approach to the quantum perceptron neural
network can achieve learning in low-cost computation. The proposed approach has
only one neuron can construct self-adaptive activation operators capable to
accomplish the learning process in a limited number of iterations and, thereby,
reduce the overall computational cost. The proposed approach is capable to
construct its own set of activation operators to be applied widely in both
quantum and classical applications to overcome the linearity limitation of
classical perceptron. The computational power of the proposed approach is
illustrated via solving variety of problems where promising and comparable
results are given.
</summary>
    <author>
      <name>Alaa Sagheer</name>
    </author>
    <author>
      <name>Mohammed Zidan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.4149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.4149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.1333v1</id>
    <updated>2014-01-07T10:29:24Z</updated>
    <published>2014-01-07T10:29:24Z</published>
    <title>Time series forecasting using neural networks</title>
    <summary>  Recent studies have shown the classification and prediction power of the
Neural Networks. It has been demonstrated that a NN can approximate any
continuous function. Neural networks have been successfully used for
forecasting of financial data series. The classical methods used for time
series prediction like Box-Jenkins or ARIMA assumes that there is a linear
relationship between inputs and outputs. Neural Networks have the advantage
that can approximate nonlinear functions. In this paper we compared the
performances of different feed forward and recurrent neural networks and
training algorithms for predicting the exchange rate EUR/RON and USD/RON. We
used data series with daily exchange rates starting from 2005 until 2013.
</summary>
    <author>
      <name>Bogdan Oancea</name>
    </author>
    <author>
      <name>≈ûTefan Cristian Ciucu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the CKS 2013 International Conference</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the CKS 2013 International Conference</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1401.1333v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.1333v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.3811v2</id>
    <updated>2014-07-02T14:36:28Z</updated>
    <published>2014-02-16T15:54:37Z</published>
    <title>Dropout Rademacher Complexity of Deep Neural Networks</title>
    <summary>  Great successes of deep neural networks have been witnessed in various real
applications. Many algorithmic and implementation techniques have been
developed, however, theoretical understanding of many aspects of deep neural
networks is far from clear. A particular interesting issue is the usefulness of
dropout, which was motivated from the intuition of preventing complex
co-adaptation of feature detectors. In this paper, we study the Rademacher
complexity of different types of dropout, and our theoretical results disclose
that for shallow neural networks (with one or none hidden layer) dropout is
able to reduce the Rademacher complexity in polynomial, whereas for deep neural
networks it can amazingly lead to an exponential reduction of the Rademacher
complexity.
</summary>
    <author>
      <name>Wei Gao</name>
    </author>
    <author>
      <name>Zhi-Hua Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pagea</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.3811v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.3811v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.02581v1</id>
    <updated>2015-05-11T12:14:40Z</updated>
    <published>2015-05-11T12:14:40Z</published>
    <title>Improving neural networks with bunches of neurons modeled by Kumaraswamy
  units: Preliminary study</title>
    <summary>  Deep neural networks have recently achieved state-of-the-art results in many
machine learning problems, e.g., speech recognition or object recognition.
Hitherto, work on rectified linear units (ReLU) provides empirical and
theoretical evidence on performance increase of neural networks comparing to
typically used sigmoid activation function. In this paper, we investigate a new
manner of improving neural networks by introducing a bunch of copies of the
same neuron modeled by the generalized Kumaraswamy distribution. As a result,
we propose novel non-linear activation function which we refer to as
Kumaraswamy unit which is closely related to ReLU. In the experimental study
with MNIST image corpora we evaluate the Kumaraswamy unit applied to
single-layer (shallow) neural network and report a significant drop in test
classification error and test cross-entropy in comparison to sigmoid unit, ReLU
and Noisy ReLU.
</summary>
    <author>
      <name>Jakub Mikolaj Tomczak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.02581v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.02581v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.08473v3</id>
    <updated>2016-01-12T03:19:42Z</updated>
    <published>2015-06-28T23:19:49Z</published>
    <title>Beating the Perils of Non-Convexity: Guaranteed Training of Neural
  Networks using Tensor Methods</title>
    <summary>  Training neural networks is a challenging non-convex optimization problem,
and backpropagation or gradient descent can get stuck in spurious local optima.
We propose a novel algorithm based on tensor decomposition for guaranteed
training of two-layer neural networks. We provide risk bounds for our proposed
method, with a polynomial sample complexity in the relevant parameters, such as
input dimension and number of neurons. While learning arbitrary target
functions is NP-hard, we provide transparent conditions on the function and the
input for learnability. Our training method is based on tensor decomposition,
which provably converges to the global optimum, under a set of mild
non-degeneracy conditions. It consists of simple embarrassingly parallel linear
and multi-linear operations, and is competitive with standard stochastic
gradient descent (SGD), in terms of computational complexity. Thus, we propose
a computationally efficient method with guaranteed risk bounds for training
neural networks with one hidden layer.
</summary>
    <author>
      <name>Majid Janzamin</name>
    </author>
    <author>
      <name>Hanie Sedghi</name>
    </author>
    <author>
      <name>Anima Anandkumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The tensor decomposition analysis is expanded, and the analysis of
  ridge regression is added for recovering the parameters of last layer of
  neural network</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.08473v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.08473v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.06535v1</id>
    <updated>2015-08-26T15:39:09Z</updated>
    <published>2015-08-26T15:39:09Z</published>
    <title>Deep Convolutional Neural Networks for Smile Recognition</title>
    <summary>  This thesis describes the design and implementation of a smile detector based
on deep convolutional neural networks. It starts with a summary of neural
networks, the difficulties of training them and new training methods, such as
Restricted Boltzmann Machines or autoencoders. It then provides a literature
review of convolutional neural networks and recurrent neural networks. In order
to select databases for smile recognition, comprehensive statistics of
databases popular in the field of facial expression recognition were generated
and are summarized in this thesis. It then proposes a model for smile
detection, of which the main part is implemented. The experimental results are
discussed in this thesis and justified based on a comprehensive model selection
performed. All experiments were run on a Tesla K40c GPU benefiting from a
speedup of up to factor 10 over the computations on a CPU. A smile detection
test accuracy of 99.45% is achieved for the Denver Intensity of Spontaneous
Facial Action (DISFA) database, significantly outperforming existing approaches
with accuracies ranging from 65.55% to 79.67%. This experiment is re-run under
various variations, such as retaining less neutral images or only the low or
high intensities, of which the results are extensively compared.
</summary>
    <author>
      <name>Patrick O. Glauner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MSc thesis</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.06535v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.06535v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00457v1</id>
    <updated>2016-04-02T04:12:58Z</updated>
    <published>2016-04-02T04:12:58Z</published>
    <title>Stability of Analytic Neural Networks with Event-triggered Synaptic
  Feedbacks</title>
    <summary>  In this paper, we investigate stability of a class of analytic neural
networks with the synaptic feedback via event-triggered rules. This model is
general and include Hopfield neural network as a special case. These
event-trigger rules can efficiently reduces loads of computation and
information transmission at synapses of the neurons. The synaptic feedback of
each neuron keeps a constant value based on the outputs of the other neurons at
its latest triggering time but changes at its next triggering time, which is
determined by certain criterion. It is proved that every trajectory of the
analytic neural network converges to certain equilibrium under this
event-triggered rule for all initial values except a set of zero measure. The
main technique of the proof is the Lojasiewicz inequality to prove the
finiteness of trajectory length. The realization of this event-triggered rule
is verified by the exclusion of Zeno behaviors. Numerical examples are provided
to illustrate the efficiency of the theoretical results.
</summary>
    <author>
      <name>Ren Zheng</name>
    </author>
    <author>
      <name>Xinlei Yi</name>
    </author>
    <author>
      <name>Wenlian Lu</name>
    </author>
    <author>
      <name>Tianping Chen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TNNLS.2015.2488903</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TNNLS.2015.2488903" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 3 figures. arXiv admin note: substantial text overlap with
  arXiv:1504.08081</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Neural Networks and Learning Systems, Vol.
  27, No. 2, 483-494, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1604.00457v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00457v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.00097v1</id>
    <updated>2016-04-30T11:46:58Z</updated>
    <published>2016-04-30T11:46:58Z</published>
    <title>Application of artificial neural networks and genetic algorithms for
  crude fractional distillation process modeling</title>
    <summary>  This work presents the application of the artificial neural networks, trained
and structurally optimized by genetic algorithms, for modeling of crude
distillation process at PKN ORLEN S.A. refinery. Models for the main
fractionator distillation column products were developed using historical data.
Quality of the fractions were predicted based on several chosen process
variables. The performance of the model was validated using test data. Neural
networks used in companion with genetic algorithms proved that they can
accurately predict fractions quality shifts, reproducing the results of the
standard laboratory analysis. Simple knowledge extraction method from neural
network model built was also performed. Genetic algorithms can be successfully
utilized in efficient training of large neural networks and finding their
optimal structures.
</summary>
    <author>
      <name>Lukasz Pater</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Publication has 9 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.00097v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.00097v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07571v2</id>
    <updated>2016-11-13T18:04:41Z</updated>
    <published>2016-05-24T18:23:58Z</published>
    <title>Sequential Neural Models with Stochastic Layers</title>
    <summary>  How can we efficiently propagate uncertainty in a latent state representation
with recurrent neural networks? This paper introduces stochastic recurrent
neural networks which glue a deterministic recurrent neural network and a state
space model together to form a stochastic and sequential neural generative
model. The clear separation of deterministic and stochastic layers allows a
structured variational inference network to track the factorization of the
model's posterior distribution. By retaining both the nonlinear recursive
structure of a recurrent neural network and averaging over the uncertainty in a
latent path, like a state space model, we improve the state of the art results
on the Blizzard and TIMIT speech modeling data sets by a large margin, while
achieving comparable performances to competing methods on polyphonic music
modeling.
</summary>
    <author>
      <name>Marco Fraccaro</name>
    </author>
    <author>
      <name>S√∏ren Kaae S√∏nderby</name>
    </author>
    <author>
      <name>Ulrich Paquet</name>
    </author>
    <author>
      <name>Ole Winther</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NIPS 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07571v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07571v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04189v2</id>
    <updated>2016-07-07T18:52:57Z</updated>
    <published>2016-06-14T01:35:12Z</published>
    <title>Inverting face embeddings with convolutional neural networks</title>
    <summary>  Deep neural networks have dramatically advanced the state of the art for many
areas of machine learning. Recently they have been shown to have a remarkable
ability to generate highly complex visual artifacts such as images and text
rather than simply recognize them.
  In this work we use neural networks to effectively invert low-dimensional
face embeddings while producing realistically looking consistent images. Our
contribution is twofold, first we show that a gradient ascent style approaches
can be used to reproduce consistent images, with a help of a guiding image.
Second, we demonstrate that we can train a separate neural network to
effectively solve the minimization problem in one pass, and generate images in
real-time. We then evaluate the loss imposed by using a neural network instead
of the gradient descent by comparing the final values of the minimized loss
function.
</summary>
    <author>
      <name>Andrey Zhmoginov</name>
    </author>
    <author>
      <name>Mark Sandler</name>
    </author>
    <link href="http://arxiv.org/abs/1606.04189v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04189v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07434v1</id>
    <updated>2016-09-23T17:11:53Z</updated>
    <published>2016-09-23T17:11:53Z</published>
    <title>Regulating Reward Training by Means of Certainty Prediction in a Neural
  Network-Implemented Pong Game</title>
    <summary>  We present the first reinforcement-learning model to self-improve its
reward-modulated training implemented through a continuously improving
"intuition" neural network. An agent was trained how to play the arcade video
game Pong with two reward-based alternatives, one where the paddle was placed
randomly during training, and a second where the paddle was simultaneously
trained on three additional neural networks such that it could develop a sense
of "certainty" as to how probable its own predicted paddle position will be to
return the ball. If the agent was less than 95% certain to return the ball, the
policy used an intuition neural network to place the paddle. We trained both
architectures for an equivalent number of epochs and tested learning
performance by letting the trained programs play against a near-perfect
opponent. Through this, we found that the reinforcement learning model that
uses an intuition neural network for placing the paddle during reward training
quickly overtakes the simple architecture in its ability to outplay the
near-perfect opponent, additionally outscoring that opponent by an increasingly
wide margin after additional epochs of training.
</summary>
    <author>
      <name>Matt Oberdorfer</name>
    </author>
    <author>
      <name>Matt Abuzalaf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.07434v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07434v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; F.1.1; I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.03828v2</id>
    <updated>2017-03-10T20:09:45Z</updated>
    <published>2016-10-12T19:07:45Z</published>
    <title>Linking structure and activity in nonlinear spiking networks</title>
    <summary>  Recent experimental advances are producing an avalanche of data on both
neural connectivity and neural activity. To take full advantage of these two
emerging datasets we need a framework that links them, revealing how collective
neural activity arises from the structure of neural connectivity and intrinsic
neural dynamics. This problem of {\it structure-driven activity} has drawn
major interest in computational neuroscience. Existing methods for relating
activity and architecture in spiking networks rely on linearizing activity
around a central operating point and thus fail to capture the nonlinear
responses of individual neurons that are the hallmark of neural information
processing. Here, we overcome this limitation and present a new relationship
between connectivity and activity in networks of nonlinear spiking neurons by
developing a diagrammatic fluctuation expansion based on statistical field
theory. We explicitly show how recurrent network structure produces pairwise
and higher-order correlated activity, and how nonlinearities impact the
networks' spiking activity. Our findings open new avenues to investigating how
single-neuron nonlinearities---including those of different cell
types---combine with connectivity to shape population activity and function.
</summary>
    <author>
      <name>Gabriel Koch Ocker</name>
    </author>
    <author>
      <name>Kre≈°imir Josiƒá</name>
    </author>
    <author>
      <name>Eric Shea-Brown</name>
    </author>
    <author>
      <name>Michael A. Buice</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">54 pages, added results with exponential transfer function and full
  derivation of Feynman rules</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.03828v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.03828v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.03474v4</id>
    <updated>2017-09-29T11:57:42Z</updated>
    <published>2017-02-12T00:52:12Z</published>
    <title>Practical Approximation Method for Firing Rate Models of Coupled Neural
  Networks with Correlated Inputs</title>
    <summary>  Rapid experimental advances now enable simultaneous electrophysiological
recording of neural activity at single-cell resolution across large regions of
the nervous system. Models of this neural network activity will necessarily
increase in size and complexity, thus increasing the computational cost of
simulating them and the challenge of analyzing them. Here we present a novel
method to approximate the activity and firing statistics of a general firing
rate network model (of Wilson-Cowan type) subject to noisy correlated
background inputs. The method requires solving a system of transcendental
equations and is fast compared to Monte Carlo simulations of coupled stochastic
differential equations. We implement the method with several examples of
coupled neural networks and show that the results are quantitatively accurate
even with moderate coupling strengths and an appreciable amount of
heterogeneity in many parameters. This work should be useful for investigating
how various neural attributes qualitatively effect the spiking statistics of
coupled neural networks. Matlab code implementing the method is freely
available at GitHub (\url{http://github.com/chengly70/FiringRateModReduction}).
</summary>
    <author>
      <name>Andrea K. Barreiro</name>
    </author>
    <author>
      <name>Cheng Ly</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.96.022413</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.96.022413" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 96, 022413 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1702.03474v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.03474v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92B20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.07028v2</id>
    <updated>2017-06-02T15:14:17Z</updated>
    <published>2017-02-22T22:21:38Z</published>
    <title>On the ability of neural nets to express distributions</title>
    <summary>  Deep neural nets have caused a revolution in many classification tasks. A
related ongoing revolution---also theoretically not understood---concerns their
ability to serve as generative models for complicated types of data such as
images and texts. These models are trained using ideas like variational
autoencoders and Generative Adversarial Networks.
  We take a first cut at explaining the expressivity of multilayer nets by
giving a sufficient criterion for a function to be approximable by a neural
network with $n$ hidden layers. A key ingredient is Barron's Theorem
\cite{Barron1993}, which gives a Fourier criterion for approximability of a
function by a neural network with 1 hidden layer. We show that a composition of
$n$ functions which satisfy certain Fourier conditions ("Barron functions") can
be approximated by a $n+1$-layer neural network.
  For probability distributions, this translates into a criterion for a
probability distribution to be approximable in Wasserstein distance---a natural
metric on probability distributions---by a neural network applied to a fixed
base distribution (e.g., multivariate gaussian).
  Building up recent lower bound work, we also give an example function that
shows that composition of Barron functions is more expressive than Barron
functions alone.
</summary>
    <author>
      <name>Holden Lee</name>
    </author>
    <author>
      <name>Rong Ge</name>
    </author>
    <author>
      <name>Tengyu Ma</name>
    </author>
    <author>
      <name>Andrej Risteski</name>
    </author>
    <author>
      <name>Sanjeev Arora</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to COLT 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.07028v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.07028v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.00382v1</id>
    <updated>2017-06-01T16:50:09Z</updated>
    <published>2017-06-01T16:50:09Z</published>
    <title>Blind nonnegative source separation using biological neural networks</title>
    <summary>  Blind source separation, i.e. extraction of independent sources from a
mixture, is an important problem for both artificial and natural signal
processing. Here, we address a special case of this problem when sources (but
not the mixing matrix) are known to be nonnegative, for example, due to the
physical nature of the sources. We search for the solution to this problem that
can be implemented using biologically plausible neural networks. Specifically,
we consider the online setting where the dataset is streamed to a neural
network. The novelty of our approach is that we formulate blind nonnegative
source separation as a similarity matching problem and derive neural networks
from the similarity matching objective. Importantly, synaptic weights in our
networks are updated according to biologically plausible local learning rules.
</summary>
    <author>
      <name>Cengiz Pehlevan</name>
    </author>
    <author>
      <name>Sreyas Mohan</name>
    </author>
    <author>
      <name>Dmitri B. Chklovskii</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/neco_a_01007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/neco_a_01007" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in Neural Computation</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.00382v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.00382v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.05283v1</id>
    <updated>2017-04-06T01:53:52Z</updated>
    <published>2017-04-06T01:53:52Z</published>
    <title>The Evolution of Neural Network-Based Chart Patterns: A Preliminary
  Study</title>
    <summary>  A neural network-based chart pattern represents adaptive parametric features,
including non-linear transformations, and a template that can be applied in the
feature space. The search of neural network-based chart patterns has been
unexplored despite its potential expressiveness. In this paper, we formulate a
general chart pattern search problem to enable cross-representational
quantitative comparison of various search schemes. We suggest a HyperNEAT
framework applying state-of-the-art deep neural network techniques to find
attractive neural network-based chart patterns; These techniques enable a fast
evaluation and search of robust patterns, as well as bringing a performance
gain. The proposed framework successfully found attractive patterns on the
Korean stock market. We compared newly found patterns with those found by
different search schemes, showing the proposed approach has potential.
</summary>
    <author>
      <name>Myoung Hoon Ha</name>
    </author>
    <author>
      <name>Byung-Ro Moon</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3071178.3071192</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3071178.3071192" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, In proceedings of Genetic and Evolutionary Computation
  Conference (GECCO 2017), Berlin, Germany</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.05283v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.05283v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.02274v1</id>
    <updated>2017-09-07T14:25:56Z</updated>
    <published>2017-09-07T14:25:56Z</published>
    <title>A Neural Network Based on Synchronized Pairs of Nano-Oscillators</title>
    <summary>  Artificial neural networks are intensively used to perform cognitive tasks
such as image classification on traditional computers. With the end of CMOS
scaling and increasing demand for efficient neural networks, alternative
architectures implementing neural functions efficiently are being studied. This
study leverages the demonstrated frequency tuning capabilities of compact
nano-oscillators and their synchronization dynamics to implement a neuron using
a pair of synchronized oscillators, and which features an unconventional
response curve. We show that this compact neuron can naturally implement
generic logic gates, including XOR. A simulated oscillator-based neural network
is then shown to achieve results equivalent to standard approaches on two
reference classification tasks. Finally, the performance of the system is
evaluated in the presence of oscillator phase noise, an important issue of
oscillating nanodevices. These results open the way for the design of
alternative architectures adapted to efficient neural network execution.
</summary>
    <author>
      <name>Damir Vodenicarevic</name>
    </author>
    <author>
      <name>Nicolas Locatelli</name>
    </author>
    <author>
      <name>Damien Querlioz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/NANO.2017.8117345</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/NANO.2017.8117345" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Nano 2017 Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.02274v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.02274v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.09541v1</id>
    <updated>2017-09-26T08:35:35Z</updated>
    <published>2017-09-26T08:35:35Z</published>
    <title>Fitting of dynamic recurrent neural network models to sensory
  stimulus-response data</title>
    <summary>  We present a theoretical study aiming at model fitting for sensory neurons.
Conventional neural network training approaches are not applicable to this
problem due to lack of continuous data. Although the stimulus can be considered
as a smooth time dependent variable, the associated response will be a set of
neural spike timings (roughly the instants of successive action potential
peaks) which have no amplitude information. A recurrent neural network model
can be fitted to such a stimulus-response data pair by using maximum likelihood
estimation method where the likelihood function is derived from Poisson
statistics of neural spiking. The universal approximation feature of the
recurrent dynamical neuron network models allow us to describe
excitatory-inhibitory characteristics of an actual sensory neural network with
any desired number of neurons. The stimulus data is generated by a Phased
Cosine Fourier series having fixed amplitude and frequency but a randomly shot
phase. Various values of amplitude, stimulus component size and sample size are
applied in order to examine the effect of stimulus to the identification
process. Results are presented in tabular form at the end of this text.
</summary>
    <author>
      <name>R. Ozgur Doruk</name>
    </author>
    <author>
      <name>Kechen Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1610.05561</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.09541v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.09541v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.09825v1</id>
    <updated>2017-10-26T17:42:23Z</updated>
    <published>2017-10-26T17:42:23Z</published>
    <title>On the role of synaptic stochasticity in training low-precision neural
  networks</title>
    <summary>  Stochasticity and limited precision of synaptic weights in neural network
models is a key aspect of both biological and hardware modeling of learning
processes. Here we show that a neural network model with stochastic binary
weights naturally gives prominence to exponentially rare dense regions of
solutions with a number of desirable properties such as robustness and good
generalization per- formance, while typical solutions are isolated and hard to
find. Binary solutions of the standard perceptron problem are obtained from a
simple gradient descent procedure on a set of real values parametrizing a
probability distribution over the binary synapses. Both analytical and
numerical results are presented. An algorithmic extension aimed at training
discrete deep neural networks is also investigated.
</summary>
    <author>
      <name>Carlo Baldassi</name>
    </author>
    <author>
      <name>Federica Gerace</name>
    </author>
    <author>
      <name>Hilbert J. Kappen</name>
    </author>
    <author>
      <name>Carlo Lucibello</name>
    </author>
    <author>
      <name>Luca Saglietti</name>
    </author>
    <author>
      <name>Enzo Tartaglione</name>
    </author>
    <author>
      <name>Riccardo Zecchina</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages + 10 pages of supplementary material</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.09825v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.09825v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.10944v2</id>
    <updated>2017-11-04T18:50:59Z</updated>
    <published>2017-10-30T13:55:59Z</published>
    <title>A Supervised STDP-based Training Algorithm for Living Neural Networks</title>
    <summary>  Neural networks have shown great potential in many applications like speech
recognition, drug discovery, image classification, and object detection. Neural
network models are inspired by biological neural networks, but they are
optimized to perform machine learning tasks on digital computers. The proposed
work explores the possibilities of using living neural networks in vitro as
basic computational elements for machine learning applications. A new
supervised STDP-based learning algorithm is proposed in this work, which
considers neuron engineering constrains. A 74.7% accuracy is achieved on the
MNIST benchmark for handwritten digit recognition.
</summary>
    <author>
      <name>Yuan Zeng</name>
    </author>
    <author>
      <name>Kevin Devincentis</name>
    </author>
    <author>
      <name>Yao Xiao</name>
    </author>
    <author>
      <name>Zubayer Ibne Ferdous</name>
    </author>
    <author>
      <name>Xiaochen Guo</name>
    </author>
    <author>
      <name>Zhiyuan Yan</name>
    </author>
    <author>
      <name>Yevgeny Berdichevsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures, submitted to ICASSP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.10944v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.10944v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.01243v2</id>
    <updated>2017-11-28T00:45:57Z</updated>
    <published>2017-11-03T17:12:15Z</published>
    <title>ResBinNet: Residual Binary Neural Network</title>
    <summary>  Recent efforts on training light-weight binary neural networks offer
promising execution/memory efficiency. This paper introduces ResBinNet, which
is a composition of two interlinked methodologies aiming to address the slow
convergence speed and limited accuracy of binary convolutional neural networks.
The first method, called residual binarization, learns a multi-level binary
representation for the features within a certain neural network layer. The
second method, called temperature adjustment, gradually binarizes the weights
of a particular layer. The two methods jointly learn a set of soft-binarized
parameters that improve the convergence rate and accuracy of binary neural
networks. We corroborate the applicability and scalability of ResBinNet by
implementing a prototype hardware accelerator. The accelerator is
reconfigurable in terms of the numerical precision of the binarized features,
offering a trade-off between runtime and inference accuracy.
</summary>
    <author>
      <name>Mohammad Ghasemzadeh</name>
    </author>
    <author>
      <name>Mohammad Samragh</name>
    </author>
    <author>
      <name>Farinaz Koushanfar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Added the results of large-scale experiments (Imagenet); Extended the
  comparison with prior art</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.01243v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.01243v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.01754v1</id>
    <updated>2017-11-06T07:28:10Z</updated>
    <published>2017-11-06T07:28:10Z</published>
    <title>Learning Solving Procedure for Artificial Neural Network</title>
    <summary>  It is expected that progress toward true artificial intelligence will be
achieved through the emergence of a system that integrates representation
learning and complex reasoning (LeCun et al. 2015). In response to this
prediction, research has been conducted on implementing the symbolic reasoning
of a von Neumann computer in an artificial neural network (Graves et al. 2016;
Graves et al. 2014; Reed et al. 2015). However, these studies have many
limitations in realizing neural-symbolic integration (Jaeger. 2016). Here, we
present a new learning paradigm: a learning solving procedure (LSP) that learns
the procedure for solving complex problems. This is not accomplished merely by
learning input-output data, but by learning algorithms through a solving
procedure that obtains the output as a sequence of tasks for a given input
problem. The LSP neural network system not only learns simple problems of
addition and multiplication, but also the algorithms of complicated problems,
such as complex arithmetic expression, sorting, and Hanoi Tower. To realize
this, the LSP neural network structure consists of a deep neural network and
long short-term memory, which are recursively combined. Through
experimentation, we demonstrate the efficiency and scalability of LSP and its
validity as a mechanism of complex reasoning.
</summary>
    <author>
      <name>Ju-Hong Lee</name>
    </author>
    <author>
      <name>Moon-Ju Kang</name>
    </author>
    <author>
      <name>Bumghi Choi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.01754v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.01754v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.06484v1</id>
    <updated>2017-11-17T10:38:51Z</updated>
    <published>2017-11-17T10:38:51Z</published>
    <title>Large Neural Network Based Detection of Apnea, Bradycardia and
  Desaturation Events</title>
    <summary>  Apnea, bradycardia and desaturation (ABD) events often precede
life-threatening events including sepsis in newborn babies. Here, we explore
machine learning for detection of ABD events as a binary classification
problem. We investigate the use of a large neural network to achieve a good
detection performance. To be user friendly, the chosen neural network does not
require a high level of parameter tuning. Furthermore, a limited amount of
training data is available and the training dataset is unbalanced. Comparing
with two widely used state-of-the-art machine learning algorithms, the large
neural network is found to be efficient. Even with a limited and unbalanced
training data, the large neural network provides a detection performance level
that is feasible to use in clinical care.
</summary>
    <author>
      <name>Antoine Honor√©</name>
    </author>
    <author>
      <name>Veronica Siljehav</name>
    </author>
    <author>
      <name>Saikat Chatterjee</name>
    </author>
    <author>
      <name>Eric Herlenius</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for NIPS Workshop ML4H, 2017</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Information Processing Systems (NIPS) 2017 Workshop on
  Machine Learning for Health, Long Beach, CA, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1711.06484v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.06484v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.09667v1</id>
    <updated>2017-11-27T13:04:57Z</updated>
    <published>2017-11-27T13:04:57Z</published>
    <title>DeepChess: End-to-End Deep Neural Network for Automatic Learning in
  Chess</title>
    <summary>  We present an end-to-end learning method for chess, relying on deep neural
networks. Without any a priori knowledge, in particular without any knowledge
regarding the rules of chess, a deep neural network is trained using a
combination of unsupervised pretraining and supervised training. The
unsupervised training extracts high level features from a given position, and
the supervised training learns to compare two chess positions and select the
more favorable one. The training relies entirely on datasets of several million
chess games, and no further domain specific knowledge is incorporated.
  The experiments show that the resulting neural network (referred to as
DeepChess) is on a par with state-of-the-art chess playing programs, which have
been developed through many years of manual feature selection and tuning.
DeepChess is the first end-to-end machine learning-based method that results in
a grandmaster-level chess playing performance.
</summary>
    <author>
      <name>Eli David</name>
    </author>
    <author>
      <name>Nathan S. Netanyahu</name>
    </author>
    <author>
      <name>Lior Wolf</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-44781-0_11</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-44781-0_11" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Winner of Best Paper Award in ICANN 2016</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Artificial Neural Networks (ICANN),
  Springer LNCS, Vol. 9887, pp. 88-96, Barcelona, Spain, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1711.09667v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.09667v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.04443v1</id>
    <updated>2018-02-13T02:32:10Z</updated>
    <published>2018-02-13T02:32:10Z</published>
    <title>On Characterizing the Capacity of Neural Networks using Algebraic
  Topology</title>
    <summary>  The learnability of different neural architectures can be characterized
directly by computable measures of data complexity. In this paper, we reframe
the problem of architecture selection as understanding how data determines the
most expressive and generalizable architectures suited to that data, beyond
inductive bias. After suggesting algebraic topology as a measure for data
complexity, we show that the power of a network to express the topological
complexity of a dataset in its decision region is a strictly limiting factor in
its ability to generalize. We then provide the first empirical characterization
of the topological capacity of neural networks. Our empirical analysis shows
that at every level of dataset complexity, neural networks exhibit topological
phase transitions. This observation allowed us to connect existing theory to
empirically driven conjectures on the choice of architectures for
fully-connected neural networks.
</summary>
    <author>
      <name>William H. Guss</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.04443v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.04443v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.09660v1</id>
    <updated>2018-02-27T00:49:45Z</updated>
    <published>2018-02-27T00:49:45Z</published>
    <title>Computational Red Teaming in a Sudoku Solving Context: Neural Network
  Based Skill Representation and Acquisition</title>
    <summary>  In this paper we provide an insight into the skill representation, where
skill representation is seen as an essential part of the skill assessment stage
in the Computational Red Teaming process. Skill representation is demonstrated
in the context of Sudoku puzzle, for which the real human skills used in Sudoku
solving, along with their acquisition, are represented computationally in a
cognitively plausible manner, by using feed-forward neural networks with
back-propagation, and supervised learning. The neural network based skills are
then coupled with a hard-coded constraint propagation computational Sudoku
solver, in which the solving sequence is kept hard-coded, and the skills are
represented through neural networks. The paper demonstrates that the modified
solver can achieve different levels of proficiency, depending on the amount of
skills acquired through the neural networks. Results are encouraging for
developing more complex skill and skill acquisition models usable in general
frameworks related to the skill assessment aspect of Computational Red Teaming.
</summary>
    <author>
      <name>George Leu</name>
    </author>
    <author>
      <name>Hussein Abbass</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-27000-5_26</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-27000-5_26" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings in Adaptation, Learning and Optimization, vol. 5,
  Springer, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1802.09660v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.09660v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.01686v1</id>
    <updated>2018-02-27T02:47:13Z</updated>
    <published>2018-02-27T02:47:13Z</published>
    <title>On Extended Long Short-term Memory and Dependent Bidirectional Recurrent
  Neural Network</title>
    <summary>  In this work, we investigate the memory capability of recurrent neural
networks (RNNs), where this capability is defined as a function that maps an
element in a sequence to the current output. We first analyze the system
function of a recurrent neural network (RNN) cell, and provide analytical
results for three RNNs. They are the simple recurrent neural network (SRN), the
long short-term memory (LSTM), and the gated recurrent unit (GRU). Based on the
analysis, we propose a new design to extend the memory length of a cell, and
call it the extended long short-term memory (ELSTM). Next, we present a
dependent bidirectional recurrent neural network (DBRNN) for the
sequence-in-sequence-out (SISO) problem, which is more robust to previous
erroneous predictions. Extensive experiments are carried out on different
language tasks to demonstrate the superiority of our proposed ELSTM and DBRNN
solutions.
</summary>
    <author>
      <name>Yuanhang Su</name>
    </author>
    <author>
      <name>Yuzhong Huang</name>
    </author>
    <author>
      <name>C. -C. Jay Kuo</name>
    </author>
    <link href="http://arxiv.org/abs/1803.01686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.01686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.5060v1</id>
    <updated>2010-04-28T15:21:37Z</updated>
    <published>2010-04-28T15:21:37Z</published>
    <title>STDP-driven networks and the \emph{C. elegans} neuronal network</title>
    <summary>  We study the dynamics of the structure of a formal neural network wherein the
strengths of the synapses are governed by spike-timing-dependent plasticity
(STDP). For properly chosen input signals, there exists a steady state with a
residual network. We compare the motif profile of such a network with that of a
real neural network of \emph{C. elegans} and identify robust qualitative
similarities. In particular, our extensive numerical simulations show that this
STDP-driven resulting network is robust under variations of the model
parameters.
</summary>
    <author>
      <name>Quansheng Ren</name>
    </author>
    <author>
      <name>Kiran M. Kolwankar</name>
    </author>
    <author>
      <name>Areejit Samal</name>
    </author>
    <author>
      <name>J√ºrgen Jost</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physa.2010.05.018</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physa.2010.05.018" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.5060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.5060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.5737v1</id>
    <updated>2013-03-20T15:32:18Z</updated>
    <published>2013-03-20T15:32:18Z</published>
    <title>Integrating Probabilistic Rules into Neural Networks: A Stochastic EM
  Learning Algorithm</title>
    <summary>  The EM-algorithm is a general procedure to get maximum likelihood estimates
if part of the observations on the variables of a network are missing. In this
paper a stochastic version of the algorithm is adapted to probabilistic neural
networks describing the associative dependency of variables. These networks
have a probability distribution, which is a special case of the distribution
generated by probabilistic inference networks. Hence both types of networks can
be combined allowing to integrate probabilistic rules as well as unspecified
associations in a sound way. The resulting network may have a number of
interesting features including cycles of probabilistic rules, hidden
'unobservable' variables, and uncertain and contradictory evidence.
</summary>
    <author>
      <name>Gerhard Paass</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.5737v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.5737v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.3362v1</id>
    <updated>2013-08-15T11:27:57Z</updated>
    <published>2013-08-15T11:27:57Z</published>
    <title>Guiding synchrony through random networks</title>
    <summary>  Sparse random networks contain structures that can be considered as diluted
feed-forward networks. Modeling of cortical circuits has shown that
feed-forward structures, if strongly pronounced compared to the embedding
random network, enable reliable signal transmission by propagating localized
(sub-network) synchrony. This assumed prominence, however, is not
experimentally observed in local cortical circuits. Here we show that nonlinear
dendritic interactions as discovered in recent single neuron experiments,
naturally enable guided synchrony propagation already in random recurrent
neural networks exhibiting mildly enhanced, biologically plausible
sub-structures.
</summary>
    <author>
      <name>Sven Jahnke</name>
    </author>
    <author>
      <name>Marc Timme</name>
    </author>
    <author>
      <name>Raoul-Martin Memmesheimer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevX.2.041016</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevX.2.041016" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. X 2, 041016 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1308.3362v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.3362v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.02382v1</id>
    <updated>2017-02-08T11:40:15Z</updated>
    <published>2017-02-08T11:40:15Z</published>
    <title>An Adversarial Regularisation for Semi-Supervised Training of Structured
  Output Neural Networks</title>
    <summary>  We propose a method for semi-supervised training of structured-output neural
networks. Inspired by the framework of Generative Adversarial Networks (GAN),
we train a discriminator network to capture the notion of a quality of network
output. To this end, we leverage the qualitative difference between outputs
obtained on the labelled training data and unannotated data. We then use the
discriminator as a source of error signal for unlabelled data. This effectively
boosts the performance of a network on a held out test set. Initial experiments
in image segmentation demonstrate that the proposed framework enables achieving
the same network performance as in a fully supervised scenario, while using two
times less annotations.
</summary>
    <author>
      <name>Mateusz Kozi≈Ñski</name>
    </author>
    <author>
      <name>Lo√Øc Simon</name>
    </author>
    <author>
      <name>Fr√©d√©ric Jurie</name>
    </author>
    <link href="http://arxiv.org/abs/1702.02382v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.02382v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.05468v1</id>
    <updated>2017-09-16T06:29:59Z</updated>
    <published>2017-09-16T06:29:59Z</published>
    <title>Machine learning technique to find quantum many-body ground states of
  bosons on a lattice</title>
    <summary>  We develop a variational method to obtain many-body ground states of the
Bose-Hubbard model using feedforward artificial neural networks. A
fully-connected network with a single hidden layer works better than a
fully-connected network with multiple hidden layers, and a multi-layer
convolutional network is more efficient than a fully-connected network. AdaGrad
and Adam are optimization methods that work well. Moreover, we show that
many-body ground states with different numbers of atoms can be generated by a
single network.
</summary>
    <author>
      <name>Hiroki Saito</name>
    </author>
    <author>
      <name>Masaya Kato</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.7566/JPSJ.87.014001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.7566/JPSJ.87.014001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 10 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J. Phys. Soc. Jpn. 87, 014001 (2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1709.05468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.05468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.quant-gas" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.3558v1</id>
    <updated>2010-03-18T11:43:35Z</updated>
    <published>2010-03-18T11:43:35Z</published>
    <title>Coverage and Connectivity Aware Neural Network Based Energy Efficient
  Routing in Wireless Sensor Networks</title>
    <summary>  There are many challenges when designing and deploying wireless sensor
networks (WSNs). One of the key challenges is how to make full use of the
limited energy to prolong the lifetime of the network, because energy is a
valuable resource in WSNs. The status of energy consumption should be
continuously monitored after network deployment. In this paper, we propose
coverage and connectivity aware neural network based energy efficient routing
in WSN with the objective of maximizing the network lifetime. In the proposed
scheme, the problem is formulated as linear programming (LP) with coverage and
connectivity aware constraints. Cluster head selection is proposed using
adaptive learning in neural networks followed by coverage and connectivity
aware routing with data transmission. The proposed scheme is compared with
existing schemes with respect to the parameters such as number of alive nodes,
packet delivery fraction, and node residual energy. The simulation results show
that the proposed scheme can be used in wide area of applications in WSNs.
</summary>
    <author>
      <name>Neeraj Kumar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SMVD University, Katra</arxiv:affiliation>
    </author>
    <author>
      <name>Manoj Kumar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SMVD University, Katra</arxiv:affiliation>
    </author>
    <author>
      <name>R. B. Patel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MITS University, Rajassthan, India</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/jgraphhoc.2010.2105</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/jgraphhoc.2010.2105" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 Pages, JGraph-Hoc Journal</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International journal on applications of graph theory in wireless
  ad hoc networks and sensor networks 2.1 (2010) 45-60</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.3558v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.3558v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.03116v2</id>
    <updated>2016-05-19T19:38:30Z</updated>
    <published>2016-03-10T01:04:07Z</published>
    <title>Low-rank passthrough neural networks</title>
    <summary>  Deep learning consists in training neural networks to perform computations
that sequentially unfold in many steps over a time dimension or an intrinsic
depth dimension. Effective learning in this setting is usually accomplished by
specialized network architectures that are designed to mitigate the vanishing
gradient problem of naive deep networks. Many of these architectures, such as
LSTMs, GRUs, Highway Networks and Deep Residual Network, are based on a single
structural principle: the state passthrough.
  We observe that these architectures, hereby characterized as Passthrough
Networks, in addition to the mitigation of the vanishing gradient problem,
enable the decoupling of the network state size from the number of parameters
of the network, a possibility that is exploited in some recent works but not
thoroughly explored.
  In this work we propose simple, yet effective, low-rank and low-rank plus
diagonal matrix parametrizations for Passthrough Networks which exploit this
decoupling property, reducing the data complexity and memory requirements of
the network while preserving its memory capacity. We present competitive
experimental results on synthetic tasks and a near state of the art result on
sequential randomly-permuted MNIST classification, a hard task on natural data.
</summary>
    <author>
      <name>Antonio Valerio Miceli Barone</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.03116v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.03116v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.03250v1</id>
    <updated>2016-07-12T07:43:01Z</updated>
    <published>2016-07-12T07:43:01Z</published>
    <title>Network Trimming: A Data-Driven Neuron Pruning Approach towards
  Efficient Deep Architectures</title>
    <summary>  State-of-the-art neural networks are getting deeper and wider. While their
performance increases with the increasing number of layers and neurons, it is
crucial to design an efficient deep architecture in order to reduce
computational and memory costs. Designing an efficient neural network, however,
is labor intensive requiring many experiments, and fine-tunings. In this paper,
we introduce network trimming which iteratively optimizes the network by
pruning unimportant neurons based on analysis of their outputs on a large
dataset. Our algorithm is inspired by an observation that the outputs of a
significant portion of neurons in a large network are mostly zero, regardless
of what inputs the network received. These zero activation neurons are
redundant, and can be removed without affecting the overall accuracy of the
network. After pruning the zero activation neurons, we retrain the network
using the weights before pruning as initialization. We alternate the pruning
and retraining to further reduce zero activations in a network. Our experiments
on the LeNet and VGG-16 show that we can achieve high compression ratio of
parameters without losing or even achieving higher accuracy than the original
network.
</summary>
    <author>
      <name>Hengyuan Hu</name>
    </author>
    <author>
      <name>Rui Peng</name>
    </author>
    <author>
      <name>Yu-Wing Tai</name>
    </author>
    <author>
      <name>Chi-Keung Tang</name>
    </author>
    <link href="http://arxiv.org/abs/1607.03250v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.03250v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.03466v2</id>
    <updated>2017-05-28T15:45:56Z</updated>
    <published>2016-10-11T18:59:12Z</published>
    <title>Fused DNN: A deep neural network fusion approach to fast and robust
  pedestrian detection</title>
    <summary>  We propose a deep neural network fusion architecture for fast and robust
pedestrian detection. The proposed network fusion architecture allows for
parallel processing of multiple networks for speed. A single shot deep
convolutional network is trained as a object detector to generate all possible
pedestrian candidates of different sizes and occlusions. This network outputs a
large variety of pedestrian candidates to cover the majority of ground-truth
pedestrians while also introducing a large number of false positives. Next,
multiple deep neural networks are used in parallel for further refinement of
these pedestrian candidates. We introduce a soft-rejection based network fusion
method to fuse the soft metrics from all networks together to generate the
final confidence scores. Our method performs better than existing
state-of-the-arts, especially when detecting small-size and occluded
pedestrians. Furthermore, we propose a method for integrating pixel-wise
semantic segmentation network into the network fusion architecture as a
reinforcement to the pedestrian detector. The approach outperforms
state-of-the-art methods on most protocols on Caltech Pedestrian dataset, with
significant boosts on several protocols. It is also faster than all other
methods.
</summary>
    <author>
      <name>Xianzhi Du</name>
    </author>
    <author>
      <name>Mostafa El-Khamy</name>
    </author>
    <author>
      <name>Jungwon Lee</name>
    </author>
    <author>
      <name>Larry S. Davis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">WACV 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.03466v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.03466v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.03736v1</id>
    <updated>2017-08-12T01:17:38Z</updated>
    <published>2017-08-12T01:17:38Z</published>
    <title>Face Parsing via a Fully-Convolutional Continuous CRF Neural Network</title>
    <summary>  In this work, we address the face parsing task with a Fully-Convolutional
continuous CRF Neural Network (FC-CNN) architecture. In contrast to previous
face parsing methods that apply region-based subnetwork hundreds of times, our
FC-CNN is fully convolutional with high segmentation accuracy. To achieve this
goal, FC-CNN integrates three subnetworks, a unary network, a pairwise network
and a continuous Conditional Random Field (C-CRF) network into a unified
framework. The high-level semantic information and low-level details across
different convolutional layers are captured by the convolutional and
deconvolutional structures in the unary network. The semantic edge context is
learnt by the pairwise network branch to construct pixel-wise affinity. Based
on a differentiable superpixel pooling layer and a differentiable C-CRF layer,
the unary network and pairwise network are combined via a novel continuous CRF
network to achieve spatial consistency in both training and test procedure of a
deep neural network. Comprehensive evaluations on LFW-PL and HELEN datasets
demonstrate that FC-CNN achieves better performance over the other
state-of-arts for accurate face labeling on challenging images.
</summary>
    <author>
      <name>Lei Zhou</name>
    </author>
    <author>
      <name>Zhi Liu</name>
    </author>
    <author>
      <name>Xiangjian He</name>
    </author>
    <link href="http://arxiv.org/abs/1708.03736v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.03736v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.01686v1</id>
    <updated>2017-09-06T06:30:51Z</updated>
    <published>2017-09-06T06:30:51Z</published>
    <title>BranchyNet: Fast Inference via Early Exiting from Deep Neural Networks</title>
    <summary>  Deep neural networks are state of the art methods for many learning tasks due
to their ability to extract increasingly better features at each network layer.
However, the improved performance of additional layers in a deep network comes
at the cost of added latency and energy usage in feedforward inference. As
networks continue to get deeper and larger, these costs become more prohibitive
for real-time and energy-sensitive applications. To address this issue, we
present BranchyNet, a novel deep network architecture that is augmented with
additional side branch classifiers. The architecture allows prediction results
for a large portion of test samples to exit the network early via these
branches when samples can already be inferred with high confidence. BranchyNet
exploits the observation that features learned at an early layer of a network
may often be sufficient for the classification of many data points. For more
difficult samples, which are expected less frequently, BranchyNet will use
further or all network layers to provide the best likelihood of correct
prediction. We study the BranchyNet architecture using several well-known
networks (LeNet, AlexNet, ResNet) and datasets (MNIST, CIFAR10) and show that
it can both improve accuracy and significantly reduce the inference time of the
network.
</summary>
    <author>
      <name>Surat Teerapittayanon</name>
    </author>
    <author>
      <name>Bradley McDanel</name>
    </author>
    <author>
      <name>H. T. Kung</name>
    </author>
    <link href="http://arxiv.org/abs/1709.01686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.01686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.06799v1</id>
    <updated>2017-10-17T14:07:34Z</updated>
    <published>2017-10-17T14:07:34Z</published>
    <title>NeuTM: A Neural Network-based Framework for Traffic Matrix Prediction in
  SDN</title>
    <summary>  This paper presents NeuTM, a framework for network Traffic Matrix (TM)
prediction based on Long Short-Term Memory Recurrent Neural Networks (LSTM
RNNs). TM prediction is defined as the problem of estimating future network
traffic matrix from the previous and achieved network traffic data. It is
widely used in network planning, resource management and network security. Long
Short-Term Memory (LSTM) is a specific recurrent neural network (RNN)
architecture that is well-suited to learn from data and classify or predict
time series with time lags of unknown size. LSTMs have been shown to model
long-range dependencies more accurately than conventional RNNs. NeuTM is a LSTM
RNN-based framework for predicting TM in large networks. By validating our
framework on real-world data from GEEANT network, we show that our model
converges quickly and gives state of the art TM prediction performance.
</summary>
    <author>
      <name>Abdelhadi Azzouni</name>
    </author>
    <author>
      <name>Guy Pujolle</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to NOMS18. arXiv admin note: substantial text overlap with
  arXiv:1705.05690</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.06799v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.06799v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00939v2</id>
    <updated>2018-02-11T10:22:38Z</updated>
    <published>2018-02-03T08:52:12Z</published>
    <title>Recent Advances in Efficient Computation of Deep Convolutional Neural
  Networks</title>
    <summary>  Deep neural networks have evolved remarkably over the past few years and they
are currently the fundamental tools of many intelligent systems. At the same
time, the computational complexity and resource consumption of these networks
also continue to increase. This will pose a significant challenge to the
deployment of such networks, especially in real-time applications or on
resource-limited devices. Thus, network acceleration has become a hot topic
within the deep learning community. As for hardware implementation of deep
neural networks, a batch of accelerators based on FPGA/ASIC have been proposed
in recent years. In this paper, we provide a comprehensive survey of recent
advances in network acceleration, compression and accelerator design from both
algorithm and hardware points of view. Specifically, we provide a thorough
analysis of each of the following topics: network pruning, low-rank
approximation, network quantization, teacher-student networks, compact network
design and hardware accelerators. Finally, we will introduce and discuss a few
possible future directions.
</summary>
    <author>
      <name>Jian Cheng</name>
    </author>
    <author>
      <name>Peisong Wang</name>
    </author>
    <author>
      <name>Gang Li</name>
    </author>
    <author>
      <name>Qinghao Hu</name>
    </author>
    <author>
      <name>Hanqing Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.00939v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00939v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.03844v1</id>
    <updated>2015-12-11T22:47:34Z</updated>
    <published>2015-12-11T22:47:34Z</published>
    <title>Efficient Deep Feature Learning and Extraction via StochasticNets</title>
    <summary>  Deep neural networks are a powerful tool for feature learning and extraction
given their ability to model high-level abstractions in highly complex data.
One area worth exploring in feature learning and extraction using deep neural
networks is efficient neural connectivity formation for faster feature learning
and extraction. Motivated by findings of stochastic synaptic connectivity
formation in the brain as well as the brain's uncanny ability to efficiently
represent information, we propose the efficient learning and extraction of
features via StochasticNets, where sparsely-connected deep neural networks can
be formed via stochastic connectivity between neurons. To evaluate the
feasibility of such a deep neural network architecture for feature learning and
extraction, we train deep convolutional StochasticNets to learn abstract
features using the CIFAR-10 dataset, and extract the learned features from
images to perform classification on the SVHN and STL-10 datasets. Experimental
results show that features learned using deep convolutional StochasticNets,
with fewer neural connections than conventional deep convolutional neural
networks, can allow for better or comparable classification accuracy than
conventional deep neural networks: relative test error decrease of ~4.5% for
classification on the STL-10 dataset and ~1% for classification on the SVHN
dataset. Furthermore, it was shown that the deep features extracted using deep
convolutional StochasticNets can provide comparable classification accuracy
even when only 10% of the training data is used for feature learning. Finally,
it was also shown that significant gains in feature extraction speed can be
achieved in embedded applications using StochasticNets. As such, StochasticNets
allow for faster feature learning and extraction performance while facilitate
for better or comparable accuracy performances.
</summary>
    <author>
      <name>Mohammad Javad Shafiee</name>
    </author>
    <author>
      <name>Parthipan Siva</name>
    </author>
    <author>
      <name>Paul Fieguth</name>
    </author>
    <author>
      <name>Alexander Wong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages. arXiv admin note: substantial text overlap with
  arXiv:1508.05463</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.03844v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.03844v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.08306v1</id>
    <updated>2017-03-21T16:12:31Z</updated>
    <published>2017-03-21T16:12:31Z</published>
    <title>A Digital Neuromorphic Architecture Efficiently Facilitating Complex
  Synaptic Response Functions Applied to Liquid State Machines</title>
    <summary>  Information in neural networks is represented as weighted connections, or
synapses, between neurons. This poses a problem as the primary computational
bottleneck for neural networks is the vector-matrix multiply when inputs are
multiplied by the neural network weights. Conventional processing architectures
are not well suited for simulating neural networks, often requiring large
amounts of energy and time. Additionally, synapses in biological neural
networks are not binary connections, but exhibit a nonlinear response function
as neurotransmitters are emitted and diffuse between neurons. Inspired by
neuroscience principles, we present a digital neuromorphic architecture, the
Spiking Temporal Processing Unit (STPU), capable of modeling arbitrary complex
synaptic response functions without requiring additional hardware components.
We consider the paradigm of spiking neurons with temporally coded information
as opposed to non-spiking rate coded neurons used in most neural networks. In
this paradigm we examine liquid state machines applied to speech recognition
and show how a liquid state machine with temporal dynamics maps onto the
STPU-demonstrating the flexibility and efficiency of the STPU for instantiating
neural algorithms.
</summary>
    <author>
      <name>Michael R. Smith</name>
    </author>
    <author>
      <name>Aaron J. Hill</name>
    </author>
    <author>
      <name>Kristofor D. Carlson</name>
    </author>
    <author>
      <name>Craig M. Vineyard</name>
    </author>
    <author>
      <name>Jonathon Donaldson</name>
    </author>
    <author>
      <name>David R. Follett</name>
    </author>
    <author>
      <name>Pamela L. Follett</name>
    </author>
    <author>
      <name>John H. Naegle</name>
    </author>
    <author>
      <name>Conrad D. James</name>
    </author>
    <author>
      <name>James B. Aimone</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 Figures, Preprint of 2017 IJCNN</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.08306v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.08306v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0401035v1</id>
    <updated>2004-01-05T13:09:33Z</updated>
    <published>2004-01-05T13:09:33Z</published>
    <title>Ising Model on Edge-Dual of Random Networks</title>
    <summary>  We consider Ising model on edge-dual of uncorrelated random networks with
arbitrary degree distribution. These networks have a finite clustering in the
thermodynamic limit. High and low temperature expansions of Ising model on the
edge-dual of random networks are derived. A detailed comparison of the critical
behavior of Ising model on scale free random networks and their edge-dual is
presented.
</summary>
    <author>
      <name>A. Ramezanpour</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.69.066114</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.69.066114" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 4 figures, 1 table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 69, 066114 (2004)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0401035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0401035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0512639v1</id>
    <updated>2005-12-24T17:28:42Z</updated>
    <published>2005-12-24T17:28:42Z</published>
    <title>Crossovers in ScaleFree Networks on Geographical Space</title>
    <summary>  Complex networks are characterized by several topological properties: degree
distribution, clustering coefficient, average shortest path length, etc. Using
a simple model to generate scale-free networks embedded on geographical space,
we analyze the relationship between topological properties of the network and
attributes (fitness and location) of the vertices in the network. We find there
are two crossovers for varying the scaling exponent of the fitness
distribution.
</summary>
    <author>
      <name>Satoru Morita</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.73.035104</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.73.035104" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physical Review E 73, 035104R (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0512639v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0512639v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.0550v1</id>
    <updated>2007-10-02T14:55:35Z</updated>
    <published>2007-10-02T14:55:35Z</published>
    <title>Community Detection in Complex Networks by Dynamical Simplex Evolution</title>
    <summary>  We benchmark the dynamical simplex evolution (DSE) method with several of the
currently available algorithms to detect communities in complex networks by
comparing the fraction of correctly identified nodes for different levels of
``fuzziness'' of random networks composed of well defined communities. The
potential benefits of the DSE method to detect hierarchical sub structures in
complex networks are discussed.
</summary>
    <author>
      <name>V. Gudkov</name>
    </author>
    <author>
      <name>V. Montealegre</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.78.016113</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.78.016113" rel="related"/>
    <link href="http://arxiv.org/abs/0710.0550v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.0550v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.3180v1</id>
    <updated>2009-08-21T18:43:19Z</updated>
    <published>2009-08-21T18:43:19Z</published>
    <title>Assortativity in random line graphs</title>
    <summary>  We investigate the degree-degree correlations in the Erdos-Renyi networks,
the growing exponential networks and the scale-free networks. We demonstrate
that these correlations are the largest for the exponential networks. We
calculate also these correlations in the line graphs, formed from the
considered networks. Theoretical and numerical results indicate that all the
line graphs are assortative, i.e. the degree-degree correlation is positive.
</summary>
    <author>
      <name>Anna Manka-Krason</name>
    </author>
    <author>
      <name>Krzysztof Kulakowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0908.3180v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.3180v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.2369v1</id>
    <updated>2012-06-11T20:11:23Z</updated>
    <published>2012-06-11T20:11:23Z</published>
    <title>Networks in Motion</title>
    <summary>  Feature article on how networks that govern communication, growth, herd
behavior, and other key processes in nature and society are becoming
increasingly amenable to modeling, forecast, and control.
</summary>
    <author>
      <name>Adilson E. Motter</name>
    </author>
    <author>
      <name>Reka Albert</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/PT.3.1518</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/PT.3.1518" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Review of current research on network dynamics</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physics Today 65(4), 43-48 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1206.2369v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.2369v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/math-ph/0612022v1</id>
    <updated>2006-12-07T07:39:48Z</updated>
    <published>2006-12-07T07:39:48Z</published>
    <title>Random Recurrent Neural Networks Dynamics</title>
    <summary>  This paper is a review dealing with the study of large size random recurrent
neural networks. The connection weights are selected according to a probability
law and it is possible to predict the network dynamics at a macroscopic scale
using an averaging principle. After a first introductory section, the section 1
reviews the various models from the points of view of the single neuron
dynamics and of the global network dynamics. A summary of notations is
presented, which is quite helpful for the sequel. In section 2, mean-field
dynamics is developed.
  The probability distribution characterizing global dynamics is computed. In
section 3, some applications of mean-field theory to the prediction of chaotic
regime for Analog Formal Random Recurrent Neural Networks (AFRRNN) are
displayed. The case of AFRRNN with an homogeneous population of neurons is
studied in section 4. Then, a two-population model is studied in section 5. The
occurrence of a cyclo-stationary chaos is displayed using the results of
\cite{Dauce01}. In section 6, an insight of the application of mean-field
theory to IF networks is given using the results of \cite{BrunelHakim99}.
</summary>
    <author>
      <name>M. Samuelides</name>
    </author>
    <author>
      <name>B. Cessac</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Review paper, 36 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPJ Special Topics "Topics in Dynamical Neural Networks : From
  Large Scale Neural Networks to Motor Control and Vision", Vol. 142, Num. 1,
  89-122, (2007).</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/math-ph/0612022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/math-ph/0612022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.0305v3</id>
    <updated>2012-04-05T01:30:21Z</updated>
    <published>2011-04-02T09:10:42Z</published>
    <title>Dynamical Synapses Enhance Neural Information Processing: Gracefulness,
  Accuracy and Mobility</title>
    <summary>  Experimental data have revealed that neuronal connection efficacy exhibits
two forms of short-term plasticity, namely, short-term depression (STD) and
short-term facilitation (STF). They have time constants residing between fast
neural signaling and rapid learning, and may serve as substrates for neural
systems manipulating temporal information on relevant time scales. The present
study investigates the impact of STD and STF on the dynamics of continuous
attractor neural networks (CANNs) and their potential roles in neural
information processing. We find that STD endows the network with slow-decaying
plateau behaviors-the network that is initially being stimulated to an active
state decays to a silent state very slowly on the time scale of STD rather than
on the time scale of neural signaling. This provides a mechanism for neural
systems to hold sensory memory easily and shut off persistent activities
gracefully. With STF, we find that the network can hold a memory trace of
external inputs in the facilitated neuronal interactions, which provides a way
to stabilize the network response to noisy inputs, leading to improved accuracy
in population decoding. Furthermore, we find that STD increases the mobility of
the network states. The increased mobility enhances the tracking performance of
the network in response to time-varying stimuli, leading to anticipative neural
responses. In general, we find that STD and STP tend to have opposite effects
on network dynamics and complementary computational advantages, suggesting that
the brain may employ a strategy of weighting them differentially depending on
the computational purpose.
</summary>
    <author>
      <name>C. C. Alan Fung</name>
    </author>
    <author>
      <name>K. Y. Michael Wong</name>
    </author>
    <author>
      <name>He Wang</name>
    </author>
    <author>
      <name>Si Wu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/NECO_a_00269</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/NECO_a_00269" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages, 17 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Comput. 24 (2012) 1147-1185</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1104.0305v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.0305v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.01239v2</id>
    <updated>2017-12-31T14:53:00Z</updated>
    <published>2017-11-03T17:07:51Z</published>
    <title>Routing Networks: Adaptive Selection of Non-linear Functions for
  Multi-Task Learning</title>
    <summary>  Multi-task learning (MTL) with neural networks leverages commonalities in
tasks to improve performance, but often suffers from task interference which
reduces the benefits of transfer. To address this issue we introduce the
routing network paradigm, a novel neural network and training algorithm. A
routing network is a kind of self-organizing neural network consisting of two
components: a router and a set of one or more function blocks. A function block
may be any neural network - for example a fully-connected or a convolutional
layer. Given an input the router makes a routing decision, choosing a function
block to apply and passing the output back to the router recursively,
terminating when a fixed recursion depth is reached. In this way the routing
network dynamically composes different function blocks for each input. We
employ a collaborative multi-agent reinforcement learning (MARL) approach to
jointly train the router and function blocks. We evaluate our model against
cross-stitch networks and shared-layer baselines on multi-task settings of the
MNIST, mini-imagenet, and CIFAR-100 datasets. Our experiments demonstrate a
significant improvement in accuracy, with sharper convergence. In addition,
routing networks have nearly constant per-task training cost while cross-stitch
networks scale linearly with the number of tasks. On CIFAR-100 (20 tasks) we
obtain cross-stitch performance levels with an 85% reduction in training time.
</summary>
    <author>
      <name>Clemens Rosenbaum</name>
    </author>
    <author>
      <name>Tim Klinger</name>
    </author>
    <author>
      <name>Matthew Riemer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under Review at ICLR 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.01239v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.01239v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.05878v1</id>
    <updated>2018-01-17T22:34:50Z</updated>
    <published>2018-01-17T22:34:50Z</published>
    <title>Threshold of front propagation in neural fields: An interface dynamics
  approach</title>
    <summary>  Neural field equations model population dynamics of large-scale networks of
neurons. Wave propagation in neural fields is often studied by constructing
traveling wave solutions in the wave coordinate frame. Nonequilibrium dynamics
are more challenging to study, due to the nonlinearity and nonlocality of
neural fields, whose interactions are described by the kernel of an integral
term. Here, we leverage interface methods to describe the threshold of wave
initiation away from equilibrium. In particular, we focus on traveling front
initiation in an excitatory neural field. In a neural field with a Heaviside
firing rate, neural activity can be described by the dynamics of the
interfaces, where the neural activity is at the firing threshold. This allows
us to derive conditions for the portion of the neural field that must be
activated for traveling fronts to be initiated in a purely excitatory neural
field. Explicit equations are possible for a single active (superthreshold)
region, and special cases of multiple disconnected active regions. The dynamic
spreading speed of the excited region can also be approximated asymptotically.
We also discuss extensions to the problem of finding the critical
spatiotemporal input needed to initiate waves.
</summary>
    <author>
      <name>Gregory Faye</name>
    </author>
    <author>
      <name>Zachary P Kilpatrick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.05878v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.05878v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.PS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.PS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0201366v1</id>
    <updated>2002-01-21T10:09:12Z</updated>
    <published>2002-01-21T10:09:12Z</published>
    <title>Rigidity transitions and constraint counting in amorphous networks:
  beyond the mean-field approach</title>
    <summary>  Subj-class: Disordered Systems and Neural Networks
</summary>
    <author>
      <name>Matthieu Micoulaut</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University Paris 6</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1209/epl/i2002-00449-1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1209/epl/i2002-00449-1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, revtex, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0201366v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0201366v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0509055v1</id>
    <updated>2005-09-02T10:20:28Z</updated>
    <published>2005-09-02T10:20:28Z</published>
    <title>Synchronization in Complex Networks: a Reply on a recent Comment</title>
    <summary>  We clarify a number of points raised in [Matias, arXiv:cond-mat/0507471v2
(2005)].
</summary>
    <author>
      <name>S. Boccaletti</name>
    </author>
    <author>
      <name>M. Chavez</name>
    </author>
    <author>
      <name>A. Amann</name>
    </author>
    <author>
      <name>D. -U. Hwang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages no figues</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0509055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0509055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.4983v1</id>
    <updated>2010-09-25T07:00:25Z</updated>
    <published>2010-09-25T07:00:25Z</published>
    <title>Pattern Classification using Simplified Neural Networks</title>
    <summary>  In recent years, many neural network models have been proposed for pattern
classification, function approximation and regression problems. This paper
presents an approach for classifying patterns from simplified NNs. Although the
predictive accuracy of ANNs is often higher than that of other methods or human
experts, it is often said that ANNs are practically "black boxes", due to the
complexity of the networks. In this paper, we have an attempted to open up
these black boxes by reducing the complexity of the network. The factor makes
this possible is the pruning algorithm. By eliminating redundant weights,
redundant input and hidden units are identified and removed from the network.
Using the pruning algorithm, we have been able to prune networks such that only
a few input units, hidden units and connections left yield a simplified
network. Experimental results on several benchmarks problems in neural networks
show the effectiveness of the proposed approach with good generalization
ability.
</summary>
    <author>
      <name>S. M. Kamruzzaman</name>
    </author>
    <author>
      <name>Ahmed Ryadh Hasan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 Pages, International Conference</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. International Conference on Information and Communication
  Technology in Management (ICTM 2005), Multimedia University, Malaysia, May
  2005</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.4983v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.4983v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.3804v1</id>
    <updated>2012-11-16T06:30:55Z</updated>
    <published>2012-11-16T06:30:55Z</published>
    <title>Neural networks using two-component Bose-Einstein condensates</title>
    <summary>  The authors previously considered a method solving optimization problems by
using a system of interconnected network of two component Bose-Einstein
condensates (Byrnes, Yan, Yamamoto New J. Phys. 13, 113025 (2011)). The use of
bosonic particles was found to give a reduced time proportional to the number
of bosons N for solving Ising model Hamiltonians by taking advantage of
enhanced bosonic cooling rates. In this paper we consider the same system in
terms of neural networks. We find that up to the accelerated cooling of the
bosons the previously proposed system is equivalent to a stochastic continuous
Hopfield network. This makes it clear that the BEC network is a physical
realization of a simulated annealing algorithm, with an additional speedup due
to bosonic enhancement. We discuss the BEC network in terms of typical neural
network tasks such as learning and pattern recognition and find that the latter
process may be accelerated by a factor of N.
</summary>
    <author>
      <name>Tim Byrnes</name>
    </author>
    <author>
      <name>Shinsuke Koyama</name>
    </author>
    <author>
      <name>Kai Yan</name>
    </author>
    <author>
      <name>Yoshihisa Yamamoto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Scientific Reports 3, 2531 (2013)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1211.3804v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.3804v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.02777v1</id>
    <updated>2015-03-10T05:48:52Z</updated>
    <published>2015-03-10T05:48:52Z</published>
    <title>Rhythmic inhibition allows neural networks to search for maximally
  consistent states</title>
    <summary>  Gamma-band rhythmic inhibition is a ubiquitous phenomenon in neural circuits
yet its computational role still remains elusive. We show that a model of
Gamma-band rhythmic inhibition allows networks of coupled cortical circuit
motifs to search for network configurations that best reconcile external inputs
with an internal consistency model encoded in the network connectivity. We show
that Hebbian plasticity allows the networks to learn the consistency model by
example. The search dynamics driven by rhythmic inhibition enable the described
networks to solve difficult constraint satisfaction problems without making
assumptions about the form of stochastic fluctuations in the network. We show
that the search dynamics are well approximated by a stochastic sampling
process. We use the described networks to reproduce perceptual multi-stability
phenomena with switching times that are a good match to experimental data and
show that they provide a general neural framework which can be used to model
other 'perceptual inference' phenomena.
</summary>
    <author>
      <name>Hesham Mostafa</name>
    </author>
    <author>
      <name>Lorenz K. Muller</name>
    </author>
    <author>
      <name>Giacomo Indiveri</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/NECO_a_00785</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/NECO_a_00785" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Computation 27:12, (2015), pg. 2510-2547</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1503.02777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.02777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07152v3</id>
    <updated>2017-06-14T17:59:12Z</updated>
    <published>2016-09-22T20:10:57Z</published>
    <title>Input Convex Neural Networks</title>
    <summary>  This paper presents the input convex neural network architecture. These are
scalar-valued (potentially deep) neural networks with constraints on the
network parameters such that the output of the network is a convex function of
(some of) the inputs. The networks allow for efficient inference via
optimization over some inputs to the network given others, and can be applied
to settings including structured prediction, data imputation, reinforcement
learning, and others. In this paper we lay the basic groundwork for these
models, proposing methods for inference, optimization and learning, and analyze
their representational power. We show that many existing neural network
architectures can be made input-convex with a minor modification, and develop
specialized optimization algorithms tailored to this setting. Finally, we
highlight the performance of the methods on multi-label prediction, image
completion, and reinforcement learning problems, where we show improvement over
the existing state of the art in many cases.
</summary>
    <author>
      <name>Brandon Amos</name>
    </author>
    <author>
      <name>Lei Xu</name>
    </author>
    <author>
      <name>J. Zico Kolter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.07152v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07152v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.04494v1</id>
    <updated>2016-02-07T19:28:08Z</updated>
    <published>2016-02-07T19:28:08Z</published>
    <title>Localization for Wireless Sensor Networks: A Neural Network Approach</title>
    <summary>  As Wireless Sensor Networks are penetrating into the industrial domain, many
research opportunities are emerging. One such essential and challenging
application is that of node localization. A feed-forward neural network based
methodology is adopted in this paper. The Received Signal Strength Indicator
(RSSI) values of the anchor node beacons are used. The number of anchor nodes
and their configurations has an impact on the accuracy of the localization
system, which is also addressed in this paper. Five different training
algorithms are evaluated to find the training algorithm that gives the best
result. The multi-layer Perceptron (MLP) neural network model was trained using
Matlab. In order to evaluate the performance of the proposed method in real
time, the model obtained was then implemented on the Arduino microcontroller.
With four anchor nodes, an average 2D localization error of 0.2953 m has been
achieved with a 12-12-2 neural network structure. The proposed method can also
be implemented on any other embedded microcontroller system.
</summary>
    <author>
      <name>Shiu Kumar</name>
    </author>
    <author>
      <name>Ronesh Sharma</name>
    </author>
    <author>
      <name>Edwin Vans</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 7 figures, 1 table, IJCNC</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Networks &amp; Communications
  (IJCNC), vol. 8, pp. 61-71, January 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.04494v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.04494v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.05141v1</id>
    <updated>2016-11-16T04:32:22Z</updated>
    <published>2016-11-16T04:32:22Z</published>
    <title>Training Spiking Deep Networks for Neuromorphic Hardware</title>
    <summary>  We describe a method to train spiking deep networks that can be run using
leaky integrate-and-fire (LIF) neurons, achieving state-of-the-art results for
spiking LIF networks on five datasets, including the large ImageNet ILSVRC-2012
benchmark. Our method for transforming deep artificial neural networks into
spiking networks is scalable and works with a wide range of neural
nonlinearities. We achieve these results by softening the neural response
function, such that its derivative remains bounded, and by training the network
with noise to provide robustness against the variability introduced by spikes.
Our analysis shows that implementations of these networks on neuromorphic
hardware will be many times more power-efficient than the equivalent
non-spiking networks on traditional hardware.
</summary>
    <author>
      <name>Eric Hunsberger</name>
    </author>
    <author>
      <name>Chris Eliasmith</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.2.10967.06566</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.2.10967.06566" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 figures, 4 tables; the "methods" section of this article
  draws heavily on arXiv:1510.08829</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.05141v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.05141v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.08083v1</id>
    <updated>2016-11-24T07:09:24Z</updated>
    <published>2016-11-24T07:09:24Z</published>
    <title>Survey of Expressivity in Deep Neural Networks</title>
    <summary>  We survey results on neural network expressivity described in "On the
Expressive Power of Deep Neural Networks". The paper motivates and develops
three natural measures of expressiveness, which all display an exponential
dependence on the depth of the network. In fact, all of these measures are
related to a fourth quantity, trajectory length. This quantity grows
exponentially in the depth of the network, and is responsible for the depth
sensitivity observed. These results translate to consequences for networks
during and after training. They suggest that parameters earlier in a network
have greater influence on its expressive power -- in particular, given a layer,
its influence on expressivity is determined by the remaining depth of the
network after that layer. This is verified with experiments on MNIST and
CIFAR-10. We also explore the effect of training on the input-output map, and
find that it trades off between the stability and expressivity.
</summary>
    <author>
      <name>Maithra Raghu</name>
    </author>
    <author>
      <name>Ben Poole</name>
    </author>
    <author>
      <name>Jon Kleinberg</name>
    </author>
    <author>
      <name>Surya Ganguli</name>
    </author>
    <author>
      <name>Jascha Sohl-Dickstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at NIPS 2016 Workshop on Interpretable Machine Learning in
  Complex Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.08083v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.08083v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05130v1</id>
    <updated>2017-01-18T16:17:35Z</updated>
    <published>2017-01-18T16:17:35Z</published>
    <title>On the Performance of Network Parallel Training in Artificial Neural
  Networks</title>
    <summary>  Artificial Neural Networks (ANNs) have received increasing attention in
recent years with applications that span a wide range of disciplines including
vital domains such as medicine, network security and autonomous transportation.
However, neural network architectures are becoming increasingly complex and
with an increasing need to obtain real-time results from such models, it has
become pivotal to use parallelization as a mechanism for speeding up network
training and deployment. In this work we propose an implementation of Network
Parallel Training through Cannon's Algorithm for matrix multiplication. We show
that increasing the number of processes speeds up training until the point
where process communication costs become prohibitive; this point varies by
network complexity. We also show through empirical efficiency calculations that
the speedup obtained is superlinear.
</summary>
    <author>
      <name>Ludvig Ericson</name>
    </author>
    <author>
      <name>Rendani Mbuvha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 Pages, 4 Figures, 1 Table</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.05130v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05130v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.00887v3</id>
    <updated>2017-02-16T17:52:03Z</updated>
    <published>2017-02-03T01:40:45Z</published>
    <title>Structured Attention Networks</title>
    <summary>  Attention networks have proven to be an effective approach for embedding
categorical inference within a deep neural network. However, for many tasks we
may want to model richer structural dependencies without abandoning end-to-end
training. In this work, we experiment with incorporating richer structural
distributions, encoded using graphical models, within deep networks. We show
that these structured attention networks are simple extensions of the basic
attention procedure, and that they allow for extending attention beyond the
standard soft-selection approach, such as attending to partial segmentations or
to subtrees. We experiment with two different classes of structured attention
networks: a linear-chain conditional random field and a graph-based parsing
model, and describe how these models can be practically implemented as neural
network layers. Experiments show that this approach is effective for
incorporating structural biases, and structured attention networks outperform
baseline attention models on a variety of synthetic and real tasks: tree
transduction, neural machine translation, question answering, and natural
language inference. We further find that models trained in this way learn
interesting unsupervised hidden representations that generalize simple
attention.
</summary>
    <author>
      <name>Yoon Kim</name>
    </author>
    <author>
      <name>Carl Denton</name>
    </author>
    <author>
      <name>Luong Hoang</name>
    </author>
    <author>
      <name>Alexander M. Rush</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.00887v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.00887v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.04698v2</id>
    <updated>2017-06-19T22:11:20Z</updated>
    <published>2017-06-14T23:56:57Z</published>
    <title>Gradient Descent for Spiking Neural Networks</title>
    <summary>  Much of studies on neural computation are based on network models of static
neurons that produce analog output, despite the fact that information
processing in the brain is predominantly carried out by dynamic neurons that
produce discrete pulses called spikes. Research in spike-based computation has
been impeded by the lack of efficient supervised learning algorithm for spiking
networks. Here, we present a gradient descent method for optimizing spiking
network models by introducing a differentiable formulation of spiking networks
and deriving the exact gradient calculation. For demonstration, we trained
recurrent spiking networks on two dynamic tasks: one that requires optimizing
fast (~millisecond) spike-based interactions for efficient encoding of
information, and a delayed memory XOR task over extended duration (~second).
The results show that our method indeed optimizes the spiking network dynamics
on the time scale of individual spikes as well as behavioral time scales. In
conclusion, our result offers a general purpose supervised learning algorithm
for spiking neural networks, thus advancing further investigations on
spike-based computation.
</summary>
    <author>
      <name>Dongsung Huh</name>
    </author>
    <author>
      <name>Terrence J. Sejnowski</name>
    </author>
    <link href="http://arxiv.org/abs/1706.04698v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.04698v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.02469v1</id>
    <updated>2017-07-08T17:17:29Z</updated>
    <published>2017-07-08T17:17:29Z</published>
    <title>Tailoring Artificial Neural Networks for Optimal Learning</title>
    <summary>  As one of the most important paradigms of recurrent neural networks, the echo
state network (ESN) has been applied to a wide range of fields, from robotics
to medicine to finance, and language processing. A key feature of the ESN
paradigm is its reservoir ---a directed and weighted network--- that represents
the connections between neurons and projects the input signals into a high
dimensional space. Despite extensive studies, the impact of the reservoir
network on the ESN performance remains unclear. Here we systematically address
this fundamental question. Through spectral analysis of the reservoir network
we reveal a key factor that largely determines the ESN memory capacity and
hence affects its performance. Moreover, we find that adding short loops to the
reservoir network can tailor ESN for specific tasks and optimal learning. We
validate our findings by applying ESN to forecast both synthetic and real
benchmark time series. Our results provide a new way to design task-specific
recurrent neural networks, as well as new insights in understanding complex
networked systems.
</summary>
    <author>
      <name>Pau Vilimelis Aceituno</name>
    </author>
    <author>
      <name>Yan Gang</name>
    </author>
    <author>
      <name>Yang-Yu Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.02469v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.02469v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.08831v1</id>
    <updated>2017-07-27T12:22:34Z</updated>
    <published>2017-07-27T12:22:34Z</published>
    <title>STN-OCR: A single Neural Network for Text Detection and Text Recognition</title>
    <summary>  Detecting and recognizing text in natural scene images is a challenging, yet
not completely solved task. In re- cent years several new systems that try to
solve at least one of the two sub-tasks (text detection and text recognition)
have been proposed. In this paper we present STN-OCR, a step towards
semi-supervised neural networks for scene text recognition, that can be
optimized end-to-end. In contrast to most existing works that consist of
multiple deep neural networks and several pre-processing steps we propose to
use a single deep neural network that learns to detect and recognize text from
natural images in a semi-supervised way. STN-OCR is a network that integrates
and jointly learns a spatial transformer network, that can learn to detect text
regions in an image, and a text recognition network that takes the identified
text regions and recognizes their textual content. We investigate how our model
behaves on a range of different tasks (detection and recognition of characters,
and lines of text). Experimental results on public benchmark datasets show the
ability of our model to handle a variety of different tasks, without
substantial changes in its overall network structure.
</summary>
    <author>
      <name>Christian Bartz</name>
    </author>
    <author>
      <name>Haojin Yang</name>
    </author>
    <author>
      <name>Christoph Meinel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.08831v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.08831v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.00855v1</id>
    <updated>2017-08-03T14:38:30Z</updated>
    <published>2017-08-03T14:38:30Z</published>
    <title>A Deep Convolutional Neural Network to Analyze Position Averaged
  Convergent Beam Electron Diffraction Patterns</title>
    <summary>  We establish a series of deep convolutional neural networks to automatically
analyze position averaged convergent beam electron diffraction patterns. The
networks first calibrate the zero-order disk size, center position, and
rotation without the need for pretreating the data. With the aligned data,
additional networks then measure the sample thickness and tilt. The performance
of the network is explored as a function of a variety of variables including
thickness, tilt, and dose. A methodology to explore the response of the neural
network to various pattern features is also presented. Processing patterns at a
rate of $\sim$0.1 s/pattern, the network is shown to be orders of magnitude
faster than a brute force method while maintaining accuracy. The approach is
thus suitable for automatically processing big, 4D STEM data. We also discuss
the generality of the method to other materials/orientations as well as a
hybrid approach that combines the features of the neural network with least
squares fitting for even more robust analysis. The source code is available at
https://github.com/subangstrom/DeepDiffraction.
</summary>
    <author>
      <name>Weizong Xu</name>
    </author>
    <author>
      <name>James M. LeBeau</name>
    </author>
    <link href="http://arxiv.org/abs/1708.00855v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00855v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.05136v4</id>
    <updated>2018-02-05T11:01:41Z</updated>
    <published>2017-11-14T15:02:47Z</published>
    <title>Deep Rewiring: Training very sparse deep networks</title>
    <summary>  Neuromorphic hardware tends to pose limits on the connectivity of deep
networks that one can run on them. But also generic hardware and software
implementations of deep learning run more efficiently for sparse networks.
Several methods exist for pruning connections of a neural network after it was
trained without connectivity constraints. We present an algorithm, DEEP R, that
enables us to train directly a sparsely connected neural network. DEEP R
automatically rewires the network during supervised training so that
connections are there where they are most needed for the task, while its total
number is all the time strictly bounded. We demonstrate that DEEP R can be used
to train very sparse feedforward and recurrent neural networks on standard
benchmark tasks with just a minor loss in performance. DEEP R is based on a
rigorous theoretical foundation that views rewiring as stochastic sampling of
network configurations from a posterior.
</summary>
    <author>
      <name>Guillaume Bellec</name>
    </author>
    <author>
      <name>David Kappel</name>
    </author>
    <author>
      <name>Wolfgang Maass</name>
    </author>
    <author>
      <name>Robert Legenstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at ICLR 2018. 10 pages (12 with references,
  24 with appendix), 4 Figures in the main text</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.05136v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.05136v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.07019v1</id>
    <updated>2017-11-16T23:36:59Z</updated>
    <published>2017-11-16T23:36:59Z</published>
    <title>PSO-Optimized Hopfield Neural Network-Based Multipath Routing for Mobile
  Ad-hoc Networks</title>
    <summary>  Mobile ad-hoc network (MANET) is a dynamic collection of mobile computers
without the need for any existing infrastructure. Nodes in a MANET act as hosts
and routers. Designing of robust routing algorithms for MANETs is a challenging
task. Disjoint multipath routing protocols address this problem and increase
the reliability, security and lifetime of network. However, selecting an
optimal multipath is an NP-complete problem. In this paper, Hopfield neural
network (HNN) which its parameters are optimized by particle swarm optimization
(PSO) algorithm is proposed as multipath routing algorithm. Link expiration
time (LET) between each two nodes is used as the link reliability estimation
metric. This approach can find either node-disjoint or link-disjoint paths in
single phase route discovery. Simulation results confirm that PSO-HNN routing
algorithm has better performance as compared to backup path set selection
algorithm (BPSA) in terms of the path set reliability and number of paths in
the set.
</summary>
    <author>
      <name>Mansour Sheikhan</name>
    </author>
    <author>
      <name>Ehsan Hemmati</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/18756891.2012.696921</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/18756891.2012.696921" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Mobile ad-hoc networks; Reliability; Multipath routing; Neural
  networks; Particle swarm optimization (PSO)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computational Intelligence Systems, Year
  2012, Volume 5, Number 3, Pages 568-581</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1712.07019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.07019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0001247v1</id>
    <updated>2000-01-18T14:55:13Z</updated>
    <published>2000-01-18T14:55:13Z</published>
    <title>On the problem of neural network decomposition into some subnets</title>
    <summary>  An artificial neural network is usually treated as a whole system, being
characterized by its ground state (the global minimum of the energy
functional), the set of fixed points, their basins of attraction, etc. However,
it is quite natural to suppose that a large network may consist of a set of
almost autonome subnets. Each subnet works independently (or almost
independently) and analyzes the same pattern from other points of view. It
seems that it is a proper model for the natural neural networks. We discuss the
problem of decomposition of a neural network into a set of weakly coupled
subnets. The used technique is similar to the method for {\it the extremal
grouping of parameters}, proposed by E.M.Braverman (1970).
</summary>
    <author>
      <name>Leonid B. Litinskii</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Institute of High Pressure Physics Russian Academy of Sciences</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">One old paper, 10 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Mathematical Modelling (1996), v.8, pp. 119-127 (in russian)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0001247v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0001247v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.1680v1</id>
    <updated>2007-05-11T15:55:31Z</updated>
    <published>2007-05-11T15:55:31Z</published>
    <title>Option Pricing Using Bayesian Neural Networks</title>
    <summary>  Options have provided a field of much study because of the complexity
involved in pricing them. The Black-Scholes equations were developed to price
options but they are only valid for European styled options. There is added
complexity when trying to price American styled options and this is why the use
of neural networks has been proposed. Neural Networks are able to predict
outcomes based on past data. The inputs to the networks here are stock
volatility, strike price and time to maturity with the output of the network
being the call option price. There are two techniques for Bayesian neural
networks used. One is Automatic Relevance Determination (for Gaussian
Approximation) and one is a Hybrid Monte Carlo method, both used with
Multi-Layer Perceptrons.
</summary>
    <author>
      <name>Michael Maio Pires</name>
    </author>
    <author>
      <name>Tshilidzi Marwala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0705.1680v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.1680v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.5087v1</id>
    <updated>2008-09-29T23:00:22Z</updated>
    <published>2008-09-29T23:00:22Z</published>
    <title>Hybrid Neural Network Architecture for On-Line Learning</title>
    <summary>  Approaches to machine intelligence based on brain models have stressed the
use of neural networks for generalization. Here we propose the use of a hybrid
neural network architecture that uses two kind of neural networks
simultaneously: (i) a surface learning agent that quickly adapt to new modes of
operation; and, (ii) a deep learning agent that is very accurate within a
specific regime of operation. The two networks of the hybrid architecture
perform complementary functions that improve the overall performance. The
performance of the hybrid architecture has been compared with that of
back-propagation perceptrons and the CC and FC networks for chaotic time-series
prediction, the CATS benchmark test, and smooth function approximation. It has
been shown that the hybrid architecture provides a superior performance based
on the RMS error criterion.
</summary>
    <author>
      <name>Yuhua Chen</name>
    </author>
    <author>
      <name>Subhash Kak</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 16 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0809.5087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.5087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.5430v6</id>
    <updated>2012-04-10T02:46:51Z</updated>
    <published>2010-05-29T05:38:41Z</published>
    <title>Modeling Quantum Mechanical Observers via Neural-Glial Networks</title>
    <summary>  We investigate the theory of observers in the quantum mechanical world by
using a novel model of the human brain which incorporates the glial network
into the Hopfield model of the neural network. Our model is based on a
microscopic construction of a quantum Hamiltonian of the synaptic junctions.
Using the Eguchi-Kawai large N reduction, we show that, when the number of
neurons and astrocytes is exponentially large, the degrees of freedom of the
dynamics of the neural and glial networks can be completely removed and,
consequently, that the retention time of the superposition of the wave
functions in the brain is as long as that of the microscopic quantum system of
pre-synaptics sites. Based on this model, the classical information entropy of
the neural-glial network is introduced. Using this quantity, we propose a
criterion for the brain to be a quantum mechanical observer.
</summary>
    <author>
      <name>Eiji Konishi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, published version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Int.J.Mod.Phys.B.26:1250060,2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.5430v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.5430v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.2290v2</id>
    <updated>2010-11-22T03:56:11Z</updated>
    <published>2010-09-13T03:25:55Z</published>
    <title>Attractor Dynamics with Synaptic Depression</title>
    <summary>  Neuronal connection weights exhibit short-term depression (STD). The present
study investigates the impact of STD on the dynamics of a continuous attractor
neural network (CANN) and its potential roles in neural information processing.
We find that the network with STD can generate both static and traveling bumps,
and STD enhances the performance of the network in tracking external inputs. In
particular, we find that STD endows the network with slow-decaying plateau
behaviors, namely, the network being initially stimulated to an active state
will decay to silence very slowly in the time scale of STD rather than that of
neural signaling. We argue that this provides a mechanism for neural systems to
hold short-term memory easily and shut off persistent activities naturally.
</summary>
    <author>
      <name>C. C. Alan Fung</name>
    </author>
    <author>
      <name>K. Y. Michael Wong</name>
    </author>
    <author>
      <name>He Wang</name>
    </author>
    <author>
      <name>Si Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 8 figures. This article has been accepted by NIPS with a
  poster spotlight presentation at the conference</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in NIPS vol. 23 (2010)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.2290v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.2290v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.1813v1</id>
    <updated>2010-12-08T17:33:04Z</updated>
    <published>2010-12-08T17:33:04Z</published>
    <title>Enhancing neural-network performance via assortativity</title>
    <summary>  The performance of attractor neural networks has been shown to depend
crucially on the heterogeneity of the underlying topology. We take this
analysis a step further by examining the effect of degree-degree correlations
-- or assortativity -- on neural-network behavior. We make use of a method
recently put forward for studying correlated networks and dynamics thereon,
both analytically and computationally, which is independent of how the topology
may have evolved. We show how the robustness to noise is greatly enhanced in
assortative (positively correlated) neural networks, especially if it is the
hub neurons that store the information.
</summary>
    <author>
      <name>Sebastiano de Franciscis</name>
    </author>
    <author>
      <name>Samuel Johnson</name>
    </author>
    <author>
      <name>Joaqu√≠n J. Torres</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.83.036114</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.83.036114" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1012.1813v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.1813v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.6276v1</id>
    <updated>2012-12-26T22:31:13Z</updated>
    <published>2012-12-26T22:31:13Z</published>
    <title>Echo State Queueing Network: a new reservoir computing learning tool</title>
    <summary>  In the last decade, a new computational paradigm was introduced in the field
of Machine Learning, under the name of Reservoir Computing (RC). RC models are
neural networks which a recurrent part (the reservoir) that does not
participate in the learning process, and the rest of the system where no
recurrence (no neural circuit) occurs. This approach has grown rapidly due to
its success in solving learning tasks and other computational applications.
Some success was also observed with another recently proposed neural network
designed using Queueing Theory, the Random Neural Network (RandNN). Both
approaches have good properties and identified drawbacks. In this paper, we
propose a new RC model called Echo State Queueing Network (ESQN), where we use
ideas coming from RandNNs for the design of the reservoir. ESQNs consist in
ESNs where the reservoir has a new dynamics inspired by recurrent RandNNs. The
paper positions ESQNs in the global Machine Learning area, and provides
examples of their use and performances. We show on largely used benchmarks that
ESQNs are very accurate tools, and we illustrate how they compare with standard
ESNs.
</summary>
    <author>
      <name>Sebasti√°n Basterrech</name>
    </author>
    <author>
      <name>Gerardo Rubino</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CCNC.2013.6488435</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CCNC.2013.6488435" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 10th IEEE Consumer Communications and Networking
  Conference (CCNC), Las Vegas, USA, 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.6276v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.6276v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92B20, 90B22, 90B20, 37M10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2; D.4.8; F.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.0214v1</id>
    <updated>2011-10-02T18:59:42Z</updated>
    <published>2011-10-02T18:59:42Z</published>
    <title>Eclectic Extraction of Propositional Rules from Neural Networks</title>
    <summary>  Artificial Neural Network is among the most popular algorithm for supervised
learning. However, Neural Networks have a well-known drawback of being a "Black
Box" learner that is not comprehensible to the Users. This lack of transparency
makes it unsuitable for many high risk tasks such as medical diagnosis that
requires a rational justification for making a decision. Rule Extraction
methods attempt to curb this limitation by extracting comprehensible rules from
a trained Network. Many such extraction algorithms have been developed over the
years with their respective strengths and weaknesses. They have been broadly
categorized into three types based on their approach to use internal model of
the Network. Eclectic Methods are hybrid algorithms that combine the other
approaches to attain more performance. In this paper, we present an Eclectic
method called HERETIC. Our algorithm uses Inductive Decision Tree learning
combined with information of the neural network structure for extracting
logical rules. Experiments and theoretical analysis show HERETIC to be better
in terms of speed and performance.
</summary>
    <author>
      <name>Ridwan Al Iqbal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICCIT 2011, Dhaka, Bangladesh</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.0214v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.0214v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.2104v1</id>
    <updated>2012-10-07T21:02:16Z</updated>
    <published>2012-10-07T21:02:16Z</published>
    <title>Complexity without chaos: Plasticity within random recurrent networks
  generates robust timing and motor control</title>
    <summary>  It is widely accepted that the complex dynamics characteristic of recurrent
neural circuits contributes in a fundamental manner to brain function. Progress
has been slow in understanding and exploiting the computational power of
recurrent dynamics for two main reasons: nonlinear recurrent networks often
exhibit chaotic behavior and most known learning rules do not work in robust
fashion in recurrent networks. Here we address both these problems by
demonstrating how random recurrent networks (RRN) that initially exhibit
chaotic dynamics can be tuned through a supervised learning rule to generate
locally stable neural patterns of activity that are both complex and robust to
noise. The outcome is a novel neural network regime that exhibits both
transiently stable and chaotic trajectories. We further show that the recurrent
learning rule dramatically increases the ability of RRNs to generate complex
spatiotemporal motor patterns, and accounts for recent experimental data
showing a decrease in neural variability in response to stimulus onset.
</summary>
    <author>
      <name>Rodrigo Laje</name>
    </author>
    <author>
      <name>Dean V. Buonomano</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/nn.3405</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/nn.3405" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Nat. Neurosci. 16 (2013) 925-933</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1210.2104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.2104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.05926v1</id>
    <updated>2015-11-18T20:17:39Z</updated>
    <published>2015-11-18T20:17:39Z</published>
    <title>Combining Neural Networks and Log-linear Models to Improve Relation
  Extraction</title>
    <summary>  The last decade has witnessed the success of the traditional feature-based
method on exploiting the discrete structures such as words or lexical patterns
to extract relations from text. Recently, convolutional and recurrent neural
networks has provided very effective mechanisms to capture the hidden
structures within sentences via continuous representations, thereby
significantly advancing the performance of relation extraction. The advantage
of convolutional neural networks is their capacity to generalize the
consecutive k-grams in the sentences while recurrent neural networks are
effective to encode long ranges of sentence context. This paper proposes to
combine the traditional feature-based method, the convolutional and recurrent
neural networks to simultaneously benefit from their advantages. Our systematic
evaluation of different network architectures and combination methods
demonstrates the effectiveness of this approach and results in the
state-of-the-art performance on the ACE 2005 and SemEval dataset.
</summary>
    <author>
      <name>Thien Huu Nguyen</name>
    </author>
    <author>
      <name>Ralph Grishman</name>
    </author>
    <link href="http://arxiv.org/abs/1511.05926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.05926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07176v1</id>
    <updated>2016-04-25T09:17:18Z</updated>
    <published>2016-04-25T09:17:18Z</published>
    <title>Protein Secondary Structure Prediction Using Cascaded Convolutional and
  Recurrent Neural Networks</title>
    <summary>  Protein secondary structure prediction is an important problem in
bioinformatics. Inspired by the recent successes of deep neural networks, in
this paper, we propose an end-to-end deep network that predicts protein
secondary structures from integrated local and global contextual features. Our
deep architecture leverages convolutional neural networks with different kernel
sizes to extract multiscale local contextual features. In addition, considering
long-range dependencies existing in amino acid sequences, we set up a
bidirectional neural network consisting of gated recurrent unit to capture
global contextual features. Furthermore, multi-task learning is utilized to
predict secondary structure labels and amino-acid solvent accessibility
simultaneously. Our proposed deep network demonstrates its effectiveness by
achieving state-of-the-art performance, i.e., 69.7% Q8 accuracy on the public
benchmark CB513, 76.9% Q8 accuracy on CASP10 and 73.1% Q8 accuracy on CASP11.
Our model and results are publicly available.
</summary>
    <author>
      <name>Zhen Li</name>
    </author>
    <author>
      <name>Yizhou Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures, Accepted by International Joint Conferences on
  Artificial Intelligence (IJCAI)</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.07176v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07176v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.BM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.BM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08128v3</id>
    <updated>2017-03-02T23:07:00Z</updated>
    <published>2016-08-29T16:14:52Z</published>
    <title>Temporal Activity Detection in Untrimmed Videos with Recurrent Neural
  Networks</title>
    <summary>  This thesis explore different approaches using Convolutional and Recurrent
Neural Networks to classify and temporally localize activities on videos,
furthermore an implementation to achieve it has been proposed. As the first
step, features have been extracted from video frames using an state of the art
3D Convolutional Neural Network. This features are fed in a recurrent neural
network that solves the activity classification and temporally location tasks
in a simple and flexible way. Different architectures and configurations have
been tested in order to achieve the best performance and learning of the video
dataset provided. In addition it has been studied different kind of post
processing over the trained network's output to achieve a better results on the
temporally localization of activities on the videos. The results provided by
the neural network developed in this thesis have been submitted to the
ActivityNet Challenge 2016 of the CVPR, achieving competitive results using a
simple and flexible architecture.
</summary>
    <author>
      <name>Alberto Montes</name>
    </author>
    <author>
      <name>Amaia Salvador</name>
    </author>
    <author>
      <name>Santiago Pascual</name>
    </author>
    <author>
      <name>Xavier Giro-i-Nieto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Best Poster Award at the 1st NIPS Workshop on Large Scale Computer
  Vision Systems (Barcelona, December 2016). Source code available at
  https://imatge-upc.github.io/activitynet-2016-cvprw/</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.08128v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08128v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.8; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.06935v1</id>
    <updated>2016-09-22T12:12:05Z</updated>
    <published>2016-09-22T12:12:05Z</published>
    <title>Quantum Neural Machine Learning - Backpropagation and Dynamics</title>
    <summary>  The current work addresses quantum machine learning in the context of Quantum
Artificial Neural Networks such that the networks' processing is divided in two
stages: the learning stage, where the network converges to a specific quantum
circuit, and the backpropagation stage where the network effectively works as a
self-programing quantum computing system that selects the quantum circuits to
solve computing problems. The results are extended to general architectures
including recurrent networks that interact with an environment, coupling with
it in the neural links' activation order, and self-organizing in a dynamical
regime that intermixes patterns of dynamical stochasticity and persistent
quasiperiodic dynamics, making emerge a form of noise resilient dynamical
record.
</summary>
    <author>
      <name>Carlos Pedro Gon√ßalves</name>
    </author>
    <link href="http://arxiv.org/abs/1609.06935v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.06935v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05, 81P68, 92B20, 82C32" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.02683v1</id>
    <updated>2016-10-09T14:42:58Z</updated>
    <published>2016-10-09T14:42:58Z</published>
    <title>Interpreting Neural Networks to Improve Politeness Comprehension</title>
    <summary>  We present an interpretable neural network approach to predicting and
understanding politeness in natural language requests. Our models are based on
simple convolutional neural networks directly on raw text, avoiding any manual
identification of complex sentiment or syntactic features, while performing
better than such feature-based models from previous work. More importantly, we
use the challenging task of politeness prediction as a testbed to next present
a much-needed understanding of what these successful networks are actually
learning. For this, we present several network visualizations based on
activation clusters, first derivative saliency, and embedding space
transformations, helping us automatically identify several subtle linguistics
markers of politeness theories. Further, this analysis reveals multiple novel,
high-scoring politeness strategies which, when added back as new features,
reduce the accuracy gap between the original featurized system and the neural
model, thus providing a clear quantitative interpretation of the success of
these neural networks.
</summary>
    <author>
      <name>Malika Aubakirova</name>
    </author>
    <author>
      <name>Mohit Bansal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at EMNLP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.02683v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.02683v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.07454v1</id>
    <updated>2016-12-22T06:17:01Z</updated>
    <published>2016-12-22T06:17:01Z</published>
    <title>How to Train Your Deep Neural Network with Dictionary Learning</title>
    <summary>  Currently there are two predominant ways to train deep neural networks. The
first one uses restricted Boltzmann machine (RBM) and the second one
autoencoders. RBMs are stacked in layers to form deep belief network (DBN); the
final representation layer is attached to the target to complete the deep
neural network. Autoencoders are nested one inside the other to form stacked
autoencoders; once the stcaked autoencoder is learnt the decoder portion is
detached and the target attached to the deepest layer of the encoder to form
the deep neural network. This work proposes a new approach to train deep neural
networks using dictionary learning as the basic building block; the idea is to
use the features from the shallower layer as inputs for training the next
deeper layer. One can use any type of dictionary learning (unsupervised,
supervised, discriminative etc.) as basic units till the pre-final layer. In
the final layer one needs to use the label consistent dictionary learning
formulation for classification. We compare our proposed framework with existing
state-of-the-art deep learning techniques on benchmark problems; we are always
within the top 10 results. In actual problems of age and gender classification,
we are better than the best known techniques.
</summary>
    <author>
      <name>Vanika Singhal</name>
    </author>
    <author>
      <name>Shikha Singh</name>
    </author>
    <author>
      <name>Angshul Majumdar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">DCC 2017 poster</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.07454v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.07454v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05039v1</id>
    <updated>2017-01-18T12:33:24Z</updated>
    <published>2017-01-18T12:33:24Z</published>
    <title>Efficient Representation of Quantum Many-body States with Deep Neural
  Networks</title>
    <summary>  The challenge of quantum many-body problems comes from the difficulty to
represent large-scale quantum states, which in general requires an
exponentially large number of parameters. Recently, a connection has been made
between quantum many-body states and the neural network representation
(\textit{arXiv:1606.02318}). An important open question is what characterizes
the representational power of deep and shallow neural networks, which is of
fundamental interest due to popularity of the deep learning methods. Here, we
give a rigorous proof that a deep neural network can efficiently represent most
physical states, including those generated by any polynomial size quantum
circuits or ground states of many body Hamiltonians with polynomial-size gaps,
while a shallow network through a restricted Boltzmann machine cannot
efficiently represent those states unless the polynomial hierarchy in
computational complexity theory collapses.
</summary>
    <author>
      <name>Xun Gao</name>
    </author>
    <author>
      <name>Lu-Ming Duan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/s41467-017-00705-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/s41467-017-00705-2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.05039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.01309v2</id>
    <updated>2017-03-14T17:14:53Z</updated>
    <published>2017-03-03T19:02:02Z</published>
    <title>SCYNet: Testing supersymmetric models at the LHC with neural networks</title>
    <summary>  SCYNet (SUSY Calculating Yield Net) is a tool for testing supersymmetric
models against LHC data. It uses neural network regression for a fast
evaluation of the profile likelihood ratio. Two neural network approaches have
been developed: one network has been trained using the parameters of the
11-dimensional phenomenological Minimal Supersymmetric Standard Model
(pMSSM-11) as an input and evaluates the corresponding profile likelihood ratio
within milliseconds. It can thus be used in global pMSSM-11 fits without time
penalty. In the second approach, the neural network has been trained using
model-independent signature-related objects, such as energies and particle
multiplicities, which were estimated from the parameters of a given new physics
model. While the calculation of the energies and particle multiplicities takes
up computation time, the corresponding neural network is more general and can
be used to predict the LHC profile likelihood ratio for a wider class of new
physics models.
</summary>
    <author>
      <name>Philip Bechtle</name>
    </author>
    <author>
      <name>Sebastian Belkner</name>
    </author>
    <author>
      <name>Daniel Dercks</name>
    </author>
    <author>
      <name>Matthias Hamer</name>
    </author>
    <author>
      <name>Tim Keller</name>
    </author>
    <author>
      <name>Michael Kr√§mer</name>
    </author>
    <author>
      <name>Bj√∂rn Sarrazin</name>
    </author>
    <author>
      <name>Jan Sch√ºtte-Engel</name>
    </author>
    <author>
      <name>Jamie Tattersall</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1140/epjc/s10052-017-5224-8</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1140/epjc/s10052-017-5224-8" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 15 figures; References added for V2, version submitted to
  EPJC</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.01309v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.01309v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07024v2</id>
    <updated>2017-08-10T13:28:37Z</updated>
    <published>2017-06-21T17:35:16Z</published>
    <title>Evolving neural networks with genetic algorithms to study the String
  Landscape</title>
    <summary>  We study possible applications of artificial neural networks to examine the
string landscape. Since the field of application is rather versatile, we
propose to dynamically evolve these networks via genetic algorithms. This means
that we start from basic building blocks and combine them such that the neural
network performs best for the application we are interested in. We study three
areas in which neural networks can be applied: to classify models according to
a fixed set of (physically) appealing features, to find a concrete realization
for a computation for which the precise algorithm is known in principle but
very tedious to actually implement, and to predict or approximate the outcome
of some involved mathematical computation which performs too inefficient to
apply it, e.g. in model scans within the string landscape. We present simple
examples that arise in string phenomenology for all three types of problems and
discuss how they can be addressed by evolving neural networks from genetic
algorithms.
</summary>
    <author>
      <name>Fabian Ruehle</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/JHEP08(2017)038</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/JHEP08(2017)038" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 7 figures, references added, typos corrected, extended
  introductory section</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.07024v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07024v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.03684v1</id>
    <updated>2017-07-01T05:38:55Z</updated>
    <published>2017-07-01T05:38:55Z</published>
    <title>Structured Sparse Ternary Weight Coding of Deep Neural Networks for
  Efficient Hardware Implementations</title>
    <summary>  Deep neural networks (DNNs) usually demand a large amount of operations for
real-time inference. Especially, fully-connected layers contain a large number
of weights, thus they usually need many off-chip memory accesses for inference.
We propose a weight compression method for deep neural networks, which allows
values of +1 or -1 only at predetermined positions of the weights so that
decoding using a table can be conducted easily. For example, the structured
sparse (8,2) coding allows at most two non-zero values among eight weights.
This method not only enables multiplication-free DNN implementations but also
compresses the weight storage by up to x32 compared to floating-point networks.
Weight distribution normalization and gradual pruning techniques are applied to
mitigate the performance degradation. The experiments are conducted with
fully-connected deep neural networks and convolutional neural networks.
</summary>
    <author>
      <name>Yoonho Boo</name>
    </author>
    <author>
      <name>Wonyong Sung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is accepted in SIPS 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.03684v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.03684v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.04822v1</id>
    <updated>2017-07-16T04:47:22Z</updated>
    <published>2017-07-16T04:47:22Z</published>
    <title>Normalized Gradient with Adaptive Stepsize Method for Deep Neural
  Network Training</title>
    <summary>  In this paper, we propose a generic and simple algorithmic framework for
first order optimization. The framework essentially contains two consecutive
steps in each iteration: 1) computing and normalizing the mini-batch stochastic
gradient; 2) selecting adaptive step size to update the decision variable
(parameter) towards the negative of the normalized gradient. We show that the
proposed approach, when customized to the popular adaptive stepsize methods,
such as AdaGrad, can enjoy a sublinear convergence rate, if the objective is
convex. We also conduct extensive empirical studies on various non-convex
neural network optimization problems, including multi layer perceptron,
convolution neural networks and recurrent neural networks. The results indicate
the normalized gradient with adaptive step size can help accelerate the
training of neural networks. In particular, significant speedup can be observed
if the networks are deep or the dependencies are long.
</summary>
    <author>
      <name>Adams Wei Yu</name>
    </author>
    <author>
      <name>Qihang Lin</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Jaime Carbonell</name>
    </author>
    <link href="http://arxiv.org/abs/1707.04822v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.04822v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.08553v1</id>
    <updated>2017-07-26T17:33:46Z</updated>
    <published>2017-07-26T17:33:46Z</published>
    <title>Direct Load Control of Thermostatically Controlled Loads Based on Sparse
  Observations Using Deep Reinforcement Learning</title>
    <summary>  This paper considers a demand response agent that must find a near-optimal
sequence of decisions based on sparse observations of its environment.
Extracting a relevant set of features from these observations is a challenging
task and may require substantial domain knowledge. One way to tackle this
problem is to store sequences of past observations and actions in the state
vector, making it high dimensional, and apply techniques from deep learning.
This paper investigates the capabilities of different deep learning techniques,
such as convolutional neural networks and recurrent neural networks, to extract
relevant features for finding near-optimal policies for a residential heating
system and electric water heater that are hindered by sparse observations. Our
simulation results indicate that in this specific scenario, feeding sequences
of time-series to an LSTM network, which is a specific type of recurrent neural
network, achieved a higher performance than stacking these time-series in the
input of a convolutional neural network or deep neural network.
</summary>
    <author>
      <name>Frederik Ruelens</name>
    </author>
    <author>
      <name>Bert J. Claessens</name>
    </author>
    <author>
      <name>Peter Vrancx</name>
    </author>
    <author>
      <name>Fred Spiessens</name>
    </author>
    <author>
      <name>Geert Deconinck</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted and waiting review in IEEE transactions on smart grid 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.08553v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.08553v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.09902v3</id>
    <updated>2017-10-23T15:42:11Z</updated>
    <published>2017-09-28T11:55:13Z</published>
    <title>Improving Efficiency in Convolutional Neural Network with Multilinear
  Filters</title>
    <summary>  The excellent performance of deep neural networks has enabled us to solve
several automatization problems, opening an era of autonomous devices. However,
current deep net architectures are heavy with millions of parameters and
require billions of floating point operations. Several works have been
developed to compress a pre-trained deep network to reduce memory footprint
and, possibly, computation. Instead of compressing a pre-trained network, in
this work, we propose a generic neural network layer structure employing
multilinear projection as the primary feature extractor. The proposed
architecture requires several times less memory as compared to the traditional
Convolutional Neural Networks (CNN), while inherits the similar design
principles of a CNN. In addition, the proposed architecture is equipped with
two computation schemes that enable computation reduction or scalability.
Experimental results show the effectiveness of our compact projection that
outperforms traditional CNN, while requiring far fewer parameters.
</summary>
    <author>
      <name>Dat Thanh Tran</name>
    </author>
    <author>
      <name>Alexandros Iosifidis</name>
    </author>
    <author>
      <name>Moncef Gabbouj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.09902v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.09902v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.01201v1</id>
    <updated>2017-11-03T15:07:24Z</updated>
    <published>2017-11-03T15:07:24Z</published>
    <title>Convolutional Drift Networks for Video Classification</title>
    <summary>  Analyzing spatio-temporal data like video is a challenging task that requires
processing visual and temporal information effectively. Convolutional Neural
Networks have shown promise as baseline fixed feature extractors through
transfer learning, a technique that helps minimize the training cost on visual
information. Temporal information is often handled using hand-crafted features
or Recurrent Neural Networks, but this can be overly specific or prohibitively
complex. Building a fully trainable system that can efficiently analyze
spatio-temporal data without hand-crafted features or complex training is an
open challenge. We present a new neural network architecture to address this
challenge, the Convolutional Drift Network (CDN). Our CDN architecture combines
the visual feature extraction power of deep Convolutional Neural Networks with
the intrinsically efficient temporal processing provided by Reservoir
Computing. In this introductory paper on the CDN, we provide a very simple
baseline implementation tested on two egocentric (first-person) video activity
datasets.We achieve video-level activity classification results on-par with
state-of-the art methods. Notably, performance on this complex spatio-temporal
task was produced by only training a single feed-forward layer in the CDN.
</summary>
    <author>
      <name>Dillon Graham</name>
    </author>
    <author>
      <name>Seyed Hamed Fatemi Langroudi</name>
    </author>
    <author>
      <name>Christopher Kanan</name>
    </author>
    <author>
      <name>Dhireesha Kudithipudi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in IEEE Rebooting Computing</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.01201v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.01201v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.0302v2</id>
    <updated>2010-11-04T11:07:01Z</updated>
    <published>2010-10-02T07:55:23Z</published>
    <title>Spatial Networks</title>
    <summary>  Complex systems are very often organized under the form of networks where
nodes and edges are embedded in space. Transportation and mobility networks,
Internet, mobile phone networks, power grids, social and contact networks,
neural networks, are all examples where space is relevant and where topology
alone does not contain all the information. Characterizing and understanding
the structure and the evolution of spatial networks is thus crucial for many
different fields ranging from urbanism to epidemiology. An important
consequence of space on networks is that there is a cost associated to the
length of edges which in turn has dramatic effects on the topological structure
of these networks. We will expose thoroughly the current state of our
understanding of how the spatial constraints affect the structure and
properties of these networks. We will review the most recent empirical
observations and the most important models of spatial networks. We will also
discuss various processes which take place on these spatial networks, such as
phase transitions, random walks, synchronization, navigation, resilience, and
disease spread.
</summary>
    <author>
      <name>Marc Barthelemy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physrep.2010.11.002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physrep.2010.11.002" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Review article, revised and augmented version, 86 pages, 86 figures,
  338 references</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physics Reports 499:1-101 (2011)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1010.0302v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.0302v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0111169v1</id>
    <updated>2001-11-09T14:05:56Z</updated>
    <published>2001-11-09T14:05:56Z</published>
    <title>A measure for the complexity of Boolean functions related to their
  implementation in neural networks</title>
    <summary>  We define a measure for the complexity of Boolean functions related to their
implementation in neural networks, and in particular close related to the
generalization ability that could be obtained through the learning process. The
measure is computed through the calculus of the number of neighbor examples
that differ in their output value. Pairs of these examples have been previously
shown to be part of the minimum size training set needed to obtain perfect
generalization in feedforward neural networks.
  The main advantage of the proposed measure, in comparison to existing ones,
is the way in which the measure is evaluated, as it can be computed from the
definition of the function itself, independently of its implementation. The
validity of the proposal is analyzed through numerical simulations performed on
different feedforward neural networks architectures, and a good agreement is
obtained between the predicted complexity and the generalization ability for
different classes of functions.
  Also an interesting analogy was found between the proposed complexity measure
and the energy function of ferromagnetic systems, that could be exploited, by
use of the existing results and mathematical tools developed within a
statistical mechanics framework, to explore in a more rigorous way the
properties of Boolean functions when implemented on neural networks.
  We conjecture that the proposed measure could help as an useful tool for
carrying a systematic study of the computational capabilities of neural
networks, essentially by permitting an easy and reliable classification of
Boolean functions. Possible extensions of the work are also discussed.
</summary>
    <author>
      <name>Leonardo Franco</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted on Nov. 9. 2001</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0111169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0111169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/q-bio/0703002v1</id>
    <updated>2007-03-01T10:18:30Z</updated>
    <published>2007-03-01T10:18:30Z</published>
    <title>Autonomous Dynamics in Neural networks: The dHAN Concept and Associative
  Thought Processes</title>
    <summary>  The neural activity of the human brain is dominated by self-sustained
activities. External sensory stimuli influence this autonomous activity but
they do not drive the brain directly. Most standard artificial neural network
models are however input driven and do not show spontaneous activities.
  It constitutes a challenge to develop organizational principles for
controlled, self-sustained activity in artificial neural networks. Here we
propose and examine the dHAN concept for autonomous associative thought
processes in dense and homogeneous associative networks. An associative
thought-process is characterized, within this approach, by a time-series of
transient attractors. Each transient state corresponds to a stored information,
a memory. The subsequent transient states are characterized by large
associative overlaps, which are identical to acquired patterns. Memory states,
the acquired patterns, have such a dual functionality.
  In this approach the self-sustained neural activity has a central functional
role. The network acquires a discrimination capability, as external stimuli
need to compete with the autonomous activity. Noise in the input is readily
filtered-out.
  Hebbian learning of external patterns occurs coinstantaneous with the ongoing
associative thought process. The autonomous dynamics needs a long-term
working-point optimization which acquires within the dHAN concept a dual
functionality: It stabilizes the time development of the associative thought
process and limits runaway synaptic growth, which generically occurs otherwise
in neural networks with self-induced activities and Hebbian-type learning
rules.
</summary>
    <author>
      <name>Claudius Gros</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/1.2709594</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/1.2709594" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ninth Granada Lectures, AIP Conference Proceedings, Vol. 887, 129
  (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/q-bio/0703002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/q-bio/0703002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.3838v1</id>
    <updated>2012-03-17T05:06:21Z</updated>
    <published>2012-03-17T05:06:21Z</published>
    <title>A Study on the Behavior of a Neural Network for Grouping the Data</title>
    <summary>  One of the frequently stated advantages of neural networks is that they can
work effectively with non-normally distributed data. But optimal results are
possible with normalized data.In this paper, how normality of the input affects
the behaviour of a K-means fast learning artificial neural network(KFLANN) for
grouping the data is presented. Basically, the grouping of high dimensional
input data is controlled by additional neural network input parameters namely
vigilance and tolerance.Neural networks learn faster and give better
performance if the input variables are pre-processed before being fed to the
input units of the neural network. A common way of dealing with data that is
not normally distributed is to perform some form of mathematical transformation
on the data that shifts it towards a normal distribution.In a neural network,
data preprocessing transforms the data into a format that will be more easily
and effectively processed for the purpose of the user. Among various methods,
Normalization is one which organizes data for more efficient access.
Experimental results on several artificial and synthetic data sets indicate
that the groups formed in the data vary with non-normally distributed data and
normalized data and also depends on the normalization method used.
</summary>
    <author>
      <name>Suneetha Chittineni</name>
    </author>
    <author>
      <name>Raveendra Babu Bhogapathi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages,2 figures,9 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 1, No 1, January 2012, pp:228-234</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1203.3838v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.3838v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.4659v1</id>
    <updated>2013-01-20T14:13:22Z</updated>
    <published>2013-01-20T14:13:22Z</published>
    <title>English Sentence Recognition using Artificial Neural Network through
  Mouse-based Gestures</title>
    <summary>  Handwriting is one of the most important means of daily communication.
Although the problem of handwriting recognition has been considered for more
than 60 years there are still many open issues, especially in the task of
unconstrained handwritten sentence recognition. This paper focuses on the
automatic system that recognizes continuous English sentence through a
mouse-based gestures in real-time based on Artificial Neural Network. The
proposed Artificial Neural Network is trained using the traditional
backpropagation algorithm for self supervised neural network which provides the
system with great learning ability and thus has proven highly successful in
training for feed-forward Artificial Neural Network. The designed algorithm is
not only capable of translating discrete gesture moves, but also continuous
gestures through the mouse. In this paper we are using the efficient neural
network approach for recognizing English sentence drawn by mouse. This approach
shows an efficient way of extracting the boundary of the English Sentence and
specifies the area of the recognition English sentence where it has been drawn
in an image and then used Artificial Neural Network to recognize the English
sentence. The proposed approach English sentence recognition (ESR) system is
designed and tested successfully. Experimental results show that the higher
speed and accuracy were examined.
</summary>
    <author>
      <name>Firoj Parwej</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/10023-4998</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/10023-4998" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages, 7 Figures. arXiv admin note: text overlap with
  arXiv:1007.0627 by other authors without attribution</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications (IJCA)USA, Volume
  61, No.17, January 2013 ISSN 0975 - 8887, http://www.ijcaonline.org,
  http://www.ijcaonline.org/archives/volume61/number17/10023-4998</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1301.4659v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.4659v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.06031v2</id>
    <updated>2016-11-29T14:08:39Z</updated>
    <published>2015-01-24T12:16:10Z</published>
    <title>From dynamics to links: a sparse reconstruction of the topology of a
  neural network</title>
    <summary>  One major challenge in neuroscience is the identification of interrelations
between signals reflecting neural activity and how information processing
occurs in the neural circuits. At the cellular and molecular level, mechanisms
of signal transduction have been studied intensively and a better knowledge and
understanding of some basic processes of information handling by neurons has
been achieved. In contrast, little is known about the organization and function
of complex neuronal networks. Experimental methods are now available to
simultaneously monitor electrical activity of a large number of neurons in real
time. Then, the qualitative and quantitative analysis of the spiking activity
of individual neurons is a very valuable tool for the study of the dynamics and
architecture of the neural networks. Such activity is not due to the sole
intrinsic properties of the individual neural cells but it is mostly
consequence of the direct influence of other neurons. The deduction of the
effective connectivity between neurons, whose experimental spike trains are
observed, is of crucial importance in neuroscience: first for the correct
interpretation of the electro-physiological activity of the involved neurons
and neural networks, and, for correctly relating the electrophysiological
activity to the functional tasks accomplished by the network. In this work we
propose a novel method for the identification of connectivity of neural
networks using recorded voltages. Our approach is based on the assumption that
the network has a topology with sparse connections. After a brief description
of our method we will report the performances and compare it to the
cross-correlation computed on the spike trains, that represents a gold standard
method in the field.
</summary>
    <author>
      <name>Giacomo Aletti</name>
    </author>
    <author>
      <name>Davide Lonardoni</name>
    </author>
    <author>
      <name>Giovanni Naldi</name>
    </author>
    <author>
      <name>Thierry Nieus</name>
    </author>
    <link href="http://arxiv.org/abs/1501.06031v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.06031v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.01774v2</id>
    <updated>2016-02-11T12:59:35Z</updated>
    <published>2015-08-07T18:16:32Z</published>
    <title>An End-to-End Neural Network for Polyphonic Piano Music Transcription</title>
    <summary>  We present a supervised neural network model for polyphonic piano music
transcription. The architecture of the proposed model is analogous to speech
recognition systems and comprises an acoustic model and a music language model.
The acoustic model is a neural network used for estimating the probabilities of
pitches in a frame of audio. The language model is a recurrent neural network
that models the correlations between pitch combinations over time. The proposed
model is general and can be used to transcribe polyphonic music without
imposing any constraints on the polyphony. The acoustic and language model
predictions are combined using a probabilistic graphical model. Inference over
the output variables is performed using the beam search algorithm. We perform
two sets of experiments. We investigate various neural network architectures
for the acoustic models and also investigate the effect of combining acoustic
and music language model predictions using the proposed architecture. We
compare performance of the neural network based acoustic models with two
popular unsupervised acoustic models. Results show that convolutional neural
network acoustic models yields the best performance across all evaluation
metrics. We also observe improved performance with the application of the music
language models. Finally, we present an efficient variant of beam search that
improves performance and reduces run-times by an order of magnitude, making the
model suitable for real-time applications.
</summary>
    <author>
      <name>Siddharth Sigtia</name>
    </author>
    <author>
      <name>Emmanouil Benetos</name>
    </author>
    <author>
      <name>Simon Dixon</name>
    </author>
    <link href="http://arxiv.org/abs/1508.01774v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.01774v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.01987v7</id>
    <updated>2016-07-05T15:39:13Z</updated>
    <published>2016-01-08T19:35:04Z</published>
    <title>Deep Learning for Limit Order Books</title>
    <summary>  This paper develops a new neural network architecture for modeling spatial
distributions (i.e., distributions on R^d) which is computationally efficient
and specifically designed to take advantage of the spatial structure of limit
order books. The new architecture yields a low-dimensional model of price
movements deep into the limit order book, allowing more effective use of
information from deep in the limit order book (i.e., many levels beyond the
best bid and best ask). This "spatial neural network" models the joint
distribution of the state of the limit order book at a future time conditional
on the current state of the limit order book. The spatial neural network
outperforms other models such as the naive empirical model, logistic regression
(with nonlinear features), and a standard neural network architecture. Both
neural networks strongly outperform the logistic regression model. Due to its
more effective use of information deep in the limit order book, the spatial
neural network especially outperforms the standard neural network in the tail
of the distribution, which is important for risk management applications. The
models are trained and tested on nearly 500 stocks. Techniques from deep
learning such as dropout are employed to improve performance. Due to the
significant computational challenges associated with the large amount of data,
models are trained with a cluster of 50 GPUs.
</summary>
    <author>
      <name>Justin Sirignano</name>
    </author>
    <link href="http://arxiv.org/abs/1601.01987v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.01987v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.TR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.TR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.06419v1</id>
    <updated>2017-06-15T02:17:53Z</updated>
    <published>2017-06-15T02:17:53Z</published>
    <title>The Compressed Model of Residual CNDS</title>
    <summary>  Convolutional neural networks have achieved a great success in the recent
years. Although, the way to maximize the performance of the convolutional
neural networks still in the beginning. Furthermore, the optimization of the
size and the time that need to train the convolutional neural networks is very
far away from reaching the researcher's ambition. In this paper, we proposed a
new convolutional neural network that combined several techniques to boost the
optimization of the convolutional neural network in the aspects of speed and
size. As we used our previous model Residual-CNDS (ResCNDS), which solved the
problems of slower convergence, overfitting, and degradation, and compressed
it. The outcome model called Residual-Squeeze-CNDS (ResSquCNDS), which we
demonstrated on our sold technique to add residual learning and our model of
compressing the convolutional neural networks. Our model of compressing adapted
from the SQUEEZENET model, but our model is more generalizable, which can be
applied almost to any neural network model, and fully integrated into the
residual learning, which addresses the problem of the degradation very
successfully. Our proposed model trained on very large-scale MIT
Places365-Standard scene datasets, which backing our hypothesis that the new
compressed model inherited the best of the previous ResCNDS8 model, and almost
get the same accuracy in the validation Top-1 and Top-5 with 87.64% smaller in
size and 13.33% faster in the training time.
</summary>
    <author>
      <name>Hussam Qassim</name>
    </author>
    <author>
      <name>David Feinzimer</name>
    </author>
    <author>
      <name>Abhishek Verma</name>
    </author>
    <link href="http://arxiv.org/abs/1706.06419v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.06419v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.01818v2</id>
    <updated>2018-01-29T18:31:30Z</updated>
    <published>2017-08-05T19:54:59Z</published>
    <title>Depth Adaptive Deep Neural Network for Semantic Segmentation</title>
    <summary>  In this work, we present the depth-adaptive deep neural network using a depth
map for semantic segmentation. Typical deep neural networks receive inputs at
the predetermined locations regardless of the distance from the camera. This
fixed receptive field presents a challenge to generalize the features of
objects at various distances in neural networks. Specifically, the
predetermined receptive fields are too small at a short distance, and vice
versa. To overcome this challenge, we develop a neural network which is able to
adapt the receptive field not only for each layer but also for each neuron at
the spatial location. To adjust the receptive field, we propose the
depth-adaptive multiscale (DaM) convolution layer consisting of the adaptive
perception neuron and the in-layer multiscale neuron. The adaptive perception
neuron is to adjust the receptive field at each spatial location using the
corresponding depth information. The in-layer multiscale neuron is to apply the
different size of the receptive field at each feature space to learn features
at multiple scales. The proposed DaM convolution is applied to two fully
convolutional neural networks. We demonstrate the effectiveness of the proposed
neural networks on the publicly available RGB-D dataset for semantic
segmentation and the novel hand segmentation dataset for hand-object
interaction. The experimental results show that the proposed method outperforms
the state-of-the-art methods without any additional layers or
pre/post-processing.
</summary>
    <author>
      <name>Byeongkeun Kang</name>
    </author>
    <author>
      <name>Yeejin Lee</name>
    </author>
    <author>
      <name>Truong Q. Nguyen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TMM.2018.2798282</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TMM.2018.2798282" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Multimedia, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.01818v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.01818v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.10741v2</id>
    <updated>2017-10-31T00:54:22Z</updated>
    <published>2017-10-30T02:04:07Z</published>
    <title>Evolving Deep Convolutional Neural Networks for Image Classification</title>
    <summary>  Evolutionary computation methods have been successfully applied to neural
networks since two decades ago, while those methods cannot scale well to the
modern deep neural networks due to the complicated architectures and large
quantities of connection weights. In this paper, we propose a new method using
genetic algorithms for evolving the architectures and connection weight
initialization values of a deep convolutional neural network to address image
classification problems. In the proposed algorithm, an efficient
variable-length gene encoding strategy is designed to represent the different
building blocks and the unpredictable optimal depth in convolutional neural
networks. In addition, a new representation scheme is developed for effectively
initializing connection weights of deep convolutional neural networks, which is
expected to avoid networks getting stuck into local minima which is typically a
major issue in the backward gradient-based optimization. Furthermore, a novel
fitness evaluation method is proposed to speed up the heuristic search with
substantially less computational resource. The proposed algorithm is examined
and compared with 22 existing algorithms on nine widely used image
classification tasks, including the state-of-the-art methods. The experimental
results demonstrate the remarkable superiority of the proposed algorithm over
the state-of-the-art algorithms in terms of classification error rate and the
number of parameters (weights).
</summary>
    <author>
      <name>Yanan Sun</name>
    </author>
    <author>
      <name>Bing Xue</name>
    </author>
    <author>
      <name>Mengjie Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1710.10741v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.10741v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.05507v1</id>
    <updated>2018-01-16T23:58:47Z</updated>
    <published>2018-01-16T23:58:47Z</published>
    <title>Gazelle: A Low Latency Framework for Secure Neural Network Inference</title>
    <summary>  The growing popularity of cloud-based machine learning raises a natural
question about the privacy guarantees that can be provided in such a setting.
Our work tackles this problem in the context where a client wishes to classify
private images using a convolutional neural network (CNN) trained by a server.
Our goal is to build efficient protocols whereby the client can acquire the
classification result without revealing their input to the server, while
guaranteeing the privacy of the server's neural network.
  To this end, we design Gazelle, a scalable and low-latency system for secure
neural network inference, using an intricate combination of homomorphic
encryption and traditional two-party computation techniques (such as garbled
circuits). Gazelle makes three contributions. First, we design the Gazelle
homomorphic encryption library which provides fast algorithms for basic
homomorphic operations such as SIMD (single instruction multiple data)
addition, SIMD multiplication and ciphertext permutation. Second, we implement
the Gazelle homomorphic linear algebra kernels which map neural network layers
to optimized homomorphic matrix-vector multiplication and convolution routines.
Third, we design optimized encryption switching protocols which seamlessly
convert between homomorphic and garbled circuit encodings to enable
implementation of complete neural network inference.
  We evaluate our protocols on benchmark neural networks trained on the MNIST
and CIFAR-10 datasets and show that Gazelle outperforms the best existing
systems such as MiniONN (ACM CCS 2017) by 20 times and Chameleon (Crypto Eprint
2017/1164) by 30 times in online runtime. Similarly when compared with fully
homomorphic approaches like CryptoNets (ICML 2016) we demonstrate three orders
of magnitude faster online run-time.
</summary>
    <author>
      <name>Chiraag Juvekar</name>
    </author>
    <author>
      <name>Vinod Vaikuntanathan</name>
    </author>
    <author>
      <name>Anantha Chandrakasan</name>
    </author>
    <link href="http://arxiv.org/abs/1801.05507v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.05507v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.07710v2</id>
    <updated>2018-01-30T15:30:26Z</updated>
    <published>2018-01-23T20:52:44Z</published>
    <title>Bayesian Neural Networks</title>
    <summary>  This paper describes and discusses Bayesian Neural Network (BNN). The paper
showcases a few different applications of them for classification and
regression problems. BNNs are comprised of a Probabilistic Model and a Neural
Network. The intent of such a design is to combine the strengths of Neural
Networks and Stochastic modeling. Neural Networks exhibit continuous function
approximator capabilities. Stochastic models allow direct specification of a
model with known interaction between parameters to generate data. During the
prediction phase, stochastic models generate a complete posterior distribution
and produce probabilistic guarantees on the predictions. Thus BNNs are a unique
combination of neural network and stochastic models with the stochastic model
forming the core of this integration. BNNs can then produce probabilistic
guarantees on it's predictions and also generate the distribution of parameters
that it has learnt from the observations. That means, in the parameter space,
one can deduce the nature and shape of the neural network's learnt parameters.
These two characteristics makes them highly attractive to theoreticians as well
as practitioners. Recently there has been a lot of activity in this area, with
the advent of numerous probabilistic programming libraries such as: PyMC3,
Edward, Stan etc. Further this area is rapidly gaining ground as a standard
machine learning approach for numerous problems
</summary>
    <author>
      <name>Vikram Mullachery</name>
    </author>
    <author>
      <name>Aniruddh Khera</name>
    </author>
    <author>
      <name>Amir Husain</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1111.4246 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.07710v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.07710v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.05405v1</id>
    <updated>2018-02-15T04:58:45Z</updated>
    <published>2018-02-15T04:58:45Z</published>
    <title>Putting a bug in ML: The moth olfactory network learns to read MNIST</title>
    <summary>  We seek to (i) characterize the learning architectures exploited in
biological neural networks for training on very few samples, and (ii) port
these algorithmic structures to a machine learning context. The Moth Olfactory
Network is among the simplest biological neural systems that can learn, and its
architecture includes key structural elements widespread in biological neural
nets, such as cascaded networks, competitive inhibition, high intrinsic noise,
sparsity, reward mechanisms, and Hebbian plasticity. The interactions of these
structural elements play a critical enabling role in rapid learning.
  We assign a computational model of the Moth Olfactory Network the task of
learning to read the MNIST digits. This model, MothNet, is closely aligned with
the moth's known biophysics and with in vivo electrode data, including data
collected from moths learning new odors. We show that MothNet successfully
learns to read given very few training samples (1 to 20 samples per class). In
this few-samples regime, it substantially outperforms standard machine learning
methods such as nearest-neighbors, support-vector machines, and convolutional
neural networks (CNNs). The MothNet architecture illustrates how our proposed
algorithmic structures, derived from biological brains, can be used to build
alternative deep neural nets (DNNs) that may potentially avoid some of DNNs
current learning rate limitations. This novel, bio-inspired neural network
architecture offers a valuable complementary approach to DNN design.
</summary>
    <author>
      <name>Charles B. Delahunt</name>
    </author>
    <author>
      <name>J. Nathan Kutz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.05405v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.05405v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.3475v1</id>
    <updated>2011-12-15T10:47:15Z</updated>
    <published>2011-12-15T10:47:15Z</published>
    <title>Discovering universal statistical laws of complex networks</title>
    <summary>  Different network models have been suggested for the topology underlying
complex interactions in natural systems. These models are aimed at replicating
specific statistical features encountered in real-world networks. However, it
is rarely considered to which degree the results obtained for one particular
network class can be extrapolated to real-world networks. We address this issue
by comparing different classical and more recently developed network models
with respect to their generalisation power, which we identify with large
structural variability and absence of constraints imposed by the construction
scheme. After having identified the most variable networks, we address the
issue of which constraints are common to all network classes and are thus
suitable candidates for being generic statistical laws of complex networks. In
fact, we find that generic, not model-related dependencies between different
network characteristics do exist. This allows, for instance, to infer global
features from local ones using regression models trained on networks with high
generalisation power. Our results confirm and extend previous findings
regarding the synchronisation properties of neural networks. Our method seems
especially relevant for large networks, which are difficult to map completely,
like the neural networks in the brain. The structure of such large networks
cannot be fully sampled with the present technology. Our approach provides a
method to estimate global properties of under-sampled networks with good
approximation. Finally, we demonstrate on three different data sets (C.
elegans' neuronal network, R. prowazekii's metabolic network, and a network of
synonyms extracted from Roget's Thesaurus) that real-world networks have
statistical relations compatible with those obtained using regression models.
</summary>
    <author>
      <name>Stefano Cardanobile</name>
    </author>
    <author>
      <name>Volker Pernice</name>
    </author>
    <author>
      <name>Moritz Deger</name>
    </author>
    <author>
      <name>Stefan Rotter</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pone.0037911</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pone.0037911" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">PLoS ONE 7(6): e37911 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1112.3475v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.3475v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9910500v1</id>
    <updated>1999-10-29T22:26:28Z</updated>
    <published>1999-10-29T22:26:28Z</published>
    <title>Generalization in the Hopfield Model</title>
    <summary>  In the Hopfield model the ability of the network to generalization is studied
in the case of the network trained by one input image ({\it the standard}).
</summary>
    <author>
      <name>Leonid B. Litinskii</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">High Pressure Physics Institute of Russian Academy of Sciences</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">RevTEX, 11 pages, 2 Postscript figures, the poster presentation on
  the "Neural Computation in Science and Technology" (October 10-13, Maale ha
  Chamisha, Israel)</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9910500v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9910500v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0201530v1</id>
    <updated>2002-01-29T11:28:15Z</updated>
    <published>2002-01-29T11:28:15Z</published>
    <title>Statistical Physics of Feedforward Neural Networks</title>
    <summary>  The article is a lightly edited version of my habilitation thesis at the
University Wuerzburg. My aim is to give a self contained, if concise,
introduction to the formal methods used when off-line learning in feedforward
networks is analyzed by statistical physics. However, due to its origin, the
article is not a comprehensive review of the field but is highly skewed towards
reporting my own research.
</summary>
    <author>
      <name>Robert Urbanczik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">44 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0201530v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0201530v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/nlin/0308031v1</id>
    <updated>2003-08-28T10:39:32Z</updated>
    <published>2003-08-28T10:39:32Z</published>
    <title>Stability of synchronized oscillations in networks of phase-oscillators</title>
    <summary>  We derive simple conditions for the stability or instability of the
synchronized oscillation of a class of networks of coupled phase-oscillators,
which includes many of the systems used in neural modelling.
</summary>
    <author>
      <name>Guy Katriel</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Discrete &amp; Continuous Dynamical Systems - B, V. 5 (2), May 2005.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/nlin/0308031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/nlin/0308031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.PS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.PS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.3511v1</id>
    <updated>2010-06-17T16:35:07Z</updated>
    <published>2010-06-17T16:35:07Z</published>
    <title>Collective behavior of heterogeneous neural networks</title>
    <summary>  We investigate a network of integrate-and-fire neurons characterized by a
distribution of spiking frequencies. Upon increasing the coupling strength, the
model exhibits a transition from an asynchronous regime to a nontrivial
collective behavior. At variance with the Kuramoto model, (i) the macroscopic
dynamics is irregular even in the thermodynamic limit, and (ii) the microscopic
(single-neuron) evolution is linearly stable.
</summary>
    <author>
      <name>Stefano Luccioli</name>
    </author>
    <author>
      <name>Antonio Politi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.105.158104</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.105.158104" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1006.3511v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.3511v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.00564v1</id>
    <updated>2016-10-03T14:22:19Z</updated>
    <published>2016-10-03T14:22:19Z</published>
    <title>End-to-End Radio Traffic Sequence Recognition with Deep Recurrent Neural
  Networks</title>
    <summary>  We investigate sequence machine learning techniques on raw radio signal
time-series data. By applying deep recurrent neural networks we learn to
discriminate between several application layer traffic types on top of a
constant envelope modulation without using an expert demodulation algorithm. We
show that complex protocol sequences can be learned and used for both
classification and generation tasks using this approach.
</summary>
    <author>
      <name>Timothy J. O'Shea</name>
    </author>
    <author>
      <name>Seth Hitefield</name>
    </author>
    <author>
      <name>Johnathan Corgan</name>
    </author>
    <link href="http://arxiv.org/abs/1610.00564v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.00564v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.01557v1</id>
    <updated>2016-11-04T22:31:03Z</updated>
    <published>2016-11-04T22:31:03Z</published>
    <title>Spatiotemporal dynamics and reliable computations in recurrent spiking
  neural networks</title>
    <summary>  Randomly connected networks of excitatory and inhibitory spiking neurons
provide a parsimonious model of neural variability, but are notoriously
unreliable for performing computations. We show that this difficulty is
overcome by incorporating the well-documented dependence of connection
probability on distance. Spatially extended spiking networks exhibit
symmetry-breaking bifurcations and generate spatiotemporal patterns that can be
trained to perform dynamical computations.
</summary>
    <author>
      <name>Ryan Pyle</name>
    </author>
    <author>
      <name>Robert Rosenbaum</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.118.018103</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.118.018103" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. Lett. 118, 018103 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1611.01557v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.01557v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.01402v1</id>
    <updated>2017-03-04T06:32:15Z</updated>
    <published>2017-03-04T06:32:15Z</published>
    <title>Skin Lesion Classification Using Deep Multi-scale Convolutional Neural
  Networks</title>
    <summary>  We present a deep learning approach to the ISIC 2017 Skin Lesion
Classification Challenge using a multi-scale convolutional neural network. Our
approach utilizes an Inception-v3 network pre-trained on the ImageNet dataset,
which is fine-tuned for skin lesion classification using two different scales
of input images.
</summary>
    <author>
      <name>Terrance DeVries</name>
    </author>
    <author>
      <name>Dhanesh Ramachandram</name>
    </author>
    <link href="http://arxiv.org/abs/1703.01402v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.01402v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.06841v1</id>
    <updated>2017-04-22T19:39:32Z</updated>
    <published>2017-04-22T19:39:32Z</published>
    <title>Medical Text Classification using Convolutional Neural Networks</title>
    <summary>  We present an approach to automatically classify clinical text at a sentence
level. We are using deep convolutional neural networks to represent complex
features. We train the network on a dataset providing a broad categorization of
health information. Through a detailed evaluation, we demonstrate that our
method outperforms several approaches widely used in natural language
processing tasks by about 15%.
</summary>
    <author>
      <name>Mark Hughes</name>
    </author>
    <author>
      <name>Irene Li</name>
    </author>
    <author>
      <name>Spyros Kotoulas</name>
    </author>
    <author>
      <name>Toyotaro Suzumura</name>
    </author>
    <link href="http://arxiv.org/abs/1704.06841v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.06841v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.01365v1</id>
    <updated>2017-05-03T11:29:28Z</updated>
    <published>2017-05-03T11:29:28Z</published>
    <title>Quantified advantage of discontinuous weight selection in approximations
  with deep neural networks</title>
    <summary>  We consider approximations of 1D Lipschitz functions by deep ReLU networks of
a fixed width. We prove that without the assumption of continuous weight
selection the uniform approximation error is lower than with this assumption at
least by a factor logarithmic in the size of the network.
</summary>
    <author>
      <name>Dmitry Yarotsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, submitted to J. Approx. Theory</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.01365v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01365v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.05534v1</id>
    <updated>2017-06-17T13:33:29Z</updated>
    <published>2017-06-17T13:33:29Z</published>
    <title>Rotation Invariance Neural Network</title>
    <summary>  Rotation invariance and translation invariance have great values in image
recognition tasks. In this paper, we bring a new architecture in convolutional
neural network (CNN) named cyclic convolutional layer to achieve rotation
invariance in 2-D symbol recognition. We can also get the position and
orientation of the 2-D symbol by the network to achieve detection purpose for
multiple non-overlap target. Last but not least, this architecture can achieve
one-shot learning in some cases using those invariance.
</summary>
    <author>
      <name>Shiyuan Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.05534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.05534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0604295v1</id>
    <updated>2006-04-12T04:27:50Z</updated>
    <published>2006-04-12T04:27:50Z</published>
    <title>Polarized networks, diameter, and synchronizability of networks</title>
    <summary>  Previous research claimed or disclaimed the role of a small diameter in the
synchronization of a network of coupled dynamical systems. We investigate this
connection and show that it is two folds. We first construct two classes of
networks, the polarized networks and the random networks with a fixed diameter,
which exhibit very different synchronizability. This shows that the diameter
itself is insufficient to determine the synchronizability of networks.
Secondly, we derive analytic estimates on the synchronizability of networks in
terms of the diameter, and find that a larger size of network admits of a more
flexible synchronizability. The analysis is confirmed by numerical results.
</summary>
    <author>
      <name>Wei Lin</name>
    </author>
    <author>
      <name>Xiaowei Zhan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0604295v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0604295v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.02601v1</id>
    <updated>2018-02-06T05:32:36Z</updated>
    <published>2018-02-06T05:32:36Z</published>
    <title>Digital Watermarking for Deep Neural Networks</title>
    <summary>  Although deep neural networks have made tremendous progress in the area of
multimedia representation, training neural models requires a large amount of
data and time. It is well-known that utilizing trained models as initial
weights often achieves lower training error than neural networks that are not
pre-trained. A fine-tuning step helps to reduce both the computational cost and
improve performance. Therefore, sharing trained models has been very important
for the rapid progress of research and development. In addition, trained models
could be important assets for the owner(s) who trained them, hence we regard
trained models as intellectual property. In this paper, we propose a digital
watermarking technology for ownership authorization of deep neural networks.
First, we formulate a new problem: embedding watermarks into deep neural
networks. We also define requirements, embedding situations, and attack types
on watermarking in deep neural networks. Second, we propose a general framework
for embedding a watermark in model parameters, using a parameter regularizer.
Our approach does not impair the performance of networks into which a watermark
is placed because the watermark is embedded while training the host network.
Finally, we perform comprehensive experiments to reveal the potential of
watermarking deep neural networks as the basis of this new research effort. We
show that our framework can embed a watermark during the training of a deep
neural network from scratch, and during fine-tuning and distilling, without
impairing its performance. The embedded watermark does not disappear even after
fine-tuning or parameter pruning; the watermark remains complete even after 65%
of parameters are pruned.
</summary>
    <author>
      <name>Yuki Nagai</name>
    </author>
    <author>
      <name>Yusuke Uchida</name>
    </author>
    <author>
      <name>Shigeyuki Sakazawa</name>
    </author>
    <author>
      <name>Shin'ichi Satoh</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s13735-018-0147-1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s13735-018-0147-1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a pre-print of an article published in International Journal
  of Multimedia Information Retrieval. The final authenticated version is
  available online at: https://doi.org/10.1007/s13735-018-0147-1 . arXiv admin
  note: substantial text overlap with arXiv:1701.04082</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.02601v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.02601v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.04622v1</id>
    <updated>2017-08-15T18:00:01Z</updated>
    <published>2017-08-15T18:00:01Z</published>
    <title>Deep Learning the Ising Model Near Criticality</title>
    <summary>  It is well established that neural networks with deep architectures perform
better than shallow networks for many tasks in machine learning. In statistical
physics, while there has been recent interest in representing physical data
with generative modelling, the focus has been on shallow neural networks. A
natural question to ask is whether deep neural networks hold any advantage over
shallow networks in representing such data. We investigate this question by
using unsupervised, generative graphical models to learn the probability
distribution of a two-dimensional Ising system. Deep Boltzmann machines, deep
belief networks, and deep restricted Boltzmann networks are trained on thermal
spin configurations from this system, and compared to the shallow architecture
of the restricted Boltzmann machine. We benchmark the models, focussing on the
accuracy of generating energetic observables near the phase transition, where
these quantities are most difficult to approximate. Interestingly, after
training the generative networks, we observe that the accuracy essentially
depends only on the number of neurons in the first hidden layer of the network,
and not on other model details such as network depth or model type. This is
evidence that shallow networks are more efficient than deep networks at
representing physical probability distributions associated with Ising systems
near criticality.
</summary>
    <author>
      <name>Alan Morningstar</name>
    </author>
    <author>
      <name>Roger G. Melko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 8 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.04622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.04622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/adap-org/9707005v1</id>
    <updated>1997-07-22T17:51:10Z</updated>
    <published>1997-07-22T17:51:10Z</published>
    <title>Coherence and clustering in ensembles of neural networks</title>
    <summary>  Large ensembles of globally coupled chaotic neural networks undergo a
transition to complete synchronization for high coupling intensities. The onset
of this fully coherent behavior is preceded by a regime where clusters of
networks with identical activity are spontaneously formed. In these regimes of
coherent collective evolution the dynamics of each neural network is still
chaotic. These results may be relevant to the study of systems where
interaction between elements is able to give rise to coherent complex behavior,
such as in cortex activity or in insect societies.
</summary>
    <author>
      <name>D. H. Zanette</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Fritz-Haber Institut der Max-Planck-Gesellschaft</arxiv:affiliation>
    </author>
    <author>
      <name>A. S. Mikhailov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Fritz-Haber Institut der Max-Planck-Gesellschaft</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures, revtex</arxiv:comment>
    <link href="http://arxiv.org/abs/adap-org/9707005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/adap-org/9707005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="adap-org" scheme="http://arxiv.org/schemas/atom"/>
    <category term="adap-org" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9708215v1</id>
    <updated>1997-08-28T15:30:17Z</updated>
    <published>1997-08-28T15:30:17Z</published>
    <title>Attractors in fully asymmetric neural networks</title>
    <summary>  The statistical properties of the length of the cycles and of the weights of
the attraction basins in fully asymmetric neural networks (i.e. with completely
uncorrelated synapses) are computed in the framework of the annealed
approximation which we previously introduced for the study of Kauffman
networks. Our results show that this model behaves essentially as a Random Map
possessing a reversal symmetry. Comparison with numerical results suggests that
the approximation could become exact in the infinite size limit.
</summary>
    <author>
      <name>U. Bastolla</name>
    </author>
    <author>
      <name>G. Parisi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/0305-4470/30/16/007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/0305-4470/30/16/007" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 6 figures, Latex, to appear on J. Phys. A</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9708215v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9708215v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9901179v1</id>
    <updated>1999-01-19T14:22:47Z</updated>
    <published>1999-01-19T14:22:47Z</published>
    <title>Weight decay induced phase transitions in multilayer neural networks</title>
    <summary>  We investigate layered neural networks with differentiable activation
function and student vectors without normalization constraint by means of
equilibrium statistical physics. We consider the learning of perfectly
realizable rules and find that the length of student vectors becomes infinite,
unless a proper weight decay term is added to the energy. Then, the system
undergoes a first order phase transition between states with very long student
vectors and states where the lengths are comparable to those of the teacher
vectors. Additionally in both configurations there is a phase transition
between a specialized and an unspecialized phase. An anti-specialized phase
with long student vectors exists in networks with a small number of hidden
units.
</summary>
    <author>
      <name>M. Ahr</name>
    </author>
    <author>
      <name>M. Biehl</name>
    </author>
    <author>
      <name>E. Schloesser</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/0305-4470/32/27/301</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/0305-4470/32/27/301" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9901179v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9901179v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9911349v1</id>
    <updated>1999-11-22T16:43:50Z</updated>
    <published>1999-11-22T16:43:50Z</published>
    <title>Parallel dynamics of the asymmetric extremely diluted Ashkin-Teller
  neural network</title>
    <summary>  The parallel dynamics of the asymmetric extremely diluted Ashkin-Teller
neural network is studied using signal-to-noise analysis techniques. Evolution
equations for the order parameters are derived, both at zero and finite
temperature. The retrieval properties of the network are discussed in terms of
the four-spin coupling strength and the temperature. It is shown that the
presence of a four-spin coupling enhances the retrieval quality.
</summary>
    <author>
      <name>D. Bolle'</name>
    </author>
    <author>
      <name>G. Jongen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s100510070192</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s100510070192" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 eps figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Eur. J. Phys. B 16, 749-754 (2000)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/9911349v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9911349v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0002335v1</id>
    <updated>2000-02-22T12:37:50Z</updated>
    <published>2000-02-22T12:37:50Z</published>
    <title>Competing neural networks: Finding a strategy for the game of matching
  pennies</title>
    <summary>  The ability of a deterministic, plastic system to learn to imitate stochastic
behavior is analyzed. Two neural networks -actually, two perceptrons- are put
to play a zero-sum game one against the other. The competition, by acting as a
kind of mutually supervised learning, drives the networks to produce an
approximation to the optimal strategy, that is to say, a random signal.
</summary>
    <author>
      <name>I. Samengo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Centro Atomico Bariloche</arxiv:affiliation>
    </author>
    <author>
      <name>D. H. Zanette</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Centro Atomico Bariloche</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.62.4049</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.62.4049" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0002335v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0002335v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0112093v1</id>
    <updated>2001-12-06T13:16:33Z</updated>
    <published>2001-12-06T13:16:33Z</published>
    <title>An optimal Q-state neural network using mutual information</title>
    <summary>  Starting from the mutual information we present a method in order to find a
hamiltonian for a fully connected neural network model with an arbitrary,
finite number of neuron states, Q. For small initial correlations between the
neurons and the patterns it leads to optimal retrieval performance. For binary
neurons, Q=2, and biased patterns we recover the Hopfield model. For
three-state neurons, Q=3, we find back the recently introduced
Blume-Emery-Griffiths network hamiltonian. We derive its phase diagram and
compare it with those of related three-state models. We find that the retrieval
region is the largest.
</summary>
    <author>
      <name>D. Bolle</name>
    </author>
    <author>
      <name>T. Verbeiren</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/S0375-9601(02)00437-1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/S0375-9601(02)00437-1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0112093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0112093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0204130v1</id>
    <updated>2002-04-05T10:25:57Z</updated>
    <published>2002-04-05T10:25:57Z</published>
    <title>Magnetic dot arrays modeling via the system of the radial basis function
  networks</title>
    <summary>  Two dimensional square lattice general model of the magnetic dot array is
introduced. In this model the intradot self-energy is predicted via the neural
network and interdot magnetostatic coupling is approximated by the collection
of several dipolar terms. The model has been applied to disk-shaped cluster
involving 193 ultrathin dots and 772 interaction centers. In this case among
the intradot magnetic structures retrieved by neural networks the important
role play single-vortex magnetization modes. Several aspects of the model have
been understood numerically by means of the simulated annealing method.
</summary>
    <author>
      <name>Denis Horvath</name>
    </author>
    <author>
      <name>Martin Gmitra</name>
    </author>
    <author>
      <name>Ivo Vavra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/S0304-8853(01)00164-0</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/S0304-8853(01)00164-0" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 8 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J. Magn. Magn. Mater. 231 (2001) 273-286</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0204130v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0204130v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0209282v1</id>
    <updated>2002-09-12T10:16:10Z</updated>
    <published>2002-09-12T10:16:10Z</published>
    <title>The attractors in sequence processing neural networks</title>
    <summary>  The average length and average relaxation time of attractors in sequence
processing neural networks are investigated. The simulation results show that a
critical point of $\alpha $, the loading ratio, is found. Below the turning
point, the average length is equal to the number of stored patterns;
conversely, the ratio of length and numbers of stored patterns, grow with an
exponential dependence $\exp (A\alpha) $. Moreover, we find that the logarithm
of average relaxation time is only linearly associated with $\alpha $ and the
turning point of coupling degree is located for examining robustness of
networks.
</summary>
    <author>
      <name>Yong Chen</name>
    </author>
    <author>
      <name>Ying Hai Wang</name>
    </author>
    <author>
      <name>Kong Qing Yang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1142/S0129183100000043</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1142/S0129183100000043" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Int. J. Mod. Phys. C11, 33(2000)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0209282v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0209282v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0512606v1</id>
    <updated>2005-12-23T11:00:38Z</updated>
    <published>2005-12-23T11:00:38Z</published>
    <title>Trapped ion chain as a neural network</title>
    <summary>  We demonstrate the possibility of realizing a neural network in a chain of
trapped ions with induced long range interactions. Such models permit to store
information distributed over the whole system. The storage capacity of such
network, which depends on the phonon spectrum of the system, can be controlled
by changing the external trapping potential and/or by applying longitudinal
local magnetic fields. The system properties suggest the possibility of
implementing robust distributed realizations of quantum logic.
</summary>
    <author>
      <name>M. Pons</name>
    </author>
    <author>
      <name>V. Ahufinger</name>
    </author>
    <author>
      <name>C. Wunderlich</name>
    </author>
    <author>
      <name>A. Sanpera</name>
    </author>
    <author>
      <name>M. Lewenstein</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.98.023003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.98.023003" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0512606v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0512606v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0605590v1</id>
    <updated>2006-05-24T09:38:55Z</updated>
    <published>2006-05-24T09:38:55Z</published>
    <title>Adaptive thresholds for layered neural networks with synaptic noise</title>
    <summary>  The inclusion of a macroscopic adaptive threshold is studied for the
retrieval dynamics of layered feedforward neural network models with synaptic
noise. It is shown that if the threshold is chosen appropriately as a function
of the cross-talk noise and of the activity of the stored patterns, adapting
itself automatically in the course of the recall process, an autonomous
functioning of the network is guaranteed.This self-control mechanism
considerably improves the quality of retrieval, in particular the storage
capacity, the basins of attraction and the mutual information content.
</summary>
    <author>
      <name>D. Bolle</name>
    </author>
    <author>
      <name>R. Heylen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures, accepted for the ICANN 2006 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0605590v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0605590v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9809111v1</id>
    <updated>1998-09-28T03:48:22Z</updated>
    <published>1998-09-28T03:48:22Z</published>
    <title>Evolution of Neural Networks to Play the Game of Dots-and-Boxes</title>
    <summary>  Dots-and-Boxes is a child's game which remains analytically unsolved. We
implement and evolve artificial neural networks to play this game, evaluating
them against simple heuristic players. Our networks do not evaluate or predict
the final outcome of the game, but rather recommend moves at each stage.
Superior generalisation of play by co-evolved populations is found, and a
comparison made with networks trained by back-propagation using simple
heuristics as an oracle.
</summary>
    <author>
      <name>Lex Weaver</name>
    </author>
    <author>
      <name>Terry Bossomaier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures, LaTeX 2.09 (works with LaTeX2e)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Alife V: Poster Presentations, May 16-18 1996, pages 43-50</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9809111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9809111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/hep-ex/9406003v2</id>
    <updated>1994-06-23T22:13:23Z</updated>
    <published>1994-06-21T22:00:31Z</published>
    <title>A Neural Network for Locating the Primary Vertex in a Pixel Detector</title>
    <summary>  Using simulated collider data for $p+p\rightarrow 2{\rm Jets}\ $ interactions
in a 2-barrel pixel detector, a neural network is trained to construct the
coordinate of the primary vertex to a high degree of accuracy. Three other
estimates of this coordinate are also considered and compared to that of the
neural network. It is shown that the network can match the best of the
traditional estimates.
</summary>
    <author>
      <name>R. Kantowski</name>
    </author>
    <author>
      <name>Caren Marzban</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/0168-9002(94)01133-8</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/0168-9002(94)01133-8" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages in Latex, and now with 9 figures as uufiles, OKHEP-94-11</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Nucl.Instrum.Meth.A355:582-588,1995</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/hep-ex/9406003v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/hep-ex/9406003v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/hep-ph/9502367v1</id>
    <updated>1995-02-22T16:21:04Z</updated>
    <published>1995-02-22T16:21:04Z</published>
    <title>JET ANALYSIS BY NEURAL NETWORKS IN HIGH ENERGY HADRON-HADRON COLLISIONS</title>
    <summary>  We study the possibility to employ neural networks to simulate jet clustering
procedures in high energy hadron-hadron collisions. We concentrate our analysis
on the Fermilab Tevatron energy and on the $k_\bot$ algorithm. We consider both
supervised multilayer feed-forward network trained by the backpropagation
algorithm and unsupervised learning, where the neural network autonomously
organizes the events in clusters.
</summary>
    <author>
      <name>P. De Felice</name>
    </author>
    <author>
      <name>G. Nardulli</name>
    </author>
    <author>
      <name>G. Pasquariello</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/0370-2693(95)00608-N</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/0370-2693(95)00608-N" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, latex, 2 figures not included.</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys.Lett. B354 (1995) 473-480</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/hep-ph/9502367v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/hep-ph/9502367v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/physics/0607046v1</id>
    <updated>2006-07-06T04:16:57Z</updated>
    <published>2006-07-06T04:16:57Z</published>
    <title>A Global Algorithm for Training Multilayer Neural Networks</title>
    <summary>  We present a global algorithm for training multilayer neural networks in this
Letter. The algorithm is focused on controlling the local fields of neurons
induced by the input of samples by random adaptations of the synaptic weights.
Unlike the backpropagation algorithm, the networks may have discrete-state
weights, and may apply either differentiable or nondifferentiable neural
transfer functions. A two-layer network is trained as an example to separate a
linearly inseparable set of samples into two categories, and its powerful
generalization capacity is emphasized. The extension to more general cases is
straightforward.
</summary>
    <author>
      <name>Hong Zhao</name>
    </author>
    <author>
      <name>Tao Jin</name>
    </author>
    <link href="http://arxiv.org/abs/physics/0607046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/physics/0607046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.1457v1</id>
    <updated>2013-04-04T18:29:28Z</updated>
    <published>2013-04-04T18:29:28Z</published>
    <title>Cross-talk and transitions between multiple spatial maps in an attractor
  neural network model of the hippocampus: phase diagram (I)</title>
    <summary>  We study the stable phases of an attractor neural network model, with binary
units, for hippocampal place cells encoding 1D or 2D spatial maps or
environments. Using statistical mechanics tools we show that, below critical
values for the noise in the neural response and for the number of environments,
the network activity is spatially localized in one environment. We calculate
the number of stored environments. For high noise and loads the network
activity extends over space, either uniformly or with spatial heterogeneities
due to the cross-talk between the maps, and memory of environments is lost.
Analytical predictions are corroborated by numerical simulations.
</summary>
    <author>
      <name>R√©mi Monasson</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LPTENS</arxiv:affiliation>
    </author>
    <author>
      <name>Sophie Rosay</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LPTENS</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.87.062813</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.87.062813" rel="related"/>
    <link href="http://arxiv.org/abs/1304.1457v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.1457v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.4164v1</id>
    <updated>2013-03-18T06:18:25Z</updated>
    <published>2013-03-18T06:18:25Z</published>
    <title>Neurally Implementable Semantic Networks</title>
    <summary>  We propose general principles for semantic networks allowing them to be
implemented as dynamical neural networks. Major features of our scheme include:
(a) the interpretation that each node in a network stands for a bound
integration of the meanings of all nodes and external events the node links
with; (b) the systematic use of nodes that stand for categories or types, with
separate nodes for instances of these types; (c) an implementation of
relationships that does not use intrinsically typed links between nodes.
</summary>
    <author>
      <name>Garrett N. Evans</name>
    </author>
    <author>
      <name>John C. Collins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.4164v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.4164v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.4; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.7326v3</id>
    <updated>2015-11-03T21:21:41Z</updated>
    <published>2014-10-27T17:46:28Z</published>
    <title>Neuroevolution in Games: State of the Art and Open Challenges</title>
    <summary>  This paper surveys research on applying neuroevolution (NE) to games. In
neuroevolution, artificial neural networks are trained through evolutionary
algorithms, taking inspiration from the way biological brains evolved. We
analyse the application of NE in games along five different axes, which are the
role NE is chosen to play in a game, the different types of neural networks
used, the way these networks are evolved, how the fitness is determined and
what type of input the network receives. The article also highlights important
open research challenges in the field.
</summary>
    <author>
      <name>Sebastian Risi</name>
    </author>
    <author>
      <name>Julian Togelius</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">- Added more references - Corrected typos - Added an overview table
  (Table 1)</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.7326v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.7326v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.2535v1</id>
    <updated>2014-09-08T21:46:04Z</updated>
    <published>2014-09-08T21:46:04Z</published>
    <title>Dynamics of Random Neural Networks with Bistable Units</title>
    <summary>  We construct and analyze a rate-based neural network model in which
self-interacting units represent clusters of neurons with strong local
connectivity and random inter-unit connections reflect long-range interactions.
When sufficiently strong, the self-interactions make the individual units
bistable. Simulation results, mean-field calculations and stability analysis
reveal the different dynamic regimes of this network and identify the locations
in parameter space of its phase transitions. We identify an interesting
dynamical regime exhibiting transient but long-lived chaotic activity that
combines features of chaotic and multiple fixed-point attractors.
</summary>
    <author>
      <name>Merav Stern</name>
    </author>
    <author>
      <name>Haim Sompolinsky</name>
    </author>
    <author>
      <name>L. F. Abbott</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.90.062710</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.90.062710" rel="related"/>
    <link href="http://arxiv.org/abs/1409.2535v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.2535v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.02551v1</id>
    <updated>2015-02-09T16:37:29Z</updated>
    <published>2015-02-09T16:37:29Z</published>
    <title>Deep Learning with Limited Numerical Precision</title>
    <summary>  Training of large-scale deep neural networks is often constrained by the
available computational resources. We study the effect of limited precision
data representation and computation on neural network training. Within the
context of low-precision fixed-point computations, we observe the rounding
scheme to play a crucial role in determining the network's behavior during
training. Our results show that deep networks can be trained using only 16-bit
wide fixed-point number representation when using stochastic rounding, and
incur little to no degradation in the classification accuracy. We also
demonstrate an energy-efficient hardware accelerator that implements
low-precision fixed-point arithmetic with stochastic rounding.
</summary>
    <author>
      <name>Suyog Gupta</name>
    </author>
    <author>
      <name>Ankur Agrawal</name>
    </author>
    <author>
      <name>Kailash Gopalakrishnan</name>
    </author>
    <author>
      <name>Pritish Narayanan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.02551v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.02551v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.03648v1</id>
    <updated>2015-02-12T13:29:03Z</updated>
    <published>2015-02-12T13:29:03Z</published>
    <title>Over-Sampling in a Deep Neural Network</title>
    <summary>  Deep neural networks (DNN) are the state of the art on many engineering
problems such as computer vision and audition. A key factor in the success of
the DNN is scalability - bigger networks work better. However, the reason for
this scalability is not yet well understood. Here, we interpret the DNN as a
discrete system, of linear filters followed by nonlinear activations, that is
subject to the laws of sampling theory. In this context, we demonstrate that
over-sampled networks are more selective, learn faster and learn more robustly.
Our findings may ultimately generalize to the human brain.
</summary>
    <author>
      <name>Andrew J. R. Simpson</name>
    </author>
    <link href="http://arxiv.org/abs/1502.03648v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.03648v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.06158v1</id>
    <updated>2015-06-19T21:05:01Z</updated>
    <published>2015-06-19T21:05:01Z</published>
    <title>Structured Training for Neural Network Transition-Based Parsing</title>
    <summary>  We present structured perceptron training for neural network transition-based
dependency parsing. We learn the neural network representation using a gold
corpus augmented by a large number of automatically parsed sentences. Given
this fixed network representation, we learn a final layer using the structured
perceptron with beam-search decoding. On the Penn Treebank, our parser reaches
94.26% unlabeled and 92.41% labeled attachment accuracy, which to our knowledge
is the best accuracy on Stanford Dependencies to date. We also provide in-depth
ablative analysis to determine which aspects of our model provide the largest
gains in accuracy.
</summary>
    <author>
      <name>David Weiss</name>
    </author>
    <author>
      <name>Chris Alberti</name>
    </author>
    <author>
      <name>Michael Collins</name>
    </author>
    <author>
      <name>Slav Petrov</name>
    </author>
    <link href="http://arxiv.org/abs/1506.06158v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.06158v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.08861v2</id>
    <updated>2016-06-14T21:35:48Z</updated>
    <published>2015-11-28T02:02:44Z</published>
    <title>Loss Functions for Neural Networks for Image Processing</title>
    <summary>  Neural networks are becoming central in several areas of computer vision and
image processing and different architectures have been proposed to solve
specific problems. The impact of the loss layer of neural networks, however,
has not received much attention in the context of image processing: the default
and virtually only choice is L2. In this paper we bring attention to
alternative choices. We study the performance of several losses, including
perceptually-motivated losses, and propose a novel, differentiable error
function. We show that the quality of the results improves significantly with
better loss functions, even when the network architecture is left unchanged.
</summary>
    <author>
      <name>Hang Zhao</name>
    </author>
    <author>
      <name>Orazio Gallo</name>
    </author>
    <author>
      <name>Iuri Frosio</name>
    </author>
    <author>
      <name>Jan Kautz</name>
    </author>
    <link href="http://arxiv.org/abs/1511.08861v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.08861v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05101v1</id>
    <updated>2016-05-17T10:43:38Z</updated>
    <published>2016-05-17T10:43:38Z</published>
    <title>Recurrent Neural Network for Text Classification with Multi-Task
  Learning</title>
    <summary>  Neural network based methods have obtained great progress on a variety of
natural language processing tasks. However, in most previous works, the models
are learned based on single-task supervised objectives, which often suffer from
insufficient training data. In this paper, we use the multi-task learning
framework to jointly learn across multiple related tasks. Based on recurrent
neural network, we propose three different mechanisms of sharing information to
model text with task-specific and shared layers. The entire network is trained
jointly on all these tasks. Experiments on four benchmark text classification
tasks show that our proposed models can improve the performance of a task with
the help of other related tasks.
</summary>
    <author>
      <name>Pengfei Liu</name>
    </author>
    <author>
      <name>Xipeng Qiu</name>
    </author>
    <author>
      <name>Xuanjing Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IJCAI 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.05101v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05101v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05296v1</id>
    <updated>2016-05-17T19:29:37Z</updated>
    <published>2016-05-17T19:29:37Z</published>
    <title>Dataflow matrix machines as programmable, dynamically expandable,
  self-referential generalized recurrent neural networks</title>
    <summary>  Dataflow matrix machines are a powerful generalization of recurrent neural
networks. They work with multiple types of linear streams and multiple types of
neurons, including higher-order neurons which dynamically update the matrix
describing weights and topology of the network in question while the network is
running. It seems that the power of dataflow matrix machines is sufficient for
them to be a convenient general purpose programming platform. This paper
explores a number of useful programming idioms and constructions arising in
this context.
</summary>
    <author>
      <name>Michael Bukatin</name>
    </author>
    <author>
      <name>Steve Matthews</name>
    </author>
    <author>
      <name>Andrey Radul</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.05296v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05296v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03024v2</id>
    <updated>2017-11-24T13:47:36Z</updated>
    <published>2016-09-10T10:18:30Z</published>
    <title>Rectifier Neural Network with a Dual-Pathway Architecture for Image
  Denoising</title>
    <summary>  Recently deep neural networks based on tanh activation function have shown
their impressive power in image denoising. In this letter, we try to use
rectifier function instead of tanh and propose a dual-pathway rectifier neural
network by combining two rectifier neurons with reversed input and output
weights in the same hidden layer. We drive the equivalent activation function
and compare it to some typical activation functions for image denoising under
the same network architecture. The experimental results show that our model
achieves superior performances faster especially when the noise is small.
</summary>
    <author>
      <name>Keting Zhang</name>
    </author>
    <author>
      <name>Liqing Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.03024v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03024v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.00591v1</id>
    <updated>2016-09-04T16:20:13Z</updated>
    <published>2016-09-04T16:20:13Z</published>
    <title>Deep Neural Networks for HDR imaging</title>
    <summary>  We propose novel methods of solving two tasks using Convolutional Neural
Networks, firstly the task of generating HDR map of a static scene using
differently exposed LDR images of the scene captured using conventional cameras
and secondly the task of finding an optimal tone mapping operator that would
give a better score on the TMQI metric compared to the existing methods. We
quantitatively show the performance of our networks and illustrate the cases
where our networks performs good as well as bad.
</summary>
    <author>
      <name>Kshiteej Sheth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.00591v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.00591v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.04358v2</id>
    <updated>2016-11-15T14:41:23Z</updated>
    <published>2016-11-14T12:24:27Z</published>
    <title>Character-level Convolutional Network for Text Classification Applied to
  Chinese Corpus</title>
    <summary>  This article provides an interesting exploration of character-level
convolutional neural network solving Chinese corpus text classification
problem. We constructed a large-scale Chinese language dataset, and the result
shows that character-level convolutional neural network works better on Chinese
corpus than its corresponding pinyin format dataset. This is the first time
that character-level convolutional neural network applied to text
classification problem.
</summary>
    <author>
      <name>Weijie Huang</name>
    </author>
    <author>
      <name>Jun Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MSc Thesis, 44 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.04358v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.04358v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09428v1</id>
    <updated>2016-11-28T23:35:25Z</updated>
    <published>2016-11-28T23:35:25Z</published>
    <title>Stochastic Thermodynamics of Learning</title>
    <summary>  Virtually every organism gathers information about its noisy environment and
builds models from that data, mostly using neural networks. Here, we use
stochastic thermodynamics to analyse the learning of a classification rule by a
neural network. We show that the information acquired by the network is bounded
by the thermodynamic cost of learning and introduce a learning efficiency
$\eta\le1$. We discuss the conditions for optimal learning and analyse Hebbian
learning in the thermodynamic limit.
</summary>
    <author>
      <name>Sebastian Goldt</name>
    </author>
    <author>
      <name>Udo Seifert</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.118.010601</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.118.010601" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures, 7 pages of supplemental material</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. Lett. 118, 010601 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1611.09428v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09428v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.04118v2</id>
    <updated>2017-01-24T01:01:28Z</updated>
    <published>2016-12-13T12:12:20Z</published>
    <title>Information Extraction with Character-level Neural Networks and Free
  Noisy Supervision</title>
    <summary>  We present an architecture for information extraction from text that augments
an existing parser with a character-level neural network. The network is
trained using a measure of consistency of extracted data with existing
databases as a form of noisy supervision. Our architecture combines the ability
of constraint-based information extraction systems to easily incorporate domain
knowledge and constraints with the ability of deep neural networks to leverage
large amounts of data to learn complex features. Boosting the existing parser's
precision, the system led to large improvements over a mature and highly tuned
constraint-based production information extraction system used at Bloomberg for
financial language text.
</summary>
    <author>
      <name>Philipp Meerkamp</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Bloomberg LP</arxiv:affiliation>
    </author>
    <author>
      <name>Zhengyi Zhou</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">AT&amp;T Labs Research</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1612.04118v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.04118v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.04426v1</id>
    <updated>2016-12-13T23:09:49Z</updated>
    <published>2016-12-13T23:09:49Z</published>
    <title>Improving Neural Language Models with a Continuous Cache</title>
    <summary>  We propose an extension to neural network language models to adapt their
prediction to the recent history. Our model is a simplified version of memory
augmented networks, which stores past hidden activations as memory and accesses
them through a dot product with the current hidden activation. This mechanism
is very efficient and scales to very large memory sizes. We also draw a link
between the use of external memory in neural network and cache models used with
count based language models. We demonstrate on several language model datasets
that our approach performs significantly better than recent memory augmented
networks.
</summary>
    <author>
      <name>Edouard Grave</name>
    </author>
    <author>
      <name>Armand Joulin</name>
    </author>
    <author>
      <name>Nicolas Usunier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ICLR 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.04426v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.04426v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05159v1</id>
    <updated>2017-01-18T17:37:35Z</updated>
    <published>2017-01-18T17:37:35Z</published>
    <title>Temporal Overdrive Recurrent Neural Network</title>
    <summary>  In this work we present a novel recurrent neural network architecture
designed to model systems characterized by multiple characteristic timescales
in their dynamics. The proposed network is composed by several recurrent groups
of neurons that are trained to separately adapt to each timescale, in order to
improve the system identification process. We test our framework on time series
prediction tasks and we show some promising, preliminary results achieved on
synthetic data. To evaluate the capabilities of our network, we compare the
performance with several state-of-the-art recurrent architectures.
</summary>
    <author>
      <name>Filippo Maria Bianchi</name>
    </author>
    <author>
      <name>Michael Kampffmeyer</name>
    </author>
    <author>
      <name>Enrico Maiorino</name>
    </author>
    <author>
      <name>Robert Jenssen</name>
    </author>
    <link href="http://arxiv.org/abs/1701.05159v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05159v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.08231v1</id>
    <updated>2017-02-27T11:10:54Z</updated>
    <published>2017-02-27T11:10:54Z</published>
    <title>Low-Precision Batch-Normalized Activations</title>
    <summary>  Artificial neural networks can be trained with relatively low-precision
floating-point and fixed-point arithmetic, using between one and 16 bits.
Previous works have focused on relatively wide-but-shallow, feed-forward
networks. We introduce a quantization scheme that is compatible with training
very deep neural networks. Quantizing the network activations in the middle of
each batch-normalization module can greatly reduce the amount of memory and
computational power needed, with little loss in accuracy.
</summary>
    <author>
      <name>Benjamin Graham</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.08231v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.08231v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.05502v1</id>
    <updated>2017-05-16T02:02:24Z</updated>
    <published>2017-05-16T02:02:24Z</published>
    <title>The power of deeper networks for expressing natural functions</title>
    <summary>  It is well-known that neural networks are universal approximators, but that
deeper networks tend to be much more efficient than shallow ones. We shed light
on this by proving that the total number of neurons $m$ required to approximate
natural classes of multivariate polynomials of $n$ variables grows only
linearly with $n$ for deep neural networks, but grows exponentially when merely
a single hidden layer is allowed. We also provide evidence that when the number
of hidden layers is increased from $1$ to $k$, the neuron requirement grows
exponentially not with $n$ but with $n^{1/k}$, suggesting that the minimum
number of layers required for computational tractability grows only
logarithmically with $n$.
</summary>
    <author>
      <name>David Rolnick</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MIT</arxiv:affiliation>
    </author>
    <author>
      <name>Max Tegmark</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MIT</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 2 figs</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.05502v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.05502v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07049v1</id>
    <updated>2017-05-19T15:25:03Z</updated>
    <published>2017-05-19T15:25:03Z</published>
    <title>What are the Receptive, Effective Receptive, and Projective Fields of
  Neurons in Convolutional Neural Networks?</title>
    <summary>  In this work, we explain in detail how receptive fields, effective receptive
fields, and projective fields of neurons in different layers, convolution or
pooling, of a Convolutional Neural Network (CNN) are calculated. While our
focus here is on CNNs, the same operations, but in the reverse order, can be
used to calculate these quantities for deconvolutional neural networks. These
are important concepts, not only for better understanding and analyzing
convolutional and deconvolutional networks, but also for optimizing their
performance in real-world applications.
</summary>
    <author>
      <name>Hung Le</name>
    </author>
    <author>
      <name>Ali Borji</name>
    </author>
    <link href="http://arxiv.org/abs/1705.07049v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07049v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.10388v1</id>
    <updated>2017-05-29T20:35:42Z</updated>
    <published>2017-05-29T20:35:42Z</published>
    <title>Model Selection in Bayesian Neural Networks via Horseshoe Priors</title>
    <summary>  Bayesian Neural Networks (BNNs) have recently received increasing attention
for their ability to provide well-calibrated posterior uncertainties. However,
model selection---even choosing the number of nodes---remains an open question.
In this work, we apply a horseshoe prior over node pre-activations of a
Bayesian neural network, which effectively turns off nodes that do not help
explain the data. We demonstrate that our prior prevents the BNN from
under-fitting even when the number of nodes required is grossly over-estimated.
Moreover, this model selection over the number of nodes doesn't come at the
expense of predictive or computational performance; in fact, we learn smaller
networks with comparable predictive performance to current approaches.
</summary>
    <author>
      <name>Soumya Ghosh</name>
    </author>
    <author>
      <name>Finale Doshi-Velez</name>
    </author>
    <link href="http://arxiv.org/abs/1705.10388v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.10388v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.06123v1</id>
    <updated>2017-09-18T19:01:53Z</updated>
    <published>2017-09-18T19:01:53Z</published>
    <title>A Probabilistic Framework for Nonlinearities in Stochastic Neural
  Networks</title>
    <summary>  We present a probabilistic framework for nonlinearities, based on doubly
truncated Gaussian distributions. By setting the truncation points
appropriately, we are able to generate various types of nonlinearities within a
unified framework, including sigmoid, tanh and ReLU, the most commonly used
nonlinearities in neural networks. The framework readily integrates into
existing stochastic neural networks (with hidden units characterized as random
variables), allowing one for the first time to learn the nonlinearities
alongside model weights in these networks. Extensive experiments demonstrate
the performance improvements brought about by the proposed framework when
integrated with the restricted Boltzmann machine (RBM), temporal RBM and the
truncated Gaussian graphical model (TGGM).
</summary>
    <author>
      <name>Qinliang Su</name>
    </author>
    <author>
      <name>Xuejun Liao</name>
    </author>
    <author>
      <name>Lawrence Carin</name>
    </author>
    <link href="http://arxiv.org/abs/1709.06123v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.06123v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.04744v1</id>
    <updated>2017-10-12T22:50:00Z</updated>
    <published>2017-10-12T22:50:00Z</published>
    <title>Can the early human visual system compete with Deep Neural Networks?</title>
    <summary>  We study and compare the human visual system and state-of-the-art deep neural
networks on classification of distorted images. Different from previous works,
we limit the display time to 100ms to test only the early mechanisms of the
human visual system, without allowing time for any eye movements or other
higher level processes. Our findings show that the human visual system still
outperforms modern deep neural networks under blurry and noisy images. These
findings motivate future research into developing more robust deep networks.
</summary>
    <author>
      <name>Samuel Dodge</name>
    </author>
    <author>
      <name>Lina Karam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as an oral paper at the Mutual Benefits of Cognitive and
  Computer Vision Workshop (held in conjunction with ICCV2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.04744v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.04744v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.10453v1</id>
    <updated>2017-10-28T12:00:09Z</updated>
    <published>2017-10-28T12:00:09Z</published>
    <title>Inducing Regular Grammars Using Recurrent Neural Networks</title>
    <summary>  Grammar induction is the task of learning a grammar from a set of examples.
Recently, neural networks have been shown to be powerful learning machines that
can identify patterns in streams of data. In this work we investigate their
effectiveness in inducing a regular grammar from data, without any assumptions
about the grammar. We train a recurrent neural network to distinguish between
strings that are in or outside a regular language, and utilize an algorithm for
extracting the learned finite-state automaton. We apply this method to several
regular languages and find unexpected results regarding the connections between
the network's states that may be regarded as evidence for generalization.
</summary>
    <author>
      <name>Mor Cohen</name>
    </author>
    <author>
      <name>Avi Caciularu</name>
    </author>
    <author>
      <name>Idan Rejwan</name>
    </author>
    <author>
      <name>Jonathan Berant</name>
    </author>
    <link href="http://arxiv.org/abs/1710.10453v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.10453v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.05865v2</id>
    <updated>2017-11-18T00:46:39Z</updated>
    <published>2017-11-16T00:14:49Z</published>
    <title>Pricing Football Players using Neural Networks</title>
    <summary>  We designed a multilayer perceptron neural network to predict the price of a
football (soccer) player using data on more than 15,000 players from the
football simulation video game FIFA 2017. The network was optimized by
experimenting with different activation functions, number of neurons and
layers, learning rate and its decay, Nesterov momentum based stochastic
gradient descent, L2 regularization, and early stopping. Simultaneous
exploration of various aspects of neural network training is performed and
their trade-offs are investigated. Our final model achieves a top-5 accuracy of
87.2% among 119 pricing categories, and places any footballer within 6.32% of
his actual price on average.
</summary>
    <author>
      <name>Sourya Dey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages technical report (v2: Revised wording and formatting from
  v1)</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.05865v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.05865v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.06636v1</id>
    <updated>2017-11-17T17:22:30Z</updated>
    <published>2017-11-17T17:22:30Z</published>
    <title>Segmenting Brain Tumors with Symmetry</title>
    <summary>  We explore encoding brain symmetry into a neural network for a brain tumor
segmentation task. A healthy human brain is symmetric at a high level of
abstraction, and the high-level asymmetric parts are more likely to be tumor
regions. Paying more attention to asymmetries has the potential to boost the
performance in brain tumor segmentation. We propose a method to encode brain
symmetry into existing neural networks and apply the method to a
state-of-the-art neural network for medical imaging segmentation. We evaluate
our symmetry-encoded network on the dataset from a brain tumor segmentation
challenge and verify that the new model extracts information in the training
images more efficiently than the original model.
</summary>
    <author>
      <name>Hejia Zhang</name>
    </author>
    <author>
      <name>Xia Zhu</name>
    </author>
    <author>
      <name>Theodore L. Willke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NIPS ML4H Workshop 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.06636v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.06636v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.06935v1</id>
    <updated>2017-11-18T22:40:27Z</updated>
    <published>2017-11-18T22:40:27Z</published>
    <title>Interleaver Design for Deep Neural Networks</title>
    <summary>  We propose a class of interleavers for a novel deep neural network (DNN)
architecture that uses algorithmically pre-determined, structured sparsity to
significantly lower memory and computational requirements, and speed up
training. The interleavers guarantee clash-free memory accesses to eliminate
idle operational cycles, optimize spread and dispersion to improve network
performance, and are designed to ease the complexity of memory address
computations in hardware. We present a design algorithm with mathematical
proofs for these properties. We also explore interleaver variations and analyze
the behavior of neural networks as a function of interleaver metrics.
</summary>
    <author>
      <name>Sourya Dey</name>
    </author>
    <author>
      <name>Peter A. Beerel</name>
    </author>
    <author>
      <name>Keith M. Chugg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 2017 Asilomar Conference on Signals, Systems, and
  Computers</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.06935v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.06935v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.07546v1</id>
    <updated>2017-11-20T21:13:06Z</updated>
    <published>2017-11-20T21:13:06Z</published>
    <title>SPARE: Spiking Networks Acceleration Using CMOS ROM-Embedded RAM as an
  In-Memory-Computation Primitive</title>
    <summary>  Data-intensive applications, like neural networks, require frequent and
extensive data transfers between the memory and the CPU. The limited bandwidth
and latency of these memory transactions results in von-Neumann bottlenecks in
most state-of-the-art hardware implementations. To that effect, we propose
SPARE, an in-memory processing architecture built on CMOS ROM-embedded RAM
(ROAM) for accelerating neural networks. The neuro-synaptic models are stored
in lookup tables locally, using the spare (no extra area) ROM (endowed by
ROAM), thereby alleviating the von-Neumann bottleneck. We evaluate SPARE for
spiking neural networks and obtain 2x, 1.9x and 1.5x improvement in energy,
performance, and area, respectively, compared to similar architecture built
with typical SRAMs.
</summary>
    <author>
      <name>Amogh Agrawal</name>
    </author>
    <author>
      <name>Aayush Ankit</name>
    </author>
    <author>
      <name>Kaushik Roy</name>
    </author>
    <link href="http://arxiv.org/abs/1711.07546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.07546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.05695v1</id>
    <updated>2017-12-15T14:56:05Z</updated>
    <published>2017-12-15T14:56:05Z</published>
    <title>Lightweight Neural Networks</title>
    <summary>  Most of the weights in a Lightweight Neural Network have a value of zero,
while the remaining ones are either +1 or -1. These universal approximators
require approximately 1.1 bits/weight of storage, posses a quick forward pass
and achieve classification accuracies similar to conventional continuous-weight
networks. Their training regimen focuses on error reduction initially, but
later emphasizes discretization of weights. They ignore insignificant inputs,
remove unnecessary weights, and drop unneeded hidden neurons. We have
successfully tested them on the MNIST, credit card fraud, and credit card
defaults data sets using networks having 2 to 16 hidden layers and up to 4.4
million weights.
</summary>
    <author>
      <name>Altaf H. Khan</name>
    </author>
    <link href="http://arxiv.org/abs/1712.05695v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.05695v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.06541v2</id>
    <updated>2018-01-10T14:36:45Z</updated>
    <published>2017-12-18T17:26:15Z</published>
    <title>Size-Independent Sample Complexity of Neural Networks</title>
    <summary>  We study the sample complexity of learning neural networks, by providing new
bounds on their Rademacher complexity assuming norm constraints on the
parameter matrix of each layer. Compared to previous work, these complexity
bounds have improved dependence on the network depth, and under some additional
assumptions, are fully independent of the network size (both depth and width).
These results are derived using some novel techniques, which may be of
independent interest.
</summary>
    <author>
      <name>Noah Golowich</name>
    </author>
    <author>
      <name>Alexander Rakhlin</name>
    </author>
    <author>
      <name>Ohad Shamir</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Discussion of Bartlett et al. [2017] and associated corollaries were
  updated, to reflect that paper's most recent version</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.06541v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.06541v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.06422v1</id>
    <updated>2018-01-19T14:41:45Z</updated>
    <published>2018-01-19T14:41:45Z</published>
    <title>Evaluating neural network explanation methods using hybrid documents and
  morphological prediction</title>
    <summary>  We propose two novel paradigms for evaluating neural network explanations in
NLP. The first paradigm works on hybrid documents, the second exploits
morphosyntactic agreements. Neither paradigm requires manual annotations;
instead, a relevance ground truth is generated automatically. In our
experiments, successful explanations for Long Short Term Memory networks
(LSTMs) were produced by a decomposition of memory cells (Murdoch &amp; Szlam,
2017), while for convolutional neural networks, a gradient-based method by
(Denil et al., 2014) works well. We also introduce LIMSSE, a substring-based
extension of LIME (Ribeiro et al., 2016) that produces the most successful
explanations in the hybrid document experiment.
</summary>
    <author>
      <name>Nina Poerner</name>
    </author>
    <author>
      <name>Hinrich Sch√ºtze</name>
    </author>
    <author>
      <name>Benjamin Roth</name>
    </author>
    <link href="http://arxiv.org/abs/1801.06422v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.06422v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.06801v1</id>
    <updated>2018-01-21T10:17:33Z</updated>
    <published>2018-01-21T10:17:33Z</published>
    <title>Curvature-based Comparison of Two Neural Networks</title>
    <summary>  In this paper we show the similarities and differences of two deep neural
networks by comparing the manifolds composed of activation vectors in each
fully connected layer of them. The main contribution of this paper includes 1)
a new data generating algorithm which is crucial for determining the dimension
of manifolds; 2) a systematic strategy to compare manifolds. Especially, we
take Riemann curvature and sectional curvature as part of criterion, which can
reflect the intrinsic geometric properties of manifolds. Some interesting
results and phenomenon are given, which help in specifying the similarities and
differences between the features extracted by two networks and demystifying the
intrinsic mechanism of deep neural networks.
</summary>
    <author>
      <name>Tao Yu</name>
    </author>
    <author>
      <name>Huan Long</name>
    </author>
    <author>
      <name>John E. Hopcroft</name>
    </author>
    <link href="http://arxiv.org/abs/1801.06801v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.06801v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.08539v1</id>
    <updated>2018-02-23T14:19:51Z</updated>
    <published>2018-02-23T14:19:51Z</published>
    <title>Computation of optimal transport and related hedging problems via
  penalization and neural networks</title>
    <summary>  This paper presents a widely applicable approach to solving (multi-marginal,
martingale) optimal transport and related problems via neural networks. The
core idea is to penalize the optimization problem in its dual formulation and
reduce it to a finite dimensional one which corresponds to optimizing a neural
network with smooth objective function. We present numerical examples from
optimal transport, martingale optimal transport, portfolio optimization under
uncertainty and generative adversarial networks that showcase the generality
and effectiveness of the approach.
</summary>
    <author>
      <name>Stephan Eckstein</name>
    </author>
    <author>
      <name>Michael Kupper</name>
    </author>
    <link href="http://arxiv.org/abs/1802.08539v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.08539v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.MF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.1801v1</id>
    <updated>2013-10-07T14:37:56Z</updated>
    <published>2013-10-07T14:37:56Z</published>
    <title>On the electrodynamics of neural networks</title>
    <summary>  We present a microscopic approach for the coupling of cortical activity, as
resulting from proper dipole currents of pyramidal neurons, to the
electromagnetic field in extracellular fluid in presence of diffusion and Ohmic
conduction. Starting from a full-fledged three-compartment model of a single
pyramidal neuron, including shunting and dendritic propagation, we derive an
observation model for dendritic dipole currents in extracellular space and
thereby for the dendritic field potential that contributes to the local field
potential of a neural population. Under reasonable simplifications, we then
derive a leaky integrate-and-fire model for the dynamics of a neural network,
which facilitates comparison with existing neural network and observation
models. In particular, we compare our results with a related model by means of
numerical simulations. Performing a continuum limit, neural activity becomes
represented by a neural field equation, while an observation model for electric
field potentials is obtained from the interaction of cortical dipole currents
with charge density in non-resistive extracellular space as described by the
Nernst-Planck equation. Our work consistently satisfies the widespread dipole
assumption discussed in the neuroscientific literature.
</summary>
    <author>
      <name>Peter beim Graben</name>
    </author>
    <author>
      <name>Serafim Rodrigues</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages; 3 figures; to appear in "Neural Fields: Theory and
  Applications", edited by S. Coombes, P. beim Graben, R. Potthast, and J. J.
  Wright; Springer Verlag Berlin (2014)</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.1801v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.1801v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.1259v2</id>
    <updated>2014-10-07T18:08:30Z</updated>
    <published>2014-09-03T21:03:41Z</published>
    <title>On the Properties of Neural Machine Translation: Encoder-Decoder
  Approaches</title>
    <summary>  Neural machine translation is a relatively new approach to statistical
machine translation based purely on neural networks. The neural machine
translation models often consist of an encoder and a decoder. The encoder
extracts a fixed-length representation from a variable-length input sentence,
and the decoder generates a correct translation from this representation. In
this paper, we focus on analyzing the properties of the neural machine
translation using two models; RNN Encoder--Decoder and a newly proposed gated
recursive convolutional neural network. We show that the neural machine
translation performs relatively well on short sentences without unknown words,
but its performance degrades rapidly as the length of the sentence and the
number of unknown words increase. Furthermore, we find that the proposed gated
recursive convolutional network learns a grammatical structure of a sentence
automatically.
</summary>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <author>
      <name>Bart van Merrienboer</name>
    </author>
    <author>
      <name>Dzmitry Bahdanau</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Eighth Workshop on Syntax, Semantics and Structure in Statistical
  Translation (SSST-8)</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.1259v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.1259v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.06576v2</id>
    <updated>2015-09-02T08:24:59Z</updated>
    <published>2015-08-26T17:14:42Z</published>
    <title>A Neural Algorithm of Artistic Style</title>
    <summary>  In fine art, especially painting, humans have mastered the skill to create
unique visual experiences through composing a complex interplay between the
content and style of an image. Thus far the algorithmic basis of this process
is unknown and there exists no artificial system with similar capabilities.
However, in other key areas of visual perception such as object and face
recognition near-human performance was recently demonstrated by a class of
biologically inspired vision models called Deep Neural Networks. Here we
introduce an artificial system based on a Deep Neural Network that creates
artistic images of high perceptual quality. The system uses neural
representations to separate and recombine content and style of arbitrary
images, providing a neural algorithm for the creation of artistic images.
Moreover, in light of the striking similarities between performance-optimised
artificial neural networks and biological vision, our work offers a path
forward to an algorithmic understanding of how humans create and perceive
artistic imagery.
</summary>
    <author>
      <name>Leon A. Gatys</name>
    </author>
    <author>
      <name>Alexander S. Ecker</name>
    </author>
    <author>
      <name>Matthias Bethge</name>
    </author>
    <link href="http://arxiv.org/abs/1508.06576v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.06576v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.07792v1</id>
    <updated>2017-07-25T02:29:31Z</updated>
    <published>2017-07-25T02:29:31Z</published>
    <title>Integrating Lexical and Temporal Signals in Neural Ranking Models for
  Searching Social Media Streams</title>
    <summary>  Time is an important relevance signal when searching streams of social media
posts. The distribution of document timestamps from the results of an initial
query can be leveraged to infer the distribution of relevant documents, which
can then be used to rerank the initial results. Previous experiments have shown
that kernel density estimation is a simple yet effective implementation of this
idea. This paper explores an alternative approach to mining temporal signals
with recurrent neural networks. Our intuition is that neural networks provide a
more expressive framework to capture the temporal coherence of neighboring
documents in time. To our knowledge, we are the first to integrate lexical and
temporal signals in an end-to-end neural network architecture, in which
existing neural ranking models are used to generate query-document similarity
vectors that feed into a bidirectional LSTM layer for temporal modeling. Our
results are mixed: existing neural models for document ranking alone yield
limited improvements over simple baselines, but the integration of lexical and
temporal signals yield significant improvements over competitive temporal
baselines.
</summary>
    <author>
      <name>Jinfeng Rao</name>
    </author>
    <author>
      <name>Hua He</name>
    </author>
    <author>
      <name>Haotian Zhang</name>
    </author>
    <author>
      <name>Ferhan Ture</name>
    </author>
    <author>
      <name>Royal Sequiera</name>
    </author>
    <author>
      <name>Salman Mohammed</name>
    </author>
    <author>
      <name>Jimmy Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SIGIR 2017 Workshop on Neural Information Retrieval (Neu-IR'17),
  August 7-11, 2017, Shinjuku, Tokyo, Japan</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.07792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.07792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.5331v1</id>
    <updated>2013-02-21T16:40:14Z</updated>
    <published>2013-02-21T16:40:14Z</published>
    <title>Spreading dynamics on spatially constrained complex brain networks</title>
    <summary>  The study of dynamical systems defined on complex networks provides a natural
framework with which to investigate myriad features of neural dynamics, and has
been widely undertaken. Typically, however, networks employed in theoretical
studies bear little relation to the spatial embedding or connectivity of the
neural networks that they attempt to replicate. Here, we employ detailed
neuroimaging data to define a network whose spatial embedding represents
accurately the folded structure of the cortical surface of a rat and
investigate the propagation of activity over this network under simple
spreading and connectivity rules. By comparison with standard network models
with the same coarse statistics, we show that the cortical geometry influences
profoundly the speed propagation of activation through the network. Our
conclusions are of high relevance to the theoretical modelling of epileptic
seizure events, and indicate that such studies which omit physiological network
structure risk simplifying the dynamics in a potentially significant way.
</summary>
    <author>
      <name>Reuben O'Dea</name>
    </author>
    <author>
      <name>Jonathan J. Crofts</name>
    </author>
    <author>
      <name>Marcus Kaiser</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1098/rsif.2013.0016</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1098/rsif.2013.0016" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of the Royal Society Interface 2013, 10(81), 20130016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1302.5331v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.5331v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92B05, 92B20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.5280v1</id>
    <updated>2014-09-18T12:17:16Z</updated>
    <published>2014-09-18T12:17:16Z</published>
    <title>Perspective: network-guided pattern formation of neural dynamics</title>
    <summary>  The understanding of neural activity patterns is fundamentally linked to an
understanding of how the brain's network architecture shapes dynamical
processes. Established approaches rely mostly on deviations of a given network
from certain classes of random graphs. Hypotheses about the supposed role of
prominent topological features (for instance, the roles of modularity, network
motifs, or hierarchical network organization) are derived from these
deviations. An alternative strategy could be to study deviations of network
architectures from regular graphs (rings, lattices) and consider the
implications of such deviations for self-organized dynamic patterns on the
network. Following this strategy, we draw on the theory of spatiotemporal
pattern formation and propose a novel perspective for analyzing dynamics on
networks, by evaluating how the self-organized dynamics are confined by network
architecture to a small set of permissible collective states. In particular, we
discuss the role of prominent topological features of brain connectivity, such
as hubs, modules and hierarchy, in shaping activity patterns. We illustrate the
notion of network-guided pattern formation with numerical simulations and
outline how it can facilitate the understanding of neural dynamics.
</summary>
    <author>
      <name>Marc-Thorsten Huett</name>
    </author>
    <author>
      <name>Marcus Kaiser</name>
    </author>
    <author>
      <name>Claus C. Hilgetag</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1098/rstb.2013.0522</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1098/rstb.2013.0522" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phil. Trans. R. Soc. B 20130522, 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1409.5280v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.5280v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.03527v1</id>
    <updated>2015-07-28T19:03:29Z</updated>
    <published>2015-07-28T19:03:29Z</published>
    <title>Do Brain Networks Evolve by Maximizing their Information Flow Capacity?</title>
    <summary>  We propose a working hypothesis supported by numerical simulations that brain
networks evolve based on the principle of the maximization of their internal
information flow capacity. We find that synchronous behavior and capacity of
information flow of the evolved networks reproduce well the same behaviors
observed in the brain dynamical networks of Caenorhabditis elegans and humans,
networks of Hindmarsh-Rose neurons with graphs given by these brain networks.
We make a strong case to verify our hypothesis by showing that the neural
networks with the closest graph distance to the brain networks of
Caenorhabditis elegans and humans are the Hindmarsh-Rose neural networks
evolved with coupling strengths that maximize information flow capacity.
Surprisingly, we find that global neural synchronization levels decrease during
brain evolution, reflecting on an underlying global no Hebbian-like evolution
process, which is driven by no Hebbian-like learning behaviors for some of the
clusters during evolution, and Hebbian-like learning rules for clusters where
neurons increase their synchronization.
</summary>
    <author>
      <name>Chris G. Antonopoulos</name>
    </author>
    <author>
      <name>Shambhavi Srivastava</name>
    </author>
    <author>
      <name>Sandro E. de S. Pinto</name>
    </author>
    <author>
      <name>Murilo S. Baptista</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pcbi.1004372</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pcbi.1004372" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 8 figures, 2 tables, supporting_information included,
  published in PLOS Computational Biology</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.03527v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.03527v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08165v2</id>
    <updated>2017-08-16T16:15:20Z</updated>
    <published>2016-06-27T08:58:29Z</published>
    <title>Supervised learning based on temporal coding in spiking neural networks</title>
    <summary>  Gradient descent training techniques are remarkably successful in training
analog-valued artificial neural networks (ANNs). Such training techniques,
however, do not transfer easily to spiking networks due to the spike generation
hard non-linearity and the discrete nature of spike communication. We show that
in a feedforward spiking network that uses a temporal coding scheme where
information is encoded in spike times instead of spike rates, the network
input-output relation is differentiable almost everywhere. Moreover, this
relation is piece-wise linear after a transformation of variables. Methods for
training ANNs thus carry directly to the training of such spiking networks as
we show when training on the permutation invariant MNIST task. In contrast to
rate-based spiking networks that are often used to approximate the behavior of
ANNs, the networks we present spike much more sparsely and their behavior can
not be directly approximated by conventional ANNs. Our results highlight a new
approach for controlling the behavior of spiking networks with realistic
temporal dynamics, opening up the potential for using these networks to process
spike patterns with complex temporal information.
</summary>
    <author>
      <name>Hesham Mostafa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended the discussion and introduction. Clarified the training
  parameters</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.08165v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08165v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.00811v2</id>
    <updated>2018-02-07T21:43:25Z</updated>
    <published>2017-11-02T16:49:19Z</published>
    <title>Expressive power of recurrent neural networks</title>
    <summary>  Deep neural networks are surprisingly efficient at solving practical tasks,
but the theory behind this phenomenon is only starting to catch up with the
practice. Numerous works show that depth is the key to this efficiency. A
certain class of deep convolutional networks -- namely those that correspond to
the Hierarchical Tucker (HT) tensor decomposition -- has been proven to have
exponentially higher expressive power than shallow networks. I.e. a shallow
network of exponential width is required to realize the same score function as
computed by the deep architecture. In this paper, we prove the expressive power
theorem (an exponential lower bound on the width of the equivalent shallow
network) for a class of recurrent neural networks -- ones that correspond to
the Tensor Train (TT) decomposition. This means that even processing an image
patch by patch with an RNN can be exponentially more efficient than a (shallow)
convolutional network with one hidden layer. Using theoretical results on the
relation between the tensor decompositions we compare expressive powers of the
HT- and TT-Networks. We also implement the recurrent TT-Networks and provide
numerical evidence of their expressivity.
</summary>
    <author>
      <name>Valentin Khrulkov</name>
    </author>
    <author>
      <name>Alexander Novikov</name>
    </author>
    <author>
      <name>Ivan Oseledets</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as a conference paper at ICLR 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.00811v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.00811v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0612537v2</id>
    <updated>2007-11-01T11:31:44Z</updated>
    <published>2006-12-21T15:51:50Z</published>
    <title>Dynamics of neural cryptography</title>
    <summary>  Synchronization of neural networks has been used for novel public channel
protocols in cryptography. In the case of tree parity machines the dynamics of
both bidirectional synchronization and unidirectional learning is driven by
attractive and repulsive stochastic forces. Thus it can be described well by a
random walk model for the overlap between participating neural networks. For
that purpose transition probabilities and scaling laws for the step sizes are
derived analytically. Both these calculations as well as numerical simulations
show that bidirectional interaction leads to full synchronization on average.
In contrast, successful learning is only possible by means of fluctuations.
Consequently, synchronization is much faster than learning, which is essential
for the security of the neural key-exchange protocol. However, this qualitative
difference between bidirectional and unidirectional interaction vanishes if
tree parity machines with more than three hidden units are used, so that those
neural networks are not suitable for neural cryptography. In addition, the
effective number of keys which can be generated by the neural key-exchange
protocol is calculated using the entropy of the weight distribution. As this
quantity increases exponentially with the system size, brute-force attacks on
neural cryptography can easily be made unfeasible.
</summary>
    <author>
      <name>Andreas Ruttor</name>
    </author>
    <author>
      <name>Wolfgang Kinzel</name>
    </author>
    <author>
      <name>Ido Kanter</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.75.056104</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.75.056104" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 15 figures; typos corrected</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 75, 056104 (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0612537v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0612537v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.1777v2</id>
    <updated>2014-07-07T07:51:04Z</updated>
    <published>2014-04-07T13:08:08Z</published>
    <title>Neural Codes for Image Retrieval</title>
    <summary>  It has been shown that the activations invoked by an image within the top
layers of a large convolutional neural network provide a high-level descriptor
of the visual content of the image. In this paper, we investigate the use of
such descriptors (neural codes) within the image retrieval application. In the
experiments with several standard retrieval benchmarks, we establish that
neural codes perform competitively even when the convolutional neural network
has been trained for an unrelated classification task (e.g.\ Image-Net). We
also evaluate the improvement in the retrieval performance of neural codes,
when the network is retrained on a dataset of images that are similar to images
encountered at test time.
  We further evaluate the performance of the compressed neural codes and show
that a simple PCA compression provides very good short codes that give
state-of-the-art accuracy on a number of datasets. In general, neural codes
turn out to be much more resilient to such compression in comparison other
state-of-the-art descriptors. Finally, we show that discriminative
dimensionality reduction trained on a dataset of pairs of matched photographs
improves the performance of PCA-compressed neural codes even further. Overall,
our quantitative experiments demonstrate the promise of neural codes as visual
descriptors for image retrieval.
</summary>
    <author>
      <name>Artem Babenko</name>
    </author>
    <author>
      <name>Anton Slesarev</name>
    </author>
    <author>
      <name>Alexandr Chigorin</name>
    </author>
    <author>
      <name>Victor Lempitsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear at ECCV 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.1777v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.1777v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.06158v1</id>
    <updated>2017-08-21T11:36:54Z</updated>
    <published>2017-08-21T11:36:54Z</published>
    <title>Opto-magnetic imaging of neural network activity in brain slices at high
  resolution using color centers in diamond</title>
    <summary>  We suggest a novel approach for wide-field imaging of the neural network
dynamics of brain slices that uses highly sensitivity magnetometry based on
nitrogen-vacancy (NV) centers in diamond. In-vitro recordings in brain slices
is a proven method for the characterization of electrical neural activity and
has strongly contributed to our understanding of the mechanisms that govern
neural information processing. However, traditional recordings can only acquire
signals from a few positions simultaneously, which severely limits their
ability to characterize the dynamics of the underlying neural networks. We
suggest to radically extend the scope of this method using the wide-field
imaging of the neural magnetic fields across the slice by means of NV
magnetometry. Employing comprehensive computational simulations and theoretical
analyses, we characterize the spatiotemporal characteristics of the neural
magnetic fields and derive the required key performance parameters of an
imaging setup based on NV magnetometry. In particular, we determine how the
technical parameters determine the achievable spatial resolution for an optimal
reconstruction of the neural currents from the measured field distributions.
Finally, we compare the imaging of neural slice activity with that of a single
planar pyramidal cell. Our results suggest that imaging of neural slice
activity will be possible with the upcoming generation of NV magnetic field
sensors, while imaging of the activity of a single planar cell remains more
challenging.
</summary>
    <author>
      <name>M√ºrsel Karadas</name>
    </author>
    <author>
      <name>Adam M. Wojciechowski</name>
    </author>
    <author>
      <name>Alexander Huck</name>
    </author>
    <author>
      <name>Nils Ole Dalby</name>
    </author>
    <author>
      <name>Ulrik Lund Andersen</name>
    </author>
    <author>
      <name>Axel Thielscher</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 8 figures; SI 11 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.06158v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.06158v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.3369v3</id>
    <updated>2017-04-22T08:29:56Z</updated>
    <published>2014-03-13T18:58:37Z</published>
    <title>Controlling Recurrent Neural Networks by Conceptors</title>
    <summary>  The human brain is a dynamical system whose extremely complex sensor-driven
neural processes give rise to conceptual, logical cognition. Understanding the
interplay between nonlinear neural dynamics and concept-level cognition remains
a major scientific challenge. Here I propose a mechanism of neurodynamical
organization, called conceptors, which unites nonlinear dynamics with basic
principles of conceptual abstraction and logic. It becomes possible to learn,
store, abstract, focus, morph, generalize, de-noise and recognize a large
number of dynamical patterns within a single neural system; novel patterns can
be added without interfering with previously acquired ones; neural noise is
automatically filtered. Conceptors help explaining how conceptual-level
information processing emerges naturally and robustly in neural systems, and
remove a number of roadblocks in the theory and applications of recurrent
neural networks.
</summary>
    <author>
      <name>Herbert Jaeger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">200 pages, 50 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.3369v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.3369v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07149v1</id>
    <updated>2016-06-23T01:07:27Z</updated>
    <published>2016-06-23T01:07:27Z</published>
    <title>An Approach to Stable Gradient Descent Adaptation of Higher-Order Neural
  Units</title>
    <summary>  Stability evaluation of a weight-update system of higher-order neural units
(HONUs) with polynomial aggregation of neural inputs (also known as classes of
polynomial neural networks) for adaptation of both feedforward and recurrent
HONUs by a gradient descent method is introduced. An essential core of the
approach is based on spectral radius of a weight-update system, and it allows
stability monitoring and its maintenance at every adaptation step individually.
Assuring stability of the weight-update system (at every single adaptation
step) naturally results in adaptation stability of the whole neural
architecture that adapts to target data. As an aside, the used approach
highlights the fact that the weight optimization of HONU is a linear problem,
so the proposed approach can be generally extended to any neural architecture
that is linear in its adaptable parameters.
</summary>
    <author>
      <name>Ivo Bukovsky</name>
    </author>
    <author>
      <name>Noriyasu Homma</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TNNLS.2016.2572310</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TNNLS.2016.2572310" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2016, 13 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Neural Networks and Learning Systems,ISSN:
  2162-237X,2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.07149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.02759v1</id>
    <updated>2017-10-07T23:33:31Z</updated>
    <published>2017-10-07T23:33:31Z</published>
    <title>Keynote: Small Neural Nets Are Beautiful: Enabling Embedded Systems with
  Small Deep-Neural-Network Architectures</title>
    <summary>  Over the last five years Deep Neural Nets have offered more accurate
solutions to many problems in speech recognition, and computer vision, and
these solutions have surpassed a threshold of acceptability for many
applications. As a result, Deep Neural Networks have supplanted other
approaches to solving problems in these areas, and enabled many new
applications. While the design of Deep Neural Nets is still something of an art
form, in our work we have found basic principles of design space exploration
used to develop embedded microprocessor architectures to be highly applicable
to the design of Deep Neural Net architectures. In particular, we have used
these design principles to create a novel Deep Neural Net called SqueezeNet
that requires as little as 480KB of storage for its model parameters. We have
further integrated all these experiences to develop something of a playbook for
creating small Deep Neural Nets for embedded systems.
</summary>
    <author>
      <name>Forrest Iandola</name>
    </author>
    <author>
      <name>Kurt Keutzer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keynote at Embedded Systems Week (ESWEEK) 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.02759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.02759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0904.2189v3</id>
    <updated>2010-05-28T22:43:09Z</updated>
    <published>2009-04-15T14:44:58Z</published>
    <title>Stochastic cellular automata model of neural networks</title>
    <summary>  We propose a stochastic dynamical model of noisy neural networks with complex
architectures and discuss activation of neural networks by a stimulus,
pacemakers and spontaneous activity. This model has a complex phase diagram
with self-organized active neural states, hybrid phase transitions, and a rich
array of behavior. We show that if spontaneous activity (noise) reaches a
threshold level then global neural oscillations emerge. Stochastic resonance is
a precursor of this dynamical phase transition. These oscillations are an
intrinsic property of even small groups of 50 neurons.
</summary>
    <author>
      <name>A. V. Goltsev</name>
    </author>
    <author>
      <name>F. V. de Abreu</name>
    </author>
    <author>
      <name>S. N. Dorogovtsev</name>
    </author>
    <author>
      <name>J. F. F. Mendes</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.81.061921</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.81.061921" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 81, 061921 (2010)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0904.2189v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0904.2189v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.06094v2</id>
    <updated>2015-12-01T08:35:42Z</updated>
    <published>2015-02-21T11:17:08Z</published>
    <title>Positive Neural Networks in Discrete Time Implement Monotone-Regular
  Behaviors</title>
    <summary>  We study the expressive power of positive neural networks. The model uses
positive connection weights and multiple input neurons. Different behaviors can
be expressed by varying the connection weights. We show that in discrete time,
and in absence of noise, the class of positive neural networks captures the
so-called monotone-regular behaviors, that are based on regular languages. A
finer picture emerges if one takes into account the delay by which a
monotone-regular behavior is implemented. Each monotone-regular behavior can be
implemented by a positive neural network with a delay of one time unit. Some
monotone-regular behaviors can be implemented with zero delay. And,
interestingly, some simple monotone-regular behaviors can not be implemented
with zero delay.
</summary>
    <author>
      <name>Tom J. Ameloot</name>
    </author>
    <author>
      <name>Jan Van den Bussche</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/NECO_a_00789</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/NECO_a_00789" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Computation, December 2015, Vol. 27, No. 12 , Pages
  2623-2660</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1502.06094v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.06094v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.03132v1</id>
    <updated>2015-04-13T11:17:37Z</updated>
    <published>2015-04-13T11:17:37Z</published>
    <title>Learning in Neural Networks Based on a Generalized Fluctuation Theorem</title>
    <summary>  Information maximization has been investigated as a possible mechanism of
learning governing the self-organization that occurs within the neural systems
of animals. Within the general context of models of neural systems
bidirectionally interacting with environments, however, the role of information
maximization remains to be elucidated. For bidirectionally interacting physical
systems, universal laws describing the fluctuation they exhibit and the
information they possess have recently been discovered. These laws are termed
fluctuation theorems. In the present study, we formulate a theory of learning
in neural networks bidirectionally interacting with environments based on the
principle of information maximization. Our formulation begins with the
introduction of a generalized fluctuation theorem, employing an interpretation
appropriate for the present application, which differs from the original
thermodynamic interpretation. We analytically and numerically demonstrate that
the learning mechanism presented in our theory allows neural networks to
efficiently explore their environments and optimally encode information about
them.
</summary>
    <author>
      <name>Takashi Hayakawa</name>
    </author>
    <author>
      <name>Toshio Aoyagi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.92.052710</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.92.052710" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 92, 052710 (2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1504.03132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.03132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.05641v4</id>
    <updated>2016-04-23T23:14:39Z</updated>
    <published>2015-11-18T02:09:20Z</published>
    <title>Net2Net: Accelerating Learning via Knowledge Transfer</title>
    <summary>  We introduce techniques for rapidly transferring the information stored in
one neural net into another neural net. The main purpose is to accelerate the
training of a significantly larger neural net. During real-world workflows, one
often trains very many different neural networks during the experimentation and
design process. This is a wasteful process in which each new model is trained
from scratch. Our Net2Net technique accelerates the experimentation process by
instantaneously transferring the knowledge from a previous network to each new
deeper or wider network. Our techniques are based on the concept of
function-preserving transformations between neural network specifications. This
differs from previous approaches to pre-training that altered the function
represented by a neural net when adding layers to it. Using our knowledge
transfer mechanism to add depth to Inception modules, we demonstrate a new
state of the art accuracy rating on the ImageNet dataset.
</summary>
    <author>
      <name>Tianqi Chen</name>
    </author>
    <author>
      <name>Ian Goodfellow</name>
    </author>
    <author>
      <name>Jonathon Shlens</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2016 submission</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.05641v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.05641v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06111v2</id>
    <updated>2016-10-13T07:45:31Z</updated>
    <published>2016-03-19T16:38:31Z</published>
    <title>How Transferable are Neural Networks in NLP Applications?</title>
    <summary>  Transfer learning is aimed to make use of valuable knowledge in a source
domain to help model performance in a target domain. It is particularly
important to neural networks, which are very likely to be overfitting. In some
fields like image processing, many studies have shown the effectiveness of
neural network-based transfer learning. For neural NLP, however, existing
studies have only casually applied transfer learning, and conclusions are
inconsistent. In this paper, we conduct systematic case studies and provide an
illuminating picture on the transferability of neural networks in NLP.
</summary>
    <author>
      <name>Lili Mou</name>
    </author>
    <author>
      <name>Zhao Meng</name>
    </author>
    <author>
      <name>Rui Yan</name>
    </author>
    <author>
      <name>Ge Li</name>
    </author>
    <author>
      <name>Yan Xu</name>
    </author>
    <author>
      <name>Lu Zhang</name>
    </author>
    <author>
      <name>Zhi Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by EMNLP-16</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.06111v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06111v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.00838v2</id>
    <updated>2017-08-13T19:35:03Z</updated>
    <published>2017-01-03T21:22:42Z</published>
    <title>Encoding Sensory and Motor Patterns as Time-Invariant Trajectories in
  Recurrent Neural Networks</title>
    <summary>  Much of the information the brain processes and stores is temporal in nature
- a spoken word or a handwritten signature, for example, is defined by how it
unfolds in time. However, it remains unclear how neural circuits encode complex
time-varying patterns. We show that by tuning the weights of a recurrent neural
network (RNN), it can recognize and then transcribe spoken digits. The model
elucidates how neural dynamics in cortical networks may resolve three
fundamental challenges: first, encode multiple time-varying sensory and motor
patterns as stable neural trajectories; second, generalize across relevant
spatial features; third, identify the same stimuli played at different speeds -
we show that this temporal invariance emerges because the recurrent dynamics
generate neural trajectories with appropriately modulated angular velocities.
Together our results generate testable predictions as to how recurrent networks
may use different mechanisms to generalize across the relevant spatial and
temporal features of complex time-varying stimuli.
</summary>
    <author>
      <name>Vishwa Goudar</name>
    </author>
    <author>
      <name>Dean Buonomano</name>
    </author>
    <link href="http://arxiv.org/abs/1701.00838v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.00838v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.09137v1</id>
    <updated>2017-03-27T15:13:49Z</updated>
    <published>2017-03-27T15:13:49Z</published>
    <title>Where to put the Image in an Image Caption Generator</title>
    <summary>  When a neural language model is used for caption generation, the image
information can be fed to the neural network either by directly incorporating
it in a recurrent neural network -- conditioning the language model by
injecting image features -- or in a layer following the recurrent neural
network -- conditioning the language model by merging the image features. While
merging implies that visual features are bound at the end of the caption
generation process, injecting can bind the visual features at a variety stages.
In this paper we empirically show that late binding is superior to early
binding in terms of different evaluation metrics. This suggests that the
different modalities (visual and linguistic) for caption generation should not
be jointly encoded by the RNN; rather, the multimodal integration should be
delayed to a subsequent stage. Furthermore, this suggests that recurrent neural
networks should not be viewed as actually generating text, but only as encoding
it for prediction in a subsequent layer.
</summary>
    <author>
      <name>Marc Tanti</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Malta</arxiv:affiliation>
    </author>
    <author>
      <name>Albert Gatt</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Malta</arxiv:affiliation>
    </author>
    <author>
      <name>Kenneth P. Camilleri</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Malta</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">under review, 29 pages, 5 figures, 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.09137v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.09137v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.04834v3</id>
    <updated>2016-08-04T18:23:03Z</updated>
    <published>2015-11-16T06:03:58Z</published>
    <title>Neural Programmer: Inducing Latent Programs with Gradient Descent</title>
    <summary>  Deep neural networks have achieved impressive supervised classification
performance in many tasks including image recognition, speech recognition, and
sequence to sequence learning. However, this success has not been translated to
applications like question answering that may involve complex arithmetic and
logic reasoning. A major limitation of these models is in their inability to
learn even simple arithmetic and logic operations. For example, it has been
shown that neural networks fail to learn to add two binary numbers reliably. In
this work, we propose Neural Programmer, an end-to-end differentiable neural
network augmented with a small set of basic arithmetic and logic operations.
Neural Programmer can call these augmented operations over several steps,
thereby inducing compositional programs that are more complex than the built-in
operations. The model learns from a weak supervision signal which is the result
of execution of the correct program, hence it does not require expensive
annotation of the correct program itself. The decisions of what operations to
call, and what data segments to apply to are inferred by Neural Programmer.
Such decisions, during training, are done in a differentiable fashion so that
the entire network can be trained jointly by gradient descent. We find that
training the model is difficult, but it can be greatly improved by adding
random noise to the gradient. On a fairly complex synthetic table-comprehension
dataset, traditional recurrent networks and attentional models perform poorly
while Neural Programmer typically obtains nearly perfect accuracy.
</summary>
    <author>
      <name>Arvind Neelakantan</name>
    </author>
    <author>
      <name>Quoc V. Le</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as a conference paper at ICLR 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.04834v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.04834v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08718v1</id>
    <updated>2017-01-30T17:34:51Z</updated>
    <published>2017-01-30T17:34:51Z</published>
    <title>Memory Augmented Neural Networks with Wormhole Connections</title>
    <summary>  Recent empirical results on long-term dependency tasks have shown that neural
networks augmented with an external memory can learn the long-term dependency
tasks more easily and achieve better generalization than vanilla recurrent
neural networks (RNN). We suggest that memory augmented neural networks can
reduce the effects of vanishing gradients by creating shortcut (or wormhole)
connections. Based on this observation, we propose a novel memory augmented
neural network model called TARDIS (Temporal Automatic Relation Discovery in
Sequences). The controller of TARDIS can store a selective set of embeddings of
its own previous hidden states into an external memory and revisit them as and
when needed. For TARDIS, memory acts as a storage for wormhole connections to
the past to propagate the gradients more effectively and it helps to learn the
temporal dependencies. The memory structure of TARDIS has similarities to both
Neural Turing Machines (NTM) and Dynamic Neural Turing Machines (D-NTM), but
both read and write operations of TARDIS are simpler and more efficient. We use
discrete addressing for read/write operations which helps to substantially to
reduce the vanishing gradient problem with very long sequences. Read and write
operations in TARDIS are tied with a heuristic once the memory becomes full,
and this makes the learning problem simpler when compared to NTM or D-NTM type
of architectures. We provide a detailed analysis on the gradient propagation in
general for MANNs. We evaluate our models on different long-term dependency
tasks and report competitive results in all of them.
</summary>
    <author>
      <name>Caglar Gulcehre</name>
    </author>
    <author>
      <name>Sarath Chandar</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <link href="http://arxiv.org/abs/1701.08718v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08718v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.5688v1</id>
    <updated>2012-11-24T17:52:22Z</updated>
    <published>2012-11-24T17:52:22Z</published>
    <title>Neural networks with dynamical synapses: from mixed-mode oscillations
  and spindles to chaos</title>
    <summary>  Understanding of short-term synaptic depression (STSD) and other forms of
synaptic plasticity is a topical problem in neuroscience. Here we study the
role of STSD in the formation of complex patterns of brain rhythms. We use a
cortical circuit model of neural networks composed of irregular spiking
excitatory and inhibitory neurons having type 1 and 2 excitability and
stochastic dynamics. In the model, neurons form a sparsely connected network
and their spontaneous activity is driven by random spikes representing synaptic
noise. Using simulations and analytical calculations, we found that if the STSD
is absent, the neural network shows either asynchronous behavior or regular
network oscillations depending on the noise level. In networks with STSD,
changing parameters of synaptic plasticity and the noise level, we observed
transitions to complex patters of collective activity: mixed-mode and spindle
oscillations, bursts of collective activity, and chaotic behaviour.
Interestingly, these patterns are stable in a certain range of the parameters
and separated by critical boundaries. Thus, the parameters of synaptic
plasticity can play a role of control parameters or switchers between different
network states. However, changes of the parameters caused by a disease may lead
to dramatic impairment of ongoing neural activity. We analyze the chaotic
neural activity by use of the 0-1 test for chaos (Gottwald, G. &amp; Melbourne, I.,
2004) and show that it has a collective nature.
</summary>
    <author>
      <name>K. -E. Lee</name>
    </author>
    <author>
      <name>A. V. Goltsev</name>
    </author>
    <author>
      <name>M. A. Lopes</name>
    </author>
    <author>
      <name>J. F. F. Mendes</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/1.4776517</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/1.4776517" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, Proceedings of 12th Granada Seminar, September 17-21, 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.5688v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.5688v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.5300v1</id>
    <updated>2014-07-20T14:42:36Z</updated>
    <published>2014-07-20T14:42:36Z</published>
    <title>A walk in the statistical mechanical formulation of neural networks</title>
    <summary>  Neural networks are nowadays both powerful operational tools (e.g., for
pattern recognition, data mining, error correction codes) and complex
theoretical models on the focus of scientific investigation. As for the
research branch, neural networks are handled and studied by psychologists,
neurobiologists, engineers, mathematicians and theoretical physicists. In
particular, in theoretical physics, the key instrument for the quantitative
analysis of neural networks is statistical mechanics. From this perspective,
here, we first review attractor networks: starting from ferromagnets and
spin-glass models, we discuss the underlying philosophy and we recover the
strand paved by Hopfield, Amit-Gutfreund-Sompolinky. One step forward, we
highlight the structural equivalence between Hopfield networks (modeling
retrieval) and Boltzmann machines (modeling learning), hence realizing a deep
bridge linking two inseparable aspects of biological and robotic spontaneous
cognition. As a sideline, in this walk we derive two alternative (with respect
to the original Hebb proposal) ways to recover the Hebbian paradigm, stemming
from ferromagnets and from spin-glasses, respectively. Further, as these notes
are thought of for an Engineering audience, we highlight also the mappings
between ferromagnets and operational amplifiers and between antiferromagnets
and flip-flops (as neural networks -built by op-amp and flip-flops- are
particular spin-glasses and the latter are indeed combinations of ferromagnets
and antiferromagnets), hoping that such a bridge plays as a concrete
prescription to capture the beauty of robotics from the statistical mechanical
perspective.
</summary>
    <author>
      <name>Elena Agliari</name>
    </author>
    <author>
      <name>Adriano Barra</name>
    </author>
    <author>
      <name>Andrea Galluzzi</name>
    </author>
    <author>
      <name>Daniele Tantari</name>
    </author>
    <author>
      <name>Flavia Tavani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Contribute to the proceeding of the conference: NCTA 2014. Contains
  12 pages,7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.5300v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.5300v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="82" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.1792v1</id>
    <updated>2014-11-06T23:09:37Z</updated>
    <published>2014-11-06T23:09:37Z</published>
    <title>How transferable are features in deep neural networks?</title>
    <summary>  Many deep neural networks trained on natural images exhibit a curious
phenomenon in common: on the first layer they learn features similar to Gabor
filters and color blobs. Such first-layer features appear not to be specific to
a particular dataset or task, but general in that they are applicable to many
datasets and tasks. Features must eventually transition from general to
specific by the last layer of the network, but this transition has not been
studied extensively. In this paper we experimentally quantify the generality
versus specificity of neurons in each layer of a deep convolutional neural
network and report a few surprising results. Transferability is negatively
affected by two distinct issues: (1) the specialization of higher layer neurons
to their original task at the expense of performance on the target task, which
was expected, and (2) optimization difficulties related to splitting networks
between co-adapted neurons, which was not expected. In an example network
trained on ImageNet, we demonstrate that either of these two issues may
dominate, depending on whether features are transferred from the bottom,
middle, or top of the network. We also document that the transferability of
features decreases as the distance between the base task and target task
increases, but that transferring features even from distant tasks can be better
than using random features. A final surprising result is that initializing a
network with transferred features from almost any number of layers can produce
a boost to generalization that lingers even after fine-tuning to the target
dataset.
</summary>
    <author>
      <name>Jason Yosinski</name>
    </author>
    <author>
      <name>Jeff Clune</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <author>
      <name>Hod Lipson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Advances in Neural Information Processing Systems 27
  (NIPS 2014)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in Neural Information Processing Systems 27, pages
  3320-3328. Dec. 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1411.1792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.1792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.02276v2</id>
    <updated>2017-10-01T18:14:19Z</updated>
    <published>2017-08-07T19:42:24Z</published>
    <title>Parallelizing Over Artificial Neural Network Training Runs with
  Multigrid</title>
    <summary>  Artificial neural networks are a popular and effective machine learning
technique. Great progress has been made parallelizing the expensive training
phase of an individual network, leading to highly specialized pieces of
hardware, many based on GPU-type architectures, and more concurrent algorithms
such as synthetic gradients. However, the training phase continues to be a
bottleneck, where the training data must be processed serially over thousands
of individual training runs. This work considers a multigrid reduction in time
(MGRIT) algorithm that is able to parallelize over the thousands of training
runs and converge to the exact same solution as traditional training would
provide. MGRIT was originally developed to provide parallelism for time
evolution problems that serially step through a finite number of time-steps.
This work recasts the training of a neural network similarly, treating neural
network training as an evolution equation that evolves the network weights from
one step to the next. Thus, this work concerns distributed computing approaches
for neural networks, but is distinct from other approaches which seek to
parallelize only over individual training runs. The work concludes with
supporting numerical results for two model problems.
</summary>
    <author>
      <name>Jacob B. Schroder</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Version 2: - Added more complete references to basic neural network
  literature - Corrected typos - Condensed results in Section 3 to be more
  concise - 22 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.02276v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.02276v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.03712v1</id>
    <updated>2017-11-10T06:54:45Z</updated>
    <published>2017-11-10T06:54:45Z</published>
    <title>Quantized Memory-Augmented Neural Networks</title>
    <summary>  Memory-augmented neural networks (MANNs) refer to a class of neural network
models equipped with external memory (such as neural Turing machines and memory
networks). These neural networks outperform conventional recurrent neural
networks (RNNs) in terms of learning long-term dependency, allowing them to
solve intriguing AI tasks that would otherwise be hard to address. This paper
concerns the problem of quantizing MANNs. Quantization is known to be effective
when we deploy deep models on embedded systems with limited resources.
Furthermore, quantization can substantially reduce the energy consumption of
the inference procedure. These benefits justify recent developments of
quantized multi layer perceptrons, convolutional networks, and RNNs. However,
no prior work has reported the successful quantization of MANNs. The in-depth
analysis presented here reveals various challenges that do not appear in the
quantization of the other networks. Without addressing them properly, quantized
MANNs would normally suffer from excessive quantization error which leads to
degraded performance. In this paper, we identify memory addressing
(specifically, content-based addressing) as the main reason for the performance
degradation and propose a robust quantization method for MANNs to address the
challenge. In our experiments, we achieved a computation-energy gain of 22x
with 8-bit fixed-point and binary quantization compared to the floating-point
implementation. Measured on the bAbI dataset, the resulting model, named the
quantized MANN (Q-MANN), improved the error rate by 46% and 30% with 8-bit
fixed-point and binary quantization, respectively, compared to the MANN
quantized using conventional techniques.
</summary>
    <author>
      <name>Seongsik Park</name>
    </author>
    <author>
      <name>Seijoon Kim</name>
    </author>
    <author>
      <name>Seil Lee</name>
    </author>
    <author>
      <name>Ho Bae</name>
    </author>
    <author>
      <name>Sungroh Yoon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.03712v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.03712v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.11240v1</id>
    <updated>2017-11-30T05:38:52Z</updated>
    <published>2017-11-30T05:38:52Z</published>
    <title>Quantum Neuron: an elementary building block for machine learning on
  quantum computers</title>
    <summary>  Even the most sophisticated artificial neural networks are built by
aggregating substantially identical units called neurons. A neuron receives
multiple signals, internally combines them, and applies a non-linear function
to the resulting weighted sum. Several attempts to generalize neurons to the
quantum regime have been proposed, but all proposals collided with the
difficulty of implementing non-linear activation functions, which is essential
for classical neurons, due to the linear nature of quantum mechanics. Here we
propose a solution to this roadblock in the form of a small quantum circuit
that naturally simulates neurons with threshold activation. Our quantum circuit
defines a building block, the "quantum neuron", that can reproduce a variety of
classical neural network constructions while maintaining the ability to process
superpositions of inputs and preserve quantum coherence and entanglement. In
the construction of feedforward networks of quantum neurons, we provide
numerical evidence that the network not only can learn a function when trained
with superposition of inputs and the corresponding output, but that this
training suffices to learn the function on all individual inputs separately.
When arranged to mimic Hopfield networks, quantum neural networks exhibit
properties of associative memory. Patterns are encoded using the simple Hebbian
rule for the weights and we demonstrate attractor dynamics from corrupted
inputs. Finally, the fact that our quantum model closely captures (traditional)
neural network dynamics implies that the vast body of literature and results on
neural networks becomes directly relevant in the context of quantum machine
learning.
</summary>
    <author>
      <name>Yudong Cao</name>
    </author>
    <author>
      <name>Gian Giacomo Guerreschi</name>
    </author>
    <author>
      <name>Al√°n Aspuru-Guzik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.11240v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.11240v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.00746v3</id>
    <updated>2018-01-18T18:05:39Z</updated>
    <published>2017-11-15T22:52:34Z</published>
    <title>Bridging the Gap Between Neural Networks and Neuromorphic Hardware with
  A Neural Network Compiler</title>
    <summary>  Different from developing neural networks (NNs) for general-purpose
processors, the development for NN chips usually faces with some
hardware-specific restrictions, such as limited precision of network signals
and parameters, constrained computation scale, and limited types of non-linear
functions.
  This paper proposes a general methodology to address the challenges. We
decouple the NN applications from the target hardware by introducing a compiler
that can transform an existing trained, unrestricted NN into an equivalent
network that meets the given hardware's constraints. We propose multiple
techniques to make the transformation adaptable to different kinds of NN chips,
and reliable for restrict hardware constraints.
  We have built such a software tool that supports both spiking neural networks
(SNNs) and traditional artificial neural networks (ANNs). We have demonstrated
its effectiveness with a fabricated neuromorphic chip and a
processing-in-memory (PIM) design. Tests show that the inference error caused
by this solution is insignificant and the transformation time is much shorter
than the retraining time. Also, we have studied the parameter-sensitivity
evaluations to explore the tradeoffs between network error and resource
utilization for different transformation strategies, which could provide
insights for co-design optimization of neuromorphic hardware and software.
</summary>
    <author>
      <name>Yu Ji</name>
    </author>
    <author>
      <name>YouHui Zhang</name>
    </author>
    <author>
      <name>WenGuang Chen</name>
    </author>
    <author>
      <name>Yuan Xie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ASPLOS 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.00746v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.00746v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.06488v1</id>
    <updated>2018-02-19T01:57:46Z</updated>
    <published>2018-02-19T01:57:46Z</published>
    <title>Tiny SSD: A Tiny Single-shot Detection Deep Convolutional Neural Network
  for Real-time Embedded Object Detection</title>
    <summary>  Object detection is a major challenge in computer vision, involving both
object classification and object localization within a scene. While deep neural
networks have been shown in recent years to yield very powerful techniques for
tackling the challenge of object detection, one of the biggest challenges with
enabling such object detection networks for widespread deployment on embedded
devices is high computational and memory requirements. Recently, there has been
an increasing focus in exploring small deep neural network architectures for
object detection that are more suitable for embedded devices, such as Tiny YOLO
and SqueezeDet. Inspired by the efficiency of the Fire microarchitecture
introduced in SqueezeNet and the object detection performance of the
single-shot detection macroarchitecture introduced in SSD, this paper
introduces Tiny SSD, a single-shot detection deep convolutional neural network
for real-time embedded object detection that is composed of a highly optimized,
non-uniform Fire sub-network stack and a non-uniform sub-network stack of
highly optimized SSD-based auxiliary convolutional feature layers designed
specifically to minimize model size while maintaining object detection
performance. The resulting Tiny SSD possess a model size of 2.3MB (~26X smaller
than Tiny YOLO) while still achieving an mAP of 61.3% on VOC 2007 (~4.2% higher
than Tiny YOLO). These experimental results show that very small deep neural
network architectures can be designed for real-time object detection that are
well-suited for embedded scenarios.
</summary>
    <author>
      <name>Alexander Wong</name>
    </author>
    <author>
      <name>Mohammad Javad Shafiee</name>
    </author>
    <author>
      <name>Francis Li</name>
    </author>
    <author>
      <name>Brendan Chwyl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.06488v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06488v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/astro-ph/0211121v2</id>
    <updated>2003-02-06T23:05:38Z</updated>
    <published>2002-11-06T19:52:20Z</published>
    <title>Lightcurve Classification in Massive Variability Surveys</title>
    <summary>  This paper pioneers the use of neural networks to provide a fast and
automatic way to classify lightcurves in massive photometric datasets. As an
example, we provide a working neural network that can distinguish microlensing
lightcurves from other forms of variability, such as eruptive, pulsating,
cataclysmic and eclipsing variable stars. The network has five input neurons, a
hidden layer of five neurons and one output neuron. The five input variables
for the network are extracted by spectral analysis from the lightcurve
datapoints and are optimised for the identification of a single, symmetric,
microlensing bump. The output of the network is the posterior probability of
microlensing. The committee of neural networks successfully passes tests on
noisy data taken by the MACHO collaboration. When used to process 5000
lightcurves on a typical tile towards the bulge, the network cleanly identifies
the single microlensing event. When fed with a sub-sample of 36 lightcurves
identified by the MACHO collaboration as microlensing, the network corroborates
this verdict in the case of 27 events, but classifies the remaining 9 events as
other forms of variability. For some of these discrepant events, it looks as
though there are secondary bumps or the bump is noisy or not properly
contained. Neural networks naturally allow for the possibility of novelty
detection -- that is, new or unexpected phenomena which we may want to follow
up. The advantages of neural networks for microlensing rate calculations, as
well as the future developments of massive variability surveys, are both
briefly discussed.
</summary>
    <author>
      <name>Vasily Belokurov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Oxford</arxiv:affiliation>
    </author>
    <author>
      <name>N. Wyn Evans</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Cambridge</arxiv:affiliation>
    </author>
    <author>
      <name>Yann Le Du</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Oxford</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1046/j.1365-8711.2003.06512.x</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1046/j.1365-8711.2003.06512.x" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 8 figures, version in press at MNRAS</arxiv:comment>
    <link href="http://arxiv.org/abs/astro-ph/0211121v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/astro-ph/0211121v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/q-bio/0510036v1</id>
    <updated>2005-10-18T20:38:33Z</updated>
    <published>2005-10-18T20:38:33Z</published>
    <title>When Response Variability Increases Neural Network Robustness to
  Synaptic Noise</title>
    <summary>  Cortical sensory neurons are known to be highly variable, in the sense that
responses evoked by identical stimuli often change dramatically from trial to
trial. The origin of this variability is uncertain, but it is usually
interpreted as detrimental noise that reduces the computational accuracy of
neural circuits. Here we investigate the possibility that such response
variability might, in fact, be beneficial, because it may partially compensate
for a decrease in accuracy due to stochastic changes in the synaptic strengths
of a network. We study the interplay between two kinds of noise, response (or
neuronal) noise and synaptic noise, by analyzing their joint influence on the
accuracy of neural networks trained to perform various tasks. We find an
interesting, generic interaction: when fluctuations in the synaptic connections
are proportional to their strengths (multiplicative noise), a certain amount of
response noise in the input neurons can significantly improve network
performance, compared to the same network without response noise. Performance
is enhanced because response noise and multiplicative synaptic noise are in
some ways equivalent. These results are demonstrated analytically for the most
basic network consisting of two input neurons and one output neuron performing
a simple classification task, but computer simulations show that the phenomenon
persists in a wide range of architectures, including recurrent (attractor)
networks and sensory-motor networks that perform coordinate transformations.
The results suggest that response variability could play an important dynamic
role in networks that continuously learn.
</summary>
    <author>
      <name>Gleb Basalyga</name>
    </author>
    <author>
      <name>Emilio Salinas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 7 figures, to appear in Neural Computation</arxiv:comment>
    <link href="http://arxiv.org/abs/q-bio/0510036v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/q-bio/0510036v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.00313v2</id>
    <updated>2017-12-07T14:30:18Z</updated>
    <published>2017-11-01T12:38:59Z</published>
    <title>Avoiding Your Teacher's Mistakes: Training Neural Networks with
  Controlled Weak Supervision</title>
    <summary>  Training deep neural networks requires massive amounts of training data, but
for many tasks only limited labeled data is available. This makes weak
supervision attractive, using weak or noisy signals like the output of
heuristic methods or user click-through data for training. In a semi-supervised
setting, we can use a large set of data with weak labels to pretrain a neural
network and then fine-tune the parameters with a small amount of data with true
labels. This feels intuitively sub-optimal as these two independent stages
leave the model unaware about the varying label quality. What if we could
somehow inform the model about the label quality? In this paper, we propose a
semi-supervised learning method where we train two neural networks in a
multi-task fashion: a "target network" and a "confidence network". The target
network is optimized to perform a given task and is trained using a large set
of unlabeled data that are weakly annotated. We propose to weight the gradient
updates to the target network using the scores provided by the second
confidence network, which is trained on a small amount of supervised data. Thus
we avoid that the weight updates computed from noisy labels harm the quality of
the target network model. We evaluate our learning strategy on two different
tasks: document ranking and sentiment classification. The results demonstrate
that our approach not only enhances the performance compared to the baselines
but also speeds up the learning process from weak labels.
</summary>
    <author>
      <name>Mostafa Dehghani</name>
    </author>
    <author>
      <name>Aliaksei Severyn</name>
    </author>
    <author>
      <name>Sascha Rothe</name>
    </author>
    <author>
      <name>Jaap Kamps</name>
    </author>
    <link href="http://arxiv.org/abs/1711.00313v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.00313v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.08831v2</id>
    <updated>2018-03-04T03:58:18Z</updated>
    <published>2018-02-24T10:31:24Z</published>
    <title>Convolutional Neural Networks combined with Runge-Kutta Methods</title>
    <summary>  A convolutional neural network for image classification can be constructed
following some mathematical ways since it models the ventral stream in visual
cortex which is regarded as a multi-period dynamical system. In this paper, a
new point of view is proposed for constructing network models as well as
providing a direction to get inspiration or explanation for neural network. If
each period in ventral stream was deemed to be a dynamical system with time as
the independent variable, there should be a set of ordinary differential
equations (ODEs) for this system. Runge-Kutta methods are common means to solve
ODE. Thus, network model ought to be built using these methods. Moreover,
convolutional networks could be employed to emulate the increments within every
time-step. The model constructed in the above way is named Runge-Kutta
Convolutional Neural Network (RKNet). According to this idea, Dense
Convolutional Networks (DenseNets) were varied to RKNets. To prove the
feasibility of RKNets, these variants were verified on benchmark datasets,
CIFAR and ImageNet. The experimental results show that the RKNets transformed
from DenseNets gained similar or even higher parameter efficiency. The success
of the experiments denotes that Runge-Kutta methods can be utilized to
construct convolutional neural networks for image classification efficiently.
Furthermore, the network models might be structured more rationally in the
future basing on RKNet and priori knowledge.
</summary>
    <author>
      <name>Mai Zhu</name>
    </author>
    <author>
      <name>Chong Fu</name>
    </author>
    <link href="http://arxiv.org/abs/1802.08831v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.08831v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0101382v1</id>
    <updated>2001-01-24T22:02:20Z</updated>
    <published>2001-01-24T22:02:20Z</published>
    <title>Nonlinear Network Dynamics on Earthquake Fault Systems</title>
    <summary>  Earthquake faults occur in networks that have dynamical modes not displayed
by single isolated faults. Using simulations of the network of strike-slip
faults in southern California, we find that the physics depends critically on
both the interactions among the faults, which are determined by the geometry of
the fault network, as well as on the stress dissipation properties of the
nonlinear frictional physics, similar to the dynamics of integrate-and-fire
neural networks.
</summary>
    <author>
      <name>Paul B. Rundle</name>
    </author>
    <author>
      <name>John B. Rundle</name>
    </author>
    <author>
      <name>Kristy F. Tiampo</name>
    </author>
    <author>
      <name>Jorge S. Sa Martins</name>
    </author>
    <author>
      <name>Seth McGinnis</name>
    </author>
    <author>
      <name>W. Klein</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.87.148501</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.87.148501" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0101382v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0101382v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/nlin/0405011v1</id>
    <updated>2004-05-06T23:13:14Z</updated>
    <published>2004-05-06T23:13:14Z</published>
    <title>Evolution of Robust Developmental Neural Networks</title>
    <summary>  We present the first evolved solutions to a computational task within the
Neuronal Organism Evolution model (Norgev) of artificial neural network
development. These networks display a remarkable robustness to external noise
sources, and can regrow to functionality when severely damaged. In this
framework, we evolved a doubling of network functionality (double-NAND
circuit). The network structure of these evolved solutions does not follow the
logic of human coding, and instead more resembles the decentralized dendritic
connection pattern of more biological networks such as the 'C. elegans' brain.
</summary>
    <author>
      <name>Alan N. Hampton</name>
    </author>
    <author>
      <name>Chris Adami</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 10 figures, to be published in Artificial Life IX</arxiv:comment>
    <link href="http://arxiv.org/abs/nlin/0405011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/nlin/0405011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.3926v1</id>
    <updated>2009-12-19T18:54:56Z</updated>
    <published>2009-12-19T18:54:56Z</published>
    <title>Application of Radial Basis Network Model for HIV/AIDs Regimen
  Specifications</title>
    <summary>  HIV/AIDs Regimen specification one of many problems for which
bioinformaticians have implemented and trained machine learning methods such as
neural networks. Predicting HIV resistance would be much easier, but
unfortunately we rarely have enough structural information available to train a
neural network. To network model designed to predict how long the HIV patient
can prolong his/her life time with certain regimen specification. To learn this
model 300 patient's details have taken as a training set to train the network
and 100 patients medical history has taken to test this model. This network
model is trained using MAT lab implementation.
</summary>
    <author>
      <name>P. Balasubramanie</name>
    </author>
    <author>
      <name>M. Lilly Florence</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computing, Volume 1, Issue 1, pp 136-140, December 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0912.3926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.3926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.2351v1</id>
    <updated>2013-01-10T22:57:01Z</updated>
    <published>2013-01-10T22:57:01Z</published>
    <title>Application of Hopfield Network to Saccades</title>
    <summary>  Human eye movement mechanisms (saccades) are very useful for scene analysis,
including object representation and pattern recognition. In this letter, a
Hopfield neural network to emulate saccades is proposed. The network uses an
energy function that includes location and identification tasks. Computer
simulation shows that the network performs those tasks cooperatively. The
result suggests that the network is applicable to shift-invariant pattern
recognition.
</summary>
    <author>
      <name>Teruyoshi Washizawa</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/72.286896</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/72.286896" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on NEURAL NETWORKS, vol.4, no.6, pp-995-997,
  NOVEMBER 1993</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1301.2351v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.2351v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.04723v1</id>
    <updated>2016-02-15T16:16:56Z</updated>
    <published>2016-02-15T16:16:56Z</published>
    <title>Efficient Representation of Low-Dimensional Manifolds using Deep
  Networks</title>
    <summary>  We consider the ability of deep neural networks to represent data that lies
near a low-dimensional manifold in a high-dimensional space. We show that deep
networks can efficiently extract the intrinsic, low-dimensional coordinates of
such data. We first show that the first two layers of a deep network can
exactly embed points lying on a monotonic chain, a special type of piecewise
linear manifold, mapping them to a low-dimensional Euclidean space. Remarkably,
the network can do this using an almost optimal number of parameters. We also
show that this network projects nearby points onto the manifold and then embeds
them with little error. We then extend these results to more general manifolds.
</summary>
    <author>
      <name>Ronen Basri</name>
    </author>
    <author>
      <name>David Jacobs</name>
    </author>
    <link href="http://arxiv.org/abs/1602.04723v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.04723v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.00366v1</id>
    <updated>2016-05-02T06:40:08Z</updated>
    <published>2016-05-02T06:40:08Z</published>
    <title>Compression Artifacts Removal Using Convolutional Neural Networks</title>
    <summary>  This paper shows that it is possible to train large and deep convolutional
neural networks (CNN) for JPEG compression artifacts reduction, and that such
networks can provide significantly better reconstruction quality compared to
previously used smaller networks as well as to any other state-of-the-art
methods. We were able to train networks with 8 layers in a single step and in
relatively short time by combining residual learning, skip architecture, and
symmetric weight initialization. We provide further insights into convolution
networks for JPEG artifact reduction by evaluating three different objectives,
generalization with respect to training dataset size, and generalization with
respect to JPEG quality level.
</summary>
    <author>
      <name>Pavel Svoboda</name>
    </author>
    <author>
      <name>Michal Hradis</name>
    </author>
    <author>
      <name>David Barina</name>
    </author>
    <author>
      <name>Pavel Zemcik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in WSCG 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.00366v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.00366v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00691v2</id>
    <updated>2017-02-16T17:30:03Z</updated>
    <published>2016-07-03T22:00:57Z</published>
    <title>Mapping the Structure of Directed Networks: Beyond the "Bow-tie" Diagram</title>
    <summary>  We reveal a hierarchical, multilayer organization of finite components --
i.e., tendrils and tubes -- around the giant connected components in directed
networks and propose efficient algorithms allowing one to uncover the entire
organization of key real-world directed networks, such as the World Wide Web,
the neural network of \emph{Caenorhabditis elegans}, and others. With
increasing damage, the giant components decrease in size while the number and
size of tendril layers increase, enhancing the susceptibility of the networks
to damage.
</summary>
    <author>
      <name>G. Tim√°r</name>
    </author>
    <author>
      <name>A. V. Goltsev</name>
    </author>
    <author>
      <name>S. N. Dorogovtsev</name>
    </author>
    <author>
      <name>J. F. F. Mendes</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.118.078301</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.118.078301" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. Lett. 118, 078301 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1607.00691v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00691v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07132v1</id>
    <updated>2016-09-22T19:57:08Z</updated>
    <published>2016-09-22T19:57:08Z</published>
    <title>A Fully Convolutional Neural Network for Speech Enhancement</title>
    <summary>  In hearing aids, the presence of babble noise degrades hearing
intelligibility of human speech greatly. However, removing the babble without
creating artifacts in human speech is a challenging task in a low SNR
environment. Here, we sought to solve the problem by finding a `mapping'
between noisy speech spectra and clean speech spectra via supervised learning.
Specifically, we propose using fully Convolutional Neural Networks, which
consist of lesser number of parameters than fully connected networks. The
proposed network, Redundant Convolutional Encoder Decoder (R-CED), demonstrates
that a convolutional network can be 12 times smaller than a recurrent network
and yet achieves better performance, which shows its applicability for an
embedded system: the hearing aids.
</summary>
    <author>
      <name>Se Rim Park</name>
    </author>
    <author>
      <name>Jinwon Lee</name>
    </author>
    <link href="http://arxiv.org/abs/1609.07132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.01143v1</id>
    <updated>2017-05-02T19:12:23Z</updated>
    <published>2017-05-02T19:12:23Z</published>
    <title>Summarized Network Behavior Prediction</title>
    <summary>  This work studies the entity-wise topical behavior from massive network logs.
Both the temporal and the spatial relationships of the behavior are explored
with the learning architectures combing the recurrent neural network (RNN) and
the convolutional neural network (CNN). To make the behavioral data appropriate
for the spatial learning in CNN, several reduction steps are taken to form the
topical metrics and place them homogeneously like pixels in the images. The
experimental result shows both the temporal- and the spatial- gains when
compared to a multilayer perceptron (MLP) network. A new learning framework
called spatially connected convolutional networks (SCCN) is introduced to more
efficiently predict the behavior.
</summary>
    <author>
      <name>Shih-Chieh Su</name>
    </author>
    <link href="http://arxiv.org/abs/1705.01143v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01143v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.06002v1</id>
    <updated>2017-09-18T15:17:50Z</updated>
    <published>2017-09-18T15:17:50Z</published>
    <title>NeuRoute: Predictive Dynamic Routing for Software-Defined Networks</title>
    <summary>  This paper introduces NeuRoute, a dynamic routing framework for Software
Defined Networks (SDN) entirely based on machine learning, specifically, Neural
Networks. Current SDN/OpenFlow controllers use a default routing based on
Dijkstra algorithm for shortest paths, and provide APIs to develop custom
routing applications. NeuRoute is a controller-agnostic dynamic routing
framework that (i) predicts traffic matrix in real time, (ii) uses a neural
network to learn traffic characteristics and (iii) generates forwarding rules
accordingly to optimize the network throughput. NeuRoute achieves the same
results as the most efficient dynamic routing heuristic but in much less
execution time.
</summary>
    <author>
      <name>Abdelhadi Azzouni</name>
    </author>
    <author>
      <name>Raouf Boutaba</name>
    </author>
    <author>
      <name>Guy Pujolle</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for CNSM 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.06002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.06002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.06937v1</id>
    <updated>2017-10-17T17:10:11Z</updated>
    <published>2017-10-17T17:10:11Z</published>
    <title>Embedding-Based Speaker Adaptive Training of Deep Neural Networks</title>
    <summary>  An embedding-based speaker adaptive training (SAT) approach is proposed and
investigated in this paper for deep neural network acoustic modeling. In this
approach, speaker embedding vectors, which are a constant given a particular
speaker, are mapped through a control network to layer-dependent element-wise
affine transformations to canonicalize the internal feature representations at
the output of hidden layers of a main network. The control network for
generating the speaker-dependent mappings is jointly estimated with the main
network for the overall speaker adaptive acoustic modeling. Experiments on
large vocabulary continuous speech recognition (LVCSR) tasks show that the
proposed SAT scheme can yield superior performance over the widely-used
speaker-aware training using i-vectors with speaker-adapted input features.
</summary>
    <author>
      <name>Xiaodong Cui</name>
    </author>
    <author>
      <name>Vaibhava Goel</name>
    </author>
    <author>
      <name>George Saon</name>
    </author>
    <link href="http://arxiv.org/abs/1710.06937v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.06937v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.00965v2</id>
    <updated>2016-01-21T02:46:25Z</updated>
    <published>2015-12-03T06:46:27Z</published>
    <title>Neural Enquirer: Learning to Query Tables with Natural Language</title>
    <summary>  We proposed Neural Enquirer as a neural network architecture to execute a
natural language (NL) query on a knowledge-base (KB) for answers. Basically,
Neural Enquirer finds the distributed representation of a query and then
executes it on knowledge-base tables to obtain the answer as one of the values
in the tables. Unlike similar efforts in end-to-end training of semantic
parsers, Neural Enquirer is fully "neuralized": it not only gives
distributional representation of the query and the knowledge-base, but also
realizes the execution of compositional queries as a series of differentiable
operations, with intermediate results (consisting of annotations of the tables
at different levels) saved on multiple layers of memory. Neural Enquirer can be
trained with gradient descent, with which not only the parameters of the
controlling components and semantic parsing component, but also the embeddings
of the tables and query words can be learned from scratch. The training can be
done in an end-to-end fashion, but it can take stronger guidance, e.g., the
step-by-step supervision for complicated queries, and benefit from it. Neural
Enquirer is one step towards building neural network systems which seek to
understand language by executing it on real-world. Our experiments show that
Neural Enquirer can learn to execute fairly complicated NL queries on tables
with rich structures.
</summary>
    <author>
      <name>Pengcheng Yin</name>
    </author>
    <author>
      <name>Zhengdong Lu</name>
    </author>
    <author>
      <name>Hang Li</name>
    </author>
    <author>
      <name>Ben Kao</name>
    </author>
    <link href="http://arxiv.org/abs/1512.00965v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.00965v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/astro-ph/0505322v1</id>
    <updated>2005-05-16T15:59:14Z</updated>
    <published>2005-05-16T15:59:14Z</published>
    <title>An Artificial Neural Network Approach to the Solution of Molecular
  Chemical Equilibrium</title>
    <summary>  A novel approach is presented for the solution of instantaneous chemical
equilibrium problems. The chemical equilibrium can be considered, due to its
intrinsically local character, as a mapping of the three-dimensional parameter
space spanned by the temperature, hydrogen density and electron density into
many one-dimensional spaces representing the number density of each species. We
take advantage of the ability of artificial neural networks to approximate
non-linear functions and construct neural networks for the fast and efficient
solution of the chemical equilibrium problem in typical stellar atmosphere
physical conditions. The neural network approach has the advantage of providing
an analytic function, which can be rapidly evaluated. The networks are trained
with a learning set (that covers the entire parameter space) until a relative
error below 1% is reached. It has been verified that the networks are not
overtrained by using an additional verification set. The networks are then
applied to a snapshot of realistic three-dimensional convection simulations of
the solar atmosphere showing good generalization properties.
</summary>
    <author>
      <name>A. Asensio Ramos</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INAF-Osservatorio Astrofisico di Arcetri</arxiv:affiliation>
    </author>
    <author>
      <name>H. Socas-Navarro</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">High Altitude Observatory, NCAR</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1051/0004-6361:20052865</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1051/0004-6361:20052865" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures, accepted for publication in A&amp;A</arxiv:comment>
    <link href="http://arxiv.org/abs/astro-ph/0505322v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/astro-ph/0505322v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0310205v1</id>
    <updated>2003-10-09T10:31:45Z</updated>
    <published>2003-10-09T10:31:45Z</published>
    <title>Influence of topology on the performance of a neural network</title>
    <summary>  We studied the computational properties of an attractor neural network (ANN)
with different network topologies. Though fully connected neural networks
exhibit, in general, a good performance, they are biologically unrealistic, as
it is unlikely that natural evolution leads to such a large connectivity. We
demonstrate that, at finite temperature, the capacity to store and retrieve
binary patterns is higher for ANN with scale--free (SF) topology than for
highly random--diluted Hopfield networks with the same number of synapses. We
also show that, at zero temperature, the relative performance of the SF network
increases with increasing values of the distribution power-law exponent. Some
consequences and possible applications of our findings are discussed.
</summary>
    <author>
      <name>Joaquin J. Torres</name>
    </author>
    <author>
      <name>Miguel A. Munoz</name>
    </author>
    <author>
      <name>J. Marro</name>
    </author>
    <author>
      <name>P. L. Garrido</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 eps Figures. 6 pages. To appear in Neurocomputing</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neurocomputing, vol. 58-60, pag. 229-234 (2004)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0310205v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0310205v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.5465v1</id>
    <updated>2010-07-30T14:48:05Z</updated>
    <published>2010-07-30T14:48:05Z</published>
    <title>The Physics of Living Neural Networks</title>
    <summary>  Improvements in technique in conjunction with an evolution of the theoretical
and conceptual approach to neuronal networks provide a new perspective on
living neurons in culture. Organization and connectivity are being measured
quantitatively along with other physical quantities such as information, and
are being related to function. In this review we first discuss some of these
advances, which enable elucidation of structural aspects. We then discuss two
recent experimental models that yield some conceptual simplicity. A
one-dimensional network enables precise quantitative comparison to analytic
models, for example of propagation and information transport. A two-dimensional
percolating network gives quantitative information on connectivity of cultured
neurons. The physical quantities that emerge as essential characteristics of
the network in vitro are propagation speeds, synaptic transmission, information
creation and capacity. Potential application to neuronal devices is discussed.
</summary>
    <author>
      <name>Jean-Pierre Eckmann</name>
    </author>
    <author>
      <name>Ofer Feinerman</name>
    </author>
    <author>
      <name>Leor Gruendlinger</name>
    </author>
    <author>
      <name>Elisha Moses</name>
    </author>
    <author>
      <name>Jordi Soriano</name>
    </author>
    <author>
      <name>Tsvi Tlusty</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physrep.2007.02.014</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physrep.2007.02.014" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PACS: 87.18.Sn, 87.19.La, 87.80.-y, 87.80.Xa, 64.60.Ak Keywords:
  complex systems, neuroscience, neural networks, transport of information,
  neural connectivity, percolation
  http://www.weizmann.ac.il/complex/tlusty/papers/PhysRep2007.pdf
  http://www.weizmann.ac.il/complex/EMoses/pdf/PhysRep-448-56.pdf</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physics Reports Volume 449, Issues 1-3, September 2007, Pages
  54-76</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1007.5465v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.5465v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.0152v1</id>
    <updated>2013-06-01T21:37:25Z</updated>
    <published>2013-06-01T21:37:25Z</published>
    <title>An Analysis of the Connections Between Layers of Deep Neural Networks</title>
    <summary>  We present an analysis of different techniques for selecting the connection
be- tween layers of deep neural networks. Traditional deep neural networks use
ran- dom connection tables between layers to keep the number of connections
small and tune to different image features. This kind of connection performs
adequately in supervised deep networks because their values are refined during
the training. On the other hand, in unsupervised learning, one cannot rely on
back-propagation techniques to learn the connections between layers. In this
work, we tested four different techniques for connecting the first layer of the
network to the second layer on the CIFAR and SVHN datasets and showed that the
accuracy can be im- proved up to 3% depending on the technique used. We also
showed that learning the connections based on the co-occurrences of the
features does not confer an advantage over a random connection table in small
networks. This work is helpful to improve the efficiency of connections between
the layers of unsupervised deep neural networks.
</summary>
    <author>
      <name>Eugenio Culurciello</name>
    </author>
    <author>
      <name>Jonghoon Jin</name>
    </author>
    <author>
      <name>Aysegul Dundar</name>
    </author>
    <author>
      <name>Jordan Bates</name>
    </author>
    <link href="http://arxiv.org/abs/1306.0152v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.0152v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.7149v4</id>
    <updated>2015-07-17T20:17:26Z</updated>
    <published>2014-12-22T20:53:30Z</published>
    <title>Deep Fried Convnets</title>
    <summary>  The fully connected layers of a deep convolutional neural network typically
contain over 90% of the network parameters, and consume the majority of the
memory required to store the network parameters. Reducing the number of
parameters while preserving essentially the same predictive performance is
critically important for operating deep neural networks in memory constrained
environments such as GPUs or embedded devices.
  In this paper we show how kernel methods, in particular a single Fastfood
layer, can be used to replace all fully connected layers in a deep
convolutional neural network. This novel Fastfood layer is also end-to-end
trainable in conjunction with convolutional layers, allowing us to combine them
into a new architecture, named deep fried convolutional networks, which
substantially reduces the memory footprint of convolutional networks trained on
MNIST and ImageNet with no drop in predictive performance.
</summary>
    <author>
      <name>Zichao Yang</name>
    </author>
    <author>
      <name>Marcin Moczulski</name>
    </author>
    <author>
      <name>Misha Denil</name>
    </author>
    <author>
      <name>Nando de Freitas</name>
    </author>
    <author>
      <name>Alex Smola</name>
    </author>
    <author>
      <name>Le Song</name>
    </author>
    <author>
      <name>Ziyu Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">svd experiments included</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.7149v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.7149v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.03805v2</id>
    <updated>2016-12-09T01:47:07Z</updated>
    <published>2016-01-15T03:33:35Z</published>
    <title>Matrix Neural Networks</title>
    <summary>  Traditional neural networks assume vectorial inputs as the network is
arranged as layers of single line of computing units called neurons. This
special structure requires the non-vectorial inputs such as matrices to be
converted into vectors. This process can be problematic. Firstly, the spatial
information among elements of the data may be lost during vectorisation.
Secondly, the solution space becomes very large which demands very special
treatments to the network parameters and high computational cost. To address
these issues, we propose matrix neural networks (MatNet), which takes matrices
directly as inputs. Each neuron senses summarised information through bilinear
mapping from lower layer units in exactly the same way as the classic feed
forward neural networks. Under this structure, back prorogation and gradient
descent combination can be utilised to obtain network parameters efficiently.
Furthermore, it can be conveniently extended for multimodal inputs. We apply
MatNet to MNIST handwritten digits classification and image super resolution
tasks to show its effectiveness. Without too much tweaking MatNet achieves
comparable performance as the state-of-the-art methods in both tasks with
considerably reduced complexity.
</summary>
    <author>
      <name>Junbin Gao</name>
    </author>
    <author>
      <name>Yi Guo</name>
    </author>
    <author>
      <name>Zhiyong Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.03805v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.03805v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.02320v1</id>
    <updated>2016-11-07T22:03:43Z</updated>
    <published>2016-11-07T22:03:43Z</published>
    <title>Adversarial Ladder Networks</title>
    <summary>  The use of unsupervised data in addition to supervised data in training
discriminative neural networks has improved the performance of this clas-
sification scheme. However, the best results were achieved with a training
process that is divided in two parts: first an unsupervised pre-training step
is done for initializing the weights of the network and after these weights are
refined with the use of supervised data. On the other hand adversarial noise
has improved the results of clas- sical supervised learning. Recently, a new
neural network topology called Ladder Network, where the key idea is based in
some properties of hierar- chichal latent variable models, has been proposed as
a technique to train a neural network using supervised and unsupervised data at
the same time with what is called semi-supervised learning. This technique has
reached state of the art classification. In this work we add adversarial noise
to the ladder network and get state of the art classification, with several
important conclusions on how adversarial noise can help in addition with new
possible lines of investi- gation. We also propose an alternative to add
adversarial noise to unsu- pervised data.
</summary>
    <author>
      <name>Juan Maro√±as Molano</name>
    </author>
    <author>
      <name>Alberto Albiol Colomer</name>
    </author>
    <author>
      <name>Roberto Paredes Palacios</name>
    </author>
    <link href="http://arxiv.org/abs/1611.02320v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.02320v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.03281v1</id>
    <updated>2017-01-12T09:48:53Z</updated>
    <published>2017-01-12T09:48:53Z</published>
    <title>Modularized Morphing of Neural Networks</title>
    <summary>  In this work we study the problem of network morphism, an effective learning
scheme to morph a well-trained neural network to a new one with the network
function completely preserved. Different from existing work where basic
morphing types on the layer level were addressed, we target at the central
problem of network morphism at a higher level, i.e., how a convolutional layer
can be morphed into an arbitrary module of a neural network. To simplify the
representation of a network, we abstract a module as a graph with blobs as
vertices and convolutional layers as edges, based on which the morphing process
is able to be formulated as a graph transformation problem. Two atomic morphing
operations are introduced to compose the graphs, based on which modules are
classified into two families, i.e., simple morphable modules and complex
modules. We present practical morphing solutions for both of these two
families, and prove that any reasonable module can be morphed from a single
convolutional layer. Extensive experiments have been conducted based on the
state-of-the-art ResNet on benchmark datasets, and the effectiveness of the
proposed solution has been verified.
</summary>
    <author>
      <name>Tao Wei</name>
    </author>
    <author>
      <name>Changhu Wang</name>
    </author>
    <author>
      <name>Chang Wen Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures, Under review as a conference paper at ICLR 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.03281v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.03281v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.08068v1</id>
    <updated>2017-03-23T13:48:45Z</updated>
    <published>2017-03-23T13:48:45Z</published>
    <title>Sequential Recurrent Neural Networks for Language Modeling</title>
    <summary>  Feedforward Neural Network (FNN)-based language models estimate the
probability of the next word based on the history of the last N words, whereas
Recurrent Neural Networks (RNN) perform the same task based only on the last
word and some context information that cycles in the network. This paper
presents a novel approach, which bridges the gap between these two categories
of networks. In particular, we propose an architecture which takes advantage of
the explicit, sequential enumeration of the word history in FNN structure while
enhancing each word representation at the projection layer through recurrent
context information that evolves in the network. The context integration is
performed using an additional word-dependent weight matrix that is also learned
during the training. Extensive experiments conducted on the Penn Treebank (PTB)
and the Large Text Compression Benchmark (LTCB) corpus showed a significant
reduction of the perplexity when compared to state-of-the-art feedforward as
well as recurrent neural network architectures.
</summary>
    <author>
      <name>Youssef Oualil</name>
    </author>
    <author>
      <name>Clayton Greenberg</name>
    </author>
    <author>
      <name>Mittul Singh</name>
    </author>
    <author>
      <name>Dietrich Klakow</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">published (INTERSPEECH 2016), 5 pages, 3 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.08068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.08068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.02019v2</id>
    <updated>2017-11-02T18:30:38Z</updated>
    <published>2017-04-06T20:46:16Z</published>
    <title>Associative content-addressable networks with exponentially many robust
  stable states</title>
    <summary>  The brain must robustly store a large number of memories, corresponding to
the many events encountered over a lifetime. However, the number of memory
states in existing neural network models either grows weakly with network size
or recall fails catastrophically with vanishingly little noise. We construct an
associative content-addressable memory with exponentially many stable states
and robust error-correction. The network possesses expander graph connectivity
on a restricted Boltzmann machine architecture. The expansion property allows
simple neural network dynamics to perform at par with modern error-correcting
codes. Appropriate networks can be constructed with sparse random connections,
glomerular nodes, and associative learning using low dynamic-range weights.
Thus, sparse quasi-random structures---characteristic of important
error-correcting codes---may provide for high-performance computation in
artificial neural networks and the brain.
</summary>
    <author>
      <name>Rishidev Chaudhuri</name>
    </author>
    <author>
      <name>Ila Fiete</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.02019v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.02019v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.02361v3</id>
    <updated>2017-11-14T15:54:38Z</updated>
    <published>2017-06-07T19:54:39Z</published>
    <title>The Effects of Noisy Labels on Deep Convolutional Neural Networks for
  Music Tagging</title>
    <summary>  Deep neural networks (DNN) have been successfully applied to music
classification including music tagging. However, there are several open
questions regarding the training, evaluation, and analysis of DNNs. In this
article, we investigate specific aspects of neural networks, the effects of
noisy labels, to deepen our understanding of their properties. We analyse and
(re-)validate a large music tagging dataset to investigate the reliability of
training and evaluation. Using a trained network, we compute label vector
similarities which is compared to groundtruth similarity.
  The results highlight several important aspects of music tagging and neural
networks. We show that networks can be effective despite relatively large error
rates in groundtruth datasets, while conjecturing that label noise can be the
cause of varying tag-wise performance differences. Lastly, the analysis of our
trained network provides valuable insight into the relationships between music
tags. These results highlight the benefit of using data-driven methods to
address automatic music tagging.
</summary>
    <author>
      <name>Keunwoo Choi</name>
    </author>
    <author>
      <name>George Fazekas</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <author>
      <name>Mark Sandler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The section that overlapped with arXiv:1709.01922 is completely
  removed since the earlier version. This is the camera-ready version</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.02361v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.02361v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.01461v3</id>
    <updated>2017-12-02T07:31:20Z</updated>
    <published>2017-07-05T16:49:28Z</published>
    <title>Labeled Memory Networks for Online Model Adaptation</title>
    <summary>  Augmenting a neural network with memory that can grow without growing the
number of trained parameters is a recent powerful concept with many exciting
applications. We propose a design of memory augmented neural networks (MANNs)
called Labeled Memory Networks (LMNs) suited for tasks requiring online
adaptation in classification models. LMNs organize the memory with classes as
the primary key.The memory acts as a second boosted stage following a regular
neural network thereby allowing the memory and the primary network to play
complementary roles. Unlike existing MANNs that write to memory for every
instance and use LRU based memory replacement, LMNs write only for instances
with non-zero loss and use label-based memory replacement. We demonstrate
significant accuracy gains on various tasks including word-modelling and
few-shot learning. In this paper, we establish their potential in online
adapting a batch trained neural network to domain-relevant labeled data at
deployment time. We show that LMNs are better than other MANNs designed for
meta-learning. We also found them to be more accurate and faster than
state-of-the-art methods of retuning model parameters for adapting to
domain-specific labeled data.
</summary>
    <author>
      <name>Shiv Shankar</name>
    </author>
    <author>
      <name>Sunita Sarawagi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at AAAI 2018, 8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.01461v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.01461v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.06262v2</id>
    <updated>2017-10-10T03:53:33Z</updated>
    <published>2017-09-19T05:50:19Z</published>
    <title>Compressing Low Precision Deep Neural Networks Using Sparsity-Induced
  Regularization in Ternary Networks</title>
    <summary>  A low precision deep neural network training technique for producing sparse,
ternary neural networks is presented. The technique incorporates hard- ware
implementation costs during training to achieve significant model compression
for inference. Training involves three stages: network training using L2
regularization and a quantization threshold regularizer, quantization pruning,
and finally retraining. Resulting networks achieve improved accuracy, reduced
memory footprint and reduced computational complexity compared with
conventional methods, on MNIST and CIFAR10 datasets. Our networks are up to 98%
sparse and 5 &amp; 11 times smaller than equivalent binary and ternary models,
translating to significant resource and speed benefits for hardware
implementations.
</summary>
    <author>
      <name>Julian Faraone</name>
    </author>
    <author>
      <name>Nicholas Fraser</name>
    </author>
    <author>
      <name>Giulio Gambardella</name>
    </author>
    <author>
      <name>Michaela Blott</name>
    </author>
    <author>
      <name>Philip H. W. Leong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear as a conference paper at the 24th International Conference
  On Neural Information Processing (ICONIP 2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.06262v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.06262v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.02286v1</id>
    <updated>2017-10-06T06:42:11Z</updated>
    <published>2017-10-06T06:42:11Z</published>
    <title>Deep Convolutional Neural Networks as Generic Feature Extractors</title>
    <summary>  Recognizing objects in natural images is an intricate problem involving
multiple conflicting objectives. Deep convolutional neural networks, trained on
large datasets, achieve convincing results and are currently the
state-of-the-art approach for this task. However, the long time needed to train
such deep networks is a major drawback. We tackled this problem by reusing a
previously trained network. For this purpose, we first trained a deep
convolutional network on the ILSVRC2012 dataset. We then maintained the learned
convolution kernels and only retrained the classification part on different
datasets. Using this approach, we achieved an accuracy of 67.68 % on CIFAR-100,
compared to the previous state-of-the-art result of 65.43 %. Furthermore, our
findings indicate that convolutional networks are able to learn generic feature
extractors that can be used for different tasks.
</summary>
    <author>
      <name>Lars Hertel</name>
    </author>
    <author>
      <name>Erhardt Barth</name>
    </author>
    <author>
      <name>Thomas K√§ster</name>
    </author>
    <author>
      <name>Thomas Martinetz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, accepted version for publication in Proceedings of the IEEE
  International Joint Conference on Neural Networks (IJCNN), July 2015,
  Killarney, Ireland</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.02286v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.02286v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.01952v2</id>
    <updated>2018-02-17T21:58:44Z</updated>
    <published>2018-01-06T01:27:16Z</published>
    <title>Generating Neural Networks with Neural Networks</title>
    <summary>  Hypernetworks are neural networks that transform a random input vector into
weights for a specified target neural network. We formulate the hypernetwork
training objective as a compromise between accuracy and diversity, where the
diversity takes into account trivial symmetry transformations of the target
network. We show that this formulation naturally arises as a relaxation of an
optimistic probability distribution objective for the generated networks, and
we explain how it is related to variational inference. We use multi-layered
perceptrons to form the mapping from the low dimensional input random vector to
the high dimensional weight space, and demonstrate how to reduce the number of
parameters in this mapping by weight sharing. We perform experiments on a four
layer convolutional target network which classifies MNIST images, and show that
the generated weights are diverse and have interesting distributions.
</summary>
    <author>
      <name>Lior Deutsch</name>
    </author>
    <link href="http://arxiv.org/abs/1801.01952v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.01952v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00212v1</id>
    <updated>2018-02-01T09:38:06Z</updated>
    <published>2018-02-01T09:38:06Z</published>
    <title>Training Neural Networks by Using Power Linear Units (PoLUs)</title>
    <summary>  In this paper, we introduce "Power Linear Unit" (PoLU) which increases the
nonlinearity capacity of a neural network and thus helps improving its
performance. PoLU adopts several advantages of previously proposed activation
functions. First, the output of PoLU for positive inputs is designed to be
identity to avoid the gradient vanishing problem. Second, PoLU has a non-zero
output for negative inputs such that the output mean of the units is close to
zero, hence reducing the bias shift effect. Thirdly, there is a saturation on
the negative part of PoLU, which makes it more noise-robust for negative
inputs. Furthermore, we prove that PoLU is able to map more portions of every
layer's input to the same space by using the power function and thus increases
the number of response regions of the neural network. We use image
classification for comparing our proposed activation function with others. In
the experiments, MNIST, CIFAR-10, CIFAR-100, Street View House Numbers (SVHN)
and ImageNet are used as benchmark datasets. The neural networks we implemented
include widely-used ELU-Network, ResNet-50, and VGG16, plus a couple of shallow
networks. Experimental results show that our proposed activation function
outperforms other state-of-the-art models with most networks.
</summary>
    <author>
      <name>Yikang Li</name>
    </author>
    <author>
      <name>Pak Lun Kevin Ding</name>
    </author>
    <author>
      <name>Baoxin Li</name>
    </author>
    <link href="http://arxiv.org/abs/1802.00212v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00212v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.02745v1</id>
    <updated>2018-02-08T08:25:51Z</updated>
    <published>2018-02-08T08:25:51Z</published>
    <title>Learning Inductive Biases with Simple Neural Networks</title>
    <summary>  People use rich prior knowledge about the world in order to efficiently learn
new concepts. These priors - also known as "inductive biases" - pertain to the
space of internal models considered by a learner, and they help the learner
make inferences that go beyond the observed data. A recent study found that
deep neural networks optimized for object recognition develop the shape bias
(Ritter et al., 2017), an inductive bias possessed by children that plays an
important role in early word learning. However, these networks use
unrealistically large quantities of training data, and the conditions required
for these biases to develop are not well understood. Moreover, it is unclear
how the learning dynamics of these networks relate to developmental processes
in childhood. We investigate the development and influence of the shape bias in
neural networks using controlled datasets of abstract patterns and synthetic
images, allowing us to systematically vary the quantity and form of the
experience provided to the learning algorithms. We find that simple neural
networks develop a shape bias after seeing as few as 3 examples of 4 object
categories. The development of these biases predicts the onset of vocabulary
acceleration in our networks, consistent with the developmental process in
children.
</summary>
    <author>
      <name>Reuben Feinman</name>
    </author>
    <author>
      <name>Brenden M. Lake</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to CogSci 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.02745v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.02745v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.08478v1</id>
    <updated>2018-02-23T10:59:40Z</updated>
    <published>2018-02-23T10:59:40Z</published>
    <title>Coloring black boxes: visualization of neural network decisions</title>
    <summary>  Neural networks are commonly regarded as black boxes performing
incomprehensible functions. For classification problems networks provide maps
from high dimensional feature space to K-dimensional image space. Images of
training vector are projected on polygon vertices, providing visualization of
network function. Such visualization may show the dynamics of learning, allow
for comparison of different networks, display training vectors around which
potential problems may arise, show differences due to regularization and
optimization procedures, investigate stability of network classification under
perturbation of original vectors, and place new data sample in relation to
training data, allowing for estimation of confidence in classification of a
given sample. An illustrative example for the three-class Wine data and
five-class Satimage data is described. The visualization method proposed here
is applicable to any black box system that provides continuous outputs.
</summary>
    <author>
      <name>Wlodzislaw Duch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 33 figures. Proc. of International Joint Conference on
  Neural Networks (IJCNN) 2003, Vol. I, pp. 1735-1740</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.08478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.08478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.07108v6</id>
    <updated>2017-10-19T16:34:35Z</updated>
    <published>2015-12-22T14:54:34Z</published>
    <title>Recent Advances in Convolutional Neural Networks</title>
    <summary>  In the last few years, deep learning has led to very good performance on a
variety of problems, such as visual recognition, speech recognition and natural
language processing. Among different types of deep neural networks,
convolutional neural networks have been most extensively studied. Leveraging on
the rapid growth in the amount of the annotated data and the great improvements
in the strengths of graphics processor units, the research on convolutional
neural networks has been emerged swiftly and achieved state-of-the-art results
on various tasks. In this paper, we provide a broad survey of the recent
advances in convolutional neural networks. We detailize the improvements of CNN
on different aspects, including layer design, activation function, loss
function, regularization, optimization and fast computation. Besides, we also
introduce various applications of convolutional neural networks in computer
vision, speech and natural language processing.
</summary>
    <author>
      <name>Jiuxiang Gu</name>
    </author>
    <author>
      <name>Zhenhua Wang</name>
    </author>
    <author>
      <name>Jason Kuen</name>
    </author>
    <author>
      <name>Lianyang Ma</name>
    </author>
    <author>
      <name>Amir Shahroudy</name>
    </author>
    <author>
      <name>Bing Shuai</name>
    </author>
    <author>
      <name>Ting Liu</name>
    </author>
    <author>
      <name>Xingxing Wang</name>
    </author>
    <author>
      <name>Li Wang</name>
    </author>
    <author>
      <name>Gang Wang</name>
    </author>
    <author>
      <name>Jianfei Cai</name>
    </author>
    <author>
      <name>Tsuhan Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pattern Recognition, Elsevier</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.07108v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.07108v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.08068v1</id>
    <updated>2017-04-26T11:45:05Z</updated>
    <published>2017-04-26T11:45:05Z</published>
    <title>Understanding the Feedforward Artificial Neural Network Model From the
  Perspective of Network Flow</title>
    <summary>  In recent years, deep learning based on artificial neural network (ANN) has
achieved great success in pattern recognition. However, there is no clear
understanding of such neural computational models. In this paper, we try to
unravel "black-box" structure of Ann model from network flow. Specifically, we
consider the feed forward Ann as a network flow model, which consists of many
directional class-pathways. Each class-pathway encodes one class. The
class-pathway of a class is obtained by connecting the activated neural nodes
in each layer from input to output, where activation value of neural node
(node-value) is defined by the weights of each layer in a trained
ANN-classifier. From the perspective of the class-pathway, training an
ANN-classifier can be regarded as the formulation process of class-pathways of
different classes. By analyzing the the distances of each two class-pathways in
a trained ANN-classifiers, we try to answer the questions, why the classifier
performs so? At last, from the neural encodes view, we define the importance of
each neural node through the class-pathways, which is helpful to optimize the
structure of a classifier. Experiments for two types of ANN model including
multi-layer MLP and CNN verify that the network flow based on class-pathway is
a reasonable explanation for ANN models.
</summary>
    <author>
      <name>Dawei Dai</name>
    </author>
    <author>
      <name>Weimin Tan</name>
    </author>
    <author>
      <name>Hong Zhan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1702.04595 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.08068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.08068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.08275v1</id>
    <updated>2017-07-26T02:17:05Z</updated>
    <published>2017-07-26T02:17:05Z</published>
    <title>An Exploration of Approaches to Integrating Neural Reranking Models in
  Multi-Stage Ranking Architectures</title>
    <summary>  We explore different approaches to integrating a simple convolutional neural
network (CNN) with the Lucene search engine in a multi-stage ranking
architecture. Our models are trained using the PyTorch deep learning toolkit,
which is implemented in C/C++ with a Python frontend. One obvious integration
strategy is to expose the neural network directly as a service. For this, we
use Apache Thrift, a software framework for building scalable cross-language
services. In exploring alternative architectures, we observe that once trained,
the feedforward evaluation of neural networks is quite straightforward.
Therefore, we can extract the parameters of a trained CNN from PyTorch and
import the model into Java, taking advantage of the Java Deeplearning4J library
for feedforward evaluation. This has the advantage that the entire end-to-end
system can be implemented in Java. As a third approach, we can extract the
neural network from PyTorch and "compile" it into a C++ program that exposes a
Thrift service. We evaluate these alternatives in terms of performance (latency
and throughput) as well as ease of integration. Experiments show that
feedforward evaluation of the convolutional neural network is significantly
slower in Java, while the performance of the compiled C++ network does not
consistently beat the PyTorch implementation.
</summary>
    <author>
      <name>Zhucheng Tu</name>
    </author>
    <author>
      <name>Matt Crane</name>
    </author>
    <author>
      <name>Royal Sequiera</name>
    </author>
    <author>
      <name>Junchen Zhang</name>
    </author>
    <author>
      <name>Jimmy Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SIGIR 2017 Workshop on Neural Information Retrieval (Neu-IR'17),
  August 7-11, 2017, Shinjuku, Tokyo, Japan</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.08275v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.08275v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.07910v1</id>
    <updated>2018-01-24T08:53:55Z</updated>
    <published>2018-01-24T08:53:55Z</published>
    <title>Waveform Modeling and Generation Using Hierarchical Recurrent Neural
  Networks for Speech Bandwidth Extension</title>
    <summary>  This paper presents a waveform modeling and generation method using
hierarchical recurrent neural networks (HRNN) for speech bandwidth extension
(BWE). Different from conventional BWE methods which predict spectral
parameters for reconstructing wideband speech waveforms, this BWE method models
and predicts waveform samples directly without using vocoders. Inspired by
SampleRNN which is an unconditional neural audio generator, the HRNN model
represents the distribution of each wideband or high-frequency waveform sample
conditioned on the input narrowband waveform samples using a neural network
composed of long short-term memory (LSTM) layers and feed-forward (FF) layers.
The LSTM layers form a hierarchical structure and each layer operates at a
specific temporal resolution to efficiently capture long-span dependencies
between temporal sequences. Furthermore, additional conditions, such as the
bottleneck (BN) features derived from narrowband speech using a deep neural
network (DNN)-based state classifier, are employed as auxiliary input to
further improve the quality of generated wideband speech. The experimental
results of comparing several waveform modeling methods show that the HRNN-based
method can achieve better speech quality and run-time efficiency than the
dilated convolutional neural network (DCNN)-based method and the plain
sample-level recurrent neural network (SRNN)-based method. Our proposed method
also outperforms the conventional vocoder-based BWE method using LSTM-RNNs in
terms of the subjective quality of the reconstructed wideband speech.
</summary>
    <author>
      <name>Zhen-Hua Ling</name>
    </author>
    <author>
      <name>Yang Ai</name>
    </author>
    <author>
      <name>Yu Gu</name>
    </author>
    <author>
      <name>Li-Rong Dai</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASLP.2018.2798811</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASLP.2018.2798811" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by IEEE Transactions on Audio, Speech and Language
  Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.07910v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.07910v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.02528v1</id>
    <updated>2018-02-07T17:12:54Z</updated>
    <published>2018-02-07T17:12:54Z</published>
    <title>Classification of Things in DBpedia using Deep Neural Networks</title>
    <summary>  The Semantic Web aims at representing knowledge about the real world at web
scale - things, their attributes and relationships among them can be
represented as nodes and edges in an inter-linked semantic graph. In the
presence of noisy data, as is typical of data on the Semantic Web, a software
Agent needs to be able to robustly infer one or more associated actionable
classes for the individuals in order to act automatically on it. We model this
problem as a multi-label classification task where we want to robustly identify
types of the individuals in a semantic graph such as DBpedia, which we use as
an exemplary dataset on the Semantic Web. Our approach first extracts multiple
features for the individuals using random walks and then performs multi-label
classification using fully-connected Neural Networks. Through systematic
exploration and experimentation, we identify the effect of hyper-parameters of
the feature extraction and the fully-connected Neural Network structure on the
classification performance. Our final results show that our method performs
better than state-of-the-art inferencing systems like SDtype and SLCN, from
which we can conclude that random-walk-based feature extraction of individuals
and their multi-label classification using Deep Neural Networks is a promising
alternative to these systems for type classification of individuals on the
Semantic Web. The main contribution of our work is to introduce a novel
approach that allows us to use Deep Neural Networks to identify types of
individuals in a noisy semantic graph by extracting features using random walks
</summary>
    <author>
      <name>Rahul Parundekar</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Classification of Things in DBpedia using Deep Neural Networks, R
  Parundekar, International Journal of Web &amp; Semantic Technology (IJWesT)
  Vol.9, No.1, January 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1802.02528v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.02528v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0704.3849v1</id>
    <updated>2007-04-29T17:54:02Z</updated>
    <published>2007-04-29T17:54:02Z</published>
    <title>Phase Transitions on Fractals and Networks</title>
    <summary>  For Encyclopedia of Complexist and System Science No abstract given
  I. Definition and Introduction
  II. Ising Model
  III. Fractals
  IV. Diffusion on Fractals
  V. Ising Model on Fractals
  VI. Other Subjects ?
  VII. Networks
  VIII. Future Directions
</summary>
    <author>
      <name>D. Stauffer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Review, 16 pages including all figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0704.3849v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0704.3849v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0708.2244v2</id>
    <updated>2008-05-07T22:13:45Z</updated>
    <published>2007-08-16T17:15:52Z</published>
    <title>Unbiased Random Threshold Networks Are Chaotic or Critical</title>
    <summary>  This paper has been withdrawn.
</summary>
    <author>
      <name>Volkan Sevim</name>
    </author>
    <author>
      <name>Per Arne Rikvold</name>
    </author>
    <link href="http://arxiv.org/abs/0708.2244v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.2244v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.4772v2</id>
    <updated>2010-02-01T22:54:57Z</updated>
    <published>2010-01-26T18:52:53Z</published>
    <title>Explosive Percolation in Social and Physical Networks</title>
    <summary>  We discuss several interesting random network models which exhibit (provable)
explosive transitions and their applications.
</summary>
    <author>
      <name>Eric J. Friedman</name>
    </author>
    <author>
      <name>Joel Nishimura</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">As first written as a brevium</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.4772v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.4772v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.5538v1</id>
    <updated>2011-04-29T02:28:50Z</updated>
    <published>2011-04-29T02:28:50Z</published>
    <title>Complex Networks</title>
    <summary>  Introduction to the Special Issue on Complex Networks, Artificial Life
journal.
</summary>
    <author>
      <name>Carlos Gershenson</name>
    </author>
    <author>
      <name>Mikhail Prokopenko</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/artl_e_00037</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/artl_e_00037" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, in press</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Artificial Life 17(4):259--261. 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1104.5538v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.5538v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.09089v1</id>
    <updated>2018-02-25T21:42:56Z</updated>
    <published>2018-02-25T21:42:56Z</published>
    <title>Kitsune: An Ensemble of Autoencoders for Online Network Intrusion
  Detection</title>
    <summary>  Neural networks have become an increasingly popular solution for network
intrusion detection systems (NIDS). Their capability of learning complex
patterns and behaviors make them a suitable solution for differentiating
between normal traffic and network attacks. However, a drawback of neural
networks is the amount of resources needed to train them. Many network gateways
and routers devices, which could potentially host an NIDS, simply do not have
the memory or processing power to train and sometimes even execute such models.
More importantly, the existing neural network solutions are trained in a
supervised manner. Meaning that an expert must label the network traffic and
update the model manually from time to time.
  In this paper, we present Kitsune: a plug and play NIDS which can learn to
detect attacks on the local network, without supervision, and in an efficient
online manner. Kitsune's core algorithm (KitNET) uses an ensemble of neural
networks called autoencoders to collectively differentiate between normal and
abnormal traffic patterns. KitNET is supported by a feature extraction
framework which efficiently tracks the patterns of every network channel. Our
evaluations show that Kitsune can detect various attacks with a performance
comparable to offline anomaly detectors, even on a Raspberry PI. This
demonstrates that Kitsune can be a practical and economic NIDS.
</summary>
    <author>
      <name>Yisroel Mirsky</name>
    </author>
    <author>
      <name>Tomer Doitshman</name>
    </author>
    <author>
      <name>Yuval Elovici</name>
    </author>
    <author>
      <name>Asaf Shabtai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Network and Distributed Systems Security Symposium (NDSS)
  2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.09089v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.09089v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/astro-ph/0108234v1</id>
    <updated>2001-08-14T14:07:28Z</updated>
    <published>2001-08-14T14:07:28Z</published>
    <title>Neural Networks as a tool for parameter estimation in mega-pixel data
  sets</title>
    <summary>  We present a neural net algorithm for parameter estimation in the context of
large cosmological data sets. Cosmological data sets present a particular
challenge to pattern-recognition algorithms since the input patterns (galaxy
redshift surveys, maps of cosmic microwave background anisotropy) are not fixed
templates overlaid with random noise, but rather are random realizations whose
information content lies in the correlations between data points. We train a
``committee'' of neural nets to distinguish between Monte Carlo simulations at
fixed parameter values. Sampling the trained networks using additional Monte
Carlo simulations generated at intermediate parameter values allows accurate
interpolation to parameter values for which the networks were never trained.
The Monte Carlo samples automatically provide the probability distributions and
truth tables required for either a frequentist or Bayseian analysis of the one
observable sky. We demonstrate that neural networks provide unbiased parameter
estimation with comparable precision as maximum-likelihood algorithms but
significant computational savings. In the context of CMB anisotropies, the
computational cost for parameter estimation via neural networks scales as
$N^{3/2}$. The results are insensitive to the noise levels and sampling schemes
typical of large cosmological data sets and provide a desirable tool for the
new generation of large, complex data sets.
</summary>
    <author>
      <name>Nicholas G. Phillips</name>
    </author>
    <author>
      <name>A. Kogut</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ApJ; 30 pages, 7 figures, uses AASTeX v5.0</arxiv:comment>
    <link href="http://arxiv.org/abs/astro-ph/0108234v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/astro-ph/0108234v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0212541v1</id>
    <updated>2002-12-20T23:01:52Z</updated>
    <published>2002-12-20T23:01:52Z</published>
    <title>Parametrical Neural Network</title>
    <summary>  The storage capacity of the Hopfield model is about 15% of the network size.
It can be increased significantly in the Potts-glass model of the associative
memory only. In this model neurons can be in more than two different states. We
show that even greater storage capacity can be achieved in the parametrical
neural network (PNN) that is based on the parametrical four-wave mixing process
that is well-known in nonlinear optics. We present a uniform formalism allowing
us to describe both PNN and the Potts-glass associative memory. To estimate the
storage capacity we use the Chebyshev-Chernov statistical technique.
</summary>
    <author>
      <name>B. V. Kryzhanovsky</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Institute of Optical Neural Technologies Russian Academy of Sciences</arxiv:affiliation>
    </author>
    <author>
      <name>L. B. Litinskii</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Institute of Optical Neural Technologies Russian Academy of Sciences</arxiv:affiliation>
    </author>
    <author>
      <name>A. L. Mikaelyan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Institute of Optical Neural Technologies Russian Academy of Sciences</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 2 Postscript figures, sumbitted to Neural Networks</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0212541v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0212541v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0204008v1</id>
    <updated>2002-04-04T18:59:49Z</updated>
    <published>2002-04-04T18:59:49Z</published>
    <title>The tip-of-the-tongue phenomenon: Irrelevant neural network localization
  or disruption of its interneuron links ?</title>
    <summary>  On the base of recently proposed three-stage quantitative neural network
model of the tip-of-the-tongue (TOT) phenomenon a possibility to occur of TOT
states coursed by neural network interneuron links' disruption has been
studied. Using a numerical example it was found that TOTs coursed by interneron
links' disruption are in (1.5 + - 0.3)x1000 times less probable then those
coursed by irrelevant (incomplete) neural network localization. It was shown
that delayed TOT states' etiology cannot be related to neural network
interneuron links' disruption.
</summary>
    <author>
      <name>Petro M. Gopych</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the VIII All-Russian conference "Neurocomputers and
  their application - 2002" with international participation, held in Moscow on
  21-22 March 2002, 5 pages and 1 Figure</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0204008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0204008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; I.2.7; J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0405032v1</id>
    <updated>2004-05-07T00:01:54Z</updated>
    <published>2004-05-07T00:01:54Z</published>
    <title>EvoNF: A Framework for Optimization of Fuzzy Inference Systems Using
  Neural Network Learning and Evolutionary Computation</title>
    <summary>  Several adaptation techniques have been investigated to optimize fuzzy
inference systems. Neural network learning algorithms have been used to
determine the parameters of fuzzy inference system. Such models are often
called as integrated neuro-fuzzy models. In an integrated neuro-fuzzy model
there is no guarantee that the neural network learning algorithm converges and
the tuning of fuzzy inference system will be successful. Success of
evolutionary search procedures for optimization of fuzzy inference system is
well proven and established in many application areas. In this paper, we will
explore how the optimization of fuzzy inference systems could be further
improved using a meta-heuristic approach combining neural network learning and
evolutionary computation. The proposed technique could be considered as a
methodology to integrate neural networks, fuzzy inference systems and
evolutionary search procedures. We present the theoretical frameworks and some
experimental results to demonstrate the efficiency of the proposed technique.
</summary>
    <author>
      <name>Ajith Abraham</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ISIC.2002.1157784</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ISIC.2002.1157784" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The 17th IEEE International Symposium on Intelligent Control,
  ISIC'02, IEEE Press, ISBN 0780376218, pp 327-332, 2002</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0405032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0405032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="1.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0412049v1</id>
    <updated>2004-12-11T12:32:10Z</updated>
    <published>2004-12-11T12:32:10Z</published>
    <title>Neural Networks in Mobile Robot Motion</title>
    <summary>  This paper deals with a path planning and intelligent control of an
autonomous robot which should move safely in partially structured environment.
This environment may involve any number of obstacles of arbitrary shape and
size; some of them are allowed to move. We describe our approach to solving the
motion-planning problem in mobile robot control using neural networks-based
technique. Our method of the construction of a collision-free path for moving
robot among obstacles is based on two neural networks. The first neural network
is used to determine the "free" space using ultrasound range finder data. The
second neural network "finds" a safe direction for the next robot section of
the path in the workspace while avoiding the nearest obstacles. Simulation
examples of generated path with proposed techniques will be presented.
</summary>
    <author>
      <name>Danica Janglova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 Pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advanced Robotic Systems, Volume 1,
  Number 1, March 2004, pp.15-22</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0412049v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0412049v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/hep-ph/9209216v1</id>
    <updated>1992-09-04T19:55:10Z</updated>
    <published>1992-09-04T19:55:10Z</published>
    <title>A Comparison of the Use of Binary Decision Trees and Neural Networks in
  Top Quark Detection</title>
    <summary>  The use of neural networks for signal vs.~background discrimination in
high-energy physics experiment has been investigated and has compared favorably
with the efficiency of traditional kinematic cuts. Recent work in top quark
identification produced a neural network that, for a given top quark mass,
yielded a higher signal to background ratio in Monte Carlo simulation than a
corresponding set of conventional cuts. In this article we discuss another
pattern-recognition algorithm, the binary decision tree. We have applied a
binary decision tree to top quark identification at the Tevatron and found it
to be comparable in performance to the neural network. Furthermore,
reservations about the "black box" nature of neural network discriminators do
not apply to binary decision trees; a binary decision tree may be reduced to a
set of kinematic cuts subject to conventional error analysis.
</summary>
    <author>
      <name>David Bowser-Chao</name>
    </author>
    <author>
      <name>Debra L. Dzialo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevD.47.1900</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevD.47.1900" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14pp. Plain TeX + mtexsis.tex (latter available through 'get
  mtexsis.tex'.) Two postscript files avail. by email</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys.Rev. D47 (1993) 1900-1905</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/hep-ph/9209216v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/hep-ph/9209216v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.3582v1</id>
    <updated>2008-02-25T09:57:31Z</updated>
    <published>2008-02-25T09:57:31Z</published>
    <title>Neural Networks and Database Systems</title>
    <summary>  Object-oriented database systems proved very valuable at handling and
administrating complex objects. In the following guidelines for embedding
neural networks into such systems are presented. It is our goal to treat
networks as normal data in the database system. From the logical point of view,
a neural network is a complex data value and can be stored as a normal data
object. It is generally accepted that rule-based reasoning will play an
important role in future database applications. The knowledge base consists of
facts and rules, which are both stored and handled by the underlying database
system. Neural networks can be seen as representation of intensional knowledge
of intelligent database systems. So they are part of a rule based knowledge
pool and can be used like conventional rules. The user has a unified view about
his knowledge base regardless of the origin of the unique rules.
</summary>
    <author>
      <name>Erich Schikuta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, Festschrift Informationssysteme, in honor of G. Vinek</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">pp. 133-152, 2007, publisher Austrian Computer Society</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0802.3582v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.3582v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.3061v1</id>
    <updated>2008-03-20T19:22:18Z</updated>
    <published>2008-03-20T19:22:18Z</published>
    <title>Constraint satisfaction problems and neural networks: a statistical
  physics perspective</title>
    <summary>  A new field of research is rapidly expanding at the crossroad between
statistical physics, information theory and combinatorial optimization. In
particular, the use of cutting edge statistical physics concepts and methods
allow one to solve very large constraint satisfaction problems like random
satisfiability, coloring, or error correction. Several aspects of these
developments should be relevant for the understanding of functional complexity
in neural networks. On the one hand the message passing procedures which are
used in these new algorithms are based on local exchange of information, and
succeed in solving some of the hardest computational problems. On the other
hand some crucial inference problems in neurobiology, like those generated in
multi-electrode recordings, naturally translate into hard constraint
satisfaction problems. This paper gives a non-technical introduction to this
field, emphasizing the main ideas at work in message passing strategies and
their possible relevance to neural networks modeling. It also introduces a new
message passing algorithm for inferring interactions between variables from
correlation data, which could be useful in the analysis of multi-electrode
recording data.
</summary>
    <author>
      <name>Marc Mezard</name>
    </author>
    <author>
      <name>Thierry Mora</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Prepared for the proceedings of the 2007 Tauc Conference on
  Complexity in Neural Network Dynamics</arxiv:comment>
    <link href="http://arxiv.org/abs/0803.3061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.3061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.1634v1</id>
    <updated>2010-10-08T09:20:36Z</updated>
    <published>2010-10-08T09:20:36Z</published>
    <title>Foreground removal from WMAP 5yr temperature maps using an MLP neural
  network</title>
    <summary>  One of the main obstacles for extracting the cosmic microwave background
(CMB) signal from observations in the mm/sub-mm range is the foreground
contamination by emission from Galactic component: mainly synchrotron,
free-free, and thermal dust emission. The statistical nature of the intrinsic
CMB signal makes it essential to minimize the systematic errors in the CMB
temperature determinations. The feasibility of using simple neural networks to
extract the CMB signal from detailed simulated data has already been
demonstrated. Here, simple neural networks are applied to the WMAP 5yr
temperature data without using any auxiliary data. A simple \emph{multilayer
perceptron} neural network with two hidden layers provides temperature
estimates over more than 75 per cent of the sky with random errors
significantly below those previously extracted from these data. Also, the
systematic errors, i.e.\ errors correlated with the Galactic foregrounds, are
very small. With these results the neural network method is well prepared for
dealing with the high - quality CMB data from the ESA Planck Surveyor
satellite.
</summary>
    <author>
      <name>H. U. N√∏rgaard-Nielsen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1051/0004-6361/201014288</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1051/0004-6361/201014288" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 13 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">A&amp;A 520, A87 (2010)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1010.1634v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.1634v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.5686v1</id>
    <updated>2012-11-24T17:39:07Z</updated>
    <published>2012-11-24T17:39:07Z</published>
    <title>Critical and resonance phenomena in neural networks</title>
    <summary>  Brain rhythms contribute to every aspect of brain function. Here, we study
critical and resonance phenomena that precede the emergence of brain rhythms.
Using an analytical approach and simulations of a cortical circuit model of
neural networks with stochastic neurons in the presence of noise, we show that
spontaneous appearance of network oscillations occurs as a dynamical
(non-equilibrium) phase transition at a critical point determined by the noise
level, network structure, the balance between excitatory and inhibitory
neurons, and other parameters. We find that the relaxation time of neural
activity to a steady state, response to periodic stimuli at the frequency of
the oscillations, amplitude of damped oscillations, and stochastic fluctuations
of neural activity are dramatically increased when approaching the critical
point of the transition.
</summary>
    <author>
      <name>A. V. Goltsev</name>
    </author>
    <author>
      <name>M. A. Lopes</name>
    </author>
    <author>
      <name>K. -E. Lee</name>
    </author>
    <author>
      <name>J. F. F. Mendes</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/1.4776498</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/1.4776498" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, Proceedings of 12th Granada Seminar, September 17-21, 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.5686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.5686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.2732v1</id>
    <updated>2014-06-10T22:07:01Z</updated>
    <published>2014-06-10T22:07:01Z</published>
    <title>Deep Epitomic Convolutional Neural Networks</title>
    <summary>  Deep convolutional neural networks have recently proven extremely competitive
in challenging image recognition tasks. This paper proposes the epitomic
convolution as a new building block for deep neural networks. An epitomic
convolution layer replaces a pair of consecutive convolution and max-pooling
layers found in standard deep convolutional neural networks. The main version
of the proposed model uses mini-epitomes in place of filters and computes
responses invariant to small translations by epitomic search instead of
max-pooling over image positions. The topographic version of the proposed model
uses large epitomes to learn filter maps organized in translational
topographies. We show that error back-propagation can successfully learn
multiple epitomic layers in a supervised fashion. The effectiveness of the
proposed method is assessed in image classification tasks on standard
benchmarks. Our experiments on Imagenet indicate improved recognition
performance compared to standard convolutional neural networks of similar
architecture. Our models pre-trained on Imagenet perform excellently on
Caltech-101. We also obtain competitive image classification results on the
small-image MNIST and CIFAR-10 datasets.
</summary>
    <author>
      <name>George Papandreou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.2732v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.2732v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.2865v2</id>
    <updated>2010-05-30T09:02:08Z</updated>
    <published>2009-11-15T13:04:26Z</published>
    <title>Neural Networks for Dynamic Shortest Path Routing Problems - A Survey</title>
    <summary>  This paper reviews the overview of the dynamic shortest path routing problem
and the various neural networks to solve it. Different shortest path
optimization problems can be solved by using various neural networks
algorithms. The routing in packet switched multi-hop networks can be described
as a classical combinatorial optimization problem i.e. a shortest path routing
problem in graphs. The survey shows that the neural networks are the best
candidates for the optimization of dynamic shortest path routing problems due
to their fastness in computation comparing to other softcomputing and
metaheuristics algorithms
</summary>
    <author>
      <name>R. Nallusamy</name>
    </author>
    <author>
      <name>K. Duraiswamy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This article has been withdrawn by the authors. Misplaced equation 1</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CiiT International Journal of Artificial Intelligent Systems and
  Machine Learning, Vol. 1, No. 2, pp. 31-34, May 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0911.2865v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.2865v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.3113v1</id>
    <updated>2009-11-16T18:32:56Z</updated>
    <published>2009-11-16T18:32:56Z</published>
    <title>Constraints for the order parameters in analogical neural networks</title>
    <summary>  In this paper we study, via equilibrium statistical mechanics, the properties
of the internal energy of an Hopfield neural network whose patterns are stored
continuously (Gaussian distributed). The model is shown to be equivalent to a
bipartite spin glass in which one party is given by dichotomic neurons and the
other party by Gaussian spin variables. Dealing with replicated systems, beyond
the Mattis magnetization, we introduce two overlaps, one for each party, as
order parameters of the theory: The first is a standard overlap among neural
configurations on different replicas, the second is an overlap among the
Gaussian spins of different replicas. The aim of this work is to show the
existence of constraints for these order parameters close to ones found in many
other complex systems as spin glasses and diluted networks: we find a class of
Ghirlanda-Guerra-like identities for both the overlaps, generalizing the well
known constraints to the neural networks, as well as new identities where noise
is involved explicitly.
</summary>
    <author>
      <name>Adriano Barra</name>
    </author>
    <author>
      <name>Francesco Guerra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, published in: Percorsi d'Ateneo, S. Vitolo Ed., Salerno,
  (2008)</arxiv:comment>
    <link href="http://arxiv.org/abs/0911.3113v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.3113v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.0342v2</id>
    <updated>2010-08-25T08:41:00Z</updated>
    <published>2010-06-02T10:10:30Z</published>
    <title>Neural Network Parameterizations of Electromagnetic Nucleon Form Factors</title>
    <summary>  The electromagnetic nucleon form-factors data are studied with artificial
feed forward neural networks. As a result the unbiased model-independent
form-factor parametrizations are evaluated together with uncertainties. The
Bayesian approach for the neural networks is adapted for chi2 error-like
function and applied to the data analysis. The sequence of the feed forward
neural networks with one hidden layer of units is considered. The given neural
network represents a particular form-factor parametrization. The so-called
evidence (the measure of how much the data favor given statistical model) is
computed with the Bayesian framework and it is used to determine the best form
factor parametrization.
</summary>
    <author>
      <name>Krzysztof M. Graczyk</name>
    </author>
    <author>
      <name>Piotr Plonski</name>
    </author>
    <author>
      <name>Robert Sulej</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/JHEP09(2010)053</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/JHEP09(2010)053" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The revised version is divided into 4 sections. The discussion of the
  prior assumptions is added. The manuscript contains 4 new figures and 2 new
  tables (32 pages, 15 figures, 2 tables)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">JHEP 1009:053,2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1006.0342v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.0342v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nucl-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nucl-th" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.1761v1</id>
    <updated>2013-11-07T17:39:31Z</updated>
    <published>2013-11-07T17:39:31Z</published>
    <title>Exploring Deep and Recurrent Architectures for Optimal Control</title>
    <summary>  Sophisticated multilayer neural networks have achieved state of the art
results on multiple supervised tasks. However, successful applications of such
multilayer networks to control have so far been limited largely to the
perception portion of the control pipeline. In this paper, we explore the
application of deep and recurrent neural networks to a continuous,
high-dimensional locomotion task, where the network is used to represent a
control policy that maps the state of the system (represented by joint angles)
directly to the torques at each joint. By using a recent reinforcement learning
algorithm called guided policy search, we can successfully train neural network
controllers with thousands of parameters, allowing us to compare a variety of
architectures. We discuss the differences between the locomotion control task
and previous supervised perception tasks, present experimental results
comparing various architectures, and discuss future directions in the
application of techniques from deep learning to the problem of optimal control.
</summary>
    <author>
      <name>Sergey Levine</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in the Neural Information Processing Systems (NIPS 2013)
  Workshop on Deep Learning</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.1761v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.1761v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.5814v1</id>
    <updated>2013-12-20T05:39:51Z</updated>
    <published>2013-12-20T05:39:51Z</published>
    <title>Optimal parameter selection for unsupervised neural network using
  genetic algorithm</title>
    <summary>  K-means Fast Learning Artificial Neural Network (K-FLANN) is an unsupervised
neural network requires two parameters: tolerance and vigilance. Best
Clustering results are feasible only by finest parameters specified to the
neural network. Selecting optimal values for these parameters is a major
problem. To solve this issue, Genetic Algorithm (GA) is used to determine
optimal parameters of K-FLANN for finding groups in multidimensional data.
K-FLANN is a simple topological network, in which output nodes grows
dynamically during the clustering process on receiving input patterns. Original
K-FLANN is enhanced to select winner unit out of the matched nodes so that
stable clusters are formed with in a less number of epochs. The experimental
results show that the GA is efficient in finding optimal values of parameters
from the large search space and is tested using artificial and synthetic data
sets.
</summary>
    <author>
      <name>suneetha chittineni</name>
    </author>
    <author>
      <name>Raveendra Babu Bhogapathi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcsea.2013.3502</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcsea.2013.3502" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages,4 figures,4 tables, International Journal of Computer
  Science, Engineering and Applications (IJCSEA) Vol.3, No.5, October 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.5814v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.5814v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.1141v1</id>
    <updated>2014-02-05T19:48:24Z</updated>
    <published>2014-02-05T19:48:24Z</published>
    <title>Quantum Cybernetics and Complex Quantum Systems Science - A Quantum
  Connectionist Exploration</title>
    <summary>  Quantum cybernetics and its connections to complex quantum systems science is
addressed from the perspective of complex quantum computing systems. In this
way, the notion of an autonomous quantum computing system is introduced in
regards to quantum artificial intelligence, and applied to quantum artificial
neural networks, considered as autonomous quantum computing systems, which
leads to a quantum connectionist framework within quantum cybernetics for
complex quantum computing systems. Several examples of quantum feedforward
neural networks are addressed in regards to Boolean functions' computation,
multilayer quantum computation dynamics, entanglement and quantum
complementarity. The examples provide a framework for a reflection on the role
of quantum artificial neural networks as a general framework for addressing
complex quantum systems that perform network-based quantum computation,
possible consequences are drawn regarding quantum technologies, as well as
fundamental research in complex quantum systems science and quantum biology.
</summary>
    <author>
      <name>Carlos Pedro Gon√ßalves</name>
    </author>
    <link href="http://arxiv.org/abs/1402.1141v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.1141v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92B20, 81P68, 82C32" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6544v6</id>
    <updated>2015-05-21T21:44:31Z</updated>
    <published>2014-12-19T21:55:01Z</published>
    <title>Qualitatively characterizing neural network optimization problems</title>
    <summary>  Training neural networks involves solving large-scale non-convex optimization
problems. This task has long been believed to be extremely difficult, with fear
of local minima and other obstacles motivating a variety of schemes to improve
optimization, such as unsupervised pretraining. However, modern neural networks
are able to achieve negligible training error on complex tasks, using only
direct training with stochastic gradient descent. We introduce a simple
analysis technique to look for evidence that such networks are overcoming local
optima. We find that, in fact, on a straight path from initialization to
solution, a variety of state of the art neural networks never encounter any
significant obstacles.
</summary>
    <author>
      <name>Ian J. Goodfellow</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Andrew M. Saxe</name>
    </author>
    <link href="http://arxiv.org/abs/1412.6544v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.6544v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.07376v3</id>
    <updated>2015-11-06T13:55:09Z</updated>
    <published>2015-05-27T15:29:52Z</published>
    <title>Texture Synthesis Using Convolutional Neural Networks</title>
    <summary>  Here we introduce a new model of natural textures based on the feature spaces
of convolutional neural networks optimised for object recognition. Samples from
the model are of high perceptual quality demonstrating the generative power of
neural networks trained in a purely discriminative fashion. Within the model,
textures are represented by the correlations between feature maps in several
layers of the network. We show that across layers the texture representations
increasingly capture the statistical properties of natural images while making
object information more and more explicit. The model provides a new tool to
generate stimuli for neuroscience and might offer insights into the deep
representations learned by convolutional neural networks.
</summary>
    <author>
      <name>Leon A. Gatys</name>
    </author>
    <author>
      <name>Alexander S. Ecker</name>
    </author>
    <author>
      <name>Matthias Bethge</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Revision for NIPS 2015 Camera Ready. In line with reviewer's comments
  we now focus on the texture model and texture synthesis performance. We limit
  the relationship of our texture model to the ventral stream and its potential
  use for neuroscience to the discussion of the paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.07376v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.07376v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.03623v1</id>
    <updated>2015-06-11T11:01:40Z</updated>
    <published>2015-06-11T11:01:40Z</published>
    <title>Max-Entropy Feed-Forward Clustering Neural Network</title>
    <summary>  The outputs of non-linear feed-forward neural network are positive, which
could be treated as probability when they are normalized to one. If we take
Entropy-Based Principle into consideration, the outputs for each sample could
be represented as the distribution of this sample for different clusters.
Entropy-Based Principle is the principle with which we could estimate the
unknown distribution under some limited conditions. As this paper defines two
processes in Feed-Forward Neural Network, our limited condition is the
abstracted features of samples which are worked out in the abstraction process.
And the final outputs are the probability distribution for different clusters
in the clustering process. As Entropy-Based Principle is considered into the
feed-forward neural network, a clustering method is born. We have conducted
some experiments on six open UCI datasets, comparing with a few baselines and
applied purity as the measurement . The results illustrate that our method
outperforms all the other baselines that are most popular clustering methods.
</summary>
    <author>
      <name>Han Xiao</name>
    </author>
    <author>
      <name>Xiaoyan Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been published in ICANN 2015</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICANN 2015: International Conference on Artificial Neural
  Networks, Amsterdam, The Netherlands, (May 14-15, 2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1506.03623v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.03623v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.05128v2</id>
    <updated>2015-10-13T12:55:45Z</updated>
    <published>2015-08-20T21:18:25Z</published>
    <title>Lifted Relational Neural Networks</title>
    <summary>  We propose a method combining relational-logic representations with neural
network learning. A general lifted architecture, possibly reflecting some
background domain knowledge, is described through relational rules which may be
handcrafted or learned. The relational rule-set serves as a template for
unfolding possibly deep neural networks whose structures also reflect the
structures of given training or testing relational examples. Different networks
corresponding to different examples share their weights, which co-evolve during
training by stochastic gradient descent algorithm. The framework allows for
hierarchical relational modeling constructs and learning of latent relational
concepts through shared hidden layers weights corresponding to the rules.
Discovery of notable relational concepts and experiments on 78 relational
learning benchmarks demonstrate favorable performance of the method.
</summary>
    <author>
      <name>Gustav Sourek</name>
    </author>
    <author>
      <name>Vojtech Aschenbrenner</name>
    </author>
    <author>
      <name>Filip Zelezny</name>
    </author>
    <author>
      <name>Ondrej Kuzelka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Expanded section on weight learning, added explanation of
  relationship to convolutional neural networks</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.05128v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.05128v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.01378v1</id>
    <updated>2015-10-05T21:45:31Z</updated>
    <published>2015-10-05T21:45:31Z</published>
    <title>Batch Normalized Recurrent Neural Networks</title>
    <summary>  Recurrent Neural Networks (RNNs) are powerful models for sequential data that
have the potential to learn long-term dependencies. However, they are
computationally expensive to train and difficult to parallelize. Recent work
has shown that normalizing intermediate representations of neural networks can
significantly improve convergence rates in feedforward neural networks . In
particular, batch normalization, which uses mini-batch statistics to
standardize features, was shown to significantly reduce training time. In this
paper, we show that applying batch normalization to the hidden-to-hidden
transitions of our RNNs doesn't help the training procedure. We also show that
when applied to the input-to-hidden transitions, batch normalization can lead
to a faster convergence of the training criterion but doesn't seem to improve
the generalization performance on both our language modelling and speech
recognition tasks. All in all, applying batch normalization to RNNs turns out
to be more challenging than applying it to feedforward networks, but certain
variants of it can still be beneficial.
</summary>
    <author>
      <name>C√©sar Laurent</name>
    </author>
    <author>
      <name>Gabriel Pereyra</name>
    </author>
    <author>
      <name>Phil√©mon Brakel</name>
    </author>
    <author>
      <name>Ying Zhang</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <link href="http://arxiv.org/abs/1510.01378v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.01378v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.02855v1</id>
    <updated>2015-10-10T00:09:00Z</updated>
    <published>2015-10-10T00:09:00Z</published>
    <title>AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction
  in Structure-based Drug Discovery</title>
    <summary>  Deep convolutional neural networks comprise a subclass of deep neural
networks (DNN) with a constrained architecture that leverages the spatial and
temporal structure of the domain they model. Convolutional networks achieve the
best predictive performance in areas such as speech and image recognition by
hierarchically composing simple local features into complex models. Although
DNNs have been used in drug discovery for QSAR and ligand-based bioactivity
predictions, none of these models have benefited from this powerful
convolutional architecture. This paper introduces AtomNet, the first
structure-based, deep convolutional neural network designed to predict the
bioactivity of small molecules for drug discovery applications. We demonstrate
how to apply the convolutional concepts of feature locality and hierarchical
composition to the modeling of bioactivity and chemical interactions. In
further contrast to existing DNN techniques, we show that AtomNet's application
of local convolutional filters to structural target information successfully
predicts new active molecules for targets with no previously known modulators.
Finally, we show that AtomNet outperforms previous docking approaches on a
diverse set of benchmarks by a large margin, achieving an AUC greater than 0.9
on 57.8% of the targets in the DUDE benchmark.
</summary>
    <author>
      <name>Izhar Wallach</name>
    </author>
    <author>
      <name>Michael Dzamba</name>
    </author>
    <author>
      <name>Abraham Heifets</name>
    </author>
    <link href="http://arxiv.org/abs/1510.02855v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.02855v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.BM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.03528v1</id>
    <updated>2015-10-13T04:36:09Z</updated>
    <published>2015-10-13T04:36:09Z</published>
    <title>$\ell_1$-regularized Neural Networks are Improperly Learnable in
  Polynomial Time</title>
    <summary>  We study the improper learning of multi-layer neural networks. Suppose that
the neural network to be learned has $k$ hidden layers and that the
$\ell_1$-norm of the incoming weights of any neuron is bounded by $L$. We
present a kernel-based method, such that with probability at least $1 -
\delta$, it learns a predictor whose generalization error is at most $\epsilon$
worse than that of the neural network. The sample complexity and the time
complexity of the presented method are polynomial in the input dimension and in
$(1/\epsilon,\log(1/\delta),F(k,L))$, where $F(k,L)$ is a function depending on
$(k,L)$ and on the activation function, independent of the number of neurons.
The algorithm applies to both sigmoid-like activation functions and ReLU-like
activation functions. It implies that any sufficiently sparse neural network is
learnable in polynomial time.
</summary>
    <author>
      <name>Yuchen Zhang</name>
    </author>
    <author>
      <name>Jason D. Lee</name>
    </author>
    <author>
      <name>Michael I. Jordan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.03528v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.03528v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.05497v2</id>
    <updated>2016-08-02T11:46:48Z</updated>
    <published>2015-11-17T18:26:11Z</published>
    <title>Learning Neural Network Architectures using Backpropagation</title>
    <summary>  Deep neural networks with millions of parameters are at the heart of many
state of the art machine learning models today. However, recent works have
shown that models with much smaller number of parameters can also perform just
as well. In this work, we introduce the problem of architecture-learning, i.e;
learning the architecture of a neural network along with weights. We introduce
a new trainable parameter called tri-state ReLU, which helps in eliminating
unnecessary neurons. We also propose a smooth regularizer which encourages the
total number of neurons after elimination to be small. The resulting objective
is differentiable and simple to optimize. We experimentally validate our method
on both small and large networks, and show that it can learn models with a
considerably small number of parameters without affecting prediction accuracy.
</summary>
    <author>
      <name>Suraj Srinivas</name>
    </author>
    <author>
      <name>R. Venkatesh Babu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">BMVC 2016 ; Title modified from 'Learning the Architecture of Deep
  Neural Networks'</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.05497v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.05497v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.07948v1</id>
    <updated>2015-11-25T04:41:20Z</updated>
    <published>2015-11-25T04:41:20Z</published>
    <title>Learning Halfspaces and Neural Networks with Random Initialization</title>
    <summary>  We study non-convex empirical risk minimization for learning halfspaces and
neural networks. For loss functions that are $L$-Lipschitz continuous, we
present algorithms to learn halfspaces and multi-layer neural networks that
achieve arbitrarily small excess risk $\epsilon&gt;0$. The time complexity is
polynomial in the input dimension $d$ and the sample size $n$, but exponential
in the quantity $(L/\epsilon^2)\log(L/\epsilon)$. These algorithms run multiple
rounds of random initialization followed by arbitrary optimization steps. We
further show that if the data is separable by some neural network with constant
margin $\gamma&gt;0$, then there is a polynomial-time algorithm for learning a
neural network that separates the training data with margin $\Omega(\gamma)$.
As a consequence, the algorithm achieves arbitrary generalization error
$\epsilon&gt;0$ with ${\rm poly}(d,1/\epsilon)$ sample and time complexity. We
establish the same learnability result when the labels are randomly flipped
with probability $\eta&lt;1/2$.
</summary>
    <author>
      <name>Yuchen Zhang</name>
    </author>
    <author>
      <name>Jason D. Lee</name>
    </author>
    <author>
      <name>Martin J. Wainwright</name>
    </author>
    <author>
      <name>Michael I. Jordan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.07948v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.07948v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.08630v2</id>
    <updated>2015-11-30T07:20:46Z</updated>
    <published>2015-11-27T11:44:17Z</published>
    <title>A C-LSTM Neural Network for Text Classification</title>
    <summary>  Neural network models have been demonstrated to be capable of achieving
remarkable performance in sentence and document modeling. Convolutional neural
network (CNN) and recurrent neural network (RNN) are two mainstream
architectures for such modeling tasks, which adopt totally different ways of
understanding natural languages. In this work, we combine the strengths of both
architectures and propose a novel and unified model called C-LSTM for sentence
representation and text classification. C-LSTM utilizes CNN to extract a
sequence of higher-level phrase representations, and are fed into a long
short-term memory recurrent neural network (LSTM) to obtain the sentence
representation. C-LSTM is able to capture both local features of phrases as
well as global and temporal sentence semantics. We evaluate the proposed
architecture on sentiment classification and question classification tasks. The
experimental results show that the C-LSTM outperforms both CNN and LSTM and can
achieve excellent performance on these tasks.
</summary>
    <author>
      <name>Chunting Zhou</name>
    </author>
    <author>
      <name>Chonglin Sun</name>
    </author>
    <author>
      <name>Zhiyuan Liu</name>
    </author>
    <author>
      <name>Francis C. M. Lau</name>
    </author>
    <link href="http://arxiv.org/abs/1511.08630v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.08630v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.07366v1</id>
    <updated>2016-02-24T01:38:14Z</updated>
    <published>2016-02-24T01:38:14Z</published>
    <title>An Intelligent QoS Identification for Untrustworthy Web Services Via
  Two-phase Neural Networks</title>
    <summary>  QoS identification for untrustworthy Web services is critical in QoS
management in the service computing since the performance of untrustworthy Web
services may result in QoS downgrade. The key issue is to intelligently learn
the characteristics of trustworthy Web services from different QoS levels, then
to identify the untrustworthy ones according to the characteristics of QoS
metrics. As one of the intelligent identification approaches, deep neural
network has emerged as a powerful technique in recent years. In this paper, we
propose a novel two-phase neural network model to identify the untrustworthy
Web services. In the first phase, Web services are collected from the published
QoS dataset. Then, we design a feedforward neural network model to build the
classifier for Web services with different QoS levels. In the second phase, we
employ a probabilistic neural network (PNN) model to identify the untrustworthy
Web services from each classification. The experimental results show the
proposed approach has 90.5% identification ratio far higher than other
competing approaches.
</summary>
    <author>
      <name>Weidong Wang</name>
    </author>
    <author>
      <name>Liqiang Wang</name>
    </author>
    <author>
      <name>Wei Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.07366v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.07366v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.07377v5</id>
    <updated>2017-01-10T04:50:47Z</updated>
    <published>2016-02-24T03:10:32Z</published>
    <title>How Deep Neural Networks Can Improve Emotion Recognition on Video Data</title>
    <summary>  We consider the task of dimensional emotion recognition on video data using
deep learning. While several previous methods have shown the benefits of
training temporal neural network models such as recurrent neural networks
(RNNs) on hand-crafted features, few works have considered combining
convolutional neural networks (CNNs) with RNNs. In this work, we present a
system that performs emotion recognition on video data using both CNNs and
RNNs, and we also analyze how much each neural network component contributes to
the system's overall performance. We present our findings on videos from the
Audio/Visual+Emotion Challenge (AV+EC2015). In our experiments, we analyze the
effects of several hyperparameters on overall performance while also achieving
superior performance to the baseline and other competing methods.
</summary>
    <author>
      <name>Pooya Khorrami</name>
    </author>
    <author>
      <name>Tom Le Paine</name>
    </author>
    <author>
      <name>Kevin Brady</name>
    </author>
    <author>
      <name>Charlie Dagli</name>
    </author>
    <author>
      <name>Thomas S. Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ICIP 2016. Fixed typo in Experiments section</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.07377v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.07377v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.03390v2</id>
    <updated>2016-12-12T12:28:07Z</updated>
    <published>2016-04-12T13:09:01Z</published>
    <title>Video Description using Bidirectional Recurrent Neural Networks</title>
    <summary>  Although traditionally used in the machine translation field, the
encoder-decoder framework has been recently applied for the generation of video
and image descriptions. The combination of Convolutional and Recurrent Neural
Networks in these models has proven to outperform the previous state of the
art, obtaining more accurate video descriptions. In this work we propose
pushing further this model by introducing two contributions into the encoding
stage. First, producing richer image representations by combining object and
location information from Convolutional Neural Networks and second, introducing
Bidirectional Recurrent Neural Networks for capturing both forward and backward
temporal relationships in the input frames.
</summary>
    <author>
      <name>√Ålvaro Peris</name>
    </author>
    <author>
      <name>Marc Bola√±os</name>
    </author>
    <author>
      <name>Petia Radeva</name>
    </author>
    <author>
      <name>Francisco Casacuberta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures, 1 table, Submitted to International Conference on
  Artificial Neural Networks (ICANN)</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.03390v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03390v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05753v1</id>
    <updated>2016-04-19T21:22:29Z</updated>
    <published>2016-04-19T21:22:29Z</published>
    <title>Sketching and Neural Networks</title>
    <summary>  High-dimensional sparse data present computational and statistical challenges
for supervised learning. We propose compact linear sketches for reducing the
dimensionality of the input, followed by a single layer neural network. We show
that any sparse polynomial function can be computed, on nearly all sparse
binary vectors, by a single layer neural network that takes a compact sketch of
the vector as input. Consequently, when a set of sparse binary vectors is
approximately separable using a sparse polynomial, there exists a single-layer
neural network that takes a short sketch as input and correctly classifies
nearly all the points. Previous work has proposed using sketches to reduce
dimensionality while preserving the hypothesis class. However, the sketch size
has an exponential dependence on the degree in the case of polynomial
classifiers. In stark contrast, our approach of using improper learning, using
a larger hypothesis class allows the sketch size to have a logarithmic
dependence on the degree. Even in the linear case, our approach allows us to
improve on the pesky $O({1}/{{\gamma}^2})$ dependence of random projections, on
the margin $\gamma$. We empirically show that our approach leads to more
compact neural networks than related methods such as feature hashing at equal
or better performance.
</summary>
    <author>
      <name>Amit Daniely</name>
    </author>
    <author>
      <name>Nevena Lazic</name>
    </author>
    <author>
      <name>Yoram Singer</name>
    </author>
    <author>
      <name>Kunal Talwar</name>
    </author>
    <link href="http://arxiv.org/abs/1604.05753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.00079v1</id>
    <updated>2016-04-30T08:39:38Z</updated>
    <published>2016-04-30T08:39:38Z</published>
    <title>Constructive neural network learning</title>
    <summary>  In this paper, we aim at developing scalable neural network-type learning
systems. Motivated by the idea of "constructive neural networks" in
approximation theory, we focus on "constructing" rather than "training"
feed-forward neural networks (FNNs) for learning, and propose a novel FNNs
learning system called the constructive feed-forward neural network (CFN).
Theoretically, we prove that the proposed method not only overcomes the
classical saturation problem for FNN approximation, but also reaches the
optimal learning rate when the regression function is smooth, while the
state-of-the-art learning rates established for traditional FNNs are only near
optimal (up to a logarithmic factor). A series of numerical simulations are
provided to show the efficiency and feasibility of CFN via comparing with the
well-known regularized least squares (RLS) with Gaussian kernel and extreme
learning machine (ELM).
</summary>
    <author>
      <name>Shaobo Lin</name>
    </author>
    <author>
      <name>Jinshan Zeng</name>
    </author>
    <author>
      <name>Xiaoqin Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.00079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.00079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06925v4</id>
    <updated>2017-09-01T06:39:55Z</updated>
    <published>2016-05-23T07:54:46Z</published>
    <title>Neural Sampling by Irregular Gating Inhibition of Spiking Neurons and
  Attractor Networks</title>
    <summary>  A long tradition in theoretical neuroscience casts sensory processing in the
brain as the process of inferring the maximally consistent interpretations of
imperfect sensory input. Recently it has been shown that Gamma-band inhibition
can enable neural attractor networks to approximately carry out such a sampling
mechanism. In this paper we propose a novel neural network model based on
irregular gating inhibition, show analytically how it implements a Monte-Carlo
Markov Chain (MCMC) sampler, and describe how it can be used to model networks
of both neural attractors as well as of single spiking neurons. Finally we show
how this model applied to spiking neurons gives rise to a new putative
mechanism that could be used to implement stochastic synaptic weights in
biological neural networks and in neuromorphic hardware.
</summary>
    <author>
      <name>Lorenz K. Muller</name>
    </author>
    <author>
      <name>Giacomo Indiveri</name>
    </author>
    <link href="http://arxiv.org/abs/1605.06925v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06925v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00189v1</id>
    <updated>2016-06-01T09:31:00Z</updated>
    <published>2016-06-01T09:31:00Z</published>
    <title>Neural Network Translation Models for Grammatical Error Correction</title>
    <summary>  Phrase-based statistical machine translation (SMT) systems have previously
been used for the task of grammatical error correction (GEC) to achieve
state-of-the-art accuracy. The superiority of SMT systems comes from their
ability to learn text transformations from erroneous to corrected text, without
explicitly modeling error types. However, phrase-based SMT systems suffer from
limitations of discrete word representation, linear mapping, and lack of global
context. In this paper, we address these limitations by using two different yet
complementary neural network models, namely a neural network global lexicon
model and a neural network joint model. These neural networks can generalize
better by using continuous space representation of words and learn non-linear
mappings. Moreover, they can leverage contextual information from the source
sentence more effectively. By adding these two components, we achieve
statistically significant improvement in accuracy for grammatical error
correction over a state-of-the-art GEC system.
</summary>
    <author>
      <name>Shamil Chollampatt</name>
    </author>
    <author>
      <name>Kaveh Taghipour</name>
    </author>
    <author>
      <name>Hwee Tou Ng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for presentation at IJCAI-16</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.00189v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00189v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.00089v1</id>
    <updated>2016-10-01T05:25:31Z</updated>
    <published>2016-10-01T05:25:31Z</published>
    <title>System Identification of NN-based Model Reference Control of RUAV during
  Hover</title>
    <summary>  UAV control system is a huge and complex system, and to design and test a UAV
control system is time-cost and money-cost. This paper considered the
simulation of identification of a nonlinear system dynamics using artificial
neural networks approach. This experiment develops a neural network model of
the plant that we want to control. In the control design stage, experiment uses
the neural network plant model to design (or train) the controller. We use
Matlab to train the network and simulate the behavior. This chapter provides
the mathematical overview of MRC technique and neural network architecture to
simulate nonlinear identification of UAV systems. MRC provides a direct and
effective method to control a complex system without an equation-driven model.
NN approach provides a good framework to implement MEC by identifying
complicated models and training a controller for it.
</summary>
    <author>
      <name>Bhaskar Prasad Rimal</name>
    </author>
    <author>
      <name>Idris E. Putro</name>
    </author>
    <author>
      <name>Agus Budiyono</name>
    </author>
    <author>
      <name>Dugki Min</name>
    </author>
    <author>
      <name>Eunmi Choi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, Book Chapter, Artificial Neural Networks- Industrial and
  Control Engineering Applications, INTECH, April, 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.00089v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.00089v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.08136v1</id>
    <updated>2016-10-26T01:10:05Z</updated>
    <published>2016-10-26T01:10:05Z</published>
    <title>Learning to Match Using Local and Distributed Representations of Text
  for Web Search</title>
    <summary>  Models such as latent semantic analysis and those based on neural embeddings
learn distributed representations of text, and match the query against the
document in the latent semantic space. In traditional information retrieval
models, on the other hand, terms have discrete or local representations, and
the relevance of a document is determined by the exact matches of query terms
in the body text. We hypothesize that matching with distributed representations
complements matching with traditional local representations, and that a
combination of the two is favorable. We propose a novel document ranking model
composed of two separate deep neural networks, one that matches the query and
the document using a local representation, and another that matches the query
and the document using learned distributed representations. The two networks
are jointly trained as part of a single neural network. We show that this
combination or `duet' performs significantly better than either neural network
individually on a Web page ranking task, and also significantly outperforms
traditional baselines and other recently proposed models based on neural
networks.
</summary>
    <author>
      <name>Bhaskar Mitra</name>
    </author>
    <author>
      <name>Fernando Diaz</name>
    </author>
    <author>
      <name>Nick Craswell</name>
    </author>
    <link href="http://arxiv.org/abs/1610.08136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.08136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.09900v2</id>
    <updated>2017-03-02T17:11:01Z</updated>
    <published>2016-10-31T12:53:20Z</published>
    <title>Inference Compilation and Universal Probabilistic Programming</title>
    <summary>  We introduce a method for using deep neural networks to amortize the cost of
inference in models from the family induced by universal probabilistic
programming languages, establishing a framework that combines the strengths of
probabilistic programming and deep learning methods. We call what we do
"compilation of inference" because our method transforms a denotational
specification of an inference problem in the form of a probabilistic program
written in a universal programming language into a trained neural network
denoted in a neural network specification language. When at test time this
neural network is fed observational data and executed, it performs approximate
inference in the original model specified by the probabilistic program. Our
training objective and learning procedure are designed to allow the trained
neural network to be used as a proposal distribution in a sequential importance
sampling inference engine. We illustrate our method on mixture models and
Captcha solving and show significant speedups in the efficiency of inference.
</summary>
    <author>
      <name>Tuan Anh Le</name>
    </author>
    <author>
      <name>Atilim Gunes Baydin</name>
    </author>
    <author>
      <name>Frank Wood</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.09900v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.09900v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T37, 68T05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.06148v2</id>
    <updated>2017-05-24T12:26:43Z</updated>
    <published>2016-11-18T16:20:41Z</published>
    <title>Compacting Neural Network Classifiers via Dropout Training</title>
    <summary>  We introduce dropout compaction, a novel method for training feed-forward
neural networks which realizes the performance gains of training a large model
with dropout regularization, yet extracts a compact neural network for run-time
efficiency. In the proposed method, we introduce a sparsity-inducing prior on
the per unit dropout retention probability so that the optimizer can
effectively prune hidden units during training. By changing the prior
hyperparameters, we can control the size of the resulting network. We performed
a systematic comparison of dropout compaction and competing methods on several
real-world speech recognition tasks and found that dropout compaction achieved
comparable accuracy with fewer than 50% of the hidden units, translating to a
2.5x speedup in run-time.
</summary>
    <author>
      <name>Yotaro Kubo</name>
    </author>
    <author>
      <name>George Tucker</name>
    </author>
    <author>
      <name>Simon Wiesler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to AISTATS 2017 (Short-version is accepted to NIPS Workshop
  on Efficient Methods for Deep Neural Networks)</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.06148v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.06148v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.01727v1</id>
    <updated>2017-01-06T14:16:03Z</updated>
    <published>2017-01-06T14:16:03Z</published>
    <title>Open quantum generalisation of Hopfield neural networks</title>
    <summary>  We propose a new framework to understand how quantum effects may impact on
the dynamics of neural networks. We implement the dynamics of neural networks
in terms of Markovian open quantum systems, which allows us to treat thermal
and quantum coherent effects on the same footing. In particular, we propose an
open quantum generalisation of the celebrated Hopfield neural network, the
simplest toy model of associative memory. We determine its phase diagram and
show that quantum fluctuations give rise to a qualitatively new non-equilibrium
phase. This novel phase is characterised by limit cycles corresponding to
high-dimensional stationary manifolds that may be regarded as a generalisation
of storage patterns to the quantum domain.
</summary>
    <author>
      <name>P. Rotondo</name>
    </author>
    <author>
      <name>M. Marcuzzi</name>
    </author>
    <author>
      <name>J. P. Garrahan</name>
    </author>
    <author>
      <name>I. Lesanovsky</name>
    </author>
    <author>
      <name>M. Muller</name>
    </author>
    <link href="http://arxiv.org/abs/1701.01727v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.01727v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.06751v1</id>
    <updated>2017-01-24T07:07:15Z</updated>
    <published>2017-01-24T07:07:15Z</published>
    <title>Collective Vertex Classification Using Recursive Neural Network</title>
    <summary>  Collective classification of vertices is a task of assigning categories to
each vertex in a graph based on both vertex attributes and link structure.
Nevertheless, some existing approaches do not use the features of neighbouring
vertices properly, due to the noise introduced by these features. In this
paper, we propose a graph-based recursive neural network framework for
collective vertex classification. In this framework, we generate hidden
representations from both attributes of vertices and representations of
neighbouring vertices via recursive neural networks. Under this framework, we
explore two types of recursive neural units, naive recursive neural unit and
long short-term memory unit. We have conducted experiments on four real-world
network datasets. The experimental results show that our frame- work with long
short-term memory model achieves better results and outperforms several
competitive baseline methods.
</summary>
    <author>
      <name>Qiongkai Xu</name>
    </author>
    <author>
      <name>Qing Wang</name>
    </author>
    <author>
      <name>Chenchen Xu</name>
    </author>
    <author>
      <name>Lizhen Qu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.06751v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.06751v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00868v1</id>
    <updated>2017-03-02T17:43:19Z</updated>
    <published>2017-03-02T17:43:19Z</published>
    <title>Using Synthetic Data to Train Neural Networks is Model-Based Reasoning</title>
    <summary>  We draw a formal connection between using synthetic training data to optimize
neural network parameters and approximate, Bayesian, model-based reasoning. In
particular, training a neural network using synthetic data can be viewed as
learning a proposal distribution generator for approximate inference in the
synthetic-data generative model. We demonstrate this connection in a
recognition task where we develop a novel Captcha-breaking architecture and
train it using synthetic data, demonstrating both state-of-the-art performance
and a way of computing task-specific posterior uncertainty. Using a neural
network trained this way, we also demonstrate successful breaking of real-world
Captchas currently used by Facebook and Wikipedia. Reasoning from these
empirical results and drawing connections with Bayesian modeling, we discuss
the robustness of synthetic data results and suggest important considerations
for ensuring good neural network generalization when training with synthetic
data.
</summary>
    <author>
      <name>Tuan Anh Le</name>
    </author>
    <author>
      <name>Atilim Gunes Baydin</name>
    </author>
    <author>
      <name>Robert Zinkov</name>
    </author>
    <author>
      <name>Frank Wood</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.00868v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00868v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05, 68T10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.7.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.10951v1</id>
    <updated>2017-03-31T15:40:24Z</updated>
    <published>2017-03-31T15:40:24Z</published>
    <title>Comparison of multi-task convolutional neural network (MT-CNN) and a few
  other methods for toxicity prediction</title>
    <summary>  Toxicity analysis and prediction are of paramount importance to human health
and environmental protection. Existing computational methods are built from a
wide variety of descriptors and regressors, which makes their performance
analysis difficult. For example, deep neural network (DNN), a successful
approach in many occasions, acts like a black box and offers little conceptual
elegance or physical understanding. The present work constructs a common set of
microscopic descriptors based on established physical models for charges,
surface areas and free energies to assess the performance of multi-task
convolutional neural network (MT-CNN) architectures and a few other approaches,
including random forest (RF) and gradient boosting decision tree (GBDT), on an
equal footing. Comparison is also given to convolutional neural network (CNN)
and non-convolutional deep neural network (DNN) algorithms. Four benchmark
toxicity data sets (i.e., endpoints) are used to evaluate various approaches.
Extensive numerical studies indicate that the present MT-CNN architecture is
able to outperform the state-of-the-art methods.
</summary>
    <author>
      <name>Kedi Wu</name>
    </author>
    <author>
      <name>Guo-Wei Wei</name>
    </author>
    <link href="http://arxiv.org/abs/1703.10951v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.10951v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.04055v1</id>
    <updated>2017-04-13T09:47:12Z</updated>
    <published>2017-04-13T09:47:12Z</published>
    <title>Land Cover Classification via Multi-temporal Spatial Data by Recurrent
  Neural Networks</title>
    <summary>  Nowadays, modern earth observation programs produce huge volumes of satellite
images time series (SITS) that can be useful to monitor geographical areas
through time. How to efficiently analyze such kind of information is still an
open question in the remote sensing field. Recently, deep learning methods
proved suitable to deal with remote sensing data mainly for scene
classification (i.e. Convolutional Neural Networks - CNNs - on single images)
while only very few studies exist involving temporal deep learning approaches
(i.e Recurrent Neural Networks - RNNs) to deal with remote sensing time series.
In this letter we evaluate the ability of Recurrent Neural Networks, in
particular the Long-Short Term Memory (LSTM) model, to perform land cover
classification considering multi-temporal spatial data derived from a time
series of satellite images. We carried out experiments on two different
datasets considering both pixel-based and object-based classification. The
obtained results show that Recurrent Neural Networks are competitive compared
to state-of-the-art classifiers, and may outperform classical approaches in
presence of low represented and/or highly mixed classes. We also show that
using the alternative feature representation generated by LSTM can improve the
performances of standard classifiers.
</summary>
    <author>
      <name>Dino Ienco</name>
    </author>
    <author>
      <name>Raffaele Gaetano</name>
    </author>
    <author>
      <name>Claire Dupaquier</name>
    </author>
    <author>
      <name>Pierre Maurel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LGRS.2017.2728698</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LGRS.2017.2728698" rel="related"/>
    <link href="http://arxiv.org/abs/1704.04055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.04055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.00697v1</id>
    <updated>2017-05-01T20:23:13Z</updated>
    <published>2017-05-01T20:23:13Z</published>
    <title>From Imitation to Prediction, Data Compression vs Recurrent Neural
  Networks for Natural Language Processing</title>
    <summary>  In recent studies [1][13][12] Recurrent Neural Networks were used for
generative processes and their surprising performance can be explained by their
ability to create good predictions. In addition, data compression is also based
on predictions. What the problem comes down to is whether a data compressor
could be used to perform as well as recurrent neural networks in natural
language processing tasks. If this is possible,then the problem comes down to
determining if a compression algorithm is even more intelligent than a neural
network in specific tasks related to human language. In our journey we
discovered what we think is the fundamental difference between a Data
Compression Algorithm and a Recurrent Neural Network.
</summary>
    <author>
      <name>Juan Andr√©s Laura</name>
    </author>
    <author>
      <name>Gabriel Masi</name>
    </author>
    <author>
      <name>Luis Argerich</name>
    </author>
    <link href="http://arxiv.org/abs/1705.00697v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.00697v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07855v3</id>
    <updated>2018-01-17T16:21:26Z</updated>
    <published>2017-05-22T17:00:38Z</published>
    <title>Machine-learning-assisted correction of correlated qubit errors in a
  topological code</title>
    <summary>  A fault-tolerant quantum computation requires an efficient means to detect
and correct errors that accumulate in encoded quantum information. In the
context of machine learning, neural networks are a promising new approach to
quantum error correction. Here we show that a recurrent neural network can be
trained, using only experimentally accessible data, to detect errors in a
widely used topological code, the surface code, with a performance above that
of the established minimum-weight perfect matching (or blossom) decoder. The
performance gain is achieved because the neural network decoder can detect
correlations between bit-flip (X) and phase-flip (Z) errors. The machine
learning algorithm adapts to the physical system, hence no noise model is
needed. The long short-term memory layers of the recurrent neural network
maintain their performance over a large number of quantum error correction
cycles, making it a practical decoder for forthcoming experimental realizations
of the surface code.
</summary>
    <author>
      <name>P. Baireuther</name>
    </author>
    <author>
      <name>T. E. O'Brien</name>
    </author>
    <author>
      <name>B. Tarasinski</name>
    </author>
    <author>
      <name>C. W. J. Beenakker</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.22331/q-2018-01-29-48</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.22331/q-2018-01-29-48" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures; V3: version accepted by Quantum</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Quantum 2, 48 (2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1705.07855v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07855v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.08695v1</id>
    <updated>2017-05-24T10:52:19Z</updated>
    <published>2017-05-24T10:52:19Z</published>
    <title>Stochastic Sequential Neural Networks with Structured Inference</title>
    <summary>  Unsupervised structure learning in high-dimensional time series data has
attracted a lot of research interests. For example, segmenting and labelling
high dimensional time series can be helpful in behavior understanding and
medical diagnosis. Recent advances in generative sequential modeling have
suggested to combine recurrent neural networks with state space models (e.g.,
Hidden Markov Models). This combination can model not only the long term
dependency in sequential data, but also the uncertainty included in the hidden
states. Inheriting these advantages of stochastic neural sequential models, we
propose a structured and stochastic sequential neural network, which models
both the long-term dependencies via recurrent neural networks and the
uncertainty in the segmentation and labels via discrete random variables. For
accurate and efficient inference, we present a bi-directional inference network
by reparamterizing the categorical segmentation and labels with the recent
proposed Gumbel-Softmax approximation and resort to the Stochastic Gradient
Variational Bayes. We evaluate the proposed model in a number of tasks,
including speech modeling, automatic segmentation and labeling in behavior
understanding, and sequential multi-objects recognition. Experimental results
have demonstrated that our proposed model can achieve significant improvement
over the state-of-the-art methods.
</summary>
    <author>
      <name>Hao Liu</name>
    </author>
    <author>
      <name>Haoli Bai</name>
    </author>
    <author>
      <name>Lirong He</name>
    </author>
    <author>
      <name>Zenglin Xu</name>
    </author>
    <link href="http://arxiv.org/abs/1705.08695v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.08695v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07145v1</id>
    <updated>2017-06-22T01:25:37Z</updated>
    <published>2017-06-22T01:25:37Z</published>
    <title>Balanced Quantization: An Effective and Efficient Approach to Quantized
  Neural Networks</title>
    <summary>  Quantized Neural Networks (QNNs), which use low bitwidth numbers for
representing parameters and performing computations, have been proposed to
reduce the computation complexity, storage size and memory usage. In QNNs,
parameters and activations are uniformly quantized, such that the
multiplications and additions can be accelerated by bitwise operations.
However, distributions of parameters in Neural Networks are often imbalanced,
such that the uniform quantization determined from extremal values may under
utilize available bitwidth. In this paper, we propose a novel quantization
method that can ensure the balance of distributions of quantized values. Our
method first recursively partitions the parameters by percentiles into balanced
bins, and then applies uniform quantization. We also introduce computationally
cheaper approximations of percentiles to reduce the computation overhead
introduced. Overall, our method improves the prediction accuracies of QNNs
without introducing extra computation during inference, has negligible impact
on training speed, and is applicable to both Convolutional Neural Networks and
Recurrent Neural Networks. Experiments on standard datasets including ImageNet
and Penn Treebank confirm the effectiveness of our method. On ImageNet, the
top-5 error rate of our 4-bit quantized GoogLeNet model is 12.7\%, which is
superior to the state-of-the-arts of QNNs.
</summary>
    <author>
      <name>Shuchang Zhou</name>
    </author>
    <author>
      <name>Yuzhi Wang</name>
    </author>
    <author>
      <name>He Wen</name>
    </author>
    <author>
      <name>Qinyao He</name>
    </author>
    <author>
      <name>Yuheng Zou</name>
    </author>
    <link href="http://arxiv.org/abs/1706.07145v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07145v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.10268v1</id>
    <updated>2017-06-30T16:47:16Z</updated>
    <published>2017-06-30T16:47:16Z</published>
    <title>SafetyNets: Verifiable Execution of Deep Neural Networks on an Untrusted
  Cloud</title>
    <summary>  Inference using deep neural networks is often outsourced to the cloud since
it is a computationally demanding task. However, this raises a fundamental
issue of trust. How can a client be sure that the cloud has performed inference
correctly? A lazy cloud provider might use a simpler but less accurate model to
reduce its own computational load, or worse, maliciously modify the inference
results sent to the client. We propose SafetyNets, a framework that enables an
untrusted server (the cloud) to provide a client with a short mathematical
proof of the correctness of inference tasks that they perform on behalf of the
client. Specifically, SafetyNets develops and implements a specialized
interactive proof (IP) protocol for verifiable execution of a class of deep
neural networks, i.e., those that can be represented as arithmetic circuits.
Our empirical results on three- and four-layer deep neural networks demonstrate
the run-time costs of SafetyNets for both the client and server are low.
SafetyNets detects any incorrect computations of the neural network by the
untrusted server with high probability, while achieving state-of-the-art
accuracy on the MNIST digit recognition (99.4%) and TIMIT speech recognition
tasks (75.22%).
</summary>
    <author>
      <name>Zahra Ghodsi</name>
    </author>
    <author>
      <name>Tianyu Gu</name>
    </author>
    <author>
      <name>Siddharth Garg</name>
    </author>
    <link href="http://arxiv.org/abs/1706.10268v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.10268v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.00185v1</id>
    <updated>2017-08-01T07:14:36Z</updated>
    <published>2017-08-01T07:14:36Z</published>
    <title>Tensorial Recurrent Neural Networks for Longitudinal Data Analysis</title>
    <summary>  Traditional Recurrent Neural Networks assume vectorized data as inputs.
However many data from modern science and technology come in certain structures
such as tensorial time series data. To apply the recurrent neural networks for
this type of data, a vectorisation process is necessary, while such a
vectorisation leads to the loss of the precise information of the spatial or
longitudinal dimensions. In addition, such a vectorized data is not an optimum
solution for learning the representation of the longitudinal data. In this
paper, we propose a new variant of tensorial neural networks which directly
take tensorial time series data as inputs. We call this new variant as
Tensorial Recurrent Neural Network (TRNN). The proposed TRNN is based on tensor
Tucker decomposition.
</summary>
    <author>
      <name>Mingyuan Bai</name>
    </author>
    <author>
      <name>Boyan Zhang</name>
    </author>
    <author>
      <name>Junbin Gao</name>
    </author>
    <link href="http://arxiv.org/abs/1708.00185v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00185v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.06219v1</id>
    <updated>2017-08-21T13:46:15Z</updated>
    <published>2017-08-21T13:46:15Z</published>
    <title>On the approximation by single hidden layer feedforward neural networks
  with fixed weights</title>
    <summary>  Feedforward neural networks have wide applicability in various disciplines of
science due to their universal approximation property. Some authors have shown
that single hidden layer feedforward neural networks (SLFNs) with fixed weights
still possess the universal approximation property provided that approximated
functions are univariate. But this phenomenon does not lay any restrictions on
the number of neurons in the hidden layer. The more this number, the more the
probability of the considered network to give precise results. In this note, we
constructively prove that SLFNs with the fixed weight $1$ and two neurons in
the hidden layer can approximate any continuous function on a compact subset of
the real line. The applicability of this result is demonstrated in various
numerical examples. Finally, we show that SLFNs with fixed weights cannot
approximate all continuous multivariate functions.
</summary>
    <author>
      <name>Namig J. Guliyev</name>
    </author>
    <author>
      <name>Vugar E. Ismailov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neunet.2017.12.007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neunet.2017.12.007" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 5 figures, submitted; for associated SageMath worksheet,
  see https://sites.google.com/site/njguliyev/papers/monic-sigmoidal</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Networks, 98 (2018), 296-304</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1708.06219v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.06219v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="41A30, 41A63, 65D15, 68T05, 92B20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; F.1.1; I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.07863v1</id>
    <updated>2017-08-25T19:04:25Z</updated>
    <published>2017-08-25T19:04:25Z</published>
    <title>$k$-Nearest Neighbor Augmented Neural Networks for Text Classification</title>
    <summary>  In recent years, many deep-learning based models are proposed for text
classification. This kind of models well fits the training set from the
statistical point of view. However, it lacks the capacity of utilizing
instance-level information from individual instances in the training set. In
this work, we propose to enhance neural network models by allowing them to
leverage information from $k$-nearest neighbor (kNN) of the input text. Our
model employs a neural network that encodes texts into text embeddings.
Moreover, we also utilize $k$-nearest neighbor of the input text as an external
memory, and utilize it to capture instance-level information from the training
set. The final prediction is made based on features from both the neural
network encoder and the kNN memory. Experimental results on several standard
benchmark datasets show that our model outperforms the baseline model on all
the datasets, and it even beats a very deep neural network model (with 29
layers) in several datasets. Our model also shows superior performance when
training instances are scarce, and when the training set is severely
unbalanced. Our model also leverages techniques such as semi-supervised
training and transfer learning quite well.
</summary>
    <author>
      <name>Zhiguo Wang</name>
    </author>
    <author>
      <name>Wael Hamza</name>
    </author>
    <author>
      <name>Linfeng Song</name>
    </author>
    <link href="http://arxiv.org/abs/1708.07863v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.07863v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.06173v1</id>
    <updated>2017-09-18T21:37:55Z</updated>
    <published>2017-09-18T21:37:55Z</published>
    <title>Robustness of Neural Networks against Storage Media Errors</title>
    <summary>  We study the trade-offs between storage/bandwidth and prediction accuracy of
neural networks that are stored in noisy media. Conventionally, it is assumed
that all parameters (e.g., weight and biases) of a trained neural network are
stored as binary arrays and are error-free. This assumption is based upon the
implementation of error correction codes (ECCs) that correct potential bit
flips in storage media. However, ECCs add storage overhead and cause bandwidth
reduction when loading the trained parameters during the inference. We study
the robustness of deep neural networks when bit errors exist but ECCs are
turned off for different neural network models and datasets. It is observed
that more sophisticated models and datasets are more vulnerable to errors in
their trained parameters. We propose a simple detection approach that can
universally improve the robustness, which in some cases can be improved by
orders of magnitude. We also propose an alternative binary representation of
the parameters such that the distortion brought by bit flips is reduced and
even theoretically vanishing when the number of bits to represent a parameter
increases.
</summary>
    <author>
      <name>Minghai Qin</name>
    </author>
    <author>
      <name>Chao Sun</name>
    </author>
    <author>
      <name>Dejan Vucinic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.06173v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.06173v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.09130v1</id>
    <updated>2017-09-26T16:56:15Z</updated>
    <published>2017-09-26T16:56:15Z</published>
    <title>Output Range Analysis for Deep Neural Networks</title>
    <summary>  Deep neural networks (NN) are extensively used for machine learning tasks
such as image classification, perception and control of autonomous systems.
Increasingly, these deep NNs are also been deployed in high-assurance
applications. Thus, there is a pressing need for developing techniques to
verify neural networks to check whether certain user-expected properties are
satisfied. In this paper, we study a specific verification problem of computing
a guaranteed range for the output of a deep neural network given a set of
inputs represented as a convex polyhedron. Range estimation is a key primitive
for verifying deep NNs. We present an efficient range estimation algorithm that
uses a combination of local search and linear programming problems to
efficiently find the maximum and minimum values taken by the outputs of the NN
over the given input set. In contrast to recently proposed "monolithic"
optimization approaches, we use local gradient descent to repeatedly find and
eliminate local minima of the function. The final global optimum is certified
using a mixed integer programming instance. We implement our approach and
compare it with Reluplex, a recently proposed solver for deep neural networks.
We demonstrate the effectiveness of the proposed approach for verification of
NNs used in automated control as well as those used in classification.
</summary>
    <author>
      <name>Souradeep Dutta</name>
    </author>
    <author>
      <name>Susmit Jha</name>
    </author>
    <author>
      <name>Sriram Sanakaranarayanan</name>
    </author>
    <author>
      <name>Ashish Tiwari</name>
    </author>
    <link href="http://arxiv.org/abs/1709.09130v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.09130v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.00974v1</id>
    <updated>2017-10-03T03:56:33Z</updated>
    <published>2017-10-03T03:56:33Z</published>
    <title>A concatenating framework of shortcut convolutional neural networks</title>
    <summary>  It is well accepted that convolutional neural networks play an important role
in learning excellent features for image classification and recognition.
However, in tradition they only allow adjacent layers connected, limiting
integration of multi-scale information. To further improve their performance,
we present a concatenating framework of shortcut convolutional neural networks.
This framework can concatenate multi-scale features by shortcut connections to
the fully-connected layer that is directly fed to the output layer. We do a
large number of experiments to investigate performance of the shortcut
convolutional neural networks on many benchmark visual datasets for different
tasks. The datasets include AR, FERET, FaceScrub, CelebA for gender
classification, CUReT for texture classification, MNIST for digit recognition,
and CIFAR-10 for object recognition. Experimental results show that the
shortcut convolutional neural networks can achieve better results than the
traditional ones on these tasks, with more stability in different settings of
pooling schemes, activation functions, optimizations, initializations, kernel
numbers and kernel sizes.
</summary>
    <author>
      <name>Yujian Li</name>
    </author>
    <author>
      <name>Ting Zhang</name>
    </author>
    <author>
      <name>Zhaoying Liu</name>
    </author>
    <author>
      <name>Haihe Hu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 5 figures, 15 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.00974v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.00974v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.05520v1</id>
    <updated>2017-10-16T05:54:38Z</updated>
    <published>2017-10-16T05:54:38Z</published>
    <title>Entanglement Entropy of Target Functions for Image Classification and
  Convolutional Neural Network</title>
    <summary>  The success of deep convolutional neural network (CNN) in computer vision
especially image classification problems requests a new information theory for
function of image, instead of image itself. In this article, after establishing
a deep mathematical connection between image classification problem and quantum
spin model, we propose to use entanglement entropy, a generalization of
classical Boltzmann-Shannon entropy, as a powerful tool to characterize the
information needed for representation of general function of image. We prove
that there is a sub-volume-law bound for entanglement entropy of target
functions of reasonable image classification problems. Therefore target
functions of image classification only occupy a small subspace of the whole
Hilbert space. As a result, a neural network with polynomial number of
parameters is efficient for representation of such target functions of image.
The concept of entanglement entropy can also be useful to characterize the
expressive power of different neural networks. For example, we show that to
maintain the same expressive power, number of channels $D$ in a convolutional
neural network should scale with the number of convolution layers $n_c$ as
$D\sim D_0^{\frac{1}{n_c}}$. Therefore, deeper CNN with large $n_c$ is more
efficient than shallow ones.
</summary>
    <author>
      <name>Ya-Hui Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9pages, 1 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.05520v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.05520v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.str-el" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.06805v1</id>
    <updated>2017-10-18T15:57:41Z</updated>
    <published>2017-10-18T15:57:41Z</published>
    <title>Enhancing the Performance of Convolutional Neural Networks on Quality
  Degraded Datasets</title>
    <summary>  Despite the appeal of deep neural networks that largely replace the
traditional handmade filters, they still suffer from isolated cases that cannot
be properly handled only by the training of convolutional filters. Abnormal
factors, including real-world noise, blur, or other quality degradations, ruin
the output of a neural network. These unexpected problems can produce critical
complications, and it is surprising that there has only been minimal research
into the effects of noise in the deep neural network model. Therefore, we
present an exhaustive investigation into the effect of noise in image
classification and suggest a generalized architecture of a dual-channel model
to treat quality degraded input images. We compare the proposed dual-channel
model with a simple single model and show it improves the overall performance
of neural networks on various types of quality degraded input datasets.
</summary>
    <author>
      <name>Jonghwa Yim</name>
    </author>
    <author>
      <name>Kyung-Ah Sohn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The International Conference on Digital Image Computing: Techniques
  and Applications (DICTA), 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.06805v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.06805v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.10352v1</id>
    <updated>2017-10-27T22:43:08Z</updated>
    <published>2017-10-27T22:43:08Z</published>
    <title>Automated Design using Neural Networks and Gradient Descent</title>
    <summary>  We propose a novel method that makes use of deep neural networks and gradient
decent to perform automated design on complex real world engineering tasks. Our
approach works by training a neural network to mimic the fitness function of a
design optimization task and then, using the differential nature of the neural
network, perform gradient decent to maximize the fitness. We demonstrate this
methods effectiveness by designing an optimized heat sink and both 2D and 3D
airfoils that maximize the lift drag ratio under steady state flow conditions.
We highlight that our method has two distinct benefits over other automated
design approaches. First, evaluating the neural networks prediction of fitness
can be orders of magnitude faster then simulating the system of interest.
Second, using gradient decent allows the design space to be searched much more
efficiently then other gradient free methods. These two strengths work together
to overcome some of the current shortcomings of automated design.
</summary>
    <author>
      <name>Oliver Hennigh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.10352v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.10352v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.11473v1</id>
    <updated>2017-10-28T22:12:08Z</updated>
    <published>2017-10-28T22:12:08Z</published>
    <title>Multi-Resolution Fully Convolutional Neural Networks for Monaural Audio
  Source Separation</title>
    <summary>  In deep neural networks with convolutional layers, each layer typically has
fixed-size/single-resolution receptive field (RF). Convolutional layers with a
large RF capture global information from the input features, while layers with
small RF size capture local details with high resolution from the input
features. In this work, we introduce novel deep multi-resolution fully
convolutional neural networks (MR-FCNN), where each layer has different RF
sizes to extract multi-resolution features that capture the global and local
details information from its input features. The proposed MR-FCNN is applied to
separate a target audio source from a mixture of many audio sources.
Experimental results show that using MR-FCNN improves the performance compared
to feedforward deep neural networks (DNNs) and single resolution deep fully
convolutional neural networks (FCNNs) on the audio source separation problem.
</summary>
    <author>
      <name>Emad M. Grais</name>
    </author>
    <author>
      <name>Hagen Wierstorf</name>
    </author>
    <author>
      <name>Dominic Ward</name>
    </author>
    <author>
      <name>Mark D. Plumbley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1703.08019</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.11473v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.11473v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5; I.5; I.2.6; I.4.3; I.4; I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.09404v1</id>
    <updated>2017-11-26T15:20:46Z</updated>
    <published>2017-11-26T15:20:46Z</published>
    <title>Improving the Adversarial Robustness and Interpretability of Deep Neural
  Networks by Regularizing their Input Gradients</title>
    <summary>  Deep neural networks have proven remarkably effective at solving many
classification problems, but have been criticized recently for two major
weaknesses: the reasons behind their predictions are uninterpretable, and the
predictions themselves can often be fooled by small adversarial perturbations.
These problems pose major obstacles for the adoption of neural networks in
domains that require security or transparency. In this work, we evaluate the
effectiveness of defenses that differentiably penalize the degree to which
small changes in inputs can alter model predictions. Across multiple attacks,
architectures, defenses, and datasets, we find that neural networks trained
with this input gradient regularization exhibit robustness to transferred
adversarial examples generated to fool all of the other models. We also find
that adversarial examples generated to fool gradient-regularized models fool
all other models equally well, and actually lead to more "legitimate,"
interpretable misclassifications as rated by people (which we confirm in a
human subject experiment). Finally, we demonstrate that regularizing input
gradients makes them more naturally interpretable as rationales for model
predictions. We conclude by discussing this relationship between
interpretability and robustness in deep neural networks.
</summary>
    <author>
      <name>Andrew Slavin Ross</name>
    </author>
    <author>
      <name>Finale Doshi-Velez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in AAAI 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.09404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.09404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.01272v2</id>
    <updated>2018-02-10T02:00:51Z</updated>
    <published>2017-12-04T02:16:03Z</published>
    <title>Layer-wise Learning of Stochastic Neural Networks with Information
  Bottleneck</title>
    <summary>  In this paper, we present a layer-wise learning of stochastic neural networks
(SNNs) from an information-theoretic perspective. In each layer of a SNN,
compression and relevance are defined to quantify the amount of information
that the layer contains about the input space and the target space,
respectively. We propose a Parametric Information Bottleneck (PIB) framework to
jointly optimize the compression and relevance of all layers in a SNN for
better exploiting the neural network's representation. In PIB, we utilize the
model parameters explicitly to approximate those two measures. We show that PIB
can be considered to be an extension of the maximum likelihood estimate (MLE)
principle to every layer level. We empirically show that in the MNIST dataset,
as compared to the MLE principle, PIB : (i) improves the generalization of
neural networks in classification tasks, (ii) is more efficient in exploiting a
neural network's representation by quickly pushing it closer to the optimal
information-theoretical representation.
</summary>
    <author>
      <name>Thanh T. Nguyen</name>
    </author>
    <author>
      <name>Jaesik Choi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.01272v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.01272v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.10093v1</id>
    <updated>2017-12-29T01:59:14Z</updated>
    <published>2017-12-29T01:59:14Z</published>
    <title>Generation of Bose-Einstein Condensates' Ground State Through Machine
  Learning</title>
    <summary>  We show that both single-component and two-component Bose-Einstein
condensates' (BECs) ground states can be simulated by deep convolutional neural
networks of the same structure. We trained the neural network via inputting the
coupling strength in the dimensionless Gross-Pitaevskii equation (GPE) and
outputting the ground state wave-function. After training, the neural network
generates ground states faster than the method of imaginary time evolution,
while the relative mean-square-error between predicted states and original
states is in the magnitude between $10^{-5}$ and $10^{-4}$. We compared the
eigen-energies based on predicted states and original states, it is shown that
the neural network can predict eigen-energies in high precisions. Therefore,
the BEC ground states, which are continuous wave-functions, can be represented
by deep convolution neural networks.
</summary>
    <author>
      <name>Xiao Liang</name>
    </author>
    <author>
      <name>Sheng Liu</name>
    </author>
    <author>
      <name>Yan Li</name>
    </author>
    <author>
      <name>Yong-Sheng Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1712.10093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.10093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.4031v2</id>
    <updated>2010-09-08T02:13:27Z</updated>
    <published>2010-04-23T00:21:19Z</published>
    <title>Observed network dynamics from altering the balance between excitatory
  and inhibitory neurons in cultured networks</title>
    <summary>  Complexity in the temporal organization of neural systems may be a reflection
of the diversity of its neural constituents. These constituents, excitatory and
inhibitory neurons, comprise an invariant ratio in vivo and form the substrate
for rhythmic oscillatory activity. To begin to elucidate the dynamical
mechanisms that underlie this balance, we construct novel neural circuits not
ordinarily found in nature. We culture several networks of neurons composed of
excitatory and inhibitory cells and use a multi-electrode array to study their
temporal dynamics as the balance is modulated. We use the electrode burst as
the temporal imprimatur to signify the presence of network activity. Burst
durations, inter-burst intervals, and the number of spikes participating within
a burst are used to illustrate the vivid dynamical differences between the
various cultured networks. When the network consists largely of excitatory
neurons, no network temporal structure is apparent. However, the addition of
inhibitory neurons evokes a temporal order. Calculation of the temporal
autocorrelation shows that when the number of inhibitory neurons is a major
fraction of the network, a striking network pattern materializes when none was
previously present.
</summary>
    <author>
      <name>Xin Chen</name>
    </author>
    <author>
      <name>Rhonda Dzakpasu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.82.031907</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.82.031907" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys Rev E, 82, 031907 (2010)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.4031v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.4031v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.7128v2</id>
    <updated>2014-02-22T14:01:35Z</updated>
    <published>2013-11-27T20:56:18Z</published>
    <title>Structured chaos shapes spike-response noise entropy in balanced neural
  networks</title>
    <summary>  Large networks of sparsely coupled, excitatory and inhibitory cells occur
throughout the brain. A striking feature of these networks is that they are
chaotic. How does this chaos manifest in the neural code? Specifically, how
variable are the spike patterns that such a network produces in response to an
input signal? To answer this, we derive a bound for the entropy of multi-cell
spike pattern distributions in large recurrent networks of spiking neurons
responding to fluctuating inputs. The analysis is based on results from random
dynamical systems theory and is complimented by detailed numerical simulations.
We find that the spike pattern entropy is an order of magnitude lower than what
would be extrapolated from single cells. This holds despite the fact that
network coupling becomes vanishingly sparse as network size grows -- a
phenomenon that depends on ``extensive chaos," as previously discovered for
balanced networks without stimulus drive. Moreover, we show how spike pattern
entropy is controlled by temporal features of the inputs. Our findings provide
insight into how neural networks may encode stimuli in the presence of
inherently chaotic dynamics.
</summary>
    <author>
      <name>Guillaume Lajoie</name>
    </author>
    <author>
      <name>Jean-Philippe Thivierge</name>
    </author>
    <author>
      <name>Eric Shea-Brown</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.7128v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.7128v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92B20, 37H99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.4400v3</id>
    <updated>2014-03-04T05:15:42Z</updated>
    <published>2013-12-16T15:34:13Z</published>
    <title>Network In Network</title>
    <summary>  We propose a novel deep network structure called "Network In Network" (NIN)
to enhance model discriminability for local patches within the receptive field.
The conventional convolutional layer uses linear filters followed by a
nonlinear activation function to scan the input. Instead, we build micro neural
networks with more complex structures to abstract the data within the receptive
field. We instantiate the micro neural network with a multilayer perceptron,
which is a potent function approximator. The feature maps are obtained by
sliding the micro networks over the input in a similar manner as CNN; they are
then fed into the next layer. Deep NIN can be implemented by stacking mutiple
of the above described structure. With enhanced local modeling via the micro
network, we are able to utilize global average pooling over feature maps in the
classification layer, which is easier to interpret and less prone to
overfitting than traditional fully connected layers. We demonstrated the
state-of-the-art classification performances with NIN on CIFAR-10 and
CIFAR-100, and reasonable performances on SVHN and MNIST datasets.
</summary>
    <author>
      <name>Min Lin</name>
    </author>
    <author>
      <name>Qiang Chen</name>
    </author>
    <author>
      <name>Shuicheng Yan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures, for iclr2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.4400v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.4400v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.3121v2</id>
    <updated>2016-02-18T19:56:41Z</updated>
    <published>2014-12-09T21:12:19Z</published>
    <title>Multimodal Transfer Deep Learning with Applications in Audio-Visual
  Recognition</title>
    <summary>  We propose a transfer deep learning (TDL) framework that can transfer the
knowledge obtained from a single-modal neural network to a network with a
different modality. Specifically, we show that we can leverage speech data to
fine-tune the network trained for video recognition, given an initial set of
audio-video parallel dataset within the same semantics. Our approach first
learns the analogy-preserving embeddings between the abstract representations
learned from intermediate layers of each network, allowing for semantics-level
transfer between the source and target modalities. We then apply our neural
network operation that fine-tunes the target network with the additional
knowledge transferred from the source network, while keeping the topology of
the target network unchanged. While we present an audio-visual recognition task
as an application of our approach, our framework is flexible and thus can work
with any multimodal dataset, or with any already-existing deep networks that
share the common underlying semantics. In this work in progress report, we aim
to provide comprehensive results of different configurations of the proposed
approach on two widely used audio-visual datasets, and we discuss potential
applications of the proposed approach.
</summary>
    <author>
      <name>Seungwhan Moon</name>
    </author>
    <author>
      <name>Suyoun Kim</name>
    </author>
    <author>
      <name>Haohan Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, MMML workshop at NIPS 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.3121v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3121v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.06645v2</id>
    <updated>2017-09-15T09:53:08Z</updated>
    <published>2017-04-21T17:28:47Z</published>
    <title>Feed-forward approximations to dynamic recurrent network architectures</title>
    <summary>  Recurrent neural network architectures can have useful computational
properties, with complex temporal dynamics and input-sensitive attractor
states. However, evaluation of recurrent dynamic architectures requires
solution of systems of differential equations, and the number of evaluations
required to determine their response to a given input can vary with the input,
or can be indeterminate altogether in the case of oscillations or instability.
In feed-forward networks, by contrast, only a single pass through the network
is needed to determine the response to a given input. Modern machine-learning
systems are designed to operate efficiently on feed-forward architectures. We
hypothesised that two-layer feedforward architectures with simple,
deterministic dynamics could approximate the responses of single-layer
recurrent network architectures. By identifying the fixed-point responses of a
given recurrent network, we trained two-layer networks to directly approximate
the fixed-point response to a given input. These feed-forward networks then
embodied useful computations, including competitive interactions, information
transformations and noise rejection. Our approach was able to find useful
approximations to recurrent networks, which can then be evaluated in linear and
deterministic time complexity.
</summary>
    <author>
      <name>Dylan Richard Muir</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Author's final version, accepted for publication in Neural
  Computation</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.06645v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.06645v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.02084v1</id>
    <updated>2016-10-06T21:56:38Z</updated>
    <published>2016-10-06T21:56:38Z</published>
    <title>Computational Tradeoffs in Biological Neural Networks: Self-Stabilizing
  Winner-Take-All Networks</title>
    <summary>  We initiate a line of investigation into biological neural networks from an
algorithmic perspective. We develop a simplified but biologically plausible
model for distributed computation in stochastic spiking neural networks and
study tradeoffs between computation time and network complexity in this model.
Our aim is to abstract real neural networks in a way that, while not capturing
all interesting features, preserves high-level behavior and allows us to make
biologically relevant conclusions.
  In this paper, we focus on the important `winner-take-all' (WTA) problem,
which is analogous to a neural leader election unit: a network consisting of
$n$ input neurons and $n$ corresponding output neurons must converge to a state
in which a single output corresponding to a firing input (the `winner') fires,
while all other outputs remain silent. Neural circuits for WTA rely on
inhibitory neurons, which suppress the activity of competing outputs and drive
the network towards a converged state with a single firing winner. We attempt
to understand how the number of inhibitors used affects network convergence
time.
  We show that it is possible to significantly outperform naive WTA
constructions through a more refined use of inhibition, solving the problem in
$O(\theta)$ rounds in expectation with just $O(\log^{1/\theta} n)$ inhibitors
for any $\theta$. An alternative construction gives convergence in
$O(\log^{1/\theta} n)$ rounds with $O(\theta)$ inhibitors. We compliment these
upper bounds with our main technical contribution, a nearly matching lower
bound for networks using $\ge \log\log n$ inhibitors. Our lower bound uses
familiar indistinguishability and locality arguments from distributed computing
theory. It lets us derive a number of interesting conclusions about the
structure of any network solving WTA with good probability, and the use of
randomness and inhibition within such a network.
</summary>
    <author>
      <name>Nancy Lynch</name>
    </author>
    <author>
      <name>Cameron Musco</name>
    </author>
    <author>
      <name>Merav Parter</name>
    </author>
    <link href="http://arxiv.org/abs/1610.02084v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.02084v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0205405v1</id>
    <updated>2002-05-20T14:56:20Z</updated>
    <published>2002-05-20T14:56:20Z</published>
    <title>Assortative mixing in networks</title>
    <summary>  A network is said to show assortative mixing if the nodes in the network that
have many connections tend to be connected to other nodes with many
connections. We define a measure of assortative mixing for networks and use it
to show that social networks are often assortatively mixed, but that
technological and biological networks tend to be disassortative. We propose a
model of an assortative network, which we study both analytically and
numerically. Within the framework of this model we find that assortative
networks tend to percolate more easily than their disassortative counterparts
and that they are also more robust to vertex removal.
</summary>
    <author>
      <name>M. E. J. Newman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.89.208701</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.89.208701" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 table, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. Lett. 89, 208701 (2002)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0205405v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0205405v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0210686v1</id>
    <updated>2002-10-31T01:10:09Z</updated>
    <published>2002-10-31T01:10:09Z</published>
    <title>Smallest small-world network</title>
    <summary>  Efficiency in passage times is an important issue in designing networks, such
as transportation or computer networks. The small-world networks have
structures that yield high efficiency, while keeping the network highly
clustered. We show that among all networks with the small-world structure, the
most efficient ones have a single ``center'', from which all shortcuts are
connected to uniformly distributed nodes over the network. The networks with
several centers and a connected subnetwork of shortcuts are shown to be
``almost'' as efficient. Genetic-algorithm simulations further support our
results.
</summary>
    <author>
      <name>Takashi Nishikawa</name>
    </author>
    <author>
      <name>Adilson E. Motter</name>
    </author>
    <author>
      <name>Ying-Cheng Lai</name>
    </author>
    <author>
      <name>Frank C. Hoppensteadt</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.66.046139</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.66.046139" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 6 figures, REVTeX4</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 66, 046139 (2002)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0210686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0210686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0308455v1</id>
    <updated>2003-08-22T16:22:00Z</updated>
    <published>2003-08-22T16:22:00Z</published>
    <title>Individualistic networks and networking individuals in Tango Argentino</title>
    <summary>  We have performed a simulation study of the social network arising from
dancing partner selection in Tango Argentino. Tango Argentino is a famous
intellectual dance which combines individualistic behaviour with consensual
social rules, generating complex patterns of network behaviour in time. We
quantify these patterns by time-dependent degree distributions and clustering
coefficients for the network structure, and by monitoring the evolution of the
individual dancers skill. In particular, we have investigated how successful
new dancers are in entering the network under the promotion of established
network members. Our approach allows us to predict the success of mentoring in
a social network.
</summary>
    <author>
      <name>Celine Kuttler</name>
    </author>
    <author>
      <name>Ralf Blossey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 3 figues included, sociophysics</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0308455v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0308455v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.soft" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.soft" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0510607v1</id>
    <updated>2005-10-23T12:40:48Z</updated>
    <published>2005-10-23T12:40:48Z</published>
    <title>Sync in Complex Dynamical Networks: Stability, Evolution, Control, and
  Application</title>
    <summary>  In the past few years, the discoveries of small-world and scale-free
properties of many natural and artificial complex networks have stimulated
significant advances in better understanding the relationship between the
topology and the collective dynamics of complex networks. This paper reports
recent progresses in the literature of synchronization of complex dynamical
networks including stability criteria, network synchronizability and uniform
synchronous criticality in different topologies, and the connection between
control and synchronization of complex networks as well. The economic-cycle
synchronous phenomenon in the World Trade Web, a scale-free type of social
economic networks, is used to illustrate an application of the network
synchronization mechanism.
</summary>
    <author>
      <name>Xiang Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 13 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computational Cognition, 2005, vol. 3,
  no. 4, pp. 16-26</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0510607v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0510607v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0404019v1</id>
    <updated>2004-04-07T07:08:09Z</updated>
    <published>2004-04-07T07:08:09Z</published>
    <title>Optimizing genetic algorithm strategies for evolving networks</title>
    <summary>  This paper explores the use of genetic algorithms for the design of networks,
where the demands on the network fluctuate in time. For varying network
constraints, we find the best network using the standard genetic algorithm
operators such as inversion, mutation and crossover. We also examine how the
choice of genetic algorithm operators affects the quality of the best network
found. Such networks typically contain redundancy in servers, where several
servers perform the same task and pleiotropy, where servers perform multiple
tasks. We explore this trade-off between pleiotropy versus redundancy on the
cost versus reliability as a measure of the quality of the network.
</summary>
    <author>
      <name>Matthew J. Berryman</name>
    </author>
    <author>
      <name>Andrew Allison</name>
    </author>
    <author>
      <name>Derek Abbott</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1117/12.548122</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1117/12.548122" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0404019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0404019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6; C.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.4084v2</id>
    <updated>2008-04-05T18:49:42Z</updated>
    <published>2007-07-27T11:01:25Z</published>
    <title>Local structure of directed networks</title>
    <summary>  Previous work on undirected small-world networks established the paradigm
that locally structured networks tend to have high density of short loops. On
the other hand, many realistic networks are directed. Here we investigate the
local organization of directed networks and find, surprisingly, that real
networks often have very few short loops as compared to random models. We
develop a theory and derive conditions for determining if a given network has
more or less loops than its randomized counterpart. These findings carry broad
implications for structural and dynamical processes sustained by directed
networks.
</summary>
    <author>
      <name>Ginestra Bianconi</name>
    </author>
    <author>
      <name>Natali Gulbahce</name>
    </author>
    <author>
      <name>Adilson E. Motter</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.100.118701</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.100.118701" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. Lett. 100, 118701 (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0707.4084v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.4084v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.3204v1</id>
    <updated>2010-05-18T13:57:26Z</updated>
    <published>2010-05-18T13:57:26Z</published>
    <title>On the motifs distribution in random hierarchical networks</title>
    <summary>  The distribution of motifs in random hierarchical networks defined by
nonsymmetric random block--hierarchical adjacency matrices, is constructed for
the first time. According to the classification of U. Alon et al of network
superfamilies by their motifs distributions, our artificial directed random
hierarchical networks falls into the superfamily of natural networks to which
the class of neuron networks belongs. This is the first example of ``handmade''
networks with the motifs distribution as in a special class of natural networks
of essential biological importance.
</summary>
    <author>
      <name>V. A. Avetisov</name>
    </author>
    <author>
      <name>S. K. Nechaev</name>
    </author>
    <author>
      <name>A. B. Shkarin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physa.2010.09.016</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physa.2010.09.016" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1005.3204v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.3204v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.5868v2</id>
    <updated>2014-11-10T18:11:10Z</updated>
    <published>2014-05-22T19:41:51Z</published>
    <title>Learning to Generate Networks</title>
    <summary>  We investigate the problem of learning to generate complex networks from
data. Specifically, we consider whether deep belief networks, dependency
networks, and members of the exponential random graph family can learn to
generate networks whose complex behavior is consistent with a set of input
examples. We find that the deep model is able to capture the complex behavior
of small networks, but that no model is able capture this behavior for networks
with more than a handful of nodes.
</summary>
    <author>
      <name>James Atwood</name>
    </author>
    <author>
      <name>Don Towsley</name>
    </author>
    <author>
      <name>Krista Gile</name>
    </author>
    <author>
      <name>David Jensen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Information Processing Systems 2014 Workshop on Networks: From
  Graphs to Rich Data</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.5868v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.5868v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.06009v1</id>
    <updated>2018-01-18T13:45:29Z</updated>
    <published>2018-01-18T13:45:29Z</published>
    <title>Thermodynamics of network model fitting with spectral entropies</title>
    <summary>  An information theoretic approach inspired by quantum statistical mechanics
was recently proposed as a means to optimize network models and to assess their
likelihood against synthetic and real-world networks. Importantly, this method
does not rely on specific topological features or network descriptors, but
leverages entropy-based measures of network distance. Entertaining the analogy
with thermodynamics, we provide a physical interpretation of model
hyperparameters and propose analytical procedures for their estimate. These
results enable the practical application of this novel and powerful framework
to network model inference. We demonstrate this method in synthetic networks
endowed with a modular structure, and in real-world brain connectivity
networks.
</summary>
    <author>
      <name>Carlo Nicolini</name>
    </author>
    <author>
      <name>Vladimir Vlasov</name>
    </author>
    <author>
      <name>Angelo Bifone</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.06009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.06009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0506157v1</id>
    <updated>2005-06-06T23:07:12Z</updated>
    <published>2005-06-06T23:07:12Z</published>
    <title>Uniform synchronous criticality of diversely random complex networks</title>
    <summary>  We investigate collective synchronous behaviors in random complex networks of
limit-cycle oscillators with the non-identical asymmetric coupling scheme, and
find a uniform coupling criticality of collective synchronization which is
independent of complexity of network topologies. Numerically simulations on
categories of random complex networks have verified this conclusion.
</summary>
    <author>
      <name>Xiang Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physa.2005.06.041</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physa.2005.06.041" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physica A, 2006, vol. 360, pp.629-636</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0506157v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0506157v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0509255v1</id>
    <updated>2005-09-09T18:47:25Z</updated>
    <published>2005-09-09T18:47:25Z</published>
    <title>Synchronization in large directed networks of coupled phase oscillators</title>
    <summary>  We extend recent theoretical approximations describing the transition to
synchronization in large undirected networks of coupled phase oscillators to
the case of directed networks. We also consider extensions to networks with
mixed positive/negative coupling strengths. We compare our theory with
numerical simulations and find good agreement.
</summary>
    <author>
      <name>Juan G. Restrepo</name>
    </author>
    <author>
      <name>Edward Ott</name>
    </author>
    <author>
      <name>Brian R. Hunt</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/1.2148388</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/1.2148388" rel="related"/>
    <link href="http://arxiv.org/abs/cond-mat/0509255v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0509255v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.1615v1</id>
    <updated>2008-12-09T04:33:38Z</updated>
    <published>2008-12-09T04:33:38Z</published>
    <title>Missing Data using Decision Forest and Computational Intelligence</title>
    <summary>  Autoencoder neural network is implemented to estimate the missing data.
Genetic algorithm is implemented for network optimization and estimating the
missing data. Missing data is treated as Missing At Random mechanism by
implementing maximum likelihood algorithm. The network performance is
determined by calculating the mean square error of the network prediction. The
network is further optimized by implementing Decision Forest. The impact of
missing data is then investigated and decision forrests are found to improve
the results.
</summary>
    <author>
      <name>D. Moon</name>
    </author>
    <author>
      <name>T. Marwala</name>
    </author>
    <link href="http://arxiv.org/abs/0812.1615v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.1615v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0904.4002v1</id>
    <updated>2009-04-25T21:30:41Z</updated>
    <published>2009-04-25T21:30:41Z</published>
    <title>Frustration and collectivity in spatial networks</title>
    <summary>  In random networks decorated with Ising spins, an increase of the density of
frustrations reduces the transition temperature of the spin-glass ordering.
This result is in contradiction to the Bethe theory. Here we investigate if
this effect depends on the small-world property of the network. The results on
the specific heat and the spin susceptibility indicate that the effect appears
also in spatial networks.
</summary>
    <author>
      <name>Anna Manka-Krason</name>
    </author>
    <author>
      <name>Krzysztof Kulakowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0904.4002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0904.4002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02981v1</id>
    <updated>2016-09-10T00:46:40Z</updated>
    <published>2016-09-10T00:46:40Z</published>
    <title>Renormalization Group Transformation for Hamiltonian Dynamical Systems
  in Biological Networks</title>
    <summary>  We apply the renormalization group theory to the dynamical systems with the
simplest example of basic biological motifs. This includes the interpretation
of complex networks as the perturbation to simple network. This is the first
step to build our original framework to infer the properties of biological
networks, and the basis work to see its effectiveness to actual complex
systems.
</summary>
    <author>
      <name>Masamichi Sato</name>
    </author>
    <link href="http://arxiv.org/abs/1609.02981v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02981v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.3462v2</id>
    <updated>2011-02-18T09:42:36Z</updated>
    <published>2009-11-18T04:22:08Z</published>
    <title>A Markovian event-based framework for stochastic spiking neural networks</title>
    <summary>  In spiking neural networks, the information is conveyed by the spike times,
that depend on the intrinsic dynamics of each neuron, the input they receive
and on the connections between neurons. In this article we study the Markovian
nature of the sequence of spike times in stochastic neural networks, and in
particular the ability to deduce from a spike train the next spike time, and
therefore produce a description of the network activity only based on the spike
times regardless of the membrane potential process.
  To study this question in a rigorous manner, we introduce and study an
event-based description of networks of noisy integrate-and-fire neurons, i.e.
that is based on the computation of the spike times. We show that the firing
times of the neurons in the networks constitute a Markov chain, whose
transition probability is related to the probability distribution of the
interspike interval of the neurons in the network. In the cases where the
Markovian model can be developed, the transition probability is explicitly
derived in such classical cases of neural networks as the linear
integrate-and-fire neuron models with excitatory and inhibitory interactions,
for different types of synapses, possibly featuring noisy synaptic integration,
transmission delays and absolute and relative refractory period. This covers
most of the cases that have been investigated in the event-based description of
spiking deterministic neural networks.
</summary>
    <author>
      <name>Jonathan Touboul</name>
    </author>
    <author>
      <name>Olivier Faugeras</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10827-011-0327-y</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10827-011-0327-y" rel="related"/>
    <link href="http://arxiv.org/abs/0911.3462v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.3462v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.0514v4</id>
    <updated>2015-02-03T18:35:36Z</updated>
    <published>2013-06-03T17:36:14Z</published>
    <title>Riemannian metrics for neural networks II: recurrent networks and
  learning symbolic data sequences</title>
    <summary>  Recurrent neural networks are powerful models for sequential data, able to
represent complex dependencies in the sequence that simpler models such as
hidden Markov models cannot handle. Yet they are notoriously hard to train.
Here we introduce a training procedure using a gradient ascent in a Riemannian
metric: this produces an algorithm independent from design choices such as the
encoding of parameters and unit activities. This metric gradient ascent is
designed to have an algorithmic cost close to backpropagation through time for
sparsely connected networks. We use this procedure on gated leaky neural
networks (GLNNs), a variant of recurrent neural networks with an architecture
inspired by finite automata and an evolution equation inspired by
continuous-time networks. GLNNs trained with a Riemannian gradient are
demonstrated to effectively capture a variety of structures in synthetic
problems: basic block nesting as in context-free grammars (an important feature
of natural languages, but difficult to learn), intersections of multiple
independent Markov-type relations, or long-distance relationships such as the
distant-XOR problem. This method does not require adjusting the network
structure or initial parameters: the network used is a sparse random graph and
the initialization is identical for all problems considered.
</summary>
    <author>
      <name>Yann Ollivier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4th version: some changes in notation, more experiments</arxiv:comment>
    <link href="http://arxiv.org/abs/1306.0514v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.0514v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05, 68T10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.07600v1</id>
    <updated>2017-02-24T14:29:35Z</updated>
    <published>2017-02-24T14:29:35Z</published>
    <title>How hard is it to cross the room? -- Training (Recurrent) Neural
  Networks to steer a UAV</title>
    <summary>  This work explores the feasibility of steering a drone with a (recurrent)
neural network, based on input from a forward looking camera, in the context of
a high-level navigation task. We set up a generic framework for training a
network to perform navigation tasks based on imitation learning. It can be
applied to both aerial and land vehicles. As a proof of concept we apply it to
a UAV (Unmanned Aerial Vehicle) in a simulated environment, learning to cross a
room containing a number of obstacles. So far only feedforward neural networks
(FNNs) have been used to train UAV control. To cope with more complex tasks, we
propose the use of recurrent neural networks (RNN) instead and successfully
train an LSTM (Long-Short Term Memory) network for controlling UAVs. Vision
based control is a sequential prediction problem, known for its highly
correlated input data. The correlation makes training a network hard,
especially an RNN. To overcome this issue, we investigate an alternative
sampling method during training, namely window-wise truncated backpropagation
through time (WW-TBPTT). Further, end-to-end training requires a lot of data
which often is not available. Therefore, we compare the performance of
retraining only the Fully Connected (FC) and LSTM control layers with networks
which are trained end-to-end. Performing the relatively simple task of crossing
a room already reveals important guidelines and good practices for training
neural control networks. Different visualizations help to explain the behavior
learned.
</summary>
    <author>
      <name>Klaas Kelchtermans</name>
    </author>
    <author>
      <name>Tinne Tuytelaars</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 30 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.07600v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.07600v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/astro-ph/9906098v1</id>
    <updated>1999-06-07T10:08:01Z</updated>
    <published>1999-06-07T10:08:01Z</published>
    <title>Hybrid Neural Networks for Frequency Estimation of Unevenly Sampled Data</title>
    <summary>  In this paper we present a hybrid system composed by a neural network based
estimator system and genetic algorithms. It uses an unsupervised Hebbian
nonlinear neural algorithm to extract the principal components which, in turn,
are used by the MUSIC frequency estimator algorithm to extract the frequencies.
We generalize this method to avoid an interpolation preprocessing step and to
improve the performance by using a new stop criterion to avoid overfitting.
Furthermore, genetic algorithms are used to optimize the neural net weight
initialization. The experimental results are obtained comparing our methodology
with the others known in literature on a Cepheid star light curve.
</summary>
    <author>
      <name>R. Tagliaferri</name>
    </author>
    <author>
      <name>A. Ciaramella</name>
    </author>
    <author>
      <name>L. Milano</name>
    </author>
    <author>
      <name>F. Barone</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IJCNN.1999.831086</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IJCNN.1999.831086" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, to appear in the proceedings of IJCNN 99, IEEE Press, 1999</arxiv:comment>
    <link href="http://arxiv.org/abs/astro-ph/9906098v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/astro-ph/9906098v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9902123v1</id>
    <updated>1999-02-09T18:02:36Z</updated>
    <published>1999-02-09T18:02:36Z</published>
    <title>Energy Functional and Fixed Points of a Neural Networks</title>
    <summary>  It turned out that the set of the fixed points is not necessarily the same as
the set of the local minima of the energy functional. It depends on the
diagonal elements of the connection matrix. The simple method which allows to
cut off fictitious fixed points with high energies was found out. Especially
the method is effective if the connection matrix is the projection one.
</summary>
    <author>
      <name>Leonid B. Litinskii</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Institute for High Pressure Physics Russian Academy of Sciences</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">RevTEX, 6 pages, one old paper</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In "Neural Nets. WIRN-VIETRI-97. Proceedings of the 9th Italian
  Workshop on Neural Nets, Vietri sul Mare, Salerno, Italy, 22-24 May 1997",
  Eds. Maria Marinaro and Roberto Tagliaferri, Springer-Verlag, 1998</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/9902123v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9902123v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0006486v1</id>
    <updated>2000-06-30T10:24:10Z</updated>
    <published>2000-06-30T10:24:10Z</published>
    <title>Forecasting price increments using an artificial Neural Network</title>
    <summary>  Financial forecasting is a difficult task due to the intrinsic complexity of
the financial system. In the present paper we relate our experience using
neural nets as financial time series forecast method. In particular we show
that a neural net able to forecast the sign of the price increments with a
success rate slightly above 50 percent can be found.
</summary>
    <author>
      <name>Filippo Castiglione</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures (to be published in Advances in Complex Systems,
  as the Proceeding of the WE-Heraeus Workshop on "Economic Dynamics from the
  Physics Point of View", Bad Honnef, Germany, March 27-30, 2000</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0006486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0006486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0105319v1</id>
    <updated>2001-05-16T14:08:57Z</updated>
    <published>2001-05-16T14:08:57Z</published>
    <title>An Information-Based Neural Approach to Constraint Satisfaction</title>
    <summary>  A novel artificial neural network approach to constraint satisfaction
problems is presented. Based on information-theoretical considerations, it
differs from a conventional mean-field approach in the form of the resulting
free energy. The method, implemented as an annealing algorithm, is numerically
explored on a testbed of K-SAT problems. The performance shows a dramatic
improvement to that of a conventional mean-field approach, and is comparable to
that of a state-of-the-art dedicated heuristic (Gsat+Walk). The real strength
of the method, however, lies in its generality -- with minor modifications it
is applicable to arbitrary types of discrete constraint satisfaction problems.
</summary>
    <author>
      <name>Henrik Jonsson</name>
    </author>
    <author>
      <name>Bo Soderberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 3 figures,(to appear in Neural Computation)</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0105319v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0105319v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0311607v2</id>
    <updated>2004-07-15T08:16:22Z</updated>
    <published>2003-11-26T16:39:49Z</published>
    <title>Neural cryptography with feedback</title>
    <summary>  Neural cryptography is based on a competition between attractive and
repulsive stochastic forces. A feedback mechanism is added to neural
cryptography which increases the repulsive forces. Using numerical simulations
and an analytic approach, the probability of a successful attack is calculated
for different model parameters. Scaling laws are derived which show that
feedback improves the security of the system. In addition, a network with
feedback generates a pseudorandom bit sequence which can be used to encrypt and
decrypt a secret message.
</summary>
    <author>
      <name>Andreas Ruttor</name>
    </author>
    <author>
      <name>Wolfgang Kinzel</name>
    </author>
    <author>
      <name>Lanir Shacham</name>
    </author>
    <author>
      <name>Ido Kanter</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.69.046110</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.69.046110" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 10 figures; abstract changed, references updated</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 69, 046110 (2004)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0311607v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0311607v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0411374v2</id>
    <updated>2005-03-01T14:39:02Z</updated>
    <published>2004-11-15T13:15:47Z</published>
    <title>Neural cryptography with queries</title>
    <summary>  Neural cryptography is based on synchronization of tree parity machines by
mutual learning. We extend previous key-exchange protocols by replacing random
inputs with queries depending on the current state of the neural networks. The
probability of a successful attack is calculated for different model parameters
using numerical simulations. The results show that queries restore the security
against cooperating attackers. The success probability can be reduced without
increasing the average synchronization time.
</summary>
    <author>
      <name>Andreas Ruttor</name>
    </author>
    <author>
      <name>Wolfgang Kinzel</name>
    </author>
    <author>
      <name>Ido Kanter</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1742-5468/2005/01/P01009</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1742-5468/2005/01/P01009" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 8 figures; typos corrected</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J. Stat. Mech. (2005) P01009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0411374v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0411374v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.4290v1</id>
    <updated>2008-05-28T09:16:44Z</updated>
    <published>2008-05-28T09:16:44Z</published>
    <title>From Data Topology to a Modular Classifier</title>
    <summary>  This article describes an approach to designing a distributed and modular
neural classifier. This approach introduces a new hierarchical clustering that
enables one to determine reliable regions in the representation space by
exploiting supervised information. A multilayer perceptron is then associated
with each of these detected clusters and charged with recognizing elements of
the associated cluster while rejecting all others. The obtained global
classifier is comprised of a set of cooperating neural networks and completed
by a K-nearest neighbor classifier charged with treating elements rejected by
all the neural networks. Experimental results for the handwritten digit
recognition problem and comparison with neural and statistical nonmodular
classifiers are given.
</summary>
    <author>
      <name>Abdel Ennaji</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>Arnaud Ribert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>Yves Lecourtier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10032-002-0095-3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10032-002-0095-3" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal On Document Analysis and Recognition 6, 1
  (2003) 1-9</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0805.4290v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.4290v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.2249v1</id>
    <updated>2012-02-10T12:57:34Z</updated>
    <published>2012-02-10T12:57:34Z</published>
    <title>Supervised Learning in Multilayer Spiking Neural Networks</title>
    <summary>  The current article introduces a supervised learning algorithm for multilayer
spiking neural networks. The algorithm presented here overcomes some
limitations of existing learning algorithms as it can be applied to neurons
firing multiple spikes and it can in principle be applied to any linearisable
neuron model. The algorithm is applied successfully to various benchmarks, such
as the XOR problem and the Iris data set, as well as complex classifications
problems. The simulations also show the flexibility of this supervised learning
algorithm which permits different encodings of the spike timing patterns,
including precise spike trains encoding.
</summary>
    <author>
      <name>Ioana Sporea</name>
    </author>
    <author>
      <name>Andr√© Gr√ºning</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/NECO_a_00396</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/NECO_a_00396" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Compuation February 2013, Vol. 25, No. 2, Pages 473-509</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1202.2249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.2249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.4316v1</id>
    <updated>2014-04-16T17:23:47Z</updated>
    <published>2014-04-16T17:23:47Z</published>
    <title>Generic Object Detection With Dense Neural Patterns and Regionlets</title>
    <summary>  This paper addresses the challenge of establishing a bridge between deep
convolutional neural networks and conventional object detection frameworks for
accurate and efficient generic object detection. We introduce Dense Neural
Patterns, short for DNPs, which are dense local features derived from
discriminatively trained deep convolutional neural networks. DNPs can be easily
plugged into conventional detection frameworks in the same way as other dense
local features(like HOG or LBP). The effectiveness of the proposed approach is
demonstrated with the Regionlets object detection framework. It achieved 46.1%
mean average precision on the PASCAL VOC 2007 dataset, and 44.1% on the PASCAL
VOC 2010 dataset, which dramatically improves the original Regionlets approach
without DNPs.
</summary>
    <author>
      <name>Will Y. Zou</name>
    </author>
    <author>
      <name>Xiaoyu Wang</name>
    </author>
    <author>
      <name>Miao Sun</name>
    </author>
    <author>
      <name>Yuanqing Lin</name>
    </author>
    <link href="http://arxiv.org/abs/1404.4316v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.4316v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.8191v1</id>
    <updated>2014-09-29T17:08:21Z</updated>
    <published>2014-09-29T17:08:21Z</published>
    <title>A Neural Networks Committee for the Contextual Bandit Problem</title>
    <summary>  This paper presents a new contextual bandit algorithm, NeuralBandit, which
does not need hypothesis on stationarity of contexts and rewards. Several
neural networks are trained to modelize the value of rewards knowing the
context. Two variants, based on multi-experts approach, are proposed to choose
online the parameters of multi-layer perceptrons. The proposed algorithms are
successfully tested on a large dataset with and without stationarity of
rewards.
</summary>
    <author>
      <name>Robin Allesiardo</name>
    </author>
    <author>
      <name>Raphael Feraud</name>
    </author>
    <author>
      <name>Djallel Bouneffouf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21st International Conference on Neural Information Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.8191v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.8191v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.03721v1</id>
    <updated>2015-08-15T11:16:39Z</updated>
    <published>2015-08-15T11:16:39Z</published>
    <title>A Comparative Study on Regularization Strategies for Embedding-based
  Neural Networks</title>
    <summary>  This paper aims to compare different regularization strategies to address a
common phenomenon, severe overfitting, in embedding-based neural networks for
NLP. We chose two widely studied neural models and tasks as our testbed. We
tried several frequently applied or newly proposed regularization strategies,
including penalizing weights (embeddings excluded), penalizing embeddings,
re-embedding words, and dropout. We also emphasized on incremental
hyperparameter tuning, and combining different regularizations. The results
provide a picture on tuning hyperparameters for neural NLP models.
</summary>
    <author>
      <name>Hao Peng</name>
    </author>
    <author>
      <name>Lili Mou</name>
    </author>
    <author>
      <name>Ge Li</name>
    </author>
    <author>
      <name>Yunchuan Chen</name>
    </author>
    <author>
      <name>Yangyang Lu</name>
    </author>
    <author>
      <name>Zhi Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP '15</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.03721v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.03721v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00726v1</id>
    <updated>2017-03-02T11:26:56Z</updated>
    <published>2017-03-02T11:26:56Z</published>
    <title>Grayscale Image Authentication using Neural Hashing</title>
    <summary>  Many different approaches for neural network based hash functions have been
proposed. Statistical analysis must correlate security of them. This paper
proposes novel neural hashing approach for gray scale image authentication. The
suggested system is rapid, robust, useful and secure. Proposed hash function
generates hash values using neural network one-way property and non-linear
techniques. As a result security and performance analysis are performed and
satisfying results are achieved. These features are dominant reasons for
preferring against traditional ones.
</summary>
    <author>
      <name>Yakup Kutlu</name>
    </author>
    <author>
      <name>Apdullah Yayƒ±k</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">international journal of Natural and Engineering Sciences
  (NESciences.com) : Image Authentication, Cryptology, Hash Function,
  Statistical and Security Analysis</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.00726v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00726v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.05744v2</id>
    <updated>2017-11-04T01:20:06Z</updated>
    <published>2017-06-18T23:20:12Z</published>
    <title>Learning Hierarchical Information Flow with Recurrent Neural Modules</title>
    <summary>  We propose ThalNet, a deep learning model inspired by neocortical
communication via the thalamus. Our model consists of recurrent neural modules
that send features through a routing center, endowing the modules with the
flexibility to share features over multiple time steps. We show that our model
learns to route information hierarchically, processing input data by a chain of
modules. We observe common architectures, such as feed forward neural networks
and skip connections, emerging as special cases of our architecture, while
novel connectivity patterns are learned for the text8 compression task. Our
model outperforms standard recurrent neural networks on several sequential
benchmarks.
</summary>
    <author>
      <name>Danijar Hafner</name>
    </author>
    <author>
      <name>Alex Irpan</name>
    </author>
    <author>
      <name>James Davidson</name>
    </author>
    <author>
      <name>Nicolas Heess</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NIPS 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.05744v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.05744v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07351v1</id>
    <updated>2017-06-22T14:59:49Z</updated>
    <published>2017-06-22T14:59:49Z</published>
    <title>An approach to reachability analysis for feed-forward ReLU neural
  networks</title>
    <summary>  We study the reachability problem for systems implemented as feed-forward
neural networks whose activation function is implemented via ReLU functions. We
draw a correspondence between establishing whether some arbitrary output can
ever be outputed by a neural system and linear problems characterising a neural
system of interest. We present a methodology to solve cases of practical
interest by means of a state-of-the-art linear programs solver. We evaluate the
technique presented by discussing the experimental results obtained by
analysing reachability properties for a number of benchmarks in the literature.
</summary>
    <author>
      <name>Alessio Lomuscio</name>
    </author>
    <author>
      <name>Lalit Maganti</name>
    </author>
    <link href="http://arxiv.org/abs/1706.07351v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07351v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.1182v1</id>
    <updated>2009-07-07T20:12:01Z</updated>
    <published>2009-07-07T20:12:01Z</published>
    <title>Catastrophic cascade of failures in interdependent networks</title>
    <summary>  Many systems, ranging from engineering to medical to societal, can only be
properly characterized by multiple interdependent networks whose normal
functioning depends on one another. Failure of a fraction of nodes in one
network may lead to a failure in another network. This in turn may cause
further malfunction of additional nodes in the first network and so on. Such a
cascade of failures, triggered by a failure of a small faction of nodes in only
one network, may lead to the complete fragmentation of all networks. We
introduce a model and an analytical framework for studying interdependent
networks. We obtain interesting and surprising results that should
significantly effect the design of robust real-world networks. For two
interdependent Erdos-Renyi (ER) networks, we find that the critical average
degree below which both networks collapse is &lt;k_c&gt;=2.445, compared to &lt;k_c&gt;=1
for a single ER network. Furthermore, while for a single network a broader
degree distribution of the network nodes results in higher robustness to random
failure, for interdependent networks, the broader the distribution is, the more
vulnerable the networks become to random failure.
</summary>
    <author>
      <name>Sergey V. Buldyrev</name>
    </author>
    <author>
      <name>Roni Parshani</name>
    </author>
    <author>
      <name>Gerald Paul</name>
    </author>
    <author>
      <name>H. Eugene Stanley</name>
    </author>
    <author>
      <name>Shlomo Havlin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/nature08932</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/nature08932" rel="related"/>
    <link href="http://arxiv.org/abs/0907.1182v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.1182v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.0303v3</id>
    <updated>2009-05-20T13:00:20Z</updated>
    <published>2007-09-03T18:57:26Z</published>
    <title>Navigability of Complex Networks</title>
    <summary>  Routing information through networks is a universal phenomenon in both
natural and manmade complex systems. When each node has full knowledge of the
global network connectivity, finding short communication paths is merely a
matter of distributed computation. However, in many real networks nodes
communicate efficiently even without such global intelligence. Here we show
that the peculiar structural characteristics of many complex networks support
efficient communication without global knowledge. We also describe a general
mechanism that explains this connection between network structure and function.
This mechanism relies on the presence of a metric space hidden behind an
observable network. Our findings suggest that real networks in nature have
underlying metric spaces that remain undiscovered. Their discovery would have
practical applications ranging from routing in the Internet and searching
social networks, to studying information flows in neural, gene regulatory
networks, or signaling pathways.
</summary>
    <author>
      <name>Marian Boguna</name>
    </author>
    <author>
      <name>Dmitri Krioukov</name>
    </author>
    <author>
      <name>kc claffy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/NPHYS1130</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/NPHYS1130" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Nature Physics 5, 74-80 (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0709.0303v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.0303v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.2508v1</id>
    <updated>2008-02-18T16:23:22Z</updated>
    <published>2008-02-18T16:23:22Z</published>
    <title>Criticality of spreading dynamics in hierarchical cluster networks
  without inhibition</title>
    <summary>  An essential requirement for the representation of functional patterns in
complex neural networks, such as the mammalian cerebral cortex, is the
existence of stable network activations within a limited critical range. In
this range, the activity of neural populations in the network persists between
the extremes of quickly dying out, or activating the whole network. The nerve
fiber network of the mammalian cerebral cortex possesses a modular organization
extending across several levels of organization. Using a basic spreading model
without inhibition, we investigated how functional activations of nodes
propagate through such a hierarchically clustered network. The simulations
demonstrated that persistent and scalable activation could be produced in
clustered networks, but not in random networks of the same size. Moreover, the
parameter range yielding critical activations was substantially larger in
hierarchical cluster networks than in small-world networks of the same size.
These findings indicate that a hierarchical cluster architecture may provide
the structural basis for the stable and diverse functional patterns observed in
cortical networks.
</summary>
    <author>
      <name>Marcus Kaiser</name>
    </author>
    <author>
      <name>Matthias Goerner</name>
    </author>
    <author>
      <name>Claus C. Hilgetag</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1367-2630/9/5/110</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1367-2630/9/5/110" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">New Journal of Physics, 9:110 (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0802.2508v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.2508v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05347v1</id>
    <updated>2016-12-16T03:32:43Z</updated>
    <published>2016-12-16T03:32:43Z</published>
    <title>Feedback arcs and node hierarchy in directed networks</title>
    <summary>  Directed networks such as gene regulation networks and neural networks are
connected by arcs (directed links). The nodes in a directed network are often
strongly interwound by a huge number of directed cycles, which lead to complex
information-processing dynamics in the network and make it highly challenging
to infer the intrinsic direction of information flow. In this theoretical
paper, based on the principle of minimum-feedback, we explore the node
hierarchy of directed networks and distinguish feedforward and feedback arcs.
Nearly optimal node hierarchy solutions, which minimize the number of feedback
arcs from lower-level nodes to higher-level nodes, are constructed by
belief-propagation and simulated-annealing methods. For real-world networks, we
quantify the extent of feedback scarcity by comparison with the ensemble of
direction-randomized networks and identify the most important feedback arcs.
Our methods are also useful for visualizing directed networks.
</summary>
    <author>
      <name>Jin-Hua Zhao</name>
    </author>
    <author>
      <name>Hai-Jun Zhou</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1674-1056/26/7/078901</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1674-1056/26/7/078901" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages (main text and appendices, including 9 figures). This paper
  is related to and is an expansion of our previous post arXiv:1605.09257</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.05347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.07122v1</id>
    <updated>2017-03-21T10:10:56Z</updated>
    <published>2017-03-21T10:10:56Z</published>
    <title>Evolving Parsimonious Networks by Mixing Activation Functions</title>
    <summary>  Neuroevolution methods evolve the weights of a neural network, and in some
cases the topology, but little work has been done to analyze the effect of
evolving the activation functions of individual nodes on network size, which is
important when training networks with a small number of samples. In this work
we extend the neuroevolution algorithm NEAT to evolve the activation function
of neurons in addition to the topology and weights of the network. The size and
performance of networks produced using NEAT with uniform activation in all
nodes, or homogenous networks, is compared to networks which contain a mixture
of activation functions, or heterogenous networks. For a number of regression
and classification benchmarks it is shown that, (1) qualitatively different
activation functions lead to different results in homogeneous networks, (2) the
heterogeneous version of NEAT is able to select well performing activation
functions, (3) producing heterogeneous networks that are significantly smaller
than homogeneous networks.
</summary>
    <author>
      <name>Alexander Hagg</name>
    </author>
    <author>
      <name>Maximilian Mensing</name>
    </author>
    <author>
      <name>Alexander Asteroth</name>
    </author>
    <link href="http://arxiv.org/abs/1703.07122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.07122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.04111v1</id>
    <updated>2017-09-13T02:18:39Z</updated>
    <published>2017-09-13T02:18:39Z</published>
    <title>Meta Networks for Neural Style Transfer</title>
    <summary>  In this paper we propose a new method to get the specified network parameters
through one time feed-forward propagation of the meta networks and explore the
application to neural style transfer. Recent works on style transfer typically
need to train image transformation networks for every new style, and the style
is encoded in the network parameters by enormous iterations of stochastic
gradient descent. To tackle these issues, we build a meta network which takes
in the style image and produces a corresponding image transformations network
directly. Compared with optimization-based methods for every style, our meta
networks can handle an arbitrary new style within $19ms$ seconds on one modern
GPU card. The fast image transformation network generated by our meta network
is only 449KB, which is capable of real-time executing on a mobile device. We
also investigate the manifold of the style transfer networks by operating the
hidden features from meta networks. Experiments have well validated the
effectiveness of our method. Code and trained models has been released
https://github.com/FalongShen/styletransfer.
</summary>
    <author>
      <name>Falong Shen</name>
    </author>
    <author>
      <name>Shuicheng Yan</name>
    </author>
    <author>
      <name>Gang Zeng</name>
    </author>
    <link href="http://arxiv.org/abs/1709.04111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.04111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.4021v3</id>
    <updated>2010-07-26T02:12:06Z</updated>
    <published>2010-05-21T17:33:44Z</published>
    <title>Software Effort Estimation using Radial Basis and Generalized Regression
  Neural Networks</title>
    <summary>  Software development effort estimation is one of the most major activities in
software project management. A number of models have been proposed to construct
a relationship between software size and effort; however we still have problems
for effort estimation. This is because project data, available in the initial
stages of project is often incomplete, inconsistent, uncertain and unclear. The
need for accurate effort estimation in software industry is still a challenge.
Artificial Neural Network models are more suitable in such situations. The
present paper is concerned with developing software effort estimation models
based on artificial neural networks. The models are designed to improve the
performance of the network that suits to the COCOMO Model. Artificial Neural
Network models are created using Radial Basis and Generalized Regression. A
case study based on the COCOMO81 database compares the proposed neural network
models with the Intermediate COCOMO. The results were analyzed using five
different criterions MMRE, MARE, VARE, Mean BRE and Prediction. It is observed
that the Radial Basis Neural Network provided better results
</summary>
    <author>
      <name>P. V. G. D. Prasad Reddy</name>
    </author>
    <author>
      <name>K. R. Sudha</name>
    </author>
    <author>
      <name>P. Rama Sree</name>
    </author>
    <author>
      <name>S. N. S. V. S. C. Ramesh</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computing, Volume 2, Issue 5, May 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.4021v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.4021v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.3106v1</id>
    <updated>2012-12-13T10:07:11Z</updated>
    <published>2012-12-13T10:07:11Z</published>
    <title>Self-organized criticality in neural network models</title>
    <summary>  It has long been argued that neural networks have to establish and maintain a
certain intermediate level of activity in order to keep away from the regimes
of chaos and silence. Strong evidence for criticality has been observed in
terms of spatio-temporal activity avalanches first in cultures of rat cortex by
Beggs and Plenz (2003) and subsequently in many more experimental setups. These
findings sparked intense research on theoretical models for criticality and
avalanche dynamics in neural networks, where usually some dynamical order
parameter is fed back onto the network topology by adapting the synaptic
couplings. We here give an overview of existing theoretical models of dynamical
networks. While most models emphasize biological and neurophysiological detail,
our path here is different: we pick up the thread of an early self-organized
critical neural network model by Bornholdt and Roehl (2001) and test its
applicability in the light of experimental data. Keeping the simplicity of
early models, and at the same time lifting the drawback of a spin formulation
with respect to the biological system, we here study an improved model
(Rybarsch and Bornholdt, 2012b) and show that it adapts to criticality
exhibiting avalanche statistics that compare well with experimental data
without the need for parameter tuning.
</summary>
    <author>
      <name>Matthias Rybarsch</name>
    </author>
    <author>
      <name>Stefan Bornholdt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In "Criticality in Neural Systems", Niebur E, Plenz D, Schuster HG
  (eds.) 2013 (in press)</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.3106v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.3106v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.2686v1</id>
    <updated>2011-01-13T22:29:53Z</updated>
    <published>2011-01-13T22:29:53Z</published>
    <title>Neural development features: Spatio-temporal development of the
  Caenorhabditis elegans neuronal network</title>
    <summary>  The nematode Caenorhabditis elegans, with information on neural connectivity,
three-dimensional position and cell linage provides a unique system for
understanding the development of neural networks. Although C. elegans has been
widely studied in the past, we present the first statistical study from a
developmental perspective, with findings that raise interesting suggestions on
the establishment of long-distance connections and network hubs. Here, we
analyze the neuro-development for temporal and spatial features, using birth
times of neurons and their three-dimensional positions. Comparisons of growth
in C. elegans with random spatial network growth highlight two findings
relevant to neural network development. First, most neurons which are linked by
long-distance connections are born around the same time and early on,
suggesting the possibility of early contact or interaction between connected
neurons during development. Second, early-born neurons are more highly
connected (tendency to form hubs) than later born neurons. This indicates that
the longer time frame available to them might underlie high connectivity. Both
outcomes are not observed for random connection formation. The study finds that
around one-third of electrically coupled long-range connections are late
forming, raising the question of what mechanisms are involved in ensuring their
accuracy, particularly in light of the extremely invariant connectivity
observed in C. elegans. In conclusion, the sequence of neural network
development highlights the possibility of early contact or interaction in
securing long-distance and high-degree connectivity.
</summary>
    <author>
      <name>Sreedevi Varier</name>
    </author>
    <author>
      <name>Marcus Kaiser</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pcbi.1001044</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pcbi.1001044" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Varier S, Kaiser M (2011) Neural Development Features:
  Spatio-Temporal Development of the Caenorhabditis elegans Neuronal Network.
  PLoS Comput Biol 7(1): e1001044</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1101.2686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.2686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.03090v1</id>
    <updated>2016-05-10T16:37:30Z</updated>
    <published>2016-05-10T16:37:30Z</published>
    <title>Metabolite transport through glial networks stabilizes the dynamics of
  learning</title>
    <summary>  Learning and memory are acquired through long-lasting changes in synapses. In
the simplest models, such synaptic potentiation typically leads to runaway
excitation, but in reality there must exist processes that robustly preserve
overall stability of the neural system dynamics. How is this accomplished?
Various approaches to this basic question have been considered. Here we propose
a particularly compelling and natural mechanism for preserving stability of
learning neural systems. This mechanism is based on the global processes by
which metabolic resources are distributed to the neurons by glial cells.
Specifically, we introduce and study a model comprised of two interacting
networks: a model neural network interconnected by synapses which undergo
spike-timing dependent plasticity (STDP); and a model glial network
interconnected by gap junctions which diffusively transport metabolic resources
among the glia and, ultimately, to neural synapses where they are consumed. Our
main result is that the biophysical constraints imposed by diffusive transport
of metabolic resources through the glial network can prevent runaway growth of
synaptic strength, both during ongoing activity and during learning. Our
findings suggest a previously unappreciated role for glial transport of
metabolites in the feedback control stabilization of neural network dynamics
during learning.
</summary>
    <author>
      <name>Yogesh S. Virkar</name>
    </author>
    <author>
      <name>Woodrow L. Shew</name>
    </author>
    <author>
      <name>Juan G. Restrepo</name>
    </author>
    <author>
      <name>Edward Ott</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.94.042310</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.94.042310" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 94, 042310 (2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1605.03090v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.03090v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.02529v3</id>
    <updated>2017-08-08T21:34:30Z</updated>
    <published>2017-03-07T18:54:28Z</published>
    <title>NoScope: Optimizing Neural Network Queries over Video at Scale</title>
    <summary>  Recent advances in computer vision-in the form of deep neural networks-have
made it possible to query increasing volumes of video data with high accuracy.
However, neural network inference is computationally expensive at scale:
applying a state-of-the-art object detector in real time (i.e., 30+ frames per
second) to a single video requires a $4000 GPU. In response, we present
NoScope, a system for querying videos that can reduce the cost of neural
network video analysis by up to three orders of magnitude via
inference-optimized model search. Given a target video, object to detect, and
reference neural network, NoScope automatically searches for and trains a
sequence, or cascade, of models that preserves the accuracy of the reference
network but is specialized to the target video and are therefore far less
computationally expensive. NoScope cascades two types of models: specialized
models that forego the full generality of the reference model but faithfully
mimic its behavior for the target video and object; and difference detectors
that highlight temporal differences across frames. We show that the optimal
cascade architecture differs across videos and objects, so NoScope uses an
efficient cost-based optimizer to search across models and cascades. With this
approach, NoScope achieves two to three order of magnitude speed-ups
(265-15,500x real-time) on binary classification tasks over fixed-angle webcam
and surveillance video while maintaining accuracy within 1-5% of
state-of-the-art neural networks.
</summary>
    <author>
      <name>Daniel Kang</name>
    </author>
    <author>
      <name>John Emmons</name>
    </author>
    <author>
      <name>Firas Abuzaid</name>
    </author>
    <author>
      <name>Peter Bailis</name>
    </author>
    <author>
      <name>Matei Zaharia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PVLDB 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.02529v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.02529v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.01714v2</id>
    <updated>2018-02-15T08:25:48Z</updated>
    <published>2017-05-04T06:45:06Z</published>
    <title>Optimal Approximation with Sparsely Connected Deep Neural Networks</title>
    <summary>  We derive fundamental lower bounds on the connectivity and the memory
requirements of deep neural networks guaranteeing uniform approximation rates
for arbitrary function classes in $L^2(\mathbb{R}^d)$. In other words, we
establish a connection between the complexity of a function class and the
complexity of deep neural networks approximating functions from this class to
within a prescribed accuracy. Additionally, we prove that our lower bounds are
achievable for a broad family of function classes. Specifically, all function
classes that are optimally approximated by a general class of representation
systems---so-called affine systems---can be approximated by deep neural
networks with minimal connectivity and memory requirements. Affine systems
encompass a wealth of representation systems from applied harmonic analysis
such as wavelets, ridgelets, curvelets, shearlets, $\alpha$-shearlets, and more
generally $\alpha$-molecules. Our central result elucidates a remarkable
universality property of neural networks and shows that they achieve the
optimum approximation properties of all affine systems combined. As a specific
example, we consider the class of $\alpha^{-1}$-cartoon-like functions, which
is approximated optimally by $\alpha$-shearlets. We also explain how our
results can be extended to the case of functions on low-dimensional immersed
manifolds. Finally, we present numerical experiments demonstrating that the
standard stochastic gradient descent algorithm generates deep neural networks
providing close-to-optimal approximation rates at minimal connectivity.
Moreover, these results indicate that stochastic gradient descent can actually
learn approximations that are sparse in the representation systems optimally
sparsifying the function class the network is trained on.
</summary>
    <author>
      <name>Helmut B√∂lcskei</name>
    </author>
    <author>
      <name>Philipp Grohs</name>
    </author>
    <author>
      <name>Gitta Kutyniok</name>
    </author>
    <author>
      <name>Philipp Petersen</name>
    </author>
    <link href="http://arxiv.org/abs/1705.01714v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01714v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.FA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="41A25, 82C32, 42C40, 42C15, 41A46, 68T05, 94A34, 94A12" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.05647v1</id>
    <updated>2017-05-16T11:17:21Z</updated>
    <published>2017-05-16T11:17:21Z</published>
    <title>Optimized brute-force algorithms for the bifurcation analysis of a
  spin-glass-like neural network model</title>
    <summary>  Bifurcation theory is a powerful tool for studying how the dynamics of a
neural network model depends on its underlying neurophysiological parameters.
However, bifurcation theory has been developed mostly for smooth dynamical
systems and for continuous-time non-smooth models, which prevents us from
understanding the changes of dynamics in some widely used classes of artificial
neural network models. This article is an attempt to fill this gap, through the
introduction of algorithms that perform a semi-analytical bifurcation analysis
of a spin-glass-like neural network model with binary firing rates and
discrete-time evolution. Our approach is based on a numerical brute-force
search of the stationary and oscillatory solutions of the spin-glass model,
from which we derive analytical expressions of its bifurcation structure by
means of the state-to-state transition probability matrix. The algorithms
determine how the network parameters affect the degree of multistability, the
emergence and the period of the neural oscillations, and the formation of
symmetry-breaking in the neural populations. While this technique can be
applied to networks with arbitrary (generally asymmetric) connectivity
matrices, in particular we introduce a highly efficient algorithm for the
bifurcation analysis of sparse networks. We also provide some examples of the
obtained bifurcation diagrams and a Python implementation of the algorithms.
</summary>
    <author>
      <name>Diego Fasoli</name>
    </author>
    <author>
      <name>Stefano Panzeri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 5 figures, 4 Python scripts</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.05647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.05647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07441v1</id>
    <updated>2017-05-21T12:13:42Z</updated>
    <published>2017-05-21T12:13:42Z</published>
    <title>Exponential Capacity in an Autoencoder Neural Network with a Hidden
  Layer</title>
    <summary>  A fundamental aspect of limitations in learning any computation in neural
architectures is characterizing their optimal capacities.
  An important, widely-used neural architecture is known as autoencoders where
the network reconstructs the input at the output layer via a representation at
a hidden layer.
  Even though capacities of several neural architectures have been addressed
using statistical physics methods, the capacity of autoencoder neural networks
is not well-explored.
  Here, we analytically show that an autoencoder network of binary neurons with
a hidden layer can achieve a capacity that grows exponentially with network
size.
  The network has fixed random weights encoding a set of dense input patterns
into a dense, expanded (or \emph{overcomplete}) hidden layer representation. A
set of learnable weights decodes the input patters at the output layer. We
perform a mean-field approximation of the model to reduce the model to a
perceptron problem with an input-output dependency. Carrying out Gardner's
\emph{replica} calculation, we show that as the expansion ratio, defined as the
number of hidden units over the number of input units, increases, the
autoencoding capacity grows exponentially even when the sparseness or the
coding level of the hidden layer representation is changed. The
replica-symmetric solution is locally stable and is in good agreement with
simulation results obtained using a local learning rule. In addition, the
degree of symmetry between the encoding and decoding weights monotonically
increases with the expansion ratio.
</summary>
    <author>
      <name>Alireza Alemi</name>
    </author>
    <author>
      <name>Alia Abbara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 figures, 14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.07441v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07441v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.04859v3</id>
    <updated>2017-07-26T16:18:52Z</updated>
    <published>2017-06-15T13:25:25Z</published>
    <title>Sobolev Training for Neural Networks</title>
    <summary>  At the heart of deep learning we aim to use neural networks as function
approximators - training them to produce outputs from inputs in emulation of a
ground truth function or data creation process. In many cases we only have
access to input-output pairs from the ground truth, however it is becoming more
common to have access to derivatives of the target output with respect to the
input - for example when the ground truth function is itself a neural network
such as in network compression or distillation. Generally these target
derivatives are not computed, or are ignored. This paper introduces Sobolev
Training for neural networks, which is a method for incorporating these target
derivatives in addition the to target values while training. By optimising
neural networks to not only approximate the function's outputs but also the
function's derivatives we encode additional information about the target
function within the parameters of the neural network. Thereby we can improve
the quality of our predictors, as well as the data-efficiency and
generalization capabilities of our learned function approximation. We provide
theoretical justifications for such an approach as well as examples of
empirical evidence on three distinct domains: regression on classical
optimisation datasets, distilling policies of an agent playing Atari, and on
large-scale applications of synthetic gradients. In all three domains the use
of Sobolev Training, employing target derivatives in addition to target values,
results in models with higher accuracy and stronger generalisation.
</summary>
    <author>
      <name>Wojciech Marian Czarnecki</name>
    </author>
    <author>
      <name>Simon Osindero</name>
    </author>
    <author>
      <name>Max Jaderberg</name>
    </author>
    <author>
      <name>Grzegorz ≈öwirszcz</name>
    </author>
    <author>
      <name>Razvan Pascanu</name>
    </author>
    <link href="http://arxiv.org/abs/1706.04859v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.04859v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08884v1</id>
    <updated>2017-06-27T14:31:09Z</updated>
    <published>2017-06-27T14:31:09Z</published>
    <title>When Neurons Fail</title>
    <summary>  We view a neural network as a distributed system of which neurons can fail
independently, and we evaluate its robustness in the absence of any (recovery)
learning phase. We give tight bounds on the number of neurons that can fail
without harming the result of a computation. To determine our bounds, we
leverage the fact that neural activation functions are Lipschitz-continuous.
Our bound is on a quantity, we call the \textit{Forward Error Propagation},
capturing how much error is propagated by a neural network when a given number
of components is failing, computing this quantity only requires looking at the
topology of the network, while experimentally assessing the robustness of a
network requires the costly experiment of looking at all the possible inputs
and testing all the possible configurations of the network corresponding to
different failure situations, facing a discouraging combinatorial explosion.
  We distinguish the case of neurons that can fail and stop their activity
(crashed neurons) from the case of neurons that can fail by transmitting
arbitrary values (Byzantine neurons). Interestingly, as we show in the paper,
our bound can easily be extended to the case where synapses can fail.
  We show how our bound can be leveraged to quantify the effect of memory cost
reduction on the accuracy of a neural network, to estimate the amount of
information any neuron needs from its preceding layer, enabling thereby a
boosting scheme that prevents neurons from waiting for unnecessary signals. We
finally discuss the trade-off between neural networks robustness and learning
cost.
</summary>
    <author>
      <name>El Mahdi El Mhamdi</name>
    </author>
    <author>
      <name>Rachid Guerraoui</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IPDPS.2017.66</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IPDPS.2017.66" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2017 IEEE International Parallel and Distributed Processing
  Symposium, Orlando, Florida</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.08884v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08884v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.00848v1</id>
    <updated>2017-09-04T07:54:53Z</updated>
    <published>2017-09-04T07:54:53Z</published>
    <title>Neural Distributed Autoassociative Memories: A Survey</title>
    <summary>  Introduction. Neural network models of autoassociative, distributed memory
allow storage and retrieval of many items (vectors) where the number of stored
items can exceed the vector dimension (the number of neurons in the network).
This opens the possibility of a sublinear time search (in the number of stored
items) for approximate nearest neighbors among vectors of high dimension. The
purpose of this paper is to review models of autoassociative, distributed
memory that can be naturally implemented by neural networks (mainly with local
learning rules and iterative dynamics based on information locally available to
neurons). Scope. The survey is focused mainly on the networks of Hopfield,
Willshaw and Potts, that have connections between pairs of neurons and operate
on sparse binary vectors. We discuss not only autoassociative memory, but also
the generalization properties of these networks. We also consider neural
networks with higher-order connections and networks with a bipartite graph
structure for non-binary data with linear constraints. Conclusions. In
conclusion we discuss the relations to similarity search, advantages and
drawbacks of these techniques, and topics for further research. An interesting
and still not completely resolved question is whether neural autoassociative
memories can search for approximate nearest neighbors faster than other index
structures for similarity search, in particular for the case of very high
dimensional vectors.
</summary>
    <author>
      <name>V. I. Gritsenko</name>
    </author>
    <author>
      <name>D. A. Rachkovskij</name>
    </author>
    <author>
      <name>A. A. Frolov</name>
    </author>
    <author>
      <name>R. Gayler</name>
    </author>
    <author>
      <name>D. Kleyko</name>
    </author>
    <author>
      <name>E. Osipov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.15407/kvt188.02.005</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.15407/kvt188.02.005" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Cybernetics and Computer Engineering, 2017. 2(188), 5-35</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1709.00848v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.00848v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.00673v1</id>
    <updated>2017-12-02T22:26:12Z</updated>
    <published>2017-12-02T22:26:12Z</published>
    <title>Towards Robust Neural Networks via Random Self-ensemble</title>
    <summary>  Recent studies have revealed the vulnerability of deep neural networks - A
small adversarial perturbation that is imperceptible to human can easily make a
well-trained deep neural network mis-classify. This makes it unsafe to apply
neural networks in security-critical applications. In this paper, we propose a
new defensive algorithm called Random Self-Ensemble (RSE) by combining two
important concepts: ${\bf randomness}$ and ${\bf ensemble}$. To protect a
targeted model, RSE adds random noise layers to the neural network to prevent
from state-of-the-art gradient-based attacks, and ensembles the prediction over
random noises to stabilize the performance. We show that our algorithm is
equivalent to ensemble an infinite number of noisy models $f_\epsilon$ without
any additional memory overhead, and the proposed training procedure based on
noisy stochastic gradient descent can ensure the ensemble model has good
predictive capability. Our algorithm significantly outperforms previous defense
techniques on real datasets. For instance, on CIFAR-10 with VGG network (which
has $92\%$ accuracy without any attack), under the state-of-the-art C&amp;W attack
within a certain distortion tolerance, the accuracy of unprotected model drops
to less than $10\%$, the best previous defense technique has $48\%$ accuracy,
while our method still has $86\%$ prediction accuracy under the same level of
attack. Finally, our method is simple and easy to integrate into any neural
network.
</summary>
    <author>
      <name>Xuanqing Liu</name>
    </author>
    <author>
      <name>Minhao Cheng</name>
    </author>
    <author>
      <name>Huan Zhang</name>
    </author>
    <author>
      <name>Cho-Jui Hsieh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.00673v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.00673v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.06563v2</id>
    <updated>2018-01-17T18:45:26Z</updated>
    <published>2017-12-18T18:16:51Z</published>
    <title>Safe Mutations for Deep and Recurrent Neural Networks through Output
  Gradients</title>
    <summary>  While neuroevolution (evolving neural networks) has a successful track record
across a variety of domains from reinforcement learning to artificial life, it
is rarely applied to large, deep neural networks. A central reason is that
while random mutation generally works in low dimensions, a random perturbation
of thousands or millions of weights is likely to break existing functionality,
providing no learning signal even if some individual weight changes were
beneficial. This paper proposes a solution by introducing a family of safe
mutation (SM) operators that aim within the mutation operator itself to find a
degree of change that does not alter network behavior too much, but still
facilitates exploration. Importantly, these SM operators do not require any
additional interactions with the environment. The most effective SM variant
capitalizes on the intriguing opportunity to scale the degree of mutation of
each individual weight according to the sensitivity of the network's outputs to
that weight, which requires computing the gradient of outputs with respect to
the weights (instead of the gradient of error, as in conventional deep
learning). This safe mutation through gradients (SM-G) operator dramatically
increases the ability of a simple genetic algorithm-based neuroevolution method
to find solutions in high-dimensional domains that require deep and/or
recurrent neural networks (which tend to be particularly brittle to mutation),
including domains that require processing raw pixels. By improving our ability
to evolve deep neural networks, this new safer approach to mutation expands the
scope of domains amenable to neuroevolution.
</summary>
    <author>
      <name>Joel Lehman</name>
    </author>
    <author>
      <name>Jay Chen</name>
    </author>
    <author>
      <name>Jeff Clune</name>
    </author>
    <author>
      <name>Kenneth O. Stanley</name>
    </author>
    <link href="http://arxiv.org/abs/1712.06563v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.06563v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.04245v4</id>
    <updated>2017-04-10T06:25:18Z</updated>
    <published>2017-01-16T11:22:38Z</published>
    <title>Learning Traffic as Images: A Deep Convolutional Neural Network for
  Large-Scale Transportation Network Speed Prediction</title>
    <summary>  This paper proposes a convolutional neural network (CNN)-based method that
learns traffic as images and predicts large-scale, network-wide traffic speed
with a high accuracy. Spatiotemporal traffic dynamics are converted to images
describing the time and space relations of traffic flow via a two-dimensional
time-space matrix. A CNN is applied to the image following two consecutive
steps: abstract traffic feature extraction and network-wide traffic speed
prediction. The effectiveness of the proposed method is evaluated by taking two
real-world transportation networks, the second ring road and north-east
transportation network in Beijing, as examples, and comparing the method with
four prevailing algorithms, namely, ordinary least squares, k-nearest
neighbors, artificial neural network, and random forest, and three deep
learning architectures, namely, stacked autoencoder, recurrent neural network,
and long-short-term memory network. The results show that the proposed method
outperforms other algorithms by an average accuracy improvement of 42.91%
within an acceptable execution time. The CNN can train the model in a
reasonable time and, thus, is suitable for large-scale transportation networks.
</summary>
    <author>
      <name>Xiaolei Ma</name>
    </author>
    <author>
      <name>Zhuang Dai</name>
    </author>
    <author>
      <name>Zhengbing He</name>
    </author>
    <author>
      <name>Jihui Na</name>
    </author>
    <author>
      <name>Yong Wang</name>
    </author>
    <author>
      <name>Yunpeng Wang</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Sensors,2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1701.04245v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.04245v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.05509v1</id>
    <updated>2015-12-17T09:45:51Z</updated>
    <published>2015-12-17T09:45:51Z</published>
    <title>An Empirical Comparison of Neural Architectures for Reinforcement
  Learning in Partially Observable Environments</title>
    <summary>  This paper explores the performance of fitted neural Q iteration for
reinforcement learning in several partially observable environments, using
three recurrent neural network architectures: Long Short-Term Memory, Gated
Recurrent Unit and MUT1, a recurrent neural architecture evolved from a pool of
several thousands candidate architectures. A variant of fitted Q iteration,
based on Advantage values instead of Q values, is also explored. The results
show that GRU performs significantly better than LSTM and MUT1 for most of the
problems considered, requiring less training episodes and less CPU time before
learning a very good policy. Advantage learning also tends to produce better
results.
</summary>
    <author>
      <name>Denis Steckelmacher</name>
    </author>
    <author>
      <name>Peter Vrancx</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 27th Benelux Conference on Artificial Intelligence</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.05509v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.05509v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.03525v2</id>
    <updated>2017-04-23T16:52:03Z</updated>
    <published>2017-02-12T13:19:03Z</published>
    <title>Learning to Parse and Translate Improves Neural Machine Translation</title>
    <summary>  There has been relatively little attention to incorporating linguistic prior
to neural machine translation. Much of the previous work was further
constrained to considering linguistic prior on the source side. In this paper,
we propose a hybrid model, called NMT+RNNG, that learns to parse and translate
by combining the recurrent neural network grammar into the attention-based
neural machine translation. Our approach encourages the neural machine
translation model to incorporate linguistic prior during training, and lets it
translate on its own afterward. Extensive experiments with four language pairs
show the effectiveness of the proposed NMT+RNNG.
</summary>
    <author>
      <name>Akiko Eriguchi</name>
    </author>
    <author>
      <name>Yoshimasa Tsuruoka</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as a short paper at the 55th Annual Meeting of the
  Association for Computational Linguistics (ACL 2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.03525v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.03525v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.08228v3</id>
    <updated>2016-03-15T00:20:54Z</updated>
    <published>2015-11-25T21:17:43Z</published>
    <title>Neural GPUs Learn Algorithms</title>
    <summary>  Learning an algorithm from examples is a fundamental problem that has been
widely studied. Recently it has been addressed using neural networks, in
particular by Neural Turing Machines (NTMs). These are fully differentiable
computers that use backpropagation to learn their own programming. Despite
their appeal NTMs have a weakness that is caused by their sequential nature:
they are not parallel and are are hard to train due to their large depth when
unfolded.
  We present a neural network architecture to address this problem: the Neural
GPU. It is based on a type of convolutional gated recurrent unit and, like the
NTM, is computationally universal. Unlike the NTM, the Neural GPU is highly
parallel which makes it easier to train and efficient to run.
  An essential property of algorithms is their ability to handle inputs of
arbitrary size. We show that the Neural GPU can be trained on short instances
of an algorithmic task and successfully generalize to long instances. We
verified it on a number of tasks including long addition and long
multiplication of numbers represented in binary. We train the Neural GPU on
numbers with upto 20 bits and observe no errors whatsoever while testing it,
even on much longer numbers.
  To achieve these results we introduce a technique for training deep recurrent
networks: parameter sharing relaxation. We also found a small amount of dropout
and gradient noise to have a large positive effect on learning and
generalization.
</summary>
    <author>
      <name>≈Åukasz Kaiser</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <link href="http://arxiv.org/abs/1511.08228v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.08228v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.00556v2</id>
    <updated>2016-02-08T07:06:58Z</updated>
    <published>2015-10-02T10:48:56Z</published>
    <title>Autonomous Perceptron Neural Network Inspired from Quantum computing</title>
    <summary>  This abstract will be modified after correcting the minor error in Eq.(2)
</summary>
    <author>
      <name>M. Zidan</name>
    </author>
    <author>
      <name>A. Sagheer</name>
    </author>
    <author>
      <name>N. Metwally</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author due to a crucial sign
  error in equation 2</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.00556v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.00556v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07704v2</id>
    <updated>2016-08-03T14:31:17Z</updated>
    <published>2016-03-24T18:54:18Z</published>
    <title>Probabilistic Reasoning via Deep Learning: Neural Association Models</title>
    <summary>  In this paper, we propose a new deep learning approach, called neural
association model (NAM), for probabilistic reasoning in artificial
intelligence. We propose to use neural networks to model association between
any two events in a domain. Neural networks take one event as input and compute
a conditional probability of the other event to model how likely these two
events are to be associated. The actual meaning of the conditional
probabilities varies between applications and depends on how the models are
trained. In this work, as two case studies, we have investigated two NAM
structures, namely deep neural networks (DNN) and relation-modulated neural
nets (RMNN), on several probabilistic reasoning tasks in AI, including
recognizing textual entailment, triple classification in multi-relational
knowledge bases and commonsense reasoning. Experimental results on several
popular datasets derived from WordNet, FreeBase and ConceptNet have all
demonstrated that both DNNs and RMNNs perform equally well and they can
significantly outperform the conventional methods available for these reasoning
tasks. Moreover, compared with DNNs, RMNNs are superior in knowledge transfer,
where a pre-trained model can be quickly extended to an unseen relation after
observing only a few training samples. To further prove the effectiveness of
the proposed models, in this work, we have applied NAMs to solving challenging
Winograd Schema (WS) problems. Experiments conducted on a set of WS problems
prove that the proposed models have the potential for commonsense reasoning.
</summary>
    <author>
      <name>Quan Liu</name>
    </author>
    <author>
      <name>Hui Jiang</name>
    </author>
    <author>
      <name>Andrew Evdokimov</name>
    </author>
    <author>
      <name>Zhen-Hua Ling</name>
    </author>
    <author>
      <name>Xiaodan Zhu</name>
    </author>
    <author>
      <name>Si Wei</name>
    </author>
    <author>
      <name>Yu Hu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Probabilistic reasoning, Winograd Schema Challenge, Deep learning,
  Neural Networks, Distributed Representation</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.07704v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07704v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04412v2</id>
    <updated>2016-05-20T06:41:57Z</updated>
    <published>2016-04-15T09:43:40Z</published>
    <title>Transient hidden chaotic attractors in a Hopfield neural system</title>
    <summary>  In this letter we unveil the existence of transient hidden coexisting chaotic
attractors, in a simplified Hopfield neural network with three neurons.
</summary>
    <author>
      <name>Marius-F. Danca</name>
    </author>
    <author>
      <name>Nikolay Kuznetsov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">revised</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.04412v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04412v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.09072v1</id>
    <updated>2017-08-30T01:07:18Z</updated>
    <published>2017-08-30T01:07:18Z</published>
    <title>Continual One-Shot Learning of Hidden Spike-Patterns with Neural Network
  Simulation Expansion and STDP Convergence Predictions</title>
    <summary>  This paper presents a constructive algorithm that achieves successful
one-shot learning of hidden spike-patterns in a competitive detection task. It
has previously been shown (Masquelier et al., 2008) that spike-timing-dependent
plasticity (STDP) and lateral inhibition can result in neurons competitively
tuned to repeating spike-patterns concealed in high rates of overall
presynaptic activity. One-shot construction of neurons with synapse weights
calculated as estimates of converged STDP outcomes results in immediate
selective detection of hidden spike-patterns. The capability of continual
learning is demonstrated through the successful one-shot detection of new sets
of spike-patterns introduced after long intervals in the simulation time.
Simulation expansion (Lightheart et al., 2013) has been proposed as an approach
to the development of constructive algorithms that are compatible with
simulations of biological neural networks. A simulation of a biological neural
network may have orders of magnitude fewer neurons and connections than the
related biological neural systems; therefore, simulated neural networks can be
assumed to be a subset of a larger neural system. The constructive algorithm is
developed using simulation expansion concepts to perform an operation
equivalent to the exchange of neurons between the simulation and the larger
hypothetical neural system. The dynamic selection of neurons to simulate within
a larger neural system (hypothetical or stored in memory) may be a starting
point for a wide range of developments and applications in machine learning and
the simulation of biology.
</summary>
    <author>
      <name>Toby Lightheart</name>
    </author>
    <author>
      <name>Steven Grainger</name>
    </author>
    <author>
      <name>Tien-Fu Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">41 pages, 16 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.09072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.09072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.01060v1</id>
    <updated>2015-08-05T13:09:43Z</updated>
    <published>2015-08-05T13:09:43Z</published>
    <title>Models of Innate Neural Attractors and Their Applications for Neural
  Information Processing</title>
    <summary>  In this work we reveal and explore a new class of attractor neural networks,
based on inborn connections provided by model molecular markers, the molecular
marker based attractor neural networks (MMBANN). We have explored conditions
for the existence of attractor states, critical relations between their
parameters and the spectrum of single neuron models, which can implement the
MMBANN. Besides, we describe functional models (perceptron and SOM) which
obtain significant advantages, while using MMBANN. In particular, the
perceptron based on MMBANN, gets specificity gain in orders of error
probabilities values, MMBANN SOM obtains real neurophysiological meaning, the
number of possible grandma cells increases 1000- fold with MMBANN. Each set of
markers has a metric, which is used to make connections between neurons
containing the markers. The resulting neural networks have sets of attractor
states, which can serve as finite grids for representation of variables in
computations. These grids may show dimensions of d = 0, 1, 2,... We work with
static and dynamic attractor neural networks of dimensions d = 0 and d = 1. We
also argue that the number of dimensions which can be represented by attractors
of activities of neural networks with the number of elements N=104 does not
exceed 8.
</summary>
    <author>
      <name>Ksenia P. Solovyeva</name>
    </author>
    <author>
      <name>Iakov M. Karandashev</name>
    </author>
    <author>
      <name>Alex Zhavoronkov</name>
    </author>
    <author>
      <name>Witali L. Dunin-Barkowski</name>
    </author>
    <link href="http://arxiv.org/abs/1508.01060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.01060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.02009v2</id>
    <updated>2016-04-06T15:39:02Z</updated>
    <published>2016-02-05T13:06:44Z</published>
    <title>Computing with hardware neurons: spiking or classical? Perspectives of
  applied Spiking Neural Networks from the hardware side</title>
    <summary>  While classical neural networks take a position of a leading method in the
machine learning community, spiking neuromorphic systems bring attention and
large projects in neuroscience. Spiking neural networks were shown to be able
to substitute networks of classical neurons in applied tasks. This work
explores recent hardware designs focusing on perspective applications (like
convolutional neural networks) for both neuron types from the energy efficiency
side to analyse whether there is a possibility for spiking neuromorphic
hardware to grow up for a wider use. Our comparison shows that spiking hardware
is at least on the same level of energy efficiency or even higher than
non-spiking on a level of basic operations. However, on a system level, spiking
systems are outmatched and consume much more energy due to inefficient data
representation with a long series of spikes. If spike-driven applications,
minimizing an amount of spikes, are developed, spiking neural systems may reach
the energy efficiency level of classical neural systems. However, in the near
future, both type of neuromorphic systems may benefit from emerging memory
technologies, minimizing the energy consumption of computation and memory for
both neuron types. That would make infrastructure and data transfer energy
dominant on the system level. We expect that spiking neurons have some
benefits, which would allow achieving better energy results. Still the problem
of an amount of spikes will still be the major bottleneck for spiking hardware
systems.
</summary>
    <author>
      <name>Sergei Dytckov</name>
    </author>
    <author>
      <name>Masoud Daneshtalab</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Withdrawn by the author as the paper is rejected from the target
  conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.02009v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.02009v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.05031v2</id>
    <updated>2017-08-26T01:37:09Z</updated>
    <published>2017-08-16T18:30:09Z</published>
    <title>Neural Collaborative Filtering</title>
    <summary>  In recent years, deep neural networks have yielded immense success on speech
recognition, computer vision and natural language processing. However, the
exploration of deep neural networks on recommender systems has received
relatively less scrutiny. In this work, we strive to develop techniques based
on neural networks to tackle the key problem in recommendation -- collaborative
filtering -- on the basis of implicit feedback. Although some recent work has
employed deep learning for recommendation, they primarily used it to model
auxiliary information, such as textual descriptions of items and acoustic
features of musics. When it comes to model the key factor in collaborative
filtering -- the interaction between user and item features, they still
resorted to matrix factorization and applied an inner product on the latent
features of users and items. By replacing the inner product with a neural
architecture that can learn an arbitrary function from data, we present a
general framework named NCF, short for Neural network-based Collaborative
Filtering. NCF is generic and can express and generalize matrix factorization
under its framework. To supercharge NCF modelling with non-linearities, we
propose to leverage a multi-layer perceptron to learn the user-item interaction
function. Extensive experiments on two real-world datasets show significant
improvements of our proposed NCF framework over the state-of-the-art methods.
Empirical evidence shows that using deeper layers of neural networks offers
better recommendation performance.
</summary>
    <author>
      <name>Xiangnan He</name>
    </author>
    <author>
      <name>Lizi Liao</name>
    </author>
    <author>
      <name>Hanwang Zhang</name>
    </author>
    <author>
      <name>Liqiang Nie</name>
    </author>
    <author>
      <name>Xia Hu</name>
    </author>
    <author>
      <name>Tat-Seng Chua</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.05031v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.05031v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.05159v1</id>
    <updated>2018-01-16T08:54:20Z</updated>
    <published>2018-01-16T08:54:20Z</published>
    <title>GitGraph - Architecture Search Space Creation through Frequent
  Computational Subgraph Mining</title>
    <summary>  The dramatic success of deep neural networks across multiple application
areas often relies on experts painstakingly designing a network architecture
specific to each task. To simplify this process and make it more accessible, an
emerging research effort seeks to automate the design of neural network
architectures, using e.g. evolutionary algorithms or reinforcement learning or
simple search in a constrained space of neural modules.
  Considering the typical size of the search space (e.g. $10^{10}$ candidates
for a $10$-layer network) and the cost of evaluating a single candidate,
current architecture search methods are very restricted. They either rely on
static pre-built modules to be recombined for the task at hand, or they define
a static hand-crafted framework within which they can generate new
architectures from the simplest possible operations.
  In this paper, we relax these restrictions, by capitalizing on the collective
wisdom contained in the plethora of neural networks published in online code
repositories. Concretely, we (a) extract and publish GitGraph, a corpus of
neural architectures and their descriptions; (b) we create problem-specific
neural architecture search spaces, implemented as a textual search mechanism
over GitGraph; (c) we propose a method of identifying unique common subgraphs
within the architectures solving each problem (e.g., image processing,
reinforcement learning), that can then serve as modules in the newly created
problem specific neural search space.
</summary>
    <author>
      <name>Kamil Bennani-Smires</name>
    </author>
    <author>
      <name>Claudiu Musat</name>
    </author>
    <author>
      <name>Andreea Hossmann</name>
    </author>
    <author>
      <name>Michael Baeriswyl</name>
    </author>
    <link href="http://arxiv.org/abs/1801.05159v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.05159v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/astro-ph/0404232v1</id>
    <updated>2004-04-12T16:45:42Z</updated>
    <published>2004-04-12T16:45:42Z</published>
    <title>Lightcurve Classification in Massive Variability Surveys II: Transients
  towards the Large Magellanic Cloud</title>
    <summary>  Automatic classification of variability is now possible with tools like
neural networks. Here, we present two neural networks for the identification of
microlensing events -- the first discriminates against variable stars and the
second against supernovae. The inputs to the networks include parameters
describing the shape and the size of the lightcurve, together with colour of
the event. The network computes the posterior probability of microlensing,
together with an estimate of the likely error. An algorithm is devised for
direct calculation of the microlensing rate from the output of the neural
networks. We present a new analysis of the microlensing candidates towards the
Large Magellanic Cloud (LMC). The neural networks confirm the microlensing
nature of only 7 of the possible 17 events identified by the MACHO experiment.
This suggests that earlier estimates of the microlensing optical depth towards
the LMC may have been overestimated. A smaller number of events is consistent
with the assumption that all the microlensing events are caused by the known
stellar populations in the outer Galaxy/LMC.
</summary>
    <author>
      <name>V. Belokurov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Cambridge</arxiv:affiliation>
    </author>
    <author>
      <name>N. W. Evans</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Cambridge</arxiv:affiliation>
    </author>
    <author>
      <name>Y. Le Du</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Oxford</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/j.1365-2966.2004.07917.x</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/j.1365-2966.2004.07917.x" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, MNRAS, in press</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Mon.Not.Roy.Astron.Soc. 352 (2004) 233</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/astro-ph/0404232v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/astro-ph/0404232v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9806078v2</id>
    <updated>2000-08-21T12:50:44Z</updated>
    <published>1998-06-05T14:38:07Z</published>
    <title>Mutual Information of Three-State Low Activity Diluted Neural Networks
  with Self-Control</title>
    <summary>  The influence of a macroscopic time-dependent threshold on the retrieval
process of three-state extremely diluted neural networks is examined. If the
threshold is chosen appropriately in function of the noise and the pattern
activity of the network, adapting itself in the course of the time evolution,
it guarantees an autonomous functioning of the network. It is found that this
self-control mechanism considerably improves the retrieval quality, especially
in the limit of low activity, including the storage capacity, the basins of
attraction and the information content. The mutual information is shown to be
the relevant parameter to study the retrieval quality of such low activity
models. Numerical results confirm these observations.
</summary>
    <author>
      <name>D. Bolle'</name>
    </author>
    <author>
      <name>D. R. C. Dominguez</name>
    </author>
    <author>
      <name>S. Amari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Change of title and small corrections (16 pages and 6 figures)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Networks 13, 455-462 (2000)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/9806078v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9806078v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0202112v1</id>
    <updated>2002-02-07T11:29:45Z</updated>
    <published>2002-02-07T11:29:45Z</published>
    <title>Secure exchange of information by synchronization of neural networks</title>
    <summary>  A connection between the theory of neural networks and cryptography is
presented. A new phenomenon, namely synchronization of neural networks is
leading to a new method of exchange of secret messages. Numerical simulations
show that two artificial networks being trained by Hebbian learning rule on
their mutual outputs develop an antiparallel state of their synaptic weights.
The synchronized weights are used to construct an ephemeral key exchange
protocol for a secure transmission of secret data. It is shown that an opponent
who knows the protocol and all details of any transmission of the data has no
chance to decrypt the secret message, since tracking the weights is a hard
problem compared to synchronization. The complexity of the generation of the
secure channel is linear with the size of the network.
</summary>
    <author>
      <name>I. Kanter</name>
    </author>
    <author>
      <name>W. Kinzel</name>
    </author>
    <author>
      <name>E. Kanter</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1209/epl/i2002-00552-9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1209/epl/i2002-00552-9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Europhys. Lett. 57, pp. 141-147 (2002</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0202112v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0202112v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0404018v1</id>
    <updated>2004-04-01T13:26:34Z</updated>
    <published>2004-04-01T13:26:34Z</published>
    <title>Analytic solution of attractor neural networks on scale-free graphs</title>
    <summary>  We study the influence of network topology on retrieval properties of
recurrent neural networks, using replica techniques for diluted systems. The
theory is presented for a network with an arbitrary degree distribution $p(k)$
and applied to power law distributions $p(k) \sim k^{-\gamma}$, i.e. to neural
networks on scale-free graphs. A bifurcation analysis identifies phase
boundaries between the paramagnetic phase and either a retrieval phase or a
spin glass phase. Using a population dynamics algorithm, the retrieval overlap
and spin glass order parameters may be calculated throughout the phase diagram.
It is shown that there is an enhancement of the retrieval properties compared
with a Poissonian random graph. We compare our findings with simulations.
</summary>
    <author>
      <name>I. P√©rez Castillo</name>
    </author>
    <author>
      <name>B. Wemmenhove</name>
    </author>
    <author>
      <name>J. P. L. Hatchett</name>
    </author>
    <author>
      <name>A. C. C. Coolen</name>
    </author>
    <author>
      <name>N. S. Skantzos</name>
    </author>
    <author>
      <name>T. Nikoletopoulos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/0305-4470/37/37/002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/0305-4470/37/37/002" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 eps figures, LaTeX</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0404018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0404018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.0627v1</id>
    <updated>2009-10-04T19:31:06Z</updated>
    <published>2009-10-04T19:31:06Z</published>
    <title>On Bootstrap Percolation in Living Neural Networks</title>
    <summary>  Recent experimental studies of living neural networks reveal that their
global activation induced by electrical stimulation can be explained using the
concept of bootstrap percolation on a directed random network. The experiment
consists in activating externally an initial random fraction of the neurons and
observe the process of firing until its equilibrium. The final portion of
neurons that are active depends in a non linear way on the initial fraction.
The main result of this paper is a theorem which enables us to find the
asymptotic of final proportion of the fired neurons in the case of random
directed graphs with given node degrees as the model for interacting network.
This gives a rigorous mathematical proof of a phenomena observed by physicists
in neural networks.
</summary>
    <author>
      <name>Hamed Amini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10955-010-0056-z</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10955-010-0056-z" rel="related"/>
    <link href="http://arxiv.org/abs/0910.0627v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.0627v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.1938v1</id>
    <updated>2014-08-08T18:54:34Z</updated>
    <published>2014-08-08T18:54:34Z</published>
    <title>Applying Spiking Neural Nets to Noise Shaping</title>
    <summary>  -In recent years, there has been an increased focus on the mechanics of
information transmission in spiking neural networks. Especially the Noise
Shaping properties of these networks and their similarity to Delta-Sigma
Modulators has received a lot of attention. However, very little of the
research done in this area has focused on the effect the weights in these
networks have on the Noise Shaping properties and on post- processing of the
network output signal. This paper concerns itself with the various modes of
network operation and beneficial as well as detrimental effects which the
systematic generation of network weights can effect. Also, a method for
post-processing of the spiking output signal is introduced, bringing the output
signal more in line with conventional Delta-Sigma Modulators. Relevancy of this
research to industrial application of neural nets as building blocks of
oversampled A/D converters is shown. Also, further points of contention are
listed, which must be thoroughly researched to add to the above mentioned
applicability of spiking neural nets.
</summary>
    <author>
      <name>C. Mayr</name>
    </author>
    <author>
      <name>R. Sch√ºffny</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEICE Transactions on Information and Systems, vol. E88-D, no. 8,
  pages 1885-1892, 2005</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1408.1938v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.1938v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.2249v1</id>
    <updated>2013-12-08T19:40:51Z</updated>
    <published>2013-12-08T19:40:51Z</published>
    <title>Scalable Object Detection using Deep Neural Networks</title>
    <summary>  Deep convolutional neural networks have recently achieved state-of-the-art
performance on a number of image recognition benchmarks, including the ImageNet
Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on
the localization sub-task was a network that predicts a single bounding box and
a confidence score for each object category in the image. Such a model captures
the whole-image context around the objects but cannot handle multiple instances
of the same object in the image without naively replicating the number of
outputs for each instance. In this work, we propose a saliency-inspired neural
network model for detection, which predicts a set of class-agnostic bounding
boxes along with a single score for each box, corresponding to its likelihood
of containing any object of interest. The model naturally handles a variable
number of instances for each class and allows for cross-class generalization at
the highest levels of the network. We are able to obtain competitive
recognition performance on VOC2007 and ILSVRC2012, while using only the top few
predicted locations in each image and a small number of neural network
evaluations.
</summary>
    <author>
      <name>Dumitru Erhan</name>
    </author>
    <author>
      <name>Christian Szegedy</name>
    </author>
    <author>
      <name>Alexander Toshev</name>
    </author>
    <author>
      <name>Dragomir Anguelov</name>
    </author>
    <link href="http://arxiv.org/abs/1312.2249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.2249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.4461v4</id>
    <updated>2014-01-28T22:29:55Z</updated>
    <published>2013-12-16T18:58:34Z</published>
    <title>Low-Rank Approximations for Conditional Feedforward Computation in Deep
  Neural Networks</title>
    <summary>  Scalability properties of deep neural networks raise key research questions,
particularly as the problems considered become larger and more challenging.
This paper expands on the idea of conditional computation introduced by Bengio,
et. al., where the nodes of a deep network are augmented by a set of gating
units that determine when a node should be calculated. By factorizing the
weight matrix into a low-rank approximation, an estimation of the sign of the
pre-nonlinearity activation can be efficiently obtained. For networks using
rectified-linear hidden units, this implies that the computation of a hidden
unit with an estimated negative pre-nonlinearity can be ommitted altogether, as
its value will become zero when nonlinearity is applied. For sparse neural
networks, this can result in considerable speed gains. Experimental results
using the MNIST and SVHN data sets with a fully-connected deep neural network
demonstrate the performance robustness of the proposed scheme with respect to
the error introduced by the conditional computation process.
</summary>
    <author>
      <name>Andrew Davis</name>
    </author>
    <author>
      <name>Itamar Arel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures. Submitted to ICLR 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.4461v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.4461v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.1869v2</id>
    <updated>2014-06-07T19:56:14Z</updated>
    <published>2014-02-08T17:16:27Z</published>
    <title>On the Number of Linear Regions of Deep Neural Networks</title>
    <summary>  We study the complexity of functions computable by deep feedforward neural
networks with piecewise linear activations in terms of the symmetries and the
number of linear regions that they have. Deep networks are able to sequentially
map portions of each layer's input-space to the same output. In this way, deep
models compute functions that react equally to complicated patterns of
different inputs. The compositional structure of these functions enables them
to re-use pieces of computation exponentially often in terms of the network's
depth. This paper investigates the complexity of such compositional maps and
contributes new theoretical results regarding the advantage of depth for neural
networks with piecewise linear activation functions. In particular, our
analysis is not specific to a single family of models, and as an example, we
employ it for rectifier and maxout networks. We improve complexity bounds from
pre-existing work and investigate the behavior of units in higher layers.
</summary>
    <author>
      <name>Guido Mont√∫far</name>
    </author>
    <author>
      <name>Razvan Pascanu</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <link href="http://arxiv.org/abs/1402.1869v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.1869v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.3563v4</id>
    <updated>2015-01-18T22:02:17Z</updated>
    <published>2014-02-14T19:57:21Z</published>
    <title>Ideomotor feedback control in a recurrent neural network</title>
    <summary>  The architecture of a neural network controlling an unknown environment is
presented. It is based on a randomly connected recurrent neural network from
which both perception and action are simultaneously read and fed back. There
are two concurrent learning rules implementing a sort of ideomotor control: (i)
perception is learned along the principle that the network should predict
reliably its incoming stimuli; (ii) action is learned along the principle that
the prediction of the network should match a target time series. The coherent
behavior of the neural network in its environment is a consequence of the
interaction between the two principles. Numerical simulations show the
promising performance of the approach, which can be turned into a local, and
thus "biologically plausible", algorithm.
</summary>
    <author>
      <name>Mathieu Galtier</name>
    </author>
    <link href="http://arxiv.org/abs/1402.3563v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.3563v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.7024v5</id>
    <updated>2015-09-23T01:00:44Z</updated>
    <published>2014-12-22T15:22:45Z</published>
    <title>Training deep neural networks with low precision multiplications</title>
    <summary>  Multipliers are the most space and power-hungry arithmetic operators of the
digital implementation of deep neural networks. We train a set of
state-of-the-art neural networks (Maxout networks) on three benchmark datasets:
MNIST, CIFAR-10 and SVHN. They are trained with three distinct formats:
floating point, fixed point and dynamic fixed point. For each of those datasets
and for each of those formats, we assess the impact of the precision of the
multiplications on the final error after training. We find that very low
precision is sufficient not just for running trained networks but also for
training them. For example, it is possible to train Maxout networks with 10
bits multiplications.
</summary>
    <author>
      <name>Matthieu Courbariaux</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <author>
      <name>Jean-Pierre David</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures, Accepted as a workshop contribution at ICLR 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.7024v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.7024v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.03436v2</id>
    <updated>2015-10-27T06:45:51Z</updated>
    <published>2015-02-11T20:56:02Z</published>
    <title>An exploration of parameter redundancy in deep networks with circulant
  projections</title>
    <summary>  We explore the redundancy of parameters in deep neural networks by replacing
the conventional linear projection in fully-connected layers with the circulant
projection. The circulant structure substantially reduces memory footprint and
enables the use of the Fast Fourier Transform to speed up the computation.
Considering a fully-connected neural network layer with d input nodes, and d
output nodes, this method improves the time complexity from O(d^2) to O(dlogd)
and space complexity from O(d^2) to O(d). The space savings are particularly
important for modern deep convolutional neural network architectures, where
fully-connected layers typically contain more than 90% of the network
parameters. We further show that the gradient computation and optimization of
the circulant projections can be performed very efficiently. Our experiments on
three standard datasets show that the proposed approach achieves this
significant gain in storage and efficiency with minimal increase in error rate
compared to neural networks with unstructured projections.
</summary>
    <author>
      <name>Yu Cheng</name>
    </author>
    <author>
      <name>Felix X. Yu</name>
    </author>
    <author>
      <name>Rogerio S. Feris</name>
    </author>
    <author>
      <name>Sanjiv Kumar</name>
    </author>
    <author>
      <name>Alok Choudhary</name>
    </author>
    <author>
      <name>Shih-Fu Chang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Computer Vision (ICCV) 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.03436v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.03436v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.03654v2</id>
    <updated>2015-11-29T21:07:19Z</updated>
    <published>2015-05-14T09:03:19Z</published>
    <title>Neural Network with Unbounded Activation Functions is Universal
  Approximator</title>
    <summary>  This paper presents an investigation of the approximation property of neural
networks with unbounded activation functions, such as the rectified linear unit
(ReLU), which is the new de-facto standard of deep learning. The ReLU network
can be analyzed by the ridgelet transform with respect to Lizorkin
distributions. By showing three reconstruction formulas by using the Fourier
slice theorem, the Radon transform, and Parseval's relation, it is shown that a
neural network with unbounded activation functions still satisfies the
universal approximation property. As an additional consequence, the ridgelet
transform, or the backprojection filter in the Radon domain, is what the
network learns after backpropagation. Subject to a constructive admissibility
condition, the trained network can be obtained by simply discretizing the
ridgelet transform, without backpropagation. Numerical examples not only
support the consistency of the admissibility condition but also imply that some
non-admissible cases result in low-pass filtering.
</summary>
    <author>
      <name>Sho Sonoda</name>
    </author>
    <author>
      <name>Noboru Murata</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">under review; first revised version</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.03654v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.03654v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.FA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.02626v3</id>
    <updated>2015-10-30T23:29:27Z</updated>
    <published>2015-06-08T19:28:43Z</published>
    <title>Learning both Weights and Connections for Efficient Neural Networks</title>
    <summary>  Neural networks are both computationally intensive and memory intensive,
making them difficult to deploy on embedded systems. Also, conventional
networks fix the architecture before training starts; as a result, training
cannot improve the architecture. To address these limitations, we describe a
method to reduce the storage and computation required by neural networks by an
order of magnitude without affecting their accuracy by learning only the
important connections. Our method prunes redundant connections using a
three-step method. First, we train the network to learn which connections are
important. Next, we prune the unimportant connections. Finally, we retrain the
network to fine tune the weights of the remaining connections. On the ImageNet
dataset, our method reduced the number of parameters of AlexNet by a factor of
9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar
experiments with VGG-16 found that the number of parameters can be reduced by
13x, from 138 million to 10.3 million, again with no loss of accuracy.
</summary>
    <author>
      <name>Song Han</name>
    </author>
    <author>
      <name>Jeff Pool</name>
    </author>
    <author>
      <name>John Tran</name>
    </author>
    <author>
      <name>William J. Dally</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at NIPS 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.02626v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.02626v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.01851v2</id>
    <updated>2016-04-08T00:44:00Z</updated>
    <published>2015-09-06T20:25:32Z</published>
    <title>Deep Online Convex Optimization by Putting Forecaster to Sleep</title>
    <summary>  Methods from convex optimization such as accelerated gradient descent are
widely used as building blocks for deep learning algorithms. However, the
reasons for their empirical success are unclear, since neural networks are not
convex and standard guarantees do not apply. This paper develops the first
rigorous link between online convex optimization and error backpropagation on
convolutional networks. The first step is to introduce circadian games, a mild
generalization of convex games with similar convergence properties. The main
result is that error backpropagation on a convolutional network is equivalent
to playing out a circadian game. It follows immediately that the waking-regret
of players in the game (the units in the neural network) controls the overall
rate of convergence of the network. Finally, we explore some implications of
the results: (i) we describe the representations learned by a neural network
game-theoretically, (ii) propose a learning setting at the level of individual
units that can be plugged into deep architectures, and (iii) propose a new
approach to adaptive model selection by applying bandit algorithms to choose
which players to wake on each round.
</summary>
    <author>
      <name>David Balduzzi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Rendered obsolete by arXiv:1604.01952. The new version contains the
  same basic results, with major changes to exposition and minor changes to
  terminology</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.01851v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.01851v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.02799v4</id>
    <updated>2017-07-24T17:15:06Z</updated>
    <published>2015-11-09T18:48:39Z</published>
    <title>Neural Module Networks</title>
    <summary>  Visual question answering is fundamentally compositional in nature---a
question like "where is the dog?" shares substructure with questions like "what
color is the dog?" and "where is the cat?" This paper seeks to simultaneously
exploit the representational capacity of deep networks and the compositional
linguistic structure of questions. We describe a procedure for constructing and
learning *neural module networks*, which compose collections of jointly-trained
neural "modules" into deep networks for question answering. Our approach
decomposes questions into their linguistic substructures, and uses these
structures to dynamically instantiate modular networks (with reusable
components for recognizing dogs, classifying colors, etc.). The resulting
compound networks are jointly trained. We evaluate our approach on two
challenging datasets for visual question answering, achieving state-of-the-art
results on both the VQA natural image dataset and a new dataset of complex
questions about abstract shapes.
</summary>
    <author>
      <name>Jacob Andreas</name>
    </author>
    <author>
      <name>Marcus Rohrbach</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <author>
      <name>Dan Klein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Corrects an error in the evaluation of the NMN-only ablation
  experiment</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.02799v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.02799v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.03060v4</id>
    <updated>2017-04-21T21:22:34Z</updated>
    <published>2016-01-12T21:16:35Z</published>
    <title>Efficient Probabilistic Inference in Generic Neural Networks Trained
  with Non-Probabilistic Feedback</title>
    <summary>  Animals perform near-optimal probabilistic inference in a wide range of
psychophysical tasks. Probabilistic inference requires trial-to-trial
representation of the uncertainties associated with task variables and
subsequent use of this representation. Previous work has implemented such
computations using neural networks with hand-crafted and task-dependent
operations. We show that generic neural networks trained with a simple
error-based learning rule perform near-optimal probabilistic inference in nine
common psychophysical tasks. In a probabilistic categorization task,
error-based learning in a generic network simultaneously explains a monkey's
learning curve and the evolution of qualitative aspects of its choice behavior.
In all tasks, the number of neurons required for a given level of performance
grows sub-linearly with the input population size, a substantial improvement on
previous implementations of probabilistic inference. The trained networks
develop a novel sparsity-based probabilistic population code. Our results
suggest that probabilistic inference emerges naturally in generic neural
networks trained with error-based learning rules.
</summary>
    <author>
      <name>A. Emin Orhan</name>
    </author>
    <author>
      <name>Wei Ji Ma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 10 figures, 6 supplementary figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.03060v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.03060v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07249v1</id>
    <updated>2016-03-23T15:55:20Z</updated>
    <published>2016-03-23T15:55:20Z</published>
    <title>A Tutorial on Deep Neural Networks for Intelligent Systems</title>
    <summary>  Developing Intelligent Systems involves artificial intelligence approaches
including artificial neural networks. Here, we present a tutorial of Deep
Neural Networks (DNNs), and some insights about the origin of the term "deep";
references to deep learning are also given. Restricted Boltzmann Machines,
which are the core of DNNs, are discussed in detail. An example of a simple
two-layer network, performing unsupervised learning for unlabeled data, is
shown. Deep Belief Networks (DBNs), which are used to build networks with more
than two layers, are also described. Moreover, examples for supervised learning
with DNNs performing simple prediction and classification tasks, are presented
and explained. This tutorial includes two intelligent pattern recognition
applications: hand- written digits (benchmark known as MNIST) and speech
recognition.
</summary>
    <author>
      <name>Juan C. Cuevas-Tello</name>
    </author>
    <author>
      <name>Manuel Valenzuela-Rendon</name>
    </author>
    <author>
      <name>Juan A. Nolazco-Flores</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 19 figures, unpublished technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.07249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.4.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07400v2</id>
    <updated>2016-06-15T01:26:31Z</updated>
    <published>2016-03-24T00:52:22Z</published>
    <title>A Reconfigurable Low Power High Throughput Architecture for Deep Network
  Training</title>
    <summary>  General purpose computing systems are used for a large variety of
applications. Extensive supports for flexibility in these systems limit their
energy efficiencies. Neural networks, including deep networks, are widely used
for signal processing and pattern recognition applications. In this paper we
propose a multicore architecture for deep neural network based processing.
Memristor crossbars are utilized to provide low power high throughput execution
of neural networks. The system has both training and recognition (evaluation of
new input) capabilities. The proposed system could be used for classification,
dimensionality reduction, feature extraction, and anomaly detection
applications. The system level area and power benefits of the specialized
architecture is compared with the NVIDIA Telsa K20 GPGPU. Our experimental
evaluations show that the proposed architecture can provide up to five orders
of magnitude more energy efficiency over GPGPUs for deep neural network
processing.
</summary>
    <author>
      <name>Raqibul Hasan</name>
    </author>
    <author>
      <name>Tarek Taha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.07400v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07400v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04382v1</id>
    <updated>2016-04-15T07:32:18Z</updated>
    <published>2016-04-15T07:32:18Z</published>
    <title>Precomputed Real-Time Texture Synthesis with Markovian Generative
  Adversarial Networks</title>
    <summary>  This paper proposes Markovian Generative Adversarial Networks (MGANs), a
method for training generative neural networks for efficient texture synthesis.
While deep neural network approaches have recently demonstrated remarkable
results in terms of synthesis quality, they still come at considerable
computational costs (minutes of run-time for low-res images). Our paper
addresses this efficiency issue. Instead of a numerical deconvolution in
previous work, we precompute a feed-forward, strided convolutional network that
captures the feature statistics of Markovian patches and is able to directly
generate outputs of arbitrary dimensions. Such network can directly decode
brown noise to realistic texture, or photos to artistic paintings. With
adversarial training, we obtain quality comparable to recent neural texture
synthesis methods. As no optimization is required any longer at generation
time, our run-time performance (0.25M pixel images at 25Hz) surpasses previous
neural texture synthesizers by a significant margin (at least 500 times
faster). We apply this idea to texture synthesis, style transfer, and video
stylization.
</summary>
    <author>
      <name>Chuan Li</name>
    </author>
    <author>
      <name>Michael Wand</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.04382v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04382v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.06490v1</id>
    <updated>2016-09-21T10:28:57Z</updated>
    <published>2016-09-21T10:28:57Z</published>
    <title>One Sentence One Model for Neural Machine Translation</title>
    <summary>  Neural machine translation (NMT) becomes a new state-of-the-art and achieves
promising translation results using a simple encoder-decoder neural network.
This neural network is trained once on the parallel corpus and the fixed
network is used to translate all the test sentences. We argue that the general
fixed network cannot best fit the specific test sentences. In this paper, we
propose the dynamic NMT which learns a general network as usual, and then
fine-tunes the network for each test sentence. The fine-tune work is done on a
small set of the bilingual training data that is obtained through similarity
search according to the test sentence. Extensive experiments demonstrate that
this method can significantly improve the translation performance, especially
when highly similar sentences are available.
</summary>
    <author>
      <name>Xiaoqing Li</name>
    </author>
    <author>
      <name>Jiajun Zhang</name>
    </author>
    <author>
      <name>Chengqing Zong</name>
    </author>
    <link href="http://arxiv.org/abs/1609.06490v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.06490v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.09639v1</id>
    <updated>2016-10-30T11:57:20Z</updated>
    <published>2016-10-30T11:57:20Z</published>
    <title>Compact Deep Convolutional Neural Networks With Coarse Pruning</title>
    <summary>  The learning capability of a neural network improves with increasing depth at
higher computational costs. Wider layers with dense kernel connectivity
patterns furhter increase this cost and may hinder real-time inference. We
propose feature map and kernel level pruning for reducing the computational
complexity of a deep convolutional neural network. Pruning feature maps reduces
the width of a layer and hence does not need any sparse representation.
Further, kernel pruning converts the dense connectivity pattern into a sparse
one. Due to coarse nature, these pruning granularities can be exploited by GPUs
and VLSI based implementations. We propose a simple and generic strategy to
choose the least adversarial pruning masks for both granularities. The pruned
networks are retrained which compensates the loss in accuracy. We obtain the
best pruning ratios when we prune a network with both granularities.
Experiments with the CIFAR-10 dataset show that more than 85% sparsity can be
induced in the convolution layers with less than 1% increase in the
missclassification rate of the baseline network.
</summary>
    <author>
      <name>Sajid Anwar</name>
    </author>
    <author>
      <name>Wonyong Sung</name>
    </author>
    <link href="http://arxiv.org/abs/1610.09639v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.09639v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.03928v3</id>
    <updated>2017-02-12T22:05:47Z</updated>
    <published>2016-12-12T21:15:57Z</published>
    <title>Paying More Attention to Attention: Improving the Performance of
  Convolutional Neural Networks via Attention Transfer</title>
    <summary>  Attention plays a critical role in human visual experience. Furthermore, it
has recently been demonstrated that attention can also play an important role
in the context of applying artificial neural networks to a variety of tasks
from fields such as computer vision and NLP. In this work we show that, by
properly defining attention for convolutional neural networks, we can actually
use this type of information in order to significantly improve the performance
of a student CNN network by forcing it to mimic the attention maps of a
powerful teacher network. To that end, we propose several novel methods of
transferring attention, showing consistent improvement across a variety of
datasets and convolutional neural network architectures. Code and models for
our experiments are available at
https://github.com/szagoruyko/attention-transfer
</summary>
    <author>
      <name>Sergey Zagoruyko</name>
    </author>
    <author>
      <name>Nikos Komodakis</name>
    </author>
    <link href="http://arxiv.org/abs/1612.03928v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.03928v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.01769v2</id>
    <updated>2017-02-24T18:31:18Z</updated>
    <published>2017-01-06T22:16:07Z</published>
    <title>Associative pattern recognition through macro-molecular self-assembly</title>
    <summary>  We show that macro-molecular self-assembly can recognize and classify
high-dimensional patterns in the concentrations of $N$ distinct molecular
species. Similar to associative neural networks, the recognition here leverages
dynamical attractors to recognize and reconstruct partially corrupted patterns.
Traditional parameters of pattern recognition theory, such as sparsity,
fidelity, and capacity are related to physical parameters, such as nucleation
barriers, interaction range, and non-equilibrium assembly forces. Notably, we
find that self-assembly bears greater similarity to continuous attractor neural
networks, such as place cell networks that store spatial memories, rather than
discrete memory networks. This relationship suggests that features and
trade-offs seen here are not tied to details of self-assembly or neural network
models but are instead intrinsic to associative pattern recognition carried out
through short-ranged interactions.
</summary>
    <author>
      <name>Weishun Zhong</name>
    </author>
    <author>
      <name>David J. Schwab</name>
    </author>
    <author>
      <name>Arvind Murugan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10955-017-1774-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10955-017-1774-2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 7 figures; reference added</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.01769v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.01769v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05056v1</id>
    <updated>2017-01-18T13:29:43Z</updated>
    <published>2017-01-18T13:29:43Z</published>
    <title>Synchronization and long-time memory in neural networks with inhibitory
  hubs and synaptic plasticity</title>
    <summary>  We investigate the dynamical role of inhibitory and highly connected nodes
(hub) in synchronization and input processing of leaky-integrate-and-fire
neural networks with short term synaptic plasticity. We take advantage of a
heterogeneous mean-field approximation to encode the role of network structure
and we tune the fraction of inhibitory neurons $f_I$ and their connectivity
level to investigate the cooperation between hub features and inhibition. We
show that, depending on $f_I$, highly connected inhibitory nodes strongly drive
the synchronization properties of the overall network through dynamical
transitions from synchronous to asynchronous regimes. Furthermore, a metastable
regime with long memory of external inputs emerges for a specific fraction of
hub inhibitory neurons, underlining the role of inhibition and connectivity
also for input processing in neural networks.
</summary>
    <author>
      <name>Elena Bertolotti</name>
    </author>
    <author>
      <name>Raffaella Burioni</name>
    </author>
    <author>
      <name>Matteo di Volo</name>
    </author>
    <author>
      <name>Alessandro Vezzani</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.95.012308</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.95.012308" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 11 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 95, 012308 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1701.05056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.02359v1</id>
    <updated>2017-02-08T10:30:32Z</updated>
    <published>2017-02-08T10:30:32Z</published>
    <title>Multi-scale Convolutional Neural Networks for Crowd Counting</title>
    <summary>  Crowd counting on static images is a challenging problem due to scale
variations. Recently deep neural networks have been shown to be effective in
this task. However, existing neural-networks-based methods often use the
multi-column or multi-network model to extract the scale-relevant features,
which is more complicated for optimization and computation wasting. To this
end, we propose a novel multi-scale convolutional neural network (MSCNN) for
single image crowd counting. Based on the multi-scale blobs, the network is
able to generate scale-relevant features for higher crowd counting performances
in a single-column architecture, which is both accuracy and cost effective for
practical applications. Complemental results show that our method outperforms
the state-of-the-art methods on both accuracy and robustness with far less
number of parameters.
</summary>
    <author>
      <name>Lingke Zeng</name>
    </author>
    <author>
      <name>Xiangmin Xu</name>
    </author>
    <author>
      <name>Bolun Cai</name>
    </author>
    <author>
      <name>Suo Qiu</name>
    </author>
    <author>
      <name>Tong Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1702.02359v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.02359v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.04122v3</id>
    <updated>2018-02-24T16:29:19Z</updated>
    <published>2017-03-12T14:03:19Z</published>
    <title>Autoregressive Convolutional Neural Networks for Asynchronous Time
  Series</title>
    <summary>  We propose Significance-Offset Convolutional Neural Network, a deep
convolutional network architecture for regression of multivariate asynchronous
time series. The model is inspired by standard autoregressive (AR) models and
gating mechanisms used in recurrent neural networks. It involves an AR-like
weighting system, where the final predictor is obtained as a weighted sum of
adjusted regressors, while the weights are data-dependent functions learnt
through a convolutional network. The architecture was designed for applications
on asynchronous time series and is evaluated on such datasets: a hedge fund
proprietary dataset of over 2 million quotes for a credit derivative index, an
artificially generated noisy autoregressive series and household electricity
consumption dataset. The proposed architecture achieves promising results as
compared to convolutional and recurrent neural networks.
</summary>
    <author>
      <name>Miko≈Çaj Bi≈Ñkowski</name>
    </author>
    <author>
      <name>Gautier Marti</name>
    </author>
    <author>
      <name>Philippe Donnat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review by the 35th International Conference on Machine Learning</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.04122v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.04122v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.07914v2</id>
    <updated>2017-07-11T23:02:09Z</updated>
    <published>2017-03-23T03:16:19Z</published>
    <title>Why do similarity matching objectives lead to Hebbian/anti-Hebbian
  networks?</title>
    <summary>  Modeling self-organization of neural networks for unsupervised learning using
Hebbian and anti-Hebbian plasticity has a long history in neuroscience. Yet,
derivations of single-layer networks with such local learning rules from
principled optimization objectives became possible only recently, with the
introduction of similarity matching objectives. What explains the success of
similarity matching objectives in deriving neural networks with local learning
rules? Here, using dimensionality reduction as an example, we introduce several
variable substitutions that illuminate the success of similarity matching. We
show that the full network objective may be optimized separately for each
synapse using local learning rules both in the offline and online settings. We
formalize the long-standing intuition of the rivalry between Hebbian and
anti-Hebbian rules by formulating a min-max optimization problem. We introduce
a novel dimensionality reduction objective using fractional matrix exponents.
To illustrate the generality of our approach, we apply it to a novel
formulation of dimensionality reduction combined with whitening. We confirm
numerically that the networks with learning rules derived from principled
objectives perform better than those with heuristic learning rules.
</summary>
    <author>
      <name>Cengiz Pehlevan</name>
    </author>
    <author>
      <name>Anirvan Sengupta</name>
    </author>
    <author>
      <name>Dmitri B. Chklovskii</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/neco_a_01018</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/neco_a_01018" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in Neural Computation</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.07914v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.07914v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07565v2</id>
    <updated>2017-11-09T23:50:55Z</updated>
    <published>2017-05-22T05:54:37Z</published>
    <title>Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain
  Surgeon</title>
    <summary>  How to develop slim and accurate deep neural networks has become crucial for
real- world applications, especially for those employed in embedded systems.
Though previous work along this research line has shown some promising results,
most existing methods either fail to significantly compress a well-trained deep
network or require a heavy retraining process for the pruned deep network to
re-boost its prediction performance. In this paper, we propose a new layer-wise
pruning method for deep neural networks. In our proposed method, parameters of
each individual layer are pruned independently based on second order
derivatives of a layer-wise error function with respect to the corresponding
parameters. We prove that the final prediction performance drop after pruning
is bounded by a linear combination of the reconstructed errors caused at each
layer. Therefore, there is a guarantee that one only needs to perform a light
retraining process on the pruned network to resume its original prediction
performance. We conduct extensive experiments on benchmark datasets to
demonstrate the effectiveness of our pruning method compared with several
state-of-the-art baseline methods.
</summary>
    <author>
      <name>Xin Dong</name>
    </author>
    <author>
      <name>Shangyu Chen</name>
    </author>
    <author>
      <name>Sinno Jialin Pan</name>
    </author>
    <link href="http://arxiv.org/abs/1705.07565v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07565v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.00536v2</id>
    <updated>2017-12-30T08:08:50Z</updated>
    <published>2017-06-02T02:10:39Z</published>
    <title>Modeling Latent Attention Within Neural Networks</title>
    <summary>  Deep neural networks are able to solve tasks across a variety of domains and
modalities of data. Despite many empirical successes, we lack the ability to
clearly understand and interpret the learned internal mechanisms that
contribute to such effective behaviors or, more critically, failure modes. In
this work, we present a general method for visualizing an arbitrary neural
network's inner mechanisms and their power and limitations. Our dataset-centric
method produces visualizations of how a trained network attends to components
of its inputs. The computed "attention masks" support improved interpretability
by highlighting which input attributes are critical in determining output. We
demonstrate the effectiveness of our framework on a variety of deep neural
network architectures in domains from computer vision, natural language
processing, and reinforcement learning. The primary contribution of our
approach is an interpretable visualization of attention that provides unique
insights into the network's underlying decision-making process irrespective of
the data modality.
</summary>
    <author>
      <name>Christopher Grimm</name>
    </author>
    <author>
      <name>Dilip Arumugam</name>
    </author>
    <author>
      <name>Siddharth Karamcheti</name>
    </author>
    <author>
      <name>David Abel</name>
    </author>
    <author>
      <name>Lawson L. S. Wong</name>
    </author>
    <author>
      <name>Michael L. Littman</name>
    </author>
    <link href="http://arxiv.org/abs/1706.00536v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.00536v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.01831v1</id>
    <updated>2017-06-06T16:08:18Z</updated>
    <published>2017-06-06T16:08:18Z</published>
    <title>Information Bottleneck in Control Tasks with Recurrent Spiking Neural
  Networks</title>
    <summary>  The nervous system encodes continuous information from the environment in the
form of discrete spikes, and then decodes these to produce smooth motor
actions. Understanding how spikes integrate, represent, and process information
to produce behavior is one of the greatest challenges in neuroscience.
Information theory has the potential to help us address this challenge.
Informational analyses of deep and feed-forward artificial neural networks
solving static input-output tasks, have led to the proposal of the
\emph{Information Bottleneck} principle, which states that deeper layers encode
more relevant yet minimal information about the inputs. Such an analyses on
networks that are recurrent, spiking, and perform control tasks is relatively
unexplored. Here, we present results from a Mutual Information analysis of a
recurrent spiking neural network that was evolved to perform the classic
pole-balancing task. Our results show that these networks deviate from the
\emph{Information Bottleneck} principle prescribed for feed-forward networks.
</summary>
    <author>
      <name>Madhavun Candadai Vasu</name>
    </author>
    <author>
      <name>Eduardo Izquierdo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-68600-4_28</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-68600-4_28" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ICANN'17 to appear in Springer Lecture Notes in Computer
  Science</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICANN 2017. Lecture Notes in Computer Science, vol 10613.
  Springer, Cham</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.01831v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.01831v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09283v2</id>
    <updated>2018-02-27T00:41:01Z</updated>
    <published>2017-06-27T03:01:37Z</published>
    <title>Entropy bifurcation of neural networks on Cayley trees</title>
    <summary>  It has been demonstrated that excitable media with a tree structure performed
better than other network topologies, it is natural to consider neural networks
defined on Cayley trees. The investigation of a symbolic space called
tree-shift of finite type is important when it comes to the discussion of the
equilibrium solutions of neural networks on Cayley trees. Entropy is a
frequently used invariant for measuring the complexity of a system, and
constant entropy for an open set of coupling weights between neurons means that
the specific network is stable. This paper gives a complete characterization
for entropy spectrum of neural networks on Cayley trees and reveals whether the
entropy bifurcates when the coupling weights change.
</summary>
    <author>
      <name>Jung-Chao Ban</name>
    </author>
    <author>
      <name>Chih-Hung Chang</name>
    </author>
    <author>
      <name>Nai-Zhu Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1701.05113</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.09283v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09283v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="37A35, 37B10, 92B20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09713v1</id>
    <updated>2017-06-29T12:23:06Z</updated>
    <published>2017-06-29T12:23:06Z</published>
    <title>Thermodynamic efficiency of learning a rule in neural networks</title>
    <summary>  Biological systems have to build models from their sensory data that allow
them to efficiently process previously unseen inputs. Here, we study a neural
network learning a linearly separable rule using examples provided by a
teacher. We analyse the ability of the network to apply the rule to new inputs,
that is to generalise from past experience. Using stochastic thermodynamics, we
show that the thermodynamic costs of the learning process provide an upper
bound on the amount of information that the network is able to learn from its
teacher for both batch and online learning. This allows us to introduce a
thermodynamic efficiency of learning. We analytically compute the dynamics and
the efficiency of a noisy neural network performing online learning in the
thermodynamic limit. In particular, we analyse three popular learning
algorithms, namely Hebbian, Perceptron and AdaTron learning. Our work extends
the methods of stochastic thermodynamics to a new type of learning problem and
might form a suitable basis for investigating the thermodynamics of
decision-making.
</summary>
    <author>
      <name>Sebastian Goldt</name>
    </author>
    <author>
      <name>Udo Seifert</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1367-2630/aa89ff</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1367-2630/aa89ff" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures; 7 pages of supplemental material with one figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.09713v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09713v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.02194v1</id>
    <updated>2017-09-07T11:54:36Z</updated>
    <published>2017-09-07T11:54:36Z</published>
    <title>Approximating meta-heuristics with homotopic recurrent neural networks</title>
    <summary>  Much combinatorial optimisation problems constitute a non-polynomial (NP)
hard optimisation problem, i.e., they can not be solved in polynomial time. One
such problem is finding the shortest route between two nodes on a graph.
Meta-heuristic algorithms such as $A^{*}$ along with mixed-integer programming
(MIP) methods are often employed for these problems. Our work demonstrates that
it is possible to approximate solutions generated by a meta-heuristic algorithm
using a deep recurrent neural network. We compare different methodologies based
on reinforcement learning (RL) and recurrent neural networks (RNN) to gauge
their respective quality of approximation. We show the viability of recurrent
neural network solutions on a graph that has over 300 nodes and argue that a
sequence-to-sequence network rather than other recurrent networks has improved
approximation quality. Additionally, we argue that homotopy continuation --
that increases chances of hitting an extremum -- further improves the estimate
generated by a vanilla RNN.
</summary>
    <author>
      <name>Alessandro Bay</name>
    </author>
    <author>
      <name>Biswa Sengupta</name>
    </author>
    <link href="http://arxiv.org/abs/1709.02194v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.02194v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.07646v3</id>
    <updated>2017-10-03T04:43:34Z</updated>
    <published>2017-09-22T09:24:08Z</published>
    <title>SwGridNet: A Deep Convolutional Neural Network based on Grid Topology
  for Image Classification</title>
    <summary>  Deep convolutional neural networks (CNNs) achieve remarkable performance on
image classification tasks. Recent studies, however, have demonstrated that
generalization abilities are more important than the depth of neural networks
for improving performance on image classification tasks. Herein, a new neural
network called SwGridNet is proposed. A SwGridNet includes many convolutional
processing units which connect mutually as a grid network where many processing
paths exist between input and output. A SwGridNet has high generalization
capability because the multipath architecture has the same effect of ensemble
learning. As described in this paper, details of the SwGridNet network
architecture are presented. Experimentally obtained results presented in this
paper show that SwGridNets respectively achieve test error rates of 2.95% and
15.67% in a CIFAR-10 and CIFAR-100 classification tasks. The results indicate
that the SwGridNet performance approximates that of state-of-the-art deep CNNs.
</summary>
    <author>
      <name>Atsushi Takeda</name>
    </author>
    <link href="http://arxiv.org/abs/1709.07646v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.07646v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.10036v3</id>
    <updated>2018-01-02T01:06:10Z</updated>
    <published>2017-10-27T09:11:26Z</published>
    <title>Generalization Tower Network: A Novel Deep Neural Network Architecture
  for Multi-Task Learning</title>
    <summary>  Deep learning (DL) advances state-of-the-art reinforcement learning (RL), by
incorporating deep neural networks in learning representations from the input
to RL. However, the conventional deep neural network architecture is limited in
learning representations for multi-task RL (MT-RL), as multiple tasks can refer
to different kinds of representations. In this paper, we thus propose a novel
deep neural network architecture, namely generalization tower network (GTN),
which can achieve MT-RL within a single learned model. Specifically, the
architecture of GTN is composed of both horizontal and vertical streams. In our
GTN architecture, horizontal streams are used to learn representation shared in
similar tasks. In contrast, the vertical streams are introduced to be more
suitable for handling diverse tasks, which encodes hierarchical shared
knowledge of these tasks. The effectiveness of the introduced vertical stream
is validated by experimental results. Experimental results further verify that
our GTN architecture is able to advance the state-of-the-art MT-RL, via being
tested on 51 Atari games.
</summary>
    <author>
      <name>Yuhang Song</name>
    </author>
    <author>
      <name>Main Xu</name>
    </author>
    <author>
      <name>Songyang Zhang</name>
    </author>
    <author>
      <name>Liangyu Huo</name>
    </author>
    <link href="http://arxiv.org/abs/1710.10036v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.10036v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.11205v1</id>
    <updated>2017-10-30T19:18:43Z</updated>
    <published>2017-10-30T19:18:43Z</published>
    <title>Critical Points of Neural Networks: Analytical Forms and Landscape
  Properties</title>
    <summary>  Due to the success of deep learning to solving a variety of challenging
machine learning tasks, there is a rising interest in understanding loss
functions for training neural networks from a theoretical aspect. Particularly,
the properties of critical points and the landscape around them are of
importance to determine the convergence performance of optimization algorithms.
In this paper, we provide full (necessary and sufficient) characterization of
the analytical forms for the critical points (as well as global minimizers) of
the square loss functions for various neural networks. We show that the
analytical forms of the critical points characterize the values of the
corresponding loss functions as well as the necessary and sufficient conditions
to achieve global minimum. Furthermore, we exploit the analytical forms of the
critical points to characterize the landscape properties for the loss functions
of these neural networks. One particular conclusion is that: The loss function
of linear networks has no spurious local minimum, while the loss function of
one-hidden-layer nonlinear networks with ReLU activation function does have
local minimum that is not global minimum.
</summary>
    <author>
      <name>Yi Zhou</name>
    </author>
    <author>
      <name>Yingbin Liang</name>
    </author>
    <link href="http://arxiv.org/abs/1710.11205v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.11205v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.11565v3</id>
    <updated>2018-02-26T09:04:36Z</updated>
    <published>2017-11-30T18:35:22Z</published>
    <title>Deep Neural Networks for Multiple Speaker Detection and Localization</title>
    <summary>  We propose to use neural networks for simultaneous detection and localization
of multiple sound sources in human-robot interaction. In contrast to
conventional signal processing techniques, neural network-based sound source
localization methods require fewer strong assumptions about the environment.
Previous neural network-based methods have been focusing on localizing a single
sound source, which do not extend to multiple sources in terms of detection and
localization. In this paper, we thus propose a likelihood-based encoding of the
network output, which naturally allows the detection of an arbitrary number of
sources. In addition, we investigate the use of sub-band cross-correlation
information as features for better localization in sound mixtures, as well as
three different network architectures based on different motivations.
Experiments on real data recorded from a robot show that our proposed methods
significantly outperform the popular spatial spectrum-based approaches.
</summary>
    <author>
      <name>Weipeng He</name>
    </author>
    <author>
      <name>Petr Motlicek</name>
    </author>
    <author>
      <name>Jean-Marc Odobez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for ICRA 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.11565v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.11565v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.00508v1</id>
    <updated>2018-01-01T20:54:33Z</updated>
    <published>2018-01-01T20:54:33Z</published>
    <title>Depth-Adaptive Computational Policies for Efficient Visual Tracking</title>
    <summary>  Current convolutional neural networks algorithms for video object tracking
spend the same amount of computation for each object and video frame. However,
it is harder to track an object in some frames than others, due to the varying
amount of clutter, scene complexity, amount of motion, and object's
distinctiveness against its background. We propose a depth-adaptive
convolutional Siamese network that performs video tracking adaptively at
multiple neural network depths. Parametric gating functions are trained to
control the depth of the convolutional feature extractor by minimizing a joint
loss of computational cost and tracking error. Our network achieves accuracy
comparable to the state-of-the-art on the VOT2016 benchmark. Furthermore, our
adaptive depth computation achieves higher accuracy for a given computational
cost than traditional fixed-structure neural networks. The presented framework
extends to other tasks that use convolutional neural networks and enables
trading speed for accuracy at runtime.
</summary>
    <author>
      <name>Chris Ying</name>
    </author>
    <author>
      <name>Katerina Fragkiadaki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">presented at EMMCVPR 2017 in Venice, Italy</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.00508v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.00508v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.09053v1</id>
    <updated>2018-01-27T08:33:22Z</updated>
    <published>2018-01-27T08:33:22Z</published>
    <title>Combining Convolution and Recursive Neural Networks for Sentiment
  Analysis</title>
    <summary>  This paper addresses the problem of sentence-level sentiment analysis. In
recent years, Convolution and Recursive Neural Networks have been proven to be
effective network architecture for sentence-level sentiment analysis.
Nevertheless, each of them has their own potential drawbacks. For alleviating
their weaknesses, we combined Convolution and Recursive Neural Networks into a
new network architecture. In addition, we employed transfer learning from a
large document-level labeled sentiment dataset to improve the word embedding in
our models. The resulting models outperform all recent Convolution and
Recursive Neural Networks. Beyond that, our models achieve comparable
performance with state-of-the-art systems on Stanford Sentiment Treebank.
</summary>
    <author>
      <name>Vinh D. Van</name>
    </author>
    <author>
      <name>Thien Thai</name>
    </author>
    <author>
      <name>Minh-Quoc Nghiem</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3155133.3155158</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3155133.3155158" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures, Proceedings of the Eighth International Symposium
  on Information and Communication Technology. ACM, 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.09053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.09053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00086v1</id>
    <updated>2018-01-31T22:14:09Z</updated>
    <published>2018-01-31T22:14:09Z</published>
    <title>Optimizing Non-decomposable Measures with Deep Networks</title>
    <summary>  We present a class of algorithms capable of directly training deep neural
networks with respect to large families of task-specific performance measures
such as the F-measure and the Kullback-Leibler divergence that are structured
and non-decomposable. This presents a departure from standard deep learning
techniques that typically use squared or cross-entropy loss functions (that are
decomposable) to train neural networks. We demonstrate that directly training
with task-specific loss functions yields much faster and more stable
convergence across problems and datasets. Our proposed algorithms and
implementations have several novel features including (i) convergence to first
order stationary points despite optimizing complex objective functions; (ii)
use of fewer training samples to achieve a desired level of convergence, (iii)
a substantial reduction in training time, and (iv) a seamless integration of
our implementation into existing symbolic gradient frameworks. We implement our
techniques on a variety of deep architectures including multi-layer perceptrons
and recurrent neural networks and show that on a variety of benchmark and real
data sets, our algorithms outperform traditional approaches to training deep
networks, as well as some recent approaches to task-specific training of neural
networks.
</summary>
    <author>
      <name>Amartya Sanyal</name>
    </author>
    <author>
      <name>Pawan Kumar</name>
    </author>
    <author>
      <name>Purushottam Kar</name>
    </author>
    <author>
      <name>Sanjay Chawla</name>
    </author>
    <author>
      <name>Fabrizio Sebastiani</name>
    </author>
    <link href="http://arxiv.org/abs/1802.00086v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00086v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.01240v1</id>
    <updated>2018-02-05T02:42:39Z</updated>
    <published>2018-02-05T02:42:39Z</published>
    <title>Enhancing Multi-Class Classification of Random Forest using Random
  Vector Functional Neural Network and Oblique Decision Surfaces</title>
    <summary>  Both neural networks and decision trees are popular machine learning methods
and are widely used to solve problems from diverse domains. These two
classifiers are commonly used base classifiers in an ensemble framework. In
this paper, we first present a new variant of oblique decision tree based on a
linear classifier, then construct an ensemble classifier based on the fusion of
a fast neural network, random vector functional link network and oblique
decision trees. Random Vector Functional Link Network has an elegant closed
form solution with extremely short training time. The neural network partitions
each training bag (obtained using bagging) at the root level into C subsets
where C is the number of classes in the dataset and subsequently, C oblique
decision trees are trained on such partitions. The proposed method provides a
rich insight into the data by grouping the confusing or hard to classify
samples for each class and thus, provides an opportunity to employ fine-grained
classification rule over the data. The performance of the ensemble classifier
is evaluated on several multi-class datasets where it demonstrates a superior
performance compared to other state-of- the-art classifiers.
</summary>
    <author>
      <name>Rakesh Katuwal</name>
    </author>
    <author>
      <name>P. N. Suganthan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.01240v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.01240v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.02629v1</id>
    <updated>2018-02-07T20:59:39Z</updated>
    <published>2018-02-07T20:59:39Z</published>
    <title>Spatially adaptive image compression using a tiled deep network</title>
    <summary>  Deep neural networks represent a powerful class of function approximators
that can learn to compress and reconstruct images. Existing image compression
algorithms based on neural networks learn quantized representations with a
constant spatial bit rate across each image. While entropy coding introduces
some spatial variation, traditional codecs have benefited significantly by
explicitly adapting the bit rate based on local image complexity and visual
saliency. This paper introduces an algorithm that combines deep neural networks
with quality-sensitive bit rate adaptation using a tiled network. We
demonstrate the importance of spatial context prediction and show improved
quantitative (PSNR) and qualitative (subjective rater assessment) results
compared to a non-adaptive baseline and a recently published image compression
model based on fully-convolutional neural networks.
</summary>
    <author>
      <name>David Minnen</name>
    </author>
    <author>
      <name>George Toderici</name>
    </author>
    <author>
      <name>Michele Covell</name>
    </author>
    <author>
      <name>Troy Chinen</name>
    </author>
    <author>
      <name>Nick Johnston</name>
    </author>
    <author>
      <name>Joel Shor</name>
    </author>
    <author>
      <name>Sung Jin Hwang</name>
    </author>
    <author>
      <name>Damien Vincent</name>
    </author>
    <author>
      <name>Saurabh Singh</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Image Processing 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1802.02629v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.02629v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.3342v1</id>
    <updated>2009-07-20T05:58:24Z</updated>
    <published>2009-07-20T05:58:24Z</published>
    <title>Neural Modeling and Control of Diesel Engine with Pollution Constraints</title>
    <summary>  The paper describes a neural approach for modelling and control of a
turbocharged Diesel engine. A neural model, whose structure is mainly based on
some physical equations describing the engine behaviour, is built for the
rotation speed and the exhaust gas opacity. The model is composed of three
interconnected neural submodels, each of them constituting a nonlinear
multi-input single-output error model. The structural identification and the
parameter estimation from data gathered on a real engine are described. The
neural direct model is then used to determine a neural controller of the
engine, in a specialized training scheme minimising a multivariable criterion.
Simulations show the effect of the pollution constraint weighting on a
trajectory tracking of the engine speed. Neural networks, which are flexible
and parsimonious nonlinear black-box models, with universal approximation
capabilities, can accurately describe or control complex nonlinear systems,
with little a priori theoretical knowledge. The presented work extends optimal
neuro-control to the multivariable case and shows the ?exibility of neural
optimisers. Considering the preliminary results, it appears that neural
networks can be used as embedded models for engine control, to satisfy the more
and more restricting pollutant emission legislation. Particularly, they are
able to model nonlinear dynamics and outperform during transients the control
schemes based on static mappings.
</summary>
    <author>
      <name>Mustapha Ouladsine</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LSIS</arxiv:affiliation>
    </author>
    <author>
      <name>G√©rard Bloch</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRAN</arxiv:affiliation>
    </author>
    <author>
      <name>Xavier Dovifaaz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRAN</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10846-005-3806-y</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10846-005-3806-y" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Intelligent and Robotic Systems 41, 2-3 (2005) 157-171</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0907.3342v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.3342v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/nlin/0607058v1</id>
    <updated>2006-07-26T04:40:14Z</updated>
    <published>2006-07-26T04:40:14Z</published>
    <title>An Artificial Neural Net approach to forecast the population of India</title>
    <summary>  Present paper endeavors to forecast the population of India through
Artificial Neural Network. A non-linear Artificial Neural Net model has been
developed and the prediction has been found to be sufficiently accurate. It has
been found that the model is performing more efficiently in predicting female
population than the male population. Results have been presented graphically.
</summary>
    <author>
      <name>Goutami Bandyopadhyay</name>
    </author>
    <author>
      <name>Surajit Chattopadhyay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/nlin/0607058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/nlin/0607058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.1986v1</id>
    <updated>2014-08-08T21:24:59Z</updated>
    <published>2014-08-08T21:24:59Z</published>
    <title>Gabor-like Image Filtering using a Neural Microcircuit</title>
    <summary>  In this letter, we present an implementation of a neural microcircuit for
image processing employing Hebbian-adaptive learning. The neuronal circuit
utilizes only excitatory synapses to correlate action potentials, extracting
the uncorrelated ones, which contain significant image information. This
circuit is capable of approximating Gabor-like image filtering and other image
processing functions
</summary>
    <author>
      <name>C. Mayr</name>
    </author>
    <author>
      <name>A. Heittmann</name>
    </author>
    <author>
      <name>R. Sch√ºffny</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TNN.2007.891687</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TNN.2007.891687" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Neural Networks, vol. 18, pages 955-959, 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1408.1986v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.1986v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.07809v1</id>
    <updated>2017-09-22T15:28:24Z</updated>
    <published>2017-09-22T15:28:24Z</published>
    <title>Neural Machine Translation</title>
    <summary>  Draft of textbook chapter on neural machine translation. a comprehensive
treatment of the topic, ranging from introduction to neural networks,
computation graphs, description of the currently dominant attentional
sequence-to-sequence model, recent refinements, alternative architectures and
challenges. Written as chapter for the textbook Statistical Machine
Translation. Used in the JHU Fall 2017 class on machine translation.
</summary>
    <author>
      <name>Philipp Koehn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">100+ pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.07809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.07809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.0031v3</id>
    <updated>2013-05-16T20:44:24Z</updated>
    <published>2012-11-30T22:43:11Z</published>
    <title>Encoding binary neural codes in networks of threshold-linear neurons</title>
    <summary>  Networks of neurons in the brain encode preferred patterns of neural activity
via their synaptic connections. Despite receiving considerable attention, the
precise relationship between network connectivity and encoded patterns is still
poorly understood. Here we consider this problem for networks of
threshold-linear neurons whose computational function is to learn and store a
set of binary patterns (e.g., a neural code) as "permitted sets" of the
network. We introduce a simple Encoding Rule that selectively turns "on"
synapses between neurons that co-appear in one or more patterns. The rule uses
synapses that are binary, in the sense of having only two states ("on" or
"off"), but also heterogeneous, with weights drawn from an underlying synaptic
strength matrix S. Our main results precisely describe the stored patterns that
result from the Encoding Rule -- including unintended "spurious" states -- and
give an explicit characterization of the dependence on S. In particular, we
find that binary patterns are successfully stored in these networks when the
excitatory connections between neurons are geometrically balanced -- i.e., they
satisfy a set of geometric constraints. Furthermore, we find that certain types
of neural codes are "natural" in the context of these networks, meaning that
the full code can be accurately learned from a highly undersampled set of
patterns. Interestingly, many commonly observed neural codes in cortical and
hippocampal areas are natural in this sense. As an application, we construct
networks that encode hippocampal place field codes nearly exactly, following
presentation of only a small fraction of patterns. To obtain our results, we
prove new theorems using classical ideas from convex and distance geometry,
such as Cayley-Menger determinants, revealing a novel connection between these
areas of mathematics and coding properties of neural networks.
</summary>
    <author>
      <name>Carina Curto</name>
    </author>
    <author>
      <name>Anda Degeratu</name>
    </author>
    <author>
      <name>Vladimir Itskov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 5 figures. Minor revisions only. Accepted to Neural
  Computation</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Computation, Vol 25, pp. 2858-2903, 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1212.0031v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.0031v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.01215v3</id>
    <updated>2017-11-22T23:27:25Z</updated>
    <published>2017-06-05T06:55:44Z</published>
    <title>DeepIoT: Compressing Deep Neural Network Structures for Sensing Systems
  with a Compressor-Critic Framework</title>
    <summary>  Recent advances in deep learning motivate the use of deep neutral networks in
sensing applications, but their excessive resource needs on constrained
embedded devices remain an important impediment. A recently explored solution
space lies in compressing (approximating or simplifying) deep neural networks
in some manner before use on the device. We propose a new compression solution,
called DeepIoT, that makes two key contributions in that space. First, unlike
current solutions geared for compressing specific types of neural networks,
DeepIoT presents a unified approach that compresses all commonly used deep
learning structures for sensing applications, including fully-connected,
convolutional, and recurrent neural networks, as well as their combinations.
Second, unlike solutions that either sparsify weight matrices or assume linear
structure within weight matrices, DeepIoT compresses neural network structures
into smaller dense matrices by finding the minimum number of non-redundant
hidden elements, such as filters and dimensions required by each layer, while
keeping the performance of sensing applications the same. Importantly, it does
so using an approach that obtains a global view of parameter redundancies,
which is shown to produce superior compression. We conduct experiments with
five different sensing-related tasks on Intel Edison devices. DeepIoT
outperforms all compared baseline algorithms with respect to execution time and
energy consumption by a significant margin. It reduces the size of deep neural
networks by 90% to 98.9%. It is thus able to shorten execution time by 71.4% to
94.5%, and decrease energy consumption by 72.2% to 95.7%. These improvements
are achieved without loss of accuracy. The results underscore the potential of
DeepIoT for advancing the exploitation of deep neural networks on
resource-constrained embedded devices.
</summary>
    <author>
      <name>Shuochao Yao</name>
    </author>
    <author>
      <name>Yiran Zhao</name>
    </author>
    <author>
      <name>Aston Zhang</name>
    </author>
    <author>
      <name>Lu Su</name>
    </author>
    <author>
      <name>Tarek Abdelzaher</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in SenSys2017. Code is available on
  https://github.com/yscacaca/DeepIoT</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.01215v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.01215v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0305024v1</id>
    <updated>2003-05-16T15:54:46Z</updated>
    <published>2003-05-16T15:54:46Z</published>
    <title>A neural network and iterative optimization hybrid for Dempster-Shafer
  clustering</title>
    <summary>  In this paper we extend an earlier result within Dempster-Shafer theory
["Fast Dempster-Shafer Clustering Using a Neural Network Structure," in Proc.
Seventh Int. Conf. Information Processing and Management of Uncertainty in
Knowledge-Based Systems (IPMU 98)] where a large number of pieces of evidence
are clustered into subsets by a neural network structure. The clustering is
done by minimizing a metaconflict function. Previously we developed a method
based on iterative optimization. While the neural method had a much lower
computation time than iterative optimization its average clustering performance
was not as good. Here, we develop a hybrid of the two methods. We let the
neural structure do the initial clustering in order to achieve a high
computational performance. Its solution is fed as the initial state to the
iterative optimization in order to improve the clustering performance.
</summary>
    <author>
      <name>Johan Schubert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 10 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">in Proceedings of EuroFusion98 International Conference on Data
  Fusion (EF'98), M. Bedworth, J. O'Brien (Eds.), pp. 29-36, Great Malvern, UK,
  6-7 October 1998</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0305024v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0305024v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.3; I.2.6; I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.01576v2</id>
    <updated>2016-11-21T20:52:34Z</updated>
    <published>2016-11-05T00:31:25Z</published>
    <title>Quasi-Recurrent Neural Networks</title>
    <summary>  Recurrent neural networks are a powerful tool for modeling sequential data,
but the dependence of each timestep's computation on the previous timestep's
output limits parallelism and makes RNNs unwieldy for very long sequences. We
introduce quasi-recurrent neural networks (QRNNs), an approach to neural
sequence modeling that alternates convolutional layers, which apply in parallel
across timesteps, and a minimalist recurrent pooling function that applies in
parallel across channels. Despite lacking trainable recurrent layers, stacked
QRNNs have better predictive accuracy than stacked LSTMs of the same hidden
size. Due to their increased parallelism, they are up to 16 times faster at
train and test time. Experiments on language modeling, sentiment
classification, and character-level neural machine translation demonstrate
these advantages and underline the viability of QRNNs as a basic building block
for a variety of sequence tasks.
</summary>
    <author>
      <name>James Bradbury</name>
    </author>
    <author>
      <name>Stephen Merity</name>
    </author>
    <author>
      <name>Caiming Xiong</name>
    </author>
    <author>
      <name>Richard Socher</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to conference track at ICLR 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.01576v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.01576v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04494v1</id>
    <updated>2016-05-15T03:19:18Z</updated>
    <published>2016-05-15T03:19:18Z</published>
    <title>Probabilistic Deep Spiking Neural Systems Enabled by Magnetic Tunnel
  Junction</title>
    <summary>  Deep Spiking Neural Networks are becoming increasingly powerful tools for
cognitive computing platforms. However, most of the existing literature on such
computing models are developed with limited insights on the underlying hardware
implementation, resulting in area and power expensive designs. Although several
neuromimetic devices emulating neural operations have been proposed recently,
their functionality has been limited to very simple neural models that may
prove to be inefficient at complex recognition tasks. In this work, we venture
into the relatively unexplored area of utilizing the inherent device
stochasticity of such neuromimetic devices to model complex neural
functionalities in a probabilistic framework in the time domain. We consider
the implementation of a Deep Spiking Neural Network capable of performing high
accuracy and low latency classification tasks where the neural computing unit
is enabled by the stochastic switching behavior of a Magnetic Tunnel Junction.
Simulation studies indicate an energy improvement of $20\times$ over a baseline
CMOS design in $45nm$ technology.
</summary>
    <author>
      <name>Abhronil Sengupta</name>
    </author>
    <author>
      <name>Maryam Parsa</name>
    </author>
    <author>
      <name>Bing Han</name>
    </author>
    <author>
      <name>Kaushik Roy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TED.2016.2568762</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TED.2016.2568762" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The article will appear in a future issue of IEEE Transactions on
  Electron Devices</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.04494v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04494v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.01197v1</id>
    <updated>2016-12-04T22:29:32Z</updated>
    <published>2016-12-04T22:29:32Z</published>
    <title>Neural Symbolic Machines: Learning Semantic Parsers on Freebase with
  Weak Supervision (Short Version)</title>
    <summary>  Extending the success of deep neural networks to natural language
understanding and symbolic reasoning requires complex operations and external
memory. Recent neural program induction approaches have attempted to address
this problem, but are typically limited to differentiable memory, and
consequently cannot scale beyond small synthetic tasks. In this work, we
propose the Manager-Programmer-Computer framework, which integrates neural
networks with non-differentiable memory to support abstract, scalable and
precise operations through a friendly neural computer interface. Specifically,
we introduce a Neural Symbolic Machine, which contains a sequence-to-sequence
neural "programmer", and a non-differentiable "computer" that is a Lisp
interpreter with code assist. To successfully apply REINFORCE for training, we
augment it with approximate gold programs found by an iterative maximum
likelihood training process. NSM is able to learn a semantic parser from weak
supervision over a large knowledge base. It achieves new state-of-the-art
performance on WebQuestionsSP, a challenging semantic parsing dataset, with
weak supervision. Compared to previous approaches, NSM is end-to-end, therefore
does not rely on feature engineering or domain specific knowledge.
</summary>
    <author>
      <name>Chen Liang</name>
    </author>
    <author>
      <name>Jonathan Berant</name>
    </author>
    <author>
      <name>Quoc Le</name>
    </author>
    <author>
      <name>Kenneth D. Forbus</name>
    </author>
    <author>
      <name>Ni Lao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in NAMPI workshop at NIPS 2016. Short version of
  arXiv:1611.00020</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.01197v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.01197v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.06611v1</id>
    <updated>2017-04-21T16:02:26Z</updated>
    <published>2017-04-21T16:02:26Z</published>
    <title>Making Neural Programming Architectures Generalize via Recursion</title>
    <summary>  Empirically, neural networks that attempt to learn programs from data have
exhibited poor generalizability. Moreover, it has traditionally been difficult
to reason about the behavior of these models beyond a certain level of input
complexity. In order to address these issues, we propose augmenting neural
architectures with a key abstraction: recursion. As an application, we
implement recursion in the Neural Programmer-Interpreter framework on four
tasks: grade-school addition, bubble sort, topological sort, and quicksort. We
demonstrate superior generalizability and interpretability with small amounts
of training data. Recursion divides the problem into smaller pieces and
drastically reduces the domain of each neural network component, making it
tractable to prove guarantees about the overall system's behavior. Our
experience suggests that in order for neural architectures to robustly learn
program semantics, it is necessary to incorporate a concept like recursion.
</summary>
    <author>
      <name>Jonathon Cai</name>
    </author>
    <author>
      <name>Richard Shin</name>
    </author>
    <author>
      <name>Dawn Song</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in ICLR 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.06611v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.06611v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.05934v1</id>
    <updated>2017-12-16T10:42:57Z</updated>
    <published>2017-12-16T10:42:57Z</published>
    <title>NDT: Neual Decision Tree Towards Fully Functioned Neural Graph</title>
    <summary>  Though traditional algorithms could be embedded into neural architectures
with the proposed principle of \cite{xiao2017hungarian}, the variables that
only occur in the condition of branch could not be updated as a special case.
To tackle this issue, we multiply the conditioned branches with Dirac symbol
(i.e. $\mathbf{1}_{x&gt;0}$), then approximate Dirac symbol with the continuous
functions (e.g. $1 - e^{-\alpha|x|}$). In this way, the gradients of
condition-specific variables could be worked out in the back-propagation
process, approximately, making a fully functioned neural graph. Within our
novel principle, we propose the neural decision tree \textbf{(NDT)}, which
takes simplified neural networks as decision function in each branch and
employs complex neural networks to generate the output in each leaf. Extensive
experiments verify our theoretical analysis and demonstrate the effectiveness
of our model.
</summary>
    <author>
      <name>Han Xiao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the draft paper. I will refine the paper until accepted</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.05934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.05934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9706043v1</id>
    <updated>1997-06-05T15:50:30Z</updated>
    <published>1997-06-05T15:50:30Z</published>
    <title>An exact learning algorithm for autoassociative neural networks with
  binary couplings</title>
    <summary>  Exact solutions for the learning problem of autoassociative networks with
binary couplings are determined by a new method. The use of a branch-and-bound
algorithm leads to a substantial saving of computational time compared with
complete enumeration. As a result, fully connected networks with up to 40
neurons could be investigated. The network capacity is found to be close to
0.83.
</summary>
    <author>
      <name>G. Milde</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TU Dresden, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>S. Kobe</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TU Dresden, Germany</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/0305-4470/30/7/016</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/0305-4470/30/7/016" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LaTeX (ioplppt style), 5 pages, 2 (LaTeX) pictures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J. Phys. A: Math. Gen. 30 (1997) 2349-2352</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/9706043v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9706043v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9801273v1</id>
    <updated>1998-01-26T18:25:12Z</updated>
    <published>1998-01-26T18:25:12Z</published>
    <title>Self-control in Sparsely Coded Networks</title>
    <summary>  A complete self-control mechanism is proposed in the dynamics of neural
networks through the introduction of a time-dependent threshold, determined in
function of both the noise and the pattern activity in the network. Especially
for sparsely coded models this mechanism is shown to considerably improve the
storage capacity, the basins of attraction and the mutual information content
of the network.
</summary>
    <author>
      <name>D. R. C. Dominguez</name>
    </author>
    <author>
      <name>D. Bolle</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.80.2961</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.80.2961" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 6 Postscript figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. Lett. 80, 2961-2964 (1998)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/9801273v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9801273v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0108408v1</id>
    <updated>2001-08-25T07:22:36Z</updated>
    <published>2001-08-25T07:22:36Z</published>
    <title>Spontaneous structure formation in a network of chaotic units with
  variable connection strengths</title>
    <summary>  As a model of temporally evolving networks, we consider a globally coupled
logistic map with variable connection weights. The model exhibits
self-organization of network structure, reflected by the collective behavior of
units. Structural order emerges even without any inter-unit synchronization of
dynamics. Within this structure, units spontaneously separate into two groups
whose distinguishing feature is that the first group possesses many
outwardly-directed connections to the second group, while the second group
possesses only few outwardly-directed connections to the first. The relevance
of the results to structure formation in neural networks is briefly discussed.
</summary>
    <author>
      <name>Junji Ito</name>
    </author>
    <author>
      <name>Kunihiko Kaneko</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.88.028701</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.88.028701" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures, REVTeX</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0108408v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0108408v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0205465v1</id>
    <updated>2002-05-23T15:06:01Z</updated>
    <published>2002-05-23T15:06:01Z</published>
    <title>Bistable Gradient Networks II: Storage Capacity and Behaviour Near
  Saturation</title>
    <summary>  We examine numerically the storage capacity and the behaviour near saturation
of an attractor neural network consisting of bistable elements with an
adjustable coupling strength, the Bistable Gradient Network (BGN). For strong
coupling, we find evidence of a first-order "memory blackout" phase transition
as in the Hopfield network. For weak coupling, on the other hand, there is no
evidence of such a transition and memorized patterns can be stable even at high
levels of loading. The enhanced storage capacity comes, however, at the cost of
imperfect retrieval of the patterns from corrupted versions.
</summary>
    <author>
      <name>Patrick N. McGraw</name>
    </author>
    <author>
      <name>Michael Menzinger</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.67.016119</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.67.016119" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 12 eps figures. Submitted to Phys. Rev. E. Sequel to
  cond-mat/0203568</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0205465v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0205465v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0402452v1</id>
    <updated>2004-02-17T19:56:54Z</updated>
    <published>2004-02-17T19:56:54Z</published>
    <title>Short-Term Memory in Orthogonal Neural Networks</title>
    <summary>  We study the ability of linear recurrent networks obeying discrete time
dynamics to store long temporal sequences that are retrievable from the
instantaneous state of the network. We calculate this temporal memory capacity
for both distributed shift register and random orthogonal connectivity
matrices. We show that the memory capacity of these networks scales with system
size.
</summary>
    <author>
      <name>Olivia L. White</name>
    </author>
    <author>
      <name>Daniel D. Lee</name>
    </author>
    <author>
      <name>Haim Sompolinsky</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.92.148102</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.92.148102" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures, to be published in Phys. Rev. Lett</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. Lett. 92, 148102 (2004)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0402452v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0402452v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505003v1</id>
    <updated>2005-04-30T16:13:32Z</updated>
    <published>2005-04-30T16:13:32Z</published>
    <title>A New Kind of Hopfield Networks for Finding Global Optimum</title>
    <summary>  The Hopfield network has been applied to solve optimization problems over
decades. However, it still has many limitations in accomplishing this task.
Most of them are inherited from the optimization algorithms it implements. The
computation of a Hopfield network, defined by a set of difference equations,
can easily be trapped into one local optimum or another, sensitive to initial
conditions, perturbations, and neuron update orders. It doesn't know how long
it will take to converge, as well as if the final solution is a global optimum,
or not. In this paper, we present a Hopfield network with a new set of
difference equations to fix those problems. The difference equations directly
implement a new powerful optimization algorithm.
</summary>
    <author>
      <name>Xiaofei Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, accepted by International Joint Conference on Neural
  Networks 2005</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/quant-ph/0609117v1</id>
    <updated>2006-09-15T15:19:31Z</updated>
    <published>2006-09-15T15:19:31Z</published>
    <title>Quantum Pattern Retrieval by Qubit Networks with Hebb Interactions</title>
    <summary>  Qubit networks with long-range interactions inspired by the Hebb rule can be
used as quantum associative memories. Starting from a uniform superposition,
the unitary evolution generated by these interactions drives the network
through a quantum phase transition at a critical computation time, after which
ferromagnetic order guarantees that a measurement retrieves the stored memory.
The maximum memory capacity p of these qubit networks is reached at a memory
density p/n=1.
</summary>
    <author>
      <name>M. Cristina Diamantini</name>
    </author>
    <author>
      <name>Carlo A. Trugenberger</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.97.130503</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.97.130503" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Physical Review Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/quant-ph/0609117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/quant-ph/0609117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.4682v1</id>
    <updated>2008-04-29T19:25:07Z</updated>
    <published>2008-04-29T19:25:07Z</published>
    <title>Introduction to Relational Networks for Classification</title>
    <summary>  The use of computational intelligence techniques for classification has been
used in numerous applications. This paper compares the use of a Multi Layer
Perceptron Neural Network and a new Relational Network on classifying the HIV
status of women at ante-natal clinics. The paper discusses the architecture of
the relational network and its merits compared to a neural network and most
other computational intelligence classifiers. Results gathered from the study
indicate comparable classification accuracies as well as revealed relationships
between data features in the classification data. Much higher classification
accuracies are recommended for future research in the area of HIV
classification as well as missing data estimation.
</summary>
    <author>
      <name>Vukosi Marivate</name>
    </author>
    <author>
      <name>Tshilidzi Marwala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0804.4682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.4682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.2294v2</id>
    <updated>2012-01-17T17:25:14Z</updated>
    <published>2011-07-12T13:56:35Z</published>
    <title>A network of phase oscillators as a device for sequential pattern
  generation</title>
    <summary>  We design a system of phase oscillators that is able to produce temporally
periodic sequences of patterns. Patterns are cluster partitions which encode
information as phase differences between phase oscillators. The architecture of
our system consists of a retrieval network with N globally coupled phase
oscillators, a pacemaker that controls the sequence retrieval, and a set of
patterns stored in the couplings between the pacemaker and the retrieval
network. The system performs in analogy to a central pattern generator of
neural networks and is very robust against perturbations in the retrieval
process.
</summary>
    <author>
      <name>Pablo Kaluza</name>
    </author>
    <author>
      <name>Hildegard Meyer-Ortmanns</name>
    </author>
    <link href="http://arxiv.org/abs/1107.2294v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.2294v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.1774v1</id>
    <updated>2012-08-08T20:17:42Z</updated>
    <published>2012-08-08T20:17:42Z</published>
    <title>Operator formalism for optical neural network based on the parametrical
  four-wave mixing process</title>
    <summary>  In this paper we develop a formalism allowing us to describe operating of a
network based on the parametrical four-wave mixing process that is well-known
in nonlinear optics. The recognition power of a network using parametric
neurons operating with q different frequencies is considered. It is shown that
the storage capacity of such a network is higher compared with the Potts-glass
models.
</summary>
    <author>
      <name>L. B. Litinskii</name>
    </author>
    <author>
      <name>B. V. Kryzhanovsky</name>
    </author>
    <author>
      <name>A. Fonarev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages,without figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1208.1774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.1774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60, 94" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.01513v1</id>
    <updated>2015-02-05T12:02:08Z</updated>
    <published>2015-02-05T12:02:08Z</published>
    <title>Microscopic instability in recurrent neural networks</title>
    <summary>  In a manner similar to the molecular chaos that underlies the stable
thermodynamics of gases, neuronal system may exhibit microscopic instability in
individual neuronal dynamics while a macroscopic order of the entire population
possibly remains stable. In this study, we analyze the microscopic stability of
a network of neurons whose macroscopic activity obeys stable dynamics,
expressing either monostable, bistable, or periodic state. We reveal that the
network exhibits a variety of dynamical states for microscopic instability
residing in given stable macroscopic dynamics. The presence of a variety of
dynamical states in such a simple random network implies more abundant
microscopic fluctuations in real neural networks, which consist of more complex
and hierarchically structured interactions.
</summary>
    <author>
      <name>Yuzuru Yamanaka</name>
    </author>
    <author>
      <name>Shun-ichi Amari</name>
    </author>
    <author>
      <name>Shigeru Shinomoto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.91.032921</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.91.032921" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.01513v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.01513v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06643v1</id>
    <updated>2016-03-21T23:07:05Z</updated>
    <published>2016-03-21T23:07:05Z</published>
    <title>Dimension reduction in heterogeneous neural networks: generalized
  Polynomial Chaos (gPC) and ANalysis-Of-VAriance (ANOVA)</title>
    <summary>  We propose, and illustrate via a neural network example, two different
approaches to coarse-graining large heterogeneous networks. Both approaches are
inspired from, and use tools developed in, methods for uncertainty
quantification in systems with multiple uncertain parameters - in our case, the
parameters are heterogeneously distributed on the network nodes. The approach
shows promise in accelerating large scale network simulations as well as
coarse-grained fixed point, periodic solution and stability analysis. We also
demonstrate that the approach can successfully deal with structural as well as
intrinsic heterogeneities.
</summary>
    <author>
      <name>Minseok Choi</name>
    </author>
    <author>
      <name>Tom Bertalan</name>
    </author>
    <author>
      <name>Carlo R. Laing</name>
    </author>
    <author>
      <name>Ioannis G. Kevrekidis</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1140/epjst/e2016-02662-3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1140/epjst/e2016-02662-3" rel="related"/>
    <link href="http://arxiv.org/abs/1603.06643v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06643v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.03287v1</id>
    <updated>2016-08-10T20:02:40Z</updated>
    <published>2016-08-10T20:02:40Z</published>
    <title>Deep vs. shallow networks : An approximation theory perspective</title>
    <summary>  The paper briefy reviews several recent results on hierarchical architectures
for learning from examples, that may formally explain the conditions under
which Deep Convolutional Neural Networks perform much better in function
approximation problems than shallow, one-hidden layer architectures. The paper
announces new results for a non-smooth activation function - the ReLU function
- used in present-day neural networks, as well as for the Gaussian networks. We
propose a new definition of relative dimension to encapsulate different notions
of sparsity of a function class that can possibly be exploited by deep networks
but not by shallow ones to drastically reduce the complexity required for
approximation and learning.
</summary>
    <author>
      <name>Hrushikesh Mhaskar</name>
    </author>
    <author>
      <name>Tomaso Poggio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 4 figures, to be published in a Journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.03287v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.03287v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.FA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.03939v1</id>
    <updated>2017-03-11T10:05:19Z</updated>
    <published>2017-03-11T10:05:19Z</published>
    <title>Ask Me Even More: Dynamic Memory Tensor Networks (Extended Model)</title>
    <summary>  We examine Memory Networks for the task of question answering (QA), under
common real world scenario where training examples are scarce and under weakly
supervised scenario, that is only extrinsic labels are available for training.
We propose extensions for the Dynamic Memory Network (DMN), specifically within
the attention mechanism, we call the resulting Neural Architecture as Dynamic
Memory Tensor Network (DMTN). Ultimately, we see that our proposed extensions
results in over 80% improvement in the number of task passed against the
baselined standard DMN and 20% more task passed compared to state-of-the-art
End-to-End Memory Network for Facebook's single task weakly trained 1K bAbi
dataset.
</summary>
    <author>
      <name>Govardana Sachithanandam Ramachandran</name>
    </author>
    <author>
      <name>Ajay Sohmshetty</name>
    </author>
    <link href="http://arxiv.org/abs/1703.03939v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.03939v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.04559v1</id>
    <updated>2017-03-14T00:54:18Z</updated>
    <published>2017-03-14T00:54:18Z</published>
    <title>Fully Convolutional Networks to Detect Clinical Dermoscopic Features</title>
    <summary>  We use a pretrained fully convolutional neural network to detect clinical
dermoscopic features from dermoscopy skin lesion images. We reformulate the
superpixel classification task as an image segmentation problem, and extend a
neural network architecture originally designed for image classification to
detect dermoscopic features. Specifically, we interpolate the feature maps from
several layers in the network to match the size of the input, concatenate the
resized feature maps, and train the network to minimize a smoothed negative F1
score. Over the public validation leaderboard of the 2017 ISIC/ISBI Lesion
Dermoscopic Feature Extraction Challenge, our approach achieves 89.3% AUROC,
the highest averaged score when compared to the other two entries. Results over
the private test leaderboard are still to be announced.
</summary>
    <author>
      <name>Jeremy Kawahara</name>
    </author>
    <author>
      <name>Ghassan Hamarneh</name>
    </author>
    <link href="http://arxiv.org/abs/1703.04559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.04559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.05537v1</id>
    <updated>2017-03-16T09:52:48Z</updated>
    <published>2017-03-16T09:52:48Z</published>
    <title>Shift Aggregate Extract Networks</title>
    <summary>  We introduce an architecture based on deep hierarchical decompositions to
learn effective representations of large graphs. Our framework extends classic
R-decompositions used in kernel methods, enabling nested "part-of-part"
relations. Unlike recursive neural networks, which unroll a template on input
graphs directly, we unroll a neural network template over the decomposition
hierarchy, allowing us to deal with the high degree variability that typically
characterize social network graphs. Deep hierarchical decompositions are also
amenable to domain compression, a technique that reduces both space and time
complexity by exploiting symmetries. We show empirically that our approach is
competitive with current state-of-the-art graph classification methods,
particularly when dealing with social network datasets.
</summary>
    <author>
      <name>Francesco Orsini</name>
    </author>
    <author>
      <name>Daniele Baracchi</name>
    </author>
    <author>
      <name>Paolo Frasconi</name>
    </author>
    <link href="http://arxiv.org/abs/1703.05537v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.05537v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.02023v1</id>
    <updated>2017-05-04T21:13:24Z</updated>
    <published>2017-05-04T21:13:24Z</published>
    <title>Senti17 at SemEval-2017 Task 4: Ten Convolutional Neural Network Voters
  for Tweet Polarity Classification</title>
    <summary>  This paper presents Senti17 system which uses ten convolutional neural
networks (ConvNet) to assign a sentiment label to a tweet. The network consists
of a convolutional layer followed by a fully-connected layer and a Softmax on
top. Ten instances of this network are initialized with the same word
embeddings as inputs but with different initializations for the network
weights. We combine the results of all instances by selecting the sentiment
label given by the majority of the ten voters. This system is ranked fourth in
SemEval-2017 Task4 over 38 systems with 67.4%
</summary>
    <author>
      <name>Hussam Hamdan</name>
    </author>
    <link href="http://arxiv.org/abs/1705.02023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.02023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.03381v1</id>
    <updated>2017-08-10T20:47:10Z</updated>
    <published>2017-08-10T20:47:10Z</published>
    <title>Topical Behavior Prediction from Massive Logs</title>
    <summary>  In this paper, we study the topical behavior in a large scale. We use the
network logs where each entry contains the entity ID, the timestamp, and the
meta data about the activity. Both the temporal and the spatial relationships
of the behavior are explored with the deep learning architectures combing the
recurrent neural network (RNN) and the convolutional neural network (CNN). To
make the behavioral data appropriate for the spatial learning in the CNN, we
propose several reduction steps to form the topical metrics and to place them
homogeneously like pixels in the images. The experimental result shows both
temporal and spatial gains when compared against a multilayer perceptron (MLP)
network. A new learning framework called the spatially connected convolutional
networks (SCCN) is introduced to predict the topical metrics more efficiently.
</summary>
    <author>
      <name>Shih-Chieh Su</name>
    </author>
    <link href="http://arxiv.org/abs/1708.03381v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.03381v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.10177v1</id>
    <updated>2017-11-28T08:48:39Z</updated>
    <published>2017-11-28T08:48:39Z</published>
    <title>Gradual Tuning: a better way of Fine Tuning the parameters of a Deep
  Neural Network</title>
    <summary>  In this paper we present an alternative strategy for fine-tuning the
parameters of a network. We named the technique Gradual Tuning. Once trained on
a first task, the network is fine-tuned on a second task by modifying a
progressively larger set of the network's parameters. We test Gradual Tuning on
different transfer learning tasks, using networks of different sizes trained
with different regularization techniques. The result shows that compared to the
usual fine tuning, our approach significantly reduces catastrophic forgetting
of the initial task, while still retaining comparable if not better performance
on the new task.
</summary>
    <author>
      <name>Guglielmo Montone</name>
    </author>
    <author>
      <name>J. Kevin O'Regan</name>
    </author>
    <author>
      <name>Alexander V. Terekhov</name>
    </author>
    <link href="http://arxiv.org/abs/1711.10177v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.10177v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.04034v1</id>
    <updated>2018-02-12T13:37:09Z</updated>
    <published>2018-02-12T13:37:09Z</published>
    <title>Lipschitz-Margin Training: Scalable Certification of Perturbation
  Invariance for Deep Neural Networks</title>
    <summary>  High sensitivity of neural networks against malicious perturbations on inputs
causes security concerns. We aim to ensure perturbation invariance in their
predictions. However, prior work requires strong assumptions on network
structures and massive computational costs, and thus their applications are
limited. In this paper, based on Lipschitz constants and prediction margins, we
present a widely applicable and computationally efficient method to lower-bound
the size of adversarial perturbations that networks can never be deceived.
Moreover, we propose an efficient training procedure to strengthen perturbation
invariance. In experimental evaluations, our method showed its ability to
provide a strong guarantee for even large networks.
</summary>
    <author>
      <name>Yusuke Tsuzuku</name>
    </author>
    <author>
      <name>Issei Sato</name>
    </author>
    <author>
      <name>Masashi Sugiyama</name>
    </author>
    <link href="http://arxiv.org/abs/1802.04034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.04034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.06627v1</id>
    <updated>2018-02-19T13:49:15Z</updated>
    <published>2018-02-19T13:49:15Z</published>
    <title>Robustness of Rotation-Equivariant Networks to Adversarial Perturbations</title>
    <summary>  Deep neural networks have been shown to be vulnerable to adversarial
examples: very small perturbations of the input having a dramatic impact on the
predictions. A wealth of adversarial attacks and distance metrics to quantify
the similarity between natural and adversarial images have been proposed,
recently enlarging the scope of adversarial examples with geometric
transformations beyond pixel-wise attacks. In this context, we investigate the
robustness to adversarial attacks of new Convolutional Neural Network
architectures providing equivariance to rotations. We found that
rotation-equivariant networks are significantly less vulnerable to
geometric-based attacks than regular networks on the MNIST, CIFAR-10, and
ImageNet datasets.
</summary>
    <author>
      <name>Beranger Dumont</name>
    </author>
    <author>
      <name>Simona Maggio</name>
    </author>
    <author>
      <name>Pablo Montalvo</name>
    </author>
    <link href="http://arxiv.org/abs/1802.06627v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06627v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/adap-org/9610003v1</id>
    <updated>1996-10-03T23:36:04Z</updated>
    <published>1996-10-03T23:36:04Z</published>
    <title>Spontaneous Origin of Topological Complexity in Self-Organizing Neural
  Networks</title>
    <summary>  Attention is drawn to the possibility that self-organizing biological neural
networks could spontaneously acquire the capability to carry out sophisticated
computations. In particular it is shown that the effective action governing the
formation of synaptic connections in models of networks of feature detectors
that encorporate Kohonen-like self-organization can spontaneously lead to
structures that are topologically nontrivial in both a 2-dimensional and
4-dimensional sense. It is suggested that the appearance of biological neural
structures with a nontrivial 4-dimensional topology is the fundamental
organizational principle underlying the emergence of advanced cognitive
capabilities.
</summary>
    <author>
      <name>George Chapline</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PostScript, 15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/adap-org/9610003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/adap-org/9610003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="adap-org" scheme="http://arxiv.org/schemas/atom"/>
    <category term="adap-org" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/astro-ph/9411071v1</id>
    <updated>1994-11-17T15:29:00Z</updated>
    <published>1994-11-17T15:29:00Z</published>
    <title>Artificial Neural Networks as Non-Linear Extensions of Statistical
  Methods in Astronomy</title>
    <summary>  We attempt to de-mistify Artificial Neural Networks (ANNs) by considering
special cases which are related to other statistical methods common in
Astronomy and other fields. In particular we show how ANNs generalise Bayesian
methods, multi-parameter fitting, Principal Component Analysis (PCA), Wiener
filtering and regularisation methods. Examples of morphological classification
of galaxies illustrate how non-linear ANNs improve on linear techniques.
</summary>
    <author>
      <name>Ofer Lahav</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/0083-6656(94)90034-5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/0083-6656(94)90034-5" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, uu-encoded compressed postscript file. Also available by
  anonymous ftp to cast0.ast.cam.ac.uk (131.111.68.35) at
  ftp://cast0.ast.cam.ac.uk/pub/lahav/vistas/vistas4.ps.Z with figure at
  ftp://cast0.ast.cam.ac.uk/pub/lahav/vistas/fig1.ps.Z To appear in Vistas in
  Astronomy, special issue on Artificial Neural Networks in Astronomy</arxiv:comment>
    <link href="http://arxiv.org/abs/astro-ph/9411071v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/astro-ph/9411071v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/astro-ph/9906107v1</id>
    <updated>1999-06-07T13:06:13Z</updated>
    <published>1999-06-07T13:06:13Z</published>
    <title>A Neural Network-based ARX Model of Virgo Noise</title>
    <summary>  In this paper a Neural Network based approach is presented to identify the
noise in the VIRGO context. VIRGO is an experiment to detect Gravitational
Waves by means of a Laser Interferometer. Preliminary results appear to be very
promising for data analysis of realistic Interferometer outputs.
</summary>
    <author>
      <name>F. Barone</name>
    </author>
    <author>
      <name>R. De Rosa</name>
    </author>
    <author>
      <name>A. Eleuteri</name>
    </author>
    <author>
      <name>F. Garufi</name>
    </author>
    <author>
      <name>L. Milano</name>
    </author>
    <author>
      <name>R. Tagliaferri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, wicsbook.sty macro file, accepted for publication in the
  proceedings of the 11th Italian Workshop on Neural Networks, WIRN Vietri 99,
  M. Marinaro and R. Tagliaferri eds., Springer-Verlag 1999</arxiv:comment>
    <link href="http://arxiv.org/abs/astro-ph/9906107v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/astro-ph/9906107v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9706280v1</id>
    <updated>1997-06-27T19:05:27Z</updated>
    <published>1997-06-27T19:05:27Z</published>
    <title>Energy functional and fixed points of a neural network</title>
    <summary>  A dynamic system, which is used in the neural network theory, Ising spin
glasses and factor analysis, has been investigated. The properties of the
connection matrix, which guarantee the coincidence of the set of the fixed
points of the dynamic system with the set of the local minima of the energy
functional, have been determined. The influence of the connection matrix
diagonal elements on the structure of the fixed points set has been
investigated.
</summary>
    <author>
      <name>Leonid B. Litinskii</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Institute for High Pressure Physics Russian Academy of Sciences</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LaTex, 7 pages, e-mail: litin@ns.hppi.troitsk.ru</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9706280v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9706280v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9804165v1</id>
    <updated>1998-04-16T08:01:43Z</updated>
    <published>1998-04-16T08:01:43Z</published>
    <title>Highly Symmetric Neural Networks of Hopfield Type (exact results)</title>
    <summary>  A set of fixed points of the Hopfield type neural network is under
investigation. Its connection matrix is constructed with regard to the Hebb
rule from a highly symmetric set of the memorized patterns. Depending on the
external parameter the analytic description of the fixed points set has been
obtained.
</summary>
    <author>
      <name>Leonid B. Litinsky</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Institute for High Pressure Physics Russian Academy of Sciences</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, Latex, sumbitted to Conference of Computational Physics,
  September, 2-5 1998 (Granada, Espana)</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9804165v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9804165v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9811244v1</id>
    <updated>1998-11-17T09:40:44Z</updated>
    <published>1998-11-17T09:40:44Z</published>
    <title>A replica approach to the state-dependent synapses neural network</title>
    <summary>  The replica method is applied to a neural network model with state-dependent
synapses built from those patterns having a correlation with the state of the
system greater than a certain threshold. Replica-symmetric and first-step
replica-symmetry-breaking results are presented for the storage capacity at
zero temperature as a function of this threshold value. A comparison is made
with existing results based upon mean-field equations obtained by using a
statistical method.
</summary>
    <author>
      <name>D. Boll√©</name>
    </author>
    <author>
      <name>G. M. Shim</name>
    </author>
    <author>
      <name>B. Van Mol</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/0305-4470/32/18/301</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/0305-4470/32/18/301" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, including 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J Phys.A: Math.Gen. 32 (1999) 3201-3207</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/9811244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9811244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9901212v1</id>
    <updated>1999-01-21T09:27:53Z</updated>
    <published>1999-01-21T09:27:53Z</published>
    <title>Analysis of Natural Gradient Descent for Multilayer Neural Networks</title>
    <summary>  Natural gradient descent is a principled method for adapting the parameters
of a statistical model on-line using an underlying Riemannian parameter space
to redefine the direction of steepest descent. The algorithm is examined via
methods of statistical physics which accurately characterize both transient and
asymptotic behavior. A solution of the learning dynamics is obtained for the
case of multilayer neural network training in the limit of large input
dimension. We find that natural gradient learning leads to optimal asymptotic
performance and outperforms gradient descent in the transient, significantly
shortening or even removing plateaus in the transient generalization
performance which typically hamper gradient descent training.
</summary>
    <author>
      <name>Magnus Rattray</name>
    </author>
    <author>
      <name>David Saad</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.59.4523</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.59.4523" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages including figures. To appear in Physical Review E</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9901212v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9901212v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9906274v2</id>
    <updated>2000-01-04T16:51:42Z</updated>
    <published>1999-06-17T13:26:16Z</published>
    <title>The Ashkin-Teller neural network near saturation</title>
    <summary>  The thermodynamic and retrieval properties of the Ashkin-Teller neural
network model storing an infinite number of patterns are examined in the
replica-symmetric mean-field approximation. In particular, for linked patterns
temperature-capacity phase diagrams are derived for different values of the
two-neuron and four-neuron coupling strengths. This model can be considered as
a particular non-trivial generalisation of the Hopfield model and exhibits a
number of interesting new features. Some aspects of replica-symmetry breaking
are discussed.
</summary>
    <author>
      <name>D. Bolle</name>
    </author>
    <author>
      <name>P. Kozlowski</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/0305-4470/32/49/301</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/0305-4470/32/49/301" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Latex, 12 pages, 5 ps figures, minor changes: some comments added</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J. Phys. A: Math. Gen. 32 (1999) 8577-8586</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/9906274v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9906274v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0002360v1</id>
    <updated>2000-02-23T16:20:10Z</updated>
    <published>2000-02-23T16:20:10Z</published>
    <title>A recurrent neural network with ever changing synapses</title>
    <summary>  A recurrent neural network with noisy input is studied analytically, on the
basis of a Discrete Time Master Equation. The latter is derived from a
biologically realizable learning rule for the weights of the connections. In a
numerical study it is found that the fixed points of the dynamics of the net
are time dependent, implying that the representation in the brain of a fixed
piece of information (e.g., a word to be recognized) is not fixed in time.
</summary>
    <author>
      <name>M. Heerema</name>
    </author>
    <author>
      <name>W. A. van Leeuwen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/0305-4470/33/9/305</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/0305-4470/33/9/305" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, LaTeX, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J. Phys. A: Math. Gen. 33 (2000)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0002360v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0002360v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0007310v1</id>
    <updated>2000-07-19T12:15:17Z</updated>
    <published>2000-07-19T12:15:17Z</published>
    <title>Controlling chaos in diluted networks with continuous neurons</title>
    <summary>  Diluted neural networks with continuous neurons and nonmonotonic transfer
function are studied, with both fixed and dynamic synapses. A noisy stimulus
with periodic variance results in a mechanism for controlling chaos in neural
systems with fixed synapses: a proper amount of external perturbation forces
the system to behave periodically with the same period as the stimulus.
</summary>
    <author>
      <name>D. Caroppo</name>
    </author>
    <author>
      <name>M. Mannarelli</name>
    </author>
    <author>
      <name>G. Nardulli</name>
    </author>
    <author>
      <name>S. Stramaglia</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1142/S0217984901001379</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1142/S0217984901001379" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0007310v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0007310v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0104011v1</id>
    <updated>2001-04-01T08:14:27Z</updated>
    <published>2001-04-01T08:14:27Z</published>
    <title>Multilayer neural networks with extensively many hidden units</title>
    <summary>  The information processing abilities of a multilayer neural network with a
number of hidden units scaling as the input dimension are studied using
statistical mechanics methods. The mapping from the input layer to the hidden
units is performed by general symmetric Boolean functions whereas the hidden
layer is connected to the output by either discrete or continuous couplings.
Introducing an overlap in the space of Boolean functions as order parameter the
storage capacity if found to scale with the logarithm of the number of
implementable Boolean functions. The generalization behaviour is smooth for
continuous couplings and shows a discontinuous transition to perfect
generalization for discrete ones.
</summary>
    <author>
      <name>Michal Rosen-Zvi</name>
    </author>
    <author>
      <name>Andreas Engel</name>
    </author>
    <author>
      <name>Ido Kanter</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.87.078101</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.87.078101" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0104011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0104011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0107456v1</id>
    <updated>2001-07-23T07:34:50Z</updated>
    <published>2001-07-23T07:34:50Z</published>
    <title>Diluted neural networks with adapting and correlated synapses</title>
    <summary>  We consider the dynamics of diluted neural networks with clipped and adapting
synapses. Unlike previous studies, the learning rate is kept constant as the
connectivity tends to infinity: the synapses evolve on a time scale
intermediate between the quenched and annealing limits and all orders of
synaptic correlations must be taken into account. The dynamics is solved by
mean-field theory, the order parameter for synapses being a function. We
describe the effects, in the double dynamics, due to synaptic correlations.
</summary>
    <author>
      <name>Massimo Mannarelli</name>
    </author>
    <author>
      <name>Giuseppe Nardulli</name>
    </author>
    <author>
      <name>Sebastiano Stramaglia</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.64.052904</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.64.052904" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures. Accepted for publication in PRE</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0107456v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0107456v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0112463v1</id>
    <updated>2001-12-26T22:43:27Z</updated>
    <published>2001-12-26T22:43:27Z</published>
    <title>Gauge Symmetry and Neural Networks</title>
    <summary>  We propose a new model of neural network. It consists of spin variables to
describe the state of neurons as in the Hopfield model and new gauge variables
to describe the state of synapses. The model possesses local gauge symmetry and
resembles lattice gauge theory of high-energy physics. Time dependence of
synapses describes the process of learning. The mean field theory predicts a
new phase corresponding to confinement phase, in which brain loses ablility of
learning and memory.
</summary>
    <author>
      <name>Tetsuo Matsui</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1142/9789812811240_0025</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1142/9789812811240_0025" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0112463v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0112463v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-lat" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0202097v1</id>
    <updated>2002-02-06T16:41:55Z</updated>
    <published>2002-02-06T16:41:55Z</published>
    <title>Retrieval and Chaos in Extremely Diluted Non-Monotonic Neural Networks</title>
    <summary>  We discuss, in this paper, the dynamical properties of extremely diluted,
non-monotonic neural networks. Assuming parallel updating and the Hebb
prescription for the synaptic connections, a flow equation for the macroscopic
overlap is derived. A rich dynamical phase diagram was obtained, showing a
stable retrieval phase, as well as a cycle two and chaotic behavior. Numerical
simulations were performed, showing good agreement with analytical results.
Furthermore, the simulations give an additional insight into the microscopic
dynamical behavior during the chaotic phase. It is shown that the freezing of
individual neuron states is related to the structure of chaotic attractors.
</summary>
    <author>
      <name>M. S. Mainieri</name>
    </author>
    <author>
      <name>R. Erichsen Jr</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/S0378-4371(02)00626-X</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/S0378-4371(02)00626-X" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0202097v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0202097v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0202537v1</id>
    <updated>2002-02-28T17:29:47Z</updated>
    <published>2002-02-28T17:29:47Z</published>
    <title>Learning and predicting time series by neural networks</title>
    <summary>  Artificial neural networks which are trained on a time series are supposed to
achieve two abilities: firstly to predict the series many time steps ahead and
secondly to learn the rule which has produced the series. It is shown that
prediction and learning are not necessarily related to each other. Chaotic
sequences can be learned but not predicted while quasiperiodic sequences can be
well predicted but not learned.
</summary>
    <author>
      <name>Ansgar Freking</name>
    </author>
    <author>
      <name>Wolfgang Kinzel</name>
    </author>
    <author>
      <name>Ido Kanter</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.65.050903</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.65.050903" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0202537v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0202537v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0203384v1</id>
    <updated>2002-03-19T17:41:21Z</updated>
    <published>2002-03-19T17:41:21Z</published>
    <title>Neural Network Model for Apparent Deterministic Chaos in Spontaneously
  Bursting Hippocampal Slices</title>
    <summary>  A neural network model that exhibits stochastic population bursting is
studied by simulation. First return maps of inter-burst intervals exhibit
recurrent unstable periodic orbit (UPO)-like trajectories similar to those
found in experiments on hippocampal slices. Applications of various control
methods and surrogate analysis for UPO-detection also yield results similar to
those of experiments. Our results question the interpretation of the
experimental data as evidence for deterministic chaos and suggest caution in
the use of UPO-based methods for detecting determinism in time-series data.
</summary>
    <author>
      <name>B. Biswal</name>
    </author>
    <author>
      <name>C. Dasgupta</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.88.088102</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.88.088102" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 .eps figures (included), requires psfrag.sty (included)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. Lett. 88, 088102 (2002)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0203384v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0203384v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0206569v1</id>
    <updated>2002-06-28T10:19:25Z</updated>
    <published>2002-06-28T10:19:25Z</published>
    <title>Parallel dynamics of the fully connected Blume-Emery-Griffiths neural
  network</title>
    <summary>  The parallel dynamics of the fully connected Blume-Emery-Griffiths neural
network model is studied at zero temperature for arbitrary using a
probabilistic approach. A recursive scheme is found determining the complete
time evolution of the order parameters, taking into account all feedback
correlations. It is based upon the evolution of the distribution of the local
field, the structure of which is determined in detail. As an illustrative
example, explicit analytic formula are given for the first few time steps of
the dynamics. Furthermore, equilibrium fixed-point equations are derived and
compared with the thermodynamic approach. The analytic results find excellent
confirmation in extensive numerical simulations.
</summary>
    <author>
      <name>D. Bolle</name>
    </author>
    <author>
      <name>J. Busquets Blanco</name>
    </author>
    <author>
      <name>G. M. Shim</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/S0378-4371(02)01528-5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/S0378-4371(02)01528-5" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0206569v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0206569v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0209279v1</id>
    <updated>2002-09-12T09:38:10Z</updated>
    <published>2002-09-12T09:38:10Z</published>
    <title>The macroscopic dynamics in separable neural networks</title>
    <summary>  The parallel dynamics is given in the case of neural networks with separable
coupling through starting from Coolen-Sherrington (CS) theory. It is shown that
this retrieve dynamics as is the case of sequential evolution in the postulate
of away from saturation and finite temperature. The finite-size effects is
governed by a homogeneous Markov process, which differs from the time-dependent
Ornstein-Uhlenbeck process in sequential dynamics. PACS number(s): 87.10.+e,
75.10.Nr, 02.50.+s
</summary>
    <author>
      <name>Yong Chen</name>
    </author>
    <author>
      <name>Ying Hai Wang</name>
    </author>
    <author>
      <name>Kong Qing Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Phys. Rev. E 63, 041901(2001)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0209279v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0209279v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0211260v1</id>
    <updated>2002-11-13T16:53:46Z</updated>
    <published>2002-11-13T16:53:46Z</published>
    <title>Pricing Derivatives by Path Integral and Neural Networks</title>
    <summary>  Recent progress in the development of efficient computational algorithms to
price financial derivatives is summarized. A first algorithm is based on a path
integral approach to option pricing, while a second algorithm makes use of a
neural network parameterization of option prices. The accuracy of the two
methods is established from comparisons with the results of the standard
procedures used in quantitative finance.
</summary>
    <author>
      <name>G. Montagna</name>
    </author>
    <author>
      <name>M. Morelli</name>
    </author>
    <author>
      <name>O. Nicrosini</name>
    </author>
    <author>
      <name>P. Amato</name>
    </author>
    <author>
      <name>M. Farina</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/S0378-4371(02)01907-6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/S0378-4371(02)01907-6" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure, 1 table. Contribution to Proceedings of
  International Econophysics Conference, Bali, August 28-31, 2002</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0211260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0211260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0304282v1</id>
    <updated>2003-04-11T16:28:18Z</updated>
    <published>2003-04-11T16:28:18Z</published>
    <title>Finite Connectivity Attractor Neural Networks</title>
    <summary>  We study a family of diluted attractor neural networks with a finite average
number of (symmetric) connections per neuron. As in finite connectivity spin
glasses, their equilibrium properties are described by order parameter
functions, for which we derive an integral equation in replica symmetric (RS)
approximation. A bifurcation analysis of this equation reveals the locations of
the paramagnetic to recall and paramagnetic to spin-glass transition lines in
the phase diagram. The line separating the retrieval phase from the spin-glass
phase is calculated at zero temperature. All phase transitions are found to be
continuous.
</summary>
    <author>
      <name>B. Wemmenhove</name>
    </author>
    <author>
      <name>A. C. C. Coolen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/0305-4470/36/37/302</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/0305-4470/36/37/302" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J. Phys. A: Math. Gen. 36 (2003) 9617</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0304282v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0304282v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0307104v1</id>
    <updated>2003-07-04T12:43:49Z</updated>
    <published>2003-07-04T12:43:49Z</published>
    <title>Multi-state neural networks based upon spin-glasses: a biased overview</title>
    <summary>  Recent results are reviewed on both the time evolution and retrieval
properties of multi-state neural networks that are based upon spin-glass
models. In particular, the properties of models with neuron states having
Q-Ising symmetry are discussed for various architectures. The main common
features and differences are highlighted.
</summary>
    <author>
      <name>D. Bolle</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Review chapter to appear in Advances in Condensed Matter and
  Statistical Mechanics, ed. by E. Korutcheva and R. Cuerno, Nova Science
  Publishers (38 pages Latex with 12 eps-figures)</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0307104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0307104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0307666v1</id>
    <updated>2003-07-28T06:28:59Z</updated>
    <published>2003-07-28T06:28:59Z</published>
    <title>A novel stochastic Hebb-like learning rule for neural networks</title>
    <summary>  We present a novel stochastic Hebb-like learning rule for neural networks.
This learning rule is stochastic with respect to the selection of the time
points when a synaptic modification is induced by pre- and postsynaptic
activation. Moreover, the learning rule does not only affect the synapse
between pre- and postsynaptic neuron which is called homosynaptic plasticity
but also on further remote synapses of the pre- and postsynaptic neuron. This
form of plasticity has recently come into the light of interest of experimental
investigations and is called heterosynaptic plasticity. Our learning rule gives
a qualitative explanation of this kind of synaptic modification.
</summary>
    <author>
      <name>Frank Emmert-Streib</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0307666v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0307666v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0403620v1</id>
    <updated>2004-03-24T17:21:53Z</updated>
    <published>2004-03-24T17:21:53Z</published>
    <title>Neural Network Revisited: Perception on Modified Poincare Map of
  Financial Time Series Data</title>
    <summary>  Artificial Neural Network Model for prediction of time-series data is
revisited on analysis of the Indonesian stock-exchange data. We introduce the
use of Multi-Layer Perceptron to percept the modified Poincare-map of the given
financial time-series data. The modified Poincare-map is believed to become the
pattern of the data that transforms the data in time-t versus the data in
time-t+1 graphically. We built the Multi-Layer Perceptron to percept and
demonstrate predicting the data on specific stock-exchange in Indonesia.
</summary>
    <author>
      <name>Hokky Situngkir</name>
    </author>
    <author>
      <name>Yohanes Surya</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physa.2004.06.095</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physa.2004.06.095" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0403620v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0403620v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0404742v1</id>
    <updated>2004-04-30T13:58:15Z</updated>
    <published>2004-04-30T13:58:15Z</published>
    <title>Asymmetrically extremely dilute neural networks with non-trivial
  dynamics</title>
    <summary>  We study graded response attractor neural networks with asymmetrically
extremely dilute interactions and Langevin dynamics. We solve our model in the
thermodynamic limit using generating functional analysis, and find (in contrast
to the binary neurons case) that even in statics one cannot eliminate the
non-persistent order parameters. The macroscopic dynamics is driven by the
(non-trivial) joint distribution of neurons and fields, rather than just the
(Gaussian) field distribution. We calculate phase transition lines and present
simulation results in support of our theory.
</summary>
    <author>
      <name>J. P. L. Hatchett</name>
    </author>
    <author>
      <name>A. C. C. Coolen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, postscript figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0404742v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0404742v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0503374v1</id>
    <updated>2005-03-15T17:32:23Z</updated>
    <published>2005-03-15T17:32:23Z</published>
    <title>System size resonance in an attractor neural network</title>
    <summary>  We study the response of an attractor neural network, in the ferromagnetic
phase, to an external, time-dependent stimulus, which drives the system
periodically two different attractors. We demonstrate a non-trivial dependance
of the system via a system size resonance, by showing a signal amplification
maximum at a certain finite size.
</summary>
    <author>
      <name>M. A. de la Casa</name>
    </author>
    <author>
      <name>E. Korutcheva</name>
    </author>
    <author>
      <name>J. M. R. Parrondo</name>
    </author>
    <author>
      <name>F. J. de la Rubia</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.72.031113</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.72.031113" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 9 figures, submitted to Europhys. Lett</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0503374v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0503374v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0505326v1</id>
    <updated>2005-05-12T16:08:48Z</updated>
    <published>2005-05-12T16:08:48Z</published>
    <title>The synchronous BEG neural network with variable dilution</title>
    <summary>  The thermodynamic and retrieval properties of the Blume-Emery-Griffiths
neural network with synchronous updating and variable dilution are studied
using replica mean-field theory. Several forms of dilution are allowed by
pruning the different types of couplings present in the Hamiltonian. The
appearance and properties of two-cycles are discussed. Capacity-temperature
phase diagrams are derived for several values of the pattern activity. The
results are compared with those for sequential updating. The effect of
self-coupling is studied. Furthermore, the optimal combination of dilution
parameters giving the largest critical capacity is obtained.
</summary>
    <author>
      <name>D. Boll√©</name>
    </author>
    <author>
      <name>J. Busquets Blanco</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages in Latex, 15 postscript figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0505326v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0505326v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0507039v1</id>
    <updated>2005-07-01T19:51:58Z</updated>
    <published>2005-07-01T19:51:58Z</published>
    <title>Pattern reconstruction and sequence processing in feed-forward layered
  neural networks near saturation</title>
    <summary>  The dynamics and the stationary states for the competition between pattern
reconstruction and asymmetric sequence processing are studied here in an
exactly solvable feed-forward layered neural network model of binary units and
patterns near saturation. Earlier work by Coolen and Sherrington on a parallel
dynamics far from saturation is extended here to account for finite stochastic
noise due to a Hebbian and a sequential learning rule. Phase diagrams are
obtained with stationary states and quasi-periodic non-stationary solutions.
The relevant dependence of these diagrams and of the quasi-periodic solutions
on the stochastic noise and on initial inputs for the overlaps is explicitly
discussed.
</summary>
    <author>
      <name>F. L. Metz</name>
    </author>
    <author>
      <name>W. K. Theumann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.72.021908</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.72.021908" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0507039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0507039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.other" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0103002v1</id>
    <updated>2001-03-02T00:20:01Z</updated>
    <published>2001-03-02T00:20:01Z</published>
    <title>Quantitative Neural Network Model of the Tip-of-the-Tongue Phenomenon
  Based on Synthesized Memory-Psycholinguistic-Metacognitive Approach</title>
    <summary>  A new three-stage computer artificial neural network model of the
tip-of-the-tongue phenomenon is proposed. Each word's node is build from some
interconnected learned auto-associative two-layer neural networks each of which
represents separate word's semantic, lexical, or phonological components. The
model synthesizes memory, psycholinguistic, and metamemory approaches, bridges
speech errors and naming chronometry research traditions, and can explain
quantitatively many tip-of-the-tongue effects.
</summary>
    <author>
      <name>Petro M. Gopych</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of The Second International Conference
  Internet-Education-Science-2000 (IES-2000): New Informational and Computer
  Technologies in Education and Science, held on October 10-12, 2000 in
  Vinnytsia, Ukraine, page 273</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0103002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0103002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/hep-ex/9501007v2</id>
    <updated>1995-01-19T14:44:56Z</updated>
    <published>1995-01-18T08:54:52Z</published>
    <title>A novel approach to error function minimization for feedforward neural
  networks</title>
    <summary>  Feedforward neural networks with error backpropagation (FFBP) are widely
applied to pattern recognition. One general problem encountered with this type
of neural networks is the uncertainty, whether the minimization procedure has
converged to a global minimum of the cost function. To overcome this problem a
novel approach to minimize the error function is presented. It allows to
monitor the approach to the global minimum and as an outcome several
ambiguities related to the choice of free parameters of the minimization
procedure are removed.
</summary>
    <author>
      <name>Ralph Sinkus</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/0168-9002(95)00247-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/0168-9002(95)00247-2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, latex, 3 figures appended as uuencoded file</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Nucl.Instrum.Meth.A361:290-296,1995</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/hep-ex/9501007v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/hep-ex/9501007v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/hep-ex/9505004v1</id>
    <updated>1995-05-06T12:21:14Z</updated>
    <published>1995-05-06T12:21:14Z</published>
    <title>Neural Network based Electron Identification in the ZEUS Calorimeter</title>
    <summary>  We present an electron identification algorithm based on a neural network
approach applied to the ZEUS uranium calorimeter. The study is motivated by the
need to select deep inelastic, neutral current, electron proton interactions
characterized by the presence of a scattered electron in the final state. The
performance of the algorithm is compared to an electron identification method
based on a classical probabilistic approach. By means of a principle component
analysis the improvement in the performance is traced back to the number of
variables used in the neural network approach.
</summary>
    <author>
      <name>H. Abramowicz</name>
    </author>
    <author>
      <name>A. Caldwell</name>
    </author>
    <author>
      <name>R. Sinkus</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/0168-9002(95)00612-5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/0168-9002(95)00612-5" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, latex, 16 figures appended as uuencoded file</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Nucl.Instrum.Meth.A365:508-517,1995</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/hep-ex/9505004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/hep-ex/9505004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/hep-ex/0012003v1</id>
    <updated>2000-12-01T19:36:20Z</updated>
    <published>2000-12-01T19:36:20Z</published>
    <title>Application of Neural Networks for Energy Reconstruction</title>
    <summary>  The possibility to use Neural Networks for reconstruction of the energy
deposited in the calorimetry system of the CMS detector is investigated. It is
shown that using feed - forward neural network, good linearity, Gaussian energy
distribution and good energy resolution can be achieved. Significant
improvement of the energy resolution and linearity is reached in comparison
with other weighting methods for energy reconstruction.
</summary>
    <author>
      <name>J. Damgov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Sofia University, Sofia, Bulgaria</arxiv:affiliation>
    </author>
    <author>
      <name>L. Litov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Sofia University, Sofia, Bulgaria</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/S0168-9002(01)01851-4</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/S0168-9002(01)01851-4" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 13 figures, LATEX, submitted to: Nuclear Instruments &amp;
  Methods A</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Nucl.Instrum.Meth. A482 (2002) 776-788</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/hep-ex/0012003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/hep-ex/0012003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/hep-ex/0303021v2</id>
    <updated>2003-04-24T12:26:51Z</updated>
    <published>2003-03-14T10:40:45Z</published>
    <title>Neural networks technique based signal-from-background separation and
  design optimization for a W/quartz fiber calorimeter</title>
    <summary>  We present a signal-from-background separation study based on neural networks
technique applied to a W/quartz fiber calorimeter. Performance results in terms
of signal efficiency and improvement of the signal-to-background ratio are
presented. We conclude that by using neural networks we can efficiently
separate signal from background and achieve a signal enhancement over the
background of the order of several thousands at high efficiency.
</summary>
    <author>
      <name>G. Mavromanolakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LaTeX 22 pages, 4 tables, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/hep-ex/0303021v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/hep-ex/0303021v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nucl-ex" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/hep-ph/0004076v1</id>
    <updated>2000-04-07T19:43:12Z</updated>
    <published>2000-04-07T19:43:12Z</published>
    <title>A comparative study of the D0 neural-network analysis of the top quark
  non-leptonic decay channel</title>
    <summary>  A simpler neural-network approach is presented for the analysis of the top
quark non-leptonic decay channel in events of the D0 Collaboration. Results for
the top quark signal are comparable to those found by the D0 Collaboration by a
more elaborate handling of the event information used as input to the neural
network.
</summary>
    <author>
      <name>R. Odorico</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Bologna, Italy</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1209/epl/i2001-00418-8</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1209/epl/i2001-00418-8" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Europhys.Lett.55:324-326,2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/hep-ph/0004076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/hep-ph/0004076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/hep-ph/0505044v2</id>
    <updated>2005-05-12T07:37:34Z</updated>
    <published>2005-05-06T09:40:08Z</published>
    <title>The neural network approach to parton fitting</title>
    <summary>  We introduce the neural network approach to global fits of parton
distribution functions. First we review previous work on unbiased
parametrizations of deep-inelastic structure functions with faithful estimation
of their uncertainties, and then we summarize the current status of neural
network parton distribution fits.
</summary>
    <author>
      <name>Joan Rojo</name>
    </author>
    <author>
      <name>Andrea Piccione</name>
    </author>
    <author>
      <name>for the NNPDF Collaboration</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the proceedings of "DIS 2005: XIII International
  Workshop on Deep Inelatic Scattering" (Madison, Wisconsin, April 2005). 5
  pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/hep-ph/0505044v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/hep-ph/0505044v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/hep-ph/0509059v3</id>
    <updated>2005-10-05T13:43:46Z</updated>
    <published>2005-09-07T08:03:24Z</published>
    <title>The neural network approach to parton distributions</title>
    <summary>  We introduce the neural network approach to global fits of parton
distrubution functions. First we review previous work on unbiased
parametrizations of deep-inelastic structure functions with faithful estimation
of their uncertainties, and then we summarize the current status of neural
network parton distribution fits.
</summary>
    <author>
      <name>Andrea Piccione</name>
    </author>
    <author>
      <name>Joan Rojo</name>
    </author>
    <author>
      <name>for the NNPDF Collaboration</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Proceedings of the HERA-LHC workshop 2005, 4 pages, 2
  figures, plot updated</arxiv:comment>
    <link href="http://arxiv.org/abs/hep-ph/0509059v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/hep-ph/0509059v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/hep-ph/0509067v2</id>
    <updated>2005-10-18T14:33:00Z</updated>
    <published>2005-09-07T13:23:36Z</published>
    <title>Neural network approach to parton distributions fitting</title>
    <summary>  We will show an application of neural networks to extract information on the
structure of hadrons. A Monte Carlo over experimental data is performed to
correctly reproduce data errors and correlations. A neural network is then
trained on each Monte Carlo replica via a genetic algorithm. Results on the
proton and deuteron structure functions, and on the nonsinglet parton
distribution will be shown.
</summary>
    <author>
      <name>Andrea Piccione</name>
    </author>
    <author>
      <name>Joan Rojo</name>
    </author>
    <author>
      <name>for the NNPDF Collaboration</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.nima.2005.11.206</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.nima.2005.11.206" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 eps figures. Talk given by Andrea Piccione at the "X
  International Workshop on Advanced Computing and Analysis Techniques in
  Physics Research", ACAT 2005, DESY-Zeuthen, Germany, 22-27 May 2005.
  Corrected fig. 4</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Nucl.Instrum.Meth. A559 (2006) 203-206</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/hep-ph/0509067v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/hep-ph/0509067v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/math/9902023v1</id>
    <updated>1999-02-03T21:25:39Z</updated>
    <published>1999-02-03T21:25:39Z</published>
    <title>Further results on controllability of recurrent neural networks</title>
    <summary>  This paper studies controllability properties of recurrent neural networks.
The new contributions are:
  (1) an extension of the result in the previous paper "Complete
controllability of continuous-time recurrent neural networks" (Sontag and
Sussmann) to a slightly different model, where inputs appear in an affine form,
  (2) a formulation and proof of a necessary and sufficient condition, in terms
of local-local controllability, and
  (3) a complete analysis of the 2-dimensional case for which the hypotheses
made in previous work do not apply
</summary>
    <author>
      <name>Eduardo D. Sontag</name>
    </author>
    <author>
      <name>Y. Qiao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/math/9902023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/math/9902023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="93B05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/math/0503361v1</id>
    <updated>2005-03-17T16:14:30Z</updated>
    <published>2005-03-17T16:14:30Z</published>
    <title>Convergence Criteria for a Hopfield-type Neural Network</title>
    <summary>  Motivated by recent applications of the Lyapunov's method in artificial
neural networks, which could be considered as dynamical systems for which the
convergence of the system trajectories to equilibrium states is a necessity. We
re-look at a well-known Krasovskii's stability criteria pertaining to a non
linear autonomous system. Instead, we consider the components of the same
autonomous system with the help of the elements of Jacobian matrix J(x), thus
proposing much simpler convergence criteria via the method of Lyapunov. We then
apply our results to artificial neural networks and discuss our results with
respect to recent ones in the field.
</summary>
    <author>
      <name>Raveen Goundar</name>
    </author>
    <author>
      <name>Jito Vanualailai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/math/0503361v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/math/0503361v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="34D20; 92B20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/math/0507141v1</id>
    <updated>2005-07-07T11:03:30Z</updated>
    <published>2005-07-07T11:03:30Z</published>
    <title>Emergence of Synchronous Oscillations in Neural Networks Excited by
  Noise</title>
    <summary>  The presence of noise in non linear dynamical systems can play a constructive
role, increasing the degree of order and coherence or evoking improvements in
the performance of the system. An example of this positive influence in a
biological system is the impulse transmission in neurons and the
synchronization of a neural network. Integrating numerically the Fokker-Planck
equation we show a self-induced synchronized oscillation. Such an oscillatory
state appears in a neural network coupled with a feedback term, when this
system is excited by noise and the noise strength is within a certain range.
</summary>
    <author>
      <name>M. -P. Zorzano</name>
    </author>
    <author>
      <name>L. Vazquez</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/S0167-2789(03)00007-1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/S0167-2789(03)00007-1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 18 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physica D 179 (2003) 105-114</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/math/0507141v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/math/0507141v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/nucl-ex/9612001v1</id>
    <updated>1996-12-02T18:03:36Z</updated>
    <published>1996-12-02T18:03:36Z</published>
    <title>Separating True V0's from Combinatoric Background with a Neural Network</title>
    <summary>  A feedforward multilayered neural network has been trained to "recognize"
true V0's in the presence of a large combinatoric background using simulated
data for 2 GeV/nucleon Ni + Cu interactions. The resulting neural network
filter has been applied to actual data from the EOS TPC experiment. An
enhancement of signal to background over more traditional selection mechanisms
has been observed.
</summary>
    <author>
      <name>Marvin Justice</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Kent State University</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/S0168-9002(97)00995-9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/S0168-9002(97)00995-9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages LaTeX using elsart.sty and psfig, 5 ps figures. Submitted to
  NIM</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Nucl.Instrum.Meth. A400 (1997) 463-468</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/nucl-ex/9612001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/nucl-ex/9612001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nucl-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nucl-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/nucl-th/0511088v1</id>
    <updated>2005-11-30T09:42:06Z</updated>
    <published>2005-11-30T09:42:06Z</published>
    <title>Nuclear mass systematics by complementing the Finite Range Droplet Model
  with neural networks</title>
    <summary>  A neural-network model is developed to reproduce the differences between
experimental nuclear mass-excess values and the theoretical values given by the
Finite Range Droplet Model. The results point to the existence of subtle
regularities of nuclear structure not yet contained in the best
microscopic/phenomenological models of atomic masses. Combining the FRDM and
the neural-network model, we create a hybrid model with improved predictive
performance on nuclear-mass systematics and related quantities.
</summary>
    <author>
      <name>S. Athanassopoulos</name>
    </author>
    <author>
      <name>E. Mavrommatis</name>
    </author>
    <author>
      <name>K. A. Gernoth</name>
    </author>
    <author>
      <name>J. W. Clark</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings for the 15th Hellenic Symposium on Nuclear Physics</arxiv:comment>
    <link href="http://arxiv.org/abs/nucl-th/0511088v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/nucl-th/0511088v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nucl-th" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nucl-th" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/physics/9807018v1</id>
    <updated>1998-07-14T16:00:59Z</updated>
    <published>1998-07-14T16:00:59Z</published>
    <title>On the determination of probability density functions by using Neural
  Networks</title>
    <summary>  It is well known that the output of a Neural Network trained to disentangle
between two classes has a probabilistic interpretation in terms of the
a-posteriori Bayesian probability, provided that a unary representation is
taken for the output patterns. This fact is used to make Neural Networks
approximate probability density functions from examples in an unbinned way,
giving a better performace than ``standard binned procedures''. In addition,
the mapped p.d.f. has an analytical expression.
</summary>
    <author>
      <name>Lluis Garrido</name>
    </author>
    <author>
      <name>Aurelio Juste</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/S0010-4655(98)00107-6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/S0010-4655(98)00107-6" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages including 3 eps figures. Submitted to Comput. Phys. Commun</arxiv:comment>
    <link href="http://arxiv.org/abs/physics/9807018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/physics/9807018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/physics/9907042v1</id>
    <updated>1999-07-24T03:39:54Z</updated>
    <published>1999-07-24T03:39:54Z</published>
    <title>Some Exact Results of Hopfield Neural Networks and Applications</title>
    <summary>  A set of fixed points of the Hopfield type neural network was under
investigation. Its connection matrix is constructed with regard to the Hebb
rule from a highly symmetric set of the memorized patterns. Depending on the
external parameter the analytic description of the fixed points set had been
obtained. And as a conclusion, some exact results of Hopfield neural networks
were gained.
</summary>
    <author>
      <name>Hong-Liang Lu</name>
    </author>
    <author>
      <name>Xi-Jun Qiu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, latex, no figures</arxiv:comment>
    <link href="http://arxiv.org/abs/physics/9907042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/physics/9907042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/q-bio/0607021v1</id>
    <updated>2006-07-17T05:23:01Z</updated>
    <published>2006-07-17T05:23:01Z</published>
    <title>Activity-dependent self-wiring is a basis of structural plastisity in
  neural networks</title>
    <summary>  Dynamical wiring and rewiring in neural networks are carried out by
activity-dependent growth and retraction of axons and dendrites, guided by
gudance molecules, released by target cells. Experience-dependent structural
changes in cortical microcurcuts lead to changes in activity, i.e. to changes
in information encoded. Specific pattens of external stimulation can lead to
creation of new synaptical connections between neurons. Calcium influxes
controlled by neuronal activity regulates processes of neurotrophic factors
release by neurons, growth cones movement and synapse differentiation in
developing neural system, therefore activity-dependent self-wiring can serve as
a basis of structural plasticity in cortical networks and can be considered as
a form of learning.
</summary>
    <author>
      <name>Fail M. Gafarov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/q-bio/0607021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/q-bio/0607021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/quant-ph/0202015v3</id>
    <updated>2004-06-25T14:36:41Z</updated>
    <published>2002-02-02T16:39:54Z</published>
    <title>Semiclassical Neural Network</title>
    <summary>  We have constructed a simple semiclassical model of neural network where
neurons have quantum links with one another in a chosen way and affect one
another in a fashion analogous to action potentials. We have examined the role
of stochasticity introduced by the quantum potential and compare the system
with the classical system of an integrate-and-fire model by Hopfield. Average
periodicity and short term retentivity of input memory are noted.
</summary>
    <author>
      <name>Fariel Shafee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages 2 figures; new sections and figures added on short term
  memory and period</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">stochastics and dynamics, Vol. 7, No. 3 (2007) 403-416</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/quant-ph/0202015v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/quant-ph/0202015v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0704.2580v1</id>
    <updated>2007-04-19T18:23:25Z</updated>
    <published>2007-04-19T18:23:25Z</published>
    <title>Period-two cycles in a feed-forward layered neural network model with
  symmetric sequence processing</title>
    <summary>  The effects of dominant sequential interactions are investigated in an
exactly solvable feed-forward layered neural network model of binary units and
patterns near saturation in which the interaction consists of a Hebbian part
and a symmetric sequential term. Phase diagrams of stationary states are
obtained and a new phase of cyclic correlated states of period two is found for
a weak Hebbian term, independently of the number of condensed patterns $c$.
</summary>
    <author>
      <name>F. L. Metz</name>
    </author>
    <author>
      <name>W. K. Theumann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.75.041907</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.75.041907" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages and 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 75, 041907 (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0704.2580v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0704.2580v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0704.3515v1</id>
    <updated>2007-04-26T11:29:19Z</updated>
    <published>2007-04-26T11:29:19Z</published>
    <title>Comparing Robustness of Pairwise and Multiclass Neural-Network Systems
  for Face Recognition</title>
    <summary>  Noise, corruptions and variations in face images can seriously hurt the
performance of face recognition systems. To make such systems robust,
multiclass neuralnetwork classifiers capable of learning from noisy data have
been suggested. However on large face data sets such systems cannot provide the
robustness at a high level. In this paper we explore a pairwise neural-network
system as an alternative approach to improving the robustness of face
recognition. In our experiments this approach is shown to outperform the
multiclass neural-network system in terms of the predictive accuracy on the
face images corrupted by noise.
</summary>
    <author>
      <name>J. Uglov</name>
    </author>
    <author>
      <name>V. Schetinin</name>
    </author>
    <author>
      <name>C. Maple</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1155/2008/468693</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1155/2008/468693" rel="related"/>
    <link href="http://arxiv.org/abs/0704.3515v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0704.3515v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.1883v1</id>
    <updated>2008-01-12T08:02:12Z</updated>
    <published>2008-01-12T08:02:12Z</published>
    <title>D-optimal Bayesian Interrogation for Parameter and Noise Identification
  of Recurrent Neural Networks</title>
    <summary>  We introduce a novel online Bayesian method for the identification of a
family of noisy recurrent neural networks (RNNs). We develop Bayesian active
learning technique in order to optimize the interrogating stimuli given past
experiences. In particular, we consider the unknown parameters as stochastic
variables and use the D-optimality principle, also known as `\emph{infomax
method}', to choose optimal stimuli. We apply a greedy technique to maximize
the information gain concerning network parameters at each time step. We also
derive the D-optimal estimation of the additive noise that perturbs the
dynamical system of the RNN. Our analytical results are approximation-free. The
analytic derivation gives rise to attractive quadratic update rules.
</summary>
    <author>
      <name>Barnabas Poczos</name>
    </author>
    <author>
      <name>Andras Lorincz</name>
    </author>
    <link href="http://arxiv.org/abs/0801.1883v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.1883v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.1282v1</id>
    <updated>2010-07-08T03:58:25Z</updated>
    <published>2010-07-08T03:58:25Z</published>
    <title>A note on sample complexity of learning binary output neural networks
  under fixed input distributions</title>
    <summary>  We show that the learning sample complexity of a sigmoidal neural network
constructed by Sontag (1992) required to achieve a given misclassification
error under a fixed purely atomic distribution can grow arbitrarily fast: for
any prescribed rate of growth there is an input distribution having this rate
as the sample complexity, and the bound is asymptotically tight. The rate can
be superexponential, a non-recursive function, etc. We further observe that
Sontag's ANN is not Glivenko-Cantelli under any input distribution having a
non-atomic part.
</summary>
    <author>
      <name>Vladimir Pestov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SBRN.2010.10</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SBRN.2010.10" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, latex in IEEE conference proceedings format</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 2010 Eleventh Brazilian Symposium on Neural Networks (S\~ao
  Bernardo do Campo, SP, Brazil, 23-28 October 2010), IEEE Computer Society,
  2010, pp. 7-12</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1007.1282v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.1282v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.3501v1</id>
    <updated>2010-10-18T07:04:28Z</updated>
    <published>2010-10-18T07:04:28Z</published>
    <title>Forecasting with Neural Networks: A comparative study using the data of
  emergency service</title>
    <summary>  This is a case study discussing the supervised artificial neural network for
the purpose of forecasting with comparison of the Box-Jenkins methodology by
using the data of well known emergency service Rescue 1122. We fits a variety
of neural network (NN) models and many problems were revealed while fitting the
ANNs model to achieve the local minima. Moreover ANNs model is giving much
better out of sample forecasts as compare to the ARIMA model. However we use
diagnostic checks for the comparison of models.
</summary>
    <author>
      <name>Muhammad Noor-Ul-Amin</name>
    </author>
    <link href="http://arxiv.org/abs/1010.3501v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.3501v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.3724v1</id>
    <updated>2010-12-16T19:30:20Z</updated>
    <published>2010-12-16T19:30:20Z</published>
    <title>The Development of Dominance Stripes and Orientation Maps in a
  Self-Organising Visual Cortex Network (VICON)</title>
    <summary>  A self-organising neural network is presented that is based on a rigorous
Bayesian analysis of the information contained in individual neural firing
events. This leads to a visual cortex network (VICON) that has many of the
properties emerge when a mammalian visual cortex is exposed to data arriving
from two imaging sensors (i.e. the two retinae), such as dominance stripes and
orientation maps.
</summary>
    <author>
      <name>Stephen Luttrell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, 19 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1012.3724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.3724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.3966v1</id>
    <updated>2012-05-17T15:57:24Z</updated>
    <published>2012-05-17T15:57:24Z</published>
    <title>Neural Networks for Handwritten English Alphabet Recognition</title>
    <summary>  This paper demonstrates the use of neural networks for developing a system
that can recognize hand-written English alphabets. In this system, each English
alphabet is represented by binary values that are used as input to a simple
feature extraction system, whose output is fed to our neural network system.
</summary>
    <author>
      <name>Yusuf Perwej</name>
    </author>
    <author>
      <name>Ashish Chaturvedi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/2449-2824</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/2449-2824" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 Figure, ISSN:0975 - 8887</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications(IJCA), April 2011,
  Volume 20, Number 7, Pages 1-5</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1205.3966v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.3966v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.3892v1</id>
    <updated>2012-06-18T11:22:08Z</updated>
    <published>2012-06-18T11:22:08Z</published>
    <title>Interpretation of M√∂ssbauer spectra in the energy and time domain with
  neural networks</title>
    <summary>  An artificial neural network for extracting reasonable and fast estimates of
hyperfine parameters from M\"ossbauer spectra in the energy or time domain is
outlined. First promising results for determining the asymmetry of the electric
field gradient at the nucleus of a diamagnetic iron center as derived with
different types of neural networks are reported.
</summary>
    <author>
      <name>H. Paulsen</name>
    </author>
    <author>
      <name>R. Linder</name>
    </author>
    <author>
      <name>F. Wagner</name>
    </author>
    <author>
      <name>H. Winkler</name>
    </author>
    <author>
      <name>S. J. P√∂ppl</name>
    </author>
    <author>
      <name>A. X. Trautwein</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1023/A:1012606309138</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1023/A:1012606309138" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Hyperfine Interactions, 126, 1-4 (2000), 421-424</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1206.3892v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.3892v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.2254v1</id>
    <updated>2012-07-10T07:46:04Z</updated>
    <published>2012-07-10T07:46:04Z</published>
    <title>A Hybrid Forecast of Exchange Rate based on Discrete Grey-Markov and
  Grey Neural Network Model</title>
    <summary>  We propose a hybrid forecast model based on discrete grey-fuzzy Markov and
grey neural network model and show that our hybrid model can improve much more
the performance of forecast than traditional grey-Markov model and neural
network models. Our simulation results are shown that our hybrid forecast
method with the combinational weight based on optimal grey relation degree
method is better than the hybrid model with combinational weight based
minimization of error-squared criterion.
</summary>
    <author>
      <name>Gol Kim</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Center of Natural Science, University of Sciences, Pyongyang, DPR Korea</arxiv:affiliation>
    </author>
    <author>
      <name>Ri Suk Yun</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Foreign Economic General Bureau, Pyongyang, DPR Korea</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1207.2254v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.2254v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.6449v1</id>
    <updated>2014-04-24T14:37:19Z</updated>
    <published>2014-04-24T14:37:19Z</published>
    <title>Univariate error function based neural network approximation</title>
    <summary>  Here we research the univariate quantitative approximation of real and
complex valued continuous functions on a compact interval or all the real line
by quasi-interpolation, Baskakov type and quadrature type neural network
operators. We perform also the related fractional approximation. These
approximations are derived by establishing Jackson type inequalities involving
the modulus of continuity of the engaged function or its high order derivative
or fractional derivatives. Our operators are defined by using a density
function induced by the error function. The approximations are pointwise and
with respect to the uniform norm. The related feed-forward neural networks are
with one hidden layer.
</summary>
    <author>
      <name>George Anastassiou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.6449v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.6449v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="26A33, 41A17, 41A25, 41A30, 41A36" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0808.1378v1</id>
    <updated>2008-08-09T22:05:48Z</updated>
    <published>2008-08-09T22:05:48Z</published>
    <title>A Novel Symbolic Type Neural Network Model- Application to River Flow
  Forecasting</title>
    <summary>  In this paper we introduce a new symbolic type neural tree network called
symbolic function network (SFN) that is based on using elementary functions to
model systems in a symbolic form. The proposed formulation permits feature
selection, functional selection, and flexible structure. We applied this model
on the River Flow forecasting problem. The results found to be superior in both
fitness and sparsity.
</summary>
    <author>
      <name>George S. Eskander</name>
    </author>
    <author>
      <name>Amir F. Atiya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in ICENCO2007, Cairo, December 2007</arxiv:comment>
    <link href="http://arxiv.org/abs/0808.1378v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0808.1378v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.06004v1</id>
    <updated>2015-03-20T06:48:14Z</updated>
    <published>2015-03-20T06:48:14Z</published>
    <title>Feeder Load Balancing using Neural Network</title>
    <summary>  The distribution system problems, such as planning, loss minimization, and
energy restoration, usually involve the phase balancing or network
reconfiguration procedures. The determination of an optimal phase balance is,
in general, a combinatorial optimization problem. This paper proposes optimal
reconfiguration of the phase balancing using the neural network, to switch on
and off the different switches, allowing the three phases supply by the
transformer to the end-users to be balanced. This paper presents the
application examples of the proposed method using the real and simulated test
data.
</summary>
    <author>
      <name>A. Ukil</name>
    </author>
    <author>
      <name>W. Siti</name>
    </author>
    <author>
      <name>J. Jordaan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/11760023_190</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/11760023_190" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages in final published version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Lecture Notes in Computer Science, Springer, vol. 3972, pp.
  1311-1316, 2006</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1503.06004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.06004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.1900v1</id>
    <updated>2009-06-10T09:56:29Z</updated>
    <published>2009-06-10T09:56:29Z</published>
    <title>How deals with discrete data for the reduction of simulation models
  using neural network</title>
    <summary>  Simulation is useful for the evaluation of a Master Production/distribution
Schedule (MPS). Also, the goal of this paper is the study of the design of a
simulation model by reducing its complexity. According to theory of
constraints, we want to build reduced models composed exclusively by
bottlenecks and a neural network. Particularly a multilayer perceptron, is
used. The structure of the network is determined by using a pruning procedure.
This work focuses on the impact of discrete data on the results and compares
different approaches to deal with these data. This approach is applied to
sawmill internal supply chain
</summary>
    <author>
      <name>Philippe Thomas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRAN</arxiv:affiliation>
    </author>
    <author>
      <name>Andr√© Thomas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRAN</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">13th IFAC Symp. On Information Control Problems in Manufacturing
  INCOM'09, Moscou : Russie (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0906.1900v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.1900v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.2854v2</id>
    <updated>2011-07-27T13:51:52Z</updated>
    <published>2011-03-15T06:47:46Z</published>
    <title>A program for the Bayesian Neural Network in the ROOT framework</title>
    <summary>  We present a Bayesian Neural Network algorithm implemented in the TMVA
package, within the ROOT framework. Comparing to the conventional utilization
of Neural Network as discriminator, this new implementation has more advantages
as a non-parametric regression tool, particularly for fitting probabilities. It
provides functionalities including cost function selection, complexity control
and uncertainty estimation. An example of such application in High Energy
Physics is shown. The algorithm is available with ROOT release later than 5.29.
</summary>
    <author>
      <name>Jiahang Zhong</name>
    </author>
    <author>
      <name>Run-Sheng Huang</name>
    </author>
    <author>
      <name>Shih-Chang Lee</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cpc.2011.07.019</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cpc.2011.07.019" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Physics Communications Volume 182, Issue 12, December
  2011, Pages 2655-2660</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1103.2854v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.2854v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.2183v1</id>
    <updated>2013-09-09T15:03:59Z</updated>
    <published>2013-09-09T15:03:59Z</published>
    <title>Application of Artificial Neural Networks in Estimating Participation in
  Elections</title>
    <summary>  It is approved that artificial neural networks can be considerable effective
in anticipating and analyzing flows in which traditional methods and statics
are not able to solve. in this article, by using two-layer feedforward network
with tan-sigmoid transmission function in input and output layers, we can
anticipate participation rate of public in kohgiloye and boyerahmad province in
future presidential election of islamic republic of iran with 91% accuracy. the
assessment standards of participation such as confusion matrix and roc diagrams
have been approved our claims.
</summary>
    <author>
      <name>Seyyed Reza Khaze</name>
    </author>
    <author>
      <name>Mohammad Masdari</name>
    </author>
    <author>
      <name>Sohrab Hojjatkhah</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijitmc.2013.1303</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijitmc.2013.1303" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Information Technology, Modeling and
  Computing (IJITMC) Vol.1, No.3,August 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1309.2183v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.2183v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.6049v1</id>
    <updated>2013-11-23T20:52:05Z</updated>
    <published>2013-11-23T20:52:05Z</published>
    <title>Skin Texture Recognition Using Neural Networks</title>
    <summary>  Skin recognition is used in many applications ranging from algorithms for
face detection, hand gesture analysis, and to objectionable image filtering. In
this work a skin recognition system was developed and tested. While many skin
segmentation algorithms relay on skin color, our work relies on both skin color
and texture features (features derives from the GLCM) to give a better and more
efficient recognition accuracy of skin textures. We used feed forward neural
networks to classify input textures images to be skin or non skin textures. The
system gave very encouraging results during the neural network generalization
face.
</summary>
    <author>
      <name>Nidhal K. El Abbadi</name>
    </author>
    <author>
      <name>Nazar Dahir</name>
    </author>
    <author>
      <name>Zaid Abd Alkareem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 6 figures, conference ACIT 2008, Tunisia</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.6049v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.6049v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.0493v1</id>
    <updated>2013-12-02T15:54:40Z</updated>
    <published>2013-12-02T15:54:40Z</published>
    <title>Bidirectional Recursive Neural Networks for Token-Level Labeling with
  Structure</title>
    <summary>  Recently, deep architectures, such as recurrent and recursive neural networks
have been successfully applied to various natural language processing tasks.
Inspired by bidirectional recurrent neural networks which use representations
that summarize the past and future around an instance, we propose a novel
architecture that aims to capture the structural information around an input,
and use it to label instances. We apply our method to the task of opinion
expression extraction, where we employ the binary parse tree of a sentence as
the structure, and word vector representations as the initial representation of
a single token. We conduct preliminary experiments to investigate its
performance and compare it to the sequential approach.
</summary>
    <author>
      <name>Ozan ƒ∞rsoy</name>
    </author>
    <author>
      <name>Claire Cardie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures, NIPS Deep Learning Workshop 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.0493v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.0493v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0334v1</id>
    <updated>2014-09-01T08:59:27Z</updated>
    <published>2014-09-01T08:59:27Z</published>
    <title>Storing sequences in binary tournament-based neural networks</title>
    <summary>  An extension to a recently introduced architecture of clique-based neural
networks is presented. This extension makes it possible to store sequences with
high efficiency. To obtain this property, network connections are provided with
orientation and with flexible redundancy carried by both spatial and temporal
redundancy, a mechanism of anticipation being introduced in the model. In
addition to the sequence storage with high efficiency, this new scheme also
offers biological plausibility. In order to achieve accurate sequence
retrieval, a double layered structure combining hetero-association and
auto-association is also proposed.
</summary>
    <author>
      <name>Xiaoran Jiang</name>
    </author>
    <author>
      <name>Vincent Gripon</name>
    </author>
    <author>
      <name>Claude Berrou</name>
    </author>
    <author>
      <name>Michael Rabbat</name>
    </author>
    <link href="http://arxiv.org/abs/1409.0334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.02478v1</id>
    <updated>2015-02-09T13:29:48Z</updated>
    <published>2015-02-09T13:29:48Z</published>
    <title>Efficient batchwise dropout training using submatrices</title>
    <summary>  Dropout is a popular technique for regularizing artificial neural networks.
Dropout networks are generally trained by minibatch gradient descent with a
dropout mask turning off some of the units---a different pattern of dropout is
applied to every sample in the minibatch. We explore a very simple alternative
to the dropout mask. Instead of masking dropped out units by setting them to
zero, we perform matrix multiplication using a submatrix of the weight
matrix---unneeded hidden units are never calculated. Performing dropout
batchwise, so that one pattern of dropout is used for each sample in a
minibatch, we can substantially reduce training times. Batchwise dropout can be
used with fully-connected and convolutional neural networks.
</summary>
    <author>
      <name>Ben Graham</name>
    </author>
    <author>
      <name>Jeremy Reizenstein</name>
    </author>
    <author>
      <name>Leigh Robinson</name>
    </author>
    <link href="http://arxiv.org/abs/1502.02478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.02478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.02506v1</id>
    <updated>2015-02-09T14:46:40Z</updated>
    <published>2015-02-09T14:46:40Z</published>
    <title>Predicting Alzheimer's disease: a neuroimaging study with 3D
  convolutional neural networks</title>
    <summary>  Pattern recognition methods using neuroimaging data for the diagnosis of
Alzheimer's disease have been the subject of extensive research in recent
years. In this paper, we use deep learning methods, and in particular sparse
autoencoders and 3D convolutional neural networks, to build an algorithm that
can predict the disease status of a patient, based on an MRI scan of the brain.
We report on experiments using the ADNI data set involving 2,265 historical
scans. We demonstrate that 3D convolutional neural networks outperform several
other classifiers reported in the literature and produce state-of-art results.
</summary>
    <author>
      <name>Adrien Payan</name>
    </author>
    <author>
      <name>Giovanni Montana</name>
    </author>
    <link href="http://arxiv.org/abs/1502.02506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.02506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.04623v2</id>
    <updated>2015-05-20T15:29:42Z</updated>
    <published>2015-02-16T16:48:56Z</published>
    <title>DRAW: A Recurrent Neural Network For Image Generation</title>
    <summary>  This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural
network architecture for image generation. DRAW networks combine a novel
spatial attention mechanism that mimics the foveation of the human eye, with a
sequential variational auto-encoding framework that allows for the iterative
construction of complex images. The system substantially improves on the state
of the art for generative models on MNIST, and, when trained on the Street View
House Numbers dataset, it generates images that cannot be distinguished from
real data with the naked eye.
</summary>
    <author>
      <name>Karol Gregor</name>
    </author>
    <author>
      <name>Ivo Danihelka</name>
    </author>
    <author>
      <name>Alex Graves</name>
    </author>
    <author>
      <name>Danilo Jimenez Rezende</name>
    </author>
    <author>
      <name>Daan Wierstra</name>
    </author>
    <link href="http://arxiv.org/abs/1502.04623v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.04623v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.04819v1</id>
    <updated>2015-04-19T10:44:19Z</updated>
    <published>2015-04-19T10:44:19Z</published>
    <title>Forecasting the term structure of crude oil futures prices with neural
  networks</title>
    <summary>  The paper contributes to the rare literature modeling term structure of crude
oil markets. We explain term structure of crude oil prices using dynamic
Nelson-Siegel model, and propose to forecast them with the generalized
regression framework based on neural networks. The newly proposed framework is
empirically tested on 24 years of crude oil futures prices covering several
important recessions and crisis periods. We find 1-month, 3-month, 6-month and
12-month-ahead forecasts obtained from focused time-delay neural network to be
significantly more accurate than forecasts from other benchmark models. The
proposed forecasting strategy produces the lowest errors across all times to
maturity.
</summary>
    <author>
      <name>Jozef Barunik</name>
    </author>
    <author>
      <name>Barbora Malinska</name>
    </author>
    <link href="http://arxiv.org/abs/1504.04819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.04819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.02617v1</id>
    <updated>2015-06-08T19:01:33Z</updated>
    <published>2015-06-08T19:01:33Z</published>
    <title>Path-SGD: Path-Normalized Optimization in Deep Neural Networks</title>
    <summary>  We revisit the choice of SGD for training deep neural networks by
reconsidering the appropriate geometry in which to optimize the weights. We
argue for a geometry invariant to rescaling of weights that does not affect the
output of the network, and suggest Path-SGD, which is an approximate steepest
descent method with respect to a path-wise regularizer related to max-norm
regularization. Path-SGD is easy and efficient to implement and leads to
empirical gains over SGD and AdaGrad.
</summary>
    <author>
      <name>Behnam Neyshabur</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Nathan Srebro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.02617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.02617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.05849v1</id>
    <updated>2015-06-18T23:11:40Z</updated>
    <published>2015-06-18T23:11:40Z</published>
    <title>An Iterative Convolutional Neural Network Algorithm Improves Electron
  Microscopy Image Segmentation</title>
    <summary>  To build the connectomics map of the brain, we developed a new algorithm that
can automatically refine the Membrane Detection Probability Maps (MDPM)
generated to perform automatic segmentation of electron microscopy (EM) images.
To achieve this, we executed supervised training of a convolutional neural
network to recover the removed center pixel label of patches sampled from a
MDPM. MDPM can be generated from other machine learning based algorithms
recognizing whether a pixel in an image corresponds to the cell membrane. By
iteratively applying this network over MDPM for multiple rounds, we were able
to significantly improve membrane segmentation results.
</summary>
    <author>
      <name>Xundong Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1506.05849v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.05849v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.07650v1</id>
    <updated>2015-06-25T07:51:55Z</updated>
    <published>2015-06-25T07:51:55Z</published>
    <title>Semantic Relation Classification via Convolutional Neural Networks with
  Simple Negative Sampling</title>
    <summary>  Syntactic features play an essential role in identifying relationship in a
sentence. Previous neural network models often suffer from irrelevant
information introduced when subjects and objects are in a long distance. In
this paper, we propose to learn more robust relation representations from the
shortest dependency path through a convolution neural network. We further
propose a straightforward negative sampling strategy to improve the assignment
of subjects and objects. Experimental results show that our method outperforms
the state-of-the-art methods on the SemEval-2010 Task 8 dataset.
</summary>
    <author>
      <name>Kun Xu</name>
    </author>
    <author>
      <name>Yansong Feng</name>
    </author>
    <author>
      <name>Songfang Huang</name>
    </author>
    <author>
      <name>Dongyan Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/1506.07650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.07650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.01839v2</id>
    <updated>2015-08-03T15:36:45Z</updated>
    <published>2015-07-07T15:20:36Z</published>
    <title>Dependency-based Convolutional Neural Networks for Sentence Embedding</title>
    <summary>  In sentence modeling and classification, convolutional neural network
approaches have recently achieved state-of-the-art results, but all such
efforts process word vectors sequentially and neglect long-distance
dependencies. To exploit both deep learning and linguistic structures, we
propose a tree-based convolutional neural network model which exploit various
long-distance relationships between words. Our model improves the sequential
baselines on all three sentiment and question classification tasks, and
achieves the highest published accuracy on TREC.
</summary>
    <author>
      <name>Mingbo Ma</name>
    </author>
    <author>
      <name>Liang Huang</name>
    </author>
    <author>
      <name>Bing Xiang</name>
    </author>
    <author>
      <name>Bowen Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">this paper has been accepted by ACL 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.01839v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.01839v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.05051v1</id>
    <updated>2015-08-20T17:21:50Z</updated>
    <published>2015-08-20T17:21:50Z</published>
    <title>Auto-Sizing Neural Networks: With Applications to n-gram Language Models</title>
    <summary>  Neural networks have been shown to improve performance across a range of
natural-language tasks. However, designing and training them can be
complicated. Frequently, researchers resort to repeated experimentation to pick
optimal settings. In this paper, we address the issue of choosing the correct
number of units in hidden layers. We introduce a method for automatically
adjusting network size by pruning out hidden units through $\ell_{\infty,1}$
and $\ell_{2,1}$ regularization. We apply this method to language modeling and
demonstrate its ability to correctly choose the number of hidden units while
maintaining perplexity. We also include these models in a machine translation
decoder and show that these smaller neural models maintain the significant
improvements of their unpruned versions.
</summary>
    <author>
      <name>Kenton Murray</name>
    </author>
    <author>
      <name>David Chiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.05051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.05051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.07551v1</id>
    <updated>2015-08-30T10:14:48Z</updated>
    <published>2015-08-30T10:14:48Z</published>
    <title>X-TREPAN: a multi class regression and adapted extraction of
  comprehensible decision tree in artificial neural networks</title>
    <summary>  In this work, the TREPAN algorithm is enhanced and extended for extracting
decision trees from neural networks. We empirically evaluated the performance
of the algorithm on a set of databases from real world events. This benchmark
enhancement was achieved by adapting Single-test TREPAN and C4.5 decision tree
induction algorithms to analyze the datasets. The models are then compared with
X-TREPAN for comprehensibility and classification accuracy. Furthermore, we
validate the experimentations by applying statistical methods. Finally, the
modified algorithm is extended to work with multi-class regression problems and
the ability to comprehend generalized feed forward networks is achieved.
</summary>
    <author>
      <name>Awudu Karim</name>
    </author>
    <author>
      <name>Shangbo Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 Pages, 8 Tables, 8 Figures, 6 Equations</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.07551v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.07551v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.09292v2</id>
    <updated>2015-11-03T17:18:32Z</updated>
    <published>2015-09-30T18:33:50Z</published>
    <title>Convolutional Networks on Graphs for Learning Molecular Fingerprints</title>
    <summary>  We introduce a convolutional neural network that operates directly on graphs.
These networks allow end-to-end learning of prediction pipelines whose inputs
are graphs of arbitrary size and shape. The architecture we present generalizes
standard molecular feature extraction methods based on circular fingerprints.
We show that these data-driven features are more interpretable, and have better
predictive performance on a variety of tasks.
</summary>
    <author>
      <name>David Duvenaud</name>
    </author>
    <author>
      <name>Dougal Maclaurin</name>
    </author>
    <author>
      <name>Jorge Aguilera-Iparraguirre</name>
    </author>
    <author>
      <name>Rafael G√≥mez-Bombarelli</name>
    </author>
    <author>
      <name>Timothy Hirzel</name>
    </author>
    <author>
      <name>Al√°n Aspuru-Guzik</name>
    </author>
    <author>
      <name>Ryan P. Adams</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures. To appear in Neural Information Processing
  Systems (NIPS)</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.09292v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.09292v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.02442v1</id>
    <updated>2015-10-08T18:55:22Z</updated>
    <published>2015-10-08T18:55:22Z</published>
    <title>Uniform Learning in a Deep Neural Network via "Oddball" Stochastic
  Gradient Descent</title>
    <summary>  When training deep neural networks, it is typically assumed that the training
examples are uniformly difficult to learn. Or, to restate, it is assumed that
the training error will be uniformly distributed across the training examples.
Based on these assumptions, each training example is used an equal number of
times. However, this assumption may not be valid in many cases. "Oddball SGD"
(novelty-driven stochastic gradient descent) was recently introduced to drive
training probabilistically according to the error distribution - training
frequency is proportional to training error magnitude. In this article, using a
deep neural network to encode a video, we show that oddball SGD can be used to
enforce uniform error across the training set.
</summary>
    <author>
      <name>Andrew J. R. Simpson</name>
    </author>
    <link href="http://arxiv.org/abs/1510.02442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.02442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.07137v2</id>
    <updated>2016-12-30T04:32:52Z</updated>
    <published>2016-01-23T22:02:25Z</published>
    <title>Posner computing: a quantum neural network model</title>
    <summary>  We present a construction, rendered in Quipper, of a quantum algorithm which
probabilistically computes a classical function from n bits to n bits. The
construction is intended to be of interest primarily for the features of
Quipper it highlights. However, intrigued by the utility of quantum information
processing in the context of neural networks, we present the algorithm as a
simplest example of a particular quantum neural network which we first define.
As the definition is inspired by recent work of Fisher concerning possible
quantum substrates to cognition, we precede it with a short description of that
work.
</summary>
    <author>
      <name>James L. Ulrich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.07137v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.07137v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.01895v1</id>
    <updated>2016-02-05T00:17:18Z</updated>
    <published>2016-02-05T00:17:18Z</published>
    <title>Generate Image Descriptions based on Deep RNN and Memory Cells for
  Images Features</title>
    <summary>  Generating natural language descriptions for images is a challenging task.
The traditional way is to use the convolutional neural network (CNN) to extract
image features, followed by recurrent neural network (RNN) to generate
sentences. In this paper, we present a new model that added memory cells to
gate the feeding of image features to the deep neural network. The intuition is
enabling our model to memorize how much information from images should be fed
at each stage of the RNN. Experiments on Flickr8K and Flickr30K datasets showed
that our model outperforms other state-of-the-art models with higher BLEU
scores.
</summary>
    <author>
      <name>Shijian Tang</name>
    </author>
    <author>
      <name>Song Han</name>
    </author>
    <link href="http://arxiv.org/abs/1602.01895v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.01895v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.05897v2</id>
    <updated>2017-05-19T18:39:00Z</updated>
    <published>2016-02-18T18:14:19Z</published>
    <title>Toward Deeper Understanding of Neural Networks: The Power of
  Initialization and a Dual View on Expressivity</title>
    <summary>  We develop a general duality between neural networks and compositional
kernels, striving towards a better understanding of deep learning. We show that
initial representations generated by common random initializations are
sufficiently rich to express all functions in the dual kernel space. Hence,
though the training objective is hard to optimize in the worst case, the
initial weights form a good starting point for optimization. Our dual view also
reveals a pragmatic and aesthetic perspective of neural networks and
underscores their expressive power.
</summary>
    <author>
      <name>Amit Daniely</name>
    </author>
    <author>
      <name>Roy Frostig</name>
    </author>
    <author>
      <name>Yoram Singer</name>
    </author>
    <link href="http://arxiv.org/abs/1602.05897v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.05897v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.09070v1</id>
    <updated>2016-02-29T17:56:06Z</updated>
    <published>2016-02-29T17:56:06Z</published>
    <title>Nanoscale artificial intelligence: creating artificial neural networks
  using autocatalytic reactions</title>
    <summary>  A general methodology is proposed to engineer a system of interacting
components (particles) which is able to self-regulate their concentrations in
order to produce any prescribed output in response to a particular input. The
methodology is based on the mathematical equivalence between artificial neurons
in neural networks and species in autocatalytic reactions, and it specifies the
relationship between the artificial neural network's parameters and the rate
coefficients of the reactions between particle species. Such systems are
characterised by a high degree of robustness as they are able to reach the
desired output despite disturbances and perturbations of the concentrations of
the various species.
</summary>
    <author>
      <name>Filippo Simini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.09070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.09070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.02434v1</id>
    <updated>2016-03-08T09:31:53Z</updated>
    <published>2016-03-08T09:31:53Z</published>
    <title>Effective Mean-Field Inference Method for Nonnegative Boltzmann Machines</title>
    <summary>  Nonnegative Boltzmann machines (NNBMs) are recurrent probabilistic neural
network models that can describe multi-modal nonnegative data. NNBMs form
rectified Gaussian distributions that appear in biological neural network
models, positive matrix factorization, nonnegative matrix factorization, and so
on. In this paper, an effective inference method for NNBMs is proposed that
uses the mean-field method, referred to as the Thouless--Anderson--Palmer
equation, and the diagonal consistency method, which was recently proposed.
</summary>
    <author>
      <name>Muneki Yasuda</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of 22nd International Conference on Pattern
  Recognition (ICPR2014), pp.3600-3605, 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1603.02434v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.02434v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.03657v1</id>
    <updated>2016-03-11T15:16:09Z</updated>
    <published>2016-03-11T15:16:09Z</published>
    <title>Efficient forward propagation of time-sequences in convolutional neural
  networks using Deep Shifting</title>
    <summary>  When a Convolutional Neural Network is used for on-the-fly evaluation of
continuously updating time-sequences, many redundant convolution operations are
performed. We propose the method of Deep Shifting, which remembers previously
calculated results of convolution operations in order to minimize the number of
calculations. The reduction in complexity is at least a constant and in the
best case quadratic. We demonstrate that this method does indeed save
significant computation time in a practical implementation, especially when the
networks receives a large number of time-frames.
</summary>
    <author>
      <name>Koen Groenland</name>
    </author>
    <author>
      <name>Sander Bohte</name>
    </author>
    <link href="http://arxiv.org/abs/1603.03657v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.03657v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.03640v1</id>
    <updated>2016-04-13T02:59:34Z</updated>
    <published>2016-04-13T02:59:34Z</published>
    <title>Bridging the Gaps Between Residual Learning, Recurrent Neural Networks
  and Visual Cortex</title>
    <summary>  We discuss relations between Residual Networks (ResNet), Recurrent Neural
Networks (RNNs) and the primate visual cortex. We begin with the observation
that a shallow RNN is exactly equivalent to a very deep ResNet with weight
sharing among the layers. A direct implementation of such a RNN, although
having orders of magnitude fewer parameters, leads to a performance similar to
the corresponding ResNet. We propose 1) a generalization of both RNN and ResNet
architectures and 2) the conjecture that a class of moderately deep RNNs is a
biologically-plausible model of the ventral stream in visual cortex. We
demonstrate the effectiveness of the architectures by testing them on the
CIFAR-10 dataset.
</summary>
    <author>
      <name>Qianli Liao</name>
    </author>
    <author>
      <name>Tomaso Poggio</name>
    </author>
    <link href="http://arxiv.org/abs/1604.03640v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03640v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06985v3</id>
    <updated>2016-05-08T10:07:34Z</updated>
    <published>2016-04-24T05:17:04Z</published>
    <title>Deep Learning with Eigenvalue Decay Regularizer</title>
    <summary>  This paper extends our previous work on regularization of neural networks
using Eigenvalue Decay by employing a soft approximation of the dominant
eigenvalue in order to enable the calculation of its derivatives in relation to
the synaptic weights, and therefore the application of back-propagation, which
is a primary demand for deep learning. Moreover, we extend our previous
theoretical analysis to deep neural networks and multiclass classification
problems. Our method is implemented as an additional regularizer in Keras, a
modular neural networks library written in Python, and evaluated in the
benchmark data sets Reuters Newswire Topics Classification, IMDB database for
binary sentiment classification, MNIST database of handwritten digits and
CIFAR-10 data set for image classification.
</summary>
    <author>
      <name>Oswaldo Ludwig</name>
    </author>
    <link href="http://arxiv.org/abs/1604.06985v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06985v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.08543v1</id>
    <updated>2016-05-27T08:49:21Z</updated>
    <published>2016-05-27T08:49:21Z</published>
    <title>Lazy Evaluation of Convolutional Filters</title>
    <summary>  In this paper we propose a technique which avoids the evaluation of certain
convolutional filters in a deep neural network. This allows to trade-off the
accuracy of a deep neural network with the computational and memory
requirements. This is especially important on a constrained device unable to
hold all the weights of the network in memory.
</summary>
    <author>
      <name>Sam Leroux</name>
    </author>
    <author>
      <name>Steven Bohez</name>
    </author>
    <author>
      <name>Cedric De Boom</name>
    </author>
    <author>
      <name>Elias De Coninck</name>
    </author>
    <author>
      <name>Tim Verbelen</name>
    </author>
    <author>
      <name>Bert Vankeirsbilck</name>
    </author>
    <author>
      <name>Pieter Simoens</name>
    </author>
    <author>
      <name>Bart Dhoedt</name>
    </author>
    <link href="http://arxiv.org/abs/1605.08543v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08543v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.09593v2</id>
    <updated>2017-09-28T10:24:50Z</updated>
    <published>2016-05-31T12:11:51Z</published>
    <title>Adaptive Learning Rate via Covariance Matrix Based Preconditioning for
  Deep Neural Networks</title>
    <summary>  Adaptive learning rate algorithms such as RMSProp are widely used for
training deep neural networks. RMSProp offers efficient training since it uses
first order gradients to approximate Hessian-based preconditioning. However,
since the first order gradients include noise caused by stochastic
optimization, the approximation may be inaccurate. In this paper, we propose a
novel adaptive learning rate algorithm called SDProp. Its key idea is effective
handling of the noise by preconditioning based on covariance matrix. For
various neural networks, our approach is more efficient and effective than
RMSProp and its variant.
</summary>
    <author>
      <name>Yasutoshi Ida</name>
    </author>
    <author>
      <name>Yasuhiro Fujiwara</name>
    </author>
    <author>
      <name>Sotetsu Iwamura</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.24963/ijcai.2017/267</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.24963/ijcai.2017/267" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at IJCAI 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.09593v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09593v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07783v1</id>
    <updated>2016-06-24T18:35:56Z</updated>
    <published>2016-06-24T18:35:56Z</published>
    <title>Sequential Convolutional Neural Networks for Slot Filling in Spoken
  Language Understanding</title>
    <summary>  We investigate the usage of convolutional neural networks (CNNs) for the slot
filling task in spoken language understanding. We propose a novel CNN
architecture for sequence labeling which takes into account the previous
context words with preserved order information and pays special attention to
the current word with its surrounding context. Moreover, it combines the
information from the past and the future words for classification. Our proposed
CNN architecture outperforms even the previously best ensembling recurrent
neural network model and achieves state-of-the-art results with an F1-score of
95.61% on the ATIS benchmark dataset without using any additional linguistic
knowledge and resources.
</summary>
    <author>
      <name>Ngoc Thang Vu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at Interspeech 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.07783v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07783v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.03785v1</id>
    <updated>2016-07-13T15:18:25Z</updated>
    <published>2016-07-13T15:18:25Z</published>
    <title>Application of Convolutional Neural Network for Image Classification on
  Pascal VOC Challenge 2012 dataset</title>
    <summary>  In this project we work on creating a model to classify images for the Pascal
VOC Challenge 2012. We use convolutional neural networks trained on a single
GPU instance provided by Amazon via their cloud service Amazon Web Services
(AWS) to classify images in the Pascal VOC 2012 data set. We train multiple
convolutional neural network models and finally settle on the best model which
produced a validation accuracy of 85.6% and a testing accuracy of 85.24%.
</summary>
    <author>
      <name>Suyash Shetty</name>
    </author>
    <link href="http://arxiv.org/abs/1607.03785v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.03785v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02893v2</id>
    <updated>2016-08-26T20:55:41Z</updated>
    <published>2016-08-08T01:30:45Z</published>
    <title>Syntactically Informed Text Compression with Recurrent Neural Networks</title>
    <summary>  We present a self-contained system for constructing natural language models
for use in text compression. Our system improves upon previous neural network
based models by utilizing recent advances in syntactic parsing -- Google's
SyntaxNet -- to augment character-level recurrent neural networks. RNNs have
proven exceptional in modeling sequence data such as text, as their
architecture allows for modeling of long-term contextual information.
</summary>
    <author>
      <name>David Cox</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.02893v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02893v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04080v1</id>
    <updated>2016-08-14T09:32:17Z</updated>
    <published>2016-08-14T09:32:17Z</published>
    <title>Dynamic Hand Gesture Recognition for Wearable Devices with Low
  Complexity Recurrent Neural Networks</title>
    <summary>  Gesture recognition is a very essential technology for many wearable devices.
While previous algorithms are mostly based on statistical methods including the
hidden Markov model, we develop two dynamic hand gesture recognition techniques
using low complexity recurrent neural network (RNN) algorithms. One is based on
video signal and employs a combined structure of a convolutional neural network
(CNN) and an RNN. The other uses accelerometer data and only requires an RNN.
Fixed-point optimization that quantizes most of the weights into two bits is
conducted to optimize the amount of memory size for weight storage and reduce
the power consumption in hardware and software based implementations.
</summary>
    <author>
      <name>Sungho Shin</name>
    </author>
    <author>
      <name>Wonyong Sung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper was accepted in ISCAS 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.04080v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04080v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.05566v1</id>
    <updated>2016-09-18T23:16:14Z</updated>
    <published>2016-09-18T23:16:14Z</published>
    <title>Label-Free Supervision of Neural Networks with Physics and Domain
  Knowledge</title>
    <summary>  In many machine learning applications, labeled data is scarce and obtaining
more labels is expensive. We introduce a new approach to supervising neural
networks by specifying constraints that should hold over the output space,
rather than direct examples of input-output pairs. These constraints are
derived from prior domain knowledge, e.g., from known laws of physics. We
demonstrate the effectiveness of this approach on real world and simulated
computer vision tasks. We are able to train a convolutional neural network to
detect and track objects without any labeled examples. Our approach can
significantly reduce the need for labeled training data, but introduces new
challenges for encoding prior knowledge into appropriate loss functions.
</summary>
    <author>
      <name>Russell Stewart</name>
    </author>
    <author>
      <name>Stefano Ermon</name>
    </author>
    <link href="http://arxiv.org/abs/1609.05566v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.05566v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.09087v2</id>
    <updated>2017-06-24T08:40:27Z</updated>
    <published>2016-09-28T20:00:07Z</published>
    <title>Detection of phase transition via convolutional neural network</title>
    <summary>  We design a Convolutional Neural Network (CNN) which studies correlation
between discretized inverse temperature and spin configuration of 2D Ising
model and show that it can find a feature of the phase transition without
teaching any a priori information for it. We also define a new order parameter
via the CNN and show that it provides well approximated critical inverse
temperature. In addition, we compare the activation functions for convolution
layer and find that the Rectified Linear Unit (ReLU) is important to detect the
phase transition of 2D Ising model.
</summary>
    <author>
      <name>Akinori Tanaka</name>
    </author>
    <author>
      <name>Akio Tomiya</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.7566/JPSJ.86.063001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.7566/JPSJ.86.063001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.09087v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09087v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-lat" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.00321v1</id>
    <updated>2016-10-02T17:35:58Z</updated>
    <published>2016-10-02T17:35:58Z</published>
    <title>Low-dose CT denoising with convolutional neural network</title>
    <summary>  To reduce the potential radiation risk, low-dose CT has attracted much
attention. However, simply lowering the radiation dose will lead to significant
deterioration of the image quality. In this paper, we propose a noise reduction
method for low-dose CT via deep neural network without accessing original
projection data. A deep convolutional neural network is trained to transform
low-dose CT images towards normal-dose CT images, patch by patch. Visual and
quantitative evaluation demonstrates a competing performance of the proposed
method.
</summary>
    <author>
      <name>Hu Chen</name>
    </author>
    <author>
      <name>Yi Zhang</name>
    </author>
    <author>
      <name>Weihua Zhang</name>
    </author>
    <author>
      <name>Peixi Liao</name>
    </author>
    <author>
      <name>Ke Li</name>
    </author>
    <author>
      <name>Jiliu Zhou</name>
    </author>
    <author>
      <name>Ge Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1609.08508</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.00321v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.00321v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.02583v3</id>
    <updated>2018-01-14T05:42:17Z</updated>
    <published>2016-10-08T21:10:40Z</published>
    <title>A Gentle Tutorial of Recurrent Neural Network with Error Backpropagation</title>
    <summary>  We describe recurrent neural networks (RNNs), which have attracted great
attention on sequential tasks, such as handwriting recognition, speech
recognition and image to text. However, compared to general feedforward neural
networks, RNNs have feedback loops, which makes it a little hard to understand
the backpropagation step. Thus, we focus on basics, especially the error
backpropagation to compute gradients with respect to model parameters. Further,
we go into detail on how error backpropagation algorithm is applied on long
short-term memory (LSTM) by unfolding the memory unit.
</summary>
    <author>
      <name>Gang Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.02583v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.02583v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.01747v1</id>
    <updated>2016-11-06T09:50:24Z</updated>
    <published>2016-11-06T09:50:24Z</published>
    <title>A Compare-Aggregate Model for Matching Text Sequences</title>
    <summary>  Many NLP tasks including machine comprehension, answer selection and text
entailment require the comparison between sequences. Matching the important
units between sequences is a key to solve these problems. In this paper, we
present a general "compare-aggregate" framework that performs word-level
matching followed by aggregation using Convolutional Neural Networks. We
particularly focus on the different comparison functions we can use to match
two vectors. We use four different datasets to evaluate the model. We find that
some simple comparison functions based on element-wise operations can work
better than standard neural network and neural tensor network.
</summary>
    <author>
      <name>Shuohang Wang</name>
    </author>
    <author>
      <name>Jing Jiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.01747v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.01747v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.02064v2</id>
    <updated>2016-11-16T09:21:40Z</updated>
    <published>2016-11-07T14:16:18Z</published>
    <title>A Fully Convolutional Neural Network based Structured Prediction
  Approach Towards the Retinal Vessel Segmentation</title>
    <summary>  Automatic segmentation of retinal blood vessels from fundus images plays an
important role in the computer aided diagnosis of retinal diseases. The task of
blood vessel segmentation is challenging due to the extreme variations in
morphology of the vessels against noisy background. In this paper, we formulate
the segmentation task as a multi-label inference task and utilize the implicit
advantages of the combination of convolutional neural networks and structured
prediction. Our proposed convolutional neural network based model achieves
strong performance and significantly outperforms the state-of-the-art for
automatic retinal blood vessel segmentation on DRIVE dataset with 95.33%
accuracy and 0.974 AUC score.
</summary>
    <author>
      <name>Avijit Dasgupta</name>
    </author>
    <author>
      <name>Sonam Singh</name>
    </author>
    <link href="http://arxiv.org/abs/1611.02064v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.02064v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.02764v1</id>
    <updated>2016-11-08T23:10:24Z</updated>
    <published>2016-11-08T23:10:24Z</published>
    <title>Inferring low-dimensional microstructure representations using
  convolutional neural networks</title>
    <summary>  We apply recent advances in machine learning and computer vision to a central
problem in materials informatics: The statistical representation of
microstructural images. We use activations in a pre-trained convolutional
neural network to provide a high-dimensional characterization of a set of
synthetic microstructural images. Next, we use manifold learning to obtain a
low-dimensional embedding of this statistical characterization. We show that
the low-dimensional embedding extracts the parameters used to generate the
images. According to a variety of metrics, the convolutional neural network
method yields dramatically better embeddings than the analogous method derived
from two-point correlations alone.
</summary>
    <author>
      <name>Nicholas Lubbers</name>
    </author>
    <author>
      <name>Turab Lookman</name>
    </author>
    <author>
      <name>Kipton Barros</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.96.052111</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.96.052111" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 Pages, 12 Figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 96, 052111 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1611.02764v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.02764v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.06791v1</id>
    <updated>2016-11-21T14:06:48Z</updated>
    <published>2016-11-21T14:06:48Z</published>
    <title>Generalized Dropout</title>
    <summary>  Deep Neural Networks often require good regularizers to generalize well.
Dropout is one such regularizer that is widely used among Deep Learning
practitioners. Recent work has shown that Dropout can also be viewed as
performing Approximate Bayesian Inference over the network parameters. In this
work, we generalize this notion and introduce a rich family of regularizers
which we call Generalized Dropout. One set of methods in this family, called
Dropout++, is a version of Dropout with trainable parameters. Classical Dropout
emerges as a special case of this method. Another member of this family selects
the width of neural network layers. Experiments show that these methods help in
improving generalization performance over Dropout.
</summary>
    <author>
      <name>Suraj Srinivas</name>
    </author>
    <author>
      <name>R. Venkatesh Babu</name>
    </author>
    <link href="http://arxiv.org/abs/1611.06791v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.06791v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.07422v1</id>
    <updated>2016-11-02T02:47:26Z</updated>
    <published>2016-11-02T02:47:26Z</published>
    <title>Deep Learning Approximation for Stochastic Control Problems</title>
    <summary>  Many real world stochastic control problems suffer from the "curse of
dimensionality". To overcome this difficulty, we develop a deep learning
approach that directly solves high-dimensional stochastic control problems
based on Monte-Carlo sampling. We approximate the time-dependent controls as
feedforward neural networks and stack these networks together through model
dynamics. The objective function for the control problem plays the role of the
loss function for the deep neural network. We test this approach using examples
from the areas of optimal trading and energy storage. Our results suggest that
the algorithm presented here achieves satisfactory accuracy and at the same
time, can handle rather high dimensional problems.
</summary>
    <author>
      <name>Jiequn Han</name>
    </author>
    <author>
      <name>Weinan E</name>
    </author>
    <link href="http://arxiv.org/abs/1611.07422v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.07422v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09159v2</id>
    <updated>2017-07-14T15:50:22Z</updated>
    <published>2016-11-28T15:09:10Z</published>
    <title>Large-Scale Shape Retrieval with Sparse 3D Convolutional Neural Networks</title>
    <summary>  In this paper we present results of performance evaluation of S3DCNN - a
Sparse 3D Convolutional Neural Network - on a large-scale 3D Shape benchmark
ModelNet40, and measure how it is impacted by voxel resolution of input shape.
We demonstrate comparable classification and retrieval performance to
state-of-the-art models, but with much less computational costs in training and
inference phases. We also notice that benefits of higher input resolution can
be limited by an ability of a neural network to generalize high level features.
</summary>
    <author>
      <name>Alexandr Notchenko</name>
    </author>
    <author>
      <name>Ermek Kapushev</name>
    </author>
    <author>
      <name>Evgeny Burnaev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures, 2 tables, accepted to 3D Deep Learning Workshop
  at NIPS 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.09159v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09159v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.06551v1</id>
    <updated>2016-11-21T18:13:42Z</updated>
    <published>2016-11-21T18:13:42Z</published>
    <title>On the Parametric Study of Lubricating Oil Production using an
  Artificial Neural Network (ANN) Approach</title>
    <summary>  In this study, an Artificial Neural Network (ANN) approach is utilized to
perform a parametric study on the process of extraction of lubricants from
heavy petroleum cuts. To train the model, we used field data collected from an
industrial plant. Operational conditions of feed and solvent flow rate,
Temperature of streams and mixing rate were considered as the input to the
model, whereas the flow rate of the main product was considered as the output
of the ANN model. A feed-forward Multi-Layer Perceptron Neural Network was
successfully applied to capture the relationship between inputs and output
parameters.
</summary>
    <author>
      <name>Masood Tehrani</name>
    </author>
    <author>
      <name>Mary Ahmadi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.06551v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.06551v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08156v1</id>
    <updated>2017-01-27T12:38:47Z</updated>
    <published>2017-01-27T12:38:47Z</published>
    <title>A Comprehensive Survey on Bengali Phoneme Recognition</title>
    <summary>  Hidden Markov model based various phoneme recognition methods for Bengali
language is reviewed. Automatic phoneme recognition for Bengali language using
multilayer neural network is reviewed. Usefulness of multilayer neural network
over single layer neural network is discussed. Bangla phonetic feature table
construction and enhancement for Bengali speech recognition is also discussed.
Comparison among these methods is discussed.
</summary>
    <author>
      <name>Sadia Tasnim Swarna</name>
    </author>
    <author>
      <name>Shamim Ehsan</name>
    </author>
    <author>
      <name>Md. Saiful Islam</name>
    </author>
    <author>
      <name>Marium E Jannat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.08156v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08156v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.01923v1</id>
    <updated>2017-02-07T08:33:35Z</updated>
    <published>2017-02-07T08:33:35Z</published>
    <title>Comparative Study of CNN and RNN for Natural Language Processing</title>
    <summary>  Deep neural networks (DNN) have revolutionized the field of natural language
processing (NLP). Convolutional neural network (CNN) and recurrent neural
network (RNN), the two main types of DNN architectures, are widely explored to
handle various NLP tasks. CNN is supposed to be good at extracting
position-invariant features and RNN at modeling units in sequence. The state of
the art on many NLP tasks often switches due to the battle between CNNs and
RNNs. This work is the first systematic comparison of CNN and RNN on a wide
range of representative NLP tasks, aiming to give basic guidance for DNN
selection.
</summary>
    <author>
      <name>Wenpeng Yin</name>
    </author>
    <author>
      <name>Katharina Kann</name>
    </author>
    <author>
      <name>Mo Yu</name>
    </author>
    <author>
      <name>Hinrich Sch√ºtze</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.01923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.01923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.08389v2</id>
    <updated>2017-06-13T19:37:28Z</updated>
    <published>2017-02-27T17:22:29Z</published>
    <title>Equivariance Through Parameter-Sharing</title>
    <summary>  We propose to study equivariance in deep neural networks through parameter
symmetries. In particular, given a group $\mathcal{G}$ that acts discretely on
the input and output of a standard neural network layer $\phi_{W}: \Re^{M} \to
\Re^{N}$, we show that $\phi_{W}$ is equivariant with respect to
$\mathcal{G}$-action iff $\mathcal{G}$ explains the symmetries of the network
parameters $W$. Inspired by this observation, we then propose two
parameter-sharing schemes to induce the desirable symmetry on $W$. Our
procedures for tying the parameters achieve $\mathcal{G}$-equivariance and,
under some conditions on the action of $\mathcal{G}$, they guarantee
sensitivity to all other permutation groups outside $\mathcal{G}$.
</summary>
    <author>
      <name>Siamak Ravanbakhsh</name>
    </author>
    <author>
      <name>Jeff Schneider</name>
    </author>
    <author>
      <name>Barnabas Poczos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">icml'17</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.08389v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.08389v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00729v1</id>
    <updated>2017-03-02T11:34:38Z</updated>
    <published>2017-03-02T11:34:38Z</published>
    <title>Mixing Complexity and its Applications to Neural Networks</title>
    <summary>  We suggest analyzing neural networks through the prism of space constraints.
We observe that most training algorithms applied in practice use bounded
memory, which enables us to use a new notion introduced in the study of
space-time tradeoffs that we call mixing complexity. This notion was devised in
order to measure the (in)ability to learn using a bounded-memory algorithm. In
this paper we describe how we use mixing complexity to obtain new results on
what can and cannot be learned using neural networks.
</summary>
    <author>
      <name>Michal Moshkovitz</name>
    </author>
    <author>
      <name>Naftali Tishby</name>
    </author>
    <link href="http://arxiv.org/abs/1703.00729v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00729v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.04617v2</id>
    <updated>2017-03-25T16:17:03Z</updated>
    <published>2017-03-14T17:43:25Z</published>
    <title>Exploring Question Understanding and Adaptation in Neural-Network-Based
  Question Answering</title>
    <summary>  The last several years have seen intensive interest in exploring
neural-network-based models for machine comprehension (MC) and question
answering (QA). In this paper, we approach the problems by closely modelling
questions in a neural network framework. We first introduce syntactic
information to help encode questions. We then view and model different types of
questions and the information shared among them as an adaptation task and
proposed adaptation models for them. On the Stanford Question Answering Dataset
(SQuAD), we show that these approaches can help attain better results over a
competitive baseline.
</summary>
    <author>
      <name>Junbei Zhang</name>
    </author>
    <author>
      <name>Xiaodan Zhu</name>
    </author>
    <author>
      <name>Qian Chen</name>
    </author>
    <author>
      <name>Lirong Dai</name>
    </author>
    <author>
      <name>Si Wei</name>
    </author>
    <author>
      <name>Hui Jiang</name>
    </author>
    <link href="http://arxiv.org/abs/1703.04617v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.04617v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.00471v2</id>
    <updated>2017-05-18T11:45:21Z</updated>
    <published>2017-04-03T08:38:47Z</published>
    <title>Modeling NNLO jet corrections with neural networks</title>
    <summary>  We present a preliminary strategy for modeling multidimensional distributions
through neural networks. We study the efficiency of the proposed strategy by
considering as input data the two-dimensional next-to-next leading order (NNLO)
jet k-factors distribution for the ATLAS 7 TeV 2011 data. We then validate the
neural network model in terms of interpolation and prediction quality by
comparing its results to alternative models.
</summary>
    <author>
      <name>Stefano Carrazza</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5506/APhysPolB.48.947</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5506/APhysPolB.48.947" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings for the Cracow Epiphany Conference 2017, final version</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.00471v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.00471v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.04456v1</id>
    <updated>2017-04-14T15:28:37Z</updated>
    <published>2017-04-14T15:28:37Z</published>
    <title>Liquid Splash Modeling with Neural Networks</title>
    <summary>  This paper proposes a new data-driven approach for modeling detailed splashes
for liquid simulations with neural networks. Our model learns to generate
small-scale splash detail for fluid-implicit-particle methods using training
data acquired from physically accurate, high-resolution simulations. We use
neural networks to model the regression of splash formation using a classifier
together with a velocity modification term. More specifically, we employ a
heteroscedastic model for the velocity updates. Our simulation results
demonstrate that our model significantly improves visual fidelity with a large
amount of realistic droplet formation and yields splash detail much more
efficiently than finer discretizations. We show this for two different spatial
scales and simulation setups.
</summary>
    <author>
      <name>Kiwon Um</name>
    </author>
    <author>
      <name>Xiangyu Hu</name>
    </author>
    <author>
      <name>Nils Thuerey</name>
    </author>
    <link href="http://arxiv.org/abs/1704.04456v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.04456v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.05146v1</id>
    <updated>2017-04-17T23:18:17Z</updated>
    <published>2017-04-17T23:18:17Z</published>
    <title>On Predicting Geolocation of Tweets using Convolutional Neural Networks</title>
    <summary>  In many Twitter studies, it is important to know where a tweet came from in
order to use the tweet content to study regional user behavior. However,
researchers using Twitter to understand user behavior often lack sufficient
geo-tagged data. Given the huge volume of Twitter data there is a need for
accurate automated geolocating solutions. Herein, we present a new method to
predict a Twitter user's location based on the information in a single tweet.
We integrate text and user profile meta-data into a single model using a
convolutional neural network. Our experiments demonstrate that our neural model
substantially outperforms baseline methods, achieving 52.8% accuracy and 92.1%
accuracy on city-level and country-level prediction respectively.
</summary>
    <author>
      <name>Binxuan Huang</name>
    </author>
    <author>
      <name>Kathleen M. Carley</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-60240-0_34</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-60240-0_34" rel="related"/>
    <link href="http://arxiv.org/abs/1704.05146v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.05146v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.06279v1</id>
    <updated>2017-04-20T18:02:50Z</updated>
    <published>2017-04-20T18:02:50Z</published>
    <title>Mutual Information, Neural Networks and the Renormalization Group</title>
    <summary>  Physical systems differing in their microscopic details often display
strikingly similar behaviour when probed at low energies. Those universal
properties, largely determining their physical characteristics, are revealed by
the powerful renormalization group (RG) procedure, which systematically retains
"slow" degrees of freedom and integrates out the rest. However, the important
degrees of freedom may be difficult to identify. Here we demonstrate a machine
learning (ML) algorithm capable of identifying the relevant degrees of freedom
without any prior knowledge about the system. We introduce an artificial neural
network based on a model-independent, information-theoretic characterization of
a real-space RG procedure, performing this task. We apply the algorithm to
classical statistical physics problems in two dimensions.
</summary>
    <author>
      <name>Maciej Koch-Janusz</name>
    </author>
    <author>
      <name>Zohar Ringel</name>
    </author>
    <link href="http://arxiv.org/abs/1704.06279v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.06279v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.07147v1</id>
    <updated>2017-04-24T11:18:58Z</updated>
    <published>2017-04-24T11:18:58Z</published>
    <title>A Neural Network model with Bidirectional Whitening</title>
    <summary>  We present here a new model and algorithm which performs an efficient Natural
gradient descent for Multilayer Perceptrons. Natural gradient descent was
originally proposed from a point of view of information geometry, and it
performs the steepest descent updates on manifolds in a Riemannian space. In
particular, we extend an approach taken by the "Whitened neural networks"
model. We make the whitening process not only in feed-forward direction as in
the original model, but also in the back-propagation phase. Its efficacy is
shown by an application of this "Bidirectional whitened neural networks" model
to a handwritten character recognition data (MNIST data).
</summary>
    <author>
      <name>Yuki Fujimoto</name>
    </author>
    <author>
      <name>Toru Ohira</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.07147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.07147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.07616v1</id>
    <updated>2017-04-25T10:22:57Z</updated>
    <published>2017-04-25T10:22:57Z</published>
    <title>Joint POS Tagging and Dependency Parsing with Transition-based Neural
  Networks</title>
    <summary>  While part-of-speech (POS) tagging and dependency parsing are observed to be
closely related, existing work on joint modeling with manually crafted feature
templates suffers from the feature sparsity and incompleteness problems. In
this paper, we propose an approach to joint POS tagging and dependency parsing
using transition-based neural networks. Three neural network based classifiers
are designed to resolve shift/reduce, tagging, and labeling conflicts.
Experiments show that our approach significantly outperforms previous methods
for joint POS tagging and dependency parsing across a variety of natural
languages.
</summary>
    <author>
      <name>Liner Yang</name>
    </author>
    <author>
      <name>Meishan Zhang</name>
    </author>
    <author>
      <name>Yang Liu</name>
    </author>
    <author>
      <name>Nan Yu</name>
    </author>
    <author>
      <name>Maosong Sun</name>
    </author>
    <author>
      <name>Guohong Fu</name>
    </author>
    <link href="http://arxiv.org/abs/1704.07616v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.07616v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.08863v2</id>
    <updated>2017-05-02T22:43:10Z</updated>
    <published>2017-04-28T09:57:52Z</published>
    <title>On weight initialization in deep neural networks</title>
    <summary>  A proper initialization of the weights in a neural network is critical to its
convergence. Current insights into weight initialization come primarily from
linear activation functions. In this paper, I develop a theory for weight
initializations with non-linear activations. First, I derive a general weight
initialization strategy for any neural network using activation functions
differentiable at 0. Next, I derive the weight initialization strategy for the
Rectified Linear Unit (RELU), and provide theoretical insights into why the
Xavier initialization is a poor choice with RELU activations. My analysis
provides a clear demonstration of the role of non-linearities in determining
the proper weight initializations.
</summary>
    <author>
      <name>Siddharth Krishna Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.08863v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.08863v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.02210v1</id>
    <updated>2017-05-05T13:32:54Z</updated>
    <published>2017-05-05T13:32:54Z</published>
    <title>SLDR-DL: A Framework for SLD-Resolution with Deep Learning</title>
    <summary>  This paper introduces an SLD-resolution technique based on deep learning.
This technique enables neural networks to learn from old and successful
resolution processes and to use learnt experiences to guide new resolution
processes. An implementation of this technique is named SLDR-DL. It includes a
Prolog library of deep feedforward neural networks and some essential functions
of resolution. In the SLDR-DL framework, users can define logical rules in the
form of definite clauses and teach neural networks to use the rules in
reasoning processes.
</summary>
    <author>
      <name>Cheng-Hao Cai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.02210v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.02210v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.09319v1</id>
    <updated>2017-05-25T18:33:24Z</updated>
    <published>2017-05-25T18:33:24Z</published>
    <title>Diagonal Rescaling For Neural Networks</title>
    <summary>  We define a second-order neural network stochastic gradient training
algorithm whose block-diagonal structure effectively amounts to normalizing the
unit activations. Investigating why this algorithm lacks in robustness then
reveals two interesting insights. The first insight suggests a new way to scale
the stepsizes, clarifying popular algorithms such as RMSProp as well as old
neural network tricks such as fanin stepsize scaling. The second insight
stresses the practical importance of dealing with fast changes of the curvature
of the cost.
</summary>
    <author>
      <name>Jean Lafond</name>
    </author>
    <author>
      <name>Nicolas Vasilache</name>
    </author>
    <author>
      <name>L√©on Bottou</name>
    </author>
    <link href="http://arxiv.org/abs/1705.09319v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.09319v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.01283v2</id>
    <updated>2017-06-13T02:16:03Z</updated>
    <published>2017-06-05T11:44:04Z</published>
    <title>Performance evaluation of coherent Ising machines against classical
  neural networks</title>
    <summary>  The coherent Ising machine is expected to find a near-optimal solution in
various combinatorial optimization problems, which has been experimentally
confirmed with optical parametric oscillators (OPOs) and a field programmable
gate array (FPGA) circuit. The similar mathematical models were proposed three
decades ago by J. J. Hopfield, et al. in the context of classical neural
networks. In this article, we compare the computational performance of both
models.
</summary>
    <author>
      <name>Yoshitaka Haribara</name>
    </author>
    <author>
      <name>Hitoshi Ishikawa</name>
    </author>
    <author>
      <name>Shoko Utsunomiya</name>
    </author>
    <author>
      <name>Kazuyuki Aihara</name>
    </author>
    <author>
      <name>Yoshihisa Yamamoto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/2058-9565/aa8190</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/2058-9565/aa8190" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 figures, submitted to IOP-QST</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Quantum Sci. Technol. 2 044002 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.01283v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.01283v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07966v1</id>
    <updated>2017-06-24T14:19:41Z</updated>
    <published>2017-06-24T14:19:41Z</published>
    <title>Irregular Convolutional Neural Networks</title>
    <summary>  Convolutional kernels are basic and vital components of deep Convolutional
Neural Networks (CNN). In this paper, we equip convolutional kernels with shape
attributes to generate the deep Irregular Convolutional Neural Networks (ICNN).
Compared to traditional CNN applying regular convolutional kernels like
${3\times3}$, our approach trains irregular kernel shapes to better fit the
geometric variations of input features. In other words, shapes are learnable
parameters in addition to weights. The kernel shapes and weights are learned
simultaneously during end-to-end training with the standard back-propagation
algorithm. Experiments for semantic segmentation are implemented to validate
the effectiveness of our proposed ICNN.
</summary>
    <author>
      <name>Jiabin Ma</name>
    </author>
    <author>
      <name>Wei Wang</name>
    </author>
    <author>
      <name>Liang Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.07966v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07966v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.02800v2</id>
    <updated>2017-12-28T03:28:43Z</updated>
    <published>2017-07-10T11:28:36Z</published>
    <title>Artificial Neural Network in Cosmic Landscape</title>
    <summary>  In this paper we propose that artificial neural network, the basis of machine
learning, is useful to generate the inflationary landscape from a cosmological
point of view. Traditional numerical simulations of a global cosmic landscape
typically need an exponential complexity when the number of fields is large.
However, a basic application of artificial neural network could solve the
problem based on the universal approximation theorem of the multilayer
perceptron. A toy model in inflation with multiple light fields is investigated
numerically as an example of such an application.
</summary>
    <author>
      <name>Junyu Liu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/JHEP12(2017)149</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/JHEP12(2017)149" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">v2, add some new contents</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">JHEP 1712 (2017) 149</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1707.02800v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.02800v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.07565v1</id>
    <updated>2017-07-24T14:09:47Z</updated>
    <published>2017-07-24T14:09:47Z</published>
    <title>Automatic breast cancer grading in lymph nodes using a deep neural
  network</title>
    <summary>  The progression of breast cancer can be quantified in lymph node whole-slide
images (WSIs). We describe a novel method for effectively performing
classification of whole-slide images and patient level breast cancer grading.
Our method utilises a deep neural network. The method performs classification
on small patches and uses model averaging for boosting. In the first step,
region of interest patches are determined and cropped automatically by color
thresholding and then classified by the deep neural network. The classification
results are used to determine a slide level class and for further aggregation
to predict a patient level grade. Fast processing speed of our method enables
high throughput image analysis.
</summary>
    <author>
      <name>Thomas Wollmann</name>
    </author>
    <author>
      <name>Karl Rohr</name>
    </author>
    <link href="http://arxiv.org/abs/1707.07565v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.07565v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.00214v1</id>
    <updated>2017-08-01T09:13:44Z</updated>
    <published>2017-08-01T09:13:44Z</published>
    <title>Natural Language Processing with Small Feed-Forward Networks</title>
    <summary>  We show that small and shallow feed-forward neural networks can achieve near
state-of-the-art results on a range of unstructured and structured language
processing tasks while being considerably cheaper in memory and computational
requirements than deep recurrent models. Motivated by resource-constrained
environments like mobile phones, we showcase simple techniques for obtaining
such small neural network models, and investigate different tradeoffs when
deciding how to allocate a small memory budget.
</summary>
    <author>
      <name>Jan A. Botha</name>
    </author>
    <author>
      <name>Emily Pitler</name>
    </author>
    <author>
      <name>Ji Ma</name>
    </author>
    <author>
      <name>Anton Bakalov</name>
    </author>
    <author>
      <name>Alex Salcianu</name>
    </author>
    <author>
      <name>David Weiss</name>
    </author>
    <author>
      <name>Ryan McDonald</name>
    </author>
    <author>
      <name>Slav Petrov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2017 short paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.00214v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00214v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.02757v1</id>
    <updated>2017-08-09T08:49:12Z</updated>
    <published>2017-08-09T08:49:12Z</published>
    <title>Isointense infant brain MRI segmentation with a dilated convolutional
  neural network</title>
    <summary>  Quantitative analysis of brain MRI at the age of 6 months is difficult
because of the limited contrast between white matter and gray matter. In this
study, we use a dilated triplanar convolutional neural network in combination
with a non-dilated 3D convolutional neural network for the segmentation of
white matter, gray matter and cerebrospinal fluid in infant brain MR images, as
provided by the MICCAI grand challenge on 6-month infant brain MRI
segmentation.
</summary>
    <author>
      <name>Pim Moeskops</name>
    </author>
    <author>
      <name>Josien P. W. Pluim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MICCAI grand challenge on 6-month infant brain MRI segmentation</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.02757v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.02757v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.02975v2</id>
    <updated>2017-11-01T23:50:17Z</updated>
    <published>2017-08-09T19:15:56Z</published>
    <title>Anomaly Detection on Graph Time Series</title>
    <summary>  In this paper, we use variational recurrent neural network to investigate the
anomaly detection problem on graph time series. The temporal correlation is
modeled by the combination of recurrent neural network (RNN) and variational
inference (VI), while the spatial information is captured by the graph
convolutional network. In order to incorporate external factors, we use feature
extractor to augment the transition of latent variables, which can learn the
influence of external factors. With the target function as accumulative ELBO,
it is easy to extend this model to on-line method. The experimental study on
traffic flow data shows the detection capability of the proposed method.
</summary>
    <author>
      <name>Daniel Hsu</name>
    </author>
    <link href="http://arxiv.org/abs/1708.02975v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.02975v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.03278v1</id>
    <updated>2017-08-10T16:02:58Z</updated>
    <published>2017-08-10T16:02:58Z</published>
    <title>Motion Feature Augmented Recurrent Neural Network for Skeleton-based
  Dynamic Hand Gesture Recognition</title>
    <summary>  Dynamic hand gesture recognition has attracted increasing interests because
of its importance for human computer interaction. In this paper, we propose a
new motion feature augmented recurrent neural network for skeleton-based
dynamic hand gesture recognition. Finger motion features are extracted to
describe finger movements and global motion features are utilized to represent
the global movement of hand skeleton. These motion features are then fed into a
bidirectional recurrent neural network (RNN) along with the skeleton sequence,
which can augment the motion features for RNN and improve the classification
performance. Experiments demonstrate that our proposed method is effective and
outperforms start-of-the-art methods.
</summary>
    <author>
      <name>Xinghao Chen</name>
    </author>
    <author>
      <name>Hengkai Guo</name>
    </author>
    <author>
      <name>Guijin Wang</name>
    </author>
    <author>
      <name>Li Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ICIP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.03278v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.03278v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.08557v2</id>
    <updated>2017-09-11T19:04:57Z</updated>
    <published>2017-08-28T23:08:21Z</published>
    <title>A parameterized activation function for learning fuzzy logic operations
  in deep neural networks</title>
    <summary>  We present a deep learning architecture for learning fuzzy logic expressions.
Our model uses an innovative, parameterized, differentiable activation function
that can learn a number of logical operations by gradient descent. This
activation function allows a neural network to determine the relationships
between its input variables and provides insight into the logical significance
of learned network parameters. We provide a theoretical basis for this
parameterization and demonstrate its effectiveness and utility by successfully
applying our model to five classification problems from the UCI Machine
Learning Repository.
</summary>
    <author>
      <name>Luke B. Godfrey</name>
    </author>
    <author>
      <name>Michael S. Gashler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, IEEE SMC 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.08557v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.08557v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.01922v1</id>
    <updated>2017-09-06T12:44:01Z</updated>
    <published>2017-09-06T12:44:01Z</published>
    <title>A Comparison on Audio Signal Preprocessing Methods for Deep Neural
  Networks on Music Tagging</title>
    <summary>  Deep neural networks (DNN) have been successfully applied for music
classification tasks including music tagging. In this paper, we investigate the
effect of audio preprocessing on music tagging with neural networks. We perform
comprehensive experiments involving audio preprocessing using different
time-frequency representations, logarithmic magnitude compression, frequency
weighting and scaling. We show that many commonly used input preprocessing
techniques are redundant except magnitude compression.
</summary>
    <author>
      <name>Keunwoo Choi</name>
    </author>
    <author>
      <name>George Fazekas</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <author>
      <name>Mark Sandler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, will be submitted to ICASSP 2017. arXiv admin note:
  substantial text overlap with arXiv:1706.02361</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.01922v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.01922v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.01013v1</id>
    <updated>2017-10-03T07:21:03Z</updated>
    <published>2017-10-03T07:21:03Z</published>
    <title>Training Feedforward Neural Networks with Standard Logistic Activations
  is Feasible</title>
    <summary>  Training feedforward neural networks with standard logistic activations is
considered difficult because of the intrinsic properties of these sigmoidal
functions. This work aims at showing that these networks can be trained to
achieve generalization performance comparable to those based on hyperbolic
tangent activations. The solution consists on applying a set of conditions in
parameter initialization, which have been derived from the study of the
properties of a single neuron from an information-theoretic perspective. The
proposed initialization is validated through an extensive experimental
analysis.
</summary>
    <author>
      <name>Emanuele Sansone</name>
    </author>
    <author>
      <name>Francesco G. B. De Natale</name>
    </author>
    <link href="http://arxiv.org/abs/1710.01013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.01013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.00333v1</id>
    <updated>2017-10-30T18:24:35Z</updated>
    <published>2017-10-30T18:24:35Z</published>
    <title>An Experimental Analysis of the Power Consumption of Convolutional
  Neural Networks for Keyword Spotting</title>
    <summary>  Nearly all previous work on small-footprint keyword spotting with neural
networks quantify model footprint in terms of the number of parameters and
multiply operations for an inference pass. These values are, however, proxy
measures since empirical performance in actual deployments is determined by
many factors. In this paper, we study the power consumption of a family of
convolutional neural networks for keyword spotting on a Raspberry Pi. We find
that both proxies are good predictors of energy usage, although the number of
multiplies is more predictive than the number of parameters. We also confirm
that models with the highest accuracies are, unsurprisingly, the most power
hungry.
</summary>
    <author>
      <name>Raphael Tang</name>
    </author>
    <author>
      <name>Weijie Wang</name>
    </author>
    <author>
      <name>Zhucheng Tu</name>
    </author>
    <author>
      <name>Jimmy Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.00333v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.00333v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.03345v1</id>
    <updated>2017-11-09T12:15:33Z</updated>
    <published>2017-11-09T12:15:33Z</published>
    <title>Frangi-Net: A Neural Network Approach to Vessel Segmentation</title>
    <summary>  In this paper, we reformulate the conventional 2-D Frangi vesselness measure
into a pre-weighted neural network ("Frangi-Net"), and illustrate that the
Frangi-Net is equivalent to the original Frangi filter. Furthermore, we show
that, as a neural network, Frangi-Net is trainable. We evaluate the proposed
method on a set of 45 high resolution fundus images. After fine-tuning, we
observe both qualitative and quantitative improvements in the segmentation
quality compared to the original Frangi measure, with an increase up to $17\%$
in F1 score.
</summary>
    <author>
      <name>Weilin Fu</name>
    </author>
    <author>
      <name>Katharina Breininger</name>
    </author>
    <author>
      <name>Tobias W√ºrfl</name>
    </author>
    <author>
      <name>Nishant Ravikumar</name>
    </author>
    <author>
      <name>Roman Schaffert</name>
    </author>
    <author>
      <name>Andreas Maier</name>
    </author>
    <link href="http://arxiv.org/abs/1711.03345v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.03345v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.04043v3</id>
    <updated>2018-02-20T16:52:36Z</updated>
    <published>2017-11-10T23:32:47Z</published>
    <title>Few-Shot Learning with Graph Neural Networks</title>
    <summary>  We propose to study the problem of few-shot learning with the prism of
inference on a partially observed graphical model, constructed from a
collection of input images whose label can be either observed or not. By
assimilating generic message-passing inference algorithms with their
neural-network counterparts, we define a graph neural network architecture that
generalizes several of the recently proposed few-shot learning models. Besides
providing improved numerical performance, our framework is easily extended to
variants of few-shot learning, such as semi-supervised or active learning,
demonstrating the ability of graph-based models to operate well on 'relational'
tasks.
</summary>
    <author>
      <name>Victor Garcia</name>
    </author>
    <author>
      <name>Joan Bruna</name>
    </author>
    <link href="http://arxiv.org/abs/1711.04043v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.04043v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.05423v1</id>
    <updated>2017-11-15T06:16:30Z</updated>
    <published>2017-11-15T06:16:30Z</published>
    <title>Reconstruction of a scalar voltage-based neural field network from
  observed time series</title>
    <summary>  We present a general method for reconstruction of a network of nonlinearly
coupled neural fields from the observations. A prominent example of such a
system is a dynamical random neural network model studied by Sompolinsky et. al
[Phys. Rev. Lett., v. 61, 259 (1988)]. We develop a technique for inferring the
properties of the system from the observations of the chaotic voltages. Only
the structure of the model is assumed to be known, while the nonlinear gain
functions of the interactions, the matrix of the coupling constants, and the
time constants of the local dynamics are reconstructed from the time series.
</summary>
    <author>
      <name>A. Pikovsky</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1209/0295-5075/119/30004</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1209/0295-5075/119/30004" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPL, v. 119, 30004 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1711.05423v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.05423v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.10056v1</id>
    <updated>2017-11-28T00:08:10Z</updated>
    <published>2017-11-28T00:08:10Z</published>
    <title>Adversary Detection in Neural Networks via Persistent Homology</title>
    <summary>  We outline a detection method for adversarial inputs to deep neural networks.
By viewing neural network computations as graphs upon which information flows
from input space to out- put distribution, we compare the differences in graphs
induced by different inputs. Specifically, by applying persistent homology to
these induced graphs, we observe that the structure of the most persistent
subgraphs which generate the first homology group differ between adversarial
and unperturbed inputs. Based on this observation, we build a detection
algorithm that depends only on the topological information extracted during
training. We test our algorithm on MNIST and achieve 98% detection adversary
accuracy with F1-score 0.98.
</summary>
    <author>
      <name>Thomas Gebhart</name>
    </author>
    <author>
      <name>Paul Schrater</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.10056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.10056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.06100v1</id>
    <updated>2017-12-17T12:42:51Z</updated>
    <published>2017-12-17T12:42:51Z</published>
    <title>Query-Based Abstractive Summarization Using Neural Networks</title>
    <summary>  In this paper, we present a model for generating summaries of text documents
with respect to a query. This is known as query-based summarization. We adapt
an existing dataset of news article summaries for the task and train a
pointer-generator model using this dataset. The generated summaries are
evaluated by measuring similarity to reference summaries. Our results show that
a neural network summarization model, similar to existing neural network models
for abstractive summarization, can be constructed to make use of queries to
produce targeted summaries.
</summary>
    <author>
      <name>Johan Hasselqvist</name>
    </author>
    <author>
      <name>Niklas Helmertz</name>
    </author>
    <author>
      <name>Mikael K√•geb√§ck</name>
    </author>
    <link href="http://arxiv.org/abs/1712.06100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.06100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.09114v1</id>
    <updated>2017-12-25T18:57:00Z</updated>
    <published>2017-12-25T18:57:00Z</published>
    <title>HEPDrone: a toolkit for the mass application of machine learning in High
  Energy Physics</title>
    <summary>  Machine learning has proven to be an indispensable tool in the selection of
interesting events in high energy physics. Such technologies will become
increasingly important as detector upgrades are introduced and data rates
increase by orders of magnitude. We propose a toolkit to enable the creation of
a drone classifier from any machine learning classifier, such that different
classifiers may be standardised into a single form and executed in parallel. We
demonstrate the capability of the drone neural network to learn the required
properties of the input neural network without the use of any training data,
only using appropriate questioning of the input neural network.
</summary>
    <author>
      <name>Sean Benson</name>
    </author>
    <author>
      <name>Konstantin Gizdov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.09114v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.09114v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.01127v1</id>
    <updated>2018-01-03T19:00:03Z</updated>
    <published>2018-01-03T19:00:03Z</published>
    <title>Self-learning Monte Carlo with Deep Neural Networks</title>
    <summary>  Self-learning Monte Carlo (SLMC) method is a general algorithm to speedup MC
simulations. Its efficiency has been demonstrated in various systems by
introducing an effective model to propose global moves in the configuration
space. In this paper, we show that deep neural networks can be naturally
incorporated into SLMC, and without any prior knowledge, can accurately learn
the original model accurately and efficiently. Demonstrated in quantum impurity
models, we reduce the complexity for a local update from $ \mathcal{O}(\beta^2)
$ in Hirsch-Fye algorithm to $ \mathcal{O}(\beta \log \beta) $, which is a
significant speedup especially for systems at low temperatures.
</summary>
    <author>
      <name>Huitao Shen</name>
    </author>
    <author>
      <name>Junwei Liu</name>
    </author>
    <author>
      <name>Liang Fu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.01127v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.01127v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.str-el" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.str-el" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.06230v1</id>
    <updated>2018-01-18T20:33:26Z</updated>
    <published>2018-01-18T20:33:26Z</published>
    <title>Overpruning in Variational Bayesian Neural Networks</title>
    <summary>  The motivations for using variational inference (VI) in neural networks
differ significantly from those in latent variable models. This has a
counter-intuitive consequence; more expressive variational approximations can
provide significantly worse predictions as compared to those with less
expressive families. In this work we make two contributions. First, we identify
a cause of this performance gap, variational over-pruning. Second, we introduce
a theoretically grounded explanation for this phenomenon. Our perspective sheds
light on several related published results and provides intuition into the
design of effective variational approximations of neural networks.
</summary>
    <author>
      <name>Brian Trippe</name>
    </author>
    <author>
      <name>Richard Turner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented the Advances in Approximate Bayesian Inference workshop at
  NIPS 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.06230v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.06230v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.05803v1</id>
    <updated>2018-02-15T23:44:55Z</updated>
    <published>2018-02-15T23:44:55Z</published>
    <title>MPC-Inspired Neural Network Policies for Sequential Decision Making</title>
    <summary>  In this paper we investigate the use of MPC-inspired neural network policies
for sequential decision making. We introduce an extension to the DAgger
algorithm for training such policies and show how they have improved training
performance and generalization capabilities. We take advantage of this
extension to show scalable and efficient training of complex planning policy
architectures in continuous state and action spaces. We provide an extensive
comparison of neural network policies by considering feed forward policies,
recurrent policies, and recurrent policies with planning structure inspired by
the Path Integral control framework. Our results suggest that MPC-type
recurrent policies have better robustness to disturbances and modeling error.
</summary>
    <author>
      <name>Marcus Pereira</name>
    </author>
    <author>
      <name>David D. Fan</name>
    </author>
    <author>
      <name>Gabriel Nakajima An</name>
    </author>
    <author>
      <name>Evangelos Theodorou</name>
    </author>
    <link href="http://arxiv.org/abs/1802.05803v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.05803v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.08535v1</id>
    <updated>2018-02-23T14:04:30Z</updated>
    <published>2018-02-23T14:04:30Z</published>
    <title>Can Neural Networks Understand Logical Entailment?</title>
    <summary>  We introduce a new dataset of logical entailments for the purpose of
measuring models' ability to capture and exploit the structure of logical
expressions against an entailment prediction task. We use this task to compare
a series of architectures which are ubiquitous in the sequence-processing
literature, in addition to a new model class---PossibleWorldNets---which
computes entailment as a "convolution over possible worlds". Results show that
convolutional networks present the wrong inductive bias for this class of
problems relative to LSTM RNNs, tree-structured neural networks outperform LSTM
RNNs due to their enhanced ability to exploit the syntax of logic, and
PossibleWorldNets outperform all benchmarks.
</summary>
    <author>
      <name>Richard Evans</name>
    </author>
    <author>
      <name>David Saxton</name>
    </author>
    <author>
      <name>David Amos</name>
    </author>
    <author>
      <name>Pushmeet Kohli</name>
    </author>
    <author>
      <name>Edward Grefenstette</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at ICLR 2018 (main conference)</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.08535v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.08535v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.00133v1</id>
    <updated>2018-02-28T23:34:20Z</updated>
    <published>2018-02-28T23:34:20Z</published>
    <title>Materials data validation and imputation with an artificial neural
  network</title>
    <summary>  We apply an artificial neural network to model and verify material
properties. The neural network algorithm has a unique capability to handle
incomplete data sets in both training and predicting, so it can regard
properties as inputs allowing it to exploit both composition-property and
property-property correlations to enhance the quality of predictions, and can
also handle a graphical data as a single entity. The framework is tested with
different validation schemes, and then applied to materials case studies of
alloys and polymers. The algorithm found twenty errors in a commercial
materials database that were confirmed against primary data sources.
</summary>
    <author>
      <name>P. C. Verpoort</name>
    </author>
    <author>
      <name>P. MacDonald</name>
    </author>
    <author>
      <name>G. J. Conduit</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.commatsci.2018.02.002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.commatsci.2018.02.002" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 11 figures, 9 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computational Materials Science 147, 176-185 (2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1803.00133v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.00133v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.06030v2</id>
    <updated>2017-12-17T11:46:06Z</updated>
    <published>2017-09-18T16:26:53Z</published>
    <title>N2N Learning: Network to Network Compression via Policy Gradient
  Reinforcement Learning</title>
    <summary>  While bigger and deeper neural network architectures continue to advance the
state-of-the-art for many computer vision tasks, real-world adoption of these
networks is impeded by hardware and speed constraints. Conventional model
compression methods attempt to address this problem by modifying the
architecture manually or using pre-defined heuristics. Since the space of all
reduced architectures is very large, modifying the architecture of a deep
neural network in this way is a difficult task. In this paper, we tackle this
issue by introducing a principled method for learning reduced network
architectures in a data-driven way using reinforcement learning. Our approach
takes a larger `teacher' network as input and outputs a compressed `student'
network derived from the `teacher' network. In the first stage of our method, a
recurrent policy network aggressively removes layers from the large `teacher'
model. In the second stage, another recurrent policy network carefully reduces
the size of each remaining layer. The resulting network is then evaluated to
obtain a reward -- a score based on the accuracy and compression of the
network. Our approach uses this reward signal with policy gradients to train
the policies to find a locally optimal student network. Our experiments show
that we can achieve compression rates of more than 10x for models such as
ResNet-34 while maintaining similar performance to the input `teacher' network.
We also present a valuable transfer learning result which shows that policies
which are pre-trained on smaller `teacher' networks can be used to rapidly
speed up training on larger `teacher' networks.
</summary>
    <author>
      <name>Anubhav Ashok</name>
    </author>
    <author>
      <name>Nicholas Rhinehart</name>
    </author>
    <author>
      <name>Fares Beainy</name>
    </author>
    <author>
      <name>Kris M. Kitani</name>
    </author>
    <link href="http://arxiv.org/abs/1709.06030v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.06030v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/q-bio/0507037v3</id>
    <updated>2018-01-23T20:29:57Z</updated>
    <published>2005-07-25T21:01:32Z</published>
    <title>Neuromodulation Influences Synchronization and Intrinsic Read-out</title>
    <summary>  The roles of neuromodulation in a neural network, such as in a cortical
microcolumn, are still incompletely understood. Neuromodulation influences
neural processing by presynaptic and postsynaptic regulation of synaptic
efficacy. Synaptic efficacy modulation can be an effective way to rapidly alter
network density and topology. We show that altering network topology, together
with density, will affect its synchronization. Fast synaptic efficacy
modulation may therefore influence the amount of correlated spiking in a
network. Neuromodulation also affects ion channel regulation for intrinsic
excitability, which alters the neuron's activation function. We show that
synchronization in a network influences the read-out of these intrinsic
properties. Highly synchronous input drives neurons, such that differences in
intrinsic properties disappear, while asynchronous input lets intrinsic
properties determine output behavior. Thus, altering network topology can alter
the balance between intrinsically vs. synaptically driven network activity. We
conclude that neuromodulation may allow a network to shift between a more
synchronized transmission mode and a more asynchronous intrinsic read-out mode.
</summary>
    <author>
      <name>Gabriele Scheler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/q-bio/0507037v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/q-bio/0507037v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.2577v1</id>
    <updated>2009-02-15T22:15:53Z</updated>
    <published>2009-02-15T22:15:53Z</published>
    <title>From Sigmoid Power Control Algorithm to Hopfield-like Neural Networks:
  "SIR" ("Signal"-to-"Interference"-Ratio)-Balancing Sigmoid-Based Networks-
  Part I: Continuous Time</title>
    <summary>  Continuous-time Hopfield network has been an important focus of research area
since 1980s whose applications vary from image restoration to combinatorial
optimization from control engineering to associative memory systems. On the
other hand, in wireless communications systems literature, power control has
been intensively studied as an essential mechanism for increasing the system
performance. A fully distributed power control algorithm (DPCA), called Sigmoid
DPCA, is presented by Uykan in [10] and [11], which is obtained by discretizing
the continuous-time system. In this paper, we present a Sigmoid-based
"Signal-to-Interference Ratio, (SIR)" balancing dynamic networks, called
Sgm"SIR"NN, which includes both the Sigmoid power control algorithm (SgmDPCA)
and the Hopfield neural networks, two different areas whose scope of interest,
motivations and settings are completely different. It's shown that the
Sgm"SIR"N5C5C5C5C5CN exhibits features which are generally attributed to
Hopfield Networks. Computer simulations show the effectiveness of the proposed
network as compared to traditional Hopfield Network.
</summary>
    <author>
      <name>Zekeriya Uykan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 5 figures, submitted to IEEE Transactions on Neural
  Networks in December 2008</arxiv:comment>
    <link href="http://arxiv.org/abs/0902.2577v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.2577v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="93C05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.6539v1</id>
    <updated>2012-04-30T03:01:25Z</updated>
    <published>2012-04-30T03:01:25Z</published>
    <title>Neuronal avalanches of a self-organized neural network with
  active-neuron-dominant structure</title>
    <summary>  Neuronal avalanche is a spontaneous neuronal activity which obeys a power-law
distribution of population event sizes with an exponent of -3/2. It has been
observed in the superficial layers of cortex both \emph{in vivo} and \emph{in
vitro}. In this paper we analyze the information transmission of a novel
self-organized neural network with active-neuron-dominant structure. Neuronal
avalanches can be observed in this network with appropriate input intensity. We
find that the process of network learning via spike-timing dependent plasticity
dramatically increases the complexity of network structure, which is finally
self-organized to be active-neuron-dominant connectivity. Both the entropy of
activity patterns and the complexity of their resulting post-synaptic inputs
are maximized when the network dynamics are propagated as neuronal avalanches.
This emergent topology is beneficial for information transmission with high
efficiency and also could be responsible for the large information capacity of
this network compared with alternative archetypal networks with different
neural connectivity.
</summary>
    <author>
      <name>Xiumin Li</name>
    </author>
    <author>
      <name>Michael Small</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/1.3701946</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/1.3701946" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Non-final version submitted to Chaos</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Chaos 22, 023104 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1204.6539v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.6539v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.3754v1</id>
    <updated>2013-10-14T17:30:00Z</updated>
    <published>2013-10-14T17:30:00Z</published>
    <title>Optimizing working memory with heterogeneity of recurrent cortical
  excitation</title>
    <summary>  A neural correlate of parametric working memory is a stimulus specific rise
in neuron firing rate that persists long after the stimulus is removed. Network
models with local excitation and broad inhibition support persistent neural
activity, linking network architecture and parametric working memory. Cortical
neurons receive noisy input fluctuations which causes persistent activity to
diffusively wander about the network, degrading memory over time. We explore
how cortical architecture that supports parametric working memory affects the
diffusion of persistent neural activity. Studying both a spiking network and a
simplified potential well model, we show that spatially heterogeneous
excitatory coupling stabilizes a discrete number of persistent states, reducing
the diffusion of persistent activity over the network. However, heterogeneous
coupling also coarse-grains the stimulus representation space, limiting the
capacity of parametric working memory. The storage errors due to
coarse-graining and diffusion tradeoff so that information transfer between the
initial and recalled stimulus is optimized at a fixed network heterogeneity.
For sufficiently long delay times, the optimal number of attractors is less
than the number of possible stimuli, suggesting that memory networks can
under-represent stimulus space to optimize performance. Our results clearly
demonstrate the effects of network architecture and stochastic fluctuations on
parametric memory storage.
</summary>
    <author>
      <name>Zachary P. Kilpatrick</name>
    </author>
    <author>
      <name>Bard Ermentrout</name>
    </author>
    <author>
      <name>Brent Doiron</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.3754v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.3754v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.03964v1</id>
    <updated>2015-05-15T05:53:45Z</updated>
    <published>2015-05-15T05:53:45Z</published>
    <title>Algebraic identification of the effective connectivity of constrained
  geometric network models of neural signaling</title>
    <summary>  Cellular neural circuit and networks consisting of interconnected neurons and
glia are ulti- mately responsible for the information processing associated
with information processing in the brain. While there are major efforts aimed
at mapping the structural and (electro)physiological connectivity of brain
networks, such as the White House BRAIN Initiative aimed at the devel- opment
of neurotechnologies capable of high density neural recordings, theoretical and
compu- tational methods for analyzing and making sense of all this data seem to
be further behind. Here, we propose and provide a summary of an approach for
calculating effective connectivity from experimental observations of neuronal
network activity. The proposed method operates on network-level data, makes use
of all relevant prior knowledge, such as dynamical models of individual cells
in the network and the physical structural connectivity of the network, and is
broadly applicable to large classes of biological and non-biological networks.
</summary>
    <author>
      <name>Marius Buibas</name>
    </author>
    <author>
      <name>Gabriel A. Silva</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.03964v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.03964v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.06708v1</id>
    <updated>2015-08-27T03:21:15Z</updated>
    <published>2015-08-27T03:21:15Z</published>
    <title>Maximum-Margin Structured Learning with Deep Networks for 3D Human Pose
  Estimation</title>
    <summary>  This paper focuses on structured-output learning using deep neural networks
for 3D human pose estimation from monocular images. Our network takes an image
and 3D pose as inputs and outputs a score value, which is high when the
image-pose pair matches and low otherwise. The network structure consists of a
convolutional neural network for image feature extraction, followed by two
sub-networks for transforming the image features and pose into a joint
embedding. The score function is then the dot-product between the image and
pose embeddings. The image-pose embedding and score function are jointly
trained using a maximum-margin cost function. Our proposed framework can be
interpreted as a special form of structured support vector machines where the
joint feature space is discriminatively learned using deep neural networks. We
test our framework on the Human3.6m dataset and obtain state-of-the-art results
compared to other recent methods. Finally, we present visualizations of the
image-pose embedding space, demonstrating the network has learned a high-level
embedding of body-orientation and pose-configuration.
</summary>
    <author>
      <name>Sijin Li</name>
    </author>
    <author>
      <name>Weichen Zhang</name>
    </author>
    <author>
      <name>Antoni B. Chan</name>
    </author>
    <link href="http://arxiv.org/abs/1508.06708v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.06708v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.08565v3</id>
    <updated>2015-11-05T07:26:01Z</updated>
    <published>2015-10-29T05:31:28Z</published>
    <title>Attention with Intention for a Neural Network Conversation Model</title>
    <summary>  In a conversation or a dialogue process, attention and intention play
intrinsic roles. This paper proposes a neural network based approach that
models the attention and intention processes. It essentially consists of three
recurrent networks. The encoder network is a word-level model representing
source side sentences. The intention network is a recurrent network that models
the dynamics of the intention process. The decoder network is a recurrent
network produces responses to the input from the source side. It is a language
model that is dependent on the intention and has an attention mechanism to
attend to particular source side words, when predicting a symbol in the
response. The model is trained end-to-end without labeling data. Experiments
show that this model generates natural responses to user inputs.
</summary>
    <author>
      <name>Kaisheng Yao</name>
    </author>
    <author>
      <name>Geoffrey Zweig</name>
    </author>
    <author>
      <name>Baolin Peng</name>
    </author>
    <link href="http://arxiv.org/abs/1510.08565v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.08565v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.05077v6</id>
    <updated>2017-04-18T20:33:53Z</updated>
    <published>2015-11-16T18:28:10Z</published>
    <title>Diversity Networks: Neural Network Compression Using Determinantal Point
  Processes</title>
    <summary>  We introduce Divnet, a flexible technique for learning networks with diverse
neurons. Divnet models neuronal diversity by placing a Determinantal Point
Process (DPP) over neurons in a given layer. It uses this DPP to select a
subset of diverse neurons and subsequently fuses the redundant neurons into the
selected ones. Compared with previous approaches, Divnet offers a more
principled, flexible technique for capturing neuronal diversity and thus
implicitly enforcing regularization. This enables effective auto-tuning of
network architecture and leads to smaller network sizes without hurting
performance. Moreover, through its focus on diversity and neuron fusing, Divnet
remains compatible with other procedures that seek to reduce memory footprints
of networks. We present experimental results to corroborate our claims: for
pruning neural networks, Divnet is seen to be notably superior to competing
approaches.
</summary>
    <author>
      <name>Zelda Mariet</name>
    </author>
    <author>
      <name>Suvrit Sra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper appeared under the shorter title Diversity Networks at
  ICLR 2016
  (http://www.iclr.cc/doku.php?id=iclr2016:main#accepted_papers_conference_track)</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.05077v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.05077v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.05176v3</id>
    <updated>2016-02-25T20:36:21Z</updated>
    <published>2015-11-16T21:08:25Z</published>
    <title>MuProp: Unbiased Backpropagation for Stochastic Neural Networks</title>
    <summary>  Deep neural networks are powerful parametric models that can be trained
efficiently using the backpropagation algorithm. Stochastic neural networks
combine the power of large parametric functions with that of graphical models,
which makes it possible to learn very complex distributions. However, as
backpropagation is not directly applicable to stochastic networks that include
discrete sampling operations within their computational graph, training such
networks remains difficult. We present MuProp, an unbiased gradient estimator
for stochastic networks, designed to make this task easier. MuProp improves on
the likelihood-ratio estimator by reducing its variance using a control variate
based on the first-order Taylor expansion of a mean-field network. Crucially,
unlike prior attempts at using backpropagation for training stochastic
networks, the resulting estimator is unbiased and well behaved. Our experiments
on structured output prediction and discrete latent variable modeling
demonstrate that MuProp yields consistently good performance across a range of
difficult tasks.
</summary>
    <author>
      <name>Shixiang Gu</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Andriy Mnih</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at ICLR 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.05176v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.05176v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.07838v7</id>
    <updated>2016-05-22T20:58:11Z</updated>
    <published>2015-11-24T19:30:19Z</published>
    <title>Dynamic Capacity Networks</title>
    <summary>  We introduce the Dynamic Capacity Network (DCN), a neural network that can
adaptively assign its capacity across different portions of the input data.
This is achieved by combining modules of two types: low-capacity sub-networks
and high-capacity sub-networks. The low-capacity sub-networks are applied
across most of the input, but also provide a guide to select a few portions of
the input on which to apply the high-capacity sub-networks. The selection is
made using a novel gradient-based attention mechanism, that efficiently
identifies input regions for which the DCN's output is most sensitive and to
which we should devote more capacity. We focus our empirical evaluation on the
Cluttered MNIST and SVHN image datasets. Our findings indicate that DCNs are
able to drastically reduce the number of computations, compared to traditional
convolutional neural networks, while maintaining similar or even better
performance.
</summary>
    <author>
      <name>Amjad Almahairi</name>
    </author>
    <author>
      <name>Nicolas Ballas</name>
    </author>
    <author>
      <name>Tim Cooijmans</name>
    </author>
    <author>
      <name>Yin Zheng</name>
    </author>
    <author>
      <name>Hugo Larochelle</name>
    </author>
    <author>
      <name>Aaron Courville</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.07838v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.07838v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07365v1</id>
    <updated>2016-08-26T05:08:24Z</updated>
    <published>2016-08-26T05:08:24Z</published>
    <title>Scalable Compression of Deep Neural Networks</title>
    <summary>  Deep neural networks generally involve some layers with mil- lions of
parameters, making them difficult to be deployed and updated on devices with
limited resources such as mobile phones and other smart embedded systems. In
this paper, we propose a scalable representation of the network parameters, so
that different applications can select the most suitable bit rate of the
network based on their own storage constraints. Moreover, when a device needs
to upgrade to a high-rate network, the existing low-rate network can be reused,
and only some incremental data are needed to be downloaded. We first
hierarchically quantize the weights of a pre-trained deep neural network to
enforce weight sharing. Next, we adaptively select the bits assigned to each
layer given the total bit budget. After that, we retrain the network to
fine-tune the quantized centroids. Experimental results show that our method
can achieve scalable compression with graceful degradation in the performance.
</summary>
    <author>
      <name>Xing Wang</name>
    </author>
    <author>
      <name>Jie Liang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures, ACM Multimedia 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.07365v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07365v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.07709v1</id>
    <updated>2017-04-25T14:19:26Z</updated>
    <published>2017-04-25T14:19:26Z</published>
    <title>Inception Recurrent Convolutional Neural Network for Object Recognition</title>
    <summary>  Deep convolutional neural networks (DCNNs) are an influential tool for
solving various problems in the machine learning and computer vision fields. In
this paper, we introduce a new deep learning model called an Inception-
Recurrent Convolutional Neural Network (IRCNN), which utilizes the power of an
inception network combined with recurrent layers in DCNN architecture. We have
empirically evaluated the recognition performance of the proposed IRCNN model
using different benchmark datasets such as MNIST, CIFAR-10, CIFAR- 100, and
SVHN. Experimental results show similar or higher recognition accuracy when
compared to most of the popular DCNNs including the RCNN. Furthermore, we have
investigated IRCNN performance against equivalent Inception Networks and
Inception-Residual Networks using the CIFAR-100 dataset. We report about 3.5%,
3.47% and 2.54% improvement in classification accuracy when compared to the
RCNN, equivalent Inception Networks, and Inception- Residual Networks on the
augmented CIFAR- 100 dataset respectively.
</summary>
    <author>
      <name>Md Zahangir Alom</name>
    </author>
    <author>
      <name>Mahmudul Hasan</name>
    </author>
    <author>
      <name>Chris Yakopcic</name>
    </author>
    <author>
      <name>Tarek M. Taha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 10 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.07709v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.07709v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09553v1</id>
    <updated>2017-06-29T02:39:00Z</updated>
    <published>2017-06-29T02:39:00Z</published>
    <title>Transforming Musical Signals through a Genre Classifying Convolutional
  Neural Network</title>
    <summary>  Convolutional neural networks (CNNs) have been successfully applied on both
discriminative and generative modeling for music-related tasks. For a
particular task, the trained CNN contains information representing the decision
making or the abstracting process. One can hope to manipulate existing music
based on this 'informed' network and create music with new features
corresponding to the knowledge obtained by the network. In this paper, we
propose a method to utilize the stored information from a CNN trained on
musical genre classification task. The network was composed of three
convolutional layers, and was trained to classify five-second song clips into
five different genres. After training, randomly selected clips were modified by
maximizing the sum of outputs from the network layers. In addition to the
potential of such CNNs to produce interesting audio transformation, more
information about the network and the original music could be obtained from the
analysis of the generated features since these features indicate how the
network 'understands' the music.
</summary>
    <author>
      <name>S. Geng</name>
    </author>
    <author>
      <name>G. Ren</name>
    </author>
    <author>
      <name>M. Ogihara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Conference on Deep Learning
  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. of the First Int. Workshop on Deep Learning and Music joint
  with IJCNN. Anchorage, US. 1(1). pp 48-49 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.09553v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09553v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.06549v1</id>
    <updated>2017-07-20T14:37:23Z</updated>
    <published>2017-07-20T14:37:23Z</published>
    <title>Simultaneous multi-patch-clamp and extracellular-array recordings:
  Single neuron reflects network activity</title>
    <summary>  The increasing number of recording electrodes enhances the capability of
capturing the network's cooperative activity, however, using too many monitors
might alter the properties of the measured neural network and induce noise.
Using a technique that merges simultaneous multi-patch-clamp and
multi-electrode array recordings of neural networks in-vitro, we show that the
membrane potential of a single neuron is a reliable and super-sensitive probe
for monitoring such cooperative activities and their detailed rhythms.
Specifically, the membrane potential and the spiking activity of a single
neuron are either highly correlated or highly anti-correlated with the
time-dependent macroscopic activity of the entire network. This surprising
observation also sheds light on the cooperative origin of neuronal burst in
cultured networks. Our findings present an alternative flexible approach to the
technique based on a massive tiling of networks by large-scale arrays of
electrodes to monitor their activity.
</summary>
    <author>
      <name>Roni Vardi</name>
    </author>
    <author>
      <name>Amir Goldental</name>
    </author>
    <author>
      <name>Shira Sardi</name>
    </author>
    <author>
      <name>Anton Sheinin</name>
    </author>
    <author>
      <name>Ido Kanter</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/srep36228</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/srep36228" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages, 9 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Scientific Reports 6, Article number: 36228 (2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1707.06549v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.06549v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.07560v1</id>
    <updated>2017-09-22T01:20:59Z</updated>
    <published>2017-09-22T01:20:59Z</published>
    <title>Neural network image reconstruction for magnetic particle imaging</title>
    <summary>  We investigate neural network image reconstruction for magnetic particle
imaging. The network performance depends strongly on the convolution effects of
the spectrum input data. The larger convolution effect appearing at a
relatively smaller nanoparticle size obstructs the network training. The
trained single-layer network reveals the weighting matrix consisted of a basis
vector in the form of Chebyshev polynomials of the second kind. The weighting
matrix corresponds to an inverse system matrix, where an incoherency of basis
vectors due to a low convolution effects as well as a nonlinear activation
function plays a crucial role in retrieving the matrix elements. Test images
are well reconstructed through trained networks having an inverse kernel
matrix. We also confirm that a multi-layer network with one hidden layer
improves the performance. The architecture of a neural network overcoming the
low incoherence of the inverse kernel through the classification property will
become a better tool for image reconstruction.
</summary>
    <author>
      <name>Byung Gyu Chae</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.07560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.07560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.01217v1</id>
    <updated>2017-09-18T04:30:13Z</updated>
    <published>2017-09-18T04:30:13Z</published>
    <title>Wide and deep volumetric residual networks for volumetric image
  classification</title>
    <summary>  3D shape models that directly classify objects from 3D information have
become more widely implementable. Current state of the art models rely on deep
convolutional and inception models that are resource intensive. Residual neural
networks have been demonstrated to be easier to optimize and do not suffer from
vanishing/exploding gradients observed in deep networks. Here we implement a
residual neural network for 3D object classification of the 3D Princeton
ModelNet dataset. Further, we show that widening network layers dramatically
improves accuracy in shallow residual nets, and residual neural networks
perform comparable to state-of-the-art 3D shape net models, and we show that
widening network layers improves classification accuracy. We provide extensive
training and architecture parameters providing a better understanding of
available network architectures for use in 3D object classification.
</summary>
    <author>
      <name>Varun Arvind</name>
    </author>
    <author>
      <name>Anthony Costa</name>
    </author>
    <author>
      <name>Marcus Badgeley</name>
    </author>
    <author>
      <name>Samuel Cho</name>
    </author>
    <author>
      <name>Eric Oermann</name>
    </author>
    <link href="http://arxiv.org/abs/1710.01217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.01217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.04048v1</id>
    <updated>2017-11-11T00:55:57Z</updated>
    <published>2017-11-11T00:55:57Z</published>
    <title>CT-SRCNN: Cascade Trained and Trimmed Deep Convolutional Neural Networks
  for Image Super Resolution</title>
    <summary>  We propose methodologies to train highly accurate and efficient deep
convolutional neural networks (CNNs) for image super resolution (SR). A cascade
training approach to deep learning is proposed to improve the accuracy of the
neural networks while gradually increasing the number of network layers. Next,
we explore how to improve the SR efficiency by making the network slimmer. Two
methodologies, the one-shot trimming and the cascade trimming, are proposed.
With the cascade trimming, the network's size is gradually reduced layer by
layer, without significant loss on its discriminative ability. Experiments on
benchmark image datasets show that our proposed SR network achieves the
state-of-the-art super resolution accuracy, while being more than 4 times
faster compared to existing deep super resolution networks.
</summary>
    <author>
      <name>Haoyu Ren</name>
    </author>
    <author>
      <name>Mostafa El-Khamy</name>
    </author>
    <author>
      <name>Jungwon Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to IEEE Winter Conf. on Applications of Computer Vision
  (WACV) 2018, Lake Tahoe, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.04048v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.04048v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.06375v1</id>
    <updated>2017-11-17T02:01:11Z</updated>
    <published>2017-11-17T02:01:11Z</published>
    <title>Shape Inpainting using 3D Generative Adversarial Network and Recurrent
  Convolutional Networks</title>
    <summary>  Recent advances in convolutional neural networks have shown promising results
in 3D shape completion. But due to GPU memory limitations, these methods can
only produce low-resolution outputs. To inpaint 3D models with semantic
plausibility and contextual details, we introduce a hybrid framework that
combines a 3D Encoder-Decoder Generative Adversarial Network (3D-ED-GAN) and a
Long-term Recurrent Convolutional Network (LRCN). The 3D-ED-GAN is a 3D
convolutional neural network trained with a generative adversarial paradigm to
fill missing 3D data in low-resolution. LRCN adopts a recurrent neural network
architecture to minimize GPU memory usage and incorporates an Encoder-Decoder
pair into a Long Short-term Memory Network. By handling the 3D model as a
sequence of 2D slices, LRCN transforms a coarse 3D shape into a more complete
and higher resolution volume. While 3D-ED-GAN captures global contextual
structure of the 3D shape, LRCN localizes the fine-grained details.
Experimental results on both real-world and synthetic data show reconstructions
from corrupted models result in complete and high-resolution 3D objects.
</summary>
    <author>
      <name>Weiyue Wang</name>
    </author>
    <author>
      <name>Qiangui Huang</name>
    </author>
    <author>
      <name>Suya You</name>
    </author>
    <author>
      <name>Chao Yang</name>
    </author>
    <author>
      <name>Ulrich Neumann</name>
    </author>
    <link href="http://arxiv.org/abs/1711.06375v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.06375v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.08028v1</id>
    <updated>2017-11-21T20:34:48Z</updated>
    <published>2017-11-21T20:34:48Z</published>
    <title>Recurrent Relational Networks for Complex Relational Reasoning</title>
    <summary>  Humans possess an ability to abstractly reason about objects and their
interactions, an ability not shared with state-of-the-art deep learning models.
Relational networks, introduced by Santoro et al. (2017), add the capacity for
relational reasoning to deep neural networks, but are limited in the complexity
of the reasoning tasks they can address. We introduce recurrent relational
networks which increase the suite of solvable tasks to those that require an
order of magnitude more steps of relational reasoning. We use recurrent
relational networks to solve Sudoku puzzles and achieve state-of-the-art
results by solving 96.6% of the hardest Sudoku puzzles, where relational
networks fail to solve any. We also apply our model to the BaBi textual QA
dataset solving 19/20 tasks which is competitive with state-of-the-art sparse
differentiable neural computers. The recurrent relational network is a general
purpose module that can augment any neural network model with the capacity to
do many-step relational reasoning.
</summary>
    <author>
      <name>Rasmus Berg Palm</name>
    </author>
    <author>
      <name>Ulrich Paquet</name>
    </author>
    <author>
      <name>Ole Winther</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ICLR 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.08028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.08028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.05204v1</id>
    <updated>2018-01-16T10:59:41Z</updated>
    <published>2018-01-16T10:59:41Z</published>
    <title>A Multi-Agent Neural Network for Dynamic Frequency Reuse in LTE Networks</title>
    <summary>  Fractional Frequency Reuse techniques can be employed to address interference
in mobile networks, improving throughput for edge users. There is a tradeoff
between the coverage and overall throughput achievable, as interference
avoidance techniques lead to a loss in a cell's overall throughput, with
spectrum efficiency decreasing with the fencing off of orthogonal resources. In
this paper we propose MANN, a dynamic multiagent frequency reuse scheme, where
individual agents in charge of cells control their configurations based on
input from neural networks. The agents' decisions are partially influenced by a
coordinator agent, which attempts to maximise a global metric of the network
(e.g., cell-edge performance). Each agent uses a neural network to estimate the
best action (i.e., cell configuration) for its current environment setup, and
attempts to maximise in turn a local metric, subject to the constraint imposed
by the coordinator agent. Results show that our solution provides improved
performance for edge users, increasing the throughput of the bottom 5% of users
by 22%, while retaining 95% of a network's overall throughput from the full
frequency reuse case. Furthermore, we show how our method improves on static
fractional frequency reuse schemes.
</summary>
    <author>
      <name>Andrei Marinescu</name>
    </author>
    <author>
      <name>Irene Macaluso</name>
    </author>
    <author>
      <name>Luiz A. DaSilva</name>
    </author>
    <link href="http://arxiv.org/abs/1801.05204v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.05204v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0712.0932v1</id>
    <updated>2007-12-06T14:11:07Z</updated>
    <published>2007-12-06T14:11:07Z</published>
    <title>Dimensionality Reduction and Reconstruction using Mirroring Neural
  Networks and Object Recognition based on Reduced Dimension Characteristic
  Vector</title>
    <summary>  In this paper, we present a Mirroring Neural Network architecture to perform
non-linear dimensionality reduction and Object Recognition using a reduced
lowdimensional characteristic vector. In addition to dimensionality reduction,
the network also reconstructs (mirrors) the original high-dimensional input
vector from the reduced low-dimensional data. The Mirroring Neural Network
architecture has more number of processing elements (adalines) in the outer
layers and the least number of elements in the central layer to form a
converging-diverging shape in its configuration. Since this network is able to
reconstruct the original image from the output of the innermost layer (which
contains all the information about the input pattern), these outputs can be
used as object signature to classify patterns. The network is trained to
minimize the discrepancy between actual output and the input by back
propagating the mean squared error from the output layer to the input layer.
After successfully training the network, it can reduce the dimension of input
vectors and mirror the patterns fed to it. The Mirroring Neural Network
architecture gave very good results on various test patterns.
</summary>
    <author>
      <name>Dasika Ratna Deepthi</name>
    </author>
    <author>
      <name>Sujeet Kuchibhotla</name>
    </author>
    <author>
      <name>K. Eswaran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented in IEEE International Conference on Advances in Computer
  Vision and Information Technology (ACVIT-07), Nov. 28-30 2007</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference On Advances in Computer Vision and
  Information Tech. (IEEE, ACVIT-07), pp. 348 - 353 (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0712.0932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.0932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.4810v1</id>
    <updated>2009-05-29T09:18:46Z</updated>
    <published>2009-05-29T09:18:46Z</published>
    <title>Back-engineering of spiking neural networks parameters</title>
    <summary>  We consider the deterministic evolution of a time-discretized spiking network
of neurons with connection weights having delays, modeled as a discretized
neural network of the generalized integrate and fire (gIF) type. The purpose is
to study a class of algorithmic methods allowing to calculate the proper
parameters to reproduce exactly a given spike train generated by an hidden
(unknown) neural network. This standard problem is known as NP-hard when delays
are to be calculated. We propose here a reformulation, now expressed as a
Linear-Programming (LP) problem, thus allowing to provide an efficient
resolution. This allows us to "back-engineer" a neural network, i.e. to find
out, given a set of initial conditions, which parameters (i.e., connection
weights in this case), allow to simulate the network spike dynamics. More
precisely we make explicit the fact that the back-engineering of a spike train,
is a Linear (L) problem if the membrane potentials are observed and a LP
problem if only spike times are observed, with a gIF model. Numerical
robustness is discussed. We also explain how it is the use of a generalized IF
neuron model instead of a leaky IF model that allows us to derive this
algorithm. Furthermore, we point out how the L or LP adjustment mechanism is
local to each unit and has the same structure as an "Hebbian" rule. A step
further, this paradigm is easily generalizable to the design of input-output
spike train transformations. This means that we have a practical method to
"program" a spiking network, i.e. find a set of parameters allowing us to
exactly reproduce the network output, given an input. Numerical verifications
and illustrations are provided.
</summary>
    <author>
      <name>H. Rostro</name>
    </author>
    <author>
      <name>B. Cessac</name>
    </author>
    <author>
      <name>J. C. Vasquez</name>
    </author>
    <author>
      <name>T. Vieville</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 17 figures, submitted</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J. Neural Eng. 9 (2012) 026024</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0905.4810v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.4810v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.03877v2</id>
    <updated>2016-02-07T09:31:36Z</updated>
    <published>2015-09-13T18:33:15Z</published>
    <title>Learning Contextual Dependencies with Convolutional Hierarchical
  Recurrent Neural Networks</title>
    <summary>  Existing deep convolutional neural networks (CNNs) have shown their great
success on image classification. CNNs mainly consist of convolutional and
pooling layers, both of which are performed on local image areas without
considering the dependencies among different image regions. However, such
dependencies are very important for generating explicit image representation.
In contrast, recurrent neural networks (RNNs) are well known for their ability
of encoding contextual information among sequential data, and they only require
a limited number of network parameters. General RNNs can hardly be directly
applied on non-sequential data. Thus, we proposed the hierarchical RNNs
(HRNNs). In HRNNs, each RNN layer focuses on modeling spatial dependencies
among image regions from the same scale but different locations. While the
cross RNN scale connections target on modeling scale dependencies among regions
from the same location but different scales. Specifically, we propose two
recurrent neural network models: 1) hierarchical simple recurrent network
(HSRN), which is fast and has low computational cost; and 2) hierarchical
long-short term memory recurrent network (HLSTM), which performs better than
HSRN with the price of more computational cost.
  In this manuscript, we integrate CNNs with HRNNs, and develop end-to-end
convolutional hierarchical recurrent neural networks (C-HRNNs). C-HRNNs not
only make use of the representation power of CNNs, but also efficiently encodes
spatial and scale dependencies among different image regions. On four of the
most challenging object/scene image classification benchmarks, our C-HRNNs
achieve state-of-the-art results on Places 205, SUN 397, MIT indoor, and
competitive results on ILSVRC 2012.
</summary>
    <author>
      <name>Zhen Zuo</name>
    </author>
    <author>
      <name>Bing Shuai</name>
    </author>
    <author>
      <name>Gang Wang</name>
    </author>
    <author>
      <name>Xiao Liu</name>
    </author>
    <author>
      <name>Xingxing Wang</name>
    </author>
    <author>
      <name>Bing Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIP.2016.2548241</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIP.2016.2548241" rel="related"/>
    <link href="http://arxiv.org/abs/1509.03877v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.03877v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06353v1</id>
    <updated>2016-03-21T08:30:43Z</updated>
    <published>2016-03-21T08:30:43Z</published>
    <title>A Discontinuous Neural Network for Non-Negative Sparse Approximation</title>
    <summary>  This paper investigates a discontinuous neural network which is used as a
model of the mammalian olfactory system and can more generally be applied to
solve non-negative sparse approximation problems. By inherently limiting the
systems integrators to having non-negative outputs, the system function becomes
discontinuous since the integrators switch between being inactive and being
active. It is shown that the presented network converges to equilibrium points
which are solutions to general non-negative least squares optimization
problems. We specify a Caratheodory solution and prove that the network is
stable, provided that the system matrix has full column-rank. Under a mild
condition on the equilibrium point, we show that the network converges to its
equilibrium within a finite number of switches. Two applications of the neural
network are shown. Firstly, we apply the network as a model of the olfactory
system and show that in principle it may be capable of performing complex
sparse signal recovery tasks. Secondly, we generalize the application to
include non-negative sparse approximation problems and compare the recovery
performance to a classical non-negative basis pursuit denoising algorithm. We
conclude that the recovery performance differs only marginally from the
classical algorithm, while the neural network has the advantage that no
performance critical regularization parameter has to be chosen prior to
recovery.
</summary>
    <author>
      <name>Martijn Arts</name>
    </author>
    <author>
      <name>Marius Cordts</name>
    </author>
    <author>
      <name>Monika Gorin</name>
    </author>
    <author>
      <name>Marc Spehr</name>
    </author>
    <author>
      <name>Rudolf Mathar</name>
    </author>
    <link href="http://arxiv.org/abs/1603.06353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.02913v1</id>
    <updated>2017-10-09T02:33:43Z</updated>
    <published>2017-10-09T02:33:43Z</published>
    <title>Machine Learning for Wireless Networks with Artificial Intelligence: A
  Tutorial on Neural Networks</title>
    <summary>  Next-generation wireless networks must support ultra-reliable, low-latency
communication and intelligently manage a massive number of Internet of Things
(IoT) devices in real-time, within a highly dynamic environment. This need for
stringent communication quality-of-service (QoS) requirements as well as mobile
edge and core intelligence can only be realized by integrating fundamental
notions of artificial intelligence (AI) and machine learning across the
wireless infrastructure and end-user devices. In this context, this paper
provides a comprehensive tutorial that introduces the main concepts of machine
learning, in general, and artificial neural networks (ANNs), in particular, and
their potential applications in wireless communications. For this purpose, we
present a comprehensive overview on a number of key types of neural networks
that include feed-forward, recurrent, spiking, and deep neural networks. For
each type of neural network, we present the basic architecture and training
procedure, as well as the associated challenges and opportunities. Then, we
provide an in-depth overview on the variety of wireless communication problems
that can be addressed using ANNs, ranging from communication using unmanned
aerial vehicles to virtual reality and edge caching.For each individual
application, we present the main motivation for using ANNs along with the
associated challenges while also providing a detailed example for a use case
scenario and outlining future works that can be addressed using ANNs. In a
nutshell, this article constitutes one of the first holistic tutorials on the
development of machine learning techniques tailored to the needs of future
wireless networks.
</summary>
    <author>
      <name>Mingzhe Chen</name>
    </author>
    <author>
      <name>Ursula Challita</name>
    </author>
    <author>
      <name>Walid Saad</name>
    </author>
    <author>
      <name>Changchuan Yin</name>
    </author>
    <author>
      <name>M√©rouane Debbah</name>
    </author>
    <link href="http://arxiv.org/abs/1710.02913v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.02913v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.07493v1</id>
    <updated>2017-12-20T14:24:00Z</updated>
    <published>2017-12-20T14:24:00Z</published>
    <title>Learning a Wavelet-like Auto-Encoder to Accelerate Deep Neural Networks</title>
    <summary>  Accelerating deep neural networks (DNNs) has been attracting increasing
attention as it can benefit a wide range of applications, e.g., enabling mobile
systems with limited computing resources to own powerful visual recognition
ability. A practical strategy to this goal usually relies on a two-stage
process: operating on the trained DNNs (e.g., approximating the convolutional
filters with tensor decomposition) and fine-tuning the amended network, leading
to difficulty in balancing the trade-off between acceleration and maintaining
recognition performance. In this work, aiming at a general and comprehensive
way for neural network acceleration, we develop a Wavelet-like Auto-Encoder
(WAE) that decomposes the original input image into two low-resolution channels
(sub-images) and incorporate the WAE into the classification neural networks
for joint training. The two decomposed channels, in particular, are encoded to
carry the low-frequency information (e.g., image profiles) and high-frequency
(e.g., image details or noises), respectively, and enable reconstructing the
original input image through the decoding process. Then, we feed the
low-frequency channel into a standard classification network such as VGG or
ResNet and employ a very lightweight network to fuse with the high-frequency
channel to obtain the classification result. Compared to existing DNN
acceleration solutions, our framework has the following advantages: i) it is
tolerant to any existing convolutional neural networks for classification
without amending their structures; ii) the WAE provides an interpretable way to
preserve the main components of the input image for classification.
</summary>
    <author>
      <name>Tianshui Chen</name>
    </author>
    <author>
      <name>Liang Lin</name>
    </author>
    <author>
      <name>Wangmeng Zuo</name>
    </author>
    <author>
      <name>Xiaonan Luo</name>
    </author>
    <author>
      <name>Lei Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at AAAI 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.07493v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.07493v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.2983v2</id>
    <updated>2008-06-18T15:39:44Z</updated>
    <published>2008-04-18T09:45:50Z</published>
    <title>Optimal network topologies for information transmission in active
  networks</title>
    <summary>  This work clarifies the relation between network circuit (topology) and
behavior (information transmission and synchronization) in active networks,
e.g. neural networks. As an application, we show how to determine a network
topology that is optimal for information transmission. By optimal, we mean that
the network is able to transmit a large amount of information, it possesses a
large number of communication channels, and it is robust under large variations
of the network coupling configuration. This theoretical approach is general and
does not depend on the particular dynamic of the elements forming the network,
since the network topology can be determined by finding a Laplacian matrix (the
matrix that describes the connections and the coupling strengths among the
elements) whose eigenvalues satisfy some special conditions. To illustrate our
ideas and theoretical approaches, we use neural networks of electrically
connected chaotic Hindmarsh-Rose neurons.
</summary>
    <author>
      <name>M. S. Baptista</name>
    </author>
    <author>
      <name>J. X. de Carvalho</name>
    </author>
    <author>
      <name>M. S. Hussein</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pone.0003479</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pone.0003479" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0804.2983v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.2983v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.PS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.3287v2</id>
    <updated>2012-12-07T09:44:18Z</updated>
    <published>2010-12-15T11:28:57Z</published>
    <title>Binary threshold networks as a natural null model for biological
  networks</title>
    <summary>  Spin models of neural networks and genetic networks are considered elegant as
they are accessible to statistical mechanics tools for spin glasses and
magnetic systems. However, the conventional choice of variables in spin systems
may cause problems in some models when parameter choices are unrealistic from a
biological perspective. Obviously, this may limit the role of a model as a
template model for biological systems. Perhaps less obviously, also ensembles
of random networks are affected and may exhibit different critical properties.
We consider here a prototypical network model that is biologically plausible in
its local mechanisms. We study a discrete dynamical network with two
characteristic properties: Nodes with binary states 0 and 1, and a modified
threshold function with $\Theta_0(0)=0$. We explore the critical properties of
random networks of such nodes and find a critical connectivity $K_c=2.0$ with
activity vanishing at the critical point. Finally, we observe that the present
model allows a more natural implementation of recent models of budding yeast
and fission yeast cell-cycle control networks.
</summary>
    <author>
      <name>Matthias Rybarsch</name>
    </author>
    <author>
      <name>Stefan Bornholdt</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.86.026114</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.86.026114" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 86, 026114 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1012.3287v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.3287v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.4064v1</id>
    <updated>2014-05-16T04:58:13Z</updated>
    <published>2014-05-16T04:58:13Z</published>
    <title>Self-organized criticality of a simplified integrate-and-fire neural
  model on random and small-world network</title>
    <summary>  We consider the criticality for firing structures of a simplified
integrate-and-fire neural model on the regular network, small-world network,
and random networks. We simplify an integrate-and-fire model suggested by
Levina, Herrmann and Geisel (LHG). In our model we set up the synaptic strength
as a constant value. We observed the power law behaviors of the probability
distribution of the avalanche size and the life time of the avalanche. The
critical exponents in the small-world network and the random network were the
same as those in the fully connected network. However, in the regular
one-dimensional ring, the model does not show the critical behaviors. In the
simplified LHG model, the short-cuts are crucial role in the self-organized
criticality. The simplified LHG model in three types of networks such as the
fully connected network, the small-world network, and random network belong to
the same universality class.
</summary>
    <author>
      <name>Hyung Wooc Choi</name>
    </author>
    <author>
      <name>Nam Jung</name>
    </author>
    <author>
      <name>Jae Woo Lee</name>
    </author>
    <link href="http://arxiv.org/abs/1405.4064v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.4064v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.05279v4</id>
    <updated>2016-08-02T20:59:14Z</updated>
    <published>2016-03-16T20:56:21Z</published>
    <title>XNOR-Net: ImageNet Classification Using Binary Convolutional Neural
  Networks</title>
    <summary>  We propose two efficient approximations to standard convolutional neural
networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks,
the filters are approximated with binary values resulting in 32x memory saving.
In XNOR-Networks, both the filters and the input to convolutional layers are
binary. XNOR-Networks approximate convolutions using primarily binary
operations. This results in 58x faster convolutional operations and 32x memory
savings. XNOR-Nets offer the possibility of running state-of-the-art networks
on CPUs (rather than GPUs) in real-time. Our binary networks are simple,
accurate, efficient, and work on challenging visual tasks. We evaluate our
approach on the ImageNet classification task. The classification accuracy with
a Binary-Weight-Network version of AlexNet is only 2.9% less than the
full-precision AlexNet (in top-1 measure). We compare our method with recent
network binarization methods, BinaryConnect and BinaryNets, and outperform
these methods by large margins on ImageNet, more than 16% in top-1 accuracy.
</summary>
    <author>
      <name>Mohammad Rastegari</name>
    </author>
    <author>
      <name>Vicente Ordonez</name>
    </author>
    <author>
      <name>Joseph Redmon</name>
    </author>
    <author>
      <name>Ali Farhadi</name>
    </author>
    <link href="http://arxiv.org/abs/1603.05279v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.05279v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.02699v1</id>
    <updated>2017-05-07T21:14:02Z</updated>
    <published>2017-05-07T21:14:02Z</published>
    <title>Spatiotemporal Recurrent Convolutional Networks for Traffic Prediction
  in Transportation Networks</title>
    <summary>  Predicting large-scale transportation network traffic has become an important
and challenging topic in recent decades. Inspired by the domain knowledge of
motion prediction, in which the future motion of an object can be predicted
based on previous scenes, we propose a network grid representation method that
can retain the fine-scale structure of a transportation network. Network-wide
traffic speeds are converted into a series of static images and input into a
novel deep architecture, namely, spatiotemporal recurrent convolutional
networks (SRCNs), for traffic forecasting. The proposed SRCNs inherit the
advantages of deep convolutional neural networks (DCNNs) and long short-term
memory (LSTM) neural networks. The spatial dependencies of network-wide traffic
can be captured by DCNNs, and the temporal dynamics can be learned by LSTMs. An
experiment on a Beijing transportation network with 278 links demonstrates that
SRCNs outperform other deep learning-based algorithms in both short-term and
long-term traffic prediction.
</summary>
    <author>
      <name>Haiyang Yu</name>
    </author>
    <author>
      <name>Zhihai Wu</name>
    </author>
    <author>
      <name>Shuqin Wang</name>
    </author>
    <author>
      <name>Yunpeng Wang</name>
    </author>
    <author>
      <name>Xiaolei Ma</name>
    </author>
    <link href="http://arxiv.org/abs/1705.02699v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.02699v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.05690v3</id>
    <updated>2017-06-08T12:31:20Z</updated>
    <published>2017-05-16T12:57:24Z</published>
    <title>A Long Short-Term Memory Recurrent Neural Network Framework for Network
  Traffic Matrix Prediction</title>
    <summary>  Network Traffic Matrix (TM) prediction is defined as the problem of
estimating future network traffic from the previous and achieved network
traffic data. It is widely used in network planning, resource management and
network security. Long Short-Term Memory (LSTM) is a specific recurrent neural
network (RNN) architecture that is well-suited to learn from experience to
classify, process and predict time series with time lags of unknown size. LSTMs
have been shown to model temporal sequences and their long-range dependencies
more accurately than conventional RNNs. In this paper, we propose a LSTM RNN
framework for predicting short and long term Traffic Matrix (TM) in large
networks. By validating our framework on real-world data from GEANT network, we
show that our LSTM models converge quickly and give state of the art TM
prediction performance for relatively small sized models.
</summary>
    <author>
      <name>Abdelhadi Azzouni</name>
    </author>
    <author>
      <name>Guy Pujolle</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted for peer review. arXiv admin note: text overlap with
  arXiv:1402.1128 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.05690v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.05690v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.02261v1</id>
    <updated>2018-02-06T23:22:44Z</updated>
    <published>2018-02-06T23:22:44Z</published>
    <title>Dynamic regulation of resource transport induces criticality in
  multilayer networks of excitable units</title>
    <summary>  Past work has shown that the function of a network of excitable units can be
enhanced if the network is in the `critical regime', where excitations are, one
average, neither damped nor amplified. In this Letter, we show that resource
transport dynamics can robustly maintain the network dynamics in the critical
regime. More specifically, we consider the example of a neural network with
neurons (nodes) and synapses (edges). We propose a model where synapse
strengths are regulated by metabolic resources distributed by a secondary
network of glial cells. We find that this two-layer network robustly preserves
the critical state and produces power-law distributed avalanches over a wide
range of parameters. In addition, the glial cell network protects the system
against the destabilizing effect of local variations in parameters and
heterogeneity in network structure. For homogeneous networks, we derive a
reduced 3-dimensional map which reproduces the behavior of the full system.
</summary>
    <author>
      <name>Yogesh S. Virkar</name>
    </author>
    <author>
      <name>Juan G. Restrepo</name>
    </author>
    <author>
      <name>Woodrow L. Shew</name>
    </author>
    <author>
      <name>Edward Ott</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.02261v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.02261v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.07811v2</id>
    <updated>2017-09-18T18:14:49Z</updated>
    <published>2017-02-25T00:22:51Z</published>
    <title>Adaptive Neural Networks for Efficient Inference</title>
    <summary>  We present an approach to adaptively utilize deep neural networks in order to
reduce the evaluation time on new examples without loss of accuracy. Rather
than attempting to redesign or approximate existing networks, we propose two
schemes that adaptively utilize networks. We first pose an adaptive network
evaluation scheme, where we learn a system to adaptively choose the components
of a deep network to be evaluated for each example. By allowing examples
correctly classified using early layers of the system to exit, we avoid the
computational time associated with full evaluation of the network. We extend
this to learn a network selection system that adaptively selects the network to
be evaluated for each example. We show that computational time can be
dramatically reduced by exploiting the fact that many examples can be correctly
classified using relatively efficient networks and that complex,
computationally costly networks are only necessary for a small fraction of
examples. We pose a global objective for learning an adaptive early exit or
network selection policy and solve it by reducing the policy learning problem
to a layer-by-layer weighted binary classification problem. Empirically, these
approaches yield dramatic reductions in computational cost, with up to a 2.8x
speedup on state-of-the-art networks from the ImageNet image recognition
challenge with minimal (&lt;1%) loss of top5 accuracy.
</summary>
    <author>
      <name>Tolga Bolukbasi</name>
    </author>
    <author>
      <name>Joseph Wang</name>
    </author>
    <author>
      <name>Ofer Dekel</name>
    </author>
    <author>
      <name>Venkatesh Saligrama</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 34th International Conference on Machine
  Learning, PMLR 70:527-536, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1702.07811v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.07811v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.4873v1</id>
    <updated>2010-08-28T14:55:38Z</updated>
    <published>2010-08-28T14:55:38Z</published>
    <title>Spiking Neurons with ASNN Based-Methods for the Neural Block Cipher</title>
    <summary>  Problem statement: This paper examines Artificial Spiking Neural Network
(ASNN) which inter-connects group of artificial neurons that uses a
mathematical model with the aid of block cipher. The aim of undertaken this
research is to come up with a block cipher where by the keys are randomly
generated by ASNN which can then have any variable block length. This will show
the private key is kept and do not have to be exchange to the other side of the
communication channel so it present a more secure procedure of key scheduling.
The process enables for a faster change in encryption keys and a network level
encryption to be implemented at a high speed without the headache of
factorization. Approach: The block cipher is converted in public cryptosystem
and had a low level of vulnerability to attack from brute, and moreover can
able to defend against linear attacks since the Artificial Neural Networks
(ANN) architecture convey non-linearity to the encryption/decryption
procedures. Result: In this paper is present to use the Spiking Neural Networks
(SNNs) with spiking neurons as its basic unit. The timing for the SNNs is
considered and the output is encoded in 1's and 0's depending on the occurrence
or not occurrence of spikes as well as the spiking neural networks use a sign
function as activation function, and present the weights and the filter
coefficients to be adjust, having more degrees of freedom than the classical
neural networks. Conclusion: In conclusion therefore, encryption algorithm can
be deployed in communication and security applications where data transfers are
most crucial. So this paper, the neural block cipher proposed where the keys
are generated by the SNN and the seed is considered the public key which
generates the both keys on both sides In future therefore a new research will
be conducted on the Spiking Neural Network (SNN) impacts on communication.
</summary>
    <author>
      <name>Saleh Ali K. Al-Omari</name>
    </author>
    <author>
      <name>Putra Sumari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 Figures, International journal of computer science &amp;
  information Technology</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International journal of computer science &amp; information Technology
  (IJCSIT) Vol.2, No.4, August 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1008.4873v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.4873v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.3943v1</id>
    <updated>2013-02-16T08:10:04Z</updated>
    <published>2013-02-16T08:10:04Z</published>
    <title>Interplay between Network Topology and Dynamics in Neural Systems</title>
    <summary>  This thesis is a compendium of research which brings together ideas from the
fields of Complex Networks and Computational Neuroscience to address two
questions regarding neural systems:
  1) How the activity of neurons, via synaptic changes, can shape the topology
of the network they form part of, and
  2) How the resulting network structure, in its turn, might condition aspects
of brain behaviour.
  Although the emphasis is on neural networks, several theoretical findings
which are relevant for complex networks in general are presented -- such as a
method for studying network evolution as a stochastic process, or a theory that
allows for ensembles of correlated networks, and sets of dynamical elements
thereon, to be treated mathematically and computationally in a
model-independent manner. Some of the results are used to explain experimental
data -- certain properties of brain tissue, the spontaneous emergence of
correlations in all kinds of networks... -- and predictions regarding
statistical aspects of the central nervous system are made. The mechanism of
Cluster Reverberation is proposed to account for the near-instant storage of
novel information the brain is capable of.
</summary>
    <author>
      <name>Samuel Johnson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PhD thesis in physics, defended at the University of Granada in May
  2011. Advisors: Joaqu\'in J. Torres and Joaqu\'in Marro. All the text is in
  English save for the acknowledgements and a summary in Spanish</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.3943v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.3943v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.8278v1</id>
    <updated>2014-09-30T09:17:44Z</updated>
    <published>2014-09-30T09:17:44Z</published>
    <title>From Caenorhabditis elegans to the Human Connectome: A Specific Modular
  Organisation Increases Metabolic, Functional, and Developmental Efficiency</title>
    <summary>  The connectome, or the entire connectivity of a neural system represented by
network, ranges various scales from synaptic connections between individual
neurons to fibre tract connections between brain regions. Although the
modularity they commonly show has been extensively studied, it is unclear
whether connection specificity of such networks can already be fully explained
by the modularity alone. To answer this question, we study two networks, the
neuronal network of C. elegans and the fibre tract network of human brains
yielded through diffusion spectrum imaging (DSI). We compare them to their
respective benchmark networks with varying modularities, which are generated by
link swapping to have desired modularity values but otherwise maximally random.
We find several network properties that are specific to the neural networks and
cannot be fully explained by the modularity alone. First, the clustering
coefficient and the characteristic path length of C. elegans and human
connectomes are both higher than those of the benchmark networks with similar
modularity. High clustering coefficient indicates efficient local information
distribution and high characteristic path length suggests reduced global
integration. Second, the total wiring length is smaller than for the
alternative configurations with similar modularity. This is due to lower
dispersion of connections, which means each neuron in C. elegans connectome or
each region of interest (ROI) in human connectome reaches fewer ganglia or
cortical areas, respectively. Third, both neural networks show lower
algorithmic entropy compared to the alternative arrangements. This implies that
fewer rules are needed to encode for the organisation of neural systems.
</summary>
    <author>
      <name>Jinseop S. Kim</name>
    </author>
    <author>
      <name>Marcus Kaiser</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1098/rstb.2013.0529</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1098/rstb.2013.0529" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phil. Trans. R. Soc. B 369: 20130529, 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1409.8278v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.8278v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.03940v1</id>
    <updated>2016-12-12T21:36:48Z</updated>
    <published>2016-12-12T21:36:48Z</published>
    <title>Understanding the Impact of Precision Quantization on the Accuracy and
  Energy of Neural Networks</title>
    <summary>  Deep neural networks are gaining in popularity as they are used to generate
state-of-the-art results for a variety of computer vision and machine learning
applications. At the same time, these networks have grown in depth and
complexity in order to solve harder problems. Given the limitations in power
budgets dedicated to these networks, the importance of low-power, low-memory
solutions has been stressed in recent years. While a large number of dedicated
hardware using different precisions has recently been proposed, there exists no
comprehensive study of different bit precisions and arithmetic in both inputs
and network parameters. In this work, we address this issue and perform a study
of different bit-precisions in neural networks (from floating-point to
fixed-point, powers of two, and binary). In our evaluation, we consider and
analyze the effect of precision scaling on both network accuracy and hardware
metrics including memory footprint, power and energy consumption, and design
area. We also investigate training-time methodologies to compensate for the
reduction in accuracy due to limited bit precision and demonstrate that in most
cases, precision scaling can deliver significant benefits in design metrics at
the cost of very modest decreases in network accuracy. In addition, we propose
that a small portion of the benefits achieved when using lower precisions can
be forfeited to increase the network size and therefore the accuracy. We
evaluate our experiments, using three well-recognized networks and datasets to
show its generality. We investigate the trade-offs and highlight the benefits
of using lower precisions in terms of energy and memory footprint.
</summary>
    <author>
      <name>Soheil Hashemi</name>
    </author>
    <author>
      <name>Nicholas Anthony</name>
    </author>
    <author>
      <name>Hokchhay Tann</name>
    </author>
    <author>
      <name>R. Iris Bahar</name>
    </author>
    <author>
      <name>Sherief Reda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for conference proceedings in DATE17</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.03940v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.03940v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00904v1</id>
    <updated>2018-02-03T03:50:41Z</updated>
    <published>2018-02-03T03:50:41Z</published>
    <title>Build a Compact Binary Neural Network through Bit-level Sensitivity and
  Data Pruning</title>
    <summary>  Convolutional neural network (CNN) has been widely used for vision-based
tasks. Due to the high computational complexity and memory storage requirement,
it is hard to directly deploy a full-precision CNN on embedded devices. The
hardware-friendly designs are needed for re-source-limited and
energy-constrained embed-ded devices. Emerging solutions are adopted for the
neural network compression, e.g., bina-ry/ternary weight network, pruned
network and quantized network. Among them, Binarized Neural Network (BNN) is
believed to be the most hardware-friendly framework due to its small network
size and low computational com-plexity. No existing work has further shrunk the
size of BNN. In this work, we explore the redun-dancy in BNN and build a
compact BNN (CBNN) based on the bit-level sensitivity analy-sis and bit-level
data pruning. The input data is converted to a high dimensional bit-sliced
for-mat. In post-training stage, we analyze the im-pact of different bit slices
to the accuracy. By pruning the redundant input bit slices and shrinking the
network size, we are able to build a more compact BNN. Our result shows that we
can further scale down the network size of the BNN up to 3.9x with no more than
1% accuracy drop. The actual runtime can be reduced up to 2x and 9.9x compared
with the baseline BNN and its full-precision counterpart, respectively.
</summary>
    <author>
      <name>Yixing Li</name>
    </author>
    <author>
      <name>Fengbo Ren</name>
    </author>
    <link href="http://arxiv.org/abs/1802.00904v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00904v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0307031v2</id>
    <updated>2003-07-16T13:10:30Z</updated>
    <published>2003-07-12T12:04:33Z</published>
    <title>Automatic Classification using Self-Organising Neural Networks in
  Astrophysical Experiments</title>
    <summary>  Self-Organising Maps (SOMs) are effective tools in classification problems,
and in recent years the even more powerful Dynamic Growing Neural Networks, a
variant of SOMs, have been developed. Automatic Classification (also called
clustering) is an important and difficult problem in many Astrophysical
experiments, for instance, Gamma Ray Burst classification, or gamma-hadron
separation. After a brief introduction to classification problem, we discuss
Self-Organising Maps in section 2. Section 3 discusses with various models of
growing neural networks and finally in section 4 we discuss the research
perspectives in growing neural networks for efficient classification in
astrophysical problems.
</summary>
    <author>
      <name>P. Boinee</name>
    </author>
    <author>
      <name>A. De Angelis</name>
    </author>
    <author>
      <name>E. Milotti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 Pages, corrected authors name format</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">S. Ciprini, A. De Angelis, P. Lubrano and O. Mansutti (eds.):
  Proc. of ``Science with the New Generation of High Energy Gamma-ray
  Experiments'' (Perugia, Italy, May 2003). Forum, Udine 2003, p. 177</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0307031v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0307031v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.1; I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0310009v3</id>
    <updated>2003-11-04T12:52:42Z</updated>
    <published>2003-10-06T15:40:44Z</published>
    <title>On Interference of Signals and Generalization in Feedforward Neural
  Networks</title>
    <summary>  This paper studies how the generalization ability of neurons can be affected
by mutual processing of different signals. This study is done on the basis of a
feedforward artificial neural network. The mutual processing of signals can
possibly be a good model of patterns in a set generalized by a neural network
and in effect may improve generalization. In this paper it is discussed that
the interference may also cause a highly random generalization. Adaptive
activation functions are discussed as a way of reducing that type of
generalization. A test of a feedforward neural network is performed that shows
the discussed random generalization.
</summary>
    <author>
      <name>Artur Rataj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures. Some changes in text to make it more concise</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0310009v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0310009v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0512018v2</id>
    <updated>2006-03-21T12:31:02Z</updated>
    <published>2005-12-05T06:57:39Z</published>
    <title>DAMNED: A Distributed and Multithreaded Neural Event-Driven simulation
  framework</title>
    <summary>  In a Spiking Neural Networks (SNN), spike emissions are sparsely and
irregularly distributed both in time and in the network architecture. Since a
current feature of SNNs is a low average activity, efficient implementations of
SNNs are usually based on an Event-Driven Simulation (EDS). On the other hand,
simulations of large scale neural networks can take advantage of distributing
the neurons on a set of processors (either workstation cluster or parallel
computer). This article presents DAMNED, a large scale SNN simulation framework
able to gather the benefits of EDS and parallel computing. Two levels of
parallelism are combined: Distributed mapping of the neural topology, at the
network level, and local multithreaded allocation of resources for simultaneous
processing of events, at the neuron level. Based on the causality of events, a
distributed solution is proposed for solving the complex problem of scheduling
without synchronization barrier.
</summary>
    <author>
      <name>Anthony Mouraud</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GRIMAAG, ISC</arxiv:affiliation>
    </author>
    <author>
      <name>Didier Puzenat</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GRIMAAG</arxiv:affiliation>
    </author>
    <author>
      <name>H√©l√®ne Paugam-Moisy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISC</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0512018v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0512018v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0605065v4</id>
    <updated>2007-04-08T20:19:23Z</updated>
    <published>2006-05-15T17:56:55Z</published>
    <title>On the possible Computational Power of the Human Mind</title>
    <summary>  The aim of this paper is to address the question: Can an artificial neural
network (ANN) model be used as a possible characterization of the power of the
human mind? We will discuss what might be the relationship between such a model
and its natural counterpart. A possible characterization of the different power
capabilities of the mind is suggested in terms of the information contained (in
its computational complexity) or achievable by it. Such characterization takes
advantage of recent results based on natural neural networks (NNN) and the
computational power of arbitrary artificial neural networks (ANN). The possible
acceptance of neural networks as the model of the human mind's operation makes
the aforementioned quite relevant.
</summary>
    <author>
      <name>Hector Zenil</name>
    </author>
    <author>
      <name>Francisco Hernandez-Quiroz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1142/9789812707420_0020</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1142/9789812707420_0020" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Complexity, Science and Society Conference, 2005, University of
  Liverpool, UK. 23 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0605065v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0605065v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0704.1028v1</id>
    <updated>2007-04-08T17:36:00Z</updated>
    <published>2007-04-08T17:36:00Z</published>
    <title>A neural network approach to ordinal regression</title>
    <summary>  Ordinal regression is an important type of learning, which has properties of
both classification and regression. Here we describe a simple and effective
approach to adapt a traditional neural network to learn ordinal categories. Our
approach is a generalization of the perceptron method for ordinal regression.
On several benchmark datasets, our method (NNRank) outperforms a neural network
classification method. Compared with the ordinal regression methods using
Gaussian processes and support vector machines, NNRank achieves comparable
performance. Moreover, NNRank has the advantages of traditional neural
networks: learning in both online and batch modes, handling very large training
datasets, and making rapid predictions. These features make NNRank a useful and
complementary tool for large-scale data processing tasks such as information
retrieval, web page ranking, collaborative filtering, and protein ranking in
Bioinformatics.
</summary>
    <author>
      <name>Jianlin Cheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0704.1028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0704.1028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.1412v1</id>
    <updated>2008-02-11T11:12:06Z</updated>
    <published>2008-02-11T11:12:06Z</published>
    <title>Extreme Learning Machine for land cover classification</title>
    <summary>  This paper explores the potential of extreme learning machine based
supervised classification algorithm for land cover classification. In
comparison to a backpropagation neural network, which requires setting of
several user-defined parameters and may produce local minima, extreme learning
machine require setting of one parameter and produce a unique solution. ETM+
multispectral data set (England) was used to judge the suitability of extreme
learning machine for remote sensing classifications. A back propagation neural
network was used to compare its performance in term of classification accuracy
and computational cost. Results suggest that the extreme learning machine
perform equally well to back propagation neural network in term of
classification accuracy with this data set. The computational cost using
extreme learning machine is very small in comparison to back propagation neural
network.
</summary>
    <author>
      <name>Mahesh Pal</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/01431160902788636</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/01431160902788636" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, mapindia 2008 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/0802.1412v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.1412v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.4650v1</id>
    <updated>2008-06-28T04:43:40Z</updated>
    <published>2008-06-28T04:43:40Z</published>
    <title>Structural Damage Detection Using Randomized Trained Neural Networks</title>
    <summary>  A computationally method on damage detection problems in structures was
conducted using neural networks. The problem that is considered in this works
consists of estimating the existence, location and extent of stiffness
reduction in structure which is indicated by the changes of the structural
static parameters such as deflection and strain. The neural network was trained
to recognize the behaviour of static parameter of the undamaged structure as
well as of the structure with various possible damage extent and location which
were modelled as random states. The proposed techniques were applied to detect
damage in a simply supported beam. The structure was analyzed using
finite-element-method (FEM) and the damage identification was conducted by a
back-propagation neural network using the change of the structural strain and
displacement. The results showed that using proposed method the strain is more
efficient for identification of damage than the displacement.
</summary>
    <author>
      <name>Ismoyo Haryanto</name>
    </author>
    <author>
      <name>Joga Dharma Setiawan</name>
    </author>
    <author>
      <name>Agus Budiyono</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Uploaded by ICIUS2007 Conference Organizer on behalf of the
  author(s). 5 pages, 9 figures, and 4 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on Intelligent
  Unmanned System (ICIUS 2007), Bali, Indonesia, October 24-25, 2007, Paper No.
  ICIUS2007-C022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0806.4650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.4650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0810.3992v2</id>
    <updated>2009-03-20T10:55:15Z</updated>
    <published>2008-10-22T08:02:47Z</published>
    <title>Introducing numerical bounds to improve event-based neural network
  simulation</title>
    <summary>  Although the spike-trains in neural networks are mainly constrained by the
neural dynamics itself, global temporal constraints (refractoriness, time
precision, propagation delays, ..) are also to be taken into account. These
constraints are revisited in this paper in order to use them in event-based
simulation paradigms.
  We first review these constraints, and discuss their consequences at the
simulation level, showing how event-based simulation of time-constrained
networks can be simplified in this context: the underlying data-structures are
strongly simplified, while event-based and clock-based mechanisms can be easily
mixed. These ideas are applied to punctual conductance-based generalized
integrate-and-fire neural networks simulation, while spike-response model
simulations are also revisited within this framework.
  As an outcome, a fast minimal complementary alternative with respect to
existing simulation event-based methods, with the possibility to simulate
interesting neuron models is implemented and experimented.
</summary>
    <author>
      <name>Bruno Cessac</name>
    </author>
    <author>
      <name>Olivier Rochel</name>
    </author>
    <author>
      <name>Thierry Vi√©ville</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/0810.3992v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.3992v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.2310v1</id>
    <updated>2009-12-11T18:40:06Z</updated>
    <published>2009-12-11T18:40:06Z</published>
    <title>NeuralNetwork Based 3D Surface Reconstruction</title>
    <summary>  This paper proposes a novel neural-network-based adaptive hybrid-reflectance
three-dimensional (3-D) surface reconstruction model. The neural network
combines the diffuse and specular components into a hybrid model. The proposed
model considers the characteristics of each point and the variant albedo to
prevent the reconstructed surface from being distorted. The neural network
inputs are the pixel values of the two-dimensional images to be reconstructed.
The normal vectors of the surface can then be obtained from the output of the
neural network after supervised learning, where the illuminant direction does
not have to be known in advance. Finally, the obtained normal vectors can be
applied to integration method when reconstructing 3-D objects. Facial images
were used for training in the proposed approach
</summary>
    <author>
      <name>Vincy Joseph</name>
    </author>
    <author>
      <name>Shalini Bhatia</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSE Volume 1 Issue 3 2009 116-121</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0912.2310v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.2310v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.3513v2</id>
    <updated>2010-08-02T20:46:19Z</updated>
    <published>2009-12-17T20:39:04Z</published>
    <title>Stimulus-Dependent Suppression of Chaos in Recurrent Neural Networks</title>
    <summary>  Neuronal activity arises from an interaction between ongoing firing generated
spontaneously by neural circuits and responses driven by external stimuli.
Using mean-field analysis, we ask how a neural network that intrinsically
generates chaotic patterns of activity can remain sensitive to extrinsic input.
We find that inputs not only drive network responses, they also actively
suppress ongoing activity, ultimately leading to a phase transition in which
chaos is completely eliminated. The critical input intensity at the phase
transition is a non-monotonic function of stimulus frequency, revealing a
"resonant" frequency at which the input is most effective at suppressing chaos
even though the power spectrum of the spontaneous activity peaks at zero and
falls exponentially. A prediction of our analysis is that the variance of
neural responses should be most strongly suppressed at frequencies matching the
range over which many sensory systems operate.
</summary>
    <author>
      <name>Kanaka Rajan</name>
    </author>
    <author>
      <name>L F Abbott</name>
    </author>
    <author>
      <name>Haim Sompolinsky</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.82.011903</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.82.011903" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physical Review E 82, 011903 (2010)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0912.3513v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.3513v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.1997v1</id>
    <updated>2010-04-12T16:12:41Z</updated>
    <published>2010-04-12T16:12:41Z</published>
    <title>An optimized recursive learning algorithm for three-layer feedforward
  neural networks for mimo nonlinear system identifications</title>
    <summary>  Back-propagation with gradient method is the most popular learning algorithm
for feed-forward neural networks. However, it is critical to determine a proper
fixed learning rate for the algorithm. In this paper, an optimized recursive
algorithm is presented for online learning based on matrix operation and
optimization methods analytically, which can avoid the trouble to select a
proper learning rate for the gradient method. The proof of weak convergence of
the proposed algorithm also is given. Although this approach is proposed for
three-layer, feed-forward neural networks, it could be extended to multiple
layer feed-forward neural networks. The effectiveness of the proposed
algorithms applied to the identification of behavior of a two-input and
two-output non-linear dynamic system is demonstrated by simulation experiments.
</summary>
    <author>
      <name>Daohang Sha</name>
    </author>
    <author>
      <name>Vladimir B. Bajic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Intelligent Automation and Soft Computing, Vol. 17, No. 2, pp.
  133-147, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.1997v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.1997v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.5022v1</id>
    <updated>2010-07-28T16:12:39Z</updated>
    <published>2010-07-28T16:12:39Z</published>
    <title>Percolation in living neural networks</title>
    <summary>  We study living neural networks by measuring the neurons' response to a
global electrical stimulation. Neural connectivity is lowered by reducing the
synaptic strength, chemically blocking neurotransmitter receptors. We use a
graph-theoretic approach to show that the connectivity undergoes a percolation
transition. This occurs as the giant component disintegrates, characterized by
a power law with critical exponent $\beta \simeq 0.65$ is independent of the
balance between excitatory and inhibitory neurons and indicates that the degree
distribution is gaussian rather than scale free
</summary>
    <author>
      <name>Ilan Breskin</name>
    </author>
    <author>
      <name>Jordi Soriano</name>
    </author>
    <author>
      <name>Elisha Moses</name>
    </author>
    <author>
      <name>Tsvi Tlusty</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.97.188102</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.97.188102" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PACS numbers: 87.18.Sn, 87.19.La, 64.60.Ak
  http://www.weizmann.ac.il/complex/tlusty/papers/PhysRevLett2006.pdf</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Breskin I Soriano J Moses E &amp; Tlusty T Percolation in Living
  Neural Networks Phys Rev Lett 97 188102-4 (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1007.5022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.5022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.4991v1</id>
    <updated>2010-09-25T07:41:52Z</updated>
    <published>2010-09-25T07:41:52Z</published>
    <title>Web Page Categorization Using Artificial Neural Networks</title>
    <summary>  Web page categorization is one of the challenging tasks in the world of ever
increasing web technologies. There are many ways of categorization of web pages
based on different approach and features. This paper proposes a new dimension
in the way of categorization of web pages using artificial neural network (ANN)
through extracting the features automatically. Here eight major categories of
web pages have been selected for categorization; these are business &amp; economy,
education, government, entertainment, sports, news &amp; media, job search, and
science. The whole process of the proposed system is done in three successive
stages. In the first stage, the features are automatically extracted through
analyzing the source of the web pages. The second stage includes fixing the
input values of the neural network; all the values remain between 0 and 1. The
variations in those values affect the output. Finally the third stage
determines the class of a certain web page out of eight predefined classes.
This stage is done using back propagation algorithm of artificial neural
network. The proposed concept will facilitate web mining, retrievals of
information from the web and also the search engines.
</summary>
    <author>
      <name>S. M. Kamruzzaman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 Pages, International Conference</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 4th International Conference on Electrical Engineering, The
  Institution of Engineers, Dhaka, Bangladesh, pp. 96-99, Jan. 2006</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.4991v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.4991v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.3072v1</id>
    <updated>2012-05-14T16:00:32Z</updated>
    <published>2012-05-14T16:00:32Z</published>
    <title>Wandering bumps in stochastic neural fields</title>
    <summary>  We study the effects of noise on stationary pulse solutions (bumps) in
spatially extended neural fields. The dynamics of a neural field is described
by an integrodifferential equation whose integral term characterizes synaptic
interactions between neurons in different spatial locations of the network.
Translationally symmetric neural fields support a continuum of stationary bump
solutions, which may be centered at any spatial location. Random fluctuations
are introduced by modeling the system as a spatially extended Langevin equation
whose noise term we take to be multiplicative or additive. For nonzero noise,
these bumps are shown to wander about the domain in a purely diffusive way. We
can approximate the effective diffusion coefficient using a small noise
expansion. Upon breaking the (continuous) translation symmetry of the system
using a spatially heterogeneous inputs or synapses, bumps in the stochastic
neural field can become temporarily pinned to a finite number of locations in
the network. In the case of spatially heterogeneous synaptic weights, as the
modulation frequency of this heterogeneity increases, the effective diffusion
of bumps in the network approaches that of the network with spatially
homogeneous weights.
</summary>
    <author>
      <name>Zachary P. Kilpatrick</name>
    </author>
    <author>
      <name>Bard Ermentrout</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.3072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.3072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.PS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.PS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.5360v1</id>
    <updated>2012-06-23T05:37:37Z</updated>
    <published>2012-06-23T05:37:37Z</published>
    <title>Analysis of a Nature Inspired Firefly Algorithm based Back-propagation
  Neural Network Training</title>
    <summary>  Optimization algorithms are normally influenced by meta-heuristic approach.
In recent years several hybrid methods for optimization are developed to find
out a better solution. The proposed work using meta-heuristic Nature Inspired
algorithm is applied with back-propagation method to train a feed-forward
neural network. Firefly algorithm is a nature inspired meta-heuristic
algorithm, and it is incorporated into back-propagation algorithm to achieve
fast and improved convergence rate in training feed-forward neural network. The
proposed technique is tested over some standard data set. It is found that
proposed method produces an improved convergence within very few iteration.
This performance is also analyzed and compared to genetic algorithm based
back-propagation. It is observed that proposed method consumes less time to
converge and providing improved convergence rate with minimum feed-forward
neural network design.
</summary>
    <author>
      <name>Sudarshan Nandy</name>
    </author>
    <author>
      <name>Partha Pratim Sarkar</name>
    </author>
    <author>
      <name>Achintya Das</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/6401-8339</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/6401-8339" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 10 figures, Published with International Journal of Computer
  Applications (IJCA)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications 43(22):8-16, April
  2012. Published by Foundation of Computer Science, New York, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1206.5360v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.5360v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.4679v1</id>
    <updated>2014-11-17T21:40:36Z</updated>
    <published>2014-11-17T21:40:36Z</published>
    <title>Pseudo Dynamic Transitional Modeling of Building Heating Energy Demand
  Using Artificial Neural Network</title>
    <summary>  This paper presents the building heating demand prediction model with
occupancy profile and operational heating power level characteristics in short
time horizon (a couple of days) using artificial neural network. In addition,
novel pseudo dynamic transitional model is introduced, which consider time
dependent attributes of operational power level characteristics and its effect
in the overall model performance is outlined. Pseudo dynamic model is applied
to a case study of French Institution building and compared its results with
static and other pseudo dynamic neural network models. The results show the
coefficients of correlation in static and pseudo dynamic neural network model
of 0.82 and 0.89 (with energy consumption error of 0.02%) during the learning
phase, and 0.61 and 0.85 during the prediction phase respectively. Further,
orthogonal array design is applied to the pseudo dynamic model to check the
schedule of occupancy profile and operational heating power level
characteristics. The results show the new schedule and provide the robust
design for pseudo dynamic model. Due to prediction in short time horizon, it
finds application for Energy Services Company (ESCOs) to manage the heating
load for dynamic control of heat production system.
</summary>
    <author>
      <name>S. Paudel</name>
    </author>
    <author>
      <name>M. Elmtiri</name>
    </author>
    <author>
      <name>W. L. Kling</name>
    </author>
    <author>
      <name>O. Le Corre</name>
    </author>
    <author>
      <name>B. Lacarriere</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.enbuild.2013.11.051</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.enbuild.2013.11.051" rel="related"/>
    <link href="http://arxiv.org/abs/1411.4679v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.4679v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.2017v1</id>
    <updated>2013-03-08T15:09:53Z</updated>
    <published>2013-03-08T15:09:53Z</published>
    <title>Security Assessment of Software Design using Neural Network</title>
    <summary>  Security flaws in software applications today has been attributed mostly to
design flaws. With limited budget and time to release software into the market,
many developers often consider security as an afterthought. Previous research
shows that integrating security into software applications at a later stage of
software development lifecycle (SDLC) has been found to be more costly than
when it is integrated during the early stages. To assist in the integration of
security early in the SDLC stages, a new approach for assessing security during
the design phase by neural network is investigated in this paper. Our findings
show that by training a back propagation neural network to identify attack
patterns, possible attacks can be identified from design scenarios presented to
it. The result of performance of the neural network is presented in this paper.
</summary>
    <author>
      <name>A. Adebiyi</name>
    </author>
    <author>
      <name>Johnnes Arreymbi</name>
    </author>
    <author>
      <name>Chris Imafidon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure, 4 tables, (IJARAI) International Journal of
  Advanced Research in Artificial Intelligence, Vol. 1(4), 2012, pp.1-7,
  ISSN:2165-4069 (Online), ISSN:2165-4050 (Print)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">(IJARAI) International Journal of Advanced Research in Artificial
  Intelligence, Vol. 1(4), 2012, pp.1-7</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1303.2017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.2017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.2381v1</id>
    <updated>2014-10-09T08:37:21Z</updated>
    <published>2014-10-09T08:37:21Z</published>
    <title>Recognition of cDNA microarray image Using Feedforward artificial neural
  network</title>
    <summary>  The complementary DNA (cDNA) sequence is considered to be the magic biometric
technique for personal identification. In this paper, we present a new method
for cDNA recognition based on the artificial neural network (ANN). Microarray
imaging is used for the concurrent identification of thousands of genes. We
have segmented the location of the spots in a cDNA microarray. Thus, a precise
localization and segmenting of a spot are essential to obtain a more accurate
intensity measurement, leading to a more precise expression measurement of a
gene. The segmented cDNA microarray image is resized and it is used as an input
for the proposed artificial neural network. For matching and recognition, we
have trained the artificial neural network. Recognition results are given for
the galleries of cDNA sequences . The numerical results show that, the proposed
matching technique is an effective in the cDNA sequences process. We also
compare our results with previous results and find out that, the proposed
technique is an effective matching performance.
</summary>
    <author>
      <name>R. M. Farouk</name>
    </author>
    <author>
      <name>S. Badr</name>
    </author>
    <author>
      <name>M. Sayed Elahl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 7 figures and 23 References</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Artificial Intelligence &amp; Applications
  (IJAIA), Vol. 5, No. 5, September 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1410.2381v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.2381v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.03562v3</id>
    <updated>2015-03-22T21:47:56Z</updated>
    <published>2015-03-12T02:24:31Z</published>
    <title>Training Binary Multilayer Neural Networks for Image Classification
  using Expectation Backpropagation</title>
    <summary>  Compared to Multilayer Neural Networks with real weights, Binary Multilayer
Neural Networks (BMNNs) can be implemented more efficiently on dedicated
hardware. BMNNs have been demonstrated to be effective on binary classification
tasks with Expectation BackPropagation (EBP) algorithm on high dimensional text
datasets. In this paper, we investigate the capability of BMNNs using the EBP
algorithm on multiclass image classification tasks. The performances of binary
neural networks with multiple hidden layers and different numbers of hidden
units are examined on MNIST. We also explore the effectiveness of image spatial
filters and the dropout technique in BMNNs. Experimental results on MNIST
dataset show that EBP can obtain 2.12% test error with binary weights and 1.66%
test error with real weights, which is comparable to the results of standard
BackPropagation algorithm on fully connected MNNs.
</summary>
    <author>
      <name>Zhiyong Cheng</name>
    </author>
    <author>
      <name>Daniel Soudry</name>
    </author>
    <author>
      <name>Zexi Mao</name>
    </author>
    <author>
      <name>Zhenzhong Lan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages with 1 figures and 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.03562v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.03562v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.2412v3</id>
    <updated>2010-12-07T03:51:14Z</updated>
    <published>2010-08-14T03:35:05Z</published>
    <title>Discover &amp; eXplore Neural Network (DXNN) Platform, a Modular TWEANN</title>
    <summary>  In this paper I present a novel type of Topology and Weight Evolving
Artificial Neural Network (TWEANN) system called Modular Discover &amp; eXplore
Neural Network (DXNN). Modular DXNN utilizes a hierarchical/modular topology
which allows for highly scalable and dynamically granular systems to evolve.
Among the novel features discussed in this paper is a simple and database
friendly encoding for hierarchical/modular NNs, a new selection method aimed at
producing highly compact and fit individuals within the population, a "Targeted
Tunning" system aimed at alleviating the curse of dimensionality, and a two
phase based neuroevolutionary approach which yields high population diversity
and removes the need for speciation algorithms. I will discuss DXNN's mutation
operators which are aimed at improving its efficiency, expandability, and
capabilities through a built in feature selection method that allows for the
evolved system to expand, discover, and explore new sensors and actuators.
Finally I will compare DXNN platform to other state of the art TWEANNs on a
control task to demonstrate its superior ability to produce highly compact
solutions faster than its competitors.
</summary>
    <author>
      <name>Gene I. Sher</name>
    </author>
    <link href="http://arxiv.org/abs/1008.2412v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.2412v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.3600v1</id>
    <updated>2012-08-17T13:59:30Z</updated>
    <published>2012-08-17T13:59:30Z</published>
    <title>Modeling and Control of CSTR using Model based Neural Network Predictive
  Control</title>
    <summary>  This paper presents a predictive control strategy based on neural network
model of the plant is applied to Continuous Stirred Tank Reactor (CSTR). This
system is a highly nonlinear process; therefore, a nonlinear predictive method,
e.g., neural network predictive control, can be a better match to govern the
system dynamics. In the paper, the NN model and the way in which it can be used
to predict the behavior of the CSTR process over a certain prediction horizon
are described, and some comments about the optimization procedure are made.
Predictive control algorithm is applied to control the concentration in a
continuous stirred tank reactor (CSTR), whose parameters are optimally
determined by solving quadratic performance index using the optimization
algorithm. An efficient control of the product concentration in cstr can be
achieved only through accurate model. Here an attempt is made to alleviate the
modeling difficulties using Artificial Intelligent technique such as Neural
Network. Simulation results demonstrate the feasibility and effectiveness of
the NNMPC technique.
</summary>
    <author>
      <name>Piyush Shrivastava</name>
    </author>
    <link href="http://arxiv.org/abs/1208.3600v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.3600v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.2548v1</id>
    <updated>2012-09-12T10:25:51Z</updated>
    <published>2012-09-12T10:25:51Z</published>
    <title>Training a Feed-forward Neural Network with Artificial Bee Colony Based
  Backpropagation Method</title>
    <summary>  Back-propagation algorithm is one of the most widely used and popular
techniques to optimize the feed forward neural network training. Nature
inspired meta-heuristic algorithms also provide derivative-free solution to
optimize complex problem. Artificial bee colony algorithm is a nature inspired
meta-heuristic algorithm, mimicking the foraging or food source searching
behaviour of bees in a bee colony and this algorithm is implemented in several
applications for an improved optimized outcome. The proposed method in this
paper includes an improved artificial bee colony algorithm based
back-propagation neural network training method for fast and improved
convergence rate of the hybrid neural network learning method. The result is
analysed with the genetic algorithm based back-propagation method, and it is
another hybridized procedure of its kind. Analysis is performed over standard
data sets, reflecting the light of efficiency of proposed method in terms of
convergence speed and rate.
</summary>
    <author>
      <name>Sudarshan Nandy</name>
    </author>
    <author>
      <name>Partha Pratim Sarkar</name>
    </author>
    <author>
      <name>Achintya Das</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcsit.2012.4404</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcsit.2012.4404" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 Pages, 11 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 4, No 4, 2012, 33-46</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1209.2548v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.2548v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.3524v1</id>
    <updated>2013-08-15T23:42:02Z</updated>
    <published>2013-08-15T23:42:02Z</published>
    <title>Innovative Second-Generation Wavelets Construction With Recurrent Neural
  Networks for Solar Radiation Forecasting</title>
    <summary>  Solar radiation prediction is an important challenge for the electrical
engineer because it is used to estimate the power developed by commercial
photovoltaic modules. This paper deals with the problem of solar radiation
prediction based on observed meteorological data. A 2-day forecast is obtained
by using novel wavelet recurrent neural networks (WRNNs). In fact, these WRNNS
are used to exploit the correlation between solar radiation and
timescale-related variations of wind speed, humidity, and temperature. The
input to the selected WRNN is provided by timescale-related bands of wavelet
coefficients obtained from meteorological time series. The experimental setup
available at the University of Catania, Italy, provided this information. The
novelty of this approach is that the proposed WRNN performs the prediction in
the wavelet domain and, in addition, also performs the inverse wavelet
transform, giving the predicted signal as output. The obtained simulation
results show a very low root-mean-square error compared to the results of the
solar radiation prediction approaches obtained by hybrid neural networks
reported in the recent literature.
</summary>
    <author>
      <name>Giacomo Capizzi</name>
    </author>
    <author>
      <name>Christian Napoli</name>
    </author>
    <author>
      <name>Francesco Bonanno</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TNNLS.2012.2216546</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TNNLS.2012.2216546" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Trans. Neural Networks and Learning Systems, vol. 23, N. 11,
  pp. 1805-1815, 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1308.3524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.3524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.1128v1</id>
    <updated>2014-02-05T19:01:51Z</updated>
    <published>2014-02-05T19:01:51Z</published>
    <title>Long Short-Term Memory Based Recurrent Neural Network Architectures for
  Large Vocabulary Speech Recognition</title>
    <summary>  Long Short-Term Memory (LSTM) is a recurrent neural network (RNN)
architecture that has been designed to address the vanishing and exploding
gradient problems of conventional RNNs. Unlike feedforward neural networks,
RNNs have cyclic connections making them powerful for modeling sequences. They
have been successfully used for sequence labeling and sequence prediction
tasks, such as handwriting recognition, language modeling, phonetic labeling of
acoustic frames. However, in contrast to the deep neural networks, the use of
RNNs in speech recognition has been limited to phone recognition in small scale
tasks. In this paper, we present novel LSTM based RNN architectures which make
more effective use of model parameters to train acoustic models for large
vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at
various numbers of parameters and configurations. We show that LSTM models
converge quickly and give state of the art speech recognition performance for
relatively small sized models.
</summary>
    <author>
      <name>Ha≈üim Sak</name>
    </author>
    <author>
      <name>Andrew Senior</name>
    </author>
    <author>
      <name>Fran√ßoise Beaufays</name>
    </author>
    <link href="http://arxiv.org/abs/1402.1128v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.1128v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.4446v2</id>
    <updated>2015-02-09T17:52:03Z</updated>
    <published>2014-12-15T02:16:07Z</published>
    <title>Domain-Adversarial Neural Networks</title>
    <summary>  We introduce a new representation learning algorithm suited to the context of
domain adaptation, in which data at training and test time come from similar
but different distributions. Our algorithm is directly inspired by theory on
domain adaptation suggesting that, for effective domain transfer to be
achieved, predictions must be made based on a data representation that cannot
discriminate between the training (source) and test (target) domains. We
propose a training objective that implements this idea in the context of a
neural network, whose hidden layer is trained to be predictive of the
classification task, but uninformative as to the domain of the input. Our
experiments on a sentiment analysis classification benchmark, where the target
domain data available at training time is unlabeled, show that our neural
network for domain adaption algorithm has better performance than either a
standard neural network or an SVM, even if trained on input features extracted
with the state-of-the-art marginalized stacked denoising autoencoders of Chen
et al. (2012).
</summary>
    <author>
      <name>Hana Ajakan</name>
    </author>
    <author>
      <name>Pascal Germain</name>
    </author>
    <author>
      <name>Hugo Larochelle</name>
    </author>
    <author>
      <name>Fran√ßois Laviolette</name>
    </author>
    <author>
      <name>Mario Marchand</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The first version of this paper was accepted at the "Second Workshop
  on Transfer and Multi-Task Learning: Theory meets Practice" (NIPS 2014,
  Montreal, Canada). See: https://sites.google.com/site/multitaskwsnips2014/</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.4446v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.4446v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6630v2</id>
    <updated>2015-01-05T13:28:46Z</updated>
    <published>2014-12-20T07:59:14Z</published>
    <title>Neural Network Regularization via Robust Weight Factorization</title>
    <summary>  Regularization is essential when training large neural networks. As deep
neural networks can be mathematically interpreted as universal function
approximators, they are effective at memorizing sampling noise in the training
data. This results in poor generalization to unseen data. Therefore, it is no
surprise that a new regularization technique, Dropout, was partially
responsible for the now-ubiquitous winning entry to ImageNet 2012 by the
University of Toronto. Currently, Dropout (and related methods such as
DropConnect) are the most effective means of regularizing large neural
networks. These amount to efficiently visiting a large number of related models
at training time, while aggregating them to a single predictor at test time.
The proposed FaMe model aims to apply a similar strategy, yet learns a
factorization of each weight matrix such that the factors are robust to noise.
</summary>
    <author>
      <name>Jan Rudy</name>
    </author>
    <author>
      <name>Weiguang Ding</name>
    </author>
    <author>
      <name>Daniel Jiwoong Im</name>
    </author>
    <author>
      <name>Graham W. Taylor</name>
    </author>
    <link href="http://arxiv.org/abs/1412.6630v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.6630v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.03581v1</id>
    <updated>2015-02-12T09:58:23Z</updated>
    <published>2015-02-12T09:58:23Z</published>
    <title>Web spam classification using supervised artificial neural network
  algorithms</title>
    <summary>  Due to the rapid growth in technology employed by the spammers, there is a
need of classifiers that are more efficient, generic and highly adaptive.
Neural Network based technologies have high ability of adaption as well as
generalization. As per our knowledge, very little work has been done in this
field using neural network. We present this paper to fill this gap. This paper
evaluates performance of three supervised learning algorithms of artificial
neural network by creating classifiers for the complex problem of latest web
spam pattern classification. These algorithms are Conjugate Gradient algorithm,
Resilient Backpropagation learning, and Levenberg-Marquardt algorithm.
</summary>
    <author>
      <name>Ashish Chandra</name>
    </author>
    <author>
      <name>Mohammad Suaib</name>
    </author>
    <author>
      <name>Dr. Rizwan Beg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 Pages in Advanced Computational Intelligence: An International
  Journal (ACII), Vol.2, No.1, January 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.03581v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.03581v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.05767v1</id>
    <updated>2015-04-22T12:47:32Z</updated>
    <published>2015-04-22T12:47:32Z</published>
    <title>Rounding Methods for Neural Networks with Low Resolution Synaptic
  Weights</title>
    <summary>  Neural network algorithms simulated on standard computing platforms typically
make use of high resolution weights, with floating-point notation. However, for
dedicated hardware implementations of such algorithms, fixed-point synaptic
weights with low resolution are preferable. The basic approach of reducing the
resolution of the weights in these algorithms by standard rounding methods
incurs drastic losses in performance. To reduce the resolution further, in the
extreme case even to binary weights, more advanced techniques are necessary. To
this end, we propose two methods for mapping neural network algorithms with
high resolution weights to corresponding algorithms that work with low
resolution weights and demonstrate that their performance is substantially
better than standard rounding. We further use these methods to investigate the
performance of three common neural network algorithms under fixed memory size
of the weight matrix with different weight resolutions. We show that dedicated
hardware systems, whose technology dictates very low weight resolutions (be
they electronic or biological) could in principle implement the algorithms we
study.
</summary>
    <author>
      <name>Lorenz K. Muller</name>
    </author>
    <author>
      <name>Giacomo Indiveri</name>
    </author>
    <link href="http://arxiv.org/abs/1504.05767v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.05767v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.00074v1</id>
    <updated>2015-05-30T05:38:00Z</updated>
    <published>2015-05-30T05:38:00Z</published>
    <title>Recognition of convolutional neural network based on CUDA Technology</title>
    <summary>  For the problem whether Graphic Processing Unit(GPU),the stream processor
with high performance of floating-point computing is applicable to neural
networks, this paper proposes the parallel recognition algorithm of
Convolutional Neural Networks(CNNs).It adopts Compute Unified Device
Architecture(CUDA)technology, definite the parallel data structures, and
describes the mapping mechanism for computing tasks on CUDA. It compares the
parallel recognition algorithm achieved on GPU of GTX200 hardware architecture
with the serial algorithm on CPU. It improves speed by nearly 60 times. Result
shows that GPU based the stream processor architecture ate more applicable to
some related applications about neural networks than CPU.
</summary>
    <author>
      <name>Yi-bin Huang</name>
    </author>
    <author>
      <name>Kang Li</name>
    </author>
    <author>
      <name>Ge Wang</name>
    </author>
    <author>
      <name>Min Cao</name>
    </author>
    <author>
      <name>Pin Li</name>
    </author>
    <author>
      <name>Yu-jia Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures and 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.00074v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.00074v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.01195v2</id>
    <updated>2015-06-04T02:10:39Z</updated>
    <published>2015-06-03T10:18:49Z</published>
    <title>Implementation of Training Convolutional Neural Networks</title>
    <summary>  Deep learning refers to the shining branch of machine learning that is based
on learning levels of representations. Convolutional Neural Networks (CNN) is
one kind of deep neural network. It can study concurrently. In this article, we
gave a detailed analysis of the process of CNN algorithm both the forward
process and back propagation. Then we applied the particular convolutional
neural network to implement the typical face recognition problem by java. Then,
a parallel strategy was proposed in section4. In addition, by measuring the
actual time of forward and backward computing, we analysed the maximal speed up
and parallel efficiency theoretically.
</summary>
    <author>
      <name>Tianyi Liu</name>
    </author>
    <author>
      <name>Shuangsang Fang</name>
    </author>
    <author>
      <name>Yuehui Zhao</name>
    </author>
    <author>
      <name>Peng Wang</name>
    </author>
    <author>
      <name>Jun Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.01195v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.01195v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.03881v1</id>
    <updated>2015-07-14T15:15:16Z</updated>
    <published>2015-07-14T15:15:16Z</published>
    <title>Statistical physics of neural systems with non-additive dendritic
  coupling</title>
    <summary>  How neurons process their inputs crucially determines the dynamics of
biological and artificial neural networks. In such neural and neural-like
systems, synaptic input is typically considered to be merely transmitted
linearly or sublinearly by the dendritic compartments. Yet, single-neuron
experiments report pronounced supralinear dendritic summation of sufficiently
synchronous and spatially close-by inputs. Here, we provide a statistical
physics approach to study the impact of such non-additive dendritic processing
on single neuron responses and the performance of associative memory tasks in
artificial neural networks. First, we compute the effect of random input to a
neuron incorporating nonlinear dendrites. This approach is independent of the
details of the neuronal dynamics. Second, we use those results to study the
impact of dendritic nonlinearities on the network dynamics in a paradigmatic
model for associative memory, both numerically and analytically. We find that
dendritic nonlinearities maintain network convergence and increase the
robustness of memory performance against noise. Interestingly, an intermediate
number of dendritic branches is optimal for memory functionality.
</summary>
    <author>
      <name>David Breuer</name>
    </author>
    <author>
      <name>Marc Timme</name>
    </author>
    <author>
      <name>Raoul-Martin Memmesheimer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevX.4.011053</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevX.4.011053" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys Rev X, 2014, 4(1):011053</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1507.03881v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.03881v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.04646v1</id>
    <updated>2015-07-16T16:43:55Z</updated>
    <published>2015-07-16T16:43:55Z</published>
    <title>A Dependency-Based Neural Network for Relation Classification</title>
    <summary>  Previous research on relation classification has verified the effectiveness
of using dependency shortest paths or subtrees. In this paper, we further
explore how to make full use of the combination of these dependency
information. We first propose a new structure, termed augmented dependency path
(ADP), which is composed of the shortest dependency path between two entities
and the subtrees attached to the shortest path. To exploit the semantic
representation behind the ADP structure, we develop dependency-based neural
networks (DepNN): a recursive neural network designed to model the subtrees,
and a convolutional neural network to capture the most important features on
the shortest path. Experiments on the SemEval-2010 dataset show that our
proposed method achieves state-of-art results.
</summary>
    <author>
      <name>Yang Liu</name>
    </author>
    <author>
      <name>Furu Wei</name>
    </author>
    <author>
      <name>Sujian Li</name>
    </author>
    <author>
      <name>Heng Ji</name>
    </author>
    <author>
      <name>Ming Zhou</name>
    </author>
    <author>
      <name>Houfeng Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This preprint is the full version of a short paper accepted in the
  annual meeting of the Association for Computational Linguistics (ACL) 2015
  (Beijing, China)</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.04646v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.04646v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.00174v1</id>
    <updated>2015-09-01T08:22:33Z</updated>
    <published>2015-09-01T08:22:33Z</published>
    <title>A Telescopic Binary Learning Machine for Training Neural Networks</title>
    <summary>  This paper proposes a new algorithm based on multi-scale stochastic local
search with binary representation for training neural networks.
  In particular, we study the effects of neighborhood evaluation strategies,
the effect of the number of bits per weight and that of the maximum weight
range used for mapping binary strings to real values. Following this
preliminary investigation, we propose a telescopic multi-scale version of local
search where the number of bits is increased in an adaptive manner, leading to
a faster search and to local minima of better quality. An analysis related to
adapting the number of bits in a dynamic way is also presented. The control on
the number of bits, which happens in a natural manner in the proposed method,
is effective to increase the generalization performance. Benchmark tasks
include a highly non-linear artificial problem, a control problem requiring
either feed-forward or recurrent architectures for feedback control, and
challenging real-world tasks in different application domains.
  The results demonstrate the effectiveness of the proposed method.
</summary>
    <author>
      <name>Mauro Brunato</name>
    </author>
    <author>
      <name>Roberto Battiti</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TNNLS.2016.2537300</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TNNLS.2016.2537300" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE Transactions on Neural Networks and Learning
  Systems, special issue on New Developments in Neural Network Structures for
  Signal Processing, Autonomous Decision, and Adaptive Control</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.00174v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.00174v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.01042v2</id>
    <updated>2015-11-16T03:54:19Z</updated>
    <published>2015-11-03T19:26:16Z</published>
    <title>Detecting Interrogative Utterances with Recurrent Neural Networks</title>
    <summary>  In this paper, we explore different neural network architectures that can
predict if a speaker of a given utterance is asking a question or making a
statement. We com- pare the outcomes of regularization methods that are
popularly used to train deep neural networks and study how different context
functions can affect the classification performance. We also compare the
efficacy of gated activation functions that are favorably used in recurrent
neural networks and study how to combine multimodal inputs. We evaluate our
models on two multimodal datasets: MSR-Skype and CALLHOME.
</summary>
    <author>
      <name>Junyoung Chung</name>
    </author>
    <author>
      <name>Jacob Devlin</name>
    </author>
    <author>
      <name>Hany Hassan Awadalla</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, accepted to NIPS 2015 Workshop on Machine Learning for
  Spoken Language Understanding and Interaction</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.01042v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.01042v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.03984v1</id>
    <updated>2015-11-12T17:40:42Z</updated>
    <published>2015-11-12T17:40:42Z</published>
    <title>Prediction of the Yield of Enzymatic Synthesis of Betulinic Acid Ester
  Using Artificial Neural Networks and Support Vector Machine</title>
    <summary>  3\b{eta}-O-phthalic ester of betulinic acid is of great importance in
anticancer studies. However, the optimization of its reaction conditions
requires a large number of experimental works. To simplify the number of times
of optimization in experimental works, here, we use artificial neural network
(ANN) and support vector machine (SVM) models for the prediction of yields of
3\b{eta}-O-phthalic ester of betulinic acid synthesized by betulinic acid and
phthalic anhydride using lipase as biocatalyst. General regression neural
network (GRNN), multilayer feed-forward neural network (MLFN) and the SVM
models were trained based on experimental data. Four indicators were set as
independent variables, including time (h), temperature (C), amount of enzyme
(mg) and molar ratio, while the yield of the 3\b{eta}-O-phthalic ester of
betulinic acid was set as the dependent variable. Results show that the GRNN
and SVM models have the best prediction results during the testing process,
with comparatively low RMS errors (4.01 and 4.23respectively) and short
training times (both 1s). The prediction accuracy of the GRNN and SVM are both
100% in testing process, under the tolerance of 30%.
</summary>
    <author>
      <name>Run Wang</name>
    </author>
    <author>
      <name>Qiaoli Mo</name>
    </author>
    <author>
      <name>Qian Zhang</name>
    </author>
    <author>
      <name>Fudi Chen</name>
    </author>
    <author>
      <name>Dazuo Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 11 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.03984v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.03984v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.00242v1</id>
    <updated>2015-12-01T12:46:11Z</updated>
    <published>2015-12-01T12:46:11Z</published>
    <title>Towards Dropout Training for Convolutional Neural Networks</title>
    <summary>  Recently, dropout has seen increasing use in deep learning. For deep
convolutional neural networks, dropout is known to work well in fully-connected
layers. However, its effect in convolutional and pooling layers is still not
clear. This paper demonstrates that max-pooling dropout is equivalent to
randomly picking activation based on a multinomial distribution at training
time. In light of this insight, we advocate employing our proposed
probabilistic weighted pooling, instead of commonly used max-pooling, to act as
model averaging at test time. Empirical evidence validates the superiority of
probabilistic weighted pooling. We also empirically show that the effect of
convolutional dropout is not trivial, despite the dramatically reduced
possibility of over-fitting due to the convolutional architecture. Elaborately
designing dropout training simultaneously in max-pooling and fully-connected
layers, we achieve state-of-the-art performance on MNIST, and very competitive
results on CIFAR-10 and CIFAR-100, relative to other approaches without data
augmentation. Finally, we compare max-pooling dropout and stochastic pooling,
both of which introduce stochasticity based on multinomial distributions at
pooling stage.
</summary>
    <author>
      <name>Haibing Wu</name>
    </author>
    <author>
      <name>Xiaodong Gu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neunet.2015.07.007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neunet.2015.07.007" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been published in Neural Networks,
  http://www.sciencedirect.com/science/article/pii/S0893608015001446</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Networks 71: 1-10 (2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1512.00242v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.00242v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.00013v2</id>
    <updated>2016-05-20T12:28:37Z</updated>
    <published>2015-12-31T21:32:58Z</published>
    <title>A single hidden layer feedforward network with only one neuron in the
  hidden layer can approximate any univariate function</title>
    <summary>  The possibility of approximating a continuous function on a compact subset of
the real line by a feedforward single hidden layer neural network with a
sigmoidal activation function has been studied in many papers. Such networks
can approximate an arbitrary continuous function provided that an unlimited
number of neurons in a hidden layer is permitted. In this paper, we consider
constructive approximation on any finite interval of $\mathbb{R}$ by neural
networks with only one neuron in the hidden layer. We construct algorithmically
a smooth, sigmoidal, almost monotone activation function $\sigma$ providing
approximation to an arbitrary continuous function within any degree of
accuracy. This algorithm is implemented in a computer program, which computes
the value of $\sigma$ at any reasonable point of the real axis.
</summary>
    <author>
      <name>Namig J. Guliyev</name>
    </author>
    <author>
      <name>Vugar E. Ismailov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/NECO_a_00849</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/NECO_a_00849" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 1 figure; to be published in Neural Computation; for
  associated SageMath worksheet, see
  http://sites.google.com/site/njguliyev/papers/sigmoidal</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Computation, 28 (2016), no. 7, 1289-1304</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1601.00013v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.00013v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="41A30, 65D15, 92B20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.03277v1</id>
    <updated>2016-01-12T12:53:04Z</updated>
    <published>2016-01-12T12:53:04Z</published>
    <title>Weightless neural network parameters and architecture selection in a
  quantum computer</title>
    <summary>  Training artificial neural networks requires a tedious empirical evaluation
to determine a suitable neural network architecture. To avoid this empirical
process several techniques have been proposed to automatise the architecture
selection process. In this paper, we propose a method to perform parameter and
architecture selection for a quantum weightless neural network (qWNN). The
architecture selection is performed through the learning procedure of a qWNN
with a learning algorithm that uses the principle of quantum superposition and
a non-linear quantum operator. The main advantage of the proposed method is
that it performs a global search in the space of qWNN architecture and
parameters rather than a local search.
</summary>
    <author>
      <name>Adenilton J. da Silva</name>
    </author>
    <author>
      <name>Wilson R. de Oliveira</name>
    </author>
    <author>
      <name>Teresa B. Ludermir</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neucom.2015.05.139</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neucom.2015.05.139" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neurocomputing, Volume 183, 26 March 2016, Pages 13-22</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1601.03277v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.03277v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.04296v1</id>
    <updated>2016-01-17T13:56:38Z</updated>
    <published>2016-01-17T13:56:38Z</published>
    <title>Building a Learning Database for the Neural Network Retrieval of Sea
  Surface Salinity from SMOS Brightness Temperatures</title>
    <summary>  This article deals with an important aspect of the neural network retrieval
of sea surface salinity (SSS) from SMOS brightness temperatures (TBs). The
neural network retrieval method is an empirical approach that offers the
possibility of being independent from any theoretical emissivity model, during
the in-flight phase. A Previous study [1] has proven that this approach is
applicable to all pixels on ocean, by designing a set of neural networks with
different inputs. The present study focuses on the choice of the learning
database and demonstrates that a judicious distribution of the geophysical
parameters allows to markedly reduce the systematic regional biases of the
retrieved SSS, which are due to the high noise on the TBs. An equalization of
the distribution of the geophysical parameters, followed by a new technique for
boosting the learning process, makes the regional biases almost disappear for
latitudes between 40{\deg}S and 40{\deg}N, while the global standard deviation
remains between 0.6 psu (at the center of the of the swath) and 1 psu (at the
edges).
</summary>
    <author>
      <name>Adel Ammar</name>
    </author>
    <author>
      <name>Sylvie Labroue</name>
    </author>
    <author>
      <name>Estelle Obligis</name>
    </author>
    <author>
      <name>Michel Cr√©pon</name>
    </author>
    <author>
      <name>Sylvie Thiria</name>
    </author>
    <link href="http://arxiv.org/abs/1601.04296v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.04296v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ao-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06318v4</id>
    <updated>2016-11-15T21:41:21Z</updated>
    <published>2016-03-21T03:33:20Z</published>
    <title>Harnessing Deep Neural Networks with Logic Rules</title>
    <summary>  Combining deep neural networks with structured logic rules is desirable to
harness flexibility and reduce uninterpretability of the neural models. We
propose a general framework capable of enhancing various types of neural
networks (e.g., CNNs and RNNs) with declarative first-order logic rules.
Specifically, we develop an iterative distillation method that transfers the
structured information of logic rules into the weights of neural networks. We
deploy the framework on a CNN for sentiment analysis, and an RNN for named
entity recognition. With a few highly intuitive rules, we obtain substantial
improvements and achieve state-of-the-art or comparable results to previous
best-performing systems.
</summary>
    <author>
      <name>Zhiting Hu</name>
    </author>
    <author>
      <name>Xuezhe Ma</name>
    </author>
    <author>
      <name>Zhengzhong Liu</name>
    </author>
    <author>
      <name>Eduard Hovy</name>
    </author>
    <author>
      <name>Eric Xing</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fix typos and experiment setting</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.06318v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06318v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06751v1</id>
    <updated>2016-04-22T17:06:31Z</updated>
    <published>2016-04-22T17:06:31Z</published>
    <title>evt_MNIST: A spike based version of traditional MNIST</title>
    <summary>  Benchmarks and datasets have important role in evaluation of machine learning
algorithms and neural network implementations. Traditional dataset for images
such as MNIST is applied to evaluate efficiency of different training
algorithms in neural networks. This demand is different in Spiking Neural
Networks (SNN) as they require spiking inputs. It is widely believed, in the
biological cortex the timing of spikes is irregular. Poisson distributions
provide adequate descriptions of the irregularity in generating appropriate
spikes. Here, we introduce a spike-based version of MNSIT (handwritten digits
dataset),using Poisson distribution and show the Poissonian property of the
generated streams. We introduce a new version of evt_MNIST which can be used
for neural network evaluation.
</summary>
    <author>
      <name>Mazdak Fatahi</name>
    </author>
    <author>
      <name>Mahmood Ahmadi</name>
    </author>
    <author>
      <name>Mahyar Shahsavari</name>
    </author>
    <author>
      <name>Arash Ahmadi</name>
    </author>
    <author>
      <name>Philippe Devienne</name>
    </author>
    <link href="http://arxiv.org/abs/1604.06751v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06751v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.09470v1</id>
    <updated>2016-06-30T13:05:03Z</updated>
    <published>2016-06-30T13:05:03Z</published>
    <title>Programming Patterns in Dataflow Matrix Machines and Generalized
  Recurrent Neural Nets</title>
    <summary>  Dataflow matrix machines arise naturally in the context of synchronous
dataflow programming with linear streams. They can be viewed as a rather
powerful generalization of recurrent neural networks. Similarly to recurrent
neural networks, large classes of dataflow matrix machines are described by
matrices of numbers, and therefore dataflow matrix machines can be synthesized
by computing their matrices. At the same time, the evidence is fairly strong
that dataflow matrix machines have sufficient expressive power to be a
convenient general-purpose programming platform. Because of the network nature
of this platform, programming patterns often correspond to patterns of
connectivity in the generalized recurrent neural networks understood as
programs. This paper explores a variety of such programming patterns.
</summary>
    <author>
      <name>Michael Bukatin</name>
    </author>
    <author>
      <name>Steve Matthews</name>
    </author>
    <author>
      <name>Andrey Radul</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.09470v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.09470v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04243v3</id>
    <updated>2016-12-21T06:52:30Z</updated>
    <published>2016-09-14T12:52:08Z</published>
    <title>Convolutional Recurrent Neural Networks for Music Classification</title>
    <summary>  We introduce a convolutional recurrent neural network (CRNN) for music
tagging. CRNNs take advantage of convolutional neural networks (CNNs) for local
feature extraction and recurrent neural networks for temporal summarisation of
the extracted features. We compare CRNN with three CNN structures that have
been used for music tagging while controlling the number of parameters with
respect to their performance and training time per sample. Overall, we found
that CRNNs show a strong performance with respect to the number of parameter
and training time, indicating the effectiveness of its hybrid structure in
music feature extraction and feature summarisation.
</summary>
    <author>
      <name>Keunwoo Choi</name>
    </author>
    <author>
      <name>George Fazekas</name>
    </author>
    <author>
      <name>Mark Sandler</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, ICASSP 2017 submitted. Revised to fix previous CNN
  architectures and update experiment results</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.04243v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04243v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.05884v2</id>
    <updated>2018-02-02T09:39:52Z</updated>
    <published>2016-09-19T19:47:52Z</published>
    <title>A Quantum Implementation Model for Artificial Neural Networks</title>
    <summary>  The learning process for multi layered neural networks with many nodes makes
heavy demands on computational resources. In some neural network models, the
learning formulas, such as the Widrow-Hoff formula, do not change the
eigenvectors of the weight matrix while flatting the eigenvalues. In infinity,
this iterative formulas result in terms formed by the principal components of
the weight matrix: i.e., the eigenvectors corresponding to the non-zero
eigenvalues. In quantum computing, the phase estimation algorithm is known to
provide speed-ups over the conventional algorithms for the eigenvalue-related
problems. Combining the quantum amplitude amplification with the phase
estimation algorithm, a quantum implementation model for artificial neural
networks using the Widrow-Hoff learning rule is presented. The complexity of
the model is found to be linear in the size of the weight matrix. This provides
a quadratic improvement over the classical algorithms.
</summary>
    <author>
      <name>Ammar Daskin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.12743/quanta.v7i1.65</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.12743/quanta.v7i1.65" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Quanta, 7, pg: 7-18, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1609.05884v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.05884v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.01251v2</id>
    <updated>2016-12-23T00:24:27Z</updated>
    <published>2016-12-05T05:21:42Z</published>
    <title>Known Unknowns: Uncertainty Quality in Bayesian Neural Networks</title>
    <summary>  We evaluate the uncertainty quality in neural networks using anomaly
detection. We extract uncertainty measures (e.g. entropy) from the predictions
of candidate models, use those measures as features for an anomaly detector,
and gauge how well the detector differentiates known from unknown classes. We
assign higher uncertainty quality to candidate models that lead to better
detectors. We also propose a novel method for sampling a variational
approximation of a Bayesian neural network, called One-Sample Bayesian
Approximation (OSBA). We experiment on two datasets, MNIST and CIFAR10. We
compare the following candidate neural network models: Maximum Likelihood,
Bayesian Dropout, OSBA, and --- for MNIST --- the standard variational
approximation. We show that Bayesian Dropout and OSBA provide better
uncertainty information than Maximum Likelihood, and are essentially equivalent
to the standard variational approximation, but much faster.
</summary>
    <author>
      <name>Ramon Oliveira</name>
    </author>
    <author>
      <name>Pedro Tabacof</name>
    </author>
    <author>
      <name>Eduardo Valle</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Workshop on Bayesian Deep Learning, NIPS 2016, Barcelona, Spain;
  EDIT: Changed analysis from Logit-AUC space to AUC (with changes to Figs. 2
  and 3)</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.01251v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.01251v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.09506v2</id>
    <updated>2017-07-14T09:03:57Z</updated>
    <published>2016-12-30T15:18:39Z</published>
    <title>Smart Content Recognition from Images Using a Mixture of Convolutional
  Neural Networks</title>
    <summary>  With rapid development of the Internet, web contents become huge. Most of the
websites are publicly available, and anyone can access the contents from
anywhere such as workplace, home and even schools. Nevertheless, not all the
web contents are appropriate for all users, especially children. An example of
these contents is pornography images which should be restricted to certain age
group. Besides, these images are not safe for work (NSFW) in which employees
should not be seen accessing such contents during work. Recently, convolutional
neural networks have been successfully applied to many computer vision
problems. Inspired by these successes, we propose a mixture of convolutional
neural networks for adult content recognition. Unlike other works, our method
is formulated on a weighted sum of multiple deep neural network models. The
weights of each CNN models are expressed as a linear regression problem learned
using Ordinary Least Squares (OLS). Experimental results demonstrate that the
proposed model outperforms both single CNN model and the average sum of CNN
models in adult content recognition.
</summary>
    <author>
      <name>Tee Connie</name>
    </author>
    <author>
      <name>Mundher Al-Shabi</name>
    </author>
    <author>
      <name>Michael Goh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in LNEE, Code: github.com/mundher/NSFW</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.09506v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.09506v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.03443v2</id>
    <updated>2017-06-17T18:29:37Z</updated>
    <published>2017-02-11T17:34:34Z</published>
    <title>Group Scissor: Scaling Neuromorphic Computing Design to Large Neural
  Networks</title>
    <summary>  Synapse crossbar is an elementary structure in Neuromorphic Computing Systems
(NCS). However, the limited size of crossbars and heavy routing congestion
impedes the NCS implementations of big neural networks. In this paper, we
propose a two-step framework (namely, group scissor) to scale NCS designs to
big neural networks. The first step is rank clipping, which integrates low-rank
approximation into the training to reduce total crossbar area. The second step
is group connection deletion, which structurally prunes connections to reduce
routing congestion between crossbars. Tested on convolutional neural networks
of LeNet on MNIST database and ConvNet on CIFAR-10 database, our experiments
show significant reduction of crossbar area and routing area in NCS designs.
Without accuracy loss, rank clipping reduces total crossbar area to 13.62\% and
51.81\% in the NCS designs of LeNet and ConvNet, respectively. Following rank
clipping, group connection deletion further reduces the routing area of LeNet
and ConvNet to 8.1\% and 52.06\%, respectively.
</summary>
    <author>
      <name>Yandan Wang</name>
    </author>
    <author>
      <name>Wei Wen</name>
    </author>
    <author>
      <name>Beiye Liu</name>
    </author>
    <author>
      <name>Donald Chiarulli</name>
    </author>
    <author>
      <name>Hai Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in DAC 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.03443v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.03443v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.07887v1</id>
    <updated>2017-03-22T23:46:51Z</updated>
    <published>2017-03-22T23:46:51Z</published>
    <title>Combining Neural Networks and Tree Search for Task and Motion Planning
  in Challenging Environments</title>
    <summary>  We consider task and motion planning in complex dynamic environments for
problems expressed in terms of a set of Linear Temporal Logic (LTL)
constraints, and a reward function. We propose a methodology based on
reinforcement learning that employs deep neural networks to learn low-level
control policies as well as task-level option policies. A major challenge in
this setting, both for neural network approaches and classical planning, is the
need to explore future worlds of a complex and interactive environment. To this
end, we integrate Monte Carlo Tree Search with hierarchical neural net control
policies trained on expressive LTL specifications. This paper investigates the
ability of neural networks to learn both LTL constraints and control policies
in order to generate task plans in complex environments. We demonstrate our
approach in a simulated autonomous driving setting, where a vehicle must drive
down a road in traffic, avoid collisions, and navigate an intersection, all
while obeying given rules of the road.
</summary>
    <author>
      <name>Chris Paxton</name>
    </author>
    <author>
      <name>Vasumathi Raman</name>
    </author>
    <author>
      <name>Gregory D. Hager</name>
    </author>
    <author>
      <name>Marin Kobilarov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pgs, currently in peer review. Video: https://youtu.be/MM2U_SGMtk8</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.07887v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.07887v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.03279v2</id>
    <updated>2017-07-21T13:04:22Z</updated>
    <published>2017-04-11T13:27:00Z</published>
    <title>Unfolding and Shrinking Neural Machine Translation Ensembles</title>
    <summary>  Ensembling is a well-known technique in neural machine translation (NMT) to
improve system performance. Instead of a single neural net, multiple neural
nets with the same topology are trained separately, and the decoder generates
predictions by averaging over the individual models. Ensembling often improves
the quality of the generated translations drastically. However, it is not
suitable for production systems because it is cumbersome and slow. This work
aims to reduce the runtime to be on par with a single system without
compromising the translation quality. First, we show that the ensemble can be
unfolded into a single large neural network which imitates the output of the
ensemble system. We show that unfolding can already improve the runtime in
practice since more work can be done on the GPU. We proceed by describing a set
of techniques to shrink the unfolded network by reducing the dimensionality of
layers. On Japanese-English we report that the resulting network has the size
and decoding speed of a single NMT network but performs on the level of a
3-ensemble system.
</summary>
    <author>
      <name>Felix Stahlberg</name>
    </author>
    <author>
      <name>Bill Byrne</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at EMNLP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.03279v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.03279v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07175v1</id>
    <updated>2017-05-19T20:29:42Z</updated>
    <published>2017-05-19T20:29:42Z</published>
    <title>Espresso: Efficient Forward Propagation for BCNNs</title>
    <summary>  There are many applications scenarios for which the computational performance
and memory footprint of the prediction phase of Deep Neural Networks (DNNs)
needs to be optimized. Binary Neural Networks (BDNNs) have been shown to be an
effective way of achieving this objective. In this paper, we show how
Convolutional Neural Networks (CNNs) can be implemented using binary
representations. Espresso is a compact, yet powerful library written in C/CUDA
that features all the functionalities required for the forward propagation of
CNNs, in a binary file less than 400KB, without any external dependencies.
Although it is mainly designed to take advantage of massive GPU parallelism,
Espresso also provides an equivalent CPU implementation for CNNs. Espresso
provides special convolutional and dense layers for BCNNs, leveraging
bit-packing and bit-wise computations for efficient execution. These techniques
provide a speed-up of matrix-multiplication routines, and at the same time,
reduce memory usage when storing parameters and activations. We experimentally
show that Espresso is significantly faster than existing implementations of
optimized binary neural networks ($\approx$ 2 orders of magnitude). Espresso is
released under the Apache 2.0 license and is available at
http://github.com/fpeder/espresso.
</summary>
    <author>
      <name>Fabrizio Pedersoli</name>
    </author>
    <author>
      <name>George Tzanetakis</name>
    </author>
    <author>
      <name>Andrea Tagliasacchi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.07175v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07175v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M45" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09559v1</id>
    <updated>2017-06-29T03:04:06Z</updated>
    <published>2017-06-29T03:04:06Z</published>
    <title>Audio Spectrogram Representations for Processing with Convolutional
  Neural Networks</title>
    <summary>  One of the decisions that arise when designing a neural network for any
application is how the data should be represented in order to be presented to,
and possibly generated by, a neural network. For audio, the choice is less
obvious than it seems to be for visual images, and a variety of representations
have been used for different applications including the raw digitized sample
stream, hand-crafted features, machine discovered features, MFCCs and variants
that include deltas, and a variety of spectral representations. This paper
reviews some of these representations and issues that arise, focusing
particularly on spectrograms for generating audio using neural networks for
style transfer.
</summary>
    <author>
      <name>L. Wyse</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Conference on Deep Learning
  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Workshop on Deep Learning
  and Music joint with IJCNN. Anchorage, US. 1(1). pp 37-41 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.09559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.00631v1</id>
    <updated>2017-08-02T08:05:09Z</updated>
    <published>2017-08-02T08:05:09Z</published>
    <title>On the Importance of Consistency in Training Deep Neural Networks</title>
    <summary>  We explain that the difficulties of training deep neural networks come from a
syndrome of three consistency issues. This paper describes our efforts in their
analysis and treatment. The first issue is the training speed inconsistency in
different layers. We propose to address it with an intuitive,
simple-to-implement, low footprint second-order method. The second issue is the
scale inconsistency between the layer inputs and the layer residuals. We
explain how second-order information provides favorable convenience in removing
this roadblock. The third and most challenging issue is the inconsistency in
residual propagation. Based on the fundamental theorem of linear algebra, we
provide a mathematical characterization of the famous vanishing gradient
problem. Thus, an important design principle for future optimization and neural
network design is derived. We conclude this paper with the construction of a
novel contractive neural network.
</summary>
    <author>
      <name>Chengxi Ye</name>
    </author>
    <author>
      <name>Yezhou Yang</name>
    </author>
    <author>
      <name>Cornelia Fermuller</name>
    </author>
    <author>
      <name>Yiannis Aloimonos</name>
    </author>
    <link href="http://arxiv.org/abs/1708.00631v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00631v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.01766v2</id>
    <updated>2017-11-08T08:22:10Z</updated>
    <published>2017-09-06T11:13:50Z</published>
    <title>Information-Propogation-Enhanced Neural Machine Translation by Relation
  Model</title>
    <summary>  Even though sequence-to-sequence neural machine translation (NMT) model have
achieved state-of-art performance in the recent fewer years, but it is widely
concerned that the recurrent neural network (RNN) units are very hard to
capture the long-distance state information, which means RNN can hardly find
the feature with long term dependency as the sequence becomes longer.
Similarly, convolutional neural network (CNN) is introduced into NMT for
speeding recently, however, CNN focus on capturing the local feature of the
sequence; To relieve this issue, we incorporate a relation network into the
standard encoder-decoder framework to enhance information-propogation in neural
network, ensuring that the information of the source sentence can flow into the
decoder adequately. Experiments show that proposed framework outperforms the
statistical MT model and the state-of-art NMT model significantly on two data
sets with different scales.
</summary>
    <author>
      <name>Wen Zhang</name>
    </author>
    <author>
      <name>Jiawei Hu</name>
    </author>
    <author>
      <name>Yang Feng</name>
    </author>
    <author>
      <name>Qun Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">i am planned to improve my experiments and modified our paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.01766v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.01766v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.05306v1</id>
    <updated>2017-09-15T16:57:11Z</updated>
    <published>2017-09-15T16:57:11Z</published>
    <title>Recursive Binary Neural Network Learning Model with 2.28b/Weight Storage
  Requirement</title>
    <summary>  This paper presents a storage-efficient learning model titled Recursive
Binary Neural Networks for sensing devices having a limited amount of on-chip
data storage such as &lt; 100's kilo-Bytes. The main idea of the proposed model is
to recursively recycle data storage of synaptic weights (parameters) during
training. This enables a device with a given storage constraint to train and
instantiate a neural network classifier with a larger number of weights on a
chip and with a less number of off-chip storage accesses. This enables higher
classification accuracy, shorter training time, less energy dissipation, and
less on-chip storage requirement. We verified the training model with deep
neural network classifiers and the permutation-invariant MNIST benchmark. Our
model uses only 2.28 bits/weight while for the same data storage constraint
achieving ~1% lower classification error as compared to the conventional
binary-weight learning model which yet has to use 8 to 16 bit storage per
weight. To achieve the similar classification error, the conventional binary
model requires ~4x more data storage for weights than the proposed model.
</summary>
    <author>
      <name>Tianchan Guan</name>
    </author>
    <author>
      <name>Xiaoyang Zeng</name>
    </author>
    <author>
      <name>Mingoo Seok</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.05306v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.05306v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.06950v1</id>
    <updated>2017-09-20T16:18:17Z</updated>
    <published>2017-09-20T16:18:17Z</published>
    <title>Spatial features of synaptic adaptation affecting learning performance</title>
    <summary>  Recent studies have proposed that the diffusion of messenger molecules, such
as monoamines, can mediate the plastic adaptation of synapses in supervised
learning of neural networks. Based on these findings we developed a model for
neural learning, where the signal for plastic adaptation is assumed to
propagate through the extracellular space. We investigate the conditions
allowing learning of Boolean rules in a neural network. Even fully excitatory
networks show very good learning performances. Moreover, the investigation of
the plastic adaptation features optimizing the performance suggests that
learning is very sensitive to the extent of the plastic adaptation and the
spatial range of synaptic connections.
</summary>
    <author>
      <name>Damian L. Berger</name>
    </author>
    <author>
      <name>Lucilla de Arcangelis</name>
    </author>
    <author>
      <name>Hans J. Herrmann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/s41598-017-11424-5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/s41598-017-11424-5" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Scientific Reports 7, 11016 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1709.06950v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.06950v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.04748v1</id>
    <updated>2017-10-12T23:41:02Z</updated>
    <published>2017-10-12T23:41:02Z</published>
    <title>HyperENTM: Evolving Scalable Neural Turing Machines through HyperNEAT</title>
    <summary>  Recent developments within memory-augmented neural networks have solved
sequential problems requiring long-term memory, which are intractable for
traditional neural networks. However, current approaches still struggle to
scale to large memory sizes and sequence lengths. In this paper we show how
access to memory can be encoded geometrically through a HyperNEAT-based Neural
Turing Machine (HyperENTM). We demonstrate that using the indirect HyperNEAT
encoding allows for training on small memory vectors in a bit-vector copy task
and then applying the knowledge gained from such training to speed up training
on larger size memory vectors. Additionally, we demonstrate that in some
instances, networks trained to copy bit-vectors of size 9 can be scaled to
sizes of 1,000 without further training. While the task in this paper is
simple, these results could open up the problems amendable to networks with
external memories to problems with larger memory vectors and theoretically
unbounded memory sizes.
</summary>
    <author>
      <name>Jakob Merrild</name>
    </author>
    <author>
      <name>Mikkel Angaju Rasmussen</name>
    </author>
    <author>
      <name>Sebastian Risi</name>
    </author>
    <link href="http://arxiv.org/abs/1710.04748v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.04748v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.02013v2</id>
    <updated>2018-02-19T04:48:35Z</updated>
    <published>2017-11-02T23:02:52Z</published>
    <title>Neural Language Modeling by Jointly Learning Syntax and Lexicon</title>
    <summary>  We propose a neural language model capable of unsupervised syntactic
structure induction. The model leverages the structure information to form
better semantic representations and better language modeling. Standard
recurrent neural networks are limited by their structure and fail to
efficiently use syntactic information. On the other hand, tree-structured
recursive networks usually require additional structural supervision at the
cost of human expert annotation. In this paper, We propose a novel neural
language model, called the Parsing-Reading-Predict Networks (PRPN), that can
simultaneously induce the syntactic structure from unannotated sentences and
leverage the inferred structure to learn a better language model. In our model,
the gradient can be directly back-propagated from the language model loss into
the neural parsing network. Experiments show that the proposed model can
discover the underlying syntactic structure and achieve state-of-the-art
performance on word/character-level language model tasks.
</summary>
    <author>
      <name>Yikang Shen</name>
    </author>
    <author>
      <name>Zhouhan Lin</name>
    </author>
    <author>
      <name>Chin-Wei Huang</name>
    </author>
    <author>
      <name>Aaron Courville</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 5 figures, ICLR 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.02013v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.02013v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.06516v1</id>
    <updated>2017-11-17T12:52:10Z</updated>
    <published>2017-11-17T12:52:10Z</published>
    <title>Classification of postoperative surgical site infections from blood
  measurements with missing data using recurrent neural networks</title>
    <summary>  Clinical measurements that can be represented as time series constitute an
important fraction of the electronic health records and are often both
uncertain and incomplete. Recurrent neural networks are a special class of
neural networks that are particularly suitable to process time series data but,
in their original formulation, cannot explicitly deal with missing data. In
this paper, we explore imputation strategies for handling missing values in
classifiers based on recurrent neural network (RNN) and apply a recently
proposed recurrent architecture, the Gated Recurrent Unit with Decay,
specifically designed to handle missing data. We focus on the problem of
detecting surgical site infection in patients by analyzing time series of their
blood sample measurements and we compare the results obtained with different
RNN-based classifiers.
</summary>
    <author>
      <name>Andreas Storvik Strauman</name>
    </author>
    <author>
      <name>Filippo Maria Bianchi</name>
    </author>
    <author>
      <name>Karl √òyvind Mikalsen</name>
    </author>
    <author>
      <name>Michael Kampffmeyer</name>
    </author>
    <author>
      <name>Cristina Soguero-Ruiz</name>
    </author>
    <author>
      <name>Robert Jenssen</name>
    </author>
    <link href="http://arxiv.org/abs/1711.06516v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.06516v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.08762v1</id>
    <updated>2017-11-23T16:32:57Z</updated>
    <published>2017-11-23T16:32:57Z</published>
    <title>DNN-Buddies: A Deep Neural Network-Based Estimation Metric for the
  Jigsaw Puzzle Problem</title>
    <summary>  This paper introduces the first deep neural network-based estimation metric
for the jigsaw puzzle problem. Given two puzzle piece edges, the neural network
predicts whether or not they should be adjacent in the correct assembly of the
puzzle, using nothing but the pixels of each piece. The proposed metric
exhibits an extremely high precision even though no manual feature extraction
is performed. When incorporated into an existing puzzle solver, the solution's
accuracy increases significantly, achieving thereby a new state-of-the-art
standard.
</summary>
    <author>
      <name>Dror Sholomon</name>
    </author>
    <author>
      <name>Eli David</name>
    </author>
    <author>
      <name>Nathan S. Netanyahu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-44781-0_21</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-44781-0_21" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Artificial Neural Networks (ICANN),
  Springer LNCS, Vol. 9887, pp. 170-178, Barcelona, Spain, September 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1711.08762v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.08762v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.09666v1</id>
    <updated>2017-11-27T13:04:46Z</updated>
    <published>2017-11-27T13:04:46Z</published>
    <title>DeepAPT: Nation-State APT Attribution Using End-to-End Deep Neural
  Networks</title>
    <summary>  In recent years numerous advanced malware, aka advanced persistent threats
(APT) are allegedly developed by nation-states. The task of attributing an APT
to a specific nation-state is extremely challenging for several reasons. Each
nation-state has usually more than a single cyber unit that develops such
advanced malware, rendering traditional authorship attribution algorithms
useless. Furthermore, those APTs use state-of-the-art evasion techniques,
making feature extraction challenging. Finally, the dataset of such available
APTs is extremely small.
  In this paper we describe how deep neural networks (DNN) could be
successfully employed for nation-state APT attribution. We use sandbox reports
(recording the behavior of the APT when run dynamically) as raw input for the
neural network, allowing the DNN to learn high level feature abstractions of
the APTs itself. Using a test set of 1,000 Chinese and Russian developed APTs,
we achieved an accuracy rate of 94.6%.
</summary>
    <author>
      <name>Ishai Rosenberg</name>
    </author>
    <author>
      <name>Guillaume Sicard</name>
    </author>
    <author>
      <name>Eli David</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-68612-7_11</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-68612-7_11" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Artificial Neural Networks (ICANN),
  Springer LNCS, Vol. 10614, pp. 91-99, Alghero, Italy, September, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1711.09666v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.09666v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.00448v1</id>
    <updated>2018-01-01T14:09:40Z</updated>
    <published>2018-01-01T14:09:40Z</published>
    <title>Level-Shifted Neural Encoded Analog-to-Digital Converter</title>
    <summary>  This paper presents the new approach in implementation of analog-to-digital
converter (ADC) that is based on Hopfield neural-network architecture. Hopfield
neural ADC (NADC) is a type of recurrent neural network that is effective in
solving simple optimization problems, such as analog-to-digital conversion. The
main idea behind the proposed design is to use multiple 2-bit Hopfield NADCs
operating as quantizers in parallel, where analog input signal to each
successive 2-bit Hopfield ADC block is passed through a voltage level shifter.
This is followed by a neural network encoder to remove the quantization errors.
In traditional Hopfield NADC based designs, increasing the number of bits could
require proper scaling of the network parameters, in particular digital output
operating region. Furthermore, the resolution improvement of traditional
Hopfield NADC creates digital error that increases with the increasing number
of bits. The proposed design is scalable in number of bits and number of
quantization levels, and can maintain the magnitude of digital output code
within a manageable operating voltage range.
</summary>
    <author>
      <name>Aigerim Tankimanova</name>
    </author>
    <author>
      <name>Akshay Kumar Maan</name>
    </author>
    <author>
      <name>Alex Pappachen James</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2017 IEEE International Conference on Electronics, Circuits and
  Systems (ICECS)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1801.00448v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.00448v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.01353v1</id>
    <updated>2018-02-05T11:25:54Z</updated>
    <published>2018-02-05T11:25:54Z</published>
    <title>Lie Transform Based Polynomial Neural Networks for Dynamical Systems
  Simulation and Identification</title>
    <summary>  In the article, we discuss the architecture of the polynomial neural network
that corresponds to the matrix representation of Lie transform. The matrix form
of Lie transform is an approximation of general solution for the nonlinear
system of ordinary differential equations. Thus, it can be used for simulation
and modeling task. On the other hand, one can identify dynamical system from
time series data simply by optimization of the coefficient matrices of the Lie
transform. Representation of the approach by polynomial neural networks
integrates the strength of both neural networks and traditional model-based
methods for dynamical systems investigation. We provide a theoretical
explanation of learning dynamical systems from time series for the proposed
method, as well as demonstrate it in several applications. Namely, we show
results of modeling and identification for both well-known systems like
Lotka-Volterra equation and more complicated examples from retail,
biochemistry, and accelerator physics.
</summary>
    <author>
      <name>Andrei Ivanov</name>
    </author>
    <author>
      <name>Sergei Andrianov</name>
    </author>
    <author>
      <name>Alena Sholokhova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.01353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.01353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.08567v2</id>
    <updated>2018-02-26T19:26:30Z</updated>
    <published>2018-02-22T04:33:32Z</published>
    <title>Adversarial Training for Probabilistic Spiking Neural Networks</title>
    <summary>  Classifiers trained using conventional empirical risk minimization or maximum
likelihood methods are known to suffer dramatic performance degradations when
tested over examples adversarially selected based on knowledge of the
classifier's decision rule. Due to the prominence of Artificial Neural Networks
(ANNs) as classifiers, their sensitivity to adversarial examples, as well as
robust training schemes, have been recently the subject of intense
investigation. In this paper, for the first time, the sensitivity of spiking
neural networks (SNNs), or third-generation neural networks, to adversarial
examples is studied. The study considers rate and time encoding, as well as
rate and first-to-spike decoding. Furthermore, a robust training mechanism is
proposed that is demonstrated to enhance the performance of SNNs under
white-box attacks.
</summary>
    <author>
      <name>Alireza Bagheri</name>
    </author>
    <author>
      <name>Osvaldo Simeone</name>
    </author>
    <author>
      <name>Bipin Rajendran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted for possible publication. arXiv admin note: text overlap
  with arXiv:1710.10704</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.08567v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.08567v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/astro-ph/0112359v1</id>
    <updated>2001-12-14T20:48:43Z</updated>
    <published>2001-12-14T20:48:43Z</published>
    <title>Neural networks as a tool for parameter estimation in astrophysical data</title>
    <summary>  We present a neural net algorithm for parameter estimation in the context of
large cosmological data sets. Cosmological data sets present a particular
challenge to pattern-recognition algorithms since the input patterns (galaxy
redshift surveys, maps of cosmic microwave background anisotropy) are not fixed
templates overlaid with random noise, but rather are random realizations whose
information content lies in the correlations between data points. We train a
``committee'' of neural nets to distinguish between Monte Carlo simulations at
fixed parameter values. Sampling the trained networks using additional Monte
Carlo simulations generated at intermediate parameter values allows accurate
interpolation to parameter values for which the networks were never trained.
The Monte Carlo samples automatically provide the probability distributions and
truth tables required for either a frequentist or Bayseian analysis of the one
observable sky. We demonstrate that neural networks provide unbiased parameter
estimation with comparable precision as maximum-likelihood algorithms but
significant computational savings. In the context of CMB anisotropies, the
computational cost for parameter estimation via neural networks scales as
$N^{3/2}$. The results are insensitive to the noise levels and sampling schemes
typical of large cosmological data sets and provide a desirable tool for the
new generation of large, complex data sets.
</summary>
    <author>
      <name>Nicholas G. Phillips</name>
    </author>
    <author>
      <name>A. Kogut</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 3 figures, submitted to "Neural Networks" special issue on
  GeoScience and Astronomy</arxiv:comment>
    <link href="http://arxiv.org/abs/astro-ph/0112359v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/astro-ph/0112359v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0111034v1</id>
    <updated>2001-11-02T15:55:49Z</updated>
    <published>2001-11-02T15:55:49Z</published>
    <title>Hebbian imprinting and retrieval in oscillatory neural networks</title>
    <summary>  We introduce a model of generalized Hebbian learning and retrieval in
oscillatory neural networks modeling cortical areas such as hippocampus and
olfactory cortex. Recent experiments have shown that synaptic plasticity
depends on spike timing, especially on synapses from excitatory pyramidal
cells, in hippocampus and in sensory and cerebellar cortex. Here we study how
such plasticity can be used to form memories and input representations when the
neural dynamics are oscillatory, as is common in the brain (particularly in the
hippocampus and olfactory cortex). Learning is assumed to occur in a phase of
neural plasticity, in which the network is clamped to external teaching
signals. By suitable manipulation of the nonlinearity of the neurons or of the
oscillation frequencies during learning, the model can be made, in a retrieval
phase, either to categorize new inputs or to map them, in a continuous fashion,
onto the space spanned by the imprinted patterns. We identify the first of
these possibilities with the function of olfactory cortex and the second with
the observed response characteristics of place cells in hippocampus. We
investigate both kinds of networks analytically and by computer simulations,
and we link the models with experimental findings, exploring, in particular,
how the spike timing dependence of the synaptic plasticity constrains the
computational function of the network and vice versa.
</summary>
    <author>
      <name>Silvia Scarpetta</name>
    </author>
    <author>
      <name>Zhaoping Li</name>
    </author>
    <author>
      <name>John Hertz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Computation 14, 2371 (2002).</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0111034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0111034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0712.0938v1</id>
    <updated>2007-12-06T13:52:04Z</updated>
    <published>2007-12-06T13:52:04Z</published>
    <title>Automatic Pattern Classification by Unsupervised Learning Using
  Dimensionality Reduction of Data with Mirroring Neural Networks</title>
    <summary>  This paper proposes an unsupervised learning technique by using Multi-layer
Mirroring Neural Network and Forgy's clustering algorithm. Multi-layer
Mirroring Neural Network is a neural network that can be trained with
generalized data inputs (different categories of image patterns) to perform
non-linear dimensionality reduction and the resultant low-dimensional code is
used for unsupervised pattern classification using Forgy's algorithm. By
adapting the non-linear activation function (modified sigmoidal function) and
initializing the weights and bias terms to small random values, mirroring of
the input pattern is initiated. In training, the weights and bias terms are
changed in such a way that the input presented is reproduced at the output by
back propagating the error. The mirroring neural network is capable of reducing
the input vector to a great degree (approximately 1/30th the original size) and
also able to reconstruct the input pattern at the output layer from this
reduced code units. The feature set (output of central hidden layer) extracted
from this network is fed to Forgy's algorithm, which classify input data
patterns into distinguishable classes. In the implementation of Forgy's
algorithm, initial seed points are selected in such a way that they are distant
enough to be perfectly grouped into different categories. Thus a new method of
unsupervised learning is formulated and demonstrated in this paper. This method
gave impressive results when applied to classification of different image
patterns.
</summary>
    <author>
      <name>Dasika Ratna Deepthi</name>
    </author>
    <author>
      <name>G. R. Aditya Krishna</name>
    </author>
    <author>
      <name>K. Eswaran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented in IEEE International Conference on Advances in Computer
  Vision and Information Technology (ACVIT-07), Nov. 28-30 2007</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Advances in Computer Vision and
  Information Tech. (IEEE, ACVIT-07), pp. 354 - 360 (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0712.0938v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.0938v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0901.3028v1</id>
    <updated>2009-01-20T11:00:35Z</updated>
    <published>2009-01-20T11:00:35Z</published>
    <title>Cognitive computation with autonomously active neural networks: an
  emerging field</title>
    <summary>  The human brain is autonomously active. To understand the functional role of
this self-sustained neural activity, and its interplay with the sensory data
input stream, is an important question in cognitive system research and we
review here the present state of theoretical modelling.
  This review will start with a brief overview of the experimental efforts,
together with a discussion of transient vs. self-sustained neural activity in
the framework of reservoir computing. The main emphasis will be then on two
paradigmal neural network architectures showing continuously ongoing
transient-state dynamics: saddle point networks and networks of attractor
relics.
  Self-active neural networks are confronted with two seemingly contrasting
demands: a stable internal dynamical state and sensitivity to incoming stimuli.
We show, that this dilemma can be solved by networks of attractor relics based
on competitive neural dynamics, where the attractor relics compete on one side
with each other for transient dominance, and on the other side with the
dynamical influence of the input signals. Unsupervised and local Hebbian-style
online learning then allows the system to build up correlations between the
internal dynamical transient states and the sensory input stream. An emergent
cognitive capability results from this set-up. The system performs online, and
on its own, a non-linear independent component analysis of the sensory data
stream, all the time being continuously and autonomously active. This process
maps the independent components of the sensory input onto the attractor relics,
which acquire in this way a semantic meaning.
</summary>
    <author>
      <name>Claudius Gros</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">keynote review. Cognitive Computation (in press, 2009)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Cognitive Computation 1, 77-99 (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0901.3028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0901.3028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.03770v2</id>
    <updated>2017-03-28T16:45:11Z</updated>
    <published>2016-12-12T16:25:23Z</published>
    <title>Neurogenesis Deep Learning</title>
    <summary>  Neural machine learning methods, such as deep neural networks (DNN), have
achieved remarkable success in a number of complex data processing tasks. These
methods have arguably had their strongest impact on tasks such as image and
audio processing - data processing domains in which humans have long held clear
advantages over conventional algorithms. In contrast to biological neural
systems, which are capable of learning continuously, deep artificial networks
have a limited ability for incorporating new information in an already trained
network. As a result, methods for continuous learning are potentially highly
impactful in enabling the application of deep networks to dynamic data sets.
Here, inspired by the process of adult neurogenesis in the hippocampus, we
explore the potential for adding new neurons to deep layers of artificial
neural networks in order to facilitate their acquisition of novel information
while preserving previously trained data representations. Our results on the
MNIST handwritten digit dataset and the NIST SD 19 dataset, which includes
lower and upper case letters and digits, demonstrate that neurogenesis is well
suited for addressing the stability-plasticity dilemma that has long challenged
adaptive machine learning algorithms.
</summary>
    <author>
      <name>Timothy J. Draelos</name>
    </author>
    <author>
      <name>Nadine E. Miner</name>
    </author>
    <author>
      <name>Christopher C. Lamb</name>
    </author>
    <author>
      <name>Jonathan A. Cox</name>
    </author>
    <author>
      <name>Craig M. Vineyard</name>
    </author>
    <author>
      <name>Kristofor D. Carlson</name>
    </author>
    <author>
      <name>William M. Severa</name>
    </author>
    <author>
      <name>Conrad D. James</name>
    </author>
    <author>
      <name>James B. Aimone</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IJCNN.2017.7965898</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IJCNN.2017.7965898" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 8 figures, Accepted to 2017 International Joint Conference
  on Neural Networks (IJCNN 2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.03770v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.03770v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.08200v1</id>
    <updated>2017-05-23T12:11:30Z</updated>
    <published>2017-05-23T12:11:30Z</published>
    <title>Logical Learning Through a Hybrid Neural Network with Auxiliary Inputs</title>
    <summary>  The human reasoning process is seldom a one-way process from an input leading
to an output. Instead, it often involves a systematic deduction by ruling out
other possible outcomes as a self-checking mechanism. In this paper, we
describe the design of a hybrid neural network for logical learning that is
similar to the human reasoning through the introduction of an auxiliary input,
namely the indicators, that act as the hints to suggest logical outcomes. We
generate these indicators by digging into the hidden information buried
underneath the original training data for direct or indirect suggestions. We
used the MNIST data to demonstrate the design and use of these indicators in a
convolutional neural network. We trained a series of such hybrid neural
networks with variations of the indicators. Our results show that these hybrid
neural networks are very robust in generating logical outcomes with inherently
higher prediction accuracy than the direct use of the original input and output
in apparent models. Such improved predictability with reassured logical
confidence is obtained through the exhaustion of all possible indicators to
rule out all illogical outcomes, which is not available in the apparent models.
Our logical learning process can effectively cope with the unknown unknowns
using a full exploitation of all existing knowledge available for learning. The
design and implementation of the hints, namely the indicators, become an
essential part of artificial intelligence for logical learning. We also
introduce an ongoing application setup for this hybrid neural network in an
autonomous grasping robot, namely as_DeepClaw, aiming at learning an optimized
grasping pose through logical learning.
</summary>
    <author>
      <name>Fang Wan</name>
    </author>
    <author>
      <name>Chaoyang Song</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 9 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.08200v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.08200v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.04145v1</id>
    <updated>2017-12-12T06:37:28Z</updated>
    <published>2017-12-12T06:37:28Z</published>
    <title>Transportation analysis of denoising autoencoders: a novel method for
  analyzing deep neural networks</title>
    <summary>  The feature map obtained from the denoising autoencoder (DAE) is investigated
by determining transportation dynamics of the DAE, which is a cornerstone for
deep learning. Despite the rapid development in its application, deep neural
networks remain analytically unexplained, because the feature maps are nested
and parameters are not faithful. In this paper, we address the problem of the
formulation of nested complex of parameters by regarding the feature map as a
transport map. Even when a feature map has different dimensions between input
and output, we can regard it as a transportation map by considering that both
the input and output spaces are embedded in a common high-dimensional space. In
addition, the trajectory is a geometric object and thus, is independent of
parameterization. In this manner, transportation can be regarded as a universal
character of deep neural networks. By determining and analyzing the
transportation dynamics, we can understand the behavior of a deep neural
network. In this paper, we investigate a fundamental case of deep neural
networks: the DAE. We derive the transport map of the DAE, and reveal that the
infinitely deep DAE transports mass to decrease a certain quantity, such as
entropy, of the data distribution. These results though analytically simple,
shed light on the correspondence between deep neural networks and the
Wasserstein gradient flows.
</summary>
    <author>
      <name>Sho Sonoda</name>
    </author>
    <author>
      <name>Noboru Murata</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at NIPS 2017 workshop on Optimal Transport &amp; Machine
  Learning (OTML2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.04145v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.04145v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00123v1</id>
    <updated>2018-02-01T01:55:11Z</updated>
    <published>2018-02-01T01:55:11Z</published>
    <title>A Modified Sigma-Pi-Sigma Neural Network with Adaptive Choice of
  Multinomials</title>
    <summary>  Sigma-Pi-Sigma neural networks (SPSNNs) as a kind of high-order neural
networks can provide more powerful mapping capability than the traditional
feedforward neural networks (Sigma-Sigma neural networks). In the existing
literature, in order to reduce the number of the Pi nodes in the Pi layer, a
special multinomial P_s is used in SPSNNs. Each monomial in P_s is linear with
respect to each particular variable sigma_i when the other variables are taken
as constants. Therefore, the monomials like sigma_i^n or sigma_i^n sigma_j with
n&gt;1 are not included. This choice may be somehow intuitive, but is not
necessarily the best. We propose in this paper a modified Sigma-Pi-Sigma neural
network (MSPSNN) with an adaptive approach to find a better multinomial for a
given problem. To elaborate, we start from a complete multinomial with a given
order. Then we employ a regularization technique in the learning process for
the given problem to reduce the number of monomials used in the multinomial,
and end up with a new SPSNN involving the same number of monomials (= the
number of nodes in the Pi-layer) as in P_s. Numerical experiments on some
benchmark problems show that our MSPSNN behaves better than the traditional
SPSNN with P_s.
</summary>
    <author>
      <name>Feng Li</name>
    </author>
    <author>
      <name>Yan Liu</name>
    </author>
    <author>
      <name>Khidir Shaib Mohamed</name>
    </author>
    <author>
      <name>Wei Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1802.00123v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00123v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.00412v1</id>
    <updated>2018-02-28T17:54:24Z</updated>
    <published>2018-02-28T17:54:24Z</published>
    <title>A theory of sequence indexing and working memory in recurrent neural
  networks</title>
    <summary>  To accommodate structured approaches of neural computation, we propose a
class of recurrent neural networks for indexing and storing sequences of
symbols or analog data vectors. These networks with randomized input weights
and orthogonal recurrent weights implement coding principles previously
described in vector symbolic architectures (VSA), and leverage properties of
reservoir computing. In general, the storage in reservoir computing is lossy
and crosstalk noise limits the retrieval accuracy and information capacity. A
novel theory to optimize memory performance in such networks is presented and
compared with simulation experiments. The theory describes linear readout of
analog data, and readout with winner-take-all error correction of symbolic data
as proposed in VSA models. We find that diverse VSA models from the literature
have universal performance properties, which are superior to what previous
analyses predicted. Further, we propose novel VSA models with the statistically
optimal Wiener filter in the readout that exhibit much higher information
capacity, in particular for storing analog data.
  The presented theory also applies to memory buffers, networks with gradual
forgetting, which can operate on infinite data streams without memory overflow.
Interestingly, we find that different forgetting mechanisms, such as
attenuating recurrent weights or neural nonlinearities, produce very similar
behavior if the forgetting time constants are aligned. Such models exhibit
extensive capacity when their forgetting time constant is optimized for given
noise conditions and network size. These results enable the design of new types
of VSA models for the online processing of data streams.
</summary>
    <author>
      <name>E. Paxon Frady</name>
    </author>
    <author>
      <name>Denis Kleyko</name>
    </author>
    <author>
      <name>Friedrich T. Sommer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">62 pages, 19 Figures, 85 equations, accepted in Neural Computation.
  arXiv admin note: text overlap with arXiv:1707.01429</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.00412v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.00412v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0112052v2</id>
    <updated>2001-12-05T10:04:51Z</updated>
    <published>2001-12-04T14:20:03Z</published>
    <title>Coupled Growing Networks</title>
    <summary>  We introduce and solve a model which considers two coupled networks growing
simultaneously. The dynamics of the networks is governed by the new arrival of
network elements (nodes) making preferential attachments to pre-existing nodes
in both networks. The model segregates the links in the networks as
intra-links, cross-links and mix-links. The corresponding degree distributions
of these links are found to be power-laws with exponents having coupled
parameters for intra- and cross-links. In the weak coupling case the model
reduces to a simple citation network. As for the strong coupling, it mimics the
mechanism of \emph{the web of human sexual contacts}.
</summary>
    <author>
      <name>Dafang Zheng</name>
    </author>
    <author>
      <name>Guler Ergun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages including 1 figures, corrected typos</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0112052v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0112052v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0202174v1</id>
    <updated>2002-02-11T09:27:04Z</updated>
    <published>2002-02-11T09:27:04Z</published>
    <title>Marvel Universe looks almost like a real social network</title>
    <summary>  We investigate the structure of the Marvel Universe collaboration network,
where two Marvel characters are considered linked if they jointly appear in the
same Marvel comic book. We show that this network is clearly not a random
network, and that it has most, but not all, characteristics of "real-life"
collaboration networks, such as movie actors or scientific collaboration
networks. The study of this artificial universe that tries to look like a real
one, helps to understand that there are underlying principles that make
real-life networks have definite characteristics.
</summary>
    <author>
      <name>R. Alberich</name>
    </author>
    <author>
      <name>J. Miro-Julia</name>
    </author>
    <author>
      <name>F. Rossello</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0202174v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0202174v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0303516v1</id>
    <updated>2003-03-25T13:32:20Z</updated>
    <published>2003-03-25T13:32:20Z</published>
    <title>The structure and function of complex networks</title>
    <summary>  Inspired by empirical studies of networked systems such as the Internet,
social networks, and biological networks, researchers have in recent years
developed a variety of techniques and models to help us understand or predict
the behavior of these systems. Here we review developments in this field,
including such concepts as the small-world effect, degree distributions,
clustering, network correlations, random graph models, models of network growth
and preferential attachment, and dynamical processes taking place on networks.
</summary>
    <author>
      <name>M. E. J. Newman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1137/S003614450342480</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1137/S003614450342480" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Review article, 58 pages, 16 figures, 3 tables, 429 references,
  published in SIAM Review (2003)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SIAM Review 45, 167-256 (2003)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0303516v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0303516v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0405381v2</id>
    <updated>2004-06-04T07:44:45Z</updated>
    <published>2004-05-17T22:21:34Z</published>
    <title>Random Networks with Tunable Degree Distribution and Clustering</title>
    <summary>  We present an algorithm for generating random networks with arbitrary degree
distribution and Clustering (frequency of triadic closure). We use this
algorithm to generate networks with exponential, power law, and poisson degree
distributions with variable levels of clustering. Such networks may be used as
models of social networks and as a testable null hypothesis about network
structure. Finally, we explore the effects of clustering on the point of the
phase transition where a giant component forms in a random network, and on the
size of the giant component. Some analysis of these effects is presented.
</summary>
    <author>
      <name>Erik Volz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.70.056115</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.70.056115" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 13 figures corrected typos, added two references,
  reorganized references</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0405381v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0405381v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0407643v1</id>
    <updated>2004-07-23T22:37:39Z</updated>
    <published>2004-07-23T22:37:39Z</published>
    <title>Preferential compactness of networks</title>
    <summary>  We introduce evolving networks where new vertices preferentially connect to
the more central parts of a network. This makes such networks compact. Finite
networks grown under the preferential compactness mechanism have complex
architectures, but infinite ones tend towards the opposite, having rapidly
decreasing distributions of connections. We present an analytical solution of
the problem for tree-like networks. Our approach links a collective
self-optimization mechanism of the emergence of complex network architectures
to self-organization mechanisms.
</summary>
    <author>
      <name>M. J. Alava</name>
    </author>
    <author>
      <name>S. N. Dorogovtsev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0407643v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0407643v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0407680v1</id>
    <updated>2004-07-26T21:31:28Z</updated>
    <published>2004-07-26T21:31:28Z</published>
    <title>The spatial structure of networks</title>
    <summary>  We study networks that connect points in geographic space, such as
transportation networks and the Internet. We find that there are strong
signatures in these networks of topography and use patterns, giving the
networks shapes that are quite distinct from one another and from
non-geographic networks. We offer an explanation of these differences in terms
of the costs and benefits of transportation and communication, and give a
simple model based on the Monte Carlo optimization of these costs and benefits
that reproduces well the qualitative features of the networks studied.
</summary>
    <author>
      <name>Michael T. Gastner</name>
    </author>
    <author>
      <name>M. E. J. Newman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1140/epjb/e2006-00046-8</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1140/epjb/e2006-00046-8" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Eur. Phys. J. B 49, 247-252 (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0407680v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0407680v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0509042v1</id>
    <updated>2005-09-01T19:46:59Z</updated>
    <published>2005-09-01T19:46:59Z</published>
    <title>Fluctuation-dissipation relations for complex networks</title>
    <summary>  In the paper, we study fluctuations over several ensembles of maximum-entropy
random networks. We derive several fluctuation-dissipation relations
characterizing susceptibilities of different networks to changes in external
fields. In the case of networks with a given degree sequence, we argue that the
scale-free topologies of real-world networks may arise as a result of
self-organization of real systems into sparse structures with low
susceptibility to random external perturbations. We also show that the
ensembles of networks with noninteracting links (both uncorrelated and with
two-point correlations) are equivalent to random networks with hidden
variables.
</summary>
    <author>
      <name>Agata Fronczak</name>
    </author>
    <author>
      <name>Piotr Fronczak</name>
    </author>
    <author>
      <name>Janusz A. Holyst</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.73.016108</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.73.016108" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 73, 016108 (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0509042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0509042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0509290v1</id>
    <updated>2005-09-12T11:45:20Z</updated>
    <published>2005-09-12T11:45:20Z</published>
    <title>Optimization of scale-free network for random failures</title>
    <summary>  It has been found that the networks with scale-free distribution are very
resilient to random failures. The purpose of this work is to determine the
network design guideline which maximize the network robustness to random
failures with the average number of links per node of the network is constant.
The optimal value of the distribution exponent and the minimum connectivity to
different network size are given in this paper. Finally, the optimization
strategy how to improve the evolving network robustness is given.
</summary>
    <author>
      <name>Jian-Guo Liu</name>
    </author>
    <author>
      <name>Zhong-Tuo Wang</name>
    </author>
    <author>
      <name>Yan-Zhong Dang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1142/S0217984906010883</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1142/S0217984906010883" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0509290v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0509290v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0606122v1</id>
    <updated>2006-06-05T17:14:20Z</updated>
    <published>2006-06-05T17:14:20Z</published>
    <title>Characterizing the dynamical importance of network nodes and links</title>
    <summary>  The largest eigenvalue of the adjacency matrix of the networks is a key
quantity determining several important dynamical processes on complex networks.
Based on this fact, we present a quantitative, objective characterization of
the dynamical importance of network nodes and links in terms of their effect on
the largest eigenvalue. We show how our characterization of the dynamical
importance of nodes can be affected by degree-degree correlations and network
community structure. We discuss how our characterization can be used to
optimize techniques for controlling certain network dynamical processes and
apply our results to real networks.
</summary>
    <author>
      <name>J. G. Restrepo</name>
    </author>
    <author>
      <name>E. Ott</name>
    </author>
    <author>
      <name>B. R. Hunt</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.97.094102</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.97.094102" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. Lett. 97, 094102 (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0606122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0606122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0703191v2</id>
    <updated>2007-03-08T13:49:51Z</updated>
    <published>2007-03-07T18:31:10Z</published>
    <title>A statistical mechanics approach for scale-free networks and
  finite-scale networks</title>
    <summary>  We present a statistical mechanics approach for the description of complex
networks. We first define an energy and an entropy associated to a degree
distribution which have a geometrical interpretation. Next we evaluate the
distribution which extremize the free energy of the network. We find two
important limiting cases: a scale-free degree distribution and a finite-scale
degree distribution. The size of the space of allowed simple networks given
these distribution is evaluated in the large network limit. Results are
compared with simulations of algorithms generating these networks.
</summary>
    <author>
      <name>Ginestra Bianconi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/1.2720642</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/1.2720642" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">(6 pages, 5 figures)</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0703191v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0703191v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0703470v2</id>
    <updated>2008-02-28T06:00:07Z</updated>
    <published>2007-03-19T18:57:45Z</published>
    <title>Portraits of Complex Networks</title>
    <summary>  We propose a method for characterizing large complex networks by introducing
a new matrix structure, unique for a given network, which encodes structural
information; provides useful visualization, even for very large networks; and
allows for rigorous statistical comparison between networks. Dynamic processes
such as percolation can be visualized using animations. Applications to graph
theory are discussed, as are generalizations to weighted networks, real-world
network similarity testing, and applicability to the graph isomorphism problem.
</summary>
    <author>
      <name>J. P. Bagrow</name>
    </author>
    <author>
      <name>E. M. Bollt</name>
    </author>
    <author>
      <name>J. D. Skufca</name>
    </author>
    <author>
      <name>D. ben-Avraham</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1209/0295-5075/81/68004</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1209/0295-5075/81/68004" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 9 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPL 81 (2008) 68004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0703470v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0703470v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/physics/0505158v1</id>
    <updated>2005-05-23T15:10:16Z</updated>
    <published>2005-05-23T15:10:16Z</published>
    <title>Surviving opinions in Sznajd models on complex networks</title>
    <summary>  The Sznajd model has been largely applied to simulate many sociophysical
phenomena. In this paper we applied the Sznajd model with more than two
opinions on three different network topologies and observed the evolution of
surviving opinions after many interactions among the nodes. As result, we
obtained a scaling law which depends of the network size and the number of
possible opinions. We also observed that this scaling law is not the same for
all network topologies, being quite similar between scale-free networks and
Sznajd networks but different for random networks.
</summary>
    <author>
      <name>Francisco A. Rodrigues</name>
    </author>
    <author>
      <name>Luciano da F. Costa</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1142/S0129183105008278</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1142/S0129183105008278" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/physics/0505158v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/physics/0505158v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/physics/0607150v1</id>
    <updated>2006-07-17T13:50:31Z</updated>
    <published>2006-07-17T13:50:31Z</published>
    <title>What do we learn from correlations of local and global network
  properties?</title>
    <summary>  In complex networks a common task is to identify the most important or
"central" nodes. There are several definitions, often called centrality
measures, which often lead to different results. Here we study extensively
correlations between four local and global measures namely the degree, the
shortest-path-betweenness, the random-walk betweenness and the subgraph
centrality on different random-network models like Erdos-Renyi, Small-World and
Barabasi-Albert as well as on different real networks like metabolic pathways,
social collaborations and computer networks. Correlations are quite different
between the real networks and the model networks questioning whether the models
really reflect all important properties of the real world.
</summary>
    <author>
      <name>Magnus Jungsbluth</name>
    </author>
    <author>
      <name>Bernd Burghardt</name>
    </author>
    <author>
      <name>Alexander K. Hartmann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physa.2007.03.029</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physa.2007.03.029" rel="related"/>
    <link href="http://arxiv.org/abs/physics/0607150v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/physics/0607150v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.0089v1</id>
    <updated>2007-05-01T09:04:08Z</updated>
    <published>2007-05-01T09:04:08Z</published>
    <title>Bounding network spectra for network design</title>
    <summary>  The identification of the limiting factors in the dynamical behavior of
complex systems is an important interdisciplinary problem which often can be
traced to the spectral properties of an underlying network. By deriving a
general relation between the eigenvalues of weighted and unweighted networks,
here I show that for a wide class of networks the dynamical behavior is tightly
bounded by few network parameters. This result provides rigorous conditions for
the design of networks with predefined dynamical properties and for the
structural control of physical processes in complex systems. The results are
illustrated using synchronization phenomena as a model process.
</summary>
    <author>
      <name>Adilson E. Motter</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1367-2630/9/6/182</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1367-2630/9/6/182" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">New J. Phys. 9, 182 (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0705.0089v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.0089v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.2074v1</id>
    <updated>2007-09-13T13:37:58Z</updated>
    <published>2007-09-13T13:37:58Z</published>
    <title>Generalized entropies and open random and scale-free networks</title>
    <summary>  We propose the concept of open network as an arbitrary selection of nodes of
a large unknown network. Using the hypothesis that information of the whole
network structure can be extrapolated from an arbitrary set of its nodes, we
use Renyi mutual entropies in different q-orders to establish the minimum
critical size of a random set of nodes that represents reliably the information
of the main network structure. We also identify the clusters of nodes
responsible for the structure of their containing network.
</summary>
    <author>
      <name>V. Gudkov</name>
    </author>
    <author>
      <name>V. Montealegre</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/1.2828754</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/1.2828754" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">talk at CTNEXT07 (July 2007)</arxiv:comment>
    <link href="http://arxiv.org/abs/0709.2074v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.2074v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.3247v1</id>
    <updated>2007-10-17T09:26:57Z</updated>
    <published>2007-10-17T09:26:57Z</published>
    <title>Generating random networks with given degree-degree correlations and
  degree-dependent clustering</title>
    <summary>  Random networks are widely used to model complex networks and research their
properties. In order to get a good approximation of complex networks
encountered in various disciplines of science, the ability to tune various
statistical properties of random networks is very important. In this manuscript
we present an algorithm which is able to construct arbitrarily degree-degree
correlated networks with adjustable degree-dependent clustering. We verify the
algorithm by using empirical networks as input and describe additionally a
simple way to fix a degree-dependent clustering function if degree-degree
correlations are given.
</summary>
    <author>
      <name>Andreas Pusch</name>
    </author>
    <author>
      <name>Sebastian Weber</name>
    </author>
    <author>
      <name>Markus Porto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.77.017101</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.77.017101" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E, 77, 017101 (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.3247v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.3247v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0810.1428v2</id>
    <updated>2008-10-08T21:17:42Z</updated>
    <published>2008-10-08T13:28:37Z</published>
    <title>Shortest path discovery of complex networks</title>
    <summary>  In this paper we present an analytic study of sampled networks in the case of
some important shortest-path sampling models. We present analytic formulas for
the probability of edge discovery in the case of an evolving and a static
network model. We also show that the number of discovered edges in a finite
network scales much slower than predicted by earlier mean field models.
Finally, we calculate the degree distribution of sampled networks, and we
demonstrate that they are analogous to a destructed network obtained by
randomly removing edges from the original network.
</summary>
    <author>
      <name>Attila Fekete</name>
    </author>
    <author>
      <name>G√°bor Vattay</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.79.065101</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.79.065101" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0810.1428v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.1428v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.1821v1</id>
    <updated>2009-05-12T12:20:44Z</updated>
    <published>2009-05-12T12:20:44Z</published>
    <title>Diffusion of O2 and N2 through thin and thick SWNT networks</title>
    <summary>  We report the electrical responses of thin and thick single-walled carbon
nanotube (SWNT) networks to N2 and O2 adsorption. In the surface desorbed state
exposure to N2 and O2 provide an increase in conductance of thin and thick SWNT
networks. The increase in conductance of both thin and thick networks is of a
greater magnitude during O2 exposure rather than N2 exposure. Thin networks
exhibit a greater response to both O2 and N2 rather than thick networks. This
is likely a result of the increased semiconducting nature of thin SWNT
networks.
</summary>
    <author>
      <name>C Morgan</name>
    </author>
    <author>
      <name>M Baxendale</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 8 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0905.1821v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.1821v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mes-hall" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.4087v1</id>
    <updated>2012-02-18T17:34:19Z</updated>
    <published>2012-02-18T17:34:19Z</published>
    <title>Epidemic spreading on interconnected networks</title>
    <summary>  Many real networks are not isolated from each other but form networks of
networks, often interrelated in non trivial ways. Here, we analyze an epidemic
spreading process taking place on top of two interconnected complex networks.
We develop a heterogeneous mean field approach that allows us to calculate the
conditions for the emergence of an endemic state. Interestingly, a global
endemic state may arise in the coupled system even though the epidemics is not
able to propagate on each network separately, and even when the number of
coupling connections is small. Our analytic results are successfully confronted
against large-scale numerical simulations.
</summary>
    <author>
      <name>Anna Saumell-Mendiola</name>
    </author>
    <author>
      <name>M. √Ångeles Serrano</name>
    </author>
    <author>
      <name>Mari√°n Bogu√±√°</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.86.026106</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.86.026106" rel="related"/>
    <link href="http://arxiv.org/abs/1202.4087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.4087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.7297v2</id>
    <updated>2015-01-02T04:36:08Z</updated>
    <published>2014-12-23T09:51:42Z</published>
    <title>Origin and implications of zero degeneracy in networks spectra</title>
    <summary>  Spectra of real world networks exhibit properties which are different from
the random networks. One such property is the existence of a very high
degeneracy at zero eigenvalues. In this work, we provide possible reasons
behind occurrence of the zero degeneracy in various networks spectra.
Comparison of zero degeneracy in protein-protein interaction networks of six
different species and in their corresponding model networks sheds light in
understanding the evolution of complex biological systems.
</summary>
    <author>
      <name>Alok Yadav</name>
    </author>
    <author>
      <name>Sarika Jalan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/1.4917286</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/1.4917286" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.7297v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.7297v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.08355v2</id>
    <updated>2017-04-13T09:12:05Z</updated>
    <published>2016-09-27T11:20:26Z</published>
    <title>Analysing degeneracies in networks spectra</title>
    <summary>  Many real-world networks exhibit a high degeneracy at few eigenvalues. We
show that a simple transformation of the network's adjacency matrix provides an
understanding of the origins of occurrence of high multiplicities in the
networks spectra. We find that the eigenvectors associated with the degenerate
eigenvalues shed light on the structures contributing to the degeneracy. Since
these degeneracies are rarely observed in model graphs, we present results for
various cancer networks. This approach gives an opportunity to search for
structures contributing to degeneracy which might have an important role in a
network.
</summary>
    <author>
      <name>Lo√Øc Marrec</name>
    </author>
    <author>
      <name>Sarika Jalan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1209/0295-5075/117/48001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1209/0295-5075/117/48001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures and Supplementary Material</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPL 117, 48001 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1609.08355v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.08355v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.00513v1</id>
    <updated>2017-09-02T01:03:08Z</updated>
    <published>2017-09-02T01:03:08Z</published>
    <title>Learning Loss for Knowledge Distillation with Conditional Adversarial
  Networks</title>
    <summary>  There is an increasing interest on accelerating neural networks for real-time
applications. We study the student-teacher strategy, in which a small and fast
student network is trained with the auxiliary information provided by a large
and accurate teacher network. We use conditional adversarial networks to learn
the loss function to transfer knowledge from teacher to student. The proposed
method is particularly effective for relatively small student networks.
Moreover, experimental results show the effect of network size when the modern
networks are used as student. We empirically study trade-off between inference
time and classification accuracy, and provide suggestions on choosing a proper
student.
</summary>
    <author>
      <name>Zheng Xu</name>
    </author>
    <author>
      <name>Yen-Chang Hsu</name>
    </author>
    <author>
      <name>Jiawei Huang</name>
    </author>
    <link href="http://arxiv.org/abs/1709.00513v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.00513v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.08167v2</id>
    <updated>2017-08-07T16:18:24Z</updated>
    <published>2017-07-25T19:22:55Z</published>
    <title>On The Robustness of a Neural Network</title>
    <summary>  With the development of neural networks based machine learning and their
usage in mission critical applications, voices are rising against the
\textit{black box} aspect of neural networks as it becomes crucial to
understand their limits and capabilities. With the rise of neuromorphic
hardware, it is even more critical to understand how a neural network, as a
distributed system, tolerates the failures of its computing nodes, neurons, and
its communication channels, synapses. Experimentally assessing the robustness
of neural networks involves the quixotic venture of testing all the possible
failures, on all the possible inputs, which ultimately hits a combinatorial
explosion for the first, and the impossibility to gather all the possible
inputs for the second.
  In this paper, we prove an upper bound on the expected error of the output
when a subset of neurons crashes. This bound involves dependencies on the
network parameters that can be seen as being too pessimistic in the average
case. It involves a polynomial dependency on the Lipschitz coefficient of the
neurons activation function, and an exponential dependency on the depth of the
layer where a failure occurs. We back up our theoretical results with
experiments illustrating the extent to which our prediction matches the
dependencies between the network parameters and robustness. Our results show
that the robustness of neural networks to the average crash can be estimated
without the need to neither test the network on all failure configurations, nor
access the training set used to train the network, both of which are
practically impossible requirements.
</summary>
    <author>
      <name>El Mahdi El Mhamdi</name>
    </author>
    <author>
      <name>Rachid Guerraoui</name>
    </author>
    <author>
      <name>Sebastien Rouault</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36th IEEE International Symposium on Reliable Distributed Systems 26
  - 29 September 2017. Hong Kong, China</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.08167v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.08167v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/astro-ph/9906201v1</id>
    <updated>1999-06-11T13:39:56Z</updated>
    <published>1999-06-11T13:39:56Z</published>
    <title>Gamma-Hadron Discrimination with a Neural Network in the ARGO-YBJ
  Experiment</title>
    <summary>  The structure of a neural network developed for the gamma hadron separation
in the ARGO-YBJ detector is presented. The discrimination power in the full
ARGO-YBJ energy range is shown in detail and the improvement in the detector
sensitivity is also discussed.
</summary>
    <author>
      <name>ARGO-YBJ Collaboration</name>
    </author>
    <author>
      <name> :</name>
    </author>
    <author>
      <name>presented by S. Bussino</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures, to appear in the proceedings of the 26th
  International Cosmic Ray Conference (Salt Lake City, 1999)</arxiv:comment>
    <link href="http://arxiv.org/abs/astro-ph/9906201v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/astro-ph/9906201v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/astro-ph/0611027v1</id>
    <updated>2006-11-01T15:46:02Z</updated>
    <published>2006-11-01T15:46:02Z</published>
    <title>Proposal: The Neural Network Telescope</title>
    <summary>  A neural network mechanism that can compensate for poor optical quality was
recently discovered in a biological context. We propose that this mechanism can
and should be adopted for astronomical purposes. This would shift emphasis away
from the quality of the optical equipment to information processing, hence
should decrease the cost and make larger instruments feasible.
</summary>
    <author>
      <name>ƒ∞brahim Semiz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LaTeX, 6 pages, one figure included in the LaTeX code</arxiv:comment>
    <link href="http://arxiv.org/abs/astro-ph/0611027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/astro-ph/0611027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9710010v2</id>
    <updated>1997-12-11T11:20:11Z</updated>
    <published>1997-10-01T13:39:08Z</published>
    <title>Limit cycles of a perceptron</title>
    <summary>  An artificial neural network can be used to generate a series of numbers. A
boolean perceptron generates bit sequences with a periodic structure. The
corresponding spectrum of cycle lengths is investigated analytically and
numerically; it has similarities with properties of rational numbers.
</summary>
    <author>
      <name>M. Schroeder</name>
    </author>
    <author>
      <name>W. Kinzel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/0305-4470/31/13/006</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/0305-4470/31/13/006" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LaTeX and 4 postscript pages of figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9710010v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9710010v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0302404v1</id>
    <updated>2003-02-20T10:31:06Z</updated>
    <published>2003-02-20T10:31:06Z</published>
    <title>Hamiltonian and gradient properties of certain type of dynamical systems</title>
    <summary>  From the sandpoint of neural network dynamics we consider dynamical system of
special type pesesses gradient (symmetric) and Hamiltonian (antisymmetric)
flows. The conditions when Hamiltonian flow properties are dominant in the
system are considered. A simple Hamiltonian has been studied for establishing
oscillatory patern conditions in system under consideration.
</summary>
    <author>
      <name>A. K. Prykarpatsky</name>
    </author>
    <author>
      <name>V. V. Gafiychuk</name>
    </author>
    <link href="http://arxiv.org/abs/cond-mat/0302404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0302404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0408668v1</id>
    <updated>2004-08-30T21:34:44Z</updated>
    <published>2004-08-30T21:34:44Z</published>
    <title>How to guess the inter magnetic bubble potential by using a simple
  perceptron ?</title>
    <summary>  It is shown that magnetic bubble films behaviour can be described by using a
2D super-Ising hamiltonian. Calculated hysteresis curves and magnetic domain
patterns are successfully compared with experimental results taken in
literature. The reciprocal problem of finding paramaters of the super-Ising
model to reproduce computed or experimental magnetic domain pictures is solved
by using a perceptron neural network.
</summary>
    <author>
      <name>S. Padovani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures, pre-print</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0408668v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0408668v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/hep-ph/9401343v1</id>
    <updated>1994-01-27T19:26:00Z</updated>
    <published>1994-01-27T19:26:00Z</published>
    <title>Higgs Search by Neural Networks at LHC</title>
    <summary>  We show that neural network classifiers can be used to discriminate Higgs
production from background at LHC for $ 150&lt; M_H&lt;200$ GeV. The results compare
favourably with ordinary multivariate analysis.
</summary>
    <author>
      <name>P. Chiappetta</name>
    </author>
    <author>
      <name>P. Colangelo</name>
    </author>
    <author>
      <name>P. De Felice</name>
    </author>
    <author>
      <name>G. Nardulli</name>
    </author>
    <author>
      <name>G. Pasquariello</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/0370-2693(94)91110-X</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/0370-2693(94)91110-X" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LaTex, 9 pages, 1 figure (Postscript file included), CPT-93/PE 2969,
  BARI-TH/159-93</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys.Lett. B322 (1994) 219-223</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/hep-ph/9401343v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/hep-ph/9401343v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/math/0212122v1</id>
    <updated>2002-12-09T16:36:30Z</updated>
    <published>2002-12-09T16:36:30Z</published>
    <title>A Proposed Artificial Neural Network Classifier to Identify Tumor
  Metastases</title>
    <summary>  In this paper we propose a classification scheme to isolate truly benign
tumors from those that initially start off as benign but subsequently show
metastases. A non-parametric artificial neural network methodology has been
chosen because of the analytical difficulties associated with extraction of
closed-form stochastic-likelihood parameters given the extremely complicated
and possibly non-linear behavior of the state variables.
</summary>
    <author>
      <name>M. Khoshnevisan</name>
    </author>
    <author>
      <name>Sukanto Bhattacharya</name>
    </author>
    <author>
      <name>Florentin Smarandache</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/math/0212122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/math/0212122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.GM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.GM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60G35, 03B52" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/physics/0506202v1</id>
    <updated>2005-06-28T15:36:25Z</updated>
    <published>2005-06-28T15:36:25Z</published>
    <title>Electron/Pion Identification with ALICE TRD Prototypes using a Neural
  Network Algorithm</title>
    <summary>  We study the electron/pion identification performance of the ALICE Transition
Radiation Detector (TRD) prototypes using a neural network (NN) algorithm.
Measurements were carried out for particle momenta from 2 to 6 GeV/c. An
improvement in pion rejection by about a factor of 3 is obtained with NN
compared to standard likelihood methods.
</summary>
    <author>
      <name> ALICE TRD Collaboration</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 9 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/physics/0506202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/physics/0506202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/q-bio/0401001v1</id>
    <updated>2003-12-31T21:47:22Z</updated>
    <published>2003-12-31T21:47:22Z</published>
    <title>K-Winners-Take-All Computation with Neural Oscillators</title>
    <summary>  Artificial spike-based computation, inspired by models of computation in the
central nervous system, may present significant performance advantages over
traditional methods for specific types of large scale problems. This paper
describes very simple network architectures for k-winners-take-all and
soft-winner-take-all computation using neural oscillators. Fast convergence is
achieved from arbitrary initial conditions, which makes the networks
particularly suitable to track time-varying inputs.
</summary>
    <author>
      <name>Wei Wang</name>
    </author>
    <author>
      <name>Jean-Jacques E. Slotine</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/q-bio/0401001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/q-bio/0401001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.3370v1</id>
    <updated>2007-05-23T13:30:40Z</updated>
    <published>2007-05-23T13:30:40Z</published>
    <title>Adaptive classification of temporal signals in fixed-weights recurrent
  neural networks: an existence proof</title>
    <summary>  We address the important theoretical question why a recurrent neural network
with fixed weights can adaptively classify time-varied signals in the presence
of additive noise and parametric perturbations. We provide a mathematical proof
assuming that unknown parameters are allowed to enter the signal nonlinearly
and the noise amplitude is sufficiently small.
</summary>
    <author>
      <name>Ivan Tyukin</name>
    </author>
    <author>
      <name>Danil Prokhorov</name>
    </author>
    <author>
      <name>Cees van Leeuwen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0705.3370v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.3370v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="82C32; 35B40; 37C70; 68T05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0706.0300v1</id>
    <updated>2007-06-03T05:17:38Z</updated>
    <published>2007-06-03T05:17:38Z</published>
    <title>Automatic Detection of Pulmonary Embolism using Computational
  Intelligence</title>
    <summary>  This article describes the implementation of a system designed to
automatically detect the presence of pulmonary embolism in lung scans. These
images are firstly segmented, before alignment and feature extraction using
PCA. The neural network was trained using the Hybrid Monte Carlo method,
resulting in a committee of 250 neural networks and good results are obtained.
</summary>
    <author>
      <name>Simon Scurrell</name>
    </author>
    <author>
      <name>Tshilidzi Marwala</name>
    </author>
    <author>
      <name>David Rubin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0706.0300v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0706.0300v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.4221v1</id>
    <updated>2014-11-16T06:39:23Z</updated>
    <published>2014-11-16T06:39:23Z</published>
    <title>A dynamic mechanism of Alzheimer based on artificial neural network</title>
    <summary>  In this paper, we provide another angle to analyze the reasons why Alzheimer
Disease exists. We analyze the dynamic mechanism of Alzheimer Disease based on
the cognitive model that established from artificial neural network. We can
provide some theoretic explanations to Alzheimer Disease through the analyzing
of this model.
</summary>
    <author>
      <name>Zhi Cheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.4221v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.4221v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.3798v1</id>
    <updated>2011-10-17T20:01:31Z</updated>
    <published>2011-10-17T20:01:31Z</published>
    <title>Studying Deeply Virtual Compton Scattering with Neural Networks</title>
    <summary>  Neural networks are utilized to fit Compton form factor H to HERMES data on
deeply virtual Compton scattering off unpolarized protons. We used this result
to predict the beam charge-spin assymetry for muon scattering off proton at the
kinematics of the COMPASS II experiment.
</summary>
    <author>
      <name>Kresimir Kumericki</name>
    </author>
    <author>
      <name>Dieter Mueller</name>
    </author>
    <author>
      <name>Andreas Schafer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LaTeX, 7 pages, 4 figures, Talk by K.K. at PHOTON 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.3798v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.3798v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.1958v1</id>
    <updated>2011-12-08T21:01:23Z</updated>
    <published>2011-12-08T21:01:23Z</published>
    <title>Parametrizing Compton form factors with neural networks</title>
    <summary>  We describe a method, based on neural networks, of revealing Compton form
factors in the deeply virtual region. We compare this approach to standard
least-squares model fitting both for a simplified toy case and for HERMES data.
</summary>
    <author>
      <name>Kresimir Kumericki</name>
    </author>
    <author>
      <name>Dieter Mueller</name>
    </author>
    <author>
      <name>Andreas Schafer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.nuclphysbps.2012.03.020</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.nuclphysbps.2012.03.020" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LaTeX, 6 pages, 3 figures, Presented by K.K. at Ringberg HERA
  workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1112.1958v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.1958v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.5853v4</id>
    <updated>2014-02-18T21:35:13Z</updated>
    <published>2013-12-20T08:45:07Z</published>
    <title>Multi-GPU Training of ConvNets</title>
    <summary>  In this work we evaluate different approaches to parallelize computation of
convolutional neural networks across several GPUs.
</summary>
    <author>
      <name>Omry Yadan</name>
    </author>
    <author>
      <name>Keith Adams</name>
    </author>
    <author>
      <name>Yaniv Taigman</name>
    </author>
    <author>
      <name>Marc'Aurelio Ranzato</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Learning, Deep Learning, Convolutional Networks, Computer
  Vision, GPU, CUDA</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.5853v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.5853v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.02890v2</id>
    <updated>2015-08-25T15:12:37Z</updated>
    <published>2015-05-12T07:30:22Z</published>
    <title>Sparse 3D convolutional neural networks</title>
    <summary>  We have implemented a convolutional neural network designed for processing
sparse three-dimensional input data. The world we live in is three dimensional
so there are a large number of potential applications including 3D object
recognition and analysis of space-time objects. In the quest for efficiency, we
experiment with CNNs on the 2D triangular-lattice and 3D tetrahedral-lattice.
</summary>
    <author>
      <name>Ben Graham</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">BMVC 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.02890v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.02890v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.04826v2</id>
    <updated>2015-08-26T12:59:38Z</updated>
    <published>2015-08-19T23:02:37Z</published>
    <title>Dither is Better than Dropout for Regularising Deep Neural Networks</title>
    <summary>  Regularisation of deep neural networks (DNN) during training is critical to
performance. By far the most popular method is known as dropout. Here, cast
through the prism of signal processing theory, we compare and contrast the
regularisation effects of dropout with those of dither. We illustrate some
serious inherent limitations of dropout and demonstrate that dither provides a
more effective regulariser.
</summary>
    <author>
      <name>Andrew J. R. Simpson</name>
    </author>
    <link href="http://arxiv.org/abs/1508.04826v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.04826v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.07303v1</id>
    <updated>2015-10-25T21:04:12Z</updated>
    <published>2015-10-25T21:04:12Z</published>
    <title>A Framework for Distributed Deep Learning Layer Design in Python</title>
    <summary>  In this paper, a framework for testing Deep Neural Network (DNN) design in
Python is presented. First, big data, machine learning (ML), and Artificial
Neural Networks (ANNs) are discussed to familiarize the reader with the
importance of such a system. Next, the benefits and detriments of implementing
such a system in Python are presented. Lastly, the specifics of the system are
explained, and some experimental results are presented to prove the
effectiveness of the system.
</summary>
    <author>
      <name>Clay McLeod</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.07303v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.07303v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.07641v2</id>
    <updated>2017-03-21T21:28:55Z</updated>
    <published>2015-10-26T20:18:56Z</published>
    <title>Phenotyping of Clinical Time Series with LSTM Recurrent Neural Networks</title>
    <summary>  We present a novel application of LSTM recurrent neural networks to
multilabel classification of diagnoses given variable-length time series of
clinical measurements. Our method outperforms a strong baseline on a variety of
metrics.
</summary>
    <author>
      <name>Zachary C. Lipton</name>
    </author>
    <author>
      <name>David C. Kale</name>
    </author>
    <author>
      <name>Randall C. Wetzel</name>
    </author>
    <link href="http://arxiv.org/abs/1510.07641v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.07641v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.07529v1</id>
    <updated>2015-12-23T15:58:05Z</updated>
    <published>2015-12-23T15:58:05Z</published>
    <title>A New PID Neural Network Controller Design for Nonlinear Processes</title>
    <summary>  In this paper, a novel adaptive tuning method of PID neural network (PIDNN)
controller for nonlinear process is proposed. The method utilizes an improved
gradient descent method to adjust PIDNN parameters where the margin stability
will be employed to get high tracking performance and robustness with regard to
external load disturbance and parameter variation. Simulation results show the
effectiveness of the proposed algorithm compared with other well-known learning
methods.
</summary>
    <author>
      <name>Ali Zribi</name>
    </author>
    <author>
      <name>Mohamed Chtourou</name>
    </author>
    <author>
      <name>Mohamed Djemel</name>
    </author>
    <link href="http://arxiv.org/abs/1512.07529v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.07529v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06650v1</id>
    <updated>2016-04-22T13:33:08Z</updated>
    <published>2016-04-22T13:33:08Z</published>
    <title>Detecting state of aggression in sentences using CNN</title>
    <summary>  In this article we study verbal expression of aggression and its detection
using machine learning and neural networks methods. We test our results using
our corpora of messages from anonymous imageboards. We also compare Random
forest classifier with convolutional neural network for "Movie reviews with one
sentence per review" corpus.
</summary>
    <author>
      <name>Rodmonga Potapova</name>
    </author>
    <author>
      <name>Denis Gordeev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted for SPECOM-2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.06650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.09081v1</id>
    <updated>2016-05-30T00:50:39Z</updated>
    <published>2016-05-30T00:50:39Z</published>
    <title>Understanding Convolutional Neural Networks</title>
    <summary>  Convoulutional Neural Networks (CNNs) exhibit extraordinary performance on a
variety of machine learning tasks. However, their mathematical properties and
behavior are quite poorly understood. There is some work, in the form of a
framework, for analyzing the operations that they perform. The goal of this
project is to present key results from this theory, and provide intuition for
why CNNs work.
</summary>
    <author>
      <name>Jayanth Koushik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Statistical Machine Learning Course Project at Carnegie Mellon
  University</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.09081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04503v1</id>
    <updated>2016-06-14T19:00:59Z</updated>
    <published>2016-06-14T19:00:59Z</published>
    <title>Shallow Discourse Parsing Using Distributed Argument Representations and
  Bayesian Optimization</title>
    <summary>  This paper describes the Georgia Tech team's approach to the CoNLL-2016
supplementary evaluation on discourse relation sense classification. We use
long short-term memories (LSTM) to induce distributed representations of each
argument, and then combine these representations with surface features in a
neural network. The architecture of the neural network is determined by
Bayesian hyperparameter search.
</summary>
    <author>
      <name> Akanksha</name>
    </author>
    <author>
      <name>Jacob Eisenstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">describes our system at the CoNLL 2016 shared task</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.04503v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04503v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04485v1</id>
    <updated>2016-08-16T05:04:46Z</updated>
    <published>2016-08-16T05:04:46Z</published>
    <title>Authorship clustering using multi-headed recurrent neural networks</title>
    <summary>  A recurrent neural network that has been trained to separately model the
language of several documents by unknown authors is used to measure similarity
between the documents. It is able to find clues of common authorship even when
the documents are very short and about disparate topics. While it is easy to
make statistically significant predictions regarding authorship, it is
difficult to group documents into definite clusters with high accuracy.
</summary>
    <author>
      <name>Douglas Bagnall</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 5 figures; notebook for PAN@CLEF 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.04485v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04485v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03296v1</id>
    <updated>2016-09-12T07:50:41Z</updated>
    <published>2016-09-12T07:50:41Z</published>
    <title>A Neural Network Alternative to Non-Negative Audio Models</title>
    <summary>  We present a neural network that can act as an equivalent to a Non-Negative
Matrix Factorization (NMF), and further show how it can be used to perform
supervised source separation. Due to the extensibility of this approach we show
how we can achieve better source separation performance as compared to
NMF-based methods, and propose a variety of derivative architectures that can
be used for further improvements.
</summary>
    <author>
      <name>Paris Smaragdis</name>
    </author>
    <author>
      <name>Shrikant Venkataramani</name>
    </author>
    <link href="http://arxiv.org/abs/1609.03296v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03296v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.00723v1</id>
    <updated>2017-02-01T18:32:12Z</updated>
    <published>2017-02-01T18:32:12Z</published>
    <title>Handwritten Recognition Using SVM, KNN and Neural Network</title>
    <summary>  Handwritten recognition (HWR) is the ability of a computer to receive and
interpret intelligible handwritten input from source such as paper documents,
photographs, touch-screens and other devices. In this paper we will using three
(3) classification t o re cognize the handwritten which is SVM, KNN and Neural
Network.
</summary>
    <author>
      <name>Norhidayu Abdul Hamid</name>
    </author>
    <author>
      <name>Nilam Nur Amir Sjarif</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages ; 22 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.00723v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.00723v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.07006v1</id>
    <updated>2017-02-22T21:03:49Z</updated>
    <published>2017-02-22T21:03:49Z</published>
    <title>Synthesising Dynamic Textures using Convolutional Neural Networks</title>
    <summary>  Here we present a parametric model for dynamic textures. The model is based
on spatiotemporal summary statistics computed from the feature representations
of a Convolutional Neural Network (CNN) trained on object recognition. We
demonstrate how the model can be used to synthesise new samples of dynamic
textures and to predict motion in simple movies.
</summary>
    <author>
      <name>Christina M. Funke</name>
    </author>
    <author>
      <name>Leon A. Gatys</name>
    </author>
    <author>
      <name>Alexander S. Ecker</name>
    </author>
    <author>
      <name>Matthias Bethge</name>
    </author>
    <link href="http://arxiv.org/abs/1702.07006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.07006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.05465v1</id>
    <updated>2017-03-16T03:15:22Z</updated>
    <published>2017-03-16T03:15:22Z</published>
    <title>Neobility at SemEval-2017 Task 1: An Attention-based Sentence Similarity
  Model</title>
    <summary>  This paper describes a neural-network model which performed competitively
(top 6) at the SemEval 2017 cross-lingual Semantic Textual Similarity (STS)
task. Our system employs an attention-based recurrent neural network model that
optimizes the sentence similarity. In this paper, we describe our participation
in the multilingual STS task which measures similarity across English, Spanish,
and Arabic.
</summary>
    <author>
      <name>Wenli Zhuang</name>
    </author>
    <author>
      <name>Ernie Chang</name>
    </author>
    <link href="http://arxiv.org/abs/1703.05465v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.05465v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07250v3</id>
    <updated>2017-06-02T14:58:53Z</updated>
    <published>2017-05-20T02:54:52Z</published>
    <title>Speedup from a different parametrization within the Neural Network
  algorithm</title>
    <summary>  A different parametrization of the hyperplanes is used in the neural network
algorithm. As demonstrated on several autoencoder examples it significantly
outperforms the usual parametrization, reaching lower training error values
with only a fraction of the number of epochs. It's argued that it makes it
easier to understand and initialize the parameters.
</summary>
    <author>
      <name>Michael F. Zimmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.07250v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07250v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.07719v2</id>
    <updated>2017-08-16T06:37:55Z</updated>
    <published>2017-07-24T19:39:22Z</published>
    <title>Global Normalization of Convolutional Neural Networks for Joint Entity
  and Relation Classification</title>
    <summary>  We introduce globally normalized convolutional neural networks for joint
entity classification and relation extraction. In particular, we propose a way
to utilize a linear-chain conditional random field output layer for predicting
entity types and relations between entities at the same time. Our experiments
show that global normalization outperforms a locally normalized softmax layer
on a benchmark dataset.
</summary>
    <author>
      <name>Heike Adel</name>
    </author>
    <author>
      <name>Hinrich Sch√ºtze</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.07719v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.07719v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09564v2</id>
    <updated>2018-02-23T22:30:45Z</updated>
    <published>2017-07-29T22:36:35Z</published>
    <title>A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for
  Neural Networks</title>
    <summary>  We present a generalization bound for feedforward neural networks in terms of
the product of the spectral norm of the layers and the Frobenius norm of the
weights. The generalization bound is derived using a PAC-Bayes analysis.
</summary>
    <author>
      <name>Behnam Neyshabur</name>
    </author>
    <author>
      <name>Srinadh Bhojanapalli</name>
    </author>
    <author>
      <name>Nathan Srebro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ICLR 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.09564v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09564v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09869v1</id>
    <updated>2017-07-23T01:35:55Z</updated>
    <published>2017-07-23T01:35:55Z</published>
    <title>A comment on the paper Prediction of Kidney Function from Biopsy Images
  using Convolutional Neural Networks</title>
    <summary>  This letter presente a comment on the paper Prediction of Kidney Function
from Biopsy Images using Convolutional Neural Networks by Ledbetter et al.
(2017)
</summary>
    <author>
      <name>Washington LC dos-Santos</name>
    </author>
    <author>
      <name>Angelo A Duarte</name>
    </author>
    <author>
      <name>Luiz AR de Freitas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.09869v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09869v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.05867v1</id>
    <updated>2017-09-18T11:28:15Z</updated>
    <published>2017-09-18T11:28:15Z</published>
    <title>Combinational neural network using Gabor filters for the classification
  of handwritten digits</title>
    <summary>  A classification algorithm that combines the components of k-nearest
neighbours and multilayer neural networks has been designed and tested. With
this method the computational time required for training the dataset has been
reduced substancially. Gabor filters were used for the feature extraction to
ensure a better performance. This algorithm is tested with MNIST dataset and it
will be integrated as a module in the object recognition software which is
currently under development.
</summary>
    <author>
      <name>N. Joshi</name>
    </author>
    <link href="http://arxiv.org/abs/1709.05867v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.05867v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.09862v1</id>
    <updated>2017-09-28T09:17:17Z</updated>
    <published>2017-09-28T09:17:17Z</published>
    <title>Applying Neural Networks in Optical Communication Systems: Possible
  Pitfalls</title>
    <summary>  We investigate the risk of overestimating the performance gain when applying
neural network based receivers in systems with pseudo random bit sequences or
with limited memory depths, resulting in repeated short patterns. We show that
with such sequences, a large artificial gain can be obtained which comes from
pattern prediction rather than predicting or compensating the studied
channel/phenomena.
</summary>
    <author>
      <name>Tobias A. Eriksson</name>
    </author>
    <author>
      <name>Henning B√ºlow</name>
    </author>
    <author>
      <name>Andreas Leven</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LPT.2017.2755663</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LPT.2017.2755663" rel="related"/>
    <link href="http://arxiv.org/abs/1709.09862v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.09862v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.06234v1</id>
    <updated>2017-10-17T12:23:26Z</updated>
    <published>2017-10-17T12:23:26Z</published>
    <title>Nonlinear Interference Mitigation via Deep Neural Networks</title>
    <summary>  A neural-network-based approach is presented to efficiently implement digital
backpropagation (DBP). For a 32x100 km fiber-optic link, the resulting
"learned" DBP significantly reduces the complexity compared to conventional DBP
implementations.
</summary>
    <author>
      <name>Christian H√§ger</name>
    </author>
    <author>
      <name>Henry D. Pfister</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.06234v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.06234v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.08721v1</id>
    <updated>2017-10-24T12:11:07Z</updated>
    <published>2017-10-24T12:11:07Z</published>
    <title>Clickbait Identification using Neural Networks</title>
    <summary>  This paper presents the results of our participation in the Clickbait
Detection Challenge 2017. The system relies on a fusion of neural networks,
incorporating different types of available informations. It does not require
any linguistic preprocessing, and hence generalizes more easily to new domains
and languages. The final combined model achieves a mean squared error of
0.0428, an accuracy of 0.826, and a F1 score of 0.564. According to the
official evaluation metric the system ranked 6th of the 13 participating teams.
</summary>
    <author>
      <name>Philippe Thomas</name>
    </author>
    <link href="http://arxiv.org/abs/1710.08721v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.08721v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.05238v1</id>
    <updated>2017-11-14T18:24:41Z</updated>
    <published>2017-11-14T18:24:41Z</published>
    <title>Quantum parameter estimation with a neural network</title>
    <summary>  We propose to use neural networks to estimate the rates of coherent and
incoherent processes in quantum systems from continuous measurement records. In
particular, we adapt an image recognition algorithm to recognize the patterns
in experimental signals and link them to physical quantities. We demonstrate
that the parameter estimation works unabatedly in the presence of detector
imperfections which complicate or rule out Bayesian filter analyses.
</summary>
    <author>
      <name>Eliska Greplova</name>
    </author>
    <author>
      <name>Christian Kraglund Andersen</name>
    </author>
    <author>
      <name>Klaus M√∏lmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.05238v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.05238v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.00580v1</id>
    <updated>2017-12-02T09:48:37Z</updated>
    <published>2017-12-02T09:48:37Z</published>
    <title>Fruit recognition from images using deep learning</title>
    <summary>  In this paper we introduce a new, high-quality, dataset of images containing
fruits. We also present the results of some numerical experiment for training a
neural network to detect fruits. We discuss the reason why we chose to use
fruits in this project by proposing a few applications that could use this kind
of neural network.
</summary>
    <author>
      <name>Horea Mure≈üan</name>
    </author>
    <author>
      <name>Mihai Oltean</name>
    </author>
    <link href="http://arxiv.org/abs/1712.00580v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.00580v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.03605v1</id>
    <updated>2017-12-10T22:57:42Z</updated>
    <published>2017-12-10T22:57:42Z</published>
    <title>Sensitivity Analysis for Predictive Uncertainty in Bayesian Neural
  Networks</title>
    <summary>  We derive a novel sensitivity analysis of input variables for predictive
epistemic and aleatoric uncertainty. We use Bayesian neural networks with
latent variables as a model class and illustrate the usefulness of our
sensitivity analysis on real-world datasets. Our method increases the
interpretability of complex black-box probabilistic models.
</summary>
    <author>
      <name>Stefan Depeweg</name>
    </author>
    <author>
      <name>Jos√© Miguel Hern√°ndez-Lobato</name>
    </author>
    <author>
      <name>Steffen Udluft</name>
    </author>
    <author>
      <name>Thomas Runkler</name>
    </author>
    <link href="http://arxiv.org/abs/1712.03605v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.03605v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.07686v2</id>
    <updated>2018-02-19T08:55:29Z</updated>
    <published>2017-12-20T19:53:23Z</published>
    <title>Pseudorehearsal in actor-critic agents with neural network function
  approximation</title>
    <summary>  Catastrophic forgetting has a significant negative impact in reinforcement
learning. The purpose of this study is to investigate how pseudorehearsal can
change performance of an actor-critic agent with neural-network function
approximation. We tested agent in a pole balancing task and compared different
pseudorehearsal approaches. We have found that pseudorehearsal can assist
learning and decrease forgetting.
</summary>
    <author>
      <name>Vladimir Marochko</name>
    </author>
    <author>
      <name>Leonard Johard</name>
    </author>
    <author>
      <name>Manuel Mazzara</name>
    </author>
    <author>
      <name>Luca Longo</name>
    </author>
    <link href="http://arxiv.org/abs/1712.07686v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.07686v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.08988v1</id>
    <updated>2018-02-25T11:15:31Z</updated>
    <published>2018-02-25T11:15:31Z</published>
    <title>Deep Neural Network for Learning to Rank Query-Text Pairs</title>
    <summary>  This paper considers the problem of document ranking in information retrieval
systems by Learning to Rank. We propose ConvRankNet combining a Siamese
Convolutional Neural Network encoder and the RankNet ranking model which could
be trained in an end-to-end fashion. We prove a general result justifying the
linear test-time complexity of pairwise Learning to Rank approach. Experiments
on the OHSUMED dataset show that ConvRankNet outperforms systematically
existing feature-based models.
</summary>
    <author>
      <name>Baoyang Song</name>
    </author>
    <link href="http://arxiv.org/abs/1802.08988v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.08988v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06514v1</id>
    <updated>2016-07-21T21:51:58Z</updated>
    <published>2016-07-21T21:51:58Z</published>
    <title>Geometric Neural Phrase Pooling: Modeling the Spatial Co-occurrence of
  Neurons</title>
    <summary>  Deep Convolutional Neural Networks (CNNs) are playing important roles in
state-of-the-art visual recognition. This paper focuses on modeling the spatial
co-occurrence of neuron responses, which is less studied in the previous work.
For this, we consider the neurons in the hidden layer as neural words, and
construct a set of geometric neural phrases on top of them. The idea that
grouping neural words into neural phrases is borrowed from the
Bag-of-Visual-Words (BoVW) model. Next, the Geometric Neural Phrase Pooling
(GNPP) algorithm is proposed to efficiently encode these neural phrases. GNPP
acts as a new type of hidden layer, which punishes the isolated neuron
responses after convolution, and can be inserted into a CNN model with little
extra computational overhead. Experimental results show that GNPP produces
significant and consistent accuracy gain in image classification.
</summary>
    <author>
      <name>Lingxi Xie</name>
    </author>
    <author>
      <name>Qi Tian</name>
    </author>
    <author>
      <name>John Flynn</name>
    </author>
    <author>
      <name>Jingdong Wang</name>
    </author>
    <author>
      <name>Alan Yuille</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear, in ECCV 2016 (18 pages, 4 figures)</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.06514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.0196v2</id>
    <updated>2011-03-23T10:24:15Z</updated>
    <published>2010-12-01T14:04:35Z</published>
    <title>Coarse Graining for Synchronization in Directed Networks</title>
    <summary>  Coarse graining model is a promising way to analyze and visualize large-scale
networks. The coarse-grained networks are required to preserve the same
statistical properties as well as the dynamic behaviors as the initial
networks. Some methods have been proposed and found effective in undirected
networks, while the study on coarse graining in directed networks lacks of
consideration. In this paper, we proposed a Topology-aware Coarse Graining
(TCG) method to coarse grain the directed networks. Performing the linear
stability analysis of synchronization and numerical simulation of the Kuramoto
model on four kinds of directed networks, including tree-like networks and
variants of Barab\'{a}si-Albert networks, Watts-Strogatz networks and
Erd\"{o}s-R\'{e}nyi networks, we find our method can effectively preserve the
network synchronizability.
</summary>
    <author>
      <name>An Zeng</name>
    </author>
    <author>
      <name>Linyuan Lu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.83.056123</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.83.056123" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physical Review E 83, 056123 (2011)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1012.0196v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.0196v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.0940v2</id>
    <updated>2011-12-20T14:34:35Z</updated>
    <published>2011-09-05T15:41:10Z</published>
    <title>Entropy rate of non-equilibrium growing networks</title>
    <summary>  New entropy measures have been recently introduced for the quantification of
the complexity of networks. Most of these entropy measures apply to static
networks or to dynamical processes defined on static complex networks. In this
paper we define the entropy rate of growing network models. This entropy rate
quantifies how many labeled networks are typically generated by the growing
network models. We analytically evaluate the difference between the entropy
rate of growing tree network models and the entropy of tree networks that have
the same asymptotic degree distribution. We find that the growing networks with
linear preferential attachment generated by dynamical models are exponentially
less than the static networks with the same degree distribution for a large
variety of relevant growing network models. We study the entropy rate for
growing network models showing structural phase transitions including models
with non-linear preferential attachment. Finally, we bring numerical evidence
that the entropy rate above and below the structural phase transitions follow a
different scaling with the network size.
</summary>
    <author>
      <name>Kun Zhao</name>
    </author>
    <author>
      <name>Arda Halu</name>
    </author>
    <author>
      <name>Simone Severini</name>
    </author>
    <author>
      <name>Ginestra Bianconi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.84.066113</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.84.066113" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">(8 pages, 4 figures)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 84, 066113 (2011)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1109.0940v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.0940v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.2109v2</id>
    <updated>2012-11-26T22:13:57Z</updated>
    <published>2012-03-09T18:25:39Z</published>
    <title>Network Cosmology</title>
    <summary>  Prediction and control of the dynamics of complex networks is a central
problem in network science. Structural and dynamical similarities of different
real networks suggest that some universal laws might accurately describe the
dynamics of these networks, albeit the nature and common origin of such laws
remain elusive. Here we show that the causal network representing the
large-scale structure of spacetime in our accelerating universe is a power-law
graph with strong clustering, similar to many complex networks such as the
Internet, social, or biological networks. We prove that this structural
similarity is a consequence of the asymptotic equivalence between the
large-scale growth dynamics of complex networks and causal networks. This
equivalence suggests that unexpectedly similar laws govern the dynamics of
complex networks and spacetime in the universe, with implications to network
science and cosmology.
</summary>
    <author>
      <name>Dmitri Krioukov</name>
    </author>
    <author>
      <name>Maksim Kitsak</name>
    </author>
    <author>
      <name>Robert S. Sinkovits</name>
    </author>
    <author>
      <name>David Rideout</name>
    </author>
    <author>
      <name>David Meyer</name>
    </author>
    <author>
      <name>Marian Boguna</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/srep00793</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/srep00793" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Nature Scientific Reports, v.2, p.793, 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1203.2109v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.2109v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.03818v1</id>
    <updated>2018-01-05T10:50:31Z</updated>
    <published>2018-01-05T10:50:31Z</published>
    <title>A Deep Generative Adversarial Architecture for Network-Wide
  Spatial-Temporal Traffic State Estimation</title>
    <summary>  This study proposes a deep generative adversarial architecture (GAA) for
network-wide spatial-temporal traffic state estimation. The GAA is able to
combine traffic flow theory with neural networks and thus improve the accuracy
of traffic state estimation. It consists of two Long Short-Term Memory Neural
Networks (LSTM NNs) which capture correlation in time and space among traffic
flow and traffic density. One of the LSTM NNs, called a discriminative network,
aims to maximize the probability of assigning correct labels to both true
traffic state matrices (i.e., traffic flow and traffic density within a given
spatial-temporal area) and the traffic state matrices generated from the other
neural network. The other LSTM NN, called a generative network, aims to
generate traffic state matrices which maximize the probability that the
discriminative network assigns true labels to them. The two LSTM NNs are
trained simultaneously such that the trained generative network can generate
traffic matrices similar to those in the training data set. Given a traffic
state matrix with missing values, we use back-propagation on three defined loss
functions to map the corrupted matrix to a latent space. The mapping vector is
then passed through the pre-trained generative network to estimate the missing
values of the corrupted matrix. The proposed GAA is compared with the existing
Bayesian network approach on loop detector data collected from Seattle,
Washington and that collected from San Diego, California. Experimental results
indicate that the GAA can achieve higher accuracy in traffic state estimation
than the Bayesian network approach.
</summary>
    <author>
      <name>Yunyi Liang</name>
    </author>
    <author>
      <name>Zhiyong Cui</name>
    </author>
    <author>
      <name>Yu Tian</name>
    </author>
    <author>
      <name>Huimiao Chen</name>
    </author>
    <author>
      <name>Yinhai Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1801.03818v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.03818v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.04838v1</id>
    <updated>2017-10-13T08:29:50Z</updated>
    <published>2017-10-13T08:29:50Z</published>
    <title>Efficient Computation in Adaptive Artificial Spiking Neural Networks</title>
    <summary>  Artificial Neural Networks (ANNs) are bio-inspired models of neural
computation that have proven highly effective. Still, ANNs lack a natural
notion of time, and neural units in ANNs exchange analog values in a
frame-based manner, a computationally and energetically inefficient form of
communication. This contrasts sharply with biological neurons that communicate
sparingly and efficiently using binary spikes. While artificial Spiking Neural
Networks (SNNs) can be constructed by replacing the units of an ANN with
spiking neurons, the current performance is far from that of deep ANNs on hard
benchmarks and these SNNs use much higher firing rates compared to their
biological counterparts, limiting their efficiency. Here we show how spiking
neurons that employ an efficient form of neural coding can be used to construct
SNNs that match high-performance ANNs and exceed state-of-the-art in SNNs on
important benchmarks, while requiring much lower average firing rates. For
this, we use spike-time coding based on the firing rate limiting adaptation
phenomenon observed in biological spiking neurons. This phenomenon can be
captured in adapting spiking neuron models, for which we derive the effective
transfer function. Neural units in ANNs trained with this transfer function can
be substituted directly with adaptive spiking neurons, and the resulting
Adaptive SNNs (AdSNNs) can carry out inference in deep neural networks using up
to an order of magnitude fewer spikes compared to previous SNNs. Adaptive
spike-time coding additionally allows for the dynamic control of neural coding
precision: we show how a simple model of arousal in AdSNNs further halves the
average required firing rate and this notion naturally extends to other forms
of attention. AdSNNs thus hold promise as a novel and efficient model for
neural computation that naturally fits to temporally continuous and
asynchronous applications.
</summary>
    <author>
      <name>Davide Zambrano</name>
    </author>
    <author>
      <name>Roeland Nusselder</name>
    </author>
    <author>
      <name>H. Steven Scholte</name>
    </author>
    <author>
      <name>Sander Bohte</name>
    </author>
    <link href="http://arxiv.org/abs/1710.04838v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.04838v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.6513v1</id>
    <updated>2014-07-24T10:06:24Z</updated>
    <published>2014-07-24T10:06:24Z</published>
    <title>Convolutional Neural Associative Memories: Massive Capacity with Noise
  Tolerance</title>
    <summary>  The task of a neural associative memory is to retrieve a set of previously
memorized patterns from their noisy versions using a network of neurons. An
ideal network should have the ability to 1) learn a set of patterns as they
arrive, 2) retrieve the correct patterns from noisy queries, and 3) maximize
the pattern retrieval capacity while maintaining the reliability in responding
to queries. The majority of work on neural associative memories has focused on
designing networks capable of memorizing any set of randomly chosen patterns at
the expense of limiting the retrieval capacity. In this paper, we show that if
we target memorizing only those patterns that have inherent redundancy (i.e.,
belong to a subspace), we can obtain all the aforementioned properties. This is
in sharp contrast with the previous work that could only improve one or two
aspects at the expense of the third. More specifically, we propose framework
based on a convolutional neural network along with an iterative algorithm that
learns the redundancy among the patterns. The resulting network has a retrieval
capacity that is exponential in the size of the network. Moreover, the
asymptotic error correction performance of our network is linear in the size of
the patterns. We then ex- tend our approach to deal with patterns lie
approximately in a subspace. This extension allows us to memorize datasets
containing natural patterns (e.g., images). Finally, we report experimental
results on both synthetic and real datasets to support our claims.
</summary>
    <author>
      <name>Amin Karbasi</name>
    </author>
    <author>
      <name>Amir Hesam Salavati</name>
    </author>
    <author>
      <name>Amin Shokrollahi</name>
    </author>
    <link href="http://arxiv.org/abs/1407.6513v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.6513v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.7881v1</id>
    <updated>2014-10-29T05:54:22Z</updated>
    <published>2014-10-29T05:54:22Z</published>
    <title>A neural circuit for navigation inspired by C. elegans Chemotaxis</title>
    <summary>  We develop an artificial neural circuit for contour tracking and navigation
inspired by the chemotaxis of the nematode Caenorhabditis elegans. In order to
harness the computational advantages spiking neural networks promise over their
non-spiking counterparts, we develop a network comprising 7-spiking neurons
with non-plastic synapses which we show is extremely robust in tracking a range
of concentrations. Our worm uses information regarding local temporal gradients
in sodium chloride concentration to decide the instantaneous path for foraging,
exploration and tracking. A key neuron pair in the C. elegans chemotaxis
network is the ASEL &amp; ASER neuron pair, which capture the gradient of
concentration sensed by the worm in their graded membrane potentials. The
primary sensory neurons for our network are a pair of artificial spiking
neurons that function as gradient detectors whose design is adapted from a
computational model of the ASE neuron pair in C. elegans. Simulations show that
our worm is able to detect the set-point with approximately four times higher
probability than the optimal memoryless Levy foraging model. We also show that
our spiking neural network is much more efficient and noise-resilient while
navigating and tracking a contour, as compared to an equivalent non-spiking
network. We demonstrate that our model is extremely robust to noise and with
slight modifications can be used for other practical applications such as
obstacle avoidance. Our network model could also be extended for use in
three-dimensional contour tracking or obstacle avoidance.
</summary>
    <author>
      <name>Shibani Santurkar</name>
    </author>
    <author>
      <name>Bipin Rajendran</name>
    </author>
    <link href="http://arxiv.org/abs/1410.7881v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.7881v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.6199v4</id>
    <updated>2014-02-19T16:33:14Z</updated>
    <published>2013-12-21T03:36:08Z</published>
    <title>Intriguing properties of neural networks</title>
    <summary>  Deep neural networks are highly expressive models that have recently achieved
state of the art performance on speech and visual recognition tasks. While
their expressiveness is the reason they succeed, it also causes them to learn
uninterpretable solutions that could have counter-intuitive properties. In this
paper we report two such properties.
  First, we find that there is no distinction between individual high level
units and random linear combinations of high level units, according to various
methods of unit analysis. It suggests that it is the space, rather than the
individual units, that contains of the semantic information in the high layers
of neural networks.
  Second, we find that deep neural networks learn input-output mappings that
are fairly discontinuous to a significant extend. We can cause the network to
misclassify an image by applying a certain imperceptible perturbation, which is
found by maximizing the network's prediction error. In addition, the specific
nature of these perturbations is not a random artifact of learning: the same
perturbation can cause a different network, that was trained on a different
subset of the dataset, to misclassify the same input.
</summary>
    <author>
      <name>Christian Szegedy</name>
    </author>
    <author>
      <name>Wojciech Zaremba</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Joan Bruna</name>
    </author>
    <author>
      <name>Dumitru Erhan</name>
    </author>
    <author>
      <name>Ian Goodfellow</name>
    </author>
    <author>
      <name>Rob Fergus</name>
    </author>
    <link href="http://arxiv.org/abs/1312.6199v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.6199v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.00019v4</id>
    <updated>2015-10-17T05:06:11Z</updated>
    <published>2015-05-29T20:16:51Z</published>
    <title>A Critical Review of Recurrent Neural Networks for Sequence Learning</title>
    <summary>  Countless learning tasks require dealing with sequential data. Image
captioning, speech synthesis, and music generation all require that a model
produce outputs that are sequences. In other domains, such as time series
prediction, video analysis, and musical information retrieval, a model must
learn from inputs that are sequences. Interactive tasks, such as translating
natural language, engaging in dialogue, and controlling a robot, often demand
both capabilities. Recurrent neural networks (RNNs) are connectionist models
that capture the dynamics of sequences via cycles in the network of nodes.
Unlike standard feedforward neural networks, recurrent networks retain a state
that can represent information from an arbitrarily long context window.
Although recurrent neural networks have traditionally been difficult to train,
and often contain millions of parameters, recent advances in network
architectures, optimization techniques, and parallel computation have enabled
successful large-scale learning with them. In recent years, systems based on
long short-term memory (LSTM) and bidirectional (BRNN) architectures have
demonstrated ground-breaking performance on tasks as varied as image
captioning, language translation, and handwriting recognition. In this survey,
we review and synthesize the research that over the past three decades first
yielded and then made practical these powerful learning models. When
appropriate, we reconcile conflicting notation and nomenclature. Our goal is to
provide a self-contained explication of the state of the art together with a
historical perspective and references to primary research.
</summary>
    <author>
      <name>Zachary C. Lipton</name>
    </author>
    <author>
      <name>John Berkowitz</name>
    </author>
    <author>
      <name>Charles Elkan</name>
    </author>
    <link href="http://arxiv.org/abs/1506.00019v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.00019v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.00149v5</id>
    <updated>2016-02-15T06:25:40Z</updated>
    <published>2015-10-01T09:03:44Z</published>
    <title>Deep Compression: Compressing Deep Neural Networks with Pruning, Trained
  Quantization and Huffman Coding</title>
    <summary>  Neural networks are both computationally intensive and memory intensive,
making them difficult to deploy on embedded systems with limited hardware
resources. To address this limitation, we introduce "deep compression", a three
stage pipeline: pruning, trained quantization and Huffman coding, that work
together to reduce the storage requirement of neural networks by 35x to 49x
without affecting their accuracy. Our method first prunes the network by
learning only the important connections. Next, we quantize the weights to
enforce weight sharing, finally, we apply Huffman coding. After the first two
steps we retrain the network to fine tune the remaining connections and the
quantized centroids. Pruning, reduces the number of connections by 9x to 13x;
Quantization then reduces the number of bits that represent each connection
from 32 to 5. On the ImageNet dataset, our method reduced the storage required
by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method
reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of
accuracy. This allows fitting the model into on-chip SRAM cache rather than
off-chip DRAM memory. Our compression method also facilitates the use of
complex neural networks in mobile applications where application size and
download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU,
compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy
efficiency.
</summary>
    <author>
      <name>Song Han</name>
    </author>
    <author>
      <name>Huizi Mao</name>
    </author>
    <author>
      <name>William J. Dally</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at ICLR 2016 (oral)</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.00149v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.00149v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.02774v1</id>
    <updated>2016-04-11T02:05:21Z</updated>
    <published>2016-04-11T02:05:21Z</published>
    <title>Reverse Engineering and Symbolic Knowledge Extraction on ≈Åukasiewicz
  Fuzzy Logics using Linear Neural Networks</title>
    <summary>  This work describes a methodology to combine logic-based systems and
connectionist systems. Our approach uses finite truth valued {\L}ukasiewicz
logic, where we take advantage of fact what in this type of logics every
connective can be define by a neuron in an artificial network having by
activation function the identity truncated to zero and one. This allowed the
injection of first-order formulas in a network architecture, and also
simplified symbolic rule extraction.
  Our method trains a neural network using Levenderg-Marquardt algorithm, where
we restrict the knowledge dissemination in the network structure. We show how
this reduces neural networks plasticity without damage drastically the learning
performance. Making the descriptive power of produced neural networks similar
to the descriptive power of {\L}ukasiewicz logic language, simplifying the
translation between symbolic and connectionist structures.
  This method is used in the reverse engineering problem of finding the formula
used on generation of a truth table for a multi-valued {\L}ukasiewicz logic.
For real data sets the method is particularly useful for attribute selection,
on binary classification problems defined using nominal attribute. After
attribute selection and possible data set completion in the resulting
connectionist model: neurons are directly representable using a disjunctive or
conjunctive formulas, in the {\L}ukasiewicz logic, or neurons are
interpretations which can be approximated by symbolic rules. This fact is
exemplified, extracting symbolic knowledge from connectionist models generated
for the data set Mushroom from UCI Machine Learning Repository.
</summary>
    <author>
      <name>Carlos Leandro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.02774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.02774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="94D04" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02699v2</id>
    <updated>2016-06-21T19:32:06Z</updated>
    <published>2016-05-09T19:11:22Z</published>
    <title>A Theoretical Analysis of Deep Neural Networks for Texture
  Classification</title>
    <summary>  We investigate the use of Deep Neural Networks for the classification of
image datasets where texture features are important for generating
class-conditional discriminative representations. To this end, we first derive
the size of the feature space for some standard textural features extracted
from the input dataset and then use the theory of Vapnik-Chervonenkis dimension
to show that hand-crafted feature extraction creates low-dimensional
representations which help in reducing the overall excess error rate. As a
corollary to this analysis, we derive for the first time upper bounds on the VC
dimension of Convolutional Neural Network as well as Dropout and Dropconnect
networks and the relation between excess error rate of Dropout and Dropconnect
networks. The concept of intrinsic dimension is used to validate the intuition
that texture-based datasets are inherently higher dimensional as compared to
handwritten digits or other object recognition datasets and hence more
difficult to be shattered by neural networks. We then derive the mean distance
from the centroid to the nearest and farthest sampling points in an
n-dimensional manifold and show that the Relative Contrast of the sample data
vanishes as dimensionality of the underlying vector space tends to infinity.
</summary>
    <author>
      <name>Saikat Basu</name>
    </author>
    <author>
      <name>Manohar Karki</name>
    </author>
    <author>
      <name>Robert DiBiano</name>
    </author>
    <author>
      <name>Supratik Mukhopadhyay</name>
    </author>
    <author>
      <name>Sangram Ganguly</name>
    </author>
    <author>
      <name>Ramakrishna Nemani</name>
    </author>
    <author>
      <name>Shreekant Gayaka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in International Joint Conference on Neural Networks, IJCNN
  2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.02699v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02699v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.02120v1</id>
    <updated>2016-11-07T15:38:39Z</updated>
    <published>2016-11-07T15:38:39Z</published>
    <title>Neural Networks Designing Neural Networks: Multi-Objective
  Hyper-Parameter Optimization</title>
    <summary>  Artificial neural networks have gone through a recent rise in popularity,
achieving state-of-the-art results in various fields, including image
classification, speech recognition, and automated control. Both the performance
and computational complexity of such models are heavily dependant on the design
of characteristic hyper-parameters (e.g., number of hidden layers, nodes per
layer, or choice of activation functions), which have traditionally been
optimized manually. With machine learning penetrating low-power mobile and
embedded areas, the need to optimize not only for performance (accuracy), but
also for implementation complexity, becomes paramount. In this work, we present
a multi-objective design space exploration method that reduces the number of
solution networks trained and evaluated through response surface modelling.
Given spaces which can easily exceed 1020 solutions, manually designing a
near-optimal architecture is unlikely as opportunities to reduce network
complexity, while maintaining performance, may be overlooked. This problem is
exacerbated by the fact that hyper-parameters which perform well on specific
datasets may yield sub-par results on others, and must therefore be designed on
a per-application basis. In our work, machine learning is leveraged by training
an artificial neural network to predict the performance of future candidate
networks. The method is evaluated on the MNIST and CIFAR-10 image datasets,
optimizing for both recognition accuracy and computational complexity.
Experimental results demonstrate that the proposed method can closely
approximate the Pareto-optimal front, while only exploring a small fraction of
the design space.
</summary>
    <author>
      <name>Sean C. Smithson</name>
    </author>
    <author>
      <name>Guang Yang</name>
    </author>
    <author>
      <name>Warren J. Gross</name>
    </author>
    <author>
      <name>Brett H. Meyer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ICCAD'16. The authoritative version will appear in the
  ACM Digital Library</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.02120v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.02120v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.01578v2</id>
    <updated>2017-07-02T14:31:37Z</updated>
    <published>2017-04-05T18:00:00Z</published>
    <title>Probing many-body localization with neural networks</title>
    <summary>  We show that a simple artificial neural network trained on entanglement
spectra of individual states of a many-body quantum system can be used to
determine the transition between a many-body localized and a thermalizing
regime. Specifically, we study the Heisenberg spin-1/2 chain in a random
external field. We employ a multilayer perceptron with a single hidden layer,
which is trained on labeled entanglement spectra pertaining to the fully
localized and fully thermal regimes. We then apply this network to classify
spectra belonging to states in the transition region. For training, we use a
cost function that contains, in addition to the usual error and regularization
parts, a term that favors a confident classification of the transition region
states. The resulting phase diagram is in good agreement with the one obtained
by more conventional methods and can be computed for small systems. In
particular, the neural network outperforms conventional methods in classifying
individual eigenstates pertaining to a single disorder realization. It allows
us to map out the structure of these eigenstates across the transition with
spatial resolution. Furthermore, we analyze the network operation using the
dreaming technique to show that the neural network correctly learns by itself
the power-law structure of the entanglement spectra in the many-body localized
regime.
</summary>
    <author>
      <name>Frank Schindler</name>
    </author>
    <author>
      <name>Nicolas Regnault</name>
    </author>
    <author>
      <name>Titus Neupert</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevB.95.245134</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevB.95.245134" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 10 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. B 95, 245134 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1704.01578v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.01578v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.str-el" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.02407v2</id>
    <updated>2017-08-08T21:05:15Z</updated>
    <published>2017-05-05T22:06:55Z</published>
    <title>Knowledge-Guided Deep Fractal Neural Networks for Human Pose Estimation</title>
    <summary>  Human pose estimation using deep neural networks aims to map input images
with large variations into multiple body keypoints which must satisfy a set of
geometric constraints and inter-dependency imposed by the human body model.
This is a very challenging nonlinear manifold learning process in a very high
dimensional feature space. We believe that the deep neural network, which is
inherently an algebraic computation system, is not the most effecient way to
capture highly sophisticated human knowledge, for example those highly coupled
geometric characteristics and interdependence between keypoints in human poses.
In this work, we propose to explore how external knowledge can be effectively
represented and injected into the deep neural networks to guide its training
process using learned projections that impose proper prior. Specifically, we
use the stacked hourglass design and inception-resnet module to construct a
fractal network to regress human pose images into heatmaps with no explicit
graphical modeling. We encode external knowledge with visual features which are
able to characterize the constraints of human body models and evaluate the
fitness of intermediate network output. We then inject these external features
into the neural network using a projection matrix learned using an auxiliary
cost function. The effectiveness of the proposed inception-resnet module and
the benefit in guided learning with knowledge projection is evaluated on two
widely used benchmarks. Our approach achieves state-of-the-art performance on
both datasets.
</summary>
    <author>
      <name>Guanghan Ning</name>
    </author>
    <author>
      <name>Zhi Zhang</name>
    </author>
    <author>
      <name>Zhihai He</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 12 figures. arXiv admin note: text overlap with
  arXiv:1609.01743, arXiv:1702.07432, arXiv:1602.00134 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.02407v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.02407v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.03138v1</id>
    <updated>2017-09-10T17:06:23Z</updated>
    <published>2017-09-10T17:06:23Z</published>
    <title>Fully Convolutional Neural Networks for Dynamic Object Detection in Grid
  Maps (Masters Thesis)</title>
    <summary>  One of the most important parts of environment perception is the detection of
obstacles in the surrounding of the vehicle. To achieve that, several sensors
like radars, LiDARs and cameras are installed in autonomous vehicles. The
produced sensor data is fused to a general representation of the surrounding.
In this thesis the dynamic occupancy grid map approach of Nuss et al. is used
while three goals are achieved. First, the approach of Nuss et al. to
distinguish between moving and non-moving obstacles is improved by using Fully
Convolutional Neural Networks to create a class prediction for each grid cell.
For this purpose, the network is initialized with public pre-trained network
models and the training is executed with a semi-automatic generated dataset.
The second goal is to provide orientation information for each detected moving
obstacle. This could improve tracking algorithms, which are based on the
dynamic occupancy grid map. The orientation extraction based on the
Convolutional Neural Network shows a better performance in comparison to an
orientation extraction directly over the velocity information of the dynamic
occupancy grid map. A general problem of developing machine learning approaches
like Neural Networks is the number of labeled data, which can always be
increased. For this reason, the last goal is to evaluate a semi-supervised
learning algorithm, to generate automatically more labeled data. The result of
this evaluation shows that the automated labeled data does not improve the
performance of the Convolutional Neural Network. All in all, the best results
are combined to compare the detection against the approach of Nuss et al. [36]
and a relative improvement of 34.8% is reached.
</summary>
    <author>
      <name>Florian Piewak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the masters thesis of Florian Piewak. A shorter version of
  this thesis was accepted at IV 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.03138v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.03138v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.02095v1</id>
    <updated>2017-10-05T16:18:08Z</updated>
    <published>2017-10-05T16:18:08Z</published>
    <title>Machine Translation Evaluation with Neural Networks</title>
    <summary>  We present a framework for machine translation evaluation using neural
networks in a pairwise setting, where the goal is to select the better
translation from a pair of hypotheses, given the reference translation. In this
framework, lexical, syntactic and semantic information from the reference and
the two hypotheses is embedded into compact distributed vector representations,
and fed into a multi-layer neural network that models nonlinear interactions
between each of the hypotheses and the reference, as well as between the two
hypotheses. We experiment with the benchmark datasets from the WMT Metrics
shared task, on which we obtain the best results published so far, with the
basic network configuration. We also perform a series of experiments to analyze
and understand the contribution of the different components of the network. We
evaluate variants and extensions, including fine-tuning of the semantic
embeddings, and sentence-based representations modeled with convolutional and
recurrent neural networks. In summary, the proposed framework is flexible and
generalizable, allows for efficient learning and scoring, and provides an MT
evaluation metric that correlates with human judgments, and is on par with the
state of the art.
</summary>
    <author>
      <name>Francisco Guzm√°n</name>
    </author>
    <author>
      <name>Shafiq R. Joty</name>
    </author>
    <author>
      <name>Llu√≠s M√†rquez</name>
    </author>
    <author>
      <name>Preslav Nakov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Translation, Reference-based MT Evaluation, Deep Neural
  Networks, Distributed Representation of Texts, Textual Similarity</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Speech &amp; Language 45: 180-200 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1710.02095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.02095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.01447v1</id>
    <updated>2018-02-02T08:57:44Z</updated>
    <published>2018-02-02T08:57:44Z</published>
    <title>Mixed-Resolution Image Representation and Compression with Convolutional
  Neural Networks</title>
    <summary>  In this paper, we propose a end-to-end mixed-resolution image compression
framework with convolutional neural networks. Firstly, given one input image,
feature description neural network (FDNN) is used to generate a new
representation of this image, so that this representation can be more
efficiently compressed by standard coder, as compared to the input image.
Furthermore, we use post-processing neural network (PPNN) to remove the coding
artifacts caused by quantization of codec. Secondly, low-resolution
representation is considered under low bit-rate for high efficiency compression
in terms of most of bit spent by image's structures. However, more bits should
be assigned to image details in the high-resolution, when most of structures
have been kept after compression at the high bit-rate. This comes from that the
low-resolution representation can't burden more information than
high-resolution representation beyond a certain bit-rate. Finally, to resolve
the problem of error back-propagation from the PPNN network to the FDNN
network, we introduce a virtual codec neural network to intimate the procedure
of standard compression and post-processing. The objective experimental results
have demonstrated the proposed method has a large margin improvement, when
comparing with several state-of-the-art approaches.
</summary>
    <author>
      <name>Lijun Zhao</name>
    </author>
    <author>
      <name>Huihui Bai</name>
    </author>
    <author>
      <name>Feng Li</name>
    </author>
    <author>
      <name>Anhong Wang</name>
    </author>
    <author>
      <name>Yao Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, and 2 figures. The summary of recent works on image
  compression and video coding is listed in this website:
  https://github.com/jakelijun/Image-compression-and-video-coding. arXiv admin
  note: substantial text overlap with arXiv:1712.05969</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.01447v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.01447v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.00459v4</id>
    <updated>2016-02-03T15:30:15Z</updated>
    <published>2015-10-02T01:11:26Z</published>
    <title>Proposal for an All-Spin Artificial Neural Network: Emulating Neural and
  Synaptic Functionalities Through Domain Wall Motion in Ferromagnets</title>
    <summary>  Non-Boolean computing based on emerging post-CMOS technologies can
potentially pave the way for low-power neural computing platforms. However,
existing work on such emerging neuromorphic architectures have either focused
on solely mimicking the neuron, or the synapse functionality. While memristive
devices have been proposed to emulate biological synapses, spintronic devices
have proved to be efficient at performing the thresholding operation of the
neuron at ultra-low currents. In this work, we propose an All-Spin Artificial
Neural Network where a single spintronic device acts as the basic building
block of the system. The device offers a direct mapping to synapse and neuron
functionalities in the brain while inter-layer network communication is
accomplished via CMOS transistors. To the best of our knowledge, this is the
first demonstration of a neural architecture where a single nanoelectronic
device is able to mimic both neurons and synapses. The ultra-low voltage
operation of low resistance magneto-metallic neurons enables the low-voltage
operation of the array of spintronic synapses, thereby leading to ultra-low
power neural architectures. Device-level simulations, calibrated to
experimental results, was used to drive the circuit and system level
simulations of the neural network for a standard pattern recognition problem.
Simulation studies indicate energy savings by ~ 100x in comparison to a
corresponding digital/ analog CMOS neuron implementation.
</summary>
    <author>
      <name>Abhronil Sengupta</name>
    </author>
    <author>
      <name>Yong Shim</name>
    </author>
    <author>
      <name>Kaushik Roy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TBCAS.2016.2525823</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TBCAS.2016.2525823" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The article will appear in a future issue of IEEE Transactions on
  Biomedical Circuits and Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.00459v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.00459v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0002076v1</id>
    <updated>2000-02-05T16:52:06Z</updated>
    <published>2000-02-05T16:52:06Z</published>
    <title>Small-worlds: How and why</title>
    <summary>  We investigate small-world networks from the point of view of their origin.
While the characteristics of small-world networks are now fairly well
understood, there is as yet no work on what drives the emergence of such a
network architecture. In situations such as neural or transportation networks,
where a physical distance between the nodes of the network exists, we study
whether the small-world topology arises as a consequence of a tradeoff between
maximal connectivity and minimal wiring. Using simulated annealing, we study
the properties of a randomly rewired network as the relative tradeoff between
wiring and connectivity is varied. When the network seeks to minimize wiring, a
regular graph results. At the other extreme, when connectivity is maximized, a
near random network is obtained. In the intermediate regime, a small-world
network is formed. However, unlike the model of Watts and Strogatz (Nature {\bf
393}, 440 (1998)), we find an alternate route to small-world behaviour through
the formation of hubs, small clusters where one vertex is connected to a large
number of neighbours.
</summary>
    <author>
      <name>Nisha Mathias</name>
    </author>
    <author>
      <name>Venkatesh Gopal</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.63.021117</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.63.021117" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, latex, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0002076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0002076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.4141v2</id>
    <updated>2012-04-26T06:20:51Z</updated>
    <published>2011-04-20T23:51:19Z</published>
    <title>Emergent Criticality Through Adaptive Information Processing in Boolean
  Networks</title>
    <summary>  We study information processing in populations of Boolean networks with
evolving connectivity and systematically explore the interplay between the
learning capability, robustness, the network topology, and the task complexity.
We solve a long-standing open question and find computationally that, for large
system sizes $N$, adaptive information processing drives the networks to a
critical connectivity $K_{c}=2$. For finite size networks, the connectivity
approaches the critical value with a power-law of the system size $N$. We show
that network learning and generalization are optimized near criticality, given
task complexity and the amount of information provided threshold values. Both
random and evolved networks exhibit maximal topological diversity near $K_{c}$.
We hypothesize that this supports efficient exploration and robustness of
solutions. Also reflected in our observation is that the variance of the values
is maximal in critical network populations. Finally, we discuss implications of
our results for determining the optimal topology of adaptive dynamical networks
that solve computational tasks.
</summary>
    <author>
      <name>Alireza Goudarzi</name>
    </author>
    <author>
      <name>Christof Teuscher</name>
    </author>
    <author>
      <name>Natali Gulbahce</name>
    </author>
    <author>
      <name>Thimo Rohlf</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.108.128702</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.108.128702" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physical Review Letters, 108(12):128702 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1104.4141v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.4141v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.10087v1</id>
    <updated>2016-10-31T19:44:50Z</updated>
    <published>2016-10-31T19:44:50Z</published>
    <title>Tensor Switching Networks</title>
    <summary>  We present a novel neural network algorithm, the Tensor Switching (TS)
network, which generalizes the Rectified Linear Unit (ReLU) nonlinearity to
tensor-valued hidden units. The TS network copies its entire input vector to
different locations in an expanded representation, with the location determined
by its hidden unit activity. In this way, even a simple linear readout from the
TS representation can implement a highly expressive deep-network-like function.
The TS network hence avoids the vanishing gradient problem by construction, at
the cost of larger representation size. We develop several methods to train the
TS network, including equivalent kernels for infinitely wide and deep TS
networks, a one-pass linear learning algorithm, and two
backpropagation-inspired representation learning algorithms. Our experimental
results demonstrate that the TS network is indeed more expressive and
consistently learns faster than standard ReLU networks.
</summary>
    <author>
      <name>Chuan-Yung Tsai</name>
    </author>
    <author>
      <name>Andrew Saxe</name>
    </author>
    <author>
      <name>David Cox</name>
    </author>
    <link href="http://arxiv.org/abs/1610.10087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.10087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.01330v1</id>
    <updated>2017-06-05T14:04:16Z</updated>
    <published>2017-06-05T14:04:16Z</published>
    <title>Neuroevolution on the Edge of Chaos</title>
    <summary>  Echo state networks represent a special type of recurrent neural networks.
Recent papers stated that the echo state networks maximize their computational
performance on the transition between order and chaos, the so-called edge of
chaos. This work confirms this statement in a comprehensive set of experiments.
Furthermore, the echo state networks are compared to networks evolved via
neuroevolution. The evolved networks outperform the echo state networks,
however, the evolution consumes significant computational resources. It is
demonstrated that echo state networks with local connections combine the best
of both worlds, the simplicity of random echo state networks and the
performance of evolved networks. Finally, it is shown that evolution tends to
stay close to the ordered side of the edge of chaos.
</summary>
    <author>
      <name>Filip Matzner</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3071178.3071292</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3071178.3071292" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Proceedings of the Genetic and Evolutionary Computation
  Conference 2017 (GECCO '17)</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.01330v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.01330v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.01427v1</id>
    <updated>2017-11-04T12:24:26Z</updated>
    <published>2017-11-04T12:24:26Z</published>
    <title>Deep Stacking Networks for Low-Resource Chinese Word Segmentation with
  Transfer Learning</title>
    <summary>  In recent years, neural networks have proven to be effective in Chinese word
segmentation. However, this promising performance relies on large-scale
training data. Neural networks with conventional architectures cannot achieve
the desired results in low-resource datasets due to the lack of labelled
training data. In this paper, we propose a deep stacking framework to improve
the performance on word segmentation tasks with insufficient data by
integrating datasets from diverse domains. Our framework consists of two parts,
domain-based models and deep stacking networks. The domain-based models are
used to learn knowledge from different datasets. The deep stacking networks are
designed to integrate domain-based models. To reduce model conflicts, we
innovatively add communication paths among models and design various structures
of deep stacking networks, including Gaussian-based Stacking Networks,
Concatenate-based Stacking Networks, Sequence-based Stacking Networks and
Tree-based Stacking Networks. We conduct experiments on six low-resource
datasets from various domains. Our proposed framework shows significant
performance improvements on all datasets compared with several strong
baselines.
</summary>
    <author>
      <name>Jingjing Xu</name>
    </author>
    <author>
      <name>Xu Sun</name>
    </author>
    <author>
      <name>Sujian Li</name>
    </author>
    <author>
      <name>Xiaoyan Cai</name>
    </author>
    <author>
      <name>Bingzhen Wei</name>
    </author>
    <link href="http://arxiv.org/abs/1711.01427v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.01427v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.02719v1</id>
    <updated>2017-12-07T17:05:54Z</updated>
    <published>2017-12-07T17:05:54Z</published>
    <title>Incremental Learning in Deep Convolutional Neural Networks Using Partial
  Network Sharing</title>
    <summary>  Deep convolutional neural network (DCNN) based supervised learning is a
widely practiced approach for large-scale image classification. However,
retraining these large networks to accommodate new, previously unseen data
demands high computational time and energy requirements. Also, previously seen
training samples may not be available at the time of retraining. We propose an
efficient training methodology and incrementally growing a DCNN to allow new
classes to be learned while sharing part of the base network. Our proposed
methodology is inspired by transfer learning techniques, although it does not
forget previously learned classes. An updated network for learning new set of
classes is formed using previously learned convolutional layers (shared from
initial part of base network) with addition of few newly added convolutional
kernels included in the later layers of the network. We evaluated the proposed
scheme on several recognition applications. The classification accuracy
achieved by our approach is comparable to the regular incremental learning
approach (where networks are updated with new training samples only, without
any network sharing).
</summary>
    <author>
      <name>Syed Shakib Sarwar</name>
    </author>
    <author>
      <name>Aayush Ankit</name>
    </author>
    <author>
      <name>Kaushik Roy</name>
    </author>
    <link href="http://arxiv.org/abs/1712.02719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.02719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.04739v3</id>
    <updated>2015-08-13T09:00:18Z</updated>
    <published>2015-03-16T17:23:42Z</published>
    <title>Complex Quantum Network Geometries: Evolution and Phase Transitions</title>
    <summary>  Networks are topological and geometric structures used to describe systems as
different as the Internet, the brain or the quantum structure of space-time.
Here we define complex quantum network geometries, describing the underlying
structure of growing simplicial 2-complexes, i.e. simplicial complexes formed
by triangles. These networks are geometric networks with energies of the links
that grow according to a non-equilibrium dynamics. The evolution in time of the
geometric networks is a classical evolution describing a given path of a path
integral defining the evolution of quantum network states. The quantum network
states are characterized by quantum occupation numbers that can be mapped
respectively to the nodes, links, and triangles incident to each link of the
network. We call the geometric networks describing the evolution of quantum
network states the quantum geometric networks. The quantum geometric networks
have many properties common to complex networks including small-world property,
high clustering coefficient, high modularity, scale-free degree
distribution.Moreover they can be distinguished between the Fermi-Dirac Network
and the Bose-Einstein Network obeying respectively the Fermi-Dirac and
Bose-Einstein statistics. We show that these networks can undergo structural
phase transitions where the geometrical properties of the networks change
drastically. Finally we comment on the relation between Quantum Complex Network
Geometries, spin networks and triangulations.
</summary>
    <author>
      <name>Ginestra Bianconi</name>
    </author>
    <author>
      <name>Christoph Rahmede</name>
    </author>
    <author>
      <name>Zhihao Wu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.92.022815</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.92.022815" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">(30 pages, 23 figures)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 92, 022815 (2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1503.04739v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.04739v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/astro-ph/0410567v1</id>
    <updated>2004-10-23T21:35:55Z</updated>
    <published>2004-10-23T21:35:55Z</published>
    <title>Strategies for Spectral Profile Inversion using Artificial Neural
  Networks</title>
    <summary>  This paper explores three different strategies for the inversion of spectral
lines (and their Stokes profiles) using artificial neural networks. It is shown
that a straightforward approach in which the network is trained with synthetic
spectra from a simplified model leads to considerable errors in the inversion
of real observations. This problem can be overcome in at least two different
ways that are studied here in detail. The first method makes use of an
additional pre-processing auto-associative neural network to project the
observed profile into the theoretical model subspace. The second method
considers a suitable regularization of the neural network used for the
inversion. These new techniques are shown to be robust and reliable when
applied to the inversion of both synthetic and observed data, with errors
typically below $\sim$100 G.
</summary>
    <author>
      <name>H. Socas-Navarro</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1086/427431</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1086/427431" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ApJ, submitted</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Astrophys.J. 621 (2005) 545-553</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/astro-ph/0410567v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/astro-ph/0410567v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9811403v1</id>
    <updated>1998-11-29T10:02:44Z</updated>
    <published>1998-11-29T10:02:44Z</published>
    <title>Response of an Excitatory-Inhibitory Neural Network to External
  Stimulation: An Application to Image Segmentation</title>
    <summary>  Neural network models comprising elements which have exclusively excitatory
or inhibitory synapses are capable of a wide range of dynamic behavior,
including chaos. In this paper, a simple excitatory-inhibitory neural pair,
which forms the building block of larger networks, is subjected to external
stimulation. The response shows transition between various types of dynamics,
depending upon the magnitude of the stimulus. Coupling such pairs over a local
neighborhood in a two-dimensional plane, the resultant network can achieve a
satisfactory segmentation of an image into ``object'' and ``background''.
Results for synthetic and and ``real-life'' images are given.
</summary>
    <author>
      <name>Sitabhra Sinha</name>
    </author>
    <author>
      <name>Jayanta Basak</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.65.046112</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.65.046112" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, latex, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">revised version in Phys. Rev. E 65 (2002) 046112.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/9811403v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9811403v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="chao-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9912328v1</id>
    <updated>1999-12-17T14:30:25Z</updated>
    <published>1999-12-17T14:30:25Z</published>
    <title>Three-state neural network: from mutual information to the hamiltonian</title>
    <summary>  The mutual information, I, of the three-state neural network can be obtained
exactly for the mean-field architecture, as a function of three macroscopic
parameters: the overlap, the neural activity and the {\em activity-overlap},
i.e. the overlap restricted to the active neurons. We perform an expansion of I
on the overlap and the activity-overlap, around their values for neurons almost
independent on the patterns. From this expansion we obtain an expression for a
Hamiltonian which optimizes the retrieval properties of this system. This
Hamiltonian has the form of a disordered Blume-Emery-Griffiths model. The
dynamics corresponding to this Hamiltonian is found. As a special
characteristic of such network, we see that information can survive even if no
overlap is present. Hence the basin of attraction of the patterns and the
retrieval capacity is much larger than for the Hopfield network. The extreme
diluted version is analized, the curves of information are plotted and the
phase diagrams are built.
</summary>
    <author>
      <name>David R. Dominguez Carreta</name>
    </author>
    <author>
      <name>Elka Korutcheva</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages (including 6 postscript figures)</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9912328v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9912328v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0608103v1</id>
    <updated>2006-08-04T01:47:28Z</updated>
    <published>2006-08-04T01:47:28Z</published>
    <title>Thouless-Anderson-Palmer equation and self-consistent signal-to-noise
  analysis for the Hopfield model with three-body interaction</title>
    <summary>  The self-consistent signal-to-noise analysis (SCSNA) is an alternative to the
replica method for deriving the set of order parameter equations for
associative memory neural network models and is closely related with the
Thouless-Anderson-Palmer equation (TAP) approach. In the recent paper by Shiino
and Yamana the Onsager reaction term of the TAP equation has been found to be
obtained from the SCSNA for Hopfield neural networks with 2-body interaction.
We study the TAP equation for an associative memory stochastic analog neural
network with 3-body interaction to investigate the structure of the Onsager
reaction term, in connection with the term proportional to the output
characteristic to the SCSNA. We report the SCSNA framework for analog networks
with 3-body interactions as well as a novel recipe based on the cavity concept
that involves two cavities and the hybrid use of the SCSNA to obtain the TAP
equation.
</summary>
    <author>
      <name>Akihisa Ichiki</name>
    </author>
    <author>
      <name>Masatoshi Shiino</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.74.017103</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.74.017103" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 74 (2006) 017103</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0608103v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0608103v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0608564v1</id>
    <updated>2006-08-25T15:28:20Z</updated>
    <published>2006-08-25T15:28:20Z</published>
    <title>A Heterosynaptic Learning Rule for Neural Networks</title>
    <summary>  In this article we intoduce a novel stochastic Hebb-like learning rule for
neural networks that is neurobiologically motivated. This learning rule
combines features of unsupervised (Hebbian) and supervised (reinforcement)
learning and is stochastic with respect to the selection of the time points
when a synapse is modified. Moreover, the learning rule does not only affect
the synapse between pre- and postsynaptic neuron, which is called homosynaptic
plasticity, but effects also further remote synapses of the pre- and
postsynaptic neuron. This more complex form of synaptic plasticity has recently
come under investigations in neurobiology and is called heterosynaptic
plasticity. We demonstrate that this learning rule is useful in training neural
networks by learning parity functions including the exclusive-or (XOR) mapping
in a multilayer feed-forward network. We find, that our stochastic learning
rule works well, even in the presence of noise. Importantly, the mean learning
time increases with the number of patterns to be learned polynomially,
indicating efficient learning.
</summary>
    <author>
      <name>Frank Emmert-Streib</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1142/S0129183106009916</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1142/S0129183106009916" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0608564v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0608564v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0405033v1</id>
    <updated>2004-05-07T00:08:16Z</updated>
    <published>2004-05-07T00:08:16Z</published>
    <title>Optimization of Evolutionary Neural Networks Using Hybrid Learning
  Algorithms</title>
    <summary>  Evolutionary artificial neural networks (EANNs) refer to a special class of
artificial neural networks (ANNs) in which evolution is another fundamental
form of adaptation in addition to learning. Evolutionary algorithms are used to
adapt the connection weights, network architecture and learning algorithms
according to the problem environment. Even though evolutionary algorithms are
well known as efficient global search algorithms, very often they miss the best
local solutions in the complex solution space. In this paper, we propose a
hybrid meta-heuristic learning approach combining evolutionary learning and
local search methods (using 1st and 2nd order error information) to improve the
learning and faster convergence obtained using a direct evolutionary approach.
The proposed technique is tested on three different chaotic time series and the
test results are compared with some popular neuro-fuzzy systems and a recently
developed cutting angle method of global optimization. Empirical results reveal
that the proposed technique is efficient in spite of the computational
complexity.
</summary>
    <author>
      <name>Ajith Abraham</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IJCNN.2002.1007591</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IJCNN.2002.1007591" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Joint Conference on Neural Networks (IJCNN'02),
  2002 IEEE World Congress on Computational Intelligence, Hawaii, ISBN
  0780372786, IEEE Press, Volume 3, pp. 2797-2802, 2002</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0405033v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0405033v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="1.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.0078v1</id>
    <updated>2007-05-01T06:27:50Z</updated>
    <published>2007-05-01T06:27:50Z</published>
    <title>Neural networks with transient state dynamics</title>
    <summary>  We investigate dynamical systems characterized by a time series of distinct
semi-stable activity patterns, as they are observed in cortical neural activity
patterns. We propose and discuss a general mechanism allowing for an adiabatic
continuation between attractor networks and a specific adjoined transient-state
network, which is strictly dissipative. Dynamical systems with transient states
retain functionality when their working point is autoregulated; avoiding
prolonged periods of stasis or drifting into a regime of rapid fluctuations. We
show, within a continuous-time neural network model, that a single local
updating rule for online learning allows simultaneously (i) for information
storage via unsupervised Hebbian-type learning, (ii) for adaptive regulation of
the working point and (iii) for the suppression of runaway synaptic growth.
Simulation results are presented; the spontaneous breaking of time-reversal
symmetry and link symmetry are discussed.
</summary>
    <author>
      <name>Claudius Gros</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1367-2630/9/4/109</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1367-2630/9/4/109" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">New J.Phys.9:109,2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0705.0078v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.0078v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.other" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.2010v1</id>
    <updated>2008-09-11T14:31:54Z</updated>
    <published>2008-09-11T14:31:54Z</published>
    <title>Maximum memory capacity on neural networks with short-term depression
  and facilitation</title>
    <summary>  In this work we study, analytically and employing Monte Carlo simulations,
the influence of the competition between several activity-dependent synaptic
processes, such as short-term synaptic facilitation and depression, on the
maximum memory storage capacity in a neural network. In contrast with the case
of synaptic depression, which drastically reduces the capacity of the network
to store and retrieve "static" activity patterns, synaptic facilitation
enhances the storage capacity in different contexts. In particular, we found
optimal values of the relevant synaptic parameters (such as the
neurotransmitter release probability or the characteristic facilitation time
constant) for which the storage capacity can be maximal and similar to the one
obtained with static synapses, that is, without activity-dependent processes.
We conclude that depressing synapses with a certain level of facilitation allow
to recover the good retrieval properties of networks with static synapses while
maintaining the nonlinear characteristics of dynamic synapses, convenient for
information processing and coding.
</summary>
    <author>
      <name>Jorge F. Mejias</name>
    </author>
    <author>
      <name>Joaquin J. Torres</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 figures, accepted in Neural Computation</arxiv:comment>
    <link href="http://arxiv.org/abs/0809.2010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.2010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.2969v2</id>
    <updated>2008-12-21T17:22:02Z</updated>
    <published>2008-12-16T15:59:36Z</published>
    <title>A Growing Self-Organizing Network for Reconstructing Curves and Surfaces</title>
    <summary>  Self-organizing networks such as Neural Gas, Growing Neural Gas and many
others have been adopted in actual applications for both dimensionality
reduction and manifold learning. Typically, in these applications, the
structure of the adapted network yields a good estimate of the topology of the
unknown subspace from where the input data points are sampled. The approach
presented here takes a different perspective, namely by assuming that the input
space is a manifold of known dimension. In return, the new type of growing
self-organizing network presented gains the ability to adapt itself in way that
may guarantee the effective and stable recovery of the exact topological
structure of the input manifold.
</summary>
    <author>
      <name>Marco Piastra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IJCNN.2009.5178709</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IJCNN.2009.5178709" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Networks, 2009. IJCNN 2009. International Joint Conference
  on , vol., no., pp.2533,2540, 14-19 June 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0812.2969v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.2969v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.0517v1</id>
    <updated>2010-05-04T13:27:36Z</updated>
    <published>2010-05-04T13:27:36Z</published>
    <title>Noise and Neuronal Heterogeneity</title>
    <summary>  We consider signal transaction in a simple neuronal model featuring intrinsic
noise. The presence of noise limits the precision of neural responses and
impacts the quality of neural signal transduction. We assess the signal
transduction quality in relation to the level of noise, and show it to be
maximized by a non-zero level of noise, analogous to the stochastic resonance
effect. The quality enhancement occurs for a finite range of stimuli to a
single neuron; we show how to construct networks of neurons that extend the
range. The range increases more rapidly with network size when we make use of
heterogeneous populations of neurons with a variety of thresholds, rather than
homogeneous populations of neurons all with the same threshold. The limited
precision of neural responses thus can have a direct effect on the optimal
network structure, with diverse functional properties of the constituent
neurons supporting an economical information processing strategy that reduces
the metabolic costs of handling a broad class of stimuli.
</summary>
    <author>
      <name>Michael J. Barber</name>
    </author>
    <author>
      <name>Manfred L. Ristig</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 9 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Weiss, M. L., editor, Neuronal Network Research Horizons,
  chapter 4, pages 119-135. Nova Science Publishers, Inc., Hauppauge, NY
  (2007).</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.0517v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.0517v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1102.5757v1</id>
    <updated>2011-02-28T19:58:20Z</updated>
    <published>2011-02-28T19:58:20Z</published>
    <title>Improving the character recognition efficiency of feed forward BP neural
  network</title>
    <summary>  This work is focused on improving the character recognition capability of
feed-forward back-propagation neural network by using one, two and three hidden
layers and the modified additional momentum term. 182 English letters were
collected for this work and the equivalent binary matrix form of these
characters was applied to the neural network as training patterns. While the
network was getting trained, the connection weights were modified at each epoch
of learning. For each training sample, the error surface was examined for
minima by computing the gradient descent. We started the experiment by using
one hidden layer and the number of hidden layers was increased up to three and
it has been observed that accuracy of the network was increased with low mean
square error but at the cost of training time. The recognition accuracy was
improved further when modified additional momentum term was used.
</summary>
    <author>
      <name>Amit Choudhary</name>
    </author>
    <author>
      <name>Rahul Rishi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 Pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1102.5757v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.5757v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.1434v1</id>
    <updated>2011-08-06T00:54:32Z</updated>
    <published>2011-08-06T00:54:32Z</published>
    <title>A Novel Approach for Authenticating Textual or Graphical Passwords Using
  Hopfield Neural Network</title>
    <summary>  Password authentication using Hopfield Networks is presented in this paper.
In this paper we discussed the Hopfield Network Scheme for Textual and
graphical passwords, for which input Password will be converted in to
probabilistic values. We observed how to get password authentication using
Probabilistic values for Textual passwords and Graphical passwords. This study
proposes the use of a Hopfield neural network technique for password
authentication. In comparison to existing layered neural network techniques,
the proposed method provides better accuracy and quicker response time to
registration and password changes.
</summary>
    <author>
      <name>ASN Chakravarthy</name>
    </author>
    <author>
      <name>P S Avadhani</name>
    </author>
    <author>
      <name>P. E. S. N Krishna Prasad</name>
    </author>
    <author>
      <name>N. Rajeevand</name>
    </author>
    <author>
      <name>D. Rajasekhar Reddy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 18 figures, published in Advanced Computing: An
  International Journal (ACIJ)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advanced Computing: An International Journal ( ACIJ ), Vol.2,
  No.4, July 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1108.1434v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.1434v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.2654v2</id>
    <updated>2016-09-26T04:13:53Z</updated>
    <published>2014-04-09T23:57:49Z</published>
    <title>Spin switches for compact implementation of neuron and synapse</title>
    <summary>  Nanomagnets driven by spin currents provide a natural implementation for a
neuron and a synapse: currents allow convenient summation of multiple inputs,
while the magnet provides the threshold function. The objective of this paper
is to explore the possibility of a hardware neural network (HNN) implementation
using a spin switch (SS) as its basic building block. SS is a recently proposed
device based on established technology with a transistor-like gain and
input-output isolation. This allows neural networks to be constructed with
purely passive interconnections without intervening clocks or amplifiers. The
weights for the neural network are conveniently adjusted through analog
voltages that can be stored in a non-volatile manner in an underlying CMOS
layer using a floating gate low dropout voltage regulator. The operation of a
multi-layer SS neural network designed for character recognition is
demonstrated using a standard simulation model based on coupled
Landau-Lifshitz-Gilbert (LLG) equations, one for each magnet in the network.
</summary>
    <author>
      <name>Vinh Quang Diep</name>
    </author>
    <author>
      <name>Brian Sutton</name>
    </author>
    <author>
      <name>Behtash Behin-Aein</name>
    </author>
    <author>
      <name>Supriyo Datta</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/1.4881575</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/1.4881575" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 9 figures and 1 table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Applied Physic Letters, 104, 222405 (2014)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.2654v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.2654v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.mes-hall" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mes-hall" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.6558v2</id>
    <updated>2014-07-03T08:07:52Z</updated>
    <published>2014-06-25T13:10:56Z</published>
    <title>$ N^4 $-Fields: Neural Network Nearest Neighbor Fields for Image
  Transforms</title>
    <summary>  We propose a new architecture for difficult image processing operations, such
as natural edge detection or thin object segmentation. The architecture is
based on a simple combination of convolutional neural networks with the nearest
neighbor search.
  We focus our attention on the situations when the desired image
transformation is too hard for a neural network to learn explicitly. We show
that in such situations, the use of the nearest neighbor search on top of the
network output allows to improve the results considerably and to account for
the underfitting effect during the neural network training. The approach is
validated on three challenging benchmarks, where the performance of the
proposed architecture matches or exceeds the state-of-the-art.
</summary>
    <author>
      <name>Yaroslav Ganin</name>
    </author>
    <author>
      <name>Victor Lempitsky</name>
    </author>
    <link href="http://arxiv.org/abs/1406.6558v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.6558v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.0510v1</id>
    <updated>2014-10-02T10:58:17Z</updated>
    <published>2014-10-02T10:58:17Z</published>
    <title>Deep Sequential Neural Network</title>
    <summary>  Neural Networks sequentially build high-level features through their
successive layers. We propose here a new neural network model where each layer
is associated with a set of candidate mappings. When an input is processed, at
each layer, one mapping among these candidates is selected according to a
sequential decision process. The resulting model is structured according to a
DAG like architecture, so that a path from the root to a leaf node defines a
sequence of transformations. Instead of considering global transformations,
like in classical multilayer networks, this model allows us for learning a set
of local transformations. It is thus able to process data with different
characteristics through specific sequences of such local transformations,
increasing the expression power of this model w.r.t a classical multilayered
network. The learning algorithm is inspired from policy gradient techniques
coming from the reinforcement learning domain and is used here instead of the
classical back-propagation based gradient descent techniques. Experiments on
different datasets show the relevance of this approach.
</summary>
    <author>
      <name>Ludovic Denoyer</name>
    </author>
    <author>
      <name>Patrick Gallinari</name>
    </author>
    <link href="http://arxiv.org/abs/1410.0510v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.0510v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.1165v3</id>
    <updated>2015-04-09T01:22:49Z</updated>
    <published>2014-10-05T14:46:47Z</published>
    <title>Understanding Locally Competitive Networks</title>
    <summary>  Recently proposed neural network activation functions such as rectified
linear, maxout, and local winner-take-all have allowed for faster and more
effective training of deep neural architectures on large and complex datasets.
The common trait among these functions is that they implement local competition
between small groups of computational units within a layer, so that only part
of the network is activated for any given input pattern. In this paper, we
attempt to visualize and understand this self-modularization, and suggest a
unified explanation for the beneficial properties of such networks. We also
show how our insights can be directly useful for efficiently performing
retrieval over large datasets using neural networks.
</summary>
    <author>
      <name>Rupesh Kumar Srivastava</name>
    </author>
    <author>
      <name>Jonathan Masci</name>
    </author>
    <author>
      <name>Faustino Gomez</name>
    </author>
    <author>
      <name>J√ºrgen Schmidhuber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages + 2 supplementary, Accepted to ICLR 2015 Conference track</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.1165v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.1165v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T30, 68T10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.04596v3</id>
    <updated>2015-08-15T13:02:08Z</updated>
    <published>2015-03-16T10:41:30Z</published>
    <title>Enhanced Image Classification With a Fast-Learning Shallow Convolutional
  Neural Network</title>
    <summary>  We present a neural network architecture and training method designed to
enable very rapid training and low implementation complexity. Due to its
training speed and very few tunable parameters, the method has strong potential
for applications requiring frequent retraining or online training. The approach
is characterized by (a) convolutional filters based on biologically inspired
visual processing filters, (b) randomly-valued classifier-stage input weights,
(c) use of least squares regression to train the classifier output weights in a
single batch, and (d) linear classifier-stage output units. We demonstrate the
efficacy of the method by applying it to image classification. Our results
match existing state-of-the-art results on the MNIST (0.37% error) and
NORB-small (2.2% error) image classification databases, but with very fast
training times compared to standard deep network approaches. The network's
performance on the Google Street View House Number (SVHN) (4% error) database
is also competitive with state-of-the art methods.
</summary>
    <author>
      <name>Mark D. McDonnell</name>
    </author>
    <author>
      <name>Tony Vladusich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures, Paper at IJCNN 2015 (International Joint
  Conference on Neural Networks, 2015)</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.04596v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.04596v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.0485v1</id>
    <updated>2009-11-03T04:07:19Z</updated>
    <published>2009-11-03T04:07:19Z</published>
    <title>Novel Intrusion Detection using Probabilistic Neural Network and
  Adaptive Boosting</title>
    <summary>  This article applies Machine Learning techniques to solve Intrusion Detection
problems within computer networks. Due to complex and dynamic nature of
computer networks and hacking techniques, detecting malicious activities
remains a challenging task for security experts, that is, currently available
defense systems suffer from low detection capability and high number of false
alarms. To overcome such performance limitations, we propose a novel Machine
Learning algorithm, namely Boosted Subspace Probabilistic Neural Network
(BSPNN), which integrates an adaptive boosting technique and a semi parametric
neural network to obtain good tradeoff between accuracy and generality. As the
result, learning bias and generalization variance can be significantly
minimized. Substantial experiments on KDD 99 intrusion benchmark indicate that
our model outperforms other state of the art learning algorithms, with
significantly improved detection accuracy, minimal false alarms and relatively
small computational complexity.
</summary>
    <author>
      <name>Tich Phuoc Tran</name>
    </author>
    <author>
      <name>Longbing Cao</name>
    </author>
    <author>
      <name>Dat Tran</name>
    </author>
    <author>
      <name>Cuong Duc Nguyen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact Factor 0.423,
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 6, No. 1, pp. 083-091, October 2009, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0911.0485v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.0485v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.3271v2</id>
    <updated>2013-02-11T19:08:36Z</updated>
    <published>2012-09-14T18:03:25Z</published>
    <title>Critical Avalanches and Subsampling in Map-based Neural Networks</title>
    <summary>  We investigate the synaptic noise as a novel mechanism for creating critical
avalanches in the activity of neural networks. We model neurons and chemical
synapses by dynamical maps with a uniform noise term in the synaptic coupling.
An advantage of utilizing maps is that the dynamical properties (action
potential profile, excitability properties, post synaptic potential summation
etc.) are not imposed to the system, but occur naturally by solving the system
equations. We discuss the relevant neuronal and synaptic properties to achieve
the critical state. We verify that networks of excitatory by rebound neurons
with fast synapses present power law avalanches. We also discuss the measuring
of neuronal avalanches by subsampling our data, shedding light on the
experimental search for Self-Organized Criticality in neural networks.
</summary>
    <author>
      <name>Mauricio Girardi-Schappo</name>
    </author>
    <author>
      <name>Osame Kinouchi</name>
    </author>
    <author>
      <name>Marcelo H. R. Tragtenberg</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.88.024701</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.88.024701" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 figures; Submitted to: Physical Review Letters</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 88, 024701, 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1209.3271v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.3271v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.3945v1</id>
    <updated>2013-09-16T13:20:30Z</updated>
    <published>2013-09-16T13:20:30Z</published>
    <title>A Neural Network based Approach for Predicting Customer Churn in
  Cellular Network Services</title>
    <summary>  Marketing literature states that it is more costly to engage a new customer
than to retain an existing loyal customer. Churn prediction models are
developed by academics and practitioners to effectively manage and control
customer churn in order to retain existing customers. As churn management is an
important activity for companies to retain loyal customers, the ability to
correctly predict customer churn is necessary. As the cellular network services
market becoming more competitive, customer churn management has become a
crucial task for mobile communication operators. This paper proposes a neural
network based approach to predict customer churn in subscription of cellular
wireless services. The results of experiments indicate that neural network
based approach can predict customer churn.
</summary>
    <author>
      <name>Anuj Sharma</name>
    </author>
    <author>
      <name>Dr. Prabin Kumar Panigrahi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/3344-4605 10.5120/3344-4605 10.5120/3344-4605 10.5120/3344-4605
  10.5120/3344-4605</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/3344-4605" rel="related"/>
    <link title="doi" href="http://dx.doi.org/10.5120/3344-4605" rel="related"/>
    <link title="doi" href="http://dx.doi.org/10.5120/3344-4605" rel="related"/>
    <link title="doi" href="http://dx.doi.org/10.5120/3344-4605" rel="related"/>
    <link title="doi" href="http://dx.doi.org/" rel="related"/>
    <link title="doi" href="http://dx.doi.org/" rel="related"/>
    <link title="doi" href="http://dx.doi.org/10.5120/3344-4605" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages. International Journal of Computer Applications August 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.3945v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.3945v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.5956v2</id>
    <updated>2014-07-24T15:06:18Z</updated>
    <published>2014-02-24T17:13:07Z</published>
    <title>The causal inference of cortical neural networks during music
  improvisations</title>
    <summary>  In this paper, we present an EEG study of two music improvisation
experiments. Professional musicians with high level of improvisation skills
were asked to perform music either according to notes (composed music) or in
improvisation. Each piece of music was performed in two different modes: strict
mode and "let-go" mode. Synchronized EEG data was measured from both musicians
and listeners. We used one of the most reliable causality measures: conditional
mutual information from mixed embedding (MIME), to analyze directed
correlations between different EEG channels, which was combined with network
theory to construct both intra-brain and cross-brain neural networks.
Differences were identified in intra-brain neural networks between composed
music and improvisation and between strict mode and "let-go" mode. Particular
brain regions such as frontal, parietal and temporal regions were found to play
a key role in differentiating the brain activities between different playing
conditions. By comparing the level of degree centralities in intra-brain neural
networks, we found musicians responding differently to listeners when playing
music in different conditions.
</summary>
    <author>
      <name>Xiaogeng Wan</name>
    </author>
    <author>
      <name>Bjorn Cruts</name>
    </author>
    <author>
      <name>Henrik Jeldtoft Jensen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pone.0112776</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pone.0112776" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 9 figures. The version was a revised in accordance with
  referee's comments. The language was also improved</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.5956v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.5956v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.5769v2</id>
    <updated>2015-06-24T09:16:28Z</updated>
    <published>2014-05-22T14:35:52Z</published>
    <title>Descriptor Matching with Convolutional Neural Networks: a Comparison to
  SIFT</title>
    <summary>  Latest results indicate that features learned via convolutional neural
networks outperform previous descriptors on classification tasks by a large
margin. It has been shown that these networks still work well when they are
applied to datasets or recognition tasks different from those they were trained
on. However, descriptors like SIFT are not only used in recognition but also
for many correspondence problems that rely on descriptor matching. In this
paper we compare features from various layers of convolutional neural nets to
standard SIFT descriptors. We consider a network that was trained on ImageNet
and another one that was trained without supervision. Surprisingly,
convolutional neural networks clearly outperform SIFT on descriptor matching.
This paper has been merged with arXiv:1406.6909
</summary>
    <author>
      <name>Philipp Fischer</name>
    </author>
    <author>
      <name>Alexey Dosovitskiy</name>
    </author>
    <author>
      <name>Thomas Brox</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been merged with arXiv:1406.6909</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.5769v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.5769v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.4.7; I.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.3264v1</id>
    <updated>2014-09-10T21:52:40Z</updated>
    <published>2014-09-10T21:52:40Z</published>
    <title>Non-backtracking operator for Ising model and its application in
  attractor neural networks</title>
    <summary>  The non-backtracking operator was recently shown to give a redemption for
spectral clustering in sparse graphs. In this paper we consider
non-backtracking operator for Ising model on a general graph with a general
coupling distribution by linearizing Belief Propagation algorithm at
paramagnetic fixed-point. The spectrum of the operator is studied, the sharp
edge of bulk and possible real eigenvalues outside the bulk are computed
analytically as a function of couplings and temperature. We show the
applications of the operator in attractor neural networks. At thermodynamic
limit, our result recovers the phase boundaries of Hopfield model obtained by
replica method. On single instances of Hopfield model, its eigenvectors can be
used to retrieve all patterns simultaneously. We also give an example on how to
control the neural networks, i.e. making network more sparse while keeping
patterns stable, using the non-backtracking operator and matrix perturbation
theory.
</summary>
    <author>
      <name>Pan Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 7 figures, comments are welcome</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.3264v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.3264v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.2693v4</id>
    <updated>2015-04-28T11:23:38Z</updated>
    <published>2014-12-08T18:45:22Z</published>
    <title>Provable Methods for Training Neural Networks with Sparse Connectivity</title>
    <summary>  We provide novel guaranteed approaches for training feedforward neural
networks with sparse connectivity. We leverage on the techniques developed
previously for learning linear networks and show that they can also be
effectively adopted to learn non-linear networks. We operate on the moments
involving label and the score function of the input, and show that their
factorization provably yields the weight matrix of the first layer of a deep
network under mild conditions. In practice, the output of our method can be
employed as effective initializers for gradient descent.
</summary>
    <author>
      <name>Hanie Sedghi</name>
    </author>
    <author>
      <name>Anima Anandkumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for presentation at Neural Information Processing
  Systems(NIPS) 2014 Deep Learning workshop and Accepted as a workshop
  contribution at ICLR 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.2693v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.2693v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.5474v4</id>
    <updated>2015-11-20T05:50:23Z</updated>
    <published>2014-12-17T16:48:54Z</published>
    <title>Flattened Convolutional Neural Networks for Feedforward Acceleration</title>
    <summary>  We present flattened convolutional neural networks that are designed for fast
feedforward execution. The redundancy of the parameters, especially weights of
the convolutional filters in convolutional neural networks has been extensively
studied and different heuristics have been proposed to construct a low rank
basis of the filters after training. In this work, we train flattened networks
that consist of consecutive sequence of one-dimensional filters across all
directions in 3D space to obtain comparable performance as conventional
convolutional networks. We tested flattened model on different datasets and
found that the flattened layer can effectively substitute for the 3D filters
without loss of accuracy. The flattened convolution pipelines provide around
two times speed-up during feedforward pass compared to the baseline model due
to the significant reduction of learning parameters. Furthermore, the proposed
method does not require efforts in manual tuning or post processing once the
model is trained.
</summary>
    <author>
      <name>Jonghoon Jin</name>
    </author>
    <author>
      <name>Aysegul Dundar</name>
    </author>
    <author>
      <name>Eugenio Culurciello</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Learning Representations (ICLR) 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.5474v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.5474v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.7753v2</id>
    <updated>2015-04-16T23:37:58Z</updated>
    <published>2014-12-24T20:58:18Z</published>
    <title>Learning Longer Memory in Recurrent Neural Networks</title>
    <summary>  Recurrent neural network is a powerful model that learns temporal patterns in
sequential data. For a long time, it was believed that recurrent networks are
difficult to train using simple optimizers, such as stochastic gradient
descent, due to the so-called vanishing gradient problem. In this paper, we
show that learning longer term patterns in real data, such as in natural
language, is perfectly possible using gradient descent. This is achieved by
using a slight structural modification of the simple recurrent neural network
architecture. We encourage some of the hidden units to change their state
slowly by making part of the recurrent weight matrix close to identity, thus
forming kind of a longer term memory. We evaluate our model in language
modeling experiments, where we obtain similar performance to the much more
complex Long Short Term Memory (LSTM) networks (Hochreiter &amp; Schmidhuber,
1997).
</summary>
    <author>
      <name>Tomas Mikolov</name>
    </author>
    <author>
      <name>Armand Joulin</name>
    </author>
    <author>
      <name>Sumit Chopra</name>
    </author>
    <author>
      <name>Michael Mathieu</name>
    </author>
    <author>
      <name>Marc'Aurelio Ranzato</name>
    </author>
    <link href="http://arxiv.org/abs/1412.7753v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.7753v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.05777v1</id>
    <updated>2015-02-20T05:26:09Z</updated>
    <published>2015-02-20T05:26:09Z</published>
    <title>Spike Event Based Learning in Neural Networks</title>
    <summary>  A scheme is derived for learning connectivity in spiking neural networks. The
scheme learns instantaneous firing rates that are conditional on the activity
in other parts of the network. The scheme is independent of the choice of
neuron dynamics or activation function, and network architecture. It involves
two simple, online, local learning rules that are applied only in response to
occurrences of spike events. This scheme provides a direct method for
transferring ideas between the fields of deep learning and computational
neuroscience. This learning scheme is demonstrated using a layered feedforward
spiking neural network trained self-supervised on a prediction and
classification task for moving MNIST images collected using a Dynamic Vision
Sensor.
</summary>
    <author>
      <name>James A. Henderson</name>
    </author>
    <author>
      <name>TingTing A. Gibson</name>
    </author>
    <author>
      <name>Janet Wiles</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Figure 4 can be viewed as a movie in a separate file</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.05777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.05777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.06825v1</id>
    <updated>2015-04-26T14:09:17Z</updated>
    <published>2015-04-26T14:09:17Z</published>
    <title>Comparison of Training Methods for Deep Neural Networks</title>
    <summary>  This report describes the difficulties of training neural networks and in
particular deep neural networks. It then provides a literature review of
training methods for deep neural networks, with a focus on pre-training. It
focuses on Deep Belief Networks composed of Restricted Boltzmann Machines and
Stacked Autoencoders and provides an outreach on further and alternative
approaches. It also includes related practical recommendations from the
literature on training them. In the second part, initial experiments using some
of the covered methods are performed on two databases. In particular,
experiments are performed on the MNIST hand-written digit dataset and on facial
emotion data from a Kaggle competition. The results are discussed in the
context of results reported in other research papers. An error rate lower than
the best contribution to the Kaggle competition is achieved using an optimized
Stacked Autoencoder.
</summary>
    <author>
      <name>Patrick O. Glauner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">50 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.06825v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.06825v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.08081v1</id>
    <updated>2015-04-30T04:55:53Z</updated>
    <published>2015-04-30T04:55:53Z</published>
    <title>Global Convergence of Analytic Neural Networks with Event-triggered
  Synaptic Feedbacks</title>
    <summary>  In this paper, we investigate convergence of a class of analytic neural
networks with event-triggered rule. This model is general and include Hopfield
neural network as a special case. The event-trigger rule efficiently reduces
the frequency of information transmission between synapses of the neurons. The
synaptic feedback of each neuron keeps a constant value based on the outputs of
its neighbours at its latest triggering time but changes until the next
triggering time of this neuron that is determined by certain criterion via its
neighborhood information. It is proved that the analytic neural network is
completely stable under this event-triggered rule. The main technique of proof
is the ${\L}$ojasiewicz inequality to prove the finiteness of trajectory
length. The realization of this event-triggered rule is verified by the
exclusion of Zeno behaviors. Numerical examples are provided to illustrate the
theoretical results and present the optimisation capability of the network
dynamics.
</summary>
    <author>
      <name>Wenlian Lu</name>
    </author>
    <author>
      <name>Ren Zheng</name>
    </author>
    <author>
      <name>Xinlei Yi</name>
    </author>
    <author>
      <name>Tianping Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1504.08081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.08081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.05401v2</id>
    <updated>2015-08-13T11:49:49Z</updated>
    <published>2015-05-20T14:34:23Z</published>
    <title>A Max-Sum algorithm for training discrete neural networks</title>
    <summary>  We present an efficient learning algorithm for the problem of training neural
networks with discrete synapses, a well-known hard (NP-complete) discrete
optimization problem. The algorithm is a variant of the so-called Max-Sum (MS)
algorithm. In particular, we show how, for bounded integer weights with $q$
distinct states and independent concave a priori distribution (e.g. $l_{1}$
regularization), the algorithm's time complexity can be made to scale as
$O\left(N\log N\right)$ per node update, thus putting it on par with
alternative schemes, such as Belief Propagation (BP), without resorting to
approximations. Two special cases are of particular interest: binary synapses
$W\in\{-1,1\}$ and ternary synapses $W\in\{-1,0,1\}$ with $l_{0}$
regularization. The algorithm we present performs as well as BP on binary
perceptron learning problems, and may be better suited to address the problem
on fully-connected two-layer networks, since inherent symmetries in two layer
networks are naturally broken using the MS approach.
</summary>
    <author>
      <name>Carlo Baldassi</name>
    </author>
    <author>
      <name>Alfredo Braunstein</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1742-5468/2015/08/P08008</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1742-5468/2015/08/P08008" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Statistical Mechanics: Theory and Experiment 2015, no.
  8, P08008</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1505.05401v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.05401v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.00210v1</id>
    <updated>2015-07-01T12:42:01Z</updated>
    <published>2015-07-01T12:42:01Z</published>
    <title>Natural Neural Networks</title>
    <summary>  We introduce Natural Neural Networks, a novel family of algorithms that speed
up convergence by adapting their internal representation during training to
improve conditioning of the Fisher matrix. In particular, we show a specific
example that employs a simple and efficient reparametrization of the neural
network weights by implicitly whitening the representation obtained at each
layer, while preserving the feed-forward computation of the network. Such
networks can be trained efficiently via the proposed Projected Natural Gradient
Descent algorithm (PRONG), which amortizes the cost of these reparametrizations
over many parameter updates and is closely related to the Mirror Descent online
learning algorithm. We highlight the benefits of our method on both
unsupervised and supervised learning tasks, and showcase its scalability by
training on the large-scale ImageNet Challenge dataset.
</summary>
    <author>
      <name>Guillaume Desjardins</name>
    </author>
    <author>
      <name>Karen Simonyan</name>
    </author>
    <author>
      <name>Razvan Pascanu</name>
    </author>
    <author>
      <name>Koray Kavukcuoglu</name>
    </author>
    <link href="http://arxiv.org/abs/1507.00210v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.00210v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.04552v1</id>
    <updated>2015-07-16T12:54:14Z</updated>
    <published>2015-07-16T12:54:14Z</published>
    <title>Generalization of neuron network model with delay feedback</title>
    <summary>  We present generalized delayed neural network (DNN) model with positive delay
feedback and neuron history. The local stability analysis around trivial local
equilibria of delayed neural networks has applied and determine the conditions
for the existence of zero root. We develop few innovative delayed neural
network models in different dimensions through transformation and extension of
some existing models. We found that zero root can have multiplicity two under
certain conditions. We further show how the characteristic equation can have
zero root and its multiplicity is dependent on the conditions undertaken.
Finally, we generalize the neural network of $N$ neurons through which we
determine the general form of Jacobian of the linear form and corresponding
characteristic equation of the system.
</summary>
    <author>
      <name>Sanjeet Maisnam</name>
    </author>
    <author>
      <name>R. K. Brojen Singh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.04552v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.04552v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.03720v1</id>
    <updated>2015-08-15T11:15:32Z</updated>
    <published>2015-08-15T11:15:32Z</published>
    <title>Classifying Relations via Long Short Term Memory Networks along Shortest
  Dependency Path</title>
    <summary>  Relation classification is an important research arena in the field of
natural language processing (NLP). In this paper, we present SDP-LSTM, a novel
neural network to classify the relation of two entities in a sentence. Our
neural architecture leverages the shortest dependency path (SDP) between two
entities; multichannel recurrent neural networks, with long short term memory
(LSTM) units, pick up heterogeneous information along the SDP. Our proposed
model has several distinct features: (1) The shortest dependency paths retain
most relevant information (to relation classification), while eliminating
irrelevant words in the sentence. (2) The multichannel LSTM networks allow
effective information integration from heterogeneous sources over the
dependency paths. (3) A customized dropout strategy regularizes the neural
network to alleviate overfitting. We test our model on the SemEval 2010
relation classification task, and achieve an $F_1$-score of 83.7\%, higher than
competing methods in the literature.
</summary>
    <author>
      <name>Xu Yan</name>
    </author>
    <author>
      <name>Lili Mou</name>
    </author>
    <author>
      <name>Ge Li</name>
    </author>
    <author>
      <name>Yunchuan Chen</name>
    </author>
    <author>
      <name>Hao Peng</name>
    </author>
    <author>
      <name>Zhi Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP '15</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.03720v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.03720v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.06104v2</id>
    <updated>2016-01-19T00:56:08Z</updated>
    <published>2015-11-19T09:44:57Z</published>
    <title>Semi-supervised Learning for Convolutional Neural Networks via Online
  Graph Construction</title>
    <summary>  The recent promising achievements of deep learning rely on the large amount
of labeled data. Considering the abundance of data on the web, most of them do
not have labels at all. Therefore, it is important to improve generalization
performance using unlabeled data on supervised tasks with few labeled
instances. In this work, we revisit graph-based semi-supervised learning
algorithms and propose an online graph construction technique which suits deep
convolutional neural network better. We consider an EM-like algorithm for
semi-supervised learning on deep neural networks: In forward pass, the graph is
constructed based on the network output, and the graph is then used for loss
calculation to help update the network by back propagation in the backward
pass. We demonstrate the strength of our online approach compared to the
conventional ones whose graph is constructed on static but not robust enough
feature representations beforehand.
</summary>
    <author>
      <name>Sheng-Yi Bai</name>
    </author>
    <author>
      <name>Sebastian Agethen</name>
    </author>
    <author>
      <name>Ting-Hsuan Chao</name>
    </author>
    <author>
      <name>Winston Hsu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">As the original submission of iclr is withdrawn, the arxiv submission
  should be withdrawn as well</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.06104v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.06104v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.08301v2</id>
    <updated>2016-01-05T02:40:41Z</updated>
    <published>2015-12-28T02:07:48Z</published>
    <title>Feedforward Sequential Memory Networks: A New Structure to Learn
  Long-term Dependency</title>
    <summary>  In this paper, we propose a novel neural network structure, namely
\emph{feedforward sequential memory networks (FSMN)}, to model long-term
dependency in time series without using recurrent feedback. The proposed FSMN
is a standard fully-connected feedforward neural network equipped with some
learnable memory blocks in its hidden layers. The memory blocks use a
tapped-delay line structure to encode the long context information into a
fixed-size representation as short-term memory mechanism. We have evaluated the
proposed FSMNs in several standard benchmark tasks, including speech
recognition and language modelling. Experimental results have shown FSMNs
significantly outperform the conventional recurrent neural networks (RNN),
including LSTMs, in modeling sequential signals like speech or language.
Moreover, FSMNs can be learned much more reliably and faster than RNNs or LSTMs
due to the inherent non-recurrent model structure.
</summary>
    <author>
      <name>Shiliang Zhang</name>
    </author>
    <author>
      <name>Cong Liu</name>
    </author>
    <author>
      <name>Hui Jiang</name>
    </author>
    <author>
      <name>Si Wei</name>
    </author>
    <author>
      <name>Lirong Dai</name>
    </author>
    <author>
      <name>Yu Hu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.08301v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.08301v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.08806v3</id>
    <updated>2016-05-11T17:56:32Z</updated>
    <published>2015-12-29T22:06:00Z</published>
    <title>Common Variable Learning and Invariant Representation Learning using
  Siamese Neural Networks</title>
    <summary>  We consider the statistical problem of learning common source of variability
in data which are synchronously captured by multiple sensors, and demonstrate
that Siamese neural networks can be naturally applied to this problem. This
approach is useful in particular in exploratory, data-driven applications,
where neither a model nor label information is available. In recent years, many
researchers have successfully applied Siamese neural networks to obtain an
embedding of data which corresponds to a "semantic similarity". We present an
interpretation of this "semantic similarity" as learning of equivalence
classes. We discuss properties of the embedding obtained by Siamese networks
and provide empirical results that demonstrate the ability of Siamese networks
to learn common variability.
</summary>
    <author>
      <name>Uri Shaham</name>
    </author>
    <author>
      <name>Roy Lederman</name>
    </author>
    <link href="http://arxiv.org/abs/1512.08806v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.08806v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.02553v2</id>
    <updated>2016-09-29T22:45:08Z</updated>
    <published>2016-01-11T18:38:18Z</published>
    <title>Environmental Noise Embeddings for Robust Speech Recognition</title>
    <summary>  We propose a novel deep neural network architecture for speech recognition
that explicitly employs knowledge of the background environmental noise within
a deep neural network acoustic model. A deep neural network is used to predict
the acoustic environment in which the system in being used. The discriminative
embedding generated at the bottleneck layer of this network is then
concatenated with traditional acoustic features as input to a deep neural
network acoustic model. Through a series of experiments on Resource Management,
CHiME-3 task, and Aurora4, we show that the proposed approach significantly
improves speech recognition accuracy in noisy and highly reverberant
environments, outperforming multi-condition training, noise-aware training,
i-vector framework, and multi-task learning on both in-domain noise and unseen
noise.
</summary>
    <author>
      <name>Suyoun Kim</name>
    </author>
    <author>
      <name>Bhiksha Raj</name>
    </author>
    <author>
      <name>Ian Lane</name>
    </author>
    <link href="http://arxiv.org/abs/1601.02553v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.02553v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04004v2</id>
    <updated>2016-04-21T20:44:52Z</updated>
    <published>2016-04-14T00:47:50Z</published>
    <title>Understanding How Image Quality Affects Deep Neural Networks</title>
    <summary>  Image quality is an important practical challenge that is often overlooked in
the design of machine vision systems. Commonly, machine vision systems are
trained and tested on high quality image datasets, yet in practical
applications the input images can not be assumed to be of high quality.
Recently, deep neural networks have obtained state-of-the-art performance on
many machine vision tasks. In this paper we provide an evaluation of 4
state-of-the-art deep neural network models for image classification under
quality distortions. We consider five types of quality distortions: blur,
noise, contrast, JPEG, and JPEG2000 compression. We show that the existing
networks are susceptible to these quality distortions, particularly to blur and
noise. These results enable future work in developing deep neural networks that
are more invariant to quality distortions.
</summary>
    <author>
      <name>Samuel Dodge</name>
    </author>
    <author>
      <name>Lina Karam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Final version will appear in IEEE Xplore in the Proceedings of the
  Conference on the Quality of Multimedia Experience (QoMEX), June 6-8, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.04004v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04004v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00611v2</id>
    <updated>2017-03-26T18:31:05Z</updated>
    <published>2016-06-02T10:37:46Z</published>
    <title>Recursive Autoconvolution for Unsupervised Learning of Convolutional
  Neural Networks</title>
    <summary>  In visual recognition tasks, such as image classification, unsupervised
learning exploits cheap unlabeled data and can help to solve these tasks more
efficiently. We show that the recursive autoconvolution operator, adopted from
physics, boosts existing unsupervised methods by learning more discriminative
filters. We take well established convolutional neural networks and train their
filters layer-wise. In addition, based on previous works we design a network
which extracts more than 600k features per sample, but with the total number of
trainable parameters greatly reduced by introducing shared filters in higher
layers. We evaluate our networks on the MNIST, CIFAR-10, CIFAR-100 and STL-10
image classification benchmarks and report several state of the art results
among other unsupervised methods.
</summary>
    <author>
      <name>Boris Knyazev</name>
    </author>
    <author>
      <name>Erhardt Barth</name>
    </author>
    <author>
      <name>Thomas Martinetz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, accepted to International Joint Conference on Neural
  Networks (IJCNN 2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.00611v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00611v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02147v1</id>
    <updated>2016-06-07T14:09:27Z</updated>
    <published>2016-06-07T14:09:27Z</published>
    <title>ENet: A Deep Neural Network Architecture for Real-Time Semantic
  Segmentation</title>
    <summary>  The ability to perform pixel-wise semantic segmentation in real-time is of
paramount importance in mobile applications. Recent deep neural networks aimed
at this task have the disadvantage of requiring a large number of floating
point operations and have long run-times that hinder their usability. In this
paper, we propose a novel deep neural network architecture named ENet
(efficient neural network), created specifically for tasks requiring low
latency operation. ENet is up to 18$\times$ faster, requires 75$\times$ less
FLOPs, has 79$\times$ less parameters, and provides similar or better accuracy
to existing models. We have tested it on CamVid, Cityscapes and SUN datasets
and report on comparisons with existing state-of-the-art methods, and the
trade-offs between accuracy and processing time of a network. We present
performance measurements of the proposed architecture on embedded systems and
suggest possible software improvements that could make ENet even faster.
</summary>
    <author>
      <name>Adam Paszke</name>
    </author>
    <author>
      <name>Abhishek Chaurasia</name>
    </author>
    <author>
      <name>Sangpil Kim</name>
    </author>
    <author>
      <name>Eugenio Culurciello</name>
    </author>
    <link href="http://arxiv.org/abs/1606.02147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04333v2</id>
    <updated>2016-06-15T11:00:47Z</updated>
    <published>2016-06-14T12:57:56Z</published>
    <title>Neither Quick Nor Proper -- Evaluation of QuickProp for Learning Deep
  Neural Networks</title>
    <summary>  Neural networks and especially convolutional neural networks are of great
interest in current computer vision research. However, many techniques,
extensions, and modifications have been published in the past, which are not
yet used by current approaches. In this paper, we study the application of a
method called QuickProp for training of deep neural networks. In particular, we
apply QuickProp during learning and testing of fully convolutional networks for
the task of semantic segmentation. We compare QuickProp empirically with
gradient descent, which is the current standard method. Experiments suggest
that QuickProp can not compete with standard gradient descent techniques for
complex computer vision tasks like semantic segmentation.
</summary>
    <author>
      <name>Clemens-Alexander Brust</name>
    </author>
    <author>
      <name>Sven Sickert</name>
    </author>
    <author>
      <name>Marcel Simon</name>
    </author>
    <author>
      <name>Erik Rodner</name>
    </author>
    <author>
      <name>Joachim Denzler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report, 11 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.04333v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04333v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07767v3</id>
    <updated>2017-02-13T21:25:26Z</updated>
    <published>2016-06-24T17:31:02Z</published>
    <title>Sampling-based Gradient Regularization for Capturing Long-Term
  Dependencies in Recurrent Neural Networks</title>
    <summary>  Vanishing (and exploding) gradients effect is a common problem for recurrent
neural networks with nonlinear activation functions which use backpropagation
method for calculation of derivatives. Deep feedforward neural networks with
many hidden layers also suffer from this effect. In this paper we propose a
novel universal technique that makes the norm of the gradient stay in the
suitable range. We construct a way to estimate a contribution of each training
example to the norm of the long-term components of the target function s
gradient. Using this subroutine we can construct mini-batches for the
stochastic gradient descent (SGD) training that leads to high performance and
accuracy of the trained network even for very complex tasks. We provide a
straightforward mathematical estimation of minibatch s impact on for the
gradient norm and prove its correctness theoretically. To check our framework
experimentally we use some special synthetic benchmarks for testing RNNs on
ability to capture long-term dependencies. Our network can detect links between
events in the (temporal) sequence at the range approx. 100 and longer.
</summary>
    <author>
      <name>Artem Chernodub</name>
    </author>
    <author>
      <name>Dimitri Nowicki</name>
    </author>
    <link href="http://arxiv.org/abs/1606.07767v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07767v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00410v1</id>
    <updated>2016-07-01T21:24:21Z</updated>
    <published>2016-07-01T21:24:21Z</published>
    <title>Domain Adaptation for Neural Networks by Parameter Augmentation</title>
    <summary>  We propose a simple domain adaptation method for neural networks in a
supervised setting. Supervised domain adaptation is a way of improving the
generalization performance on the target domain by using the source domain
dataset, assuming that both of the datasets are labeled. Recently, recurrent
neural networks have been shown to be successful on a variety of NLP tasks such
as caption generation; however, the existing domain adaptation techniques are
limited to (1) tune the model parameters by the target dataset after the
training by the source dataset, or (2) design the network to have dual output,
one for the source domain and the other for the target domain. Reformulating
the idea of the domain adaptation technique proposed by Daume (2007), we
propose a simple domain adaptation method, which can be applied to neural
networks trained with a cross-entropy loss. On captioning datasets, we show
performance improvements over other domain adaptation methods.
</summary>
    <author>
      <name>Yusuke Watanabe</name>
    </author>
    <author>
      <name>Kazuma Hashimoto</name>
    </author>
    <author>
      <name>Yoshimasa Tsuruoka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 page. To appear in the first ACL Workshop on Representation
  Learning for NLP</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.00410v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00410v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04980v1</id>
    <updated>2016-08-17T14:37:34Z</updated>
    <published>2016-08-17T14:37:34Z</published>
    <title>Mollifying Networks</title>
    <summary>  The optimization of deep neural networks can be more challenging than
traditional convex optimization problems due to the highly non-convex nature of
the loss function, e.g. it can involve pathological landscapes such as
saddle-surfaces that can be difficult to escape for algorithms based on simple
gradient descent. In this paper, we attack the problem of optimization of
highly non-convex neural networks by starting with a smoothed -- or
\textit{mollified} -- objective function that gradually has a more non-convex
energy landscape during the training. Our proposition is inspired by the recent
studies in continuation methods: similar to curriculum methods, we begin
learning an easier (possibly convex) objective function and let it evolve
during the training, until it eventually goes back to being the original,
difficult to optimize, objective function. The complexity of the mollified
networks is controlled by a single hyperparameter which is annealed during the
training. We show improvements on various difficult optimization tasks and
establish a relationship with recent works on continuation methods for neural
networks and mollifiers.
</summary>
    <author>
      <name>Caglar Gulcehre</name>
    </author>
    <author>
      <name>Marcin Moczulski</name>
    </author>
    <author>
      <name>Francesco Visin</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <link href="http://arxiv.org/abs/1608.04980v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04980v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02228v2</id>
    <updated>2016-10-19T17:51:25Z</updated>
    <published>2016-09-08T00:02:20Z</published>
    <title>Learning to learn with backpropagation of Hebbian plasticity</title>
    <summary>  Hebbian plasticity is a powerful principle that allows biological brains to
learn from their lifetime experience. By contrast, artificial neural networks
trained with backpropagation generally have fixed connection weights that do
not change once training is complete. While recent methods can endow neural
networks with long-term memories, Hebbian plasticity is currently not amenable
to gradient descent. Here we derive analytical expressions for activity
gradients in neural networks with Hebbian plastic connections. Using these
expressions, we can use backpropagation to train not just the baseline weights
of the connections, but also their plasticity. As a result, the networks "learn
how to learn" in order to solve the problem at hand: the trained networks
automatically perform fast learning of unpredictable environmental features
during their lifetime, expanding the range of solvable problems. We test the
algorithm on various on-line learning tasks, including pattern completion,
one-shot learning, and reversal learning. The algorithm successfully learns how
to learn the relevant associations from one-shot instruction, and fine-tunes
the temporal dynamics of plasticity to allow for continual learning in response
to changing environmental parameters. We conclude that backpropagation of
Hebbian plasticity offers a powerful model for lifelong learning.
</summary>
    <author>
      <name>Thomas Miconi</name>
    </author>
    <link href="http://arxiv.org/abs/1609.02228v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02228v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.02365v1</id>
    <updated>2016-10-07T18:30:10Z</updated>
    <published>2016-10-07T18:30:10Z</published>
    <title>Deep Learning with Coherent Nanophotonic Circuits</title>
    <summary>  Artificial Neural Networks are computational network models inspired by
signal processing in the brain. These models have dramatically improved the
performance of many learning tasks, including speech and object recognition.
However, today's computing hardware is inefficient at implementing neural
networks, in large part because much of it was designed for von Neumann
computing schemes. Significant effort has been made to develop electronic
architectures tuned to implement artificial neural networks that improve upon
both computational speed and energy efficiency. Here, we propose a new
architecture for a fully-optical neural network that, using unique advantages
of optics, promises a computational speed enhancement of at least two orders of
magnitude over the state-of-the-art and three orders of magnitude in power
efficiency for conventional learning tasks. We experimentally demonstrate
essential parts of our architecture using a programmable nanophotonic
processor.
</summary>
    <author>
      <name>Yichen Shen</name>
    </author>
    <author>
      <name>Nicholas C. Harris</name>
    </author>
    <author>
      <name>Scott Skirlo</name>
    </author>
    <author>
      <name>Mihika Prabhu</name>
    </author>
    <author>
      <name>Tom Baehr-Jones</name>
    </author>
    <author>
      <name>Michael Hochberg</name>
    </author>
    <author>
      <name>Xin Sun</name>
    </author>
    <author>
      <name>Shijie Zhao</name>
    </author>
    <author>
      <name>Hugo Larochelle</name>
    </author>
    <author>
      <name>Dirk Englund</name>
    </author>
    <author>
      <name>Marin Soljacic</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/nphoton.2017.93</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/nphoton.2017.93" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.02365v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.02365v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.07857v1</id>
    <updated>2016-10-21T09:11:53Z</updated>
    <published>2016-10-21T09:11:53Z</published>
    <title>Hybrid clustering-classification neural network in the medical
  diagnostics of reactive arthritis</title>
    <summary>  The hybrid clustering-classification neural network is proposed. This network
allows increasing a quality of information processing under the condition of
overlapping classes due to the rational choice of a learning rate parameter and
introducing a special procedure of fuzzy reasoning in the clustering process,
which occurs both with an external learning signal (supervised) and without the
one (unsupervised). As similarity measure neighborhood function or membership
one, cosine structures are used, which allow to provide a high flexibility due
to self-learning-learning process and to provide some new useful properties.
Many realized experiments have confirmed the efficiency of proposed hybrid
clustering-classification neural network; also, this network was used for
solving diagnostics task of reactive arthritis.
</summary>
    <author>
      <name>Yevgeniy Bodyanskiy</name>
    </author>
    <author>
      <name>Olena Vynokurova</name>
    </author>
    <author>
      <name>Volodymyr Savvo</name>
    </author>
    <author>
      <name>Tatiana Tverdokhlib</name>
    </author>
    <author>
      <name>Pavlo Mulesa</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Intelligent Systems and Applications,
  2016, Vol. 8, No. 8, pp.1-9</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.07857v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.07857v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.02767v1</id>
    <updated>2016-11-08T23:18:50Z</updated>
    <published>2016-11-08T23:18:50Z</published>
    <title>A backward pass through a CNN using a generative model of its
  activations</title>
    <summary>  Neural networks have shown to be a practical way of building a very complex
mapping between a pre-specified input space and output space. For example, a
convolutional neural network (CNN) mapping an image into one of a thousand
object labels is approaching human performance in this particular task. However
the mapping (neural network) does not automatically lend itself to other forms
of queries, for example, to detect/reconstruct object instances, to enforce
top-down signal on ambiguous inputs, or to recover object instances from
occlusion. One way to address these queries is a backward pass through the
network that fuses top-down and bottom-up information. In this paper, we show a
way of building such a backward pass by defining a generative model of the
neural network's activations. Approximate inference of the model would
naturally take the form of a backward pass through the CNN layers, and it
addresses the aforementioned queries in a unified framework.
</summary>
    <author>
      <name>Huayan Wang</name>
    </author>
    <author>
      <name>Anna Chen</name>
    </author>
    <author>
      <name>Yi Liu</name>
    </author>
    <author>
      <name>Dileep George</name>
    </author>
    <author>
      <name>D. Scott Phoenix</name>
    </author>
    <link href="http://arxiv.org/abs/1611.02767v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.02767v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.06455v4</id>
    <updated>2016-12-14T06:58:08Z</updated>
    <published>2016-11-20T00:34:09Z</published>
    <title>Time Series Classification from Scratch with Deep Neural Networks: A
  Strong Baseline</title>
    <summary>  We propose a simple but strong baseline for time series classification from
scratch with deep neural networks. Our proposed baseline models are pure
end-to-end without any heavy preprocessing on the raw data or feature crafting.
The proposed Fully Convolutional Network (FCN) achieves premium performance to
other state-of-the-art approaches and our exploration of the very deep neural
networks with the ResNet structure is also competitive. The global average
pooling in our convolutional model enables the exploitation of the Class
Activation Map (CAM) to find out the contributing region in the raw data for
the specific labels. Our models provides a simple choice for the real world
application and a good starting point for the future research. An overall
analysis is provided to discuss the generalization capability of our models,
learned features, network structures and the classification semantics.
</summary>
    <author>
      <name>Zhiguang Wang</name>
    </author>
    <author>
      <name>Weizhong Yan</name>
    </author>
    <author>
      <name>Tim Oates</name>
    </author>
    <link href="http://arxiv.org/abs/1611.06455v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.06455v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.04052v1</id>
    <updated>2016-12-13T07:58:34Z</updated>
    <published>2016-12-13T07:58:34Z</published>
    <title>Theory and Tools for the Conversion of Analog to Spiking Convolutional
  Neural Networks</title>
    <summary>  Deep convolutional neural networks (CNNs) have shown great potential for
numerous real-world machine learning applications, but performing inference in
large CNNs in real-time remains a challenge. We have previously demonstrated
that traditional CNNs can be converted into deep spiking neural networks
(SNNs), which exhibit similar accuracy while reducing both latency and
computational load as a consequence of their data-driven, event-based style of
computing. Here we provide a novel theory that explains why this conversion is
successful, and derive from it several new tools to convert a larger and more
powerful class of deep networks into SNNs. We identify the main sources of
approximation errors in previous conversion methods, and propose simple
mechanisms to fix these issues. Furthermore, we develop spiking implementations
of common CNN operations such as max-pooling, softmax, and batch-normalization,
which allow almost loss-less conversion of arbitrary CNN architectures into the
spiking domain. Empirical evaluation of different network architectures on the
MNIST and CIFAR10 benchmarks leads to the best SNN results reported to date.
</summary>
    <author>
      <name>Bodo Rueckauer</name>
    </author>
    <author>
      <name>Iulia-Alexandra Lungu</name>
    </author>
    <author>
      <name>Yuhuang Hu</name>
    </author>
    <author>
      <name>Michael Pfeiffer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 2 figures, presented at the workshop "Computing with Spikes"
  at NIPS 2016, Barcelona, Spain</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.04052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.04052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.08059v1</id>
    <updated>2016-12-23T18:28:12Z</updated>
    <published>2016-12-23T18:28:12Z</published>
    <title>Emerging Frontiers of Neuroengineering: A Network Science of Brain
  Connectivity</title>
    <summary>  Neuroengineering is faced with unique challenges in repairing or replacing
complex neural systems that are composed of many interacting parts. These
interactions form intricate patterns over large spatiotemporal scales, and
produce emergent behaviors that are difficult to predict from individual
elements. Network science provides a particularly appropriate framework in
which to study and intervene in such systems, by treating neural elements
(cells, volumes) as nodes in a graph and neural interactions (synapses, white
matter tracts) as edges in that graph. Here, we review the emerging discipline
of network neuroscience, which uses and develops tools from graph theory to
better understand and manipulate neural systems, from micro- to macroscales. We
present examples of how human brain imaging data is being modeled with network
analysis and underscore potential pitfalls. We then highlight current
computational and theoretical frontiers, and emphasize their utility in
informing diagnosis and monitoring, brain-machine interfaces, and brain
stimulation. A flexible and rapidly evolving enterprise, network neuroscience
provides a set of powerful approaches and fundamental insights critical to the
neuroengineer's toolkit.
</summary>
    <author>
      <name>Danielle S. Bassett</name>
    </author>
    <author>
      <name>Ankit N. Khambhati</name>
    </author>
    <author>
      <name>Scott T. Grafton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 6 figures. Manuscript accepted to the journal "Annual
  Review of Biomedical Engineering"</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.08059v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.08059v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.00299v3</id>
    <updated>2018-03-05T02:03:00Z</updated>
    <published>2017-01-02T00:09:14Z</published>
    <title>Dynamic Deep Neural Networks: Optimizing Accuracy-Efficiency Trade-offs
  by Selective Execution</title>
    <summary>  We introduce Dynamic Deep Neural Networks (D2NN), a new type of feed-forward
deep neural network that allows selective execution. Given an input, only a
subset of D2NN neurons are executed, and the particular subset is determined by
the D2NN itself. By pruning unnecessary computation depending on input, D2NNs
provide a way to improve computational efficiency. To achieve dynamic selective
execution, a D2NN augments a feed-forward deep neural network (directed acyclic
graph of differentiable modules) with controller modules. Each controller
module is a sub-network whose output is a decision that controls whether other
modules can execute. A D2NN is trained end to end. Both regular and controller
modules in a D2NN are learnable and are jointly trained to optimize both
accuracy and efficiency. Such training is achieved by integrating
backpropagation with reinforcement learning. With extensive experiments of
various D2NN architectures on image classification tasks, we demonstrate that
D2NNs are general and flexible, and can effectively optimize
accuracy-efficiency trade-offs.
</summary>
    <author>
      <name>Lanlan Liu</name>
    </author>
    <author>
      <name>Jia Deng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">fixed typos; updated CIFAR-10 results and added more details;
  corrected the cascade D2NN configuration details</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.00299v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.00299v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.04923v1</id>
    <updated>2017-01-18T01:57:55Z</updated>
    <published>2017-01-18T01:57:55Z</published>
    <title>Compression of Deep Neural Networks for Image Instance Retrieval</title>
    <summary>  Image instance retrieval is the problem of retrieving images from a database
which contain the same object. Convolutional Neural Network (CNN) based
descriptors are becoming the dominant approach for generating {\it global image
descriptors} for the instance retrieval problem. One major drawback of
CNN-based {\it global descriptors} is that uncompressed deep neural network
models require hundreds of megabytes of storage making them inconvenient to
deploy in mobile applications or in custom hardware. In this work, we study the
problem of neural network model compression focusing on the image instance
retrieval task. We study quantization, coding, pruning and weight sharing
techniques for reducing model size for the instance retrieval problem. We
provide extensive experimental results on the trade-off between retrieval
performance and model size for different types of networks on several data sets
providing the most comprehensive study on this topic. We compress models to the
order of a few MBs: two orders of magnitude smaller than the uncompressed
models while achieving negligible loss in retrieval performance.
</summary>
    <author>
      <name>Vijay Chandrasekhar</name>
    </author>
    <author>
      <name>Jie Lin</name>
    </author>
    <author>
      <name>Qianli Liao</name>
    </author>
    <author>
      <name>Olivier Mor√®re</name>
    </author>
    <author>
      <name>Antoine Veillard</name>
    </author>
    <author>
      <name>Lingyu Duan</name>
    </author>
    <author>
      <name>Tomaso Poggio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, accepted by DCC 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.04923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.04923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.00177v1</id>
    <updated>2017-02-01T09:41:52Z</updated>
    <published>2017-02-01T09:41:52Z</published>
    <title>PCA-Initialized Deep Neural Networks Applied To Document Image Analysis</title>
    <summary>  In this paper, we present a novel approach for initializing deep neural
networks, i.e., by turning PCA into neural layers. Usually, the initialization
of the weights of a deep neural network is done in one of the three following
ways: 1) with random values, 2) layer-wise, usually as Deep Belief Network or
as auto-encoder, and 3) re-use of layers from another network (transfer
learning). Therefore, typically, many training epochs are needed before
meaningful weights are learned, or a rather similar dataset is required for
seeding a fine-tuning of transfer learning. In this paper, we describe how to
turn a PCA into an auto-encoder, by generating an encoder layer of the PCA
parameters and furthermore adding a decoding layer. We analyze the
initialization technique on real documents. First, we show that a PCA-based
initialization is quick and leads to a very stable initialization. Furthermore,
for the task of layout analysis we investigate the effectiveness of PCA-based
initialization and show that it outperforms state-of-the-art random weight
initialization methods.
</summary>
    <author>
      <name>Mathias Seuret</name>
    </author>
    <author>
      <name>Michele Alberti</name>
    </author>
    <author>
      <name>Rolf Ingold</name>
    </author>
    <author>
      <name>Marcus Liwicki</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICDAR 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1702.00177v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.00177v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.01997v1</id>
    <updated>2017-02-07T13:16:05Z</updated>
    <published>2017-02-07T13:16:05Z</published>
    <title>Truncated Variational EM for Semi-Supervised Neural Simpletrons</title>
    <summary>  Inference and learning for probabilistic generative networks is often very
challenging and typically prevents scalability to as large networks as used for
deep discriminative approaches. To obtain efficiently trainable, large-scale
and well performing generative networks for semi-supervised learning, we here
combine two recent developments: a neural network reformulation of hierarchical
Poisson mixtures (Neural Simpletrons), and a novel truncated variational EM
approach (TV-EM). TV-EM provides theoretical guarantees for learning in
generative networks, and its application to Neural Simpletrons results in
particularly compact, yet approximately optimal, modifications of learning
equations. If applied to standard benchmarks, we empirically find, that
learning converges in fewer EM iterations, that the complexity per EM iteration
is reduced, and that final likelihood values are higher on average. For the
task of classification on data sets with few labels, learning improvements
result in consistently lower error rates if compared to applications without
truncation. Experiments on the MNIST data set herein allow for comparison to
standard and state-of-the-art models in the semi-supervised setting. Further
experiments on the NIST SD19 data set show the scalability of the approach when
a manifold of additional unlabeled data is available.
</summary>
    <author>
      <name>Dennis Forster</name>
    </author>
    <author>
      <name>J√∂rg L√ºcke</name>
    </author>
    <link href="http://arxiv.org/abs/1702.01997v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.01997v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.03833v1</id>
    <updated>2017-02-13T15:43:52Z</updated>
    <published>2017-02-13T15:43:52Z</published>
    <title>Estimation of the volume of the left ventricle from MRI images using
  deep neural networks</title>
    <summary>  Segmenting human left ventricle (LV) in magnetic resonance imaging (MRI)
images and calculating its volume are important for diagnosing cardiac
diseases. In 2016, Kaggle organized a competition to estimate the volume of LV
from MRI images. The dataset consisted of a large number of cases, but only
provided systole and diastole volumes as labels. We designed a system based on
neural networks to solve this problem. It began with a detector combined with a
neural network classifier for detecting regions of interest (ROIs) containing
LV chambers. Then a deep neural network named hypercolumns fully convolutional
network was used to segment LV in ROIs. The 2D segmentation results were
integrated across different images to estimate the volume. With ground-truth
volume labels, this model was trained end-to-end. To improve the result, an
additional dataset with only segmentation label was used. The model was trained
alternately on these two datasets with different types of teaching signals. We
also proposed a variance estimation method for the final prediction. Our
algorithm ranked the 4th on the test set in this competition.
</summary>
    <author>
      <name>Fangzhou Liao</name>
    </author>
    <author>
      <name>Xi Chen</name>
    </author>
    <author>
      <name>Xiaolin Hu</name>
    </author>
    <author>
      <name>Sen Song</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TCYB.2017.2778799</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TCYB.2017.2778799" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.03833v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.03833v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.05419v2</id>
    <updated>2017-06-29T08:27:26Z</updated>
    <published>2017-02-17T16:16:01Z</published>
    <title>A Random Matrix Approach to Neural Networks</title>
    <summary>  This article studies the Gram random matrix model $G=\frac1T\Sigma^{\rm
T}\Sigma$, $\Sigma=\sigma(WX)$, classically found in the analysis of random
feature maps and random neural networks, where $X=[x_1,\ldots,x_T]\in{\mathbb
R}^{p\times T}$ is a (data) matrix of bounded norm, $W\in{\mathbb R}^{n\times
p}$ is a matrix of independent zero-mean unit variance entries, and
$\sigma:{\mathbb R}\to{\mathbb R}$ is a Lipschitz continuous (activation)
function --- $\sigma(WX)$ being understood entry-wise. By means of a key
concentration of measure lemma arising from non-asymptotic random matrix
arguments, we prove that, as $n,p,T$ grow large at the same rate, the resolvent
$Q=(G+\gamma I_T)^{-1}$, for $\gamma&gt;0$, has a similar behavior as that met in
sample covariance matrix models, involving notably the moment
$\Phi=\frac{T}n{\mathbb E}[G]$, which provides in passing a deterministic
equivalent for the empirical spectral measure of $G$. Application-wise, this
result enables the estimation of the asymptotic performance of single-layer
random neural networks. This in turn provides practical insights into the
underlying mechanisms into play in random neural networks, entailing several
unexpected consequences, as well as a fast practical means to tune the network
hyperparameters.
</summary>
    <author>
      <name>Cosme Louart</name>
    </author>
    <author>
      <name>Zhenyu Liao</name>
    </author>
    <author>
      <name>Romain Couillet</name>
    </author>
    <link href="http://arxiv.org/abs/1702.05419v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.05419v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.05870v5</id>
    <updated>2017-10-23T03:31:59Z</updated>
    <published>2017-02-20T06:17:02Z</published>
    <title>Cosine Normalization: Using Cosine Similarity Instead of Dot Product in
  Neural Networks</title>
    <summary>  Traditionally, multi-layer neural networks use dot product between the output
vector of previous layer and the incoming weight vector as the input to
activation function. The result of dot product is unbounded, thus increases the
risk of large variance. Large variance of neuron makes the model sensitive to
the change of input distribution, thus results in poor generalization, and
aggravates the internal covariate shift which slows down the training. To bound
dot product and decrease the variance, we propose to use cosine similarity or
centered cosine similarity (Pearson Correlation Coefficient) instead of dot
product in neural networks, which we call cosine normalization. We compare
cosine normalization with batch, weight and layer normalization in
fully-connected neural networks as well as convolutional networks on the data
sets of MNIST, 20NEWS GROUP, CIFAR-10/100 and SVHN. Experiments show that
cosine normalization achieves better performance than other normalization
techniques.
</summary>
    <author>
      <name>Chunjie Luo</name>
    </author>
    <author>
      <name>Jianfeng Zhan</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Qiang Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1702.05870v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.05870v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.08640v5</id>
    <updated>2017-05-03T19:46:16Z</updated>
    <published>2017-03-25T02:50:00Z</published>
    <title>Bond Energies from a Diatomics-in-Molecules Neural Network</title>
    <summary>  Neural networks are being used to make new types of empirical chemical models
as inexpensive as force fields, but with accuracy close to the ab-initio
methods used to build them. Besides modeling potential energy surfaces,
neural-nets can provide qualitative insights and make qualitative chemical
trends quantitatively predictable. In this work we present a neural-network
that predicts the energies of molecules as a sum of bond energies. The network
learns the total energies of the popular GDB9 dataset to a competitive MAE of
0.94 kcal/mol. The method is naturally linearly scaling, and applicable to
molecules of nanoscopic size. More importantly it gives chemical insight into
the relative strengths of bonds as a function of their molecular environment,
despite only being trained on total energy information. We show that the
network makes predictions of relative bond strengths in good agreement with
measured trends and human predictions. We show that DIM-NN learns the same
heuristic trends in relative bond strength developed by expert synthetic
chemists, and ab-initio bond order measures such as NBO analysis.
</summary>
    <author>
      <name>Kun Yao</name>
    </author>
    <author>
      <name>John Herr</name>
    </author>
    <author>
      <name>Seth Brown</name>
    </author>
    <author>
      <name>John Parkhill</name>
    </author>
    <link href="http://arxiv.org/abs/1703.08640v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.08640v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.05598v2</id>
    <updated>2017-10-24T23:10:33Z</updated>
    <published>2017-05-16T08:58:25Z</published>
    <title>Learning how to explain neural networks: PatternNet and
  PatternAttribution</title>
    <summary>  DeConvNet, Guided BackProp, LRP, were invented to better understand deep
neural networks. We show that these methods do not produce the theoretically
correct explanation for a linear model. Yet they are used on multi-layer
networks with millions of parameters. This is a cause for concern since linear
models are simple neural networks. We argue that explanation methods for neural
nets should work reliably in the limit of simplicity, the linear models. Based
on our analysis of linear models we propose a generalization that yields two
explanation techniques (PatternNet and PatternAttribution) that are
theoretically sound for linear models and produce improved explanations for
deep networks.
</summary>
    <author>
      <name>Pieter-Jan Kindermans</name>
    </author>
    <author>
      <name>Kristof T. Sch√ºtt</name>
    </author>
    <author>
      <name>Maximilian Alber</name>
    </author>
    <author>
      <name>Klaus-Robert M√ºller</name>
    </author>
    <author>
      <name>Dumitru Erhan</name>
    </author>
    <author>
      <name>Been Kim</name>
    </author>
    <author>
      <name>Sven D√§hne</name>
    </author>
    <link href="http://arxiv.org/abs/1705.05598v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.05598v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.05665v1</id>
    <updated>2017-05-16T12:09:44Z</updated>
    <published>2017-05-16T12:09:44Z</published>
    <title>Learning Image Relations with Contrast Association Networks</title>
    <summary>  Inferring the relations between two images is an important class of tasks in
computer vision. Examples of such tasks include computing optical flow and
stereo disparity. We treat the relation inference tasks as a machine learning
problem and tackle it with neural networks. A key to the problem is learning a
representation of relations. We propose a new neural network module, contrast
association unit (CAU), which explicitly models the relations between two sets
of input variables. Due to the non-negativity of the weights in CAU, we adopt a
multiplicative update algorithm for learning these weights. Experiments show
that neural networks with CAUs are more effective in learning five fundamental
image transformations than conventional neural networks.
</summary>
    <author>
      <name>Yao Lu</name>
    </author>
    <author>
      <name>Zhirong Yang</name>
    </author>
    <author>
      <name>Juho Kannala</name>
    </author>
    <author>
      <name>Samuel Kaski</name>
    </author>
    <link href="http://arxiv.org/abs/1705.05665v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.05665v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.03256v1</id>
    <updated>2017-06-10T17:26:20Z</updated>
    <published>2017-06-10T17:26:20Z</published>
    <title>Progressive Neural Networks for Transfer Learning in Emotion Recognition</title>
    <summary>  Many paralinguistic tasks are closely related and thus representations
learned in one domain can be leveraged for another. In this paper, we
investigate how knowledge can be transferred between three paralinguistic
tasks: speaker, emotion, and gender recognition. Further, we extend this
problem to cross-dataset tasks, asking how knowledge captured in one emotion
dataset can be transferred to another. We focus on progressive neural networks
and compare these networks to the conventional deep learning method of
pre-training and fine-tuning. Progressive neural networks provide a way to
transfer knowledge and avoid the forgetting effect present when pre-training
neural networks on different tasks. Our experiments demonstrate that: (1)
emotion recognition can benefit from using representations originally learned
for different paralinguistic tasks and (2) transfer learning can effectively
leverage additional datasets to improve the performance of emotion recognition
systems.
</summary>
    <author>
      <name>John Gideon</name>
    </author>
    <author>
      <name>Soheil Khorram</name>
    </author>
    <author>
      <name>Zakaria Aldeneh</name>
    </author>
    <author>
      <name>Dimitrios Dimitriadis</name>
    </author>
    <author>
      <name>Emily Mower Provost</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures, to appear in the proceedings of Interspeech 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.03256v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.03256v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.04265v2</id>
    <updated>2017-06-22T09:50:41Z</updated>
    <published>2017-06-13T21:57:53Z</published>
    <title>Transfer entropy-based feedback improves performance in artificial
  neural networks</title>
    <summary>  The structure of the majority of modern deep neural networks is characterized
by uni- directional feed-forward connectivity across a very large number of
layers. By contrast, the architecture of the cortex of vertebrates contains
fewer hierarchical levels but many recurrent and feedback connections. Here we
show that a small, few-layer artificial neural network that employs feedback
will reach top level performance on a standard benchmark task, otherwise only
obtained by large feed-forward structures. To achieve this we use feed-forward
transfer entropy between neurons to structure feedback connectivity. Transfer
entropy can here intuitively be understood as a measure for the relevance of
certain pathways in the network, which are then amplified by feedback. Feedback
may therefore be key for high network performance in small brain-like
architectures.
</summary>
    <author>
      <name>Sebastian Herzog</name>
    </author>
    <author>
      <name>Christian Tetzlaff</name>
    </author>
    <author>
      <name>Florentin W√∂rg√∂tter</name>
    </author>
    <link href="http://arxiv.org/abs/1706.04265v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.04265v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.06083v3</id>
    <updated>2017-11-09T01:16:40Z</updated>
    <published>2017-06-19T17:53:11Z</published>
    <title>Towards Deep Learning Models Resistant to Adversarial Attacks</title>
    <summary>  Recent work has demonstrated that neural networks are vulnerable to
adversarial examples, i.e., inputs that are almost indistinguishable from
natural data and yet classified incorrectly by the network. In fact, some of
the latest findings suggest that the existence of adversarial attacks may be an
inherent weakness of deep learning models. To address this problem, we study
the adversarial robustness of neural networks through the lens of robust
optimization. This approach provides us with a broad and unifying view on much
of the prior work on this topic. Its principled nature also enables us to
identify methods for both training and attacking neural networks that are
reliable and, in a certain sense, universal. In particular, they specify a
concrete security guarantee that would protect against any adversary. These
methods let us train networks with significantly improved resistance to a wide
range of adversarial attacks. They also suggest the notion of security against
a first-order adversary as a natural and broad security guarantee. We believe
that robustness against such well-defined classes of adversaries is an
important stepping stone towards fully resistant deep learning models.
</summary>
    <author>
      <name>Aleksander Madry</name>
    </author>
    <author>
      <name>Aleksandar Makelov</name>
    </author>
    <author>
      <name>Ludwig Schmidt</name>
    </author>
    <author>
      <name>Dimitris Tsipras</name>
    </author>
    <author>
      <name>Adrian Vladu</name>
    </author>
    <link href="http://arxiv.org/abs/1706.06083v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.06083v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07178v1</id>
    <updated>2017-06-22T06:53:19Z</updated>
    <published>2017-06-22T06:53:19Z</published>
    <title>Shape recognition of volcanic ash by simple convolutional neural network</title>
    <summary>  Shape analyses of tephra grains result in understanding eruption mechanism of
volcanoes. However, we have to define and select parameter set such as
convexity for the precise discrimination of tephra grains. Selection of the
best parameter set for the recognition of tephra shapes is complicated.
Actually, many shape parameters have been suggested. Recently, neural network
has made a great success in the field of machine learning. Convolutional neural
network can recognize the shape of images without human bias and shape
parameters. We applied the simple convolutional neural network developed for
the handwritten digits to the recognition of tephra shapes. The network was
trained by Morphologi tephra images, and it can recognize the tephra shapes
with approximately 90% of accuracy.
</summary>
    <author>
      <name>Daigo Shoji</name>
    </author>
    <author>
      <name>Rina Noguchi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.07178v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07178v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.geo-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.geo-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.02444v2</id>
    <updated>2018-02-01T03:37:54Z</updated>
    <published>2017-07-08T14:04:37Z</published>
    <title>Global optimality conditions for deep neural networks</title>
    <summary>  We study the error landscape of deep linear and nonlinear neural networks
with the squared error loss. Minimizing the loss of a deep linear neural
network is a nonconvex problem, and despite recent progress, our understanding
of this loss surface is still incomplete. For deep linear networks, we present
necessary and sufficient conditions for a critical point of the risk function
to be a global minimum. Our conditions provide an efficiently checkable test
for global optimality, which is remarkable because such tests are typically
intractable in nonconvex optimization. We further extend these results to deep
nonlinear neural networks and prove similar sufficient conditions for global
optimality, albeit in a more limited function space setting.
</summary>
    <author>
      <name>Chulhee Yun</name>
    </author>
    <author>
      <name>Suvrit Sra</name>
    </author>
    <author>
      <name>Ali Jadbabaie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages. Accepted to ICLR 2018. NOT a camera-ready version</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.02444v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.02444v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.07287v2</id>
    <updated>2018-01-20T13:54:40Z</updated>
    <published>2017-07-23T12:07:58Z</published>
    <title>Learning uncertainty in regression tasks by artificial neural networks</title>
    <summary>  We suggest a general approach to quantification of different forms of
uncertainty in regression tasks performed by artificial neural networks. It is
based on the simultaneous training of two neural networks with a joint loss
function. One of the networks performs predictions and the other simultaneously
quantifies the uncertainty of predictions by estimating the locally averaged
loss of the first one. Unlike in many classical uncertainty quantification
methods, the targets are not assumed to be sampled from a probability
distribution of an a priori given form. We analyze how the hyperparameters
affect the learning process and, additionally, show that our method even allows
for better predictions compared to standard neural networks without uncertainty
counterparts. Finally, we show that particular cases of our approach include
maximization of log-likelihood, assuming Gaussian or Laplace noise.
</summary>
    <author>
      <name>Pavel Gurevich</name>
    </author>
    <author>
      <name>Hannes Stuke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.07287v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.07287v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09520v2</id>
    <updated>2017-11-14T21:30:51Z</updated>
    <published>2017-07-29T14:37:48Z</published>
    <title>Orthogonal Recurrent Neural Networks with Scaled Cayley Transform</title>
    <summary>  Recurrent Neural Networks (RNNs) are designed to handle sequential data but
suffer from vanishing or exploding gradients. Recent work on Unitary Recurrent
Neural Networks (uRNNs) have been used to address this issue and in some cases,
exceed the capabilities of Long Short-Term Memory networks (LSTMs). We propose
a simpler and novel update scheme to maintain orthogonal recurrent weight
matrices without using complex valued matrices. This is done by parametrizing
with a skew-symmetric matrix using the Cayley transform. Such a parametrization
is unable to represent matrices with negative one eigenvalues, but this
limitation is overcome by scaling the recurrent weight matrix by a diagonal
matrix consisting of ones and negative ones. The proposed training scheme
involves a straightforward gradient calculation and update step. In several
experiments, the proposed scaled Cayley orthogonal recurrent neural network
(scoRNN) achieves superior results with fewer trainable parameters than other
unitary RNNs.
</summary>
    <author>
      <name>Kyle Helfrich</name>
    </author>
    <author>
      <name>Devin Willmott</name>
    </author>
    <author>
      <name>Qiang Ye</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.09520v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09520v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.01867v4</id>
    <updated>2017-12-22T14:11:44Z</updated>
    <published>2017-09-06T15:53:16Z</published>
    <title>Neural Networks Regularization Through Class-wise Invariant
  Representation Learning</title>
    <summary>  Training deep neural networks is known to require a large number of training
samples. However, in many applications only few training samples are available.
In this work, we tackle the issue of training neural networks for
classification task when few training samples are available. We attempt to
solve this issue by proposing a new regularization term that constrains the
hidden layers of a network to learn class-wise invariant representations. In
our regularization framework, learning invariant representations is generalized
to the class membership where samples with the same class should have the same
representation. Numerical experiments over MNIST and its variants showed that
our proposal helps improving the generalization of neural network particularly
when trained with few samples. We provide the source code of our framework
https://github.com/sbelharbi/learning-class-invariant-features .
</summary>
    <author>
      <name>Soufiane Belharbi</name>
    </author>
    <author>
      <name>Cl√©ment Chatelain</name>
    </author>
    <author>
      <name>Romain H√©rault</name>
    </author>
    <author>
      <name>S√©bastien Adam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ELSEVIER, 13 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.01867v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.01867v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.02291v2</id>
    <updated>2017-12-14T11:25:18Z</updated>
    <published>2017-09-07T14:51:37Z</published>
    <title>Basic Filters for Convolutional Neural Networks Applied to Music:
  Training or Design?</title>
    <summary>  When convolutional neural networks are used to tackle learning problems based
on music or, more generally, time series data, raw one-dimensional data are
commonly pre-processed to obtain spectrogram or mel-spectrogram coefficients,
which are then used as input to the actual neural network. In this
contribution, we investigate, both theoretically and experimentally, the
influence of this pre-processing step on the network's performance and pose the
question, whether replacing it by applying adaptive or learned filters directly
to the raw data, can improve learning success. The theoretical results show
that approximately reproducing mel-spectrogram coefficients by applying
adaptive filters and subsequent time-averaging is in principle possible. We
also conducted extensive experimental work on the task of singing voice
detection in music. The results of these experiments show that for
classification based on Convolutional Neural Networks the features obtained
from adaptive filter banks followed by time-averaging perform better than the
canonical Fourier-transform-based mel-spectrogram coefficients. Alternative
adaptive approaches with center frequencies or time-averaging lengths learned
from training data perform equally well.
</summary>
    <author>
      <name>Monika Doerfler</name>
    </author>
    <author>
      <name>Thomas Grill</name>
    </author>
    <author>
      <name>Roswitha Bammer</name>
    </author>
    <author>
      <name>Arthur Flexer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Completely revised version; 21 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.02291v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.02291v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.09247v2</id>
    <updated>2018-01-26T16:27:08Z</updated>
    <published>2017-09-26T20:20:25Z</published>
    <title>Stochastic Spiking Neural Networks Enabled by Magnetic Tunnel Junctions:
  From Nontelegraphic to Telegraphic Switching Regimes</title>
    <summary>  Stochastic spiking neural networks based on nanoelectronic spin devices can
be a possible pathway to achieving "brainlike" compact and energy-effcient
cognitive intelligence. The computational model attempt to exploit the
intrinsic device stochasticity of nanoelectronic synaptic or neural components
to perform learning or inference. However, there has been limited analysis on
the scaling effect of stochastic spin devices and its impact on the operation
of such stochastic networks at the system level. This work attempts to explore
the design space and analyze the performance of nanomagnet-based stochastic
neuromorphic computing architectures for magnets with different barrier
heights. We illustrate how the underlying network architecture must be modified
to account for the random telegraphic switching behavior displayed by magnets
with low barrier heights as they are scaled into the superparamagnetic regime.
We perform a device-to-system-level analysis on a deep neural-network
architecture for a digit-recognition problem on the MNIST data set.
</summary>
    <author>
      <name>Chamika M. Liyanagedera</name>
    </author>
    <author>
      <name>Abhronil Sengupta</name>
    </author>
    <author>
      <name>Akhilesh Jaiswal</name>
    </author>
    <author>
      <name>Kaushik Roy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevApplied.8.064017</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevApplied.8.064017" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 14 Figures, 1 Table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. Applied 8, 064017 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1709.09247v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.09247v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.07099v1</id>
    <updated>2017-10-19T11:43:34Z</updated>
    <published>2017-10-19T11:43:34Z</published>
    <title>Sea Level Anomaly Prediction using Recurrent Neural Networks</title>
    <summary>  Sea level change, one of the most dire impacts of anthropogenic global
warming, will affect a large amount of the world's population. However, sea
level change is not uniform in time and space, and the skill of conventional
prediction methods is limited due to the ocean's internal variabi-lity on
timescales from weeks to decades. Here we study the potential of neural network
methods which have been used successfully in other applications, but rarely
been applied for this task. We develop a combination of a convolutional neural
network (CNN) and a recurrent neural network (RNN) to ana-lyse both the spatial
and the temporal evolution of sea level and to suggest an independent, accurate
method to predict interannual sea level anomalies (SLA). We test our method for
the northern and equatorial Pacific Ocean, using gridded altimeter-derived SLA
data. We show that the used network designs outperform a simple regression and
that adding a CNN improves the skill significantly. The predictions are stable
over several years.
</summary>
    <author>
      <name>Anne Braakmann-Folgmann</name>
    </author>
    <author>
      <name>Ribana Roscher</name>
    </author>
    <author>
      <name>Susanne Wenzel</name>
    </author>
    <author>
      <name>Bernd Uebbing</name>
    </author>
    <author>
      <name>J√ºrgen Kusche</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2017 conference on Big Data from Space</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1710.07099v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.07099v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.07363v1</id>
    <updated>2017-10-19T22:43:47Z</updated>
    <published>2017-10-19T22:43:47Z</published>
    <title>Historical Document Image Segmentation with LDA-Initialized Deep Neural
  Networks</title>
    <summary>  In this paper, we present a novel approach to perform deep neural networks
layer-wise weight initialization using Linear Discriminant Analysis (LDA).
Typically, the weights of a deep neural network are initialized with: random
values, greedy layer-wise pre-training (usually as Deep Belief Network or as
auto-encoder) or by re-using the layers from another network (transfer
learning). Hence, many training epochs are needed before meaningful weights are
learned, or a rather similar dataset is required for seeding a fine-tuning of
transfer learning. In this paper, we describe how to turn an LDA into either a
neural layer or a classification layer. We analyze the initialization technique
on historical documents. First, we show that an LDA-based initialization is
quick and leads to a very stable initialization. Furthermore, for the task of
layout analysis at pixel level, we investigate the effectiveness of LDA-based
initialization and show that it outperforms state-of-the-art random weight
initialization methods.
</summary>
    <author>
      <name>Michele Alberti</name>
    </author>
    <author>
      <name>Mathias Seuret</name>
    </author>
    <author>
      <name>Vinaychandran Pondenkandath</name>
    </author>
    <author>
      <name>Rolf Ingold</name>
    </author>
    <author>
      <name>Marcus Liwicki</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3151509.3151519</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3151509.3151519" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICDAR-HIP 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1710.07363v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.07363v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.09412v1</id>
    <updated>2017-10-25T18:30:49Z</updated>
    <published>2017-10-25T18:30:49Z</published>
    <title>mixup: Beyond Empirical Risk Minimization</title>
    <summary>  Large deep neural networks are powerful, but exhibit undesirable behaviors
such as memorization and sensitivity to adversarial examples. In this work, we
propose mixup, a simple learning principle to alleviate these issues. In
essence, mixup trains a neural network on convex combinations of pairs of
examples and their labels. By doing so, mixup regularizes the neural network to
favor simple linear behavior in-between training examples. Our experiments on
the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show
that mixup improves the generalization of state-of-the-art neural network
architectures. We also find that mixup reduces the memorization of corrupt
labels, increases the robustness to adversarial examples, and stabilizes the
training of generative adversarial networks.
</summary>
    <author>
      <name>Hongyi Zhang</name>
    </author>
    <author>
      <name>Moustapha Cisse</name>
    </author>
    <author>
      <name>Yann N. Dauphin</name>
    </author>
    <author>
      <name>David Lopez-Paz</name>
    </author>
    <link href="http://arxiv.org/abs/1710.09412v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.09412v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.09875v1</id>
    <updated>2017-10-26T19:09:54Z</updated>
    <published>2017-10-26T19:09:54Z</published>
    <title>Phase Transitions in Image Denoising via Sparsely Coding Convolutional
  Neural Networks</title>
    <summary>  Neural networks are analogous in many ways to spin glasses, systems which are
known for their rich set of dynamics and equally complex phase diagrams. We
apply well-known techniques in the study of spin glasses to a convolutional
sparsely encoding neural network and observe power law finite-size scaling
behavior in the sparsity and reconstruction error as the network denoises
32$\times$32 RGB CIFAR-10 images. This finite-size scaling indicates the
presence of a continuous phase transition at a critical value of this sparsity.
By using the power law scaling relations inherent to finite-size scaling, we
can determine the optimal value of sparsity for any network size by tuning the
system to the critical point and operate the system at the minimum denoising
error.
</summary>
    <author>
      <name>Jacob Carroll</name>
    </author>
    <author>
      <name>Nils Carlson</name>
    </author>
    <author>
      <name>Garrett T. Kenyon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures, submitted to NIPS 2017 workshop: Advances in
  Modeling and Learning Interactions from Complex Data</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.09875v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.09875v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.11176v2</id>
    <updated>2018-01-04T17:01:21Z</updated>
    <published>2017-10-30T18:35:01Z</published>
    <title>CrescendoNet: A Simple Deep Convolutional Neural Network with Ensemble
  Behavior</title>
    <summary>  We introduce a new deep convolutional neural network, CrescendoNet, by
stacking simple building blocks without residual connections. Each Crescendo
block contains independent convolution paths with increased depths. The numbers
of convolution layers and parameters are only increased linearly in Crescendo
blocks. In experiments, CrescendoNet with only 15 layers outperforms almost all
networks without residual connections on benchmark datasets, CIFAR10, CIFAR100,
and SVHN. Given sufficient amount of data as in SVHN dataset, CrescendoNet with
15 layers and 4.1M parameters can match the performance of DenseNet-BC with 250
layers and 15.3M parameters. CrescendoNet provides a new way to construct high
performance deep convolutional neural networks without residual connections.
Moreover, through investigating the behavior and performance of subnetworks in
CrescendoNet, we note that the high performance of CrescendoNet may come from
its implicit ensemble behavior, which differs from the FractalNet that is also
a deep convolutional neural network without residual connections. Furthermore,
the independence between paths in CrescendoNet allows us to introduce a new
path-wise training procedure, which can reduce the memory needed for training.
</summary>
    <author>
      <name>Xiang Zhang</name>
    </author>
    <author>
      <name>Nishant Vishwamitra</name>
    </author>
    <author>
      <name>Hongxin Hu</name>
    </author>
    <author>
      <name>Feng Luo</name>
    </author>
    <link href="http://arxiv.org/abs/1710.11176v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.11176v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.01343v1</id>
    <updated>2017-11-03T21:44:08Z</updated>
    <published>2017-11-03T21:44:08Z</published>
    <title>Accelerating Training of Deep Neural Networks via Sparse Edge Processing</title>
    <summary>  We propose a reconfigurable hardware architecture for deep neural networks
(DNNs) capable of online training and inference, which uses algorithmically
pre-determined, structured sparsity to significantly lower memory and
computational requirements. This novel architecture introduces the notion of
edge-processing to provide flexibility and combines junction pipelining and
operational parallelization to speed up training. The overall effect is to
reduce network complexity by factors up to 30x and training time by up to 35x
relative to GPUs, while maintaining high fidelity of inference results. This
has the potential to enable extensive parameter searches and development of the
largely unexplored theoretical foundation of DNNs. The architecture
automatically adapts itself to different network sizes given available hardware
resources. As proof of concept, we show results obtained for different bit
widths.
</summary>
    <author>
      <name>Sourya Dey</name>
    </author>
    <author>
      <name>Yinan Shao</name>
    </author>
    <author>
      <name>Keith M. Chugg</name>
    </author>
    <author>
      <name>Peter A. Beerel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-68600-4_32</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-68600-4_32" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 26th International Conference on Artificial Neural
  Networks (ICANN) 2017 in Alghero, Italy</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings Part 1 of ICANN 2017, pp 273-280. Lecture Notes in
  Computer Science, vol 10613. Springer, Cham</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1711.01343v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.01343v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.02448v2</id>
    <updated>2018-01-03T17:29:28Z</updated>
    <published>2017-11-07T13:03:35Z</published>
    <title>Cortical microcircuits as gated-recurrent neural networks</title>
    <summary>  Cortical circuits exhibit intricate recurrent architectures that are
remarkably similar across different brain areas. Such stereotyped structure
suggests the existence of common computational principles. However, such
principles have remained largely elusive. Inspired by gated-memory networks,
namely long short-term memory networks (LSTMs), we introduce a recurrent neural
network in which information is gated through inhibitory cells that are
subtractive (subLSTM). We propose a natural mapping of subLSTMs onto known
canonical excitatory-inhibitory cortical microcircuits. Our empirical
evaluation across sequential image classification and language modelling tasks
shows that subLSTM units can achieve similar performance to LSTM units. These
results suggest that cortical circuits can be optimised to solve complex
contextual problems and proposes a novel view on their computational function.
Overall our work provides a step towards unifying recurrent networks as used in
machine learning with their biological counterparts.
</summary>
    <author>
      <name>Rui Ponte Costa</name>
    </author>
    <author>
      <name>Yannis M. Assael</name>
    </author>
    <author>
      <name>Brendan Shillingford</name>
    </author>
    <author>
      <name>Nando de Freitas</name>
    </author>
    <author>
      <name>Tim P. Vogels</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Advances in Neural Information Processing Systems 30
  (NIPS 2017). 13 pages, 2 figures (and 1 supp. figure)</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.02448v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.02448v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.08591v1</id>
    <updated>2017-11-23T06:47:37Z</updated>
    <published>2017-11-23T06:47:37Z</published>
    <title>Regularization of Deep Neural Networks with Spectral Dropout</title>
    <summary>  The big breakthrough on the ImageNet challenge in 2012 was partially due to
the `dropout' technique used to avoid overfitting. Here, we introduce a new
approach called `Spectral Dropout' to improve the generalization ability of
deep neural networks. We cast the proposed approach in the form of regular
Convolutional Neural Network (CNN) weight layers using a decorrelation
transform with fixed basis functions. Our spectral dropout method prevents
overfitting by eliminating weak and `noisy' Fourier domain coefficients of the
neural network activations, leading to remarkably better results than the
current regularization methods. Furthermore, the proposed is very efficient due
to the fixed basis functions used for spectral transformation. In particular,
compared to Dropout and Drop-Connect, our method significantly speeds up the
network convergence rate during the training process (roughly x2), with
considerably higher neuron pruning rates (an increase of ~ 30%). We demonstrate
that the spectral dropout can also be used in conjunction with other
regularization approaches resulting in additional performance gains.
</summary>
    <author>
      <name>Salman Khan</name>
    </author>
    <author>
      <name>Munawar Hayat</name>
    </author>
    <author>
      <name>Fatih Porikli</name>
    </author>
    <link href="http://arxiv.org/abs/1711.08591v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.08591v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.00504v1</id>
    <updated>2017-11-30T16:31:36Z</updated>
    <published>2017-11-30T16:31:36Z</published>
    <title>A Neural Stochastic Volatility Model</title>
    <summary>  In this paper, we show that the recent integration of statistical models with
deep recurrent neural networks provides a new way of formulating volatility
(the degree of variation of time series) models that have been widely used in
time series analysis and prediction in finance. The model comprises a pair of
complementary stochastic recurrent neural networks: the generative network
models the joint distribution of the stochastic volatility process; the
inference network approximates the conditional distribution of the latent
variables given the observables. Our focus here is on the formulation of
temporal dynamics of volatility over time under a stochastic recurrent neural
network framework. Experiments on real-world stock price datasets demonstrate
that the proposed model generates a better volatility estimation and prediction
that outperforms stronge baseline methods, including the deterministic models,
such as GARCH and its variants, and the stochastic MCMC-based models, and the
Gaussian-process-based, on the average negative log-likelihood measure.
</summary>
    <author>
      <name>Rui Luo</name>
    </author>
    <author>
      <name>Weinan Zhang</name>
    </author>
    <author>
      <name>Xiaojun Xu</name>
    </author>
    <author>
      <name>Jun Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1712.00504v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.00504v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.03222v1</id>
    <updated>2017-10-18T17:57:22Z</updated>
    <published>2017-10-18T17:57:22Z</published>
    <title>Nanophotonic Particle Simulation and Inverse Design Using Artificial
  Neural Networks</title>
    <summary>  We propose a method to use artificial neural networks to approximate light
scattering by multilayer nanoparticles. We find the network needs to be trained
on only a small sampling of the data in order to approximate the simulation to
high precision. Once the neural network is trained, it can simulate such
optical processes orders of magnitude faster than conventional simulations.
Furthermore, the trained neural network can be used solve nanophotonic inverse
design problems by using back- propogation - where the gradient is analytical,
not numerical.
</summary>
    <author>
      <name>John Peurifoy</name>
    </author>
    <author>
      <name>Yichen Shen</name>
    </author>
    <author>
      <name>Li Jing</name>
    </author>
    <author>
      <name>Yi Yang</name>
    </author>
    <author>
      <name>Fidel Cano-Renteria</name>
    </author>
    <author>
      <name>Brendan Delacy</name>
    </author>
    <author>
      <name>Max Tegmark</name>
    </author>
    <author>
      <name>John D. Joannopoulos</name>
    </author>
    <author>
      <name>Marin Soljacic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures in text, 3 in supplementals, work presented at FiO</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.03222v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.03222v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.app-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.09809v1</id>
    <updated>2017-12-28T10:28:38Z</updated>
    <published>2017-12-28T10:28:38Z</published>
    <title>A Multi-Scale and Multi-Depth Convolutional Neural Network for Remote
  Sensing Imagery Pan-Sharpening</title>
    <summary>  Pan-sharpening is a fundamental and significant task in the field of remote
sensing imagery processing, in which high-resolution spatial details from
panchromatic images are employed to enhance the spatial resolution of
multi-spectral (MS) images. As the transformation from low spatial resolution
MS image to high-resolution MS image is complex and highly non-linear, inspired
by the powerful representation for non-linear relationships of deep neural
networks, we introduce multi-scale feature extraction and residual learning
into the basic convolutional neural network (CNN) architecture and propose the
multi-scale and multi-depth convolutional neural network (MSDCNN) for the
pan-sharpening of remote sensing imagery. Both the quantitative assessment
results and the visual assessment confirm that the proposed network yields
high-resolution MS images that are superior to the images produced by the
compared state-of-the-art methods.
</summary>
    <author>
      <name>Qiangqiang Yuan</name>
    </author>
    <author>
      <name>Yancong Wei</name>
    </author>
    <author>
      <name>Xiangchao Meng</name>
    </author>
    <author>
      <name>Huanfeng Shen</name>
    </author>
    <author>
      <name>Liangpei Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1712.09809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.09809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.02148v2</id>
    <updated>2018-01-10T23:07:16Z</updated>
    <published>2018-01-07T06:37:50Z</published>
    <title>Australia's long-term electricity demand forecasting using deep neural
  networks</title>
    <summary>  Accurate prediction of long-term electricity demand has a significant role in
demand side management and electricity network planning and operation. Demand
over-estimation results in over-investment in network assets, driving up the
electricity prices, while demand under-estimation may lead to under-investment
resulting in unreliable and insecure electricity. In this manuscript, we apply
deep neural networks to predict Australia's long-term electricity demand. A
stacked autoencoder is used in combination with multilayer perceptrons or
cascade-forward multilayer perceptrons to predict the nation-wide electricity
consumption rates for 1-24 months ahead of time. The experimental results show
that the deep structures have better performance than classical neural
networks, especially for 12-month to 24-month prediction horizon.
</summary>
    <author>
      <name>Homayoun Hamedmoghadam</name>
    </author>
    <author>
      <name>Nima Joorabloo</name>
    </author>
    <author>
      <name>Mahdi Jalili</name>
    </author>
    <link href="http://arxiv.org/abs/1801.02148v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.02148v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.05267v1</id>
    <updated>2018-02-14T18:59:34Z</updated>
    <published>2018-02-14T18:59:34Z</published>
    <title>Reinforcement Learning with Neural Networks for Quantum Feedback</title>
    <summary>  Artificial neural networks are revolutionizing science. While the most
prevalent technique involves supervised training on queries with a known
correct answer, more advanced challenges often require discovering answers
autonomously. In reinforcement learning, control strategies are improved
according to a reward function. The power of this approach has been highlighted
by spectactular recent successes, such as playing Go. So far, it has remained
an open question whether neural-network-based reinforcement learning can be
successfully applied in physics. Here, we show how to use this method for
finding quantum feedback schemes, where a network-based "agent" interacts with
and occasionally decides to measure a quantum system. We illustrate the utility
by finding gate sequences that preserve the quantum information stored in a
small collection of qubits against noise. This specific application will help
to find hardware-adapted feedback schemes for small quantum modules while
demonstrating more generally the promise of neural-network based reinforcement
learning in physics.
</summary>
    <author>
      <name>Thomas F√∂sel</name>
    </author>
    <author>
      <name>Petru Tighineanu</name>
    </author>
    <author>
      <name>Talitha Weiss</name>
    </author>
    <author>
      <name>Florian Marquardt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages maintext + methods, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.05267v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.05267v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.07905v1</id>
    <updated>2018-02-22T05:08:48Z</updated>
    <published>2018-02-22T05:08:48Z</published>
    <title>Closed-loop control of a modular neuromorphic biohybrid</title>
    <summary>  Neural networks modularity is a major challenge for the development of
control circuits of neural activity. Under physiological limitations, the
accessible regions for external stimulation are possibly different from the
functionally relevant ones, requiring complex indirect control designs.
Moreover, control over one region might affect activity of other downstream
networks, once sparse connections exist. We address these questions by
developing a hybrid device of a cortical culture functionally integrated with a
biomimetic hardware neural network. This design enables the study of modular
networks controllability, while connectivity is well-defined and key features
of cortical networks are accessible. Using a closed-loop control to monitor the
activity of the coupled hybrid, we show that both modules are congruently
modified, in the macroscopic as well as the microscopic activity levels.
Control impacts efficiently the activity on both sides whether the control
circuit is an indirect series one, or implemented independently only on one of
the modules. Hence, these results present global functional impacts of a local
control intervention. Overall, this strategy provides an experimental access to
the controllability of neural activity irregularities, when embedded in a
modular organization.
</summary>
    <author>
      <name>Hanna Keren</name>
    </author>
    <author>
      <name>Johannes Partzsch</name>
    </author>
    <author>
      <name>Shimon Marom</name>
    </author>
    <author>
      <name>Christian Mayr</name>
    </author>
    <link href="http://arxiv.org/abs/1802.07905v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.07905v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.10328v1</id>
    <updated>2018-02-28T09:47:20Z</updated>
    <published>2018-02-28T09:47:20Z</published>
    <title>Neural Photometric Stereo Reconstruction for General Reflectance
  Surfaces</title>
    <summary>  We present a novel convolutional neural network architecture for photometric
stereo (Woodham, 1980), a problem of recovering 3D object surface normals from
multiple images observed under varying illuminations. Despite its long history
in computer vision, the problem still shows fundamental challenges for surfaces
with unknown general reflectance properties (BRDFs). Leveraging deep neural
networks to learn complicated reflectance models is promising, but studies in
this direction are very limited due to difficulties in acquiring accurate
ground truth for training and also in designing networks invariant to
permutation of input images. In order to address these challenges, we propose a
reconstruction based unsupervised learning framework where surface normals and
BRDFs are predicted by the network and fed into the rendering equation to
synthesize observed images. The network is trained during testing by minimizing
reconstruction loss between observed and synthesized images. Thus, our learning
process does not require ground truth normals or even pre-training on external
images. Our method is shown to achieve the state-of-the-art performance on a
challenging real-world scene benchmark.
</summary>
    <author>
      <name>Tatsunori Taniai</name>
    </author>
    <author>
      <name>Takanori Maehara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages + 20 pages (appendices)</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.10328v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.10328v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.10542v1</id>
    <updated>2018-02-28T17:21:44Z</updated>
    <published>2018-02-28T17:21:44Z</published>
    <title>Memory-based Parameter Adaptation</title>
    <summary>  Deep neural networks have excelled on a wide range of problems, from vision
to language and game playing. Neural networks very gradually incorporate
information into weights as they process data, requiring very low learning
rates. If the training distribution shifts, the network is slow to adapt, and
when it does adapt, it typically performs badly on the training distribution
before the shift. Our method, Memory-based Parameter Adaptation, stores
examples in memory and then uses a context-based lookup to directly modify the
weights of a neural network. Much higher learning rates can be used for this
local adaptation, reneging the need for many iterations over similar data
before good predictions can be made. As our method is memory-based, it
alleviates several shortcomings of neural networks, such as catastrophic
forgetting, fast, stable acquisition of new knowledge, learning with an
imbalanced class labels, and fast learning during evaluation. We demonstrate
this on a range of supervised tasks: large-scale image classification and
language modelling.
</summary>
    <author>
      <name>Pablo Sprechmann</name>
    </author>
    <author>
      <name>Siddhant M. Jayakumar</name>
    </author>
    <author>
      <name>Jack W. Rae</name>
    </author>
    <author>
      <name>Alexander Pritzel</name>
    </author>
    <author>
      <name>Adri√† Puigdom√®nech Badia</name>
    </author>
    <author>
      <name>Benigno Uria</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Demis Hassabis</name>
    </author>
    <author>
      <name>Razvan Pascanu</name>
    </author>
    <author>
      <name>Charles Blundell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at ICLR 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.10542v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.10542v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.00094v1</id>
    <updated>2018-02-28T21:28:28Z</updated>
    <published>2018-02-28T21:28:28Z</published>
    <title>Neural Networks Should Be Wide Enough to Learn Disconnected Decision
  Regions</title>
    <summary>  In the recent literature the important role of depth in deep learning has
been emphasized. In this paper we argue that sufficient width of a feedforward
network is equally important by answering the simple question under which
conditions the decision regions of a neural network are connected. It turns out
that for a class of activation functions including leaky ReLU, neural networks
having a pyramidal structure, that is no layer has more hidden units than the
input dimension, produce necessarily connected decision regions. This implies
that a sufficiently wide layer is necessary to produce disconnected decision
regions. We discuss the implications of this result for the construction of
neural networks, in particular the relation to the problem of adversarial
manipulation of classifiers.
</summary>
    <author>
      <name>Quynh Nguyen</name>
    </author>
    <author>
      <name>Mahesh Mukkamala</name>
    </author>
    <author>
      <name>Matthias Hein</name>
    </author>
    <link href="http://arxiv.org/abs/1803.00094v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.00094v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.5931v1</id>
    <updated>2014-12-18T16:36:01Z</updated>
    <published>2014-12-18T16:36:01Z</published>
    <title>Dynamic changes in network synchrony reveal resting-state functional
  networks</title>
    <summary>  Experimental fMRI studies have shown that spontaneous brain activity i.e. in
the absence of any external input, exhibit complex spatial and temporal
patterns of co-activity between segregated brain regions. These so-called
large-scale resting-state functional connectivity networks represent
dynamically organized neural assemblies interacting with each other in a
complex way. It has been suggested that looking at the dynamical properties of
complex patterns of brain functional co-activity may reveal neural mechanisms
underlying the dynamic changes in functional interactions. Here, we examine how
global network dynamics is shaped by different network configurations, derived
from realistic brain functional interactions. We focus on two main dynamics
measures: synchrony and variations in synchrony. Neural activity and the
inferred hemodynamic response of the network nodes are simulated using system
of 90 FitzHugh-Nagumo neural models subject to system noise and time-delayed
interactions. These models are embedded into the topology of the complex brain
functional interactions, whose architecture is additionally reduced to its main
structural pathways. In the simulated functional networks, patterns of
correlated regional activity clearly arise from dynamical properties that
maximize synchrony and variations in synchrony. Our results on the fast changes
of the level of the network synchrony also show how flexible changes in the
large-scale network dynamics could be.
</summary>
    <author>
      <name>Vesna Vuksanoviƒá</name>
    </author>
    <author>
      <name>Philipp H√∂vel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/1.4913526</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/1.4913526" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 9 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.5931v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.5931v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.00486v1</id>
    <updated>2017-10-02T05:09:52Z</updated>
    <published>2017-10-02T05:09:52Z</published>
    <title>DeepSafe: A Data-driven Approach for Checking Adversarial Robustness in
  Neural Networks</title>
    <summary>  Deep neural networks have become widely used, obtaining remarkable results in
domains such as computer vision, speech recognition, natural language
processing, audio recognition, social network filtering, machine translation,
and bio-informatics, where they have produced results comparable to human
experts. However, these networks can be easily fooled by adversarial
perturbations: minimal changes to correctly-classified inputs, that cause the
network to mis-classify them. This phenomenon represents a concern for both
safety and security, but it is currently unclear how to measure a network's
robustness against such perturbations. Existing techniques are limited to
checking robustness around a few individual input points, providing only very
limited guarantees. We propose a novel approach for automatically identifying
safe regions of the input space, within which the network is robust against
adversarial perturbations. The approach is data-guided, relying on clustering
to identify well-defined geometric regions as candidate safe regions. We then
utilize verification techniques to confirm that these regions are safe or to
provide counter-examples showing that they are not safe. We also introduce the
notion of targeted robustness which, for a given target label and region,
ensures that a NN does not map any input in the region to the target label. We
evaluated our technique on the MNIST dataset and on a neural network
implementation of a controller for the next-generation Airborne Collision
Avoidance System for unmanned aircraft (ACAS Xu). For these networks, our
approach identified multiple regions which were completely safe as well as some
which were only safe for specific labels. It also discovered several
adversarial perturbations of interest.
</summary>
    <author>
      <name>Divya Gopinath</name>
    </author>
    <author>
      <name>Guy Katz</name>
    </author>
    <author>
      <name>Corina S. Pasareanu</name>
    </author>
    <author>
      <name>Clark Barrett</name>
    </author>
    <link href="http://arxiv.org/abs/1710.00486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.00486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0401336v1</id>
    <updated>2004-01-19T15:45:18Z</updated>
    <published>2004-01-19T15:45:18Z</published>
    <title>The three-state layered neural network with finite dilution</title>
    <summary>  The dynamics and the stationary states of an exactly solvable three-state
layered feed-forward neural network model with asymmetric synaptic connections,
finite dilution and low pattern activity are studied in extension of a recent
work on a recurrent network. Detailed phase diagrams are obtained for the
stationary states and for the time evolution of the retrieval overlap with a
single pattern. It is shown that the network develops instabilities for low
thresholds and that there is a gradual improvement in network performance with
increasing threshold up to an optimal stage. The robustness to synaptic noise
is checked and the effects of dilution and of variable threshold on the
information content of the network are also established.
</summary>
    <author>
      <name>W. K. Theumann</name>
    </author>
    <author>
      <name>R. Erichsen Jr</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physa.2004.04.130</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physa.2004.04.130" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Latex, 11 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0401336v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0401336v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0506535v1</id>
    <updated>2005-06-21T12:44:26Z</updated>
    <published>2005-06-21T12:44:26Z</published>
    <title>Information and Topology in Attractor Neural Network</title>
    <summary>  A wide range of networks, including small-world topology, can be modelled by
the connectivity $\gamma$, and randomness $\omega$ of the links. Both learning
and attractor abilities of a neural network can be measured by the mutual
information (MI), as a function of the load rate and overlap between patterns
and retrieval states. We use MI to search for the optimal topology, for storage
and attractor properties of the network. We find that, while the largest
storage implies an optimal $MI(\gamma,\omega)$ at $\gamma_{opt}(\omega)\to 0$,
the largest basin of attraction leads to an optimal topology at moderate levels
of $\gamma_{opt}$, whenever $0\leq\omega&lt;0.3$. This $\gamma_{opt}$ is related
to the clustering and path-length of the network. We also build a diagram for
the dynamical phases with random and local initial overlap, and show that very
diluted networks lose their attractor ability.
</summary>
    <author>
      <name>D. Dominguez</name>
    </author>
    <author>
      <name>K. Koroutchev</name>
    </author>
    <author>
      <name>E. Serrano</name>
    </author>
    <author>
      <name>F. B. Rodriguez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pgs., 5 figs</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0506535v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0506535v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9705102v1</id>
    <updated>1997-05-01T00:00:00Z</updated>
    <published>1997-05-01T00:00:00Z</published>
    <title>Connectionist Theory Refinement: Genetically Searching the Space of
  Network Topologies</title>
    <summary>  An algorithm that learns from a set of examples should ideally be able to
exploit the available resources of (a) abundant computing power and (b)
domain-specific knowledge to improve its ability to generalize. Connectionist
theory-refinement systems, which use background knowledge to select a neural
network's topology and initial weights, have proven to be effective at
exploiting domain-specific knowledge; however, most do not exploit available
computing power. This weakness occurs because they lack the ability to refine
the topology of the neural networks they produce, thereby limiting
generalization, especially when given impoverished domain theories. We present
the REGENT algorithm which uses (a) domain-specific knowledge to help create an
initial population of knowledge-based neural networks and (b) genetic operators
of crossover and mutation (specifically designed for knowledge-based networks)
to continually search for better network topologies. Experiments on three
real-world domains indicate that our new algorithm is able to significantly
increase generalization compared to a standard connectionist theory-refinement
system, as well as our previous algorithm for growing knowledge-based networks.
</summary>
    <author>
      <name>D. W. Opitz</name>
    </author>
    <author>
      <name>J. W. Shavlik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">See http://www.jair.org/ for any accompanying files</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Artificial Intelligence Research, Vol 6, (1997),
  177-209</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9705102v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9705102v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.3523v1</id>
    <updated>2008-05-22T19:12:59Z</updated>
    <published>2008-05-22T19:12:59Z</published>
    <title>Reliability of Layered Neural Oscillator Networks</title>
    <summary>  We study the reliability of large networks of coupled neural oscillators in
response to fluctuating stimuli. Reliability means that a stimulus elicits
essentially identical responses upon repeated presentations. We view the
problem on two scales: neuronal reliability, which concerns the repeatability
of spike times of individual neurons embedded within a network, and
pooled-response reliability, which addresses the repeatability of the total
synaptic output from the network. We find that individual embedded neurons can
be reliable or unreliable depending on network conditions, whereas pooled
responses of sufficiently large networks are mostly reliable. We study also the
effects of noise, and find that some types affect reliability more seriously
than others.
</summary>
    <author>
      <name>Kevin K. Lin</name>
    </author>
    <author>
      <name>Eric Shea-Brown</name>
    </author>
    <author>
      <name>Lai-Sang Young</name>
    </author>
    <link href="http://arxiv.org/abs/0805.3523v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.3523v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.6319v1</id>
    <updated>2012-07-26T16:28:11Z</updated>
    <published>2012-07-26T16:28:11Z</published>
    <title>The simplest maximum entropy model for collective behavior in a neural
  network</title>
    <summary>  Recent work emphasizes that the maximum entropy principle provides a bridge
between statistical mechanics models for collective behavior in neural networks
and experiments on networks of real neurons. Most of this work has focused on
capturing the measured correlations among pairs of neurons. Here we suggest an
alternative, constructing models that are consistent with the distribution of
global network activity, i.e. the probability that K out of N cells in the
network generate action potentials in the same small time bin. The inverse
problem that we need to solve in constructing the model is analytically
tractable, and provides a natural "thermodynamics" for the network in the limit
of large N. We analyze the responses of neurons in a small patch of the retina
to naturalistic stimuli, and find that the implied thermodynamics is very close
to an unusual critical point, in which the entropy (in proper units) is exactly
equal to the energy.
</summary>
    <author>
      <name>Gasper Tkacik</name>
    </author>
    <author>
      <name>Olivier Marre</name>
    </author>
    <author>
      <name>Thierry Mora</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Michael J. Berry II</name>
    </author>
    <author>
      <name>William Bialek</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1742-5468/2013/03/P03011</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1742-5468/2013/03/P03011" rel="related"/>
    <link href="http://arxiv.org/abs/1207.6319v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.6319v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7843v1</id>
    <updated>2013-04-30T03:18:12Z</updated>
    <published>2013-04-30T03:18:12Z</published>
    <title>A Hybrid Rule Based Fuzzy-Neural Expert System For Passive Network
  Monitoring</title>
    <summary>  An enhanced approach for network monitoring is to create a network monitoring
tool that has artificial intelligence characteristics. There are a number of
approaches available. One such approach is by the use of a combination of rule
based, fuzzy logic and neural networks to create a hybrid ANFIS system. Such
system will have a dual knowledge database approach. One containing membership
function values to compare to and do deductive reasoning and another database
with rules deductively formulated by an expert (a network administrator). The
knowledge database will be updated continuously with newly acquired patterns.
In short, the system will be composed of 2 parts, learning from data sets and
fine-tuning the knowledge-base using neural network and the use of fuzzy logic
in making decision based on the rules and membership functions inside the
knowledge base. This paper will discuss the idea, steps and issues involved in
creating such a system.
</summary>
    <author>
      <name>Azruddin Ahmad</name>
    </author>
    <author>
      <name>Gobithasan Rudrusamy</name>
    </author>
    <author>
      <name>Rahmat Budiarto</name>
    </author>
    <author>
      <name>Azman Samsudin</name>
    </author>
    <author>
      <name>Sureswaran Ramadass</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2002 Proceedings of the Arab Conference on Information Technology
  ACIT 2002, Dhaka, Pg.746-752</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.7843v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7843v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.2076v2</id>
    <updated>2017-03-17T14:14:26Z</updated>
    <published>2013-05-09T13:01:49Z</published>
    <title>Retrieving Infinite Numbers of Patterns in a Spin-Glass Model of Immune
  Networks</title>
    <summary>  The similarity between neural and immune networks has been known for decades,
but so far we did not understand the mechanism that allows the immune system,
unlike associative neural networks, to recall and execute a large number of
memorized defense strategies {\em in parallel}. The explanation turns out to
lie in the network topology. Neurons interact typically with a large number of
other neurons, whereas interactions among lymphocytes in immune networks are
very specific, and described by graphs with finite connectivity. In this paper
we use replica techniques to solve a statistical mechanical immune network
model with `coordinator branches' (T-cells) and `effector branches' (B-cells),
and show how the finite connectivity enables the system to manage an extensive
number of immune clones simultaneously, even above the percolation threshold.
The system exhibits only weak ergodicity breaking, so that both multiple
antigen defense and homeostasis can be accomplished.
</summary>
    <author>
      <name>Elena Agliari</name>
    </author>
    <author>
      <name>Alessia Annibale</name>
    </author>
    <author>
      <name>Adriano Barra</name>
    </author>
    <author>
      <name>A. C. C. Coolen</name>
    </author>
    <author>
      <name>Daniele Tantari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Editor's Choice 2017</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPL 117, 28003 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1305.2076v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.2076v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.CB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.3829v1</id>
    <updated>2012-09-18T02:25:50Z</updated>
    <published>2012-09-18T02:25:50Z</published>
    <title>Self-organized criticality in a network of interacting neurons</title>
    <summary>  This paper contains an analysis of a simple neural network that exhibits
self-organized criticality. Such criticality follows from the combination of a
simple neural network with an excitatory feedback loop that generates
bistability, in combination with an anti-Hebbian synapse in its input pathway.
Using the methods of statistical field theory, we show how one can formulate
the stochastic dynamics of such a network as the action of a path integral,
which we then investigate using renormalization group methods. The results
indicate that the network exhibits hysteresis in switching back and forward
between its two stable states, each of which loses its stability at a
saddle-node bifurcation. The renormalization group analysis shows that the
fluctuations in the neighborhood of such bifurcations have the signature of
directed percolation. Thus the network states undergo the neural analog of a
phase transition in the universality class of directed percolation. The network
replicates precisely the behavior of the original sand-pile model of Bak, Tang
&amp; Wiesenfeld.
</summary>
    <author>
      <name>J D Cowan</name>
    </author>
    <author>
      <name>J Neuman</name>
    </author>
    <author>
      <name>W van Drongelen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1742-5468/2013/04/P04030</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1742-5468/2013/04/P04030" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 4 figures, submitted to Journal of Statistical Mechanics</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.3829v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.3829v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.00296v1</id>
    <updated>2015-05-02T01:26:42Z</updated>
    <published>2015-05-02T01:26:42Z</published>
    <title>Object-Scene Convolutional Neural Networks for Event Recognition in
  Images</title>
    <summary>  Event recognition from still images is of great importance for image
understanding. However, compared with event recognition in videos, there are
much fewer research works on event recognition in images. This paper addresses
the issue of event recognition from images and proposes an effective method
with deep neural networks. Specifically, we design a new architecture, called
Object-Scene Convolutional Neural Network (OS-CNN). This architecture is
decomposed into object net and scene net, which extract useful information for
event understanding from the perspective of objects and scene context,
respectively. Meanwhile, we investigate different network architectures for
OS-CNN design, and adapt the deep (AlexNet) and very-deep (GoogLeNet) networks
to the task of event recognition. Furthermore, we find that the deep and
very-deep networks are complementary to each other. Finally, based on the
proposed OS-CNN and comparative study of different network architectures, we
come up with a solution of five-stream CNN for the track of cultural event
recognition at the ChaLearn Looking at People (LAP) challenge 2015. Our method
obtains the performance of 85.5% and ranks the $1^{st}$ place in this
challenge.
</summary>
    <author>
      <name>Limin Wang</name>
    </author>
    <author>
      <name>Zhe Wang</name>
    </author>
    <author>
      <name>Wenbin Du</name>
    </author>
    <author>
      <name>Yu Qiao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR, ChaLearn Looking at People (LAP) challenge, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.00296v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.00296v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.05236v4</id>
    <updated>2016-01-08T07:22:41Z</updated>
    <published>2015-11-17T01:03:03Z</published>
    <title>Reduced-Precision Strategies for Bounded Memory in Deep Neural Nets</title>
    <summary>  This work investigates how using reduced precision data in Convolutional
Neural Networks (CNNs) affects network accuracy during classification. More
specifically, this study considers networks where each layer may use different
precision data. Our key result is the observation that the tolerance of CNNs to
reduced precision data not only varies across networks, a well established
observation, but also within networks. Tuning precision per layer is appealing
as it could enable energy and performance improvements. In this paper we study
how error tolerance across layers varies and propose a method for finding a low
precision configuration for a network while maintaining high accuracy. A
diverse set of CNNs is analyzed showing that compared to a conventional
implementation using a 32-bit floating-point representation for all layers, and
with less than 1% loss in relative accuracy, the data footprint required by
these networks can be reduced by an average of 74% and up to 92%.
</summary>
    <author>
      <name>Patrick Judd</name>
    </author>
    <author>
      <name>Jorge Albericio</name>
    </author>
    <author>
      <name>Tayler Hetherington</name>
    </author>
    <author>
      <name>Tor Aamodt</name>
    </author>
    <author>
      <name>Natalie Enright Jerger</name>
    </author>
    <author>
      <name>Raquel Urtasun</name>
    </author>
    <author>
      <name>Andreas Moshovos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ICLR 2016, 12 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.05236v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.05236v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.01946v1</id>
    <updated>2016-04-07T10:31:01Z</updated>
    <published>2016-04-07T10:31:01Z</published>
    <title>Optimizing Performance of Recurrent Neural Networks on GPUs</title>
    <summary>  As recurrent neural networks become larger and deeper, training times for
single networks are rising into weeks or even months. As such there is a
significant incentive to improve the performance and scalability of these
networks. While GPUs have become the hardware of choice for training and
deploying recurrent models, the implementations employed often make use of only
basic optimizations for these architectures. In this article we demonstrate
that by exposing parallelism between operations within the network, an order of
magnitude speedup across a range of network sizes can be achieved over a naive
implementation. We describe three stages of optimization that have been
incorporated into the fifth release of NVIDIA's cuDNN: firstly optimizing a
single cell, secondly a single layer, and thirdly the entire network.
</summary>
    <author>
      <name>Jeremy Appleyard</name>
    </author>
    <author>
      <name>Tomas Kocisky</name>
    </author>
    <author>
      <name>Phil Blunsom</name>
    </author>
    <link href="http://arxiv.org/abs/1604.01946v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.01946v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.03058v5</id>
    <updated>2016-11-19T01:37:40Z</updated>
    <published>2016-04-11T18:39:33Z</published>
    <title>Binarized Neural Networks on the ImageNet Classification Task</title>
    <summary>  We trained Binarized Neural Networks (BNNs) on the high resolution ImageNet
ILSVRC-2102 dataset classification task and achieved a good performance. With a
moderate size network of 13 layers, we obtained top-5 classification accuracy
rate of 84.1 % on validation set through network distillation, much better than
previous published results of 73.2% on XNOR network and 69.1% on binarized
GoogleNET. We expect networks of better performance can be obtained by
following our current strategies. We provide a detailed discussion and
preliminary analysis on strategies used in the network training.
</summary>
    <author>
      <name>Xundong Wu</name>
    </author>
    <author>
      <name>Yong Wu</name>
    </author>
    <author>
      <name>Yong Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/1604.03058v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03058v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05340v2</id>
    <updated>2016-06-17T18:13:20Z</updated>
    <published>2016-06-16T19:59:57Z</published>
    <title>Exponential expressivity in deep neural networks through transient chaos</title>
    <summary>  We combine Riemannian geometry with the mean field theory of high dimensional
chaos to study the nature of signal propagation in generic, deep neural
networks with random weights. Our results reveal an order-to-chaos expressivity
phase transition, with networks in the chaotic phase computing nonlinear
functions whose global curvature grows exponentially with depth but not width.
We prove this generic class of deep random functions cannot be efficiently
computed by any shallow network, going beyond prior work restricted to the
analysis of single functions. Moreover, we formalize and quantitatively
demonstrate the long conjectured idea that deep networks can disentangle highly
curved manifolds in input space into flat manifolds in hidden space. Our
theoretical analysis of the expressive power of deep networks broadly applies
to arbitrary nonlinearities, and provides a quantitative underpinning for
previously abstract notions about the geometry of deep functions.
</summary>
    <author>
      <name>Ben Poole</name>
    </author>
    <author>
      <name>Subhaneil Lahiri</name>
    </author>
    <author>
      <name>Maithra Raghu</name>
    </author>
    <author>
      <name>Jascha Sohl-Dickstein</name>
    </author>
    <author>
      <name>Surya Ganguli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fixed equation references</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.05340v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05340v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02545v3</id>
    <updated>2018-01-04T17:02:11Z</updated>
    <published>2016-09-08T19:43:30Z</published>
    <title>Supervised Learning in Spiking Neural Networks with FORCE Training</title>
    <summary>  Populations of neurons display an extraordinary diversity in the behaviors
they affect and display. Machine learning techniques have recently emerged that
allow us to create networks of model neurons that display behaviours of similar
complexity. Here, we demonstrate the direct applicability of one such
technique, the FORCE method, to spiking neural networks. We train these
networks to mimic dynamical systems, classify inputs, and store discrete
sequences that correspond to the notes of a song. Finally, we use FORCE
training to create two biologically motivated model circuits. One is inspired
by the zebra-finch and successfully reproduces songbird singing. The second
network is motivated by the hippocampus and is trained to store and replay a
movie scene. FORCE trained networks reproduce behaviors comparable in
complexity to their inspired circuits and yield information not easily
obtainable with other techniques such as behavioral responses to
pharmacological manipulations and spike timing statistics.
</summary>
    <author>
      <name>Wilten Nicola</name>
    </author>
    <author>
      <name>Claudia Clopath</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/s41467-017-01827-3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/s41467-017-01827-3" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Nicola, W., &amp; Clopath, C. (2017). Supervised learning in spiking
  neural networks with FORCE training. Nature communications, 8(1), 2208</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1609.02545v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02545v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.05281v1</id>
    <updated>2016-09-17T04:18:02Z</updated>
    <published>2016-09-17T04:18:02Z</published>
    <title>GeThR-Net: A Generalized Temporally Hybrid Recurrent Neural Network for
  Multimodal Information Fusion</title>
    <summary>  Data generated from real world events are usually temporal and contain
multimodal information such as audio, visual, depth, sensor etc. which are
required to be intelligently combined for classification tasks. In this paper,
we propose a novel generalized deep neural network architecture where temporal
streams from multiple modalities are combined. There are total M+1 (M is the
number of modalities) components in the proposed network. The first component
is a novel temporally hybrid Recurrent Neural Network (RNN) that exploits the
complimentary nature of the multimodal temporal information by allowing the
network to learn both modality specific temporal dynamics as well as the
dynamics in a multimodal feature space. M additional components are added to
the network which extract discriminative but non-temporal cues from each
modality. Finally, the predictions from all of these components are linearly
combined using a set of automatically learned weights. We perform exhaustive
experiments on three different datasets spanning four modalities. The proposed
network is relatively 3.5%, 5.7% and 2% better than the best performing
temporal multimodal baseline for UCF-101, CCV and Multimodal Gesture datasets
respectively.
</summary>
    <author>
      <name>Ankit Gandhi</name>
    </author>
    <author>
      <name>Arjun Sharma</name>
    </author>
    <author>
      <name>Arijit Biswas</name>
    </author>
    <author>
      <name>Om Deshmukh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ECCV workshop on Computer Vision for Audio-Visual Media,
  2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.05281v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.05281v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.09990v1</id>
    <updated>2016-10-31T16:02:23Z</updated>
    <published>2016-10-31T16:02:23Z</published>
    <title>Integration of continuous-time dynamics in a spiking neural network
  simulator</title>
    <summary>  Contemporary modeling approaches to the dynamics of neural networks consider
two main classes of models: biologically grounded spiking neurons and
functionally inspired rate-based units. The unified simulation framework
presented here supports the combination of the two for multi-scale modeling
approaches, the quantitative validation of mean-field approaches by spiking
network simulations, and an increase in reliability by usage of the same
simulation code and the same network model specifications for both model
classes. While most efficient spiking simulations rely on the communication of
discrete events, rate models require time-continuous interactions between
neurons. Exploiting the conceptual similarity to the inclusion of gap junctions
in spiking network simulations, we arrive at a reference implementation of
instantaneous and delayed interactions between rate-based models in a spiking
network simulator. The separation of rate dynamics from the general connection
and communication infrastructure ensures flexibility of the framework. We
further demonstrate the broad applicability of the framework by considering
various examples from the literature ranging from random networks to neural
field models. The study provides the prerequisite for interactions between
rate-based and spiking models in a joint simulation.
</summary>
    <author>
      <name>Jan Hahne</name>
    </author>
    <author>
      <name>David Dahmen</name>
    </author>
    <author>
      <name>Jannis Schuecker</name>
    </author>
    <author>
      <name>Andreas Frommer</name>
    </author>
    <author>
      <name>Matthias Bolten</name>
    </author>
    <author>
      <name>Moritz Helias</name>
    </author>
    <author>
      <name>Markus Diesmann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3389/fninf.2017.00034</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3389/fninf.2017.00034" rel="related"/>
    <link href="http://arxiv.org/abs/1610.09990v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.09990v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.04496v1</id>
    <updated>2017-03-13T17:25:52Z</updated>
    <published>2017-03-13T17:25:52Z</published>
    <title>Comparison of echo state network output layer classification methods on
  noisy data</title>
    <summary>  Echo state networks are a recently developed type of recurrent neural network
where the internal layer is fixed with random weights, and only the output
layer is trained on specific data. Echo state networks are increasingly being
used to process spatiotemporal data in real-world settings, including speech
recognition, event detection, and robot control. A strength of echo state
networks is the simple method used to train the output layer - typically a
collection of linear readout weights found using a least squares approach.
Although straightforward to train and having a low computational cost to use,
this method may not yield acceptable accuracy performance on noisy data.
  This study compares the performance of three echo state network output layer
methods to perform classification on noisy data: using trained linear weights,
using sparse trained linear weights, and using trained low-rank approximations
of reservoir states. The methods are investigated experimentally on both
synthetic and natural datasets. The experiments suggest that using regularized
least squares to train linear output weights is superior on data with low
noise, but using the low-rank approximations may significantly improve accuracy
on datasets contaminated with higher noise levels.
</summary>
    <author>
      <name>Ashley Prater</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IJCNN.2017.7966179</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IJCNN.2017.7966179" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages. International Joint Conference on Neural Networks (IJCNN
  2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.04496v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.04496v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.09387v1</id>
    <updated>2017-03-28T03:24:33Z</updated>
    <published>2017-03-28T03:24:33Z</published>
    <title>Adversarial Transformation Networks: Learning to Generate Adversarial
  Examples</title>
    <summary>  Multiple different approaches of generating adversarial examples have been
proposed to attack deep neural networks. These approaches involve either
directly computing gradients with respect to the image pixels, or directly
solving an optimization on the image pixels. In this work, we present a
fundamentally new method for generating adversarial examples that is fast to
execute and provides exceptional diversity of output. We efficiently train
feed-forward neural networks in a self-supervised manner to generate
adversarial examples against a target network or set of networks. We call such
a network an Adversarial Transformation Network (ATN). ATNs are trained to
generate adversarial examples that minimally modify the classifier's outputs
given the original input, while constraining the new classification to match an
adversarial target class. We present methods to train ATNs and analyze their
effectiveness targeting a variety of MNIST classifiers as well as the latest
state-of-the-art ImageNet classifier Inception ResNet v2.
</summary>
    <author>
      <name>Shumeet Baluja</name>
    </author>
    <author>
      <name>Ian Fischer</name>
    </author>
    <link href="http://arxiv.org/abs/1703.09387v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.09387v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.00109v1</id>
    <updated>2017-04-01T02:42:55Z</updated>
    <published>2017-04-01T02:42:55Z</published>
    <title>Snapshot Ensembles: Train 1, get M for free</title>
    <summary>  Ensembles of neural networks are known to be much more robust and accurate
than individual networks. However, training multiple deep networks for model
averaging is computationally expensive. In this paper, we propose a method to
obtain the seemingly contradictory goal of ensembling multiple neural networks
at no additional training cost. We achieve this goal by training a single
neural network, converging to several local minima along its optimization path
and saving the model parameters. To obtain repeated rapid convergence, we
leverage recent work on cyclic learning rate schedules. The resulting
technique, which we refer to as Snapshot Ensembling, is simple, yet
surprisingly effective. We show in a series of experiments that our approach is
compatible with diverse network architectures and learning tasks. It
consistently yields lower error rates than state-of-the-art single models at no
additional training cost, and compares favorably with traditional network
ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain
error rates of 3.4% and 17.4% respectively.
</summary>
    <author>
      <name>Gao Huang</name>
    </author>
    <author>
      <name>Yixuan Li</name>
    </author>
    <author>
      <name>Geoff Pleiss</name>
    </author>
    <author>
      <name>Zhuang Liu</name>
    </author>
    <author>
      <name>John E. Hopcroft</name>
    </author>
    <author>
      <name>Kilian Q. Weinberger</name>
    </author>
    <link href="http://arxiv.org/abs/1704.00109v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.00109v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.05358v1</id>
    <updated>2017-06-16T17:27:41Z</updated>
    <published>2017-06-16T17:27:41Z</published>
    <title>Local Feature Descriptor Learning with Adaptive Siamese Network</title>
    <summary>  Although the recent progress in the deep neural network has led to the
development of learnable local feature descriptors, there is no explicit answer
for estimation of the necessary size of a neural network. Specifically, the
local feature is represented in a low dimensional space, so the neural network
should have more compact structure. The small networks required for local
feature descriptor learning may be sensitive to initial conditions and learning
parameters and more likely to become trapped in local minima. In order to
address the above problem, we introduce an adaptive pruning Siamese
Architecture based on neuron activation to learn local feature descriptors,
making the network more computationally efficient with an improved recognition
rate over more complex networks. Our experiments demonstrate that our learned
local feature descriptors outperform the state-of-art methods in patch
matching.
</summary>
    <author>
      <name>Chong Huang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tim</arxiv:affiliation>
    </author>
    <author>
      <name>Qiong Liu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tim</arxiv:affiliation>
    </author>
    <author>
      <name>Yan-Ying Chen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tim</arxiv:affiliation>
    </author>
    <author>
      <name> Kwang-Ting</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tim</arxiv:affiliation>
    </author>
    <author>
      <name> Cheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 tables, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.05358v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.05358v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.02661v1</id>
    <updated>2017-07-10T00:14:32Z</updated>
    <published>2017-07-10T00:14:32Z</published>
    <title>Feature Joint-State Posterior Estimation in Factorial Speech Processing
  Models using Deep Neural Networks</title>
    <summary>  This paper proposes a new method for calculating joint-state posteriors of
mixed-audio features using deep neural networks to be used in factorial speech
processing models. The joint-state posterior information is required in
factorial models to perform joint-decoding. The novelty of this work is its
architecture which enables the network to infer joint-state posteriors from the
pairs of state posteriors of stereo features. This paper defines an objective
function to solve an underdetermined system of equations, which is used by the
network for extracting joint-state posteriors. It develops the required
expressions for fine-tuning the network in a unified way. The experiments
compare the proposed network decoding results to those of the vector Taylor
series method and show 2.3% absolute performance improvement in the monaural
speech separation and recognition challenge. This achievement is substantial
when we consider the simplicity of joint-state posterior extraction provided by
deep neural networks.
</summary>
    <author>
      <name>Mahdi Khademian</name>
    </author>
    <author>
      <name>Mohammad Mehdi Homayounpour</name>
    </author>
    <link href="http://arxiv.org/abs/1707.02661v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.02661v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.06682v1</id>
    <updated>2017-07-20T19:12:58Z</updated>
    <published>2017-07-20T19:12:58Z</published>
    <title>Resting state fMRI functional connectivity-based classification using a
  convolutional neural network architecture</title>
    <summary>  Machine learning techniques have become increasingly popular in the field of
resting state fMRI (functional magnetic resonance imaging) network based
classification. However, the application of convolutional networks has been
proposed only very recently and has remained largely unexplored. In this paper
we describe a convolutional neural network architecture for functional
connectome classification called connectome-convolutional neural network
(CCNN). Our results on simulated datasets and a publicly available dataset for
amnestic mild cognitive impairment classification demonstrate that our CCNN
model can efficiently distinguish between subject groups. We also show that the
connectome-convolutional network is capable to combine information from diverse
functional connectivity metrics and that models using a combination of
different connectivity descriptors are able to outperform classifiers using
only one metric. From this flexibility follows that our proposed CCNN model can
be easily adapted to a wide range of connectome based classification or
regression tasks, by varying which connectivity descriptor combinations are
used to train the network.
</summary>
    <author>
      <name>Regina Meszl√©nyi</name>
    </author>
    <author>
      <name>Krisztian Buza</name>
    </author>
    <author>
      <name>Zolt√°n Vidny√°nszky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 4 figures, 1 table, plus supplementary material</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.06682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.06682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T99" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.1; I.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09641v2</id>
    <updated>2017-08-16T14:02:23Z</updated>
    <published>2017-07-30T17:12:20Z</published>
    <title>Towards Visual Explanations for Convolutional Neural Networks via Input
  Resampling</title>
    <summary>  The predictive power of neural networks often costs model interpretability.
Several techniques have been developed for explaining model outputs in terms of
input features; however, it is difficult to translate such interpretations into
actionable insight. Here, we propose a framework to analyze predictions in
terms of the model's internal features by inspecting information flow through
the network. Given a trained network and a test image, we select neurons by two
metrics, both measured over a set of images created by perturbations to the
input image: (1) magnitude of the correlation between the neuron activation and
the network output and (2) precision of the neuron activation. We show that the
former metric selects neurons that exert large influence over the network
output while the latter metric selects neurons that activate on generalizable
features. By comparing the sets of neurons selected by these two metrics, our
framework suggests a way to investigate the internal attention mechanisms of
convolutional neural networks.
</summary>
    <author>
      <name>Benjamin J. Lengerich</name>
    </author>
    <author>
      <name>Sandeep Konam</name>
    </author>
    <author>
      <name>Eric P. Xing</name>
    </author>
    <author>
      <name>Stephanie Rosenthal</name>
    </author>
    <author>
      <name>Manuela Veloso</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at ICML 2017 Workshop on Visualization for Deep Learning</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.09641v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09641v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.02979v1</id>
    <updated>2017-08-09T19:34:26Z</updated>
    <published>2017-08-09T19:34:26Z</published>
    <title>Tikhonov Regularization for Long Short-Term Memory Networks</title>
    <summary>  It is a well-known fact that adding noise to the input data often improves
network performance. While the dropout technique may be a cause of memory loss,
when it is applied to recurrent connections, Tikhonov regularization, which can
be regarded as the training with additive noise, avoids this issue naturally,
though it implies regularizer derivation for different architectures. In case
of feedforward neural networks this is straightforward, while for networks with
recurrent connections and complicated layers it leads to some difficulties. In
this paper, a Tikhonov regularizer is derived for Long-Short Term Memory (LSTM)
networks. Although it is independent of time for simplicity, it considers
interaction between weights of the LSTM unit, which in theory makes it possible
to regularize the unit with complicated dependences by using only one parameter
that measures the input data perturbation. The regularizer that is proposed in
this paper has three parameters: one to control the regularization process, and
other two to maintain computation stability while the network is being trained.
The theory developed in this paper can be applied to get such regularizers for
different recurrent neural networks with Hadamard products and Lipschitz
continuous functions.
</summary>
    <author>
      <name>Andrei Turkin</name>
    </author>
    <link href="http://arxiv.org/abs/1708.02979v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.02979v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.04168v1</id>
    <updated>2017-08-14T15:11:04Z</updated>
    <published>2017-08-14T15:11:04Z</published>
    <title>Robust transformations of firing patterns for neural networks</title>
    <summary>  As a promising computational paradigm, occurrence of critical states in
artificial and biological neural networks has attracted wide-spread attention.
An often-made explicit or implicit assumption is that one single critical state
is responsible for two separate notions of criticality (avalanche criticality
and dynamical edge of chaos criticality). Previously, we provided an isolated
counter-example for co-occurrence. Here, we reveal a persistent paradigm of
structural transitions that such networks undergo, as the overall connectivity
strength is varied over its biologically meaningful range. Among these
transitions, only one avalanche critical point emerges, with edge of chaos
failing to co-occur. Our observations are based on ensembles of networks
obtained from variations of network configuration and their neurons. This
suggests that not only non-coincidence of criticality, but also the persistent
paradigm of network structural changes in function of the overall connectivity
strength, could be generic features of a large class of biological neural
networks.
</summary>
    <author>
      <name>Karlis Kanders</name>
    </author>
    <author>
      <name>Tom Lorimer</name>
    </author>
    <author>
      <name>Yoko Uwate</name>
    </author>
    <author>
      <name>Willi-Hans Steeb</name>
    </author>
    <author>
      <name>Ruedi Stoop</name>
    </author>
    <link href="http://arxiv.org/abs/1708.04168v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.04168v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.06019v2</id>
    <updated>2017-09-18T05:02:07Z</updated>
    <published>2017-08-20T21:10:42Z</published>
    <title>A Capacity Scaling Law for Artificial Neural Networks</title>
    <summary>  By assuming an ideal neural network with gating functions handling the worst
case data, we derive the calculation of two critical numbers predicting the
behavior of perceptron networks. First, we derive the calculation of what we
call the lossless memory (LM) dimension. The LM dimension is a generalization
of the Vapnik-Chervonenkis (VC) dimension that avoids structured data and
therefore provides an upper bound for perfectly fitting any training data.
Second, we derive what we call the MacKay (MK) dimension. This limit indicates
necessary forgetting, that is, the lower limit for most generalization uses of
the network. Our derivations are performed by embedding the ideal network into
Shannon's communication model which allows to interpret the two points as
capacities measured in bits. We validate our upper bounds with repeatable
experiments using different network configurations, diverse implementations,
varying activation functions, and several learning algorithms. The bottom line
is that the two capacity points scale strictly linear with the number of
weights. Among other practical applications, our result allows network
implementations with gating functions (e.\,g., sigmoid or rectified linear
units) to be evaluated against our upper limit independent of a concrete task.
</summary>
    <author>
      <name>Gerald Friedland</name>
    </author>
    <author>
      <name>Mario Krell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 4 figures, 2 listings of source code</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.06019v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.06019v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; F.1.1; I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.09161v1</id>
    <updated>2017-09-26T17:56:31Z</updated>
    <published>2017-09-26T17:56:31Z</published>
    <title>EDEN: Evolutionary Deep Networks for Efficient Machine Learning</title>
    <summary>  Deep neural networks continue to show improved performance with increasing
depth, an encouraging trend that implies an explosion in the possible
permutations of network architectures and hyperparameters for which there is
little intuitive guidance. To address this increasing complexity, we propose
Evolutionary DEep Networks (EDEN), a computationally efficient
neuro-evolutionary algorithm which interfaces to any deep neural network
platform, such as TensorFlow. We show that EDEN evolves simple yet successful
architectures built from embedding, 1D and 2D convolutional, max pooling and
fully connected layers along with their hyperparameters. Evaluation of EDEN
across seven image and sentiment classification datasets shows that it reliably
finds good networks -- and in three cases achieves state-of-the-art results --
even on a single GPU, in just 6-24 hours. Our study provides a first attempt at
applying neuro-evolution to the creation of 1D convolutional networks for
sentiment analysis including the optimisation of the embedding layer.
</summary>
    <author>
      <name>Emmanuel Dufourq</name>
    </author>
    <author>
      <name>Bruce A. Bassett</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures, 3 tables and see video
  https://vimeo.com/234510097</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.09161v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.09161v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.01467v2</id>
    <updated>2018-01-12T04:59:06Z</updated>
    <published>2017-10-04T05:38:50Z</published>
    <title>Mean-field theory of input dimensionality reduction in unsupervised deep
  neural networks</title>
    <summary>  Deep neural networks as powerful tools are widely used in various domains.
However, the nature of computations at each layer of the deep networks is far
from being well understood. Increasing the interpretability of deep neural
networks is thus important. Here, we construct a mean-field framework to
understand how compact representations are developed across layers, not only in
deterministic random deep networks but also in generative deep networks where
network parameters are learned from input data. Our theory shows that the deep
computation implements a dimensionality reduction while maintaining a finite
level of weak correlations between neurons for possible feature extraction.
This work may pave the way for understanding how a sensory hierarchy works in
general.
</summary>
    <author>
      <name>Haiping Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures, a physics explanation of decorrelation and
  dimensionality reduction is added</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.01467v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.01467v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.10328v1</id>
    <updated>2017-10-27T20:48:57Z</updated>
    <published>2017-10-27T20:48:57Z</published>
    <title>Revisit Fuzzy Neural Network: Demystifying Batch Normalization and ReLU
  with Generalized Hamming Network</title>
    <summary>  We revisit fuzzy neural network with a cornerstone notion of generalized
hamming distance, which provides a novel and theoretically justified framework
to re-interpret many useful neural network techniques in terms of fuzzy logic.
In particular, we conjecture and empirically illustrate that, the celebrated
batch normalization (BN) technique actually adapts the normalized bias such
that it approximates the rightful bias induced by the generalized hamming
distance. Once the due bias is enforced analytically, neither the optimization
of bias terms nor the sophisticated batch normalization is needed. Also in the
light of generalized hamming distance, the popular rectified linear units
(ReLU) can be treated as setting a minimal hamming distance threshold between
network inputs and weights. This thresholding scheme, on the one hand, can be
improved by introducing double thresholding on both extremes of neuron outputs.
On the other hand, ReLUs turn out to be non-essential and can be removed from
networks trained for simple tasks like MNIST classification. The proposed
generalized hamming network (GHN) as such not only lends itself to rigorous
analysis and interpretation within the fuzzy logic theory but also demonstrates
fast learning speed, well-controlled behaviour and state-of-the-art
performances on a variety of learning tasks.
</summary>
    <author>
      <name>Lixin Fan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures, NIPS 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.10328v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.10328v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.00215v2</id>
    <updated>2017-11-23T09:37:02Z</updated>
    <published>2017-11-01T05:50:19Z</published>
    <title>Minimum Energy Quantized Neural Networks</title>
    <summary>  This work targets the automated minimum-energy optimization of Quantized
Neural Networks (QNNs) - networks using low precision weights and activations.
These networks are trained from scratch at an arbitrary fixed point precision.
At iso-accuracy, QNNs using fewer bits require deeper and wider network
architectures than networks using higher precision operators, while they
require less complex arithmetic and less bits per weights. This fundamental
trade-off is analyzed and quantified to find the minimum energy QNN for any
benchmark and hence optimize energy-efficiency. To this end, the energy
consumption of inference is modeled for a generic hardware platform. This
allows drawing several conclusions across different benchmarks. First, energy
consumption varies orders of magnitude at iso-accuracy depending on the number
of bits used in the QNN. Second, in a typical system, BinaryNets or int4
implementations lead to the minimum energy solution, outperforming int8
networks up to 2-10x at iso-accuracy. All code used for QNN training is
available from https://github.com/BertMoons.
</summary>
    <author>
      <name>Bert Moons</name>
    </author>
    <author>
      <name>Koen Goetschalckx</name>
    </author>
    <author>
      <name>Nick Van Berckelaer</name>
    </author>
    <author>
      <name>Marian Verhelst</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">preprint for work presented at the 51st Asilomar Conference on
  Signals, Systems and Computers</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.00215v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.00215v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.01511v2</id>
    <updated>2017-12-16T05:05:33Z</updated>
    <published>2017-12-05T07:45:18Z</published>
    <title>Joint Embedding and Classification for SAR Target Recognition</title>
    <summary>  Deep learning can be an effective and efficient means to automatically detect
and classify targets in synthetic aperture radar (SAR) images, but it is
critical for trained neural networks to be robust to variations that exist
between training and test environments. The layers in a neural network can be
understood as successive transformations of an input image into embedded
feature representations and ultimately into a semantic class label. To address
the overfitting problem in SAR target classification, we train neural networks
to optimize the spatial clustering of points in the embedded space in addition
to optimizing the final classification score. We demonstrate that networks
trained with this dual embedding and classification loss outperform networks
with classification loss only. We study placing the embedding loss after
different network layers and find that applying the embedding loss on the
classification space results in the best SAR classification performance.
Finally, our visualization of the network's ten-dimensional classification
space supports our claim that the embedding loss encourages greater separation
between target class clusters for both training and testing partitions of the
MSTAR dataset.
</summary>
    <author>
      <name>Jiayun Wang</name>
    </author>
    <author>
      <name>Patrick Virtue</name>
    </author>
    <author>
      <name>Stella X. Yu</name>
    </author>
    <link href="http://arxiv.org/abs/1712.01511v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.01511v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.05055v1</id>
    <updated>2017-12-14T00:02:37Z</updated>
    <published>2017-12-14T00:02:37Z</published>
    <title>MentorNet: Regularizing Very Deep Neural Networks on Corrupted Labels</title>
    <summary>  Recent studies have discovered that deep networks are capable of memorizing
the entire data even when the labels are completely random. Since deep models
are trained on big data where labels are often noisy, the ability to overfit
noise can lead to poor performance. To overcome the overfitting on corrupted
training data, we propose a novel technique to regularize deep networks in the
data dimension. This is achieved by learning a neural network called MentorNet
to supervise the training of the base network, namely, StudentNet. Our work is
inspired by curriculum learning and advances the theory by learning a
curriculum from data by neural networks. We demonstrate the efficacy of
MentorNet on several benchmarks. Comprehensive experiments show that it is able
to significantly improve the generalization performance of the state-of-the-art
deep networks on corrupted training data.
</summary>
    <author>
      <name>Lu Jiang</name>
    </author>
    <author>
      <name>Zhengyuan Zhou</name>
    </author>
    <author>
      <name>Thomas Leung</name>
    </author>
    <author>
      <name>Li-Jia Li</name>
    </author>
    <author>
      <name>Li Fei-Fei</name>
    </author>
    <link href="http://arxiv.org/abs/1712.05055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.05055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00844v1</id>
    <updated>2018-02-02T20:53:31Z</updated>
    <published>2018-02-02T20:53:31Z</published>
    <title>Intriguing Properties of Randomly Weighted Networks: Generalizing While
  Learning Next to Nothing</title>
    <summary>  Training deep neural networks results in strong learned representations that
show good generalization capabilities. In most cases, training involves
iterative modification of all weights inside the network via back-propagation.
In Extreme Learning Machines, it has been suggested to set the first layer of a
network to fixed random values instead of learning it. In this paper, we
propose to take this approach a step further and fix almost all layers of a
deep convolutional neural network, allowing only a small portion of the weights
to be learned. As our experiments show, fixing even the majority of the
parameters of the network often results in performance which is on par with the
performance of learning all of them. The implications of this intriguing
property of deep neural networks are discussed and we suggest ways to harness
it to create more robust representations.
</summary>
    <author>
      <name>Amir Rosenfeld</name>
    </author>
    <author>
      <name>John K. Tsotsos</name>
    </author>
    <link href="http://arxiv.org/abs/1802.00844v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00844v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.01417v1</id>
    <updated>2018-03-04T20:45:06Z</updated>
    <published>2018-03-04T20:45:06Z</published>
    <title>Efficient and Accurate MRI Super-Resolution using a Generative
  Adversarial Network and 3D Multi-Level Densely Connected Network</title>
    <summary>  High-resolution (HR) magnetic resonance images (MRI) provide detailed
anatomical information important for clinical application and quantitative
image analysis. However, HR MRI conventionally comes at the cost of longer scan
time, smaller spatial coverage, and lower signal-to-noise ratio (SNR). Recent
studies have shown that single image super-resolution (SISR), a technique to
recover HR details from one single low-resolution (LR) input image, could
provide high-quality image details with the help of advanced deep convolutional
neural networks (CNN). However, deep neural networks consume memory heavily and
run slowly, especially in 3D settings. In this paper, we propose a novel 3D
neural network design, namely a multi-level densely connected super-resolution
network (mDCSRN) with generative adversarial network (GAN)-guided training. The
mDCSRN quickly trains and inferences and the GAN promotes realistic output
hardly distinguishable from original HR images. Our results from experiments on
a dataset with 1,113 subjects show that our new architecture beats other
popular deep learning methods in recovering 4x resolution-downgraded im-ages
and runs 6x faster.
</summary>
    <author>
      <name>Yuhua Chen</name>
    </author>
    <author>
      <name>Feng Shi</name>
    </author>
    <author>
      <name>Anthony G. Christodoulou</name>
    </author>
    <author>
      <name>Zhengwei Zhou</name>
    </author>
    <author>
      <name>Yibin Xie</name>
    </author>
    <author>
      <name>Debiao Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures, 2 tables. submitted MICCAI 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.01417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.01417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.1979v1</id>
    <updated>2009-03-11T14:14:51Z</updated>
    <published>2009-03-11T14:14:51Z</published>
    <title>Semantic learning in autonomously active recurrent neural networks</title>
    <summary>  The human brain is autonomously active, being characterized by a
self-sustained neural activity which would be present even in the absence of
external sensory stimuli. Here we study the interrelation between the
self-sustained activity in autonomously active recurrent neural nets and
external sensory stimuli.
  There is no a priori semantical relation between the influx of external
stimuli and the patterns generated internally by the autonomous and ongoing
brain dynamics. The question then arises when and how are semantic correlations
between internal and external dynamical processes learned and built up?
  We study this problem within the paradigm of transient state dynamics for the
neural activity in recurrent neural nets, i.e. for an autonomous neural
activity characterized by an infinite time-series of transiently stable
attractor states. We propose that external stimuli will be relevant during the
sensitive periods, {\it viz} the transition period between one transient state
and the subsequent semi-stable attractor. A diffusive learning signal is
generated unsupervised whenever the stimulus influences the internal dynamics
qualitatively.
  For testing we have presented to the model system stimuli corresponding to
the bars and stripes problem. We found that the system performs a non-linear
independent component analysis on its own, being continuously and autonomously
active. This emergent cognitive capability results here from a general
principle for the neural dynamics, the competition between neural ensembles.
</summary>
    <author>
      <name>C. Gros</name>
    </author>
    <author>
      <name>G. Kaczor</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/jigpal/jzp045</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/jigpal/jzp045" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Algorithms in Cognition, Informatics and Logic, special
  issue on `Perspectives and Challenges for Recurrent Neural Networks', in
  press</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Logic Journal of the IGPL, Vol. 18, 686 (2010)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0903.1979v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.1979v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9901015v1</id>
    <updated>1999-01-04T17:59:37Z</updated>
    <published>1999-01-04T17:59:37Z</published>
    <title>Messages on Networks as a Many-Body Problem</title>
    <summary>  In this paper we initiate an approach that deals with the problem of
calculating average properties of messages traveling on networks, by employing
concepts and methods that are used for the study of the many-body problem in
the field of physics. We set up a framework that simplifies enormously the
problem and, through a concrete example, we show how it can be applied to a
broad class of networks and protocols.
</summary>
    <author>
      <name>Haralambos Marmanis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9901015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9901015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0210538v1</id>
    <updated>2002-10-24T06:17:02Z</updated>
    <published>2002-10-24T06:17:02Z</published>
    <title>An Electrical Network Model of Plant Intelligence</title>
    <summary>  A simple electrical network model, having logical gate capacities, is
proposed here for computations in plant cells. It is compared and contrasted
with the animal brain network structure and functions.
</summary>
    <author>
      <name>Bikas K. Chakrabarti</name>
    </author>
    <author>
      <name>Omjyoti Dutta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 eps figures, 1 Table, Talk at the Condensed Matter
  Days-2002, held in Bhagalpur University, Bhagalpur, during August 29-31, 2002
  (to be published in Ind. J. Phys.)</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0210538v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0210538v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0308321v2</id>
    <updated>2003-10-13T22:30:20Z</updated>
    <published>2003-08-15T22:10:50Z</published>
    <title>Information Dynamics in the Networked World</title>
    <summary>  We review three studies of information flow in social networks that help
reveal their underlying social structure, how information spreads through them
and why small world experiments work.
</summary>
    <author>
      <name>Bernardo A. Huberman</name>
    </author>
    <author>
      <name>Lada A. Adamic</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/b98716</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/b98716" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Chapter submitted to "Complex Networks", Eli Ben-Naim, Hans
  Frauenfelder, Zoltan Toroczkai, eds. 29 pages, 17 figures. Submitted a minor
  revision (typos, re-wording, clarification) on Oct. 13, 2003</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0308321v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0308321v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0407598v1</id>
    <updated>2004-07-22T15:26:04Z</updated>
    <published>2004-07-22T15:26:04Z</published>
    <title>Epidemic spread in weighted networks</title>
    <summary>  We study the detailed epidemic spreading process in scale-free networks with
weight that denote familiarity between two people or computers. The result
shows that spreading velocity reaches a peak quickly then decays representing
power-law time behavior, and comparing to non-weighted networks, precise
hierarchical dynamics is not found although the nodes with larger strength is
preferential to be infected.
</summary>
    <author>
      <name>G. Yan</name>
    </author>
    <author>
      <name>Tao Zhou</name>
    </author>
    <author>
      <name>Jie Wang</name>
    </author>
    <author>
      <name>Zhongqian Fu</name>
    </author>
    <author>
      <name>Binghong Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0407598v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0407598v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0503291v1</id>
    <updated>2005-03-11T17:06:19Z</updated>
    <published>2005-03-11T17:06:19Z</published>
    <title>Log-periodic oscillations in degree distributions of hierarchical
  scale-free networks</title>
    <summary>  Hierarchical models of scale free networks are introduced where numbers of
nodes in clusters of a given hierarchy are stochastic variables. Our models
show periodic oscillations of degree distribution P(k) in the log-log scale.
Periods and amplitudes of such oscillations depend on network parameters.
Numerical simulations are in a good agreement to analytical calculations.
</summary>
    <author>
      <name>Krzysztof Suchecki</name>
    </author>
    <author>
      <name>Janusz A. Holyst</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0503291v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0503291v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0510822v2</id>
    <updated>2006-10-18T00:42:08Z</updated>
    <published>2005-10-31T00:51:29Z</published>
    <title>Synchronizabilities of Networks: A New index</title>
    <summary>  The random matrix theory is used to bridge the network structures and the
dynamical processes defined on them. We propose a possible dynamical mechanism
for the enhancement effect of network structures on synchronization processes,
based upon which a dynamic-based index of the synchronizability is introduced
in the present paper.
</summary>
    <author>
      <name>Huijie Yang</name>
    </author>
    <author>
      <name>Fangcui Zhao</name>
    </author>
    <author>
      <name>Binghong Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/1.2364178</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/1.2364178" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4pages, 2figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CHAOS 16,043112(2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0510822v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0510822v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0609605v2</id>
    <updated>2006-09-27T13:44:35Z</updated>
    <published>2006-09-23T14:22:51Z</published>
    <title>Pores in a two-dimensional network of DNA strands - computer simulations</title>
    <summary>  Formation of random network of DNA strands is simulated on a two-dimensional
triangular lattice. We investigate the size distribution of pores in the
network. The results are interpreted within theory of percolation on Bethe
lattice.
</summary>
    <author>
      <name>M. J. Krawczyk</name>
    </author>
    <author>
      <name>K. Kulakowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures. Corrected the reference value of the exponent tau</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Lect. Notes Comput. Sci. 3991 (2006) 665</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0609605v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0609605v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.soft" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.soft" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/physics/0607159v1</id>
    <updated>2006-07-18T07:26:59Z</updated>
    <published>2006-07-18T07:26:59Z</published>
    <title>An indicator for community structure</title>
    <summary>  An indicator for presence of community structure in networks is suggested. It
allows one to check whether such structures can exist, in principle, in any
particular network, without a need to apply computationally cost algorithms. In
this way we exclude a large class of networks that do not possess any community
structure.
</summary>
    <author>
      <name>V. Gol'dshtein</name>
    </author>
    <author>
      <name>G. A. Koganov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/physics/0607159v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/physics/0607159v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.00444v1</id>
    <updated>2015-05-03T16:11:26Z</updated>
    <published>2015-05-03T16:11:26Z</published>
    <title>Some Theoretical Properties of a Network of Discretely Firing Neurons</title>
    <summary>  The problem of optimising a network of discretely firing neurons is
addressed. An objective function is introduced which measures the average
number of bits that are needed for the network to encode its state. When this
is minimised, it is shown that this leads to a number of results, such as
topographic mappings, piecewise linear dependence on the input of the
probability of a neuron firing, and factorial encoder networks.
</summary>
    <author>
      <name>Stephen Luttrell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.00444v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.00444v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.08053v1</id>
    <updated>2017-10-23T00:56:32Z</updated>
    <published>2017-10-23T00:56:32Z</published>
    <title>Phase space sampling and operator confidence with generative adversarial
  networks</title>
    <summary>  We demonstrate that a generative adversarial network can be trained to
produce Ising model configurations in distinct regions of phase space. In
training a generative adversarial network, the discriminator neural network
becomes very good a discerning examples from the training set and examples from
the testing set. We demonstrate that this ability can be used as an anomaly
detector, producing estimations of operator values along with a confidence in
the prediction.
</summary>
    <author>
      <name>Kyle Mills</name>
    </author>
    <author>
      <name>Isaac Tamblyn</name>
    </author>
    <link href="http://arxiv.org/abs/1710.08053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.08053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/q-bio/0606041v2</id>
    <updated>2006-11-10T12:57:37Z</updated>
    <published>2006-06-29T16:58:05Z</published>
    <title>Designing Complex Networks</title>
    <summary>  We suggest a new perspective of research towards understanding the relations
between structure and dynamics of a complex network: Can we design a network,
e.g. by modifying the features of units or interactions, such that it exhibits
a desired dynamics? Here we present a case study where we positively answer
this question analytically for networks of spiking neural oscillators. First,
we present a method of finding the set of all networks (defined by all mutual
coupling strengths) that exhibit an arbitrary given periodic pattern of spikes
as an invariant solution. In such a pattern all the spike times of all the
neurons are exactly predefined. The method is very general as it covers
networks of different types of neurons, excitatory and inhibitory couplings,
interaction delays that may be heterogeneously distributed, and arbitrary
network connectivities. Second, we show how to design networks if further
restrictions are imposed, for instance by predefining the detailed network
connectivity. We illustrate the applicability of the method by examples of
Erd\"{o}s-R\'{e}nyi and power-law random networks. Third, the method can be
used to design networks that optimize network properties. To illustrate this
idea, we design networks that exhibit a predefined pattern dynamics while at
the same time minimizing the networks' wiring costs.
</summary>
    <author>
      <name>Raoul-Martin Memmesheimer</name>
    </author>
    <author>
      <name>Marc Timme</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physd.2006.09.037</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physd.2006.09.037" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages, 12 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physica D 224:182 (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/q-bio/0606041v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/q-bio/0606041v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.8268v2</id>
    <updated>2015-02-11T11:38:03Z</updated>
    <published>2014-09-29T19:54:33Z</published>
    <title>Slow poisoning and destruction of networks: Edge proximity and its
  implications for biological and infrastructure networks</title>
    <summary>  We propose a network metric, edge proximity, ${\cal P}_e$, which demonstrates
the importance of specific edges in a network, hitherto not captured by
existing network metrics. The effects of removing edges with high ${\cal P}_e$
might initially seem inconspicuous but are eventually shown to be very harmful
for networks. Compared to existing strategies, the removal of edges by ${\cal
P}_e$ leads to a remarkable increase in the diameter and average shortest path
length in undirected real and random networks till the first disconnection and
well beyond. ${\cal P}_e$ can be consistently used to rupture the network into
two nearly equal parts, thus presenting a very potent strategy to greatly harm
a network. Targeting by ${\cal P}_e$ causes notable efficiency loss in U.S. and
European power grid networks. ${\cal P}_e$ identifies proteins with essential
cellular functions in protein-protein interaction networks. It pinpoints
regulatory neural connections and important portions of the neural and brain
networks, respectively. Energy flow interactions identified by ${\cal P}_e$
form the backbone of long food web chains. Finally, we scrutinize the potential
of ${\cal P}_e$ in edge controllability dynamics of directed networks.
</summary>
    <author>
      <name>Soumya Jyoti Banerjee</name>
    </author>
    <author>
      <name>Saptarshi Sinha</name>
    </author>
    <author>
      <name>Soumen Roy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.91.022807</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.91.022807" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 Pages, 5 figures, Revtex-4.1</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 91, 022807 (2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1409.8268v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.8268v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.03781v1</id>
    <updated>2017-12-11T14:09:06Z</updated>
    <published>2017-12-11T14:09:06Z</published>
    <title>Learning Nested Sparse Structures in Deep Neural Networks</title>
    <summary>  Recently, there have been increasing demands to construct compact deep
architectures to remove unnecessary redundancy and to improve the inference
speed. While many recent works focus on reducing the redundancy by eliminating
unneeded weight parameters, it is not possible to apply a single deep
architecture for multiple devices with different resources. When a new device
or circumstantial condition requires a new deep architecture, it is necessary
to construct and train a new network from scratch. In this work, we propose a
novel deep learning framework, called a nested sparse network, which exploits
an n-in-1-type nested structure in a neural network. A nested sparse network
consists of multiple levels of networks with a different sparsity ratio
associated with each level, and higher level networks share parameters with
lower level networks to enable stable nested learning. The proposed framework
realizes a resource-aware versatile architecture as the same network can meet
diverse resource requirements. Moreover, the proposed nested network can learn
different forms of knowledge in its internal networks at different levels,
enabling multiple tasks using a single network, such as coarse-to-fine
hierarchical classification. In order to train the proposed nested sparse
network, we propose efficient weight connection learning and channel and layer
scheduling strategies. We evaluate our network in multiple tasks, including
adaptive deep compression, knowledge distillation, and learning class
hierarchy, and demonstrate that nested sparse networks perform competitively,
but more efficiently, than existing methods.
</summary>
    <author>
      <name>Eunwoo Kim</name>
    </author>
    <author>
      <name>Chanho Ahn</name>
    </author>
    <author>
      <name>Songhwai Oh</name>
    </author>
    <link href="http://arxiv.org/abs/1712.03781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.03781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/astro-ph/0312064v1</id>
    <updated>2003-12-02T16:58:53Z</updated>
    <published>2003-12-02T16:58:53Z</published>
    <title>Photometric redshifts with the Multilayer Perceptron Neural Network:
  application to the HDF-S and SDSS</title>
    <summary>  We present a technique for the estimation of photometric redshifts based on
feed-forward neural networks. The Multilayer Perceptron (MLP) Artificial Neural
Network is used to predict photometric redshifts in the HDF-S from an ultra
deep multicolor catalog. Various possible approaches for the training of the
neural network are explored, including the deepest and most complete
spectroscopic redshift catalog currently available (the Hubble Deep Field North
dataset) and models of the spectral energy distribution of galaxies available
in the literature. The MLP can be trained on observed data, theoretical data
and mixed samples. The prediction of the method is tested on the spectroscopic
sample in the HDF-S (44 galaxies). Over the entire redshift range, $0.1&lt;z&lt;3.5$,
the agreement between the photometric and spectroscopic redshifts in the HDF-S
is good: the training on mixed data produces sigma_z(test) ~ 0.11, showing that
model libraries together with observed data provide a sufficiently complete
description of the galaxy population. The neural system capability is also
tested in a low redshift regime, 0&lt;z&lt;0.4, using the Sloan Digital Sky Survey
Data Release One (DR1) spectroscopic sample. The resulting accuracy on 88108
galaxies is sigma_z(test) ~ 0.022. Inputs other than galaxy colors - such as
morphology, angular size and surface brightness - may be easily incorporated in
the neural network technique. An important feature, in view of the application
of the technique to large databases, is the computational speed: in the
evaluation phase, redshifts of 10^5 galaxies are estimated in few seconds.
</summary>
    <author>
      <name>E. Vanzella</name>
    </author>
    <author>
      <name>S. Cristiani</name>
    </author>
    <author>
      <name>A. Fontana</name>
    </author>
    <author>
      <name>M. Nonino</name>
    </author>
    <author>
      <name>S. Arnouts</name>
    </author>
    <author>
      <name>E. Giallongo</name>
    </author>
    <author>
      <name>A. Grazian</name>
    </author>
    <author>
      <name>G. Fasano</name>
    </author>
    <author>
      <name>P. Popesso</name>
    </author>
    <author>
      <name>P. Saracco</name>
    </author>
    <author>
      <name>S. Zaggia</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1051/0004-6361:20040176</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1051/0004-6361:20040176" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages. Submitted to A&amp;A in Feb. 2003, revised in Oct. 2003</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Astron.Astrophys. 423 (2004) 761-776</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/astro-ph/0312064v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/astro-ph/0312064v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.1007v1</id>
    <updated>2009-12-05T12:41:40Z</updated>
    <published>2009-12-05T12:41:40Z</published>
    <title>Designing Kernel Scheme for Classifiers Fusion</title>
    <summary>  In this paper, we propose a special fusion method for combining ensembles of
base classifiers utilizing new neural networks in order to improve overall
efficiency of classification. While ensembles are designed such that each
classifier is trained independently while the decision fusion is performed as a
final procedure, in this method, we would be interested in making the fusion
process more adaptive and efficient. This new combiner, called Neural Network
Kernel Least Mean Square1, attempts to fuse outputs of the ensembles of
classifiers. The proposed Neural Network has some special properties such as
Kernel abilities,Least Mean Square features, easy learning over variants of
patterns and traditional neuron capabilities. Neural Network Kernel Least Mean
Square is a special neuron which is trained with Kernel Least Mean Square
properties. This new neuron is used as a classifiers combiner to fuse outputs
of base neural network classifiers. Performance of this method is analyzed and
compared with other fusion methods. The analysis represents higher performance
of our new method as opposed to others.
</summary>
    <author>
      <name>Mehdi Salkhordeh Haghighi</name>
    </author>
    <author>
      <name>Hadi Sadoghi Yazdi</name>
    </author>
    <author>
      <name>Abedin Vahedian</name>
    </author>
    <author>
      <name>Hamed Modaghegh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS November 2009, ISSN 1947 5500,
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 6, No. 2, pp. 239-248, November 2009, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0912.1007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.1007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.05153v1</id>
    <updated>2015-02-18T08:39:39Z</updated>
    <published>2015-02-18T08:39:39Z</published>
    <title>Neural Synchronization based Secret Key Exchange over Public Channels: A
  survey</title>
    <summary>  Exchange of secret keys over public channels based on neural synchronization
using a variety of learning rules offer an appealing alternative to number
theory based cryptography algorithms. Though several forms of attacks are
possible on this neural protocol e.g. geometric, genetic and majority attacks,
our survey finds that deterministic algorithms that synchronize with the
end-point networks have high time complexity, while probabilistic and
population-based algorithms have demonstrated ability to decode the key during
its exchange over the public channels. Our survey also discusses queries,
heuristics, erroneous information, group key exchange, synaptic depths, etc,
that have been proposed to increase the time complexity of algorithmic
interception or decoding of the key during exchange. The Tree Parity Machine
and its variants, neural networks with tree topologies incorporating parity
checking of state bits, appear to be one of the most secure and stable models
of the end-point networks. Our survey also mentions some noteworthy studies on
neural networks applied to other necessary aspects of cryptography. We conclude
that discovery of neural architectures with very high synchronization speed,
and designing the encoding and entropy of the information exchanged during
mutual learning, and design of extremely sensitive chaotic maps for
transformation of synchronized states of the networks to chaotic encryption
keys, are the primary issues in this field.
</summary>
    <author>
      <name>Sandip Chakraborty</name>
    </author>
    <author>
      <name>Jiban Dalal</name>
    </author>
    <author>
      <name>Bikramjit Sarkar</name>
    </author>
    <author>
      <name>Debaprasad Mukherjee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures, Review article</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.05153v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.05153v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.08985v2</id>
    <updated>2015-10-10T03:18:45Z</updated>
    <published>2015-09-30T01:06:36Z</published>
    <title>Generalizing Pooling Functions in Convolutional Neural Networks: Mixed,
  Gated, and Tree</title>
    <summary>  We seek to improve deep neural networks by generalizing the pooling
operations that play a central role in current architectures. We pursue a
careful exploration of approaches to allow pooling to learn and to adapt to
complex and variable patterns. The two primary directions lie in (1) learning a
pooling function via (two strategies of) combining of max and average pooling,
and (2) learning a pooling function in the form of a tree-structured fusion of
pooling filters that are themselves learned. In our experiments every
generalized pooling operation we explore improves performance when used in
place of average or max pooling. We experimentally demonstrate that the
proposed pooling operations provide a boost in invariance properties relative
to conventional pooling and set the state of the art on several widely adopted
benchmark datasets; they are also easy to implement, and can be applied within
various deep neural network architectures. These benefits come with only a
light increase in computational overhead during training and a very modest
increase in the number of model parameters.
</summary>
    <author>
      <name>Chen-Yu Lee</name>
    </author>
    <author>
      <name>Patrick W. Gallagher</name>
    </author>
    <author>
      <name>Zhuowen Tu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Patent disclosure, UCSD Docket No. SD2015-184, "Forest Convolutional
  Neural Network", filed on March 4, 2015. UCSD Docket No. SD2016-053,
  "Generalizing Pooling Functions in Convolutional Neural Network", filed on
  Sept 23, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.08985v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.08985v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.02954v2</id>
    <updated>2016-01-03T17:18:06Z</updated>
    <published>2015-11-10T01:20:51Z</published>
    <title>Reducing the Training Time of Neural Networks by Partitioning</title>
    <summary>  This paper presents a new method for pre-training neural networks that can
decrease the total training time for a neural network while maintaining the
final performance, which motivates its use on deep neural networks. By
partitioning the training task in multiple training subtasks with sub-models,
which can be performed independently and in parallel, it is shown that the size
of the sub-models reduces almost quadratically with the number of subtasks
created, quickly scaling down the sub-models used for the pre-training. The
sub-models are then merged to provide a pre-trained initial set of weights for
the original model. The proposed method is independent of the other aspects of
the training, such as architecture of the neural network, training method, and
objective, making it compatible with a wide range of existing approaches. The
speedup without loss of performance is validated experimentally on MNIST and on
CIFAR10 data sets, also showing that even performing the subtasks sequentially
can decrease the training time. Moreover, we show that larger models may
present higher speedups and conjecture about the benefits of the method in
distributed learning systems.
</summary>
    <author>
      <name>Conrado S. Miranda</name>
    </author>
    <author>
      <name>Fernando J. Von Zuben</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Figure 2b has lower quality due to file size constraints</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.02954v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.02954v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08557v1</id>
    <updated>2016-02-27T05:37:44Z</updated>
    <published>2016-02-27T05:37:44Z</published>
    <title>Multiplier-less Artificial Neurons Exploiting Error Resiliency for
  Energy-Efficient Neural Computing</title>
    <summary>  Large-scale artificial neural networks have shown significant promise in
addressing a wide range of classification and recognition applications.
However, their large computational requirements stretch the capabilities of
computing platforms. The fundamental components of these neural networks are
the neurons and its synapses. The core of a digital hardware neuron consists of
multiplier, accumulator and activation function. Multipliers consume most of
the processing energy in the digital neurons, and thereby in the hardware
implementations of artificial neural networks. We propose an approximate
multiplier that utilizes the notion of computation sharing and exploits error
resilience of neural network applications to achieve improved energy
consumption. We also propose Multiplier-less Artificial Neuron (MAN) for even
larger improvement in energy consumption and adapt the training process to
ensure minimal degradation in accuracy. We evaluated the proposed design on 5
recognition applications. The results show, 35% and 60% reduction in energy
consumption, for neuron sizes of 8 bits and 12 bits, respectively, with a
maximum of ~2.83% loss in network accuracy, compared to a conventional neuron
implementation. We also achieve 37% and 62% reduction in area for a neuron size
of 8 bits and 12 bits, respectively, under iso-speed conditions.
</summary>
    <author>
      <name>Syed Shakib Sarwar</name>
    </author>
    <author>
      <name>Swagath Venkataramani</name>
    </author>
    <author>
      <name>Anand Raghunathan</name>
    </author>
    <author>
      <name>Kaushik Roy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in Design, Automation and Test in Europe 2016 conference
  (DATE-2016)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Design, Automation &amp; Test in Europe Conference &amp; Exhibition
  (DATE), 2016, pp. 145-150</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.08557v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08557v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05335v4</id>
    <updated>2017-08-11T10:34:03Z</updated>
    <published>2016-05-17T20:00:16Z</published>
    <title>Variable Synaptic Strengths Controls the Firing Rate Distribution in
  Feedforward Neural Networks</title>
    <summary>  Heterogeneity of firing rate statistics is known to have severe consequences
on neural coding. Recent experimental recordings in weakly electric fish
indicate that the distribution-width of superficial pyramidal cell firing rates
(trial- and time-averaged) in the electrosensory lateral line lobe (ELL)
depends on the stimulus, and also that network inputs can mediate changes in
the firing rate distribution across the population. We previously developed
theoretical methods to understand how two attributes (synaptic and intrinsic
heterogeneity) interact and alter the firing rate distribution in a population
of integrate-and-fire neurons with random recurrent coupling. Inspired by our
experimental data, we extend these theoretical results to a delayed feedforward
spiking network that qualitatively capture the changes of firing rate
heterogeneity observed in in-vivo recordings. We demonstrate how heterogeneous
neural attributes alter firing rate heterogeneity, accounting for the effect
with various sensory stimuli. The model predicts how the strength of the
effective network connectivity is related to intrinsic heterogeneity in such
delayed feedforward networks: the strength of the feedforward input is
positively correlated with excitability (threshold value for spiking) with low
firing rate heterogeneity and is negatively correlated with excitability with
high firing rate heterogeneity. We also show how our theory can be use to
predict effective neural architecture. We demonstrate that neural attributes do
not interact in a simple manner but rather in a complex stimulus-dependent
fashion to control neural heterogeneity and discuss how it can ultimately shape
population codes.
</summary>
    <author>
      <name>Cheng Ly</name>
    </author>
    <author>
      <name>Gary Marsat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.05335v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05335v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01164v2</id>
    <updated>2016-09-27T16:05:36Z</updated>
    <published>2016-06-03T16:17:01Z</published>
    <title>Dense Associative Memory for Pattern Recognition</title>
    <summary>  A model of associative memory is studied, which stores and reliably retrieves
many more patterns than the number of neurons in the network. We propose a
simple duality between this dense associative memory and neural networks
commonly used in deep learning. On the associative memory side of this duality,
a family of models that smoothly interpolates between two limiting cases can be
constructed. One limit is referred to as the feature-matching mode of pattern
recognition, and the other one as the prototype regime. On the deep learning
side of the duality, this family corresponds to feedforward neural networks
with one hidden layer and various activation functions, which transmit the
activities of the visible neurons to the hidden layer. This family of
activation functions includes logistics, rectified linear units, and rectified
polynomials of higher degrees. The proposed duality makes it possible to apply
energy-based intuition from associative memory to analyze computational
properties of neural networks with unusual activation functions - the higher
rectified polynomials which until now have not been used in deep learning. The
utility of the dense memories is illustrated for two test cases: the logical
gate XOR and the recognition of handwritten digits from the MNIST data set.
</summary>
    <author>
      <name>Dmitry Krotov</name>
    </author>
    <author>
      <name>John J Hopfield</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at NIPS 2016</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in Neural Information Processing Systems 29 (2016),
  1172-1180</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.01164v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01164v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07061v1</id>
    <updated>2016-09-22T16:48:03Z</updated>
    <published>2016-09-22T16:48:03Z</published>
    <title>Quantized Neural Networks: Training Neural Networks with Low Precision
  Weights and Activations</title>
    <summary>  We introduce a method to train Quantized Neural Networks (QNNs) --- neural
networks with extremely low precision (e.g., 1-bit) weights and activations, at
run-time. At train-time the quantized weights and activations are used for
computing the parameter gradients. During the forward pass, QNNs drastically
reduce memory size and accesses, and replace most arithmetic operations with
bit-wise operations. As a result, power consumption is expected to be
drastically reduced. We trained QNNs over the MNIST, CIFAR-10, SVHN and
ImageNet datasets. The resulting QNNs achieve prediction accuracy comparable to
their 32-bit counterparts. For example, our quantized version of AlexNet with
1-bit weights and 2-bit activations achieves $51\%$ top-1 accuracy. Moreover,
we quantize the parameter gradients to 6-bits as well which enables gradients
computation using only bit-wise operation. Quantized recurrent neural networks
were tested over the Penn Treebank dataset, and achieved comparable accuracy as
their 32-bit counterparts using only 4-bits. Last but not least, we programmed
a binary matrix multiplication GPU kernel with which it is possible to run our
MNIST QNN 7 times faster than with an unoptimized GPU kernel, without suffering
any loss in classification accuracy. The QNN code is available online.
</summary>
    <author>
      <name>Itay Hubara</name>
    </author>
    <author>
      <name>Matthieu Courbariaux</name>
    </author>
    <author>
      <name>Daniel Soudry</name>
    </author>
    <author>
      <name>Ran El-Yaniv</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1602.02830</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.07061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.09792v4</id>
    <updated>2018-02-25T23:42:06Z</updated>
    <published>2017-05-27T09:04:55Z</published>
    <title>Deep Complex Networks</title>
    <summary>  At present, the vast majority of building blocks, techniques, and
architectures for deep learning are based on real-valued operations and
representations. However, recent work on recurrent neural networks and older
fundamental theoretical analysis suggests that complex numbers could have a
richer representational capacity and could also facilitate noise-robust memory
retrieval mechanisms. Despite their attractive properties and potential for
opening up entirely new neural architectures, complex-valued deep neural
networks have been marginalized due to the absence of the building blocks
required to design such models. In this work, we provide the key atomic
components for complex-valued deep neural networks and apply them to
convolutional feed-forward networks and convolutional LSTMs. More precisely, we
rely on complex convolutions and present algorithms for complex
batch-normalization, complex weight initialization strategies for
complex-valued neural nets and we use them in experiments with end-to-end
training schemes. We demonstrate that such complex-valued models are
competitive with their real-valued counterparts. We test deep complex models on
several computer vision tasks, on music transcription using the MusicNet
dataset and on Speech Spectrum Prediction using the TIMIT dataset. We achieve
state-of-the-art performance on these audio-related tasks.
</summary>
    <author>
      <name>Chiheb Trabelsi</name>
    </author>
    <author>
      <name>Olexa Bilaniuk</name>
    </author>
    <author>
      <name>Ying Zhang</name>
    </author>
    <author>
      <name>Dmitriy Serdyuk</name>
    </author>
    <author>
      <name>Sandeep Subramanian</name>
    </author>
    <author>
      <name>Jo√£o Felipe Santos</name>
    </author>
    <author>
      <name>Soroush Mehri</name>
    </author>
    <author>
      <name>Negar Rostamzadeh</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <author>
      <name>Christopher J Pal</name>
    </author>
    <link href="http://arxiv.org/abs/1705.09792v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.09792v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.11146v2</id>
    <updated>2017-10-14T15:08:04Z</updated>
    <published>2017-05-31T15:31:26Z</published>
    <title>SuperSpike: Supervised learning in multi-layer spiking neural networks</title>
    <summary>  A vast majority of computation in the brain is performed by spiking neural
networks. Despite the ubiquity of such spiking, we currently lack an
understanding of how biological spiking neural circuits learn and compute
in-vivo, as well as how we can instantiate such capabilities in artificial
spiking circuits in-silico. Here we revisit the problem of supervised learning
in temporally coding multi-layer spiking neural networks. First, by using a
surrogate gradient approach, we derive SuperSpike, a nonlinear voltage-based
three factor learning rule capable of training multi-layer networks of
deterministic integrate-and-fire neurons to perform nonlinear computations on
spatiotemporal spike patterns. Second, inspired by recent results on feedback
alignment, we compare the performance of our learning rule under different
credit assignment strategies for propagating output errors to hidden units.
Specifically, we test uniform, symmetric and random feedback, finding that
simpler tasks can be solved with any type of feedback, while more complex tasks
require symmetric feedback. In summary, our results open the door to obtaining
a better scientific understanding of learning and computation in spiking neural
networks by advancing our ability to train them to solve nonlinear problems
involving transformations between different spatiotemporal spike-time patterns.
</summary>
    <author>
      <name>Friedemann Zenke</name>
    </author>
    <author>
      <name>Surya Ganguli</name>
    </author>
    <link href="http://arxiv.org/abs/1705.11146v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.11146v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.02268v1</id>
    <updated>2017-09-06T12:59:14Z</updated>
    <published>2017-09-06T12:59:14Z</published>
    <title>Phylogenetic Convolutional Neural Networks in Metagenomics</title>
    <summary>  Background: Convolutional Neural Networks can be effectively used only when
data are endowed with an intrinsic concept of neighbourhood in the input space,
as is the case of pixels in images. We introduce here Ph-CNN, a novel deep
learning architecture for the classification of metagenomics data based on the
Convolutional Neural Networks, with the patristic distance defined on the
phylogenetic tree being used as the proximity measure. The patristic distance
between variables is used together with a sparsified version of
MultiDimensional Scaling to embed the phylogenetic tree in a Euclidean space.
Results: Ph-CNN is tested with a domain adaptation approach on synthetic data
and on a metagenomics collection of gut microbiota of 38 healthy subjects and
222 Inflammatory Bowel Disease patients, divided in 6 subclasses.
Classification performance is promising when compared to classical algorithms
like Support Vector Machines and Random Forest and a baseline fully connected
neural network, e.g. the Multi-Layer Perceptron. Conclusion: Ph-CNN represents
a novel deep learning approach for the classification of metagenomics data.
Operatively, the algorithm has been implemented as a custom Keras layer taking
care of passing to the following convolutional layer not only the data but also
the ranked list of neighbourhood of each sample, thus mimicking the case of
image data, transparently to the user. Keywords: Metagenomics; Deep learning;
Convolutional Neural Networks; Phylogenetic trees
</summary>
    <author>
      <name>Diego Fioravanti</name>
    </author>
    <author>
      <name>Ylenia Giarratano</name>
    </author>
    <author>
      <name>Valerio Maggio</name>
    </author>
    <author>
      <name>Claudio Agostinelli</name>
    </author>
    <author>
      <name>Marco Chierici</name>
    </author>
    <author>
      <name>Giuseppe Jurman</name>
    </author>
    <author>
      <name>Cesare Furlanello</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at BMTL 2017, Naples</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.02268v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.02268v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.06309v1</id>
    <updated>2017-09-19T09:22:12Z</updated>
    <published>2017-09-19T09:22:12Z</published>
    <title>Aspect-Based Relational Sentiment Analysis Using a Stacked Neural
  Network Architecture</title>
    <summary>  Sentiment analysis can be regarded as a relation extraction problem in which
the sentiment of some opinion holder towards a certain aspect of a product,
theme or event needs to be extracted. We present a novel neural architecture
for sentiment analysis as a relation extraction problem that addresses this
problem by dividing it into three subtasks: i) identification of aspect and
opinion terms, ii) labeling of opinion terms with a sentiment, and iii)
extraction of relations between opinion terms and aspect terms. For each
subtask, we propose a neural network based component and combine all of them
into a complete system for relational sentiment analysis. The component for
aspect and opinion term extraction is a hybrid architecture consisting of a
recurrent neural network stacked on top of a convolutional neural network. This
approach outperforms a standard convolutional deep neural architecture as well
as a recurrent network architecture and performs competitively compared to
other methods on two datasets of annotated customer reviews. To extract
sentiments for individual opinion terms, we propose a recurrent architecture in
combination with word distance features and achieve promising results,
outperforming a majority baseline by 18% accuracy and providing the first
results for the USAGE dataset. Our relation extraction component outperforms
the current state-of-the-art in aspect-opinion relation extraction by 15%
F-Measure.
</summary>
    <author>
      <name>Soufian Jebbara</name>
    </author>
    <author>
      <name>Philipp Cimiano</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3233/978-1-61499-672-9-1123</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3233/978-1-61499-672-9-1123" rel="related"/>
    <link href="http://arxiv.org/abs/1709.06309v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.06309v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.08904v1</id>
    <updated>2017-10-24T17:39:00Z</updated>
    <published>2017-10-24T17:39:00Z</published>
    <title>Pre-Processing-Free Gear Fault Diagnosis Using Small Datasets with Deep
  Convolutional Neural Network-Based Transfer Learning</title>
    <summary>  Early fault diagnosis in complex mechanical systems such as gearbox has
always been a great challenge, even with the recent development in deep neural
networks. The performance of a classic fault diagnosis system predominantly
depends on the features extracted and the classifier subsequently applied.
Although a large number of attempts have been made regarding feature extraction
techniques, the methods require great human involvements are heavily depend on
domain expertise and may thus be non-representative and biased from application
to application. On the other hand, while the deep neural networks based
approaches feature adaptive feature extractions and inherent classifications,
they usually require a substantial set of training data and thus hinder their
usage for engineering applications with limited training data such as gearbox
fault diagnosis. This paper develops a deep convolutional neural network-based
transfer learning approach that not only entertains pre-processing free
adaptive feature extractions, but also requires only a small set of training
data. The proposed approach performs gear fault diagnosis using pre-processing
free raw accelerometer data and experiments with various sizes of training data
were conducted. The superiority of the proposed approach is revealed by
comparing the performance with other methods such as locally trained
convolution neural network and angle-frequency analysis based support vector
machine. The achieved accuracy indicates that the proposed approach is not only
viable and robust, but also has the potential to be readily applicable to other
fault diagnosis practices.
</summary>
    <author>
      <name>Pei Cao</name>
    </author>
    <author>
      <name>Shengli Zhang</name>
    </author>
    <author>
      <name>Jiong Tang</name>
    </author>
    <link href="http://arxiv.org/abs/1710.08904v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.08904v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.02833v1</id>
    <updated>2017-11-08T05:11:37Z</updated>
    <published>2017-11-08T05:11:37Z</published>
    <title>Traffic Prediction Based on Random Connectivity in Deep Learning with
  Long Short-Term Memory</title>
    <summary>  Traffic prediction plays an important role in evaluating the performance of
telecommunication networks and attracts intense research interests. A
significant number of algorithms and models have been proposed to learn
knowledge from traffic data and improve the prediction accuracy. In the recent
big data era, the relevant research enthusiasm remains and deep learning has
been exploited to extract the useful information in depth. In particular, Long
Short-Term Memory (LSTM), one kind of Recurrent Neural Network (RNN) schemes,
has attracted significant attentions due to the long-range dependency embedded
in the sequential traffic data. However, LSTM has considerable computational
cost, which can not be tolerated in tasks with stringent latency requirement.
In this paper, we propose a deep learning model based on LSTM, called Random
Connectivity LSTM (RCLSTM). Compared to the conventional LSTM, RCLSTM achieves
a significant breakthrough in the architecture formation of neural network,
whose connectivity is determined in a stochastic manner rather than full
connected. So, the neural network in RCLSTM can exhibit certain sparsity, which
means many neural connections are absent (distinguished from the full
connectivity) and thus the number of parameters to be trained is reduced and
much fewer computations are required. We apply the RCLSTM solution to predict
traffic and validate that the RCLSTM with even 35% neural connectivity still
shows a strong capability in traffic prediction. Also, along with increasing
the number of training samples, the performance of RCLSTM becomes closer to the
conventional LSTM. Moreover, the RCLSTM exhibits even superior prediction
accuracy than the conventional LSTM when the length of input traffic sequences
increases.
</summary>
    <author>
      <name>Yuxiu Hua</name>
    </author>
    <author>
      <name>Zhifeng Zhao</name>
    </author>
    <author>
      <name>Rongpeng Li</name>
    </author>
    <author>
      <name>Xianfu Chen</name>
    </author>
    <author>
      <name>Zhiming Liu</name>
    </author>
    <author>
      <name>Honggang Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.02833v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.02833v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.03073v2</id>
    <updated>2017-11-09T02:35:25Z</updated>
    <published>2017-11-08T18:02:47Z</published>
    <title>Lower bounds over Boolean inputs for deep neural networks with ReLU
  gates</title>
    <summary>  Motivated by the resurgence of neural networks in being able to solve complex
learning tasks we undertake a study of high depth networks using ReLU gates
which implement the function $x \mapsto \max\{0,x\}$. We try to understand the
role of depth in such neural networks by showing size lowerbounds against such
network architectures in parameter regimes hitherto unexplored. In particular
we show the following two main results about neural nets computing Boolean
functions of input dimension $n$,
  1. We use the method of random restrictions to show almost linear,
$\Omega(\epsilon^{2(1-\delta)}n^{1-\delta})$, lower bound for completely weight
unrestricted LTF-of-ReLU circuits to match the Andreev function on at least
$\frac{1}{2} +\epsilon$ fraction of the inputs for $\epsilon &gt;
\sqrt{2\frac{\log^{\frac {2}{2-\delta}}(n)}{n}}$ for any $\delta \in (0,\frac 1
2)$
  2. We use the method of sign-rank to show exponential in dimension lower
bounds for ReLU circuits ending in a LTF gate and of depths upto $O(n^{\xi})$
with $\xi &lt; \frac{1}{8}$ with some restrictions on the weights in the bottom
most layer. All other weights in these circuits are kept unrestricted. This in
turns also implies the same lowerbounds for LTF circuits with the same
architecture and the same weight restrictions on their bottom most layer.
  Along the way we also show that there exists a $\mathbb{R}^ n\rightarrow
\mathbb{R}$ Sum-of-ReLU-of-ReLU function which Sum-of-ReLU neural nets can
never represent no matter how large they are allowed to be.
</summary>
    <author>
      <name>Anirbit Mukherjee</name>
    </author>
    <author>
      <name>Amitabh Basu</name>
    </author>
    <link href="http://arxiv.org/abs/1711.03073v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.03073v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.05733v2</id>
    <updated>2018-02-07T19:00:47Z</updated>
    <published>2018-01-17T16:20:32Z</published>
    <title>Deep Convolutional Neural Networks for Eigenvalue Problems in Mechanics</title>
    <summary>  In this paper we show that deep convolutional neural networks (CNN) can
massively outperform traditional densely connected neural networks (both deep
or shallow) in predicting eigenvalue problems in mechanics. In this sense, we
strike out in a novel direction in mechanics computations with strongly
predictive neural networks whose success depends not only neural architectures
being deep but also being fundamentally different from neural architectures
which have been used in mechanics till now. To show this, we consider a model
problem: predicting the eigenvalues of a 1-D phononic crystal, however, the
general observations pertaining to the predictive superiority of CNNs over MLPs
should extend to other problems in mechanics as well. In the present problem,
the optimal CNN architecture reaches $98\%$ accuracy level on unseen data when
trained with just 20,000 training samples. Fully-connected multi-layer
perceptrons (MLP) - the network of choice in mechanics research - on the other
hand, does not improve beyond $85\%$ accuracy even with $100,000$ training
samples. We also show that even with a relatively small amount of training
data, CNNs have the capability to generalize well for our problems and that
they automatically learn deep symmetry operations such as translational
invariance. Most importantly, however, we show how CNNs can naturally represent
mechanical material tensors and that the convolution operation of CNNs has the
ability to serve as local receptive fields which is a natural representation of
mechanical response. Strategies proposed here may be used for other problems of
mechanics and may, in the future, be used to completely sidestep certain
cumbersome algorithms with a purely data driven approach based upon deep
architectures of modern neural networks such as deep CNNs.
</summary>
    <author>
      <name>David Finol</name>
    </author>
    <author>
      <name>Yan Lu</name>
    </author>
    <author>
      <name>Vijay Mahadevan</name>
    </author>
    <author>
      <name>Ankit Srivastava</name>
    </author>
    <link href="http://arxiv.org/abs/1801.05733v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.05733v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0509711v4</id>
    <updated>2005-12-24T12:48:36Z</updated>
    <published>2005-09-28T02:28:05Z</published>
    <title>Optimization of network structure to random failures</title>
    <summary>  Network's resilience to the malfunction of its components has been of great
concern. The goal of this work is to determine the network design guidelines,
which maximizes the network efficiency while keeping the cost of the network
(that is the average connectivity) constant. With a global optimization method,
memory tabu search (MTS), we get the optimal network structure with the
approximately best efficiency. We analyze the statistical characters of the
network and find that a network with a small quantity of hub nodes, high degree
of clustering may be much more resilient to perturbations than a random network
and the optimal network is one kind of highly heterogeneous networks. The
results strongly suggest that networks with higher efficiency are more robust
to random failures. In addition, we propose a simple model to describe the
statistical properties of the optimal network and investigate the
synchronizability of this model.
</summary>
    <author>
      <name>Bing Wang</name>
    </author>
    <author>
      <name>Huanwen Tang</name>
    </author>
    <author>
      <name>Chonghui Guo</name>
    </author>
    <author>
      <name>Zhilong Xiu</name>
    </author>
    <author>
      <name>Tao Zhou</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physa.2005.12.050</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physa.2005.12.050" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 6 figures, accepted by Physica A</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0509711v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0509711v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.0894v1</id>
    <updated>2009-07-06T08:13:06Z</updated>
    <published>2009-07-06T08:13:06Z</published>
    <title>Percolation on interacting networks</title>
    <summary>  Most networks of interest do not live in isolation. Instead they form
components of larger systems in which multiple networks with distinct
topologies coexist and where elements distributed amongst different networks
may interact directly. Here we develop a mathematical framework based on
generating functions for analyzing a system of L interacting networks given the
connectivity within and between networks. We derive exact expressions for the
percolation threshold describing the onset of large-scale connectivity in the
system of networks and each network individually. These general expressions
apply to networks with arbitrary degree distributions and we explicitly
evaluate them for L=2 interacting networks with a few choices of degree
distributions. We show that the percolation threshold in an individual network
can be significantly lowered once "hidden" connections to other networks are
considered. We show applications of the framework to two real-world systems
involving communications networks and socio-tecnical congruence in software
systems.
</summary>
    <author>
      <name>E. A. Leicht</name>
    </author>
    <author>
      <name>Raissa M. D'Souza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figues</arxiv:comment>
    <link href="http://arxiv.org/abs/0907.0894v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.0894v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.2561v1</id>
    <updated>2013-01-11T17:59:20Z</updated>
    <published>2013-01-11T17:59:20Z</published>
    <title>Modeling complex systems with adaptive networks</title>
    <summary>  Adaptive networks are a novel class of dynamical networks whose topologies
and states coevolve. Many real-world complex systems can be modeled as adaptive
networks, including social networks, transportation networks, neural networks
and biological networks. In this paper, we introduce fundamental concepts and
unique properties of adaptive networks through a brief, non-comprehensive
review of recent literature on mathematical/computational modeling and analysis
of such networks. We also report our recent work on several applications of
computational adaptive network modeling and analysis to real-world problems,
including temporal development of search and rescue operational networks,
automated rule discovery from empirical network evolution data, and cultural
integration in corporate merger.
</summary>
    <author>
      <name>Hiroki Sayama</name>
    </author>
    <author>
      <name>Irene Pestov</name>
    </author>
    <author>
      <name>Jeffrey Schmidt</name>
    </author>
    <author>
      <name>Benjamin James Bush</name>
    </author>
    <author>
      <name>Chun Wong</name>
    </author>
    <author>
      <name>Junichi Yamanoi</name>
    </author>
    <author>
      <name>Thilo Gross</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.camwa.2012.12.005</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.camwa.2012.12.005" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 11 figures, 3 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computers and Mathematics with Applications, 65, 1645-1664 (2013)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1301.2561v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.2561v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.0218v3</id>
    <updated>2014-06-27T14:26:07Z</updated>
    <published>2014-02-02T17:32:34Z</published>
    <title>Multiple percolation transitions in a configuration model of network of
  networks</title>
    <summary>  Recently much attention has been paid to the study of the robustness of
interdependent and multiplex networks and, in particular, networks of networks.
The robustness of interdependent networks can be evaluated by the size of a
mutually connected component when a fraction of nodes have been removed from
these networks. Here we characterize the emergence of the mutually connected
component in a network of networks in which every node of a network (layer)
$\alpha$ is connected with $q_{\alpha}$ randomly chosen replicas in some other
networks and is interdependent of these nodes with probability $r$. We find
that when the superdegrees $q_{\alpha}$ of different layers in the network of
networks are distributed heterogeneously, multiple percolation phase transition
can occur, and depending on the value of $r$ these transition are continuous or
discontinuous.
</summary>
    <author>
      <name>Ginestra Bianconi</name>
    </author>
    <author>
      <name>Sergey N. Dorogovtsev</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.89.062814</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.89.062814" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">(11 pages, 4 figures)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 89, 062814 (2014)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1402.0218v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.0218v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.3405v2</id>
    <updated>2015-04-29T16:01:05Z</updated>
    <published>2014-12-10T18:48:33Z</published>
    <title>Emergent Complex Network Geometry</title>
    <summary>  Networks are mathematical structures that are universally used to describe a
large variety of complex systems such as the brain or the Internet.
Characterizing the geometrical properties of these networks has become
increasingly relevant for routing problems, inference and data mining. In real
growing networks, topological, structural and geometrical properties emerge
spontaneously from their dynamical rules. Nevertheless we still miss a model in
which networks develop an emergent complex geometry. Here we show that a single
two parameter network model, the growing geometrical network, can generate
complex network geometries with non-trivial distribution of curvatures,
combining exponential growth and small-world properties with finite spectral
dimensionality. In one limit, the non-equilibrium dynamical rules of these
networks can generate scale-free networks with clustering and communities, in
another limit planar random geometries with non-trivial modularity. Finally we
find that these properties of the geometrical growing networks are present in a
large set of real networks describing biological, social and technological
systems.
</summary>
    <author>
      <name>Zhihao Wu</name>
    </author>
    <author>
      <name>Giulia Menichetti</name>
    </author>
    <author>
      <name>Christoph Rahmede</name>
    </author>
    <author>
      <name>Ginestra Bianconi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/srep10073</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/srep10073" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">(24 pages, 7 figures, 1 table)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Nature Scientific Reports 5, 10073 (2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1412.3405v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3405v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.0279v2</id>
    <updated>2010-10-07T20:31:25Z</updated>
    <published>2010-10-02T00:11:44Z</published>
    <title>Sandpile cascades on interacting tree-like networks</title>
    <summary>  The vulnerability of an isolated network to cascades is fundamentally
affected by its interactions with other networks. Motivated by failures
cascading among electrical grids, we study the Bak-Tang-Wiesenfeld sandpile
model on two sparsely-coupled random regular graphs. By approximating
avalanches (cascades) as a multi-type branching process and using a
generalization of Lagrange's expansion to multiple variables, we calculate the
distribution of avalanche sizes within each network. Due to coupling, large
avalanches in the individual networks are mitigated--in contrast to the
conclusion for a simpler model [36]. Yet when compared to uncoupled networks,
interdependent networks more frequently suffer avalanches that are large in
both networks. Thus sparse connections between networks stabilize them
individually but destabilize them jointly, as coupling introduces reservoirs
for extra load yet also inflicts new stresses. These results suggest that in
practice, to greedily mitigate large avalanches in one network, add connections
between networks; conversely, to mitigate avalanches that are large in both
networks, remove connections between networks. We also show that when only one
network receives load, the largest avalanches in the second network increase in
size and in frequency, an effect that is amplified with increased coupling
between networks and with increased disparity in total capacity. Our framework
is applicable to modular networks as well as to interacting networks and
provides building blocks for better prediction of cascading processes on
networks in general.
</summary>
    <author>
      <name>Charles D. Brummitt</name>
    </author>
    <author>
      <name>Raissa M. D'Souza</name>
    </author>
    <author>
      <name>Elizabeth A. Leicht</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages, 9 figures; corrected typo in Eq. 18 10/7/2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1010.0279v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.0279v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.6029v1</id>
    <updated>2014-07-22T20:32:46Z</updated>
    <published>2014-07-22T20:32:46Z</published>
    <title>A binary Hopfield network with $1/\log(n)$ information rate and
  applications to grid cell decoding</title>
    <summary>  A Hopfield network is an auto-associative, distributive model of neural
memory storage and retrieval. A form of error-correcting code, the Hopfield
network can learn a set of patterns as stable points of the network dynamic,
and retrieve them from noisy inputs -- thus Hopfield networks are their own
decoders. Unlike in coding theory, where the information rate of a good code
(in the Shannon sense) is finite but the cost of decoding does not play a role
in the rate, the information rate of Hopfield networks trained with
state-of-the-art learning algorithms is of the order ${\log(n)}/{n}$, a
quantity that tends to zero asymptotically with $n$, the number of neurons in
the network. For specially constructed networks, the best information rate
currently achieved is of order ${1}/{\sqrt{n}}$. In this work, we design simple
binary Hopfield networks that have asymptotically vanishing error rates at an
information rate of ${1}/{\log(n)}$. These networks can be added as the
decoders of any neural code with noisy neurons. As an example, we apply our
network to a binary neural decoder of the grid cell code to attain information
rate ${1}/{\log(n)}$.
</summary>
    <author>
      <name>Ila Fiete</name>
    </author>
    <author>
      <name>David J. Schwab</name>
    </author>
    <author>
      <name>Ngoc M. Tran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">extended abstract, 4 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.6029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.6029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.01543v2</id>
    <updated>2017-11-13T19:44:32Z</updated>
    <published>2016-12-05T21:04:17Z</published>
    <title>Towards the Limit of Network Quantization</title>
    <summary>  Network quantization is one of network compression techniques to reduce the
redundancy of deep neural networks. It reduces the number of distinct network
parameter values by quantization in order to save the storage for them. In this
paper, we design network quantization schemes that minimize the performance
loss due to quantization given a compression ratio constraint. We analyze the
quantitative relation of quantization errors to the neural network loss
function and identify that the Hessian-weighted distortion measure is locally
the right objective function for the optimization of network quantization. As a
result, Hessian-weighted k-means clustering is proposed for clustering network
parameters to quantize. When optimal variable-length binary codes, e.g.,
Huffman codes, are employed for further compression, we derive that the network
quantization problem can be related to the entropy-constrained scalar
quantization (ECSQ) problem in information theory and consequently propose two
solutions of ECSQ for network quantization, i.e., uniform quantization and an
iterative solution similar to Lloyd's algorithm. Finally, using the simple
uniform quantization followed by Huffman coding, we show from our experiments
that the compression ratios of 51.25, 22.17 and 40.65 are achievable for LeNet,
32-layer ResNet and AlexNet, respectively.
</summary>
    <author>
      <name>Yoojin Choi</name>
    </author>
    <author>
      <name>Mostafa El-Khamy</name>
    </author>
    <author>
      <name>Jungwon Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at ICLR 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.01543v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.01543v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.02475v2</id>
    <updated>2018-01-13T06:22:55Z</updated>
    <published>2018-01-08T15:12:54Z</published>
    <title>Ensemble One-dimensional Convolution Neural Networks for Skeleton-based
  Action Recognition</title>
    <summary>  In this paper, we proposed a effective but extensible residual
one-dimensional convolution neural network as base network, based on the this
network, we proposed four subnets to explore the features of skeleton sequences
from each aspect. Given a skeleton sequences, the spatial information are
encoded into the skeleton joints coordinate in a frame and the temporal
information are present by multiple frames. Limited by the skeleton sequence
representations, two-dimensional convolution neural network cannot be used
directly, we chose one-dimensional convolution layer as the basic layer. Each
sub network could extract discriminative features from different aspects. Our
first subnet is a two-stream network which could explore both temporal and
spatial information. The second is a body-parted network, which could gain
micro spatial features and macro temporal features. The third one is an
attention network, the main contribution of which is to focus the key frames
and feature channels which high related with the action classes in a skeleton
sequence. One frame-difference network, as the last subnet, mainly processes
the joints changes between the consecutive frames. Four subnets ensemble
together by late fusion, the key problem of ensemble method is each subnet
should have a certain performance and between the subnets, there are diversity
existing. Each subnet shares a wellperformance basenet and differences between
subnets guaranteed the diversity. Experimental results show that the ensemble
network gets a state-of-the-art performance on three widely used datasets.
</summary>
    <author>
      <name>Yangyang Xu</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">the title of Table 3 has something wrong and the expermient is not
  enough</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.02475v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.02475v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9709219v2</id>
    <updated>1997-11-09T14:52:27Z</updated>
    <published>1997-09-19T14:46:01Z</published>
    <title>Stability of the replica symmetric solution for the information conveyed
  by by a neural network</title>
    <summary>  The information that a pattern of firing in the output layer of a feedforward
network of threshold-linear neurons conveys about the network's inputs is
considered. A replica-symmetric solution is found to be stable for all but
small amounts of noise. The region of instability depends on the contribution
of the threshold and the sparseness: for distributed pattern distributions, the
unstable region extends to higher noise variances than for very sparse
distributions, for which it is almost nonexistant.
</summary>
    <author>
      <name>Simon Schultz</name>
    </author>
    <author>
      <name>Alessandro Treves</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.57.3302</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.57.3302" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, LaTeX, 5 figures. Also available at
  http://www.mrc-bbc.ox.ac.uk/~schultz/papers.html . Submitted to Phys. Rev. E
  Minor changes</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9709219v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9709219v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9808121v1</id>
    <updated>1998-08-12T05:41:33Z</updated>
    <published>1998-08-12T05:41:33Z</published>
    <title>Phase Transitions of an Oscillator Neural Network with a Standard Hebb
  Learning Rule</title>
    <summary>  Studies have been made on the phase transition phenomena of an oscillator
network model based on a standard Hebb learning rule like the Hopfield model.
The relative phase informations---the in-phase and anti-phase, can be embedded
in the network. By self-consistent signal-to-noise analysis (SCSNA), it was
found that the storage capacity is given by $\alpha_c = 0.042$, which is better
than that of Cook's model. However, the retrieval quality is worse. In
addition, an investigation was made into an acceleration effect caused by
asymmetry of the phase dynamics. Finally, it was numerically shown that the
storage capacity can be improved by modifying the shape of the coupling
function.
</summary>
    <author>
      <name>Toru Aonishi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Osaka Univ.</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.58.4865</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.58.4865" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9808121v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9808121v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0105008v2</id>
    <updated>2001-08-10T03:20:54Z</updated>
    <published>2001-05-01T06:34:27Z</published>
    <title>Storage Capacity of Two-dimensional Neural Networks</title>
    <summary>  We investigate the maximum number of embedded patterns in the two-dimensional
Hopfield model. The grand state energies of two specific network states,
namely, the energies of the pure-ferromagnetic state and the state of specific
one stored pattern are calculated exactly in terms of the correlation function
of the ferromagnetic Ising model. We also investigate the energy landscape
around them by computer simulations. Taking into account the qualitative
features of the phase diagrams obtained by Nishimori, Whyte and Sherrington
[Phys. Rev. E {\bf 51}, 3628 (1995)], we conclude that the network cannot
retrieve more than three patterns.
</summary>
    <author>
      <name>Shinsuke Koyama</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Hokkaido Univ.</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.65.016124</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.65.016124" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13pages, 7figures, revtex4</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0105008v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0105008v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0504722v1</id>
    <updated>2005-04-27T14:13:04Z</updated>
    <published>2005-04-27T14:13:04Z</published>
    <title>Canalizing Kauffman networks: non-ergodicity and its effect on their
  critical behavior</title>
    <summary>  Boolean Networks have been used to study numerous phenomena, including gene
regulation, neural networks, social interactions, and biological evolution.
Here, we propose a general method for determining the critical behavior of
Boolean systems built from arbitrary ensembles of Boolean functions. In
particular, we solve the critical condition for systems of units operating
according to canalizing functions and present strong numerical evidence that
our approach correctly predicts the phase transition from order to chaos in
such systems.
</summary>
    <author>
      <name>Andre A. Moreira</name>
    </author>
    <author>
      <name>Luis A. N. Amaral</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.94.218702</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.94.218702" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to be published in PRL</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0504722v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0504722v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.3214v1</id>
    <updated>2007-05-22T17:02:40Z</updated>
    <published>2007-05-22T17:02:40Z</published>
    <title>Stable Irregular Dynamics in Complex Neural Networks</title>
    <summary>  For infinitely large sparse networks of spiking neurons mean field theory
shows that a balanced state of highly irregular activity arises under various
conditions. Here we analytically investigate the microscopic irregular dynamics
in finite networks of arbitrary connectivity, keeping track of all individual
spike times. For delayed, purely inhibitory interactions we demonstrate that
the irregular dynamics is not chaotic but rather stable and convergent towards
periodic orbits. Moreover, every generic periodic orbit of these dynamical
systems is stable. These results highlight that chaotic and stable dynamics are
equally capable of generating irregular activity.
</summary>
    <author>
      <name>Sven Jahnke</name>
    </author>
    <author>
      <name>Raoul-Martin Memmesheimer</name>
    </author>
    <author>
      <name>Marc Timme</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.100.048102</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.100.048102" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0705.3214v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.3214v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.3475v1</id>
    <updated>2007-10-18T10:00:15Z</updated>
    <published>2007-10-18T10:00:15Z</published>
    <title>Can distributed delays perfectly stabilize dynamical networks?</title>
    <summary>  Signal transmission delays tend to destabilize dynamical networks leading to
oscillation, but their dispersion contributes oppositely toward stabilization.
We analyze an integro-differential equation that describes the collective
dynamics of a neural network with distributed signal delays. With the gamma
distributed delays less dispersed than exponential distribution, the system
exhibits reentrant phenomena, in which the stability is once lost but then
recovered as the mean delay is increased. With delays dispersed more highly
than exponential, the system never destabilizes.
</summary>
    <author>
      <name>Takahiro Omi</name>
    </author>
    <author>
      <name>Shigeru Shinomoto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.77.046214</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.77.046214" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4pages 5figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">PHYSICAL REVIEW E 77, 046214 (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.3475v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.3475v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.4173v1</id>
    <updated>2010-12-19T14:48:55Z</updated>
    <published>2010-12-19T14:48:55Z</published>
    <title>A Self-Organising Neural Network for Processing Data from Multiple
  Sensors</title>
    <summary>  This paper shows how a folded Markov chain network can be applied to the
problem of processing data from multiple sensors, with an emphasis on the
special case of 2 sensors. It is necessary to design the network so that it can
transform a high dimensional input vector into a posterior probability, for
which purpose the partitioned mixture distribution network is ideally suited.
The underlying theory is presented in detail, and a simple numerical simulation
is given that shows the emergence of ocular dominance stripes.
</summary>
    <author>
      <name>S P Luttrell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1012.4173v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.4173v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.1422v2</id>
    <updated>2013-09-30T16:13:32Z</updated>
    <published>2012-05-07T15:01:45Z</published>
    <title>Artificial Neural Network based on SQUIDs: demonstration of network
  training and operation</title>
    <summary>  We propose a scheme for the realization of artificial neural networks based
on Superconducting Quantum Interference Devices (SQUIDs). In order to
demonstrate the operation of this scheme we designed and successfully tested a
small network that implements a XOR gate and is trained by means of examples.
The proposed scheme can be particularly convenient as support for
superconducting applications such as detectors for astrophysics, high energy
experiments, medicine imaging and so on.
</summary>
    <author>
      <name>F. Chiarello</name>
    </author>
    <author>
      <name>P. Carelli</name>
    </author>
    <author>
      <name>M. G. Castellano</name>
    </author>
    <author>
      <name>G. Torrioli</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/0953-2048/26/12/125009</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/0953-2048/26/12/125009" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">F Chiarello et al 2013 Supercond. Sci. Technol. 26 125009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1205.1422v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.1422v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.supr-con" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.supr-con" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.5947v1</id>
    <updated>2014-06-23T15:34:54Z</updated>
    <published>2014-06-23T15:34:54Z</published>
    <title>Committees of deep feedforward networks trained with few data</title>
    <summary>  Deep convolutional neural networks are known to give good results on image
classification tasks. In this paper we present a method to improve the
classification result by combining multiple such networks in a committee. We
adopt the STL-10 dataset which has very few training examples and show that our
method can achieve results that are better than the state of the art. The
networks are trained layer-wise and no backpropagation is used. We also explore
the effects of dataset augmentation by mirroring, rotation, and scaling.
</summary>
    <author>
      <name>Bogdan Miclut</name>
    </author>
    <author>
      <name>Thomas Kaester</name>
    </author>
    <author>
      <name>Thomas Martinetz</name>
    </author>
    <author>
      <name>Erhardt Barth</name>
    </author>
    <link href="http://arxiv.org/abs/1406.5947v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.5947v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.7610v3</id>
    <updated>2015-03-05T21:55:38Z</updated>
    <published>2014-11-27T14:22:36Z</published>
    <title>Learning Stochastic Recurrent Networks</title>
    <summary>  Leveraging advances in variational inference, we propose to enhance recurrent
neural networks with latent variables, resulting in Stochastic Recurrent
Networks (STORNs). The model i) can be trained with stochastic gradient
methods, ii) allows structured and multi-modal conditionals at each time step,
iii) features a reliable estimator of the marginal likelihood and iv) is a
generalisation of deterministic recurrent neural networks. We evaluate the
method on four polyphonic musical data sets and motion capture data.
</summary>
    <author>
      <name>Justin Bayer</name>
    </author>
    <author>
      <name>Christian Osendorfer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to conference track of ICLR 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.7610v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.7610v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.0268v1</id>
    <updated>2013-03-01T20:21:32Z</updated>
    <published>2013-03-01T20:21:32Z</published>
    <title>Maximal Information Divergence from Statistical Models defined by Neural
  Networks</title>
    <summary>  We review recent results about the maximal values of the Kullback-Leibler
information divergence from statistical models defined by neural networks,
including naive Bayes models, restricted Boltzmann machines, deep belief
networks, and various classes of exponential families. We illustrate approaches
to compute the maximal divergence from a given model starting from simple sub-
or super-models. We give a new result for deep and narrow belief networks with
finite-valued units.
</summary>
    <author>
      <name>Guido Montufar</name>
    </author>
    <author>
      <name>Johannes Rauh</name>
    </author>
    <author>
      <name>Nihat Ay</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-40020-9_85</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-40020-9_85" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Geometric science of information : first international conference,
  GSI 2013, Paris, France, August 28-30, 2013. Proceedings / F. Nielsen...
  (eds.). Springer, 2013. - P. 759-766</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1303.0268v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.0268v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62E17, 94A17, 60E05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.0115v1</id>
    <updated>2009-05-30T21:44:28Z</updated>
    <published>2009-05-30T21:44:28Z</published>
    <title>A "Cellular Neuronal" Approach to Optimization Problems</title>
    <summary>  The Hopfield-Tank (1985) recurrent neural network architecture for the
Traveling Salesman Problem is generalized to a fully interconnected "cellular"
neural network of regular oscillators. Tours are defined by synchronization
patterns, allowing the simultaneous representation of all cyclic permutations
of a given tour. The network converges to local optima some of which correspond
to shortest-distance tours, as can be shown analytically in a stationary phase
approximation. Simulated annealing is required for global optimization, but the
stochastic element might be replaced by chaotic intermittency in a further
generalization of the architecture to a network of chaotic oscillators.
</summary>
    <author>
      <name>Gregory S. Duane</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/1.3184829</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/1.3184829" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">-2nd revised version submitted to Chaos (original version submitted
  6/07)</arxiv:comment>
    <link href="http://arxiv.org/abs/0906.0115v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.0115v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.PS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.1090v1</id>
    <updated>2013-11-05T15:33:30Z</updated>
    <published>2013-11-05T15:33:30Z</published>
    <title>Polyhedrons and Perceptrons Are Functionally Equivalent</title>
    <summary>  Mathematical definitions of polyhedrons and perceptron networks are
discussed. The formalization of polyhedrons is done in a rather traditional
way. For networks, previously proposed systems are developed. Perceptron
networks in disjunctive normal form (DNF) and conjunctive normal forms (CNF)
are introduced. The main theme is that single output perceptron neural networks
and characteristic functions of polyhedrons are one and the same class of
functions. A rigorous formulation and proof that three layers suffice is
obtained. The various constructions and results are among several steps
required for algorithms that replace incremental and statistical learning with
more efficient, direct and exact geometric methods for calculation of
perceptron architecture and weights.
</summary>
    <author>
      <name>Daniel Crespin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 0 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.1090v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.1090v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.00941v2</id>
    <updated>2015-04-07T22:39:18Z</updated>
    <published>2015-04-03T21:22:52Z</published>
    <title>A Simple Way to Initialize Recurrent Networks of Rectified Linear Units</title>
    <summary>  Learning long term dependencies in recurrent networks is difficult due to
vanishing and exploding gradients. To overcome this difficulty, researchers
have developed sophisticated optimization techniques and network architectures.
In this paper, we propose a simpler solution that use recurrent neural networks
composed of rectified linear units. Key to our solution is the use of the
identity matrix or its scaled version to initialize the recurrent weight
matrix. We find that our solution is comparable to LSTM on our four benchmarks:
two toy problems involving long-range temporal structures, a large language
modeling problem and a benchmark speech recognition problem.
</summary>
    <author>
      <name>Quoc V. Le</name>
    </author>
    <author>
      <name>Navdeep Jaitly</name>
    </author>
    <author>
      <name>Geoffrey E. Hinton</name>
    </author>
    <link href="http://arxiv.org/abs/1504.00941v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.00941v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.07576v3</id>
    <updated>2016-06-03T10:54:16Z</updated>
    <published>2016-02-24T16:17:15Z</published>
    <title>Group Equivariant Convolutional Networks</title>
    <summary>  We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a
natural generalization of convolutional neural networks that reduces sample
complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of
layer that enjoys a substantially higher degree of weight sharing than regular
convolution layers. G-convolutions increase the expressive capacity of the
network without increasing the number of parameters. Group convolution layers
are easy to use and can be implemented with negligible computational overhead
for discrete groups generated by translations, reflections and rotations.
G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST.
</summary>
    <author>
      <name>Taco S. Cohen</name>
    </author>
    <author>
      <name>Max Welling</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on Machine Learning
  (ICML), 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.07576v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.07576v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.02003v1</id>
    <updated>2016-03-07T11:04:17Z</updated>
    <published>2016-03-07T11:04:17Z</published>
    <title>From A to Z: Supervised Transfer of Style and Content Using Deep Neural
  Network Generators</title>
    <summary>  We propose a new neural network architecture for solving single-image
analogies - the generation of an entire set of stylistically similar images
from just a single input image. Solving this problem requires separating image
style from content. Our network is a modified variational autoencoder (VAE)
that supports supervised training of single-image analogies and in-network
evaluation of outputs with a structured similarity objective that captures
pixel covariances. On the challenging task of generating a 62-letter font from
a single example letter we produce images with 22.4% lower dissimilarity to the
ground truth than state-of-the-art.
</summary>
    <author>
      <name>Paul Upchurch</name>
    </author>
    <author>
      <name>Noah Snavely</name>
    </author>
    <author>
      <name>Kavita Bala</name>
    </author>
    <link href="http://arxiv.org/abs/1603.02003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.02003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00734v1</id>
    <updated>2016-04-04T03:58:31Z</updated>
    <published>2016-04-04T03:58:31Z</published>
    <title>Capturing Semantic Similarity for Entity Linking with Convolutional
  Neural Networks</title>
    <summary>  A key challenge in entity linking is making effective use of contextual
information to disambiguate mentions that might refer to different entities in
different contexts. We present a model that uses convolutional neural networks
to capture semantic correspondence between a mention's context and a proposed
target entity. These convolutional networks operate at multiple granularities
to exploit various kinds of topic information, and their rich parameterization
gives them the capacity to learn which n-grams characterize different topics.
We combine these networks with a sparse linear model to achieve
state-of-the-art performance on multiple entity linking datasets, outperforming
the prior systems of Durrett and Klein (2014) and Nguyen et al. (2014).
</summary>
    <author>
      <name>Matthew Francis-Landau</name>
    </author>
    <author>
      <name>Greg Durrett</name>
    </author>
    <author>
      <name>Dan Klein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at NAACL 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.00734v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00734v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.01178v1</id>
    <updated>2016-04-05T08:50:27Z</updated>
    <published>2016-04-05T08:50:27Z</published>
    <title>Modeling Relational Information in Question-Answer Pairs with
  Convolutional Neural Networks</title>
    <summary>  In this paper, we propose convolutional neural networks for learning an
optimal representation of question and answer sentences. Their main aspect is
the use of relational information given by the matches between words from the
two members of the pair. The matches are encoded as embeddings with additional
parameters (dimensions), which are tuned by the network. These allows for
better capturing interactions between questions and answers, resulting in a
significant boost in accuracy. We test our models on two widely used answer
sentence selection benchmarks. The results clearly show the effectiveness of
our relational information, which allows our relatively simple network to
approach the state of the art.
</summary>
    <author>
      <name>Aliaksei Severyn</name>
    </author>
    <author>
      <name>Alessandro Moschitti</name>
    </author>
    <link href="http://arxiv.org/abs/1604.01178v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.01178v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.02085v2</id>
    <updated>2016-06-28T06:10:29Z</updated>
    <published>2016-04-07T17:31:13Z</published>
    <title>A robust autoassociative memory with coupled networks of Kuramoto-type
  oscillators</title>
    <summary>  Uncertain recognition success, unfavorable scaling of connection complexity
or dependence on complex external input impair the usefulness of current
oscillatory neural networks for pattern recognition or restrict technical
realizations to small networks. We propose a new network architecture of
coupled oscillators for pattern recognition which shows none of the mentioned
aws. Furthermore we illustrate the recognition process with simulation results
and analyze the new dynamics analytically: Possible output patterns are
isolated attractors of the system. Additionally, simple criteria for
recognition success are derived from a lower bound on the basins of attraction.
</summary>
    <author>
      <name>Daniel Heger</name>
    </author>
    <author>
      <name>Katharina Krischer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.94.022309</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.94.022309" rel="related"/>
    <link href="http://arxiv.org/abs/1604.02085v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.02085v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00540v1</id>
    <updated>2016-06-02T05:39:54Z</updated>
    <published>2016-06-02T05:39:54Z</published>
    <title>Multi-pretrained Deep Neural Network</title>
    <summary>  Pretraining is widely used in deep neutral network and one of the most famous
pretraining models is Deep Belief Network (DBN). The optimization formulas are
different during the pretraining process for different pretraining models. In
this paper, we pretrained deep neutral network by different pretraining models
and hence investigated the difference between DBN and Stacked Denoising
Autoencoder (SDA) when used as pretraining model. The experimental results show
that DBN get a better initial model. However the model converges to a
relatively worse model after the finetuning process. Yet after pretrained by
SDA for the second time the model converges to a better model if finetuned.
</summary>
    <author>
      <name>Zhen Hu</name>
    </author>
    <author>
      <name>Zhuyin Xue</name>
    </author>
    <author>
      <name>Tong Cui</name>
    </author>
    <author>
      <name>Shiqiang Zong</name>
    </author>
    <author>
      <name>Chenglong He</name>
    </author>
    <link href="http://arxiv.org/abs/1606.00540v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00540v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03674v2</id>
    <updated>2017-03-06T13:34:29Z</updated>
    <published>2016-06-12T07:22:58Z</published>
    <title>Critical Echo State Networks that Anticipate Input using Morphable
  Transfer Functions</title>
    <summary>  The paper investigates a new type of truly critical echo state networks where
individual transfer functions for every neuron can be modified to anticipate
the expected next input. Deviations from expected input are only forgotten
slowly in power law fashion. The paper outlines the theory, numerically
analyzes a one neuron model network and finally discusses technical and also
biological implications of this type of approach.
</summary>
    <author>
      <name>Norbert Michael Mayer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14th International Symposium on Neural Networks (ISNN), Sapporo,
  Hakodate, Japan, June 21st - 26th 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.03674v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03674v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.00760v2</id>
    <updated>2016-11-08T14:22:39Z</updated>
    <published>2016-08-02T10:41:40Z</published>
    <title>Dynamics of complex-valued fractional-order neural networks</title>
    <summary>  The dynamics of complex-valued fractional-order neuronal networks are
investigated, focusing on stability, instability and Hopf bifurcations.
Sufficient conditions for the asymptotic stability and instability of a steady
state of the network are derived, based on the complex system parameters and
the fractional order of the system, considering simplified neuronal
connectivity structures (hub and ring). In some specific cases, it is possible
to identify the critical values of the fractional order for which Hopf
bifurcations may occur. Numerical simulations are presented to illustrate the
theoretical findings and to investigate the stability of the limit cycles which
appear due to Hopf bifurcations.
</summary>
    <author>
      <name>Eva Kaslik</name>
    </author>
    <author>
      <name>Ileana Rodica Radulescu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neunet.2017.02.011</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neunet.2017.02.011" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Networks, vol 89: 39-49, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1608.00760v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00760v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.03946v1</id>
    <updated>2016-10-13T06:42:51Z</updated>
    <published>2016-10-13T06:42:51Z</published>
    <title>A Neural Network for Coordination Boundary Prediction</title>
    <summary>  We propose a neural-network based model for coordination boundary prediction.
The network is designed to incorporate two signals: the similarity between
conjuncts and the observation that replacing the whole coordination phrase with
a conjunct tends to produce a coherent sentences. The modeling makes use of
several LSTM networks. The model is trained solely on conjunction annotations
in a Treebank, without using external resources. We show improvements on
predicting coordination boundaries on the PTB compared to two state-of-the-art
parsers; as well as improvement over previous coordination boundary prediction
systems on the Genia corpus.
</summary>
    <author>
      <name>Jessica Ficler</name>
    </author>
    <author>
      <name>Yoav Goldberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.03946v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.03946v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.09353v1</id>
    <updated>2016-10-28T19:10:52Z</updated>
    <published>2016-10-28T19:10:52Z</published>
    <title>Eigenvalue spectra of large correlated random matrices</title>
    <summary>  Using the diagrammatic method, we derive a set of self-consistent equations
that describe eigenvalue distributions of large correlated asymmetric random
matrices. The matrix elements can have different variances and be correlated
with each other. The analytical results are confirmed by numerical simulations.
The results have implications for the dynamics of neural and other biological
networks where plasticity induces correlations in the connection strengths
within the network. We find that the presence of correlations can have a major
impact on network stability.
</summary>
    <author>
      <name>Alexander Kuczala</name>
    </author>
    <author>
      <name>Tatyana O. Sharpee</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.94.050101</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.94.050101" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.09353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.09353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.04390v1</id>
    <updated>2017-01-16T18:37:30Z</updated>
    <published>2017-01-16T18:37:30Z</published>
    <title>Identifying polymer states by machine learning</title>
    <summary>  The ability of a feed-forward neural network to learn and classify different
states of polymer configurations is systematically explored. Performing
numerical experiments, we find that a simple network model can, after adequate
training, recognize multiple structures, including gas-like coil, liquid-like
globular, and crystalline anti-Mackay and Mackay structures. The network can be
trained to identify the transition points between various states, which compare
well with those identified by independent specific-heat calculations. Our study
demonstrates that neural network provides an unconventional tool to study the
phase transitions in polymeric systems.
</summary>
    <author>
      <name>Qianshi Wei</name>
    </author>
    <author>
      <name>Roger G. Melko</name>
    </author>
    <author>
      <name>Jeff Z. Y. Chen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.95.032504</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.95.032504" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 95, 032504 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1701.04390v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.04390v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.soft" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.soft" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.10713v1</id>
    <updated>2017-03-30T23:38:20Z</updated>
    <published>2017-03-30T23:38:20Z</published>
    <title>Detecting Resting-state Neural Connectivity Using Dynamic Network
  Analysis on Multiband fMRI Data</title>
    <summary>  This paper describes an approach of using dynamic Structural Equation
Modeling (SEM) analysis to estimate the connectivity networks from
resting-state fMRI data measured by a multiband EPI sequence. Two structural
equation models were estimated at each voxel with respect to the sensory-motor
network and default-mode network. The resulting connectivity maps indicate that
supplementary motor area has significant connections to left/right primary
motor areas, and medial prefrontal cortex link significantly with posterior
cingulate cortex and inferior parietal lobules. The results imply that high
temporal resolution images obtained with multiband fMRI data can provide
dynamic and directional information on the neural connectivity.
</summary>
    <author>
      <name>Jiancheng Zhuang</name>
    </author>
    <link href="http://arxiv.org/abs/1703.10713v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.10713v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.05396v1</id>
    <updated>2017-04-18T15:33:10Z</updated>
    <published>2017-04-18T15:33:10Z</published>
    <title>A Study of Deep Learning Robustness Against Computation Failures</title>
    <summary>  For many types of integrated circuits, accepting larger failure rates in
computations can be used to improve energy efficiency. We study the performance
of faulty implementations of certain deep neural networks based on pessimistic
and optimistic models of the effect of hardware faults. After identifying the
impact of hyperparameters such as the number of layers on robustness, we study
the ability of the network to compensate for computational failures through an
increase of the network size. We show that some networks can achieve equivalent
performance under faulty implementations, and quantify the required increase in
computational complexity.
</summary>
    <author>
      <name>Jean-Charles Vialatte</name>
    </author>
    <author>
      <name>Fran√ßois Leduc-Primeau</name>
    </author>
    <link href="http://arxiv.org/abs/1704.05396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.05396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.05796v1</id>
    <updated>2017-06-19T06:11:17Z</updated>
    <published>2017-06-19T06:11:17Z</published>
    <title>Distributed synaptic weights in a LIF neural network and learning rules</title>
    <summary>  Leaky integrate-and-fire (LIF) models are mean-field limits, with a large
number of neurons, used to describe neural networks. We consider inhomogeneous
networks structured by a connec-tivity parameter (strengths of the synaptic
weights) with the effect of processing the input current with different
intensities. We first study the properties of the network activity depending on
the distribution of synaptic weights and in particular its discrimination
capacity. Then, we consider simple learning rules and determine the synaptic
weight distribution it generates. We outline the role of noise as a selection
principle and the capacity to memorized a learned signal.
</summary>
    <author>
      <name>Beno√Æt Perthame</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MAMBA, LJLL</arxiv:affiliation>
    </author>
    <author>
      <name>Delphine Salort</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LCQB</arxiv:affiliation>
    </author>
    <author>
      <name>Gilles Wainrib</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DI-ENS</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physd.2017.05.005</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physd.2017.05.005" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Physica D: Nonlinear Phenomena, Elsevier, 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.05796v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.05796v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08032v1</id>
    <updated>2017-06-25T04:05:09Z</updated>
    <published>2017-06-25T04:05:09Z</published>
    <title>A Deep Neural Architecture for Sentence-level Sentiment Classification
  in Twitter Social Networking</title>
    <summary>  This paper introduces a novel deep learning framework including a
lexicon-based approach for sentence-level prediction of sentiment label
distribution. We propose to first apply semantic rules and then use a Deep
Convolutional Neural Network (DeepCNN) for character-level embeddings in order
to increase information for word-level embedding. After that, a Bidirectional
Long Short-Term Memory Network (Bi-LSTM) produces a sentence-wide feature
representation from the word-level embedding. We evaluate our approach on three
Twitter sentiment classification datasets. Experimental results show that our
model can improve the classification accuracy of sentence-level sentiment
analysis in Twitter social networking.
</summary>
    <author>
      <name>Huy Nguyen</name>
    </author>
    <author>
      <name>Minh-Le Nguyen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PACLING Conference 2017, 6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.08032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.01555v1</id>
    <updated>2017-07-05T19:37:23Z</updated>
    <published>2017-07-05T19:37:23Z</published>
    <title>A Deep Network with Visual Text Composition Behavior</title>
    <summary>  While natural languages are compositional, how state-of-the-art neural models
achieve compositionality is still unclear. We propose a deep network, which not
only achieves competitive accuracy for text classification, but also exhibits
compositional behavior. That is, while creating hierarchical representations of
a piece of text, such as a sentence, the lower layers of the network distribute
their layer-specific attention weights to individual words. In contrast, the
higher layers compose meaningful phrases and clauses, whose lengths increase as
the networks get deeper until fully composing the sentence.
</summary>
    <author>
      <name>Hongyu Guo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted to ACL2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.01555v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.01555v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.02792v1</id>
    <updated>2017-09-08T17:50:18Z</updated>
    <published>2017-09-08T17:50:18Z</published>
    <title>Classifying vortex wakes using neural networks</title>
    <summary>  Unsteady flows contain information about the objects creating them. Aquatic
organisms offer intriguing paradigms for extracting flow information using
local sensory measurements. In contrast, classical methods for flow analysis
require global knowledge of the flow field. Here, we train neural networks to
classify flow patterns using local vorticity measurements. Specifically, we
consider vortex wakes behind an oscillating airfoil and we evaluate the
accuracy of the network in distinguishing between three wake types, 2S, 2P+2S
and 2P+4S. The network uncovers the salient features of each wake type.
</summary>
    <author>
      <name>Brendan Colvert</name>
    </author>
    <author>
      <name>Mohamad Alsalman</name>
    </author>
    <author>
      <name>Eva Kanso</name>
    </author>
    <link href="http://arxiv.org/abs/1709.02792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.02792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.02956v1</id>
    <updated>2017-09-09T14:23:50Z</updated>
    <published>2017-09-09T14:23:50Z</published>
    <title>Deep Residual Networks and Weight Initialization</title>
    <summary>  Residual Network (ResNet) is the state-of-the-art architecture that realizes
successful training of really deep neural network. It is also known that good
weight initialization of neural network avoids problem of vanishing/exploding
gradients. In this paper, simplified models of ResNets are analyzed. We argue
that goodness of ResNet is correlated with the fact that ResNets are relatively
insensitive to choice of initial weights. We also demonstrate how batch
normalization improves backpropagation of deep ResNets without tuning initial
values of weights.
</summary>
    <author>
      <name>Masato Taki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.02956v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.02956v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.10248v2</id>
    <updated>2017-10-30T16:03:48Z</updated>
    <published>2017-10-27T17:26:57Z</published>
    <title>Tensor network language model</title>
    <summary>  We propose a new statistical model suitable for machine learning of systems
with long distance correlations such as natural languages. The model is based
on directed acyclic graph decorated by multi-linear tensor maps in the vertices
and vector spaces in the edges, called tensor network. Such tensor networks
have been previously employed for effective numerical computation of the
renormalization group flow on the space of effective quantum field theories and
lattice models of statistical mechanics. We provide explicit algebro-geometric
analysis of the parameter moduli space for tree graphs, discuss model
properties and applications such as statistical translation.
</summary>
    <author>
      <name>Vasily Pestun</name>
    </author>
    <author>
      <name>Yiannis Vlassopoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.10248v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.10248v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.04679v1</id>
    <updated>2017-11-13T16:17:45Z</updated>
    <published>2017-11-13T16:17:45Z</published>
    <title>Attention-based Information Fusion using Multi-Encoder-Decoder Recurrent
  Neural Networks</title>
    <summary>  With the rising number of interconnected devices and sensors, modeling
distributed sensor networks is of increasing interest. Recurrent neural
networks (RNN) are considered particularly well suited for modeling sensory and
streaming data. When predicting future behavior, incorporating information from
neighboring sensor stations is often beneficial. We propose a new RNN based
architecture for context specific information fusion across multiple spatially
distributed sensor stations. Hereby, latent representations of multiple local
models, each modeling one sensor station, are jointed and weighted, according
to their importance for the prediction. The particular importance is assessed
depending on the current context using a separate attention function. We
demonstrate the effectiveness of our model on three different real-world sensor
network datasets.
</summary>
    <author>
      <name>Stephan Baier</name>
    </author>
    <author>
      <name>Sigurd Spieckermann</name>
    </author>
    <author>
      <name>Volker Tresp</name>
    </author>
    <link href="http://arxiv.org/abs/1711.04679v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.04679v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.10746v1</id>
    <updated>2017-11-29T09:48:06Z</updated>
    <published>2017-11-29T09:48:06Z</published>
    <title>RoboJam: A Musical Mixture Density Network for Collaborative Touchscreen
  Interaction</title>
    <summary>  RoboJam is a machine-learning system for generating music that assists users
of a touchscreen music app by performing responses to their short
improvisations. This system uses a recurrent artificial neural network to
generate sequences of touchscreen interactions and absolute timings, rather
than high-level musical notes. To accomplish this, RoboJam's network uses a
mixture density layer to predict appropriate touch interaction locations in
space and time. In this paper, we describe the design and implementation of
RoboJam's network and how it has been integrated into a touchscreen music app.
A preliminary evaluation analyses the system in terms of training, musical
generation and user interaction.
</summary>
    <author>
      <name>Charles P. Martin</name>
    </author>
    <author>
      <name>Jim Torresen</name>
    </author>
    <link href="http://arxiv.org/abs/1711.10746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.10746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03620v1</id>
    <updated>2018-02-10T17:01:08Z</updated>
    <published>2018-02-10T17:01:08Z</published>
    <title>Optimal approximation of continuous functions by very deep ReLU networks</title>
    <summary>  We prove that deep ReLU neural networks with conventional fully-connected
architectures with $W$ weights can approximate continuous $\nu$-variate
functions $f$ with uniform error not exceeding $a_\nu\omega_f(c_\nu
W^{-2/\nu}),$ where $\omega_f$ is the modulus of continuity of $f$ and $a_\nu,
c_\nu$ are some $\nu$-dependent constants. This bound is tight. Our
construction is inherently deep and nonlinear: the obtained approximation rate
cannot be achieved by networks with fewer than $\Omega(W/\ln W)$ layers or by
networks with weights continuously depending on $f$.
</summary>
    <author>
      <name>Dmitry Yarotsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.03620v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03620v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0201433v1</id>
    <updated>2002-01-24T01:03:25Z</updated>
    <published>2002-01-24T01:03:25Z</published>
    <title>Exact solutions of epidemic models on networks</title>
    <summary>  The study of social networks, and in particular the spread of disease on
networks, has attracted considerable recent attention in the physics community.
In this paper, we show that a large class of standard epidemiological models,
the so-called susceptible/infective/removed models, and many of their
generalizations, can be solved exactly on a wide variety of networks. Solutions
are possible for cases with heterogeneous or correlated probabilities of
transmission, cases incorporating vaccination, and cases in which the network
has complex structure of various kinds. We confirm the correctness of our
solutions by comparison with computer simulations of epidemics propagating on
the corresponding networks.
</summary>
    <author>
      <name>M. E. J. Newman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0201433v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0201433v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0306512v1</id>
    <updated>2003-06-19T18:48:27Z</updated>
    <published>2003-06-19T18:48:27Z</published>
    <title>Topological Speed Limits to Network Synchronization</title>
    <summary>  We study collective synchronization of pulse-coupled oscillators interacting
on asymmetric random networks. We demonstrate that random matrix theory can be
used to accurately predict the speed of synchronization in such networks in
dependence on the dynamical and network parameters. Furthermore, we show that
the speed of synchronization is limited by the network connectivity and stays
finite, even if the coupling strength becomes infinite. In addition, our
results indicate that synchrony is robust under structural perturbations of the
network dynamics.
</summary>
    <author>
      <name>Marc Timme</name>
    </author>
    <author>
      <name>Fred Wolf</name>
    </author>
    <author>
      <name>Theo Geisel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.92.074101</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.92.074101" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. Lett. 92, 074101 (2004)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0306512v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0306512v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0310426v1</id>
    <updated>2003-10-17T15:36:19Z</updated>
    <published>2003-10-17T15:36:19Z</published>
    <title>Spatial Small Worlds: New Geographic Patterns for an Information Economy</title>
    <summary>  Networks are structures that pervade many natural and man-made phenomena.
Recent findings have characterized many networks as not random structures, but
as efficent complex formations. Current research has examined complex networks
as largely non-spatial phenomena. Location, distance, and geograhpy, though,
are vital aspects of a wide variety of networks. This paper will examine the
United State's portion of the Internet's infrastructure as a complex network
and what role distance and geography play in its formation. From these findings
implications will be drawn on the economic, political, and national security
impacts of network formation and evolution in an information economy.
</summary>
    <author>
      <name>Sean P. Gorman</name>
    </author>
    <author>
      <name>Rajendra Kulkarni</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">55 pages, 11 figures, forthcoming in Environment and Planning B</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0310426v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0310426v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0311333v1</id>
    <updated>2003-11-14T03:36:42Z</updated>
    <published>2003-11-14T03:36:42Z</published>
    <title>Network connection strengths: Another power-law?</title>
    <summary>  It has been discovered recently that many social, biological and ecological
systems have the so-called small-world and scale-free features, which has
provoked new research interest in the studies of various complex networks. Yet,
most network models studied thus far are binary, with the linking strengths
being either 0 or 1, while which are best described by weighted-linking
networks, in which the vertices interact with each other with varying
strengths. Here we found that the distribution of connection strengths of
scientific collaboration networks decays also in a power-law form and we
conjecture that all weighted-linking networks of this type follow the same
distribution.
</summary>
    <author>
      <name>Chunguang Li</name>
    </author>
    <author>
      <name>Guanrong Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0311333v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0311333v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0312537v1</id>
    <updated>2003-12-19T16:29:08Z</updated>
    <published>2003-12-19T16:29:08Z</published>
    <title>Self-organized critical network dynamics</title>
    <summary>  We propose a simple model that aims at describing, in a stylized manner, how
local breakdowns due unbalances or congestion propagate in real dynamical
networks. The model converges to a self-organized critical stationary state in
which the network shapes itself as a consequence of avalanches of rewiring
processes. Depending on the model's specification, we obtain either single
scale or scale-free networks. We characterize in detail the relation between
the statistical properties of the network and the nature of the critical state,
by computing the critical exponents. The model also displays a non-trivial,
sudden, collapse to a complete network.
</summary>
    <author>
      <name>Ginestra Bianconi</name>
    </author>
    <author>
      <name>Matteo Marsili</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0312537v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0312537v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0403719v1</id>
    <updated>2004-03-30T02:37:11Z</updated>
    <published>2004-03-30T02:37:11Z</published>
    <title>Scale-free trees: the skeletons of complex networks</title>
    <summary>  We investigate the properties of the spanning trees of various real-world and
model networks. The spanning tree representing the communication kernel of the
original network is determined by maximizing total weight of edges, whose
weights are given by the edge betweenness centralities. We find that a
scale-free tree and shortcuts organize a complex network. The spanning tree
shows robust betweenness centrality distribution that was observed in
scale-free tree models. It turns out that the shortcut distribution
characterizes the properties of original network, such as the clustering
coefficient and the classification of networks by the betweenness centrality
distribution.
</summary>
    <author>
      <name>Dong-Hee Kim</name>
    </author>
    <author>
      <name>Jae Dong Noh</name>
    </author>
    <author>
      <name>Hawoong Jeong</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.70.046126</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.70.046126" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 70, 046126 (2004).</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0403719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0403719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0410546v1</id>
    <updated>2004-10-21T14:45:42Z</updated>
    <published>2004-10-21T14:45:42Z</published>
    <title>On the properties of cycles of simple Boolean networks</title>
    <summary>  We study two types of simple Boolean networks, namely two loops with a
cross-link and one loop with an additional internal link. Such networks occur
as relevant components of critical K=2 Kauffman networks. We determine mostly
analytically the numbers and lengths of cycles of these networks and find many
of the features that have been observed in Kauffman networks. In particular,
the mean number and length of cycles can diverge faster than any power law.
</summary>
    <author>
      <name>V. Kaufman</name>
    </author>
    <author>
      <name>B. Drossel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1140/epjb/e2005-00034-6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1140/epjb/e2005-00034-6" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 8 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Eur. Phys. J. B 43(1), 115-124 (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0410546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0410546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0512079v2</id>
    <updated>2007-03-23T06:21:47Z</updated>
    <published>2005-12-05T09:17:02Z</published>
    <title>Optimizing synchronizability of networks</title>
    <summary>  In this paper, we investigate the factors that affect the synchronization of
coupled oscillators on networks. By using the edge-intercrossing method, we
keep the degree distribution unchanged to see other statistical properties'
effects on network's synchronizability. By optimizing the eigenratio $R$ of the
coupling matrix with \textit{Memory Tabu Search} (MTS), we observe that a
network with lower degree of clustering, without modular structure and
displaying disassortative connecting pattern may be easy to synchronize.
Moreover, the optimal network contains fewer small-size loops. The optimization
process on scale-free network strongly suggests that the heterogeneity plays
the main role in determining the synchronizability.
</summary>
    <author>
      <name>Bing Wang</name>
    </author>
    <author>
      <name>Huanwen Tang</name>
    </author>
    <author>
      <name>Tao Zhou</name>
    </author>
    <author>
      <name>Zhilong Xiu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0512079v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0512079v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0603693v1</id>
    <updated>2006-03-26T01:54:20Z</updated>
    <published>2006-03-26T01:54:20Z</published>
    <title>Ising model on two connected Barabasi-Albert networks</title>
    <summary>  We investigate analytically the behavior of Ising model on two connected
Barabasi-Albert networks. Depending on relative ordering of both networks there
are two possible phases corresponding to parallel or antiparallel alingment of
spins in both networks. A difference between critical temperatures of both
phases disappears in the limit of vanishing inter-network coupling for
identical networks. The analytic predictions are confirmed by numerical
simulations.
</summary>
    <author>
      <name>Krzysztof Suchecki</name>
    </author>
    <author>
      <name>Janusz A. Holyst</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.74.011122</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.74.011122" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages including 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0603693v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0603693v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0604036v1</id>
    <updated>2006-04-03T08:37:32Z</updated>
    <published>2006-04-03T08:37:32Z</published>
    <title>Networks and Our Limited Information Horizon</title>
    <summary>  In this paper we quantify our limited information horizon, by measuring the
information necessary to locate specific nodes in a network. To investigate
different ways to overcome this horizon, and the interplay between
communication and topology in social networks, we let agents communicate in a
model society. Thereby they build a perception of the network that they can use
to create strategic links to improve their standing in the network. We observe
a narrow distribution of links when the communication is low and a network with
a broad distribution of links when the communication is high.
</summary>
    <author>
      <name>M. Rosvall</name>
    </author>
    <author>
      <name>K. Sneppen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1142/S0218127407018580</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1142/S0218127407018580" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages and 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0604036v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0604036v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0607017v2</id>
    <updated>2006-07-08T19:03:08Z</updated>
    <published>2006-07-01T20:19:15Z</published>
    <title>Optimal routing on complex networks</title>
    <summary>  We present a novel heuristic algorithm for routing optimization on complex
networks. Previously proposed routing optimization algorithms aim at avoiding
or reducing link overload. Our algorithm balances traffic on a network by
minimizing the maximum node betweenness with as little path lengthening as
possible, thus being useful in cases when networks are jamming due to queuing
overload. By using the resulting routing table, a network can sustain
significantly higher traffic without jamming than in the case of traditional
shortest path routing.
</summary>
    <author>
      <name>Bogdan Danila</name>
    </author>
    <author>
      <name>Yong Yu</name>
    </author>
    <author>
      <name>John A. Marsh</name>
    </author>
    <author>
      <name>Kevin E. Bassler</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.74.046106</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.74.046106" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys Rev E 74, 046106 (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0607017v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0607017v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0611735v2</id>
    <updated>2008-07-21T09:01:16Z</updated>
    <published>2006-11-29T09:39:27Z</published>
    <title>Random matrix analysis of network Laplacians</title>
    <summary>  We analyze eigenvalues fluctuations of the Laplacian of various networks
under the random matrix theory framework. Analyses of random networks,
scale-free networks and small-world networks show that nearest neighbor spacing
distribution of the Laplacian of these networks follow Gaussian orthogonal
ensemble statistics of random matrix theory. Furthermore, we study nearest
neighbor spacing distribution as a function of the random connections and find
that transition to the Gaussian orthogonal ensemble statistics occurs at the
small-world transition.
</summary>
    <author>
      <name>Sarika Jalan</name>
    </author>
    <author>
      <name>Jayendra N. Bandyopadhyay</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physa.2007.09.026</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physa.2007.09.026" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 5 figures, replaced with the final version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physica A, 387, Issues 2-3, 667 (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0611735v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0611735v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/nlin/0211013v1</id>
    <updated>2002-11-08T16:40:30Z</updated>
    <published>2002-11-08T16:40:30Z</published>
    <title>A Spin Glass Model of Human Logic Systems</title>
    <summary>  In this paper, we discuss different models for human logic systems and
describe a game with nature. G\"odel`s incompleteness theorem is taken into
account to construct a model of logical networks based on axioms obtained by
symmetry breaking. These classical logic networks are then coupled using rules
that depend on whether two networks contain axioms or anti-axioms. The social
lattice of axiom based logic networks is then placed with the environment
network in a game including entropy as a cost factor. The classical logical
networks are then replaced with ``preference axioms'' to the role of fuzzy
logic.
</summary>
    <author>
      <name>Fariel Shafee</name>
    </author>
    <link href="http://arxiv.org/abs/nlin/0211013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/nlin/0211013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/physics/0510143v1</id>
    <updated>2005-10-14T20:00:04Z</updated>
    <published>2005-10-14T20:00:04Z</published>
    <title>Vertex similarity in networks</title>
    <summary>  We consider methods for quantifying the similarity of vertices in networks.
We propose a measure of similarity based on the concept that two vertices are
similar if their immediate neighbors in the network are themselves similar.
This leads to a self-consistent matrix formulation of similarity that can be
evaluated iteratively using only a knowledge of the adjacency matrix of the
network. We test our similarity measure on computer-generated networks for
which the expected results are known, and on a number of real-world networks.
</summary>
    <author>
      <name>E. A. Leicht</name>
    </author>
    <author>
      <name>Petter Holme</name>
    </author>
    <author>
      <name>M. E. J. Newman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.73.026120</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.73.026120" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 73, 026120 (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/physics/0510143v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/physics/0510143v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/q-bio/0309019v1</id>
    <updated>2003-09-29T17:33:13Z</updated>
    <published>2003-09-29T17:33:13Z</published>
    <title>Scaling in Counter Expressed Gene Networks Constructed from Gene
  Expression Data</title>
    <summary>  We study counter expressed gene networks constructed from gene-expression
data obtained from many types of cancers. The networks are synthesized by
connecting vertices belonging to each others' list of K-farthest-neighbors,
with K being an a priori selected non-negative integer. In the range of K
corresponding to minimum homogeneity, the degree distribution of the networks
shows scaling. Clustering in these networks is smaller than that in equivalent
random graphs and remains zero till significantly large K. Their small
diameter, however, implies small-world behavior which is corroborated by their
eigenspectrum. We discuss implications of these findings in several contexts.
</summary>
    <author>
      <name>Himanshu Agrawal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages REVTeX</arxiv:comment>
    <link href="http://arxiv.org/abs/q-bio/0309019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/q-bio/0309019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/q-bio/0402045v1</id>
    <updated>2004-02-27T21:30:18Z</updated>
    <published>2004-02-27T21:30:18Z</published>
    <title>Performance of networks of artificial neurons: The role of clustering</title>
    <summary>  The performance of the Hopfield neural network model is numerically studied
on various complex networks, such as the Watts-Strogatz network, the
Barab{\'a}si-Albert network, and the neuronal network of the C. elegans.
Through the use of a systematic way of controlling the clustering coefficient,
with the degree of each neuron kept unchanged, we find that the networks with
the lower clustering exhibit much better performance. The results are discussed
in the practical viewpoint of application, and the biological implications are
also suggested.
</summary>
    <author>
      <name>Beom Jun Kim</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Ajou Univ.</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.69.045101</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.69.045101" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, to appear in PRE as Rapid Comm</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 69, 045101(R) (2004)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/q-bio/0402045v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/q-bio/0402045v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/q-bio/0405028v1</id>
    <updated>2004-05-31T23:45:47Z</updated>
    <published>2004-05-31T23:45:47Z</published>
    <title>The Heart of Protein-Protein Interaction Networks</title>
    <summary>  Recent developments in complex networks have paved the way to a series of
important biological insights, such as the fact that many of the essential
proteins of S. cerevisae corresponds to the so-called hubs of the respective
protein-protein interaction networks. Despite the special importance of hubs,
other types of nodes such as those corresponding to the network border, as well
as the innermost nodes, also deserve special attention. This work reports on
how the application of the concept of distance transform to networks showed
that a great deal of the innermost nodes correspond to essential proteins, with
interesting biological implications.
</summary>
    <author>
      <name>Luciano da Fontoura Costa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/q-bio/0405028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/q-bio/0405028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.4503v1</id>
    <updated>2007-05-31T02:53:30Z</updated>
    <published>2007-05-31T02:53:30Z</published>
    <title>Approximating the largest eigenvalue of network adjacency matrices</title>
    <summary>  The largest eigenvalue of the adjacency matrix of a network plays an
important role in several network processes (e.g., synchronization of
oscillators, percolation on directed networks, linear stability of equilibria
of network coupled systems, etc.). In this paper we develop approximations to
the largest eigenvalue of adjacency matrices and discuss the relationships
between these approximations. Numerical experiments on simulated networks are
used to test our results.
</summary>
    <author>
      <name>Juan G. Restrepo</name>
    </author>
    <author>
      <name>Edward Ott</name>
    </author>
    <author>
      <name>Brian R. Hunt</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.76.056119</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.76.056119" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0705.4503v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.4503v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0706.3069v1</id>
    <updated>2007-06-20T22:15:24Z</updated>
    <published>2007-06-20T22:15:24Z</published>
    <title>Network synchronization: Spectral versus statistical properties</title>
    <summary>  We consider synchronization of weighted networks, possibly with asymmetrical
connections. We show that the synchronizability of the networks cannot be
directly inferred from their statistical properties. Small local changes in the
network structure can sensitively affect the eigenvalues relevant for
synchronization, while the gross statistical network properties remain
essentially unchanged. Consequently, commonly used statistical properties,
including the degree distribution, degree homogeneity, average degree, average
distance, degree correlation, and clustering coefficient, can fail to
characterize the synchronizability of networks.
</summary>
    <author>
      <name>Fatihcan M. Atay</name>
    </author>
    <author>
      <name>Turker Biyikoglu</name>
    </author>
    <author>
      <name>Juergen Jost</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physd.2006.09.018</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physd.2006.09.018" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physica D 224:35-41 (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0706.3069v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0706.3069v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.1446v1</id>
    <updated>2007-09-10T16:41:47Z</updated>
    <published>2007-09-10T16:41:47Z</published>
    <title>Algorithm for counting large directed loops</title>
    <summary>  We derive a Belief-Propagation algorithm for counting large loops in a
directed network. We evaluate the distribution of the number of small loops in
a directed random network with given degree sequence. We apply the algorithm to
a few characteristic directed networks of various network sizes and loop
structures and compare the algorithm with exhaustive counting results when
possible. The algorithm is adequate in estimating loop counts for large
directed networks and can be used to compare the loop structure of directed
networks and their randomized counterparts.
</summary>
    <author>
      <name>Ginestra Bianconi</name>
    </author>
    <author>
      <name>Natali Gulbahce</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1751-8113/41/22/224003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1751-8113/41/22/224003" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">(9 pages, 3 figures)</arxiv:comment>
    <link href="http://arxiv.org/abs/0709.1446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.1446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.1247v2</id>
    <updated>2009-10-16T14:36:33Z</updated>
    <published>2008-03-08T15:22:00Z</published>
    <title>Entropies of complex networks with hierarchically constrained topologies</title>
    <summary>  The entropy of a hierarchical network topology in an ensemble of sparse
random networks with "hidden variables" associated to its nodes, is the
log-likelihood that a given network topology is present in the chosen
ensemble.We obtain a general formula for this entropy,which has a clear simple
interpretation in some simple limiting cases. The results provide new keys with
which to solve the general problem of "fitting" a given network with an
appropriate ensemble of random networks.
</summary>
    <author>
      <name>Ginestra Bianconi</name>
    </author>
    <author>
      <name>Anthony C. C. Coolen</name>
    </author>
    <author>
      <name>Conrad J. Perez Vicente</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.78.016114</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.78.016114" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">(18 pages)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. 78, 0161141 (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0803.1247v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.1247v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.1199v1</id>
    <updated>2009-03-06T13:02:53Z</updated>
    <published>2009-03-06T13:02:53Z</published>
    <title>Spreading on a complex network avoiding certain motifs</title>
    <summary>  Spreading of either information or matter can often be treated as a network
problem. It can be of great importance to be able to estimate the likelihood
that spreading through a network reaches essentially the entire network while
still not reaching certain sub-classes of the network. We show that excluding
nodes and edges from the network has a subtle effect on the percolation. We
study two specific examples of degree distributions (exponential and scale
free) for which analytical solutions can be obtained. The two cases exhibit
qualitatively different behavior.
</summary>
    <author>
      <name>Tomas Alarcon</name>
    </author>
    <author>
      <name>Henrik Jeldtoft Jensen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0903.1199v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.1199v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.4009v1</id>
    <updated>2009-03-24T03:50:17Z</updated>
    <published>2009-03-24T03:50:17Z</published>
    <title>Random graphs with clustering</title>
    <summary>  We offer a solution to a long-standing problem in the physics of networks,
the creation of a plausible, solvable model of a network that displays
clustering or transitivity -- the propensity for two neighbors of a network
node also to be neighbors of one another. We show how standard random graph
models can be generalized to incorporate clustering and give exact solutions
for various properties of the resulting networks, including sizes of network
components, size of the giant component if there is one, position of the phase
transition at which the giant component forms, and position of the phase
transition for percolation on the network.
</summary>
    <author>
      <name>M. E. J. Newman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.103.058701</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.103.058701" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. Lett. 103, 058701 (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0903.4009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.4009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.5943v1</id>
    <updated>2012-01-28T11:05:26Z</updated>
    <published>2012-01-28T11:05:26Z</published>
    <title>Cognitive Memory Network</title>
    <summary>  A resistive memory network that has no crossover wiring is proposed to
overcome the hardware limitations to size and functional complexity that is
associated with conventional analogue neural networks. The proposed memory
network is based on simple network cells that are arranged in a hierarchical
modular architecture. Cognitive functionality of this network is demonstrated
by an example of character recognition. The network is trained by an
evolutionary process to completely recognise characters deformed by random
noise, rotation, scaling and shifting
</summary>
    <author>
      <name>Alex Pappachen James</name>
    </author>
    <author>
      <name>Sima Dimitrijev</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1049/el.2010.0279</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1049/el.2010.0279" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Electronics Letters,46, 10, 677 - 678, 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1201.5943v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.5943v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.2877v1</id>
    <updated>2012-05-13T15:24:01Z</updated>
    <published>2012-05-13T15:24:01Z</published>
    <title>Clustering of random scale-free networks</title>
    <summary>  We derive the finite size dependence of the clustering coefficient of
scale-free random graphs generated by the configuration model with degree
distribution exponent $2&lt;\gamma&lt;3$. Degree heterogeneity increases the presence
of triangles in the network up to levels that compare to those found in many
real networks even for extremely large nets. We also find that for values of
$\gamma \approx 2$, clustering is virtually size independent and, at the same
time, becomes a {\it de facto} non self-averaging topological property. This
implies that a single instance network is not representative of the ensemble
even for very large network sizes.
</summary>
    <author>
      <name>Pol Colomer-de-Simon</name>
    </author>
    <author>
      <name>Marian Boguna</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.86.026120</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.86.026120" rel="related"/>
    <link href="http://arxiv.org/abs/1205.2877v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.2877v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.0476v1</id>
    <updated>2009-11-03T02:37:23Z</updated>
    <published>2009-11-03T02:37:23Z</published>
    <title>Analytic Solution to Clustering Coefficients on Weighted Networks</title>
    <summary>  Clustering coefficient is an important topological feature of complex
networks. It is, however, an open question to give out its analytic expression
on weighted networks yet. Here we applied an extended mean-field approach to
investigate clustering coefficients in the typical weighted networks proposed
by Barrat, Barth\'elemy and Vespignani (BBV networks). We provide analytical
solutions of this model and find that the local clustering in BBV networks
depends on the node degree and strength. Our analysis is well in agreement with
results of numerical simulations.
</summary>
    <author>
      <name>Yichao Zhang</name>
    </author>
    <author>
      <name>Zhongzhi Zhang</name>
    </author>
    <author>
      <name>Jihong Guan</name>
    </author>
    <author>
      <name>Shuigeng Zhou</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1742-5468/2010/03/P03013</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1742-5468/2010/03/P03013" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A paper with 9 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">stacks.iop.org/JSTAT/2010/P03013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0911.0476v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.0476v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.2614v1</id>
    <updated>2011-05-13T04:56:08Z</updated>
    <published>2011-05-13T04:56:08Z</published>
    <title>Growth and Optimality in Network Evolution</title>
    <summary>  In this paper we investigate networks whose evolution is governed by the
interaction of a random assembly process and an optimization process. In the
first process, new nodes are added one at a time and form connections to
randomly selected old nodes. In between node additions, the network is rewired
to minimize its pathlength. For timescales, at which neither the assembly nor
the optimization processes are dominant, we find a rich variety of complex
networks with power law tails in the degree distributions. These networks also
exhibit non-trivial clustering, a hierarchical organization and interesting
degree mixing patterns.
</summary>
    <author>
      <name>Markus Brede</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Artificial Life (2011)</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.2614v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.2614v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.5637v1</id>
    <updated>2013-10-21T16:46:01Z</updated>
    <published>2013-10-21T16:46:01Z</published>
    <title>Searching for feasible stationary states in reaction networks by solving
  a Boolean constraint satisfaction problem</title>
    <summary>  We analyze the solutions, on single network instances, of a recently
introduced class of constraint-satisfaction problems (CSPs), describing
feasible steady states of chemical reaction networks. First, we show that the
CSPs generalize the scheme known as Network Expansion, which is recovered in a
specific limit. Next, a full statistical mechanics characterization (including
the phase diagram and a discussion of physical origin of the phase transitions)
for Network Expansion is obtained. Finally, we provide a message-passing
algorithm to solve the original CSPs in the most general form.
</summary>
    <author>
      <name>Alessandro Seganti</name>
    </author>
    <author>
      <name>Federico Ricci-Tersenghi</name>
    </author>
    <author>
      <name>Andrea De Martino</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.89.022139</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.89.022139" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages (incl. appendix)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 89, 022139 (2014)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1310.5637v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.5637v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.2336v1</id>
    <updated>2013-12-09T08:21:01Z</updated>
    <published>2013-12-09T08:21:01Z</published>
    <title>Hierarchical scale-free network is fragile against random failure</title>
    <summary>  We investigate site percolation in a hierarchical scale-free network known as
the Dorogovtsev- Goltsev-Mendes network. We use the generating function method
to show that the percolation threshold is 1, i.e., the system is not in the
percolating phase when the occupation probability is less than 1. The present
result is contrasted to bond percolation in the same network of which the
percolation threshold is zero. We also show that the percolation threshold of
intentional attacks is 1. Our results suggest that this hierarchical scale-free
network is very fragile against both random failure and intentional attacks.
Such a structural defect is common in many hierarchical network models.
</summary>
    <author>
      <name>Takehisa Hasegawa</name>
    </author>
    <author>
      <name>Koji Nemoto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.88.062807</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.88.062807" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 88 (2013) 062807</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1312.2336v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.2336v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.08320v1</id>
    <updated>2015-10-18T20:21:21Z</updated>
    <published>2015-10-18T20:21:21Z</published>
    <title>Networkcontrology</title>
    <summary>  An increasing number of complex systems are now modeled as networks of
coupled dynamical entities. Nonlinearity and high-dimensionality are hallmarks
of the dynamics of such networks but have generally been regarded as obstacles
to control. Here I discuss recent advances on mathematical and computational
approaches to control high-dimensional nonlinear network dynamics under general
constraints on the admissible interventions. I also discuss the potential of
network control to address pressing scientific problems in various disciplines.
</summary>
    <author>
      <name>Adilson E. Motter</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/1.4931570</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/1.4931570" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Review article on control of network dynamics published as part of
  the 25th Anniversary Issue of Chaos</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Chaos 25, 097621 (2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1510.08320v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.08320v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0305025v1</id>
    <updated>2003-05-16T16:00:23Z</updated>
    <published>2003-05-16T16:00:23Z</published>
    <title>Simultaneous Dempster-Shafer clustering and gradual determination of
  number of clusters using a neural network structure</title>
    <summary>  In this paper we extend an earlier result within Dempster-Shafer theory
["Fast Dempster-Shafer Clustering Using a Neural Network Structure," in Proc.
Seventh Int. Conf. Information Processing and Management of Uncertainty in
Knowledge-Based Systems (IPMU'98)] where several pieces of evidence were
clustered into a fixed number of clusters using a neural structure. This was
done by minimizing a metaconflict function. We now develop a method for
simultaneous clustering and determination of number of clusters during
iteration in the neural structure. We let the output signals of neurons
represent the degree to which a pieces of evidence belong to a corresponding
cluster. From these we derive a probability distribution regarding the number
of clusters, which gradually during the iteration is transformed into a
determination of number of clusters. This gradual determination is fed back
into the neural structure at each iteration to influence the clustering
process.
</summary>
    <author>
      <name>Johan Schubert</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IDC.1999.754191</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IDC.1999.754191" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 10 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">in Proceedings of the 1999 Information, Decision and Control
  Conference (IDC'99), pp. 401-406, Adelaide, Australia, 8-10 February 1999,
  IEEE, Piscataway, 1999</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0305025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0305025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.3; I.2.6; I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.01769v1</id>
    <updated>2015-05-07T16:40:43Z</updated>
    <published>2015-05-07T16:40:43Z</published>
    <title>Spikes Synchronization in Neural Networks with Synaptic Plasticity</title>
    <summary>  In this paper, we investigated the neural spikes synchronisation in a neural
network with synaptic plasticity and external perturbation. In the simulations
the neural dynamics is described by the Hodgkin Huxley model considering
chemical synapses (excitatory) among neurons. According to neural spikes
synchronisation is expected that a perturbation produce non synchronised
regimes. However, in the literature there are works showing that the
combination of synaptic plasticity and external perturbation may generate
synchronised regime. This article describes the effect of the synaptic
plasticity on the synchronisation, where we consider a perturbation with a
uniform distribution. This study is relevant to researches of neural disorders
control.
</summary>
    <author>
      <name>Rafael R. Borges</name>
    </author>
    <author>
      <name>Kelly C. Iarosz</name>
    </author>
    <author>
      <name>Antonio M. Batista</name>
    </author>
    <author>
      <name>Iber√™ L. Caldas</name>
    </author>
    <author>
      <name>Fernando S. Borges</name>
    </author>
    <author>
      <name>Ewandson L. Lameu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Portuguese, Submitted to Revista Brasileira de Ensino de F\'isica
  (RBEF)</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.01769v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.01769v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.00177v3</id>
    <updated>2016-06-16T10:01:49Z</updated>
    <published>2015-12-01T08:43:19Z</published>
    <title>LSTM Neural Reordering Feature for Statistical Machine Translation</title>
    <summary>  Artificial neural networks are powerful models, which have been widely
applied into many aspects of machine translation, such as language modeling and
translation modeling. Though notable improvements have been made in these
areas, the reordering problem still remains a challenge in statistical machine
translations. In this paper, we present a novel neural reordering model that
directly models word pairs and alignment. By utilizing LSTM recurrent neural
networks, much longer context could be learned for reordering prediction.
Experimental results on NIST OpenMT12 Arabic-English and Chinese-English
1000-best rescoring task show that our LSTM neural reordering feature is robust
and achieves significant improvements over various baseline systems.
</summary>
    <author>
      <name>Yiming Cui</name>
    </author>
    <author>
      <name>Shijin Wang</name>
    </author>
    <author>
      <name>Jianfeng Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.18653/v1/N16-1112</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.18653/v1/N16-1112" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, accepted by NAACL2016 short paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.00177v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.00177v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06143v2</id>
    <updated>2016-10-13T20:10:09Z</updated>
    <published>2016-03-19T20:58:47Z</published>
    <title>Neurally-Guided Procedural Models: Amortized Inference for Procedural
  Graphics Programs using Neural Networks</title>
    <summary>  Probabilistic inference algorithms such as Sequential Monte Carlo (SMC)
provide powerful tools for constraining procedural models in computer graphics,
but they require many samples to produce desirable results. In this paper, we
show how to create procedural models which learn how to satisfy constraints. We
augment procedural models with neural networks which control how the model
makes random choices based on the output it has generated thus far. We call
such models neurally-guided procedural models. As a pre-computation, we train
these models to maximize the likelihood of example outputs generated via SMC.
They are then used as efficient SMC importance samplers, generating
high-quality results with very few samples. We evaluate our method on
L-system-like models with image-based constraints. Given a desired quality
threshold, neurally-guided models can generate satisfactory results up to 10x
faster than unguided models.
</summary>
    <author>
      <name>Daniel Ritchie</name>
    </author>
    <author>
      <name>Anna Thomas</name>
    </author>
    <author>
      <name>Pat Hanrahan</name>
    </author>
    <author>
      <name>Noah D. Goodman</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Information Processing Systems (NIPS 2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1603.06143v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06143v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02609v1</id>
    <updated>2016-05-09T14:47:53Z</updated>
    <published>2016-05-09T14:47:53Z</published>
    <title>Dynamic Decomposition of Spatiotemporal Neural Signals</title>
    <summary>  Neural signals are characterized by rich temporal and spatiotemporal dynamics
that reflect the organization of cortical networks. Theoretical research has
shown how neural networks can operate at different dynamic ranges that
correspond to specific types of information processing. Here we present a data
analysis framework that uses a linearized model of these dynamic states in
order to decompose the measured neural signal into a series of components that
capture both rhythmic and non-rhythmic neural activity. The method is based on
stochastic differential equations and Gaussian process regression. Through
computer simulations and analysis of magnetoencephalographic data, we
demonstrate the efficacy of the method in identifying meaningful modulations of
oscillatory signals corrupted by structured temporal and spatiotemporal noise.
These results suggest that the method is particularly suitable for the analysis
and interpretation of complex temporal and spatiotemporal neural signals.
</summary>
    <author>
      <name>Luca Ambrogioni</name>
    </author>
    <author>
      <name>Marcel A. J. van Gerven</name>
    </author>
    <author>
      <name>Eric Maris</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pcbi.1005540</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pcbi.1005540" rel="related"/>
    <link href="http://arxiv.org/abs/1605.02609v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02609v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08725v1</id>
    <updated>2016-07-29T08:35:10Z</updated>
    <published>2016-07-29T08:35:10Z</published>
    <title>Recurrent Neural Machine Translation</title>
    <summary>  The vanilla attention-based neural machine translation has achieved promising
performance because of its capability in leveraging varying-length source
annotations. However, this model still suffers from failures in long sentence
translation, for its incapability in capturing long-term dependencies. In this
paper, we propose a novel recurrent neural machine translation (RNMT), which
not only preserves the ability to model varying-length source annotations but
also better captures long-term dependencies. Instead of the conventional
attention mechanism, RNMT employs a recurrent neural network to extract the
context vector, where the target-side previous hidden state serves as its
initial state, and the source annotations serve as its inputs. We refer to this
new component as contexter. As the encoder, contexter and decoder in our model
are all derivable recurrent neural networks, our model can still be trained
end-to-end on large-scale corpus via stochastic algorithms. Experiments on
Chinese-English translation tasks demonstrate the superiority of our model to
attention-based neural machine translation, especially on long sentences.
Besides, further analysis of the contexter revels that our model can implicitly
reflect the alignment to source sentence.
</summary>
    <author>
      <name>Biao Zhang</name>
    </author>
    <author>
      <name>Deyi Xiong</name>
    </author>
    <author>
      <name>Jinsong Su</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to EMNLP2016, original version</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.08725v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08725v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.06258v3</id>
    <updated>2016-12-05T00:14:01Z</updated>
    <published>2016-10-20T01:03:20Z</published>
    <title>Using Fast Weights to Attend to the Recent Past</title>
    <summary>  Until recently, research on artificial neural networks was largely restricted
to systems with only two types of variable: Neural activities that represent
the current or recent input and weights that learn to capture regularities
among inputs, outputs and payoffs. There is no good reason for this
restriction. Synapses have dynamics at many different time-scales and this
suggests that artificial neural networks might benefit from variables that
change slower than activities but much faster than the standard weights. These
"fast weights" can be used to store temporary memories of the recent past and
they provide a neurally plausible way of implementing the type of attention to
the past that has recently proved very helpful in sequence-to-sequence models.
By using fast weights we can avoid the need to store copies of neural activity
patterns.
</summary>
    <author>
      <name>Jimmy Ba</name>
    </author>
    <author>
      <name>Geoffrey Hinton</name>
    </author>
    <author>
      <name>Volodymyr Mnih</name>
    </author>
    <author>
      <name>Joel Z. Leibo</name>
    </author>
    <author>
      <name>Catalin Ionescu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Added [Schmidhuber 1993] citation to the last paragraph of the
  introduction. Fixed typo appendix A.1 uniform initialization to 1/\sqrt{H}</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.06258v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.06258v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.08220v3</id>
    <updated>2017-01-10T01:40:51Z</updated>
    <published>2016-12-24T21:36:07Z</published>
    <title>Understanding Neural Networks through Representation Erasure</title>
    <summary>  While neural networks have been successfully applied to many natural language
processing tasks, they come at the cost of interpretability. In this paper, we
propose a general methodology to analyze and interpret decisions from a neural
model by observing the effects on the model of erasing various parts of the
representation, such as input word-vector dimensions, intermediate hidden
units, or input words. We present several approaches to analyzing the effects
of such erasure, from computing the relative difference in evaluation metrics,
to using reinforcement learning to erase the minimum set of input words in
order to flip a neural model's decision. In a comprehensive analysis of
multiple NLP tasks, including linguistic feature classification, sentence-level
sentiment analysis, and document level sentiment aspect prediction, we show
that the proposed methodology not only offers clear explanations about neural
model decisions, but also provides a way to conduct error analysis on neural
models.
</summary>
    <author>
      <name>Jiwei Li</name>
    </author>
    <author>
      <name>Will Monroe</name>
    </author>
    <author>
      <name>Dan Jurafsky</name>
    </author>
    <link href="http://arxiv.org/abs/1612.08220v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.08220v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.00909v1</id>
    <updated>2017-08-02T19:53:22Z</updated>
    <published>2017-08-02T19:53:22Z</published>
    <title>Machine learning for neural decoding</title>
    <summary>  While machine learning tools have been rapidly advancing, the majority of
neural decoding approaches still use last century's methods. Improving the
performance of neural decoding algorithms allows us to better understand what
information is contained in the brain, and can help advance engineering
applications such as brain machine interfaces. Here, we apply modern machine
learning techniques, including neural networks and gradient boosting, to decode
from spiking activity in 1) motor cortex, 2) somatosensory cortex, and 3)
hippocampus. We compare the predictive ability of these modern methods with
traditional decoding methods such as Wiener and Kalman filters. Modern methods,
in particular neural networks and ensembles, significantly outperformed the
traditional approaches. For instance, for all of the three brain areas, an LSTM
decoder explained over 40% of the unexplained variance from a Wiener filter.
These results suggest that modern machine learning techniques should become the
standard methodology for neural decoding. We provide code to facilitate wider
implementation of these methods.
</summary>
    <author>
      <name>Joshua I. Glaser</name>
    </author>
    <author>
      <name>Raeed H. Chowdhury</name>
    </author>
    <author>
      <name>Matthew G. Perich</name>
    </author>
    <author>
      <name>Lee E. Miller</name>
    </author>
    <author>
      <name>Konrad P. Kording</name>
    </author>
    <link href="http://arxiv.org/abs/1708.00909v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00909v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/physics/0209093v1</id>
    <updated>2002-09-26T16:27:52Z</updated>
    <published>2002-09-26T16:27:52Z</published>
    <title>Subthreshold dynamics of the neural membrane potential driven by
  stochastic synaptic input</title>
    <summary>  In the cerebral cortex, neurons are subject to a continuous bombardment of
synaptic inputs originating from the network's background activity. This leads
to ongoing, mostly subthreshold membrane dynamics that depends on the
statistics of the background activity and of the synapses made on a neuron.
Subthreshold membrane polarization is, in turn, a potent modulator of neural
responses. The present paper analyzes the subthreshold dynamics of the neural
membrane potential driven by synaptic inputs of stationary statistics. Synaptic
inputs are considered in linear interaction. The analysis identifies regimes of
input statistics which give rise to stationary, fluctuating, oscillatory, and
unstable dynamics. In particular, I show that (i) mere noise inputs can drive
the membrane potential into sustained, quasiperiodic oscillations (noise-driven
oscillations), in the absence of a stimulus-derived, intraneural, or network
pacemaker; (ii) adding hyperpolarizing to depolarizing synaptic input can
increase neural activity (hyperpolarization-induced activity), in the absence
of hyperpolarization-activated currents.
</summary>
    <author>
      <name>Ulrich Hillenbrand</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.66.021909</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.66.021909" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physical Review E 66, 021909 (2002)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/physics/0209093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/physics/0209093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.4930v2</id>
    <updated>2012-01-31T08:09:57Z</updated>
    <published>2011-11-21T16:58:58Z</published>
    <title>Comparative study of Financial Time Series Prediction by Artificial
  Neural Network with Gradient Descent Learning</title>
    <summary>  Financial forecasting is an example of a signal processing problem which is
challenging due to Small sample sizes, high noise, non-stationarity, and
non-linearity,but fast forecasting of stock market price is very important for
strategic business planning.Present study is aimed to develop a comparative
predictive model with Feedforward Multilayer Artificial Neural Network &amp;
Recurrent Time Delay Neural Network for the Financial Timeseries
Prediction.This study is developed with the help of historical stockprice
dataset made available by GoogleFinance.To develop this prediction model
Backpropagation method with Gradient Descent learning has been
implemented.Finally the Neural Net, learned with said algorithm is found to be
skillful predictor for non-stationary noisy Financial Timeseries.
</summary>
    <author>
      <name>Arka Ghosh</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal Of Scientific &amp; Engineering Research
  ISSN-2229-5518 Volume 3 Issue 1 January2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1111.4930v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.4930v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.5244v3</id>
    <updated>2010-07-26T21:21:02Z</updated>
    <published>2010-01-28T18:55:53Z</published>
    <title>Computing Networks: A General Framework to Contrast Neural and Swarm
  Cognitions</title>
    <summary>  This paper presents the Computing Networks (CNs) framework. CNs are used to
generalize neural and swarm architectures. Artificial neural networks, ant
colony optimization, particle swarm optimization, and realistic biological
models are used as examples of instantiations of CNs. The description of these
architectures as CNs allows their comparison. Their differences and
similarities allow the identification of properties that enable neural and
swarm architectures to perform complex computations and exhibit complex
cognitive abilities. In this context, the most relevant characteristics of CNs
are the existence multiple dynamical and functional scales. The relationship
between multiple dynamical and functional scales with adaptation, cognition (of
brains and swarms) and computation is discussed.
</summary>
    <author>
      <name>Carlos Gershenson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2478/s13230-010-0015-z</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2478/s13230-010-0015-z" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Paladyn, Journal of Behavioral Robotics 1(2): 147-153, 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1001.5244v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.5244v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; I.2.0; H.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.3531v1</id>
    <updated>2011-03-17T22:47:26Z</updated>
    <published>2011-03-17T22:47:26Z</published>
    <title>Visually-salient contour detection using a V1 neural model with
  horizontal connections</title>
    <summary>  A convolution model which accounts for neural activity dynamics in the
primary visual cortex is derived and used to detect visually salient contours
in images. Image inputs to the model are modulated by long-range horizontal
connections, allowing contextual effects in the image to determine visual
saliency, i.e. line segments arranged in a closed contour elicit a larger
neural response than line segments forming background clutter. The model is
tested on 3 types of contour, including a line, a circular closed contour, and
a non-circular closed contour. Using a modified association field to describe
horizontal connections the model is found to perform well for different
parameter values. For each type of contour a different facilitation mechanism
is found. Operating as a feed-forward network, the model assigns saliency by
increasing the neural activity of line segments facilitated by the horizontal
connections. Alternatively, operating as a feedback network, the model can
achieve further improvement over several iterations through cooperative
interactions. This model has no dynamical stability issues, and is suitable for
use in biologically-inspired neural networks.
</summary>
    <author>
      <name>P. N. Loxley</name>
    </author>
    <author>
      <name>L. M. Bettencourt</name>
    </author>
    <link href="http://arxiv.org/abs/1103.3531v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.3531v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.1653v1</id>
    <updated>2013-06-07T08:43:11Z</updated>
    <published>2013-06-07T08:43:11Z</published>
    <title>Non-constant bounded holomorphic functions of hyperbolic numbers -
  Candidates for hyperbolic activation functions</title>
    <summary>  The Liouville theorem states that bounded holomorphic complex functions are
necessarily constant. Holomorphic functions fulfill the socalled Cauchy-Riemann
(CR) conditions. The CR conditions mean that a complex $z$-derivative is
independent of the direction. Holomorphic functions are ideal for activation
functions of complex neural networks, but the Liouville theorem makes them
useless. Yet recently the use of hyperbolic numbers, lead to the construction
of hyperbolic number neural networks. We will describe the Cauchy-Riemann
conditions for hyperbolic numbers and show that there exists a new interesting
type of bounded holomorphic functions of hyperbolic numbers, which are not
constant. We give examples of such functions. They therefore substantially
expand the available candidates for holomorphic activation functions for
hyperbolic number neural networks.
  Keywords: Hyperbolic numbers, Liouville theorem, Cauchy-Riemann conditions,
bounded holomorphic functions
</summary>
    <author>
      <name>Eckhard Hitzer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">in Y. Kuroe, T. Nitta (eds.), Proceedings of the First SICE
  Symposium on Computational Intelligence [Concentrating on Clifford Neural
  Computing], 30 Sep. 2011, KIT, Kyoto, Japan, catalogue no. 11PG0009, pp. 23 -
  28, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1306.1653v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.1653v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.RA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.4864v1</id>
    <updated>2014-12-16T02:55:05Z</updated>
    <published>2014-12-16T02:55:05Z</published>
    <title>Learning with Pseudo-Ensembles</title>
    <summary>  We formalize the notion of a pseudo-ensemble, a (possibly infinite)
collection of child models spawned from a parent model by perturbing it
according to some noise process. E.g., dropout (Hinton et. al, 2012) in a deep
neural network trains a pseudo-ensemble of child subnetworks generated by
randomly masking nodes in the parent network. We present a novel regularizer
based on making the behavior of a pseudo-ensemble robust with respect to the
noise process generating it. In the fully-supervised setting, our regularizer
matches the performance of dropout. But, unlike dropout, our regularizer
naturally extends to the semi-supervised setting, where it produces
state-of-the-art results. We provide a case study in which we transform the
Recursive Neural Tensor Network of (Socher et. al, 2013) into a
pseudo-ensemble, which significantly improves its performance on a real-world
sentiment analysis benchmark.
</summary>
    <author>
      <name>Philip Bachman</name>
    </author>
    <author>
      <name>Ouais Alsharif</name>
    </author>
    <author>
      <name>Doina Precup</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Advances in Neural Information Processing Systems 27
  (NIPS 2014), Advances in Neural Information Processing Systems 27, Dec. 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.4864v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.4864v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.02914v2</id>
    <updated>2015-06-15T08:30:06Z</updated>
    <published>2015-06-09T14:02:02Z</published>
    <title>Training Restricted Boltzmann Machines via the Thouless-Anderson-Palmer
  Free Energy</title>
    <summary>  Restricted Boltzmann machines are undirected neural networks which have been
shown to be effective in many applications, including serving as
initializations for training deep multi-layer neural networks. One of the main
reasons for their success is the existence of efficient and practical
stochastic algorithms, such as contrastive divergence, for unsupervised
training. We propose an alternative deterministic iterative procedure based on
an improved mean field method from statistical physics known as the
Thouless-Anderson-Palmer approach. We demonstrate that our algorithm provides
performance equal to, and sometimes superior to, persistent contrastive
divergence, while also providing a clear and easy to evaluate objective
function. We believe that this strategy can be easily generalized to other
models as well as to more accurate higher-order approximations, paving the way
for systematic improvements in training Boltzmann machines with hidden units.
</summary>
    <author>
      <name>Marylou Gabri√©</name>
    </author>
    <author>
      <name>Eric W. Tramel</name>
    </author>
    <author>
      <name>Florent Krzakala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 7 figures, demo online at
  http://www.lps.ens.fr/~krzakala/WASP.html</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in Neural Information Processing Systems (NIPS 2015) 28,
  pages 640--648</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1506.02914v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.02914v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.06579v1</id>
    <updated>2015-06-22T12:57:15Z</updated>
    <published>2015-06-22T12:57:15Z</published>
    <title>Understanding Neural Networks Through Deep Visualization</title>
    <summary>  Recent years have produced great advances in training large, deep neural
networks (DNNs), including notable successes in training convolutional neural
networks (convnets) to recognize natural images. However, our understanding of
how these models work, especially what computations they perform at
intermediate layers, has lagged behind. Progress in the field will be further
accelerated by the development of better tools for visualizing and interpreting
neural nets. We introduce two such tools here. The first is a tool that
visualizes the activations produced on each layer of a trained convnet as it
processes an image or video (e.g. a live webcam stream). We have found that
looking at live activations that change in response to user input helps build
valuable intuitions about how convnets work. The second tool enables
visualizing features at each layer of a DNN via regularized optimization in
image space. Because previous versions of this idea produced less recognizable
images, here we introduce several new regularization methods that combine to
produce qualitatively clearer, more interpretable visualizations. Both tools
are open source and work on a pre-trained convnet with minimal setup.
</summary>
    <author>
      <name>Jason Yosinski</name>
    </author>
    <author>
      <name>Jeff Clune</name>
    </author>
    <author>
      <name>Anh Nguyen</name>
    </author>
    <author>
      <name>Thomas Fuchs</name>
    </author>
    <author>
      <name>Hod Lipson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages. To appear at ICML Deep Learning Workshop 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.06579v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.06579v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.06615v4</id>
    <updated>2015-12-01T22:59:24Z</updated>
    <published>2015-08-26T19:25:34Z</published>
    <title>Character-Aware Neural Language Models</title>
    <summary>  We describe a simple neural language model that relies only on
character-level inputs. Predictions are still made at the word-level. Our model
employs a convolutional neural network (CNN) and a highway network over
characters, whose output is given to a long short-term memory (LSTM) recurrent
neural network language model (RNN-LM). On the English Penn Treebank the model
is on par with the existing state-of-the-art despite having 60% fewer
parameters. On languages with rich morphology (Arabic, Czech, French, German,
Spanish, Russian), the model outperforms word-level/morpheme-level LSTM
baselines, again with fewer parameters. The results suggest that on many
languages, character inputs are sufficient for language modeling. Analysis of
word representations obtained from the character composition part of the model
reveals that the model is able to encode, from characters only, both semantic
and orthographic information.
</summary>
    <author>
      <name>Yoon Kim</name>
    </author>
    <author>
      <name>Yacine Jernite</name>
    </author>
    <author>
      <name>David Sontag</name>
    </author>
    <author>
      <name>Alexander M. Rush</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.06615v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.06615v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.01537v1</id>
    <updated>2015-12-04T20:43:30Z</updated>
    <published>2015-12-04T20:43:30Z</published>
    <title>Reuse of Neural Modules for General Video Game Playing</title>
    <summary>  A general approach to knowledge transfer is introduced in which an agent
controlled by a neural network adapts how it reuses existing networks as it
learns in a new domain. Networks trained for a new domain can improve their
performance by routing activation selectively through previously learned neural
structure, regardless of how or for what it was learned. A neuroevolution
implementation of this approach is presented with application to
high-dimensional sequential decision-making domains. This approach is more
general than previous approaches to neural transfer for reinforcement learning.
It is domain-agnostic and requires no prior assumptions about the nature of
task relatedness or mappings. The method is analyzed in a stochastic version of
the Arcade Learning Environment, demonstrating that it improves performance in
some of the more complex Atari 2600 games, and that the success of transfer can
be predicted based on a high-level characterization of game dynamics.
</summary>
    <author>
      <name>Alexander Braylan</name>
    </author>
    <author>
      <name>Mark Hollenbeck</name>
    </author>
    <author>
      <name>Elliot Meyerson</name>
    </author>
    <author>
      <name>Risto Miikkulainen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at AAAI 16</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.01537v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.01537v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00709v1</id>
    <updated>2016-06-02T14:53:33Z</updated>
    <published>2016-06-02T14:53:33Z</published>
    <title>f-GAN: Training Generative Neural Samplers using Variational Divergence
  Minimization</title>
    <summary>  Generative neural samplers are probabilistic models that implement sampling
using feedforward neural networks: they take a random input vector and produce
a sample from a probability distribution defined by the network weights. These
models are expressive and allow efficient computation of samples and
derivatives, but cannot be used for computing likelihoods or for
marginalization. The generative-adversarial training method allows to train
such models through the use of an auxiliary discriminative neural network. We
show that the generative-adversarial approach is a special case of an existing
more general variational divergence estimation approach. We show that any
f-divergence can be used for training generative neural samplers. We discuss
the benefits of various choices of divergence functions on training complexity
and the quality of the obtained generative models.
</summary>
    <author>
      <name>Sebastian Nowozin</name>
    </author>
    <author>
      <name>Botond Cseke</name>
    </author>
    <author>
      <name>Ryota Tomioka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.00709v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00709v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.02345v2</id>
    <updated>2017-02-24T02:26:15Z</updated>
    <published>2016-11-07T23:47:05Z</published>
    <title>Neural Taylor Approximations: Convergence and Exploration in Rectifier
  Networks</title>
    <summary>  Modern convolutional networks, incorporating rectifiers and max-pooling, are
neither smooth nor convex. Standard guarantees therefore do not apply.
Nevertheless, methods from convex optimization such as gradient descent and
Adam are widely used as building blocks for deep learning algorithms. This
paper provides the first convergence guarantee applicable to modern convnets.
The guarantee matches a lower bound for convex nonsmooth functions. The key
technical tool is the neural Taylor approximation -- a straightforward
application of Taylor expansions to neural networks -- and the associated
Taylor loss. Experiments on a range of optimizers, layers, and tasks provide
evidence that the analysis accurately captures the dynamics of neural
optimization.
  The second half of the paper applies the Taylor approximation to isolate the
main difficulty in training rectifier nets: that gradients are shattered. We
investigate the hypothesis that, by exploring the space of activation
configurations more thoroughly, adaptive optimizers such as RMSProp and Adam
are able to converge to better solutions.
</summary>
    <author>
      <name>David Balduzzi</name>
    </author>
    <author>
      <name>Brian McWilliams</name>
    </author>
    <author>
      <name>Tony Butler-Yeoman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.02345v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.02345v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.02072v1</id>
    <updated>2017-02-07T15:57:55Z</updated>
    <published>2017-02-07T15:57:55Z</published>
    <title>Adaptive Neural Control for a Class of Stochastic Nonlinear Systems with
  Unknown Parameters, Unknown Nonlinear Functions and Stochastic Disturbances</title>
    <summary>  In this paper, adaptive neural control (ANC) is investigated for a class of
strict-feedback nonlinear stochastic systems with unknown parameters, unknown
nonlinear functions and stochastic disturbances. The new controller of adaptive
neural network with state feedback is presented by using a universal
approximation of radial basis function neural network and backstepping. An
adaptive neural network state-feedback controller is designed by constructing a
suitable Lyapunov function. Adaptive bounding design technique is used to deal
with the unknown nonlinear functions and unknown parameters. It is shown that,
the global asymptotically stable in probability can be achieved for the
closed-loop system. The simulation results are presented to demonstrate the
effectiveness of the proposed control strategy in the presence of unknown
parameters, unknown nonlinear functions and stochastic disturbances.
</summary>
    <author>
      <name>Chao-Yang Chena</name>
    </author>
    <author>
      <name>Wei-Hua Gui</name>
    </author>
    <author>
      <name>Zhi-Hong Guan</name>
    </author>
    <author>
      <name>Ru-Liang Wang</name>
    </author>
    <author>
      <name>Shao-Wu Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures, Neurocomputing</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.02072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.02072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07149v1</id>
    <updated>2017-05-19T19:06:27Z</updated>
    <published>2017-05-19T19:06:27Z</published>
    <title>Local Information with Feedback Perturbation Suffices for Dictionary
  Learning in Neural Circuits</title>
    <summary>  While the sparse coding principle can successfully model information
processing in sensory neural systems, it remains unclear how learning can be
accomplished under neural architectural constraints. Feasible learning rules
must rely solely on synaptically local information in order to be implemented
on spatially distributed neurons. We describe a neural network with spiking
neurons that can address the aforementioned fundamental challenge and solve the
L1-minimizing dictionary learning problem, representing the first model able to
do so. Our major innovation is to introduce feedback synapses to create a
pathway to turn the seemingly non-local information into local ones. The
resulting network encodes the error signal needed for learning as the change of
network steady states caused by feedback, and operates akin to the classical
stochastic gradient descent method.
</summary>
    <author>
      <name>Tsung-Han Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.07149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.11040v2</id>
    <updated>2017-12-04T00:24:04Z</updated>
    <published>2017-05-31T11:40:57Z</published>
    <title>End-to-End Differentiable Proving</title>
    <summary>  We introduce neural networks for end-to-end differentiable proving of queries
to knowledge bases by operating on dense vector representations of symbols.
These neural networks are constructed recursively by taking inspiration from
the backward chaining algorithm as used in Prolog. Specifically, we replace
symbolic unification with a differentiable computation on vector
representations of symbols using a radial basis function kernel, thereby
combining symbolic reasoning with learning subsymbolic vector representations.
By using gradient descent, the resulting neural network can be trained to infer
facts from a given incomplete knowledge base. It learns to (i) place
representations of similar symbols in close proximity in a vector space, (ii)
make use of such similarities to prove queries, (iii) induce logical rules, and
(iv) use provided and induced logical rules for multi-hop reasoning. We
demonstrate that this architecture outperforms ComplEx, a state-of-the-art
neural link prediction model, on three out of four benchmark knowledge bases
while at the same time inducing interpretable function-free first-order logic
rules.
</summary>
    <author>
      <name>Tim Rockt√§schel</name>
    </author>
    <author>
      <name>Sebastian Riedel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NIPS 2017 camera-ready, NIPS 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.11040v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.11040v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.02575v1</id>
    <updated>2017-07-09T12:51:47Z</updated>
    <published>2017-07-09T12:51:47Z</published>
    <title>Neural Machine Translation between Herbal Prescriptions and Diseases</title>
    <summary>  The current study applies deep learning to herbalism. Toward the goal, we
acquired the de-identified health insurance reimbursements that were claimed in
a 10-year period from 2004 to 2013 in the National Health Insurance Database of
Taiwan, the total number of reimbursement records equaling 340 millions. Two
artificial intelligence techniques were applied to the dataset: residual
convolutional neural network multitask classifier and attention-based recurrent
neural network. The former works to translate from herbal prescriptions to
diseases; and the latter from diseases to herbal prescriptions. Analysis of the
classification results indicates that herbal prescriptions are specific to:
anatomy, pathophysiology, sex and age of the patient, and season and year of
the prescription. Further analysis identifies temperature and gross domestic
product as the meteorological and socioeconomic factors that are associated
with herbal prescriptions. Analysis of the neural machine transitional result
indicates that the recurrent neural network learnt not only syntax but also
semantics of diseases and herbal prescriptions.
</summary>
    <author>
      <name>Sun-Chong Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.02575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.02575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.03027v1</id>
    <updated>2017-08-09T22:34:36Z</updated>
    <published>2017-08-09T22:34:36Z</published>
    <title>Using Deep Neural Networks to Automate Large Scale Statistical Analysis
  for Big Data Applications</title>
    <summary>  Statistical analysis (SA) is a complex process to deduce population
properties from analysis of data. It usually takes a well-trained analyst to
successfully perform SA, and it becomes extremely challenging to apply SA to
big data applications. We propose to use deep neural networks to automate the
SA process. In particular, we propose to construct convolutional neural
networks (CNNs) to perform automatic model selection and parameter estimation,
two most important SA tasks. We refer to the resulting CNNs as the neural model
selector and the neural model estimator, respectively, which can be properly
trained using labeled data systematically generated from candidate models.
Simulation study shows that both the selector and estimator demonstrate
excellent performances. The idea and proposed framework can be further extended
to automate the entire SA process and have the potential to revolutionize how
SA is performed in big data analytics.
</summary>
    <author>
      <name>Rongrong Zhang</name>
    </author>
    <author>
      <name>Wei Deng</name>
    </author>
    <author>
      <name>Michael Yu Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/1708.03027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.03027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.03276v2</id>
    <updated>2017-11-22T17:37:04Z</updated>
    <published>2017-09-11T07:36:20Z</published>
    <title>Quantum neural networks driven by information reservoir</title>
    <summary>  This study concerns with the dynamics of a quantum neural network unit in
order to examine the suitability of simple neural computing tasks. More
specifically, we examine the dynamics of an interacting spin model chosen as a
candidate of a quantum perceptron for closed and open quantum systems. We adopt
a collisional model enables examining both Markovian and non-Markovian dynamics
of the proposed quantum system. We show that our quantum neural network (QNN)
unit has a stable output quantum state in contact with an environment carrying
information content. By the performed numerical simulations one can compare the
dynamics in the presence and absence of quantum memory effects. We find that
our QNN unit is suitable for implementing general neural computing tasks in
contact with a Markovian information environment and quantum memory effects
cause complications on the stability of the output state.
</summary>
    <author>
      <name>Deniz T√ºrkpen√ße</name>
    </author>
    <author>
      <name>Tahir √áetin Akƒ±ncƒ±</name>
    </author>
    <author>
      <name>Serhat ≈ûeker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 Pages, 4 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.03276v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.03276v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.10704v3</id>
    <updated>2018-02-22T04:49:44Z</updated>
    <published>2017-10-29T22:13:53Z</published>
    <title>Training Probabilistic Spiking Neural Networks with First-to-spike
  Decoding</title>
    <summary>  Third-generation neural networks, or Spiking Neural Networks (SNNs), aim at
harnessing the energy efficiency of spike-domain processing by building on
computing elements that operate on, and exchange, spikes. In this paper, the
problem of training a two-layer SNN is studied for the purpose of
classification, under a Generalized Linear Model (GLM) probabilistic neural
model that was previously considered within the computational neuroscience
literature. Conventional classification rules for SNNs operate offline based on
the number of output spikes at each output neuron. In contrast, a novel
training method is proposed here for a first-to-spike decoding rule, whereby
the SNN can perform an early classification decision once spike firing is
detected at an output neuron. Numerical results bring insights into the optimal
parameter selection for the GLM neuron and on the accuracy-complexity trade-off
performance of conventional and first-to-spike decoding.
</summary>
    <author>
      <name>Alireza Bagheri</name>
    </author>
    <author>
      <name>Osvaldo Simeone</name>
    </author>
    <author>
      <name>Bipin Rajendran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A shorter version will be published on Proc. IEEE ICASSP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.10704v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.10704v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.09334v1</id>
    <updated>2018-01-29T01:06:18Z</updated>
    <published>2018-01-29T01:06:18Z</published>
    <title>Ensemble Neural Relation Extraction with Adaptive Boosting</title>
    <summary>  Relation extraction has been widely studied to extract new relational facts
from open corpus. Previous relation extraction methods are faced with the
problem of wrong labels and noisy data, which substantially decrease the
performance of the model. In this paper, we propose an ensemble neural network
model - Adaptive Boosting LSTMs with Attention, to more effectively perform
relation extraction. Specifically, our model first employs the recursive neural
network LSTMs to embed each sentence. Then we import attention into LSTMs by
considering that the words in a sentence do not contribute equally to the
semantic meaning of the sentence. Next via adaptive boosting, we build
strategically several such neural classifiers. By ensembling multiple such LSTM
classifiers with adaptive boosting, we could build a more effective and robust
joint ensemble neural networks based relation extractor. Experiment results on
real dataset demonstrate the superior performance of the proposed model,
improving F1-score by about 8% compared to the state-of-the-art models. The
code of this work is publicly available on https://github.com/RE-2018/re.
</summary>
    <author>
      <name>Dongdong Yang</name>
    </author>
    <author>
      <name>Senzhang Wang</name>
    </author>
    <author>
      <name>Zhoujun Li</name>
    </author>
    <link href="http://arxiv.org/abs/1801.09334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.09334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.08680v1</id>
    <updated>2018-02-23T18:57:58Z</updated>
    <published>2018-02-23T18:57:58Z</published>
    <title>Advantages of versatile neural-network decoding for topological codes</title>
    <summary>  Finding optimal correction of errors in generic stabilizer codes is a
computationally hard problem, even for simple noise models. While this task can
be simplified for codes with some structure, such as topological stabilizer
codes, developing good and efficient decoders still remains a challenge. In our
work, we systematically study a very versatile class of decoders based on
feedforward neural networks. To demonstrate adaptability, we apply neural
decoders to the triangular color and toric codes under various noise models
with realistic features, such as spatially-correlated errors. We report that
neural decoders provide significant improvement over leading efficient decoders
in terms of the error-correction threshold. Using neural networks simplifies
the process of designing well-performing decoders, and does not require prior
knowledge of the underlying noise model.
</summary>
    <author>
      <name>Nishad Maskara</name>
    </author>
    <author>
      <name>Aleksander Kubica</name>
    </author>
    <author>
      <name>Tomas Jochym-O'Connor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 6 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.08680v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.08680v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.2935v3</id>
    <updated>2010-01-18T02:39:52Z</updated>
    <published>2009-05-18T17:16:03Z</published>
    <title>Experimental demonstration of associative memory with memristive neural
  networks</title>
    <summary>  When someone mentions the name of a known person we immediately recall her
face and possibly many other traits. This is because we possess the so-called
associative memory, that is the ability to correlate different memories to the
same fact or event. Associative memory is such a fundamental and encompassing
human ability (and not just human) that the network of neurons in our brain
must perform it quite easily. The question is then whether electronic neural
networks (electronic schemes that act somewhat similarly to human brains) can
be built to perform this type of function. Although the field of neural
networks has developed for many years, a key element, namely the synapses
between adjacent neurons, has been lacking a satisfactory electronic
representation. The reason for this is that a passive circuit element able to
reproduce the synapse behaviour needs to remember its past dynamical history,
store a continuous set of states, and be "plastic" according to the
pre-synaptic and post-synaptic neuronal activity. Here we show that all this
can be accomplished by a memory-resistor (memristor for short). In particular,
by using simple and inexpensive off-the-shelf components we have built a
memristor emulator which realizes all required synaptic properties. Most
importantly, we have demonstrated experimentally the formation of associative
memory in a simple neural network consisting of three electronic neurons
connected by two memristor-emulator synapses. This experimental demonstration
opens up new possibilities in the understanding of neural processes using
memory devices, an important step forward to reproduce complex learning,
adaptive and spontaneous behaviour with electronic neural networks.
</summary>
    <author>
      <name>Yuriy V. Pershin</name>
    </author>
    <author>
      <name>Massimiliano Di Ventra</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Networks 23, 881 (2010)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0905.2935v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.2935v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mes-hall" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.06944v4</id>
    <updated>2017-04-27T15:39:27Z</updated>
    <published>2015-08-27T17:24:13Z</published>
    <title>Continuous parameter working memory in a balanced chaotic neural network</title>
    <summary>  It has been proposed that neural noise in the cortex arises from chaotic
dynamics in the balanced state: in this model of cortical dynamics, the
excitatory and inhibitory inputs to each neuron approximately cancel, and
activity is driven by fluctuations of the synaptic inputs around their mean. It
remains unclear whether neural networks in the balanced state can perform tasks
that are highly sensitive to noise, such as storage of continuous parameters in
working memory, while also accounting for the irregular behavior of single
neurons. Here we show that continuous parameter working memory can be
maintained in the balanced state, in a neural circuit with a simple network
architecture. We show analytically that in the limit of an infinite network,
the dynamics generated by this architecture are characterized by a continuous
set of steady balanced states, allowing for the indefinite storage of a
continuous parameter. In finite networks, we show that the chaotic noise drives
diffusive motion along the approximate attractor, which gradually degrades the
stored memory. We analyze the dynamics and show that the slow diffusive motion
induces slowly decaying temporal cross correlations in the activity, which
differ substantially from those previously described in the balanced state. We
calculate the diffusivity, and show that it is inversely proportional to the
system size. For large enough (but realistic) neural population sizes, and with
suitable tuning of the network connections, the proposed balanced network can
sustain continuous parameter values in memory over time scales larger by
several orders of magnitude than the single neuron time scale.
</summary>
    <author>
      <name>Nimrod Shaham</name>
    </author>
    <author>
      <name>Yoram Burak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Expanded and revised version of the manuscript. Accepted to PLoS
  Computational Biology (2017). 29 pages, 8 figures and 4 supplementary figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.06944v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.06944v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.01382v2</id>
    <updated>2017-08-21T17:34:32Z</updated>
    <published>2017-06-05T15:43:40Z</published>
    <title>Neuro-RAM Unit with Applications to Similarity Testing and Compression
  in Spiking Neural Networks</title>
    <summary>  We study distributed algorithms implemented in a simplified biologically
inspired model for stochastic spiking neural networks. We focus on tradeoffs
between computation time and network complexity, along with the role of
randomness in efficient neural computation.
  It is widely accepted that neural computation is inherently stochastic. In
recent work, we explored how this stochasticity could be leveraged to solve the
`winner-take-all' leader election task. Here, we focus on using randomness in
neural algorithms for similarity testing and compression. In the most basic
setting, given two $n$-length patterns of firing neurons, we wish to
distinguish if the patterns are equal or $\epsilon$-far from equal.
  Randomization allows us to solve this task with a very compact network, using
$O \left (\frac{\sqrt{n}\log n}{\epsilon}\right)$ auxiliary neurons, which is
sublinear in the input size. At the heart of our solution is the design of a
$t$-round neural random access memory, or indexing network, which we call a
neuro-RAM. This module can be implemented with $O(n/t)$ auxiliary neurons and
is useful in many applications beyond similarity testing.
  Using a VC dimension-based argument, we show that the tradeoff between
runtime and network size in our neuro-RAM is nearly optimal. Our result has
several implications -- since our neuro-RAM can be implemented with
deterministic threshold gates, it shows that, in contrast to similarity
testing, randomness does not provide significant computational advantages for
this problem. It also establishes a separation between feedforward networks
whose gates spike with sigmoidal probability functions, and well-studied
deterministic sigmoidal networks, whose gates output real number sigmoidal
values, and which can implement a neuro-RAM much more efficiently.
</summary>
    <author>
      <name>Nancy Lynch</name>
    </author>
    <author>
      <name>Cameron Musco</name>
    </author>
    <author>
      <name>Merav Parter</name>
    </author>
    <link href="http://arxiv.org/abs/1706.01382v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.01382v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/astro-ph/0110492v1</id>
    <updated>2001-10-22T16:38:32Z</updated>
    <published>2001-10-22T16:38:32Z</published>
    <title>Morphological Classification of Galaxies Using Artificial Neural
  Networks</title>
    <summary>  The results of morphological galaxy classifications performed by humans and
by automated methods are compared. In particular, a comparison is made between
the eyeball classifications of 454 galaxies in the Sloan Digital Sky Survey
(SDSS) commissioning data (Shimasaku et al. 2001) with those of supervised
artificial neural network programs constructed using the MATLAB Neural Network
Toolbox package. Networks in this package have not previously been used for
galaxy classification. It is found that simple neural networks are able to
improve on the results of linear classifiers, giving correlation coefficients
of the order of 0.8 +/- 0.1, compared with those of around 0.7 +/- 0.1 for
linear classifiers. The networks are trained using the resilient
backpropagation algorithm, which, to the author's knowledge, has not been
specifically used in the galaxy classification literature. The galaxy
parameters used and the network architecture are both important, and in
particular the galaxy concentration index, a measure of the concentration of
light towards the centre of the galaxy, is the most significant parameter.
Simple networks are briefly applied to 29,429 galaxies with redshifts from the
SDSS Early Data Release. They give an approximate ratio of types E/S0:Sp:Irr of
14 +/- 5 : 86 +/- 12 : 0 +/- 0.1, which broadly agrees with the well known
approximate ratios of 20:80:1 observed at low redshift.
</summary>
    <author>
      <name>Nicholas M. Ball</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MSc thesis (1 year postgraduate course), University of Sussex, UK; 80
  pages, submitted August 30th 2001</arxiv:comment>
    <link href="http://arxiv.org/abs/astro-ph/0110492v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/astro-ph/0110492v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9906247v1</id>
    <updated>1999-06-16T15:13:20Z</updated>
    <published>1999-06-16T15:13:20Z</published>
    <title>Erratum: Small-world networks: Evidence for a crossover picture</title>
    <summary>  We correct the value of the exponent \tau.
</summary>
    <author>
      <name>Marc Barthelemy</name>
    </author>
    <author>
      <name>Luis A. N. Amaral</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.82.5180</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.82.5180" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physical Review Letters 82, 5180 (1999)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/9906247v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9906247v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9910202v1</id>
    <updated>1999-10-13T20:31:14Z</updated>
    <published>1999-10-13T20:31:14Z</published>
    <title>Central limit theorems for nonlinear hierarchical sequences of random
  variables</title>
    <summary>  We study central limit theorems for certain nonlinear sequences of random
variables. In particular, we prove the central limit theorems for the bounded
conductivity of the random resistor networks on hierarchical lattices.
</summary>
    <author>
      <name>Jung M. Woo</name>
    </author>
    <author>
      <name>Jan Wehr</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, no figure</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9910202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9910202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0107612v2</id>
    <updated>2004-07-28T22:04:05Z</updated>
    <published>2001-07-30T19:49:50Z</published>
    <title>Extremely Dilute Modular Neuronal Networks: Neocortical Memory Retrieval
  Dynamics</title>
    <summary>  Withdrawn by the author.
</summary>
    <author>
      <name>Carlo Fulvi Mari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Withdrawn by the author. Revised version published in the Journal of
  Computational Neuroscience 17: 57-79 (2004)</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0107612v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0107612v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0108072v1</id>
    <updated>2001-08-03T19:27:05Z</updated>
    <published>2001-08-03T19:27:05Z</published>
    <title>Application of the renormalization group method in wireless market
  intelligence</title>
    <summary>  We use a renormalization group method, similar to that developed for random
spin chains, to infer information about the layouts of cellular wireless
networks.
</summary>
    <author>
      <name>M. V. Simkin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Telephia Inc., San Francisco</arxiv:affiliation>
    </author>
    <author>
      <name>J. Olness</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Telephia Inc., San Francisco</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">includes a colour figure</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0108072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0108072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0507471v2</id>
    <updated>2005-08-11T09:26:27Z</updated>
    <published>2005-07-20T07:50:24Z</published>
    <title>Synchronization in Complex Networks: a Comment on two recent PRL papers</title>
    <summary>  I show that the conclusions of [Hwang, Chavez, Amann, &amp; Boccaletti, PRL 94,
138701 (2005); Chavez, Hwang, Amann, Hentschel, &amp; Boccaletti, PRL 94, 218701
(2005)] are closely related to those of previous publications.
</summary>
    <author>
      <name>Manuel A. Matias</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0507471v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0507471v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.0753v2</id>
    <updated>2008-03-15T09:06:18Z</updated>
    <published>2008-02-06T08:15:26Z</published>
    <title>Using complex networks to model 2-D and 3-D soil porous architecture</title>
    <summary>  This paper has been withdrawn by the author to comply with the journal policy
to which it has been submitted.
</summary>
    <author>
      <name>Sacha Jon Mooney</name>
    </author>
    <author>
      <name>Dean Korosak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/0802.0753v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.0753v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.08571v1</id>
    <updated>2015-12-29T01:21:08Z</updated>
    <published>2015-12-29T01:21:08Z</published>
    <title>Structured Pruning of Deep Convolutional Neural Networks</title>
    <summary>  Real time application of deep learning algorithms is often hindered by high
computational complexity and frequent memory accesses. Network pruning is a
promising technique to solve this problem. However, pruning usually results in
irregular network connections that not only demand extra representation efforts
but also do not fit well on parallel computation. We introduce structured
sparsity at various scales for convolutional neural networks, which are channel
wise, kernel wise and intra kernel strided sparsity. This structured sparsity
is very advantageous for direct computational resource savings on embedded
computers, parallel computing environments and hardware based systems. To
decide the importance of network connections and paths, the proposed method
uses a particle filtering approach. The importance weight of each particle is
assigned by computing the misclassification rate with corresponding
connectivity pattern. The pruned network is re-trained to compensate for the
losses due to pruning. While implementing convolutions as matrix products, we
particularly show that intra kernel strided sparsity with a simple constraint
can significantly reduce the size of kernel and feature map matrices. The
pruned network is finally fixed point optimized with reduced word length
precision. This results in significant reduction in the total storage size
providing advantages for on-chip memory based implementations of deep neural
networks.
</summary>
    <author>
      <name>Sajid Anwar</name>
    </author>
    <author>
      <name>Kyuyeon Hwang</name>
    </author>
    <author>
      <name>Wonyong Sung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 8 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.08571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.08571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.00768v1</id>
    <updated>2017-02-02T18:01:07Z</updated>
    <published>2017-02-02T18:01:07Z</published>
    <title>Scaling Properties of Human Brain Functional Networks</title>
    <summary>  We investigate scaling properties of human brain functional networks in the
resting-state. Analyzing network degree distributions, we statistically test
whether their tails scale as power-law or not. Initial studies, based on
least-squares fitting, were shown to be inadequate for precise estimation of
power-law distributions. Subsequently, methods based on maximum-likelihood
estimators have been proposed and applied to address this question.
Nevertheless, no clear consensus has emerged, mainly because results have shown
substantial variability depending on the data-set used or its resolution. In
this study, we work with high-resolution data (10K nodes) from the Human
Connectome Project and take into account network weights. We test for the
power-law, exponential, log-normal and generalized Pareto distributions. Our
results show that the statistics generally do not support a power-law, but
instead these degree distributions tend towards the thin-tail limit of the
generalized Pareto model. This may have implications for the number of hubs in
human brain functional networks.
</summary>
    <author>
      <name>Riccardo Zucca</name>
    </author>
    <author>
      <name>Xerxes D. Arsiwalla</name>
    </author>
    <author>
      <name>Hoang Le</name>
    </author>
    <author>
      <name>Mikail Rubinov</name>
    </author>
    <author>
      <name>Paul Verschure</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-44778-0_13</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-44778-0_13" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Artificial Neural Networks - ICANN 2016</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Artificial Neural Networks and Machine Learning, Lecture Notes in
  Computer Science, vol 9886, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1702.00768v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.00768v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.09668v2</id>
    <updated>2018-01-01T07:22:36Z</updated>
    <published>2017-10-26T12:50:45Z</published>
    <title>PDE-Net: Learning PDEs from Data</title>
    <summary>  In this paper, we present an initial attempt to learn evolution PDEs from
data. Inspired by the latest development of neural network designs in deep
learning, we propose a new feed-forward deep network, called PDE-Net, to
fulfill two objectives at the same time: to accurately predict dynamics of
complex systems and to uncover the underlying hidden PDE models. The basic idea
of the proposed PDE-Net is to learn differential operators by learning
convolution kernels (filters), and apply neural networks or other machine
learning methods to approximate the unknown nonlinear responses. Comparing with
existing approaches, which either assume the form of the nonlinear response is
known or fix certain finite difference approximations of differential
operators, our approach has the most flexibility by learning both differential
operators and the nonlinear responses. A special feature of the proposed
PDE-Net is that all filters are properly constrained, which enables us to
easily identify the governing PDE models while still maintaining the expressive
and predictive power of the network. These constrains are carefully designed by
fully exploiting the relation between the orders of differential operators and
the orders of sum rules of filters (an important concept originated from
wavelet theory). We also discuss relations of the PDE-Net with some existing
networks in computer vision such as Network-In-Network (NIN) and Residual
Neural Network (ResNet). Numerical experiments show that the PDE-Net has the
potential to uncover the hidden PDE of the observed dynamics, and predict the
dynamical behavior for a relatively long time, even in a noisy environment.
</summary>
    <author>
      <name>Zichao Long</name>
    </author>
    <author>
      <name>Yiping Lu</name>
    </author>
    <author>
      <name>Xianzhong Ma</name>
    </author>
    <author>
      <name>Bin Dong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.09668v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.09668v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.02143v1</id>
    <updated>2018-01-07T06:06:31Z</updated>
    <published>2018-01-07T06:06:31Z</published>
    <title>Deep Bidirectional and Unidirectional LSTM Recurrent Neural Network for
  Network-wide Traffic Speed Prediction</title>
    <summary>  Short-term traffic forecasting based on deep learning methods, especially
long short-term memory (LSTM) neural networks, has received much attention in
recent years. However, the potential of deep learning methods in traffic
forecasting has not yet fully been exploited in terms of the depth of the model
architecture, the spatial scale of the prediction area, and the predictive
power of spatial-temporal data. In this paper, a deep stacked bidirectional and
unidirectional LSTM (SBU- LSTM) neural network architecture is proposed, which
considers both forward and backward dependencies in time series data, to
predict network-wide traffic speed. A bidirectional LSTM (BDLSM) layer is
exploited to capture spatial features and bidirectional temporal dependencies
from historical data. To the best of our knowledge, this is the first time that
BDLSTMs have been applied as building blocks for a deep architecture model to
measure the backward dependency of traffic data for prediction. The proposed
model can handle missing values in input data by using a masking mechanism.
Further, this scalable model can predict traffic speed for both freeway and
complex urban traffic networks. Comparisons with other classical and
state-of-the-art models indicate that the proposed SBU-LSTM neural network
achieves superior prediction performance for the whole traffic network in both
accuracy and robustness.
</summary>
    <author>
      <name>Zhiyong Cui</name>
    </author>
    <author>
      <name>Ruimin Ke</name>
    </author>
    <author>
      <name>Yinhai Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Workshop on Urban Computing (UrbComp) 2017, Held in
  conjunction with the ACM SIGKDD 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.02143v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.02143v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.04396v1</id>
    <updated>2018-01-13T08:24:15Z</updated>
    <published>2018-01-13T08:24:15Z</published>
    <title>Cost-Sensitive Convolution based Neural Networks for Imbalanced
  Time-Series Classification</title>
    <summary>  Some deep convolutional neural networks were proposed for time-series
classification and class imbalanced problems. However, those models performed
degraded and even failed to recognize the minority class of an imbalanced
temporal sequences dataset. Minority samples would bring troubles for temporal
deep learning classifiers due to the equal treatments of majority and minority
class. Until recently, there were few works applying deep learning on
imbalanced time-series classification (ITSC) tasks. Here, this paper aimed at
tackling ITSC problems with deep learning. An adaptive cost-sensitive learning
strategy was proposed to modify temporal deep learning models. Through the
proposed strategy, classifiers could automatically assign misclassification
penalties to each class. In the experimental section, the proposed method was
utilized to modify five neural networks. They were evaluated on a large volume,
real-life and imbalanced time-series dataset with six metrics. Each single
network was also tested alone and combined with several mainstream data
samplers. Experimental results illustrated that the proposed cost-sensitive
modified networks worked well on ITSC tasks. Compared to other methods, the
cost-sensitive convolution neural network and residual network won out in the
terms of all metrics. Consequently, the proposed cost-sensitive learning
strategy can be used to modify deep learning classifiers from cost-insensitive
to cost-sensitive. Those cost-sensitive convolutional networks can be
effectively applied to address ITSC issues.
</summary>
    <author>
      <name>Yue Geng</name>
    </author>
    <author>
      <name>Xinyu Luo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages,12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.04396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.04396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.00387v2</id>
    <updated>2015-11-03T18:15:15Z</updated>
    <published>2015-05-03T01:56:57Z</published>
    <title>Highway Networks</title>
    <summary>  There is plenty of theoretical and empirical evidence that depth of neural
networks is a crucial ingredient for their success. However, network training
becomes more difficult with increasing depth and training of very deep networks
remains an open problem. In this extended abstract, we introduce a new
architecture designed to ease gradient-based training of very deep networks. We
refer to networks with this architecture as highway networks, since they allow
unimpeded information flow across several layers on "information highways". The
architecture is characterized by the use of gating units which learn to
regulate the flow of information through a network. Highway networks with
hundreds of layers can be trained directly using stochastic gradient descent
and with a variety of activation functions, opening up the possibility of
studying extremely deep and efficient architectures.
</summary>
    <author>
      <name>Rupesh Kumar Srivastava</name>
    </author>
    <author>
      <name>Klaus Greff</name>
    </author>
    <author>
      <name>J√ºrgen Schmidhuber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures. Presented at ICML 2015 Deep Learning workshop.
  Full paper is at arXiv:1507.06228</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.00387v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.00387v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; G.1.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08323v2</id>
    <updated>2016-11-07T12:38:17Z</updated>
    <published>2016-02-26T13:54:47Z</published>
    <title>Deep Spiking Networks</title>
    <summary>  We introduce an algorithm to do backpropagation on a spiking network. Our
network is "spiking" in the sense that our neurons accumulate their activation
into a potential over time, and only send out a signal (a "spike") when this
potential crosses a threshold and the neuron is reset. Neurons only update
their states when receiving signals from other neurons. Total computation of
the network thus scales with the number of spikes caused by an input rather
than network size. We show that the spiking Multi-Layer Perceptron behaves
identically, during both prediction and training, to a conventional deep
network of rectified-linear units, in the limiting case where we run the
spiking network for a long time. We apply this architecture to a conventional
classification problem (MNIST) and achieve performance very close to that of a
conventional Multi-Layer Perceptron with the same architecture. Our network is
a natural architecture for learning based on streaming event-based data, and is
a stepping stone towards using spiking neural networks to learn efficiently on
streaming data.
</summary>
    <author>
      <name>Peter O'Connor</name>
    </author>
    <author>
      <name>Max Welling</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages main paper + 1 page reference + 7 pages Appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.08323v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08323v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.02024v2</id>
    <updated>2016-11-10T11:02:54Z</updated>
    <published>2016-11-07T12:45:23Z</published>
    <title>Sigma Delta Quantized Networks</title>
    <summary>  Deep neural networks can be obscenely wasteful. When processing video, a
convolutional network expends a fixed amount of computation for each frame with
no regard to the similarity between neighbouring frames. As a result, it ends
up repeatedly doing very similar computations. To put an end to such waste, we
introduce Sigma-Delta networks. With each new input, each layer in this network
sends a discretized form of its change in activation to the next layer. Thus
the amount of computation that the network does scales with the amount of
change in the input and layer activations, rather than the size of the network.
We introduce an optimization method for converting any pre-trained deep network
into an optimally efficient Sigma-Delta network, and show that our algorithm,
if run on the appropriate hardware, could cut at least an order of magnitude
from the computational cost of processing video data.
</summary>
    <author>
      <name>Peter O'Connor</name>
    </author>
    <author>
      <name>Max Welling</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 Pages + 1 Reference + 3 Appendix, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.02024v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.02024v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.00372v1</id>
    <updated>2017-02-01T17:45:55Z</updated>
    <published>2017-02-01T17:45:55Z</published>
    <title>Visual Saliency Prediction Using a Mixture of Deep Neural Networks</title>
    <summary>  Visual saliency models have recently begun to incorporate deep learning to
achieve predictive capacity much greater than previous unsupervised methods.
However, most existing models predict saliency using local mechanisms limited
to the receptive field of the network. We propose a model that incorporates
global scene semantic information in addition to local information gathered by
a convolutional neural network. Our model is formulated as a mixture of
experts. Each expert network is trained to predict saliency for a set of
closely related images. The final saliency map is computed as a weighted
mixture of the expert networks' output, with weights determined by a separate
gating network. This gating network is guided by global scene information to
predict weights. The expert networks and the gating network are trained
simultaneously in an end-to-end manner. We show that our mixture formulation
leads to improvement in performance over an otherwise identical non-mixture
model that does not incorporate global scene information.
</summary>
    <author>
      <name>Samuel Dodge</name>
    </author>
    <author>
      <name>Lina Karam</name>
    </author>
    <link href="http://arxiv.org/abs/1702.00372v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.00372v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.08033v1</id>
    <updated>2017-03-23T12:19:09Z</updated>
    <published>2017-03-23T12:19:09Z</published>
    <title>Generative Adversarial Residual Pairwise Networks for One Shot Learning</title>
    <summary>  Deep neural networks achieve unprecedented performance levels over many tasks
and scale well with large quantities of data, but performance in the low-data
regime and tasks like one shot learning still lags behind. While recent work
suggests many hypotheses from better optimization to more complicated network
structures, in this work we hypothesize that having a learnable and more
expressive similarity objective is an essential missing component. Towards
overcoming that, we propose a network design inspired by deep residual networks
that allows the efficient computation of this more expressive pairwise
similarity objective. Further, we argue that regularization is key in learning
with small amounts of data, and propose an additional generator network based
on the Generative Adversarial Networks where the discriminator is our residual
pairwise network. This provides a strong regularizer by leveraging the
generated data samples. The proposed model can generate plausible variations of
exemplars over unseen classes and outperforms strong discriminative baselines
for few shot classification tasks. Notably, our residual pairwise network
design outperforms previous state-of-theart on the challenging mini-Imagenet
dataset for one shot learning by getting over 55% accuracy for the 5-way
classification task over unseen classes.
</summary>
    <author>
      <name>Akshay Mehrotra</name>
    </author>
    <author>
      <name>Ambedkar Dukkipati</name>
    </author>
    <link href="http://arxiv.org/abs/1703.08033v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.08033v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.08119v1</id>
    <updated>2017-03-23T15:56:33Z</updated>
    <published>2017-03-23T15:56:33Z</published>
    <title>Quality Resilient Deep Neural Networks</title>
    <summary>  We study deep neural networks for classification of images with quality
distortions. We first show that networks fine-tuned on distorted data greatly
outperform the original networks when tested on distorted data. However,
fine-tuned networks perform poorly on quality distortions that they have not
been trained for. We propose a mixture of experts ensemble method that is
robust to different types of distortions. The "experts" in our model are
trained on a particular type of distortion. The output of the model is a
weighted sum of the expert models, where the weights are determined by a
separate gating network. The gating network is trained to predict optimal
weights for a particular distortion type and level. During testing, the network
is blind to the distortion level and type, yet can still assign appropriate
weights to the expert models. We additionally investigate weight sharing
methods for the mixture model and show that improved performance can be
achieved with a large reduction in the number of unique network parameters.
</summary>
    <author>
      <name>Samuel Dodge</name>
    </author>
    <author>
      <name>Lina Karam</name>
    </author>
    <link href="http://arxiv.org/abs/1703.08119v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.08119v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.08847v2</id>
    <updated>2017-05-02T01:11:21Z</updated>
    <published>2017-04-28T08:43:55Z</published>
    <title>Parseval Networks: Improving Robustness to Adversarial Examples</title>
    <summary>  We introduce Parseval networks, a form of deep neural networks in which the
Lipschitz constant of linear, convolutional and aggregation layers is
constrained to be smaller than 1. Parseval networks are empirically and
theoretically motivated by an analysis of the robustness of the predictions
made by deep neural networks when their input is subject to an adversarial
perturbation. The most important feature of Parseval networks is to maintain
weight matrices of linear and convolutional layers to be (approximately)
Parseval tight frames, which are extensions of orthogonal matrices to
non-square matrices. We describe how these constraints can be maintained
efficiently during SGD. We show that Parseval networks match the
state-of-the-art in terms of accuracy on CIFAR-10/100 and Street View House
Numbers (SVHN) while being more robust than their vanilla counterpart against
adversarial examples. Incidentally, Parseval networks also tend to train faster
and make a better usage of the full capacity of the networks.
</summary>
    <author>
      <name>Moustapha Cisse</name>
    </author>
    <author>
      <name>Piotr Bojanowski</name>
    </author>
    <author>
      <name>Edouard Grave</name>
    </author>
    <author>
      <name>Yann Dauphin</name>
    </author>
    <author>
      <name>Nicolas Usunier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.08847v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.08847v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.05340v1</id>
    <updated>2017-09-15T01:48:20Z</updated>
    <published>2017-09-15T01:48:20Z</published>
    <title>Dynamic Capacity Estimation in Hopfield Networks</title>
    <summary>  Understanding the memory capacity of neural networks remains a challenging
problem in implementing artificial intelligence systems. In this paper, we
address the notion of capacity with respect to Hopfield networks and propose a
dynamic approach to monitoring a network's capacity. We define our
understanding of capacity as the maximum number of stored patterns which can be
retrieved when probed by the stored patterns. Prior work in this area has
presented static expressions dependent on neuron count $N$, forcing network
designers to assume worst-case input characteristics for bias and correlation
when setting the capacity of the network. Instead, our model operates
simultaneously with the learning Hopfield network and concludes on a capacity
estimate based on the patterns which were stored. By continuously updating the
crosstalk associated with the stored patterns, our model guards the network
from overwriting its memory traces and exceeding its capacity. We simulate our
model using artificially generated random patterns, which can be set to a
desired bias and correlation, and observe capacity estimates between 93% and
97% accurate. As a result, our model doubles the memory efficiency of Hopfield
networks in comparison to the static and worst-case capacity estimate while
minimizing the risk of lost patterns.
</summary>
    <author>
      <name>Saarthak Sarup</name>
    </author>
    <author>
      <name>Mingoo Seok</name>
    </author>
    <link href="http://arxiv.org/abs/1709.05340v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.05340v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.3.2; C.1.3; H.3.3; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.03112v1</id>
    <updated>2017-10-09T14:16:00Z</updated>
    <published>2017-10-09T14:16:00Z</published>
    <title>Handwritten digit string recognition by combination of residual network
  and RNN-CTC</title>
    <summary>  Recurrent neural network (RNN) and connectionist temporal classification
(CTC) have showed successes in many sequence labeling tasks with the strong
ability of dealing with the problems where the alignment between the inputs and
the target labels is unknown. Residual network is a new structure of
convolutional neural network and works well in various computer vision tasks.
In this paper, we take advantage of the architectures mentioned above to create
a new network for handwritten digit string recognition. First we design a
residual network to extract features from input images, then we employ a RNN to
model the contextual information within feature sequences and predict
recognition results. At the top of this network, a standard CTC is applied to
calculate the loss and yield the final results. These three parts compose an
end-to-end trainable network. The proposed new architecture achieves the
highest performances on ORAND-CAR-A and ORAND-CAR-B with recognition rates
89.75% and 91.14%, respectively. In addition, the experiments on a generated
captcha dataset which has much longer string length show the potential of the
proposed network to handle long strings.
</summary>
    <author>
      <name>Hongjian Zhan</name>
    </author>
    <author>
      <name>Qingqing Wang</name>
    </author>
    <author>
      <name>Yue Lu</name>
    </author>
    <link href="http://arxiv.org/abs/1710.03112v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.03112v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.09687v1</id>
    <updated>2017-10-26T13:36:07Z</updated>
    <published>2017-10-26T13:36:07Z</published>
    <title>Quantifying Transient Spreading Dynamics on Networks</title>
    <summary>  Spreading phenomena on networks are essential for the collective dynamics of
various natural and technological systems, from information spreading in gene
regulatory networks to neural circuits or from epidemics to supply networks
experiencing perturbations. Still, how local disturbances spread across
networks is not yet quantitatively understood. Here we analyze generic
spreading dynamics in deterministic network dynamical systems close to a given
operating point. Standard dynamical systems' theory does not explicitly provide
measures for arrival times and amplitudes of a transient, spreading signal
because it focuses on invariant sets, invariant measures and other quantities
less relevant for transient behavior. We here change the perspective and
introduce effective expectation values for deterministic dynamics to work out a
theory explicitly quantifying when and how strongly a perturbation initiated at
one unit of a network impacts any other. The theory provides explicit timing
and amplitude information as a function of the relative position of initially
perturbed and responding unit as well as on the entire network topology.
</summary>
    <author>
      <name>Justine Wolter</name>
    </author>
    <author>
      <name>Benedict L√ºnsmann</name>
    </author>
    <author>
      <name>Xiaozhu Zhang</name>
    </author>
    <author>
      <name>Malte Schr√∂der</name>
    </author>
    <author>
      <name>Marc Timme</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages and 4 figures main manuscript 9 pages and 3 figures appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.09687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.09687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.07938v1</id>
    <updated>2018-01-24T11:45:20Z</updated>
    <published>2018-01-24T11:45:20Z</published>
    <title>Navigation of brain networks</title>
    <summary>  Understanding the mechanisms of neural communication in large-scale brain
networks remains a major goal in neuroscience. We investigated whether
navigation is a parsimonious routing model for connectomics. Navigating a
network involves progressing to the next node that is closest in distance to a
desired destination. We developed a measure to quantify navigation efficiency
and found that connectomes in a range of mammalian species (human, mouse and
macaque) can be successfully navigated with near-optimal efficiency (&gt;80% of
optimal efficiency for typical connection densities). Rewiring network topology
or repositioning network nodes resulted in 45%-60% reductions in navigation
performance. Specifically, we found that brain networks cannot be progressively
rewired (randomized or clusterized) to result in topologies with significantly
improved navigation performance. Navigation was also found to: i) promote a
resource-efficient distribution of the information traffic load, potentially
relieving communication bottlenecks; and, ii) explain significant variation in
functional connectivity. Unlike prevalently studied communication strategies in
connectomics, navigation does not mandate biologically unrealistic assumptions
about global knowledge of network topology. We conclude that the wiring and
spatial embedding of brain networks is conducive to effective decentralized
communication. Graph-theoretic studies of the connectome should consider
measures of network efficiency and centrality that are consistent with
decentralized models of neural communication.
</summary>
    <author>
      <name>Caio Seguin</name>
    </author>
    <author>
      <name>Martijn P. van den Heuvel</name>
    </author>
    <author>
      <name>Andrew Zalesky</name>
    </author>
    <link href="http://arxiv.org/abs/1801.07938v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.07938v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0405012v1</id>
    <updated>2004-05-05T00:36:17Z</updated>
    <published>2004-05-05T00:36:17Z</published>
    <title>Is Neural Network a Reliable Forecaster on Earth? A MARS Query!</title>
    <summary>  Long-term rainfall prediction is a challenging task especially in the modern
world where we are facing the major environmental problem of global warming. In
general, climate and rainfall are highly non-linear phenomena in nature
exhibiting what is known as the butterfly effect. While some regions of the
world are noticing a systematic decrease in annual rainfall, others notice
increases in flooding and severe storms. The global nature of this phenomenon
is very complicated and requires sophisticated computer modeling and simulation
to predict accurately. In this paper, we report a performance analysis for
Multivariate Adaptive Regression Splines (MARS)and artificial neural networks
for one month ahead prediction of rainfall. To evaluate the prediction
efficiency, we made use of 87 years of rainfall data in Kerala state, the
southern part of the Indian peninsula situated at latitude -longitude pairs
(8o29'N - 76o57' E). We used an artificial neural network trained using the
scaled conjugate gradient algorithm. The neural network and MARS were trained
with 40 years of rainfall data. For performance evaluation, network predicted
outputs were compared with the actual rainfall data. Simulation results reveal
that MARS is a good forecasting tool and performed better than the considered
neural network.
</summary>
    <author>
      <name>Ajith Abraham</name>
    </author>
    <author>
      <name>Dan Steinberg</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Bio-Inspired Applications of Connectionism, Lecture Notes in
  Computer Science. Volume. 2085, Springer Verlag Germany, Jose Mira and
  Alberto Prieto (Eds.), ISBN 3540422374, Spain, pp.679-686, 2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0405012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0405012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/nucl-th/0701096v1</id>
    <updated>2007-01-31T15:05:23Z</updated>
    <published>2007-01-31T15:05:23Z</published>
    <title>A Global Model of $Œ≤^-$-Decay Half-Lives Using Neural Networks</title>
    <summary>  Statistical modeling of nuclear data using artificial neural networks (ANNs)
and, more recently, support vector machines (SVMs), is providing novel
approaches to systematics that are complementary to phenomenological and
semi-microscopic theories. We present a global model of $\beta^-$-decay
halflives of the class of nuclei that decay 100% by $\beta^-$ mode in their
ground states. A fully-connected multilayered feed forward network has been
trained using the Levenberg-Marquardt algorithm, Bayesian regularization, and
cross-validation. The halflife estimates generated by the model are discussed
and compared with the available experimental data, with previous results
obtained with neural networks, and with estimates coming from traditional
global nuclear models. Predictions of the new neural-network model are given
for nuclei far from stability, with particular attention to those involved in
r-process nucleosynthesis. This study demonstrates that in the framework of the
$\beta^-$-decay problem considered here, global models based on ANNs can at
least match the predictive performance of the best conventional global models
rooted in nuclear theory. Accordingly, such statistical models can provide a
valuable tool for further mapping of the nuclidic chart.
</summary>
    <author>
      <name>N. Costiris</name>
    </author>
    <author>
      <name>E. Mavrommatis</name>
    </author>
    <author>
      <name>K. A. Gernoth</name>
    </author>
    <author>
      <name>J. W. Clark</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 16th Panhellenic Symposium of the Hellenic Nuclear
  Physics Society</arxiv:comment>
    <link href="http://arxiv.org/abs/nucl-th/0701096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/nucl-th/0701096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nucl-th" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nucl-th" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.6960v1</id>
    <updated>2013-04-25T16:56:29Z</updated>
    <published>2013-04-25T16:56:29Z</published>
    <title>Metastability in a stochastic neural network modeled as a velocity jump
  Markov process</title>
    <summary>  One of the major challenges in neuroscience is to determine how noise that is
present at the molecular and cellular levels affects dynamics and information
processing at the macroscopic level of synaptically coupled neuronal
populations. Often noise is incorprated into deterministic network models using
extrinsic noise sources. An alternative approach is to assume that noise arises
intrinsically as a collective population effect, which has led to a master
equation formulation of stochastic neural networks. In this paper we extend the
master equation formulation by introducing a stochastic model of neural
population dynamics in the form of a velocity jump Markov process. The latter
has the advantage of keeping track of synaptic processing as well as spiking
activity, and reduces to the neural master equation in a particular limit. The
population synaptic variables evolve according to piecewise deterministic
dynamics, which depends on population spiking activity. The latter is
characterised by a set of discrete stochastic variables evolving according to a
jump Markov process, with transition rates that depend on the synaptic
variables. We consider the particular problem of rare transitions between
metastable states of a network operating in a bistable regime in the
deterministic limit. Assuming that the synaptic dynamics is much slower than
the transitions between discrete spiking states, we use a WKB approximation and
singular perturbation theory to determine the mean first passage time to cross
the separatrix between the two metastable states. Such an analysis can also be
applied to other velocity jump Markov processes, including stochastic
voltage-gated ion channels and stochastic gene networks.
</summary>
    <author>
      <name>Paul C. Bressloff</name>
    </author>
    <author>
      <name>Jay M. Newby</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1137/120898978</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1137/120898978" rel="related"/>
    <link href="http://arxiv.org/abs/1304.6960v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.6960v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.6878v2</id>
    <updated>2015-03-03T09:06:44Z</updated>
    <published>2014-10-25T03:54:12Z</published>
    <title>Application of Artificial Neural Network to Search for
  Gravitational-Wave Signals Associated with Short Gamma-Ray Bursts</title>
    <summary>  We apply a machine learning algorithm, the artificial neural network, to the
search for gravitational-wave signals associated with short gamma-ray bursts.
The multi-dimensional samples consisting of data corresponding to the
statistical and physical quantities from the coherent search pipeline are fed
into the artificial neural network to distinguish simulated gravitational-wave
signals from background noise artifacts. Our result shows that the data
classification efficiency at a fixed false alarm probability is improved by the
artificial neural network in comparison to the conventional detection
statistic. Therefore, this algorithm increases the distance at which a
gravitational-wave signal could be observed in coincidence with a gamma-ray
burst. In order to demonstrate the performance, we also evaluate a few seconds
of gravitational-wave data segment using the trained networks and obtain the
false alarm probability. We suggest that the artificial neural network can be a
complementary method to the conventional detection statistic for identifying
gravitational-wave signals related to the short gamma-ray bursts.
</summary>
    <author>
      <name>Kyungmin Kim</name>
    </author>
    <author>
      <name>Ian W. Harry</name>
    </author>
    <author>
      <name>Kari A. Hodge</name>
    </author>
    <author>
      <name>Young-Min Kim</name>
    </author>
    <author>
      <name>Chang-Hwan Lee</name>
    </author>
    <author>
      <name>Hyun Kyu Lee</name>
    </author>
    <author>
      <name>John J. Oh</name>
    </author>
    <author>
      <name>Sang Hoon Oh</name>
    </author>
    <author>
      <name>Edwin J. Son</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/0264-9381/32/24/245002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/0264-9381/32/24/245002" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 10 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Class. Quantum Grav. 32 (2015) 245002</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1410.6878v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.6878v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.3246v1</id>
    <updated>2010-01-19T10:28:11Z</updated>
    <published>2010-01-19T10:28:11Z</published>
    <title>Salience-Affected Neural Networks</title>
    <summary>  We present a simple neural network model which combines a locally-connected
feedforward structure, as is traditionally used to model inter-neuron
connectivity, with a layer of undifferentiated connections which model the
diffuse projections from the human limbic system to the cortex. This new layer
makes it possible to model global effects such as salience, at the same time as
the local network processes task-specific or local information. This simple
combination network displays interactions between salience and regular
processing which correspond to known effects in the developing brain, such as
enhanced learning as a result of heightened affect.
  The cortex biases neuronal responses to affect both learning and memory,
through the use of diffuse projections from the limbic system to the cortex.
Standard ANNs do not model this non-local flow of information represented by
the ascending systems, which are a significant feature of the structure of the
brain, and although they do allow associational learning with multiple-trial,
they simply don't provide the capacity for one-time learning.
  In this research we model this effect using an artificial neural network
(ANN), creating a salience-affected neural network (SANN). We adapt an ANN to
embody the capacity to respond to an input salience signal and to produce a
reverse salience signal during testing.
  This research demonstrates that input combinations similar to the inputs in
the training data sets will produce similar reverse salience signals during
testing. Furthermore, this research has uncovered a novel method for training
ANNs with a single training iteration.
</summary>
    <author>
      <name>Leendert A. Remmelzwaal</name>
    </author>
    <author>
      <name>Jonathan Tapson</name>
    </author>
    <author>
      <name>George F. R. Ellis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.3246v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.3246v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.00317v1</id>
    <updated>2015-08-03T05:58:52Z</updated>
    <published>2015-08-03T05:58:52Z</published>
    <title>Time-series modeling with undecimated fully convolutional neural
  networks</title>
    <summary>  We present a new convolutional neural network-based time-series model.
Typical convolutional neural network (CNN) architectures rely on the use of
max-pooling operators in between layers, which leads to reduced resolution at
the top layers. Instead, in this work we consider a fully convolutional network
(FCN) architecture that uses causal filtering operations, and allows for the
rate of the output signal to be the same as that of the input signal. We
furthermore propose an undecimated version of the FCN, which we refer to as the
undecimated fully convolutional neural network (UFCNN), and is motivated by the
undecimated wavelet transform. Our experimental results verify that using the
undecimated version of the FCN is necessary in order to allow for effective
time-series modeling. The UFCNN has several advantages compared to other
time-series models such as the recurrent neural network (RNN) and long
short-term memory (LSTM), since it does not suffer from either the vanishing or
exploding gradients problems, and is therefore easier to train. Convolution
operations can also be implemented more efficiently compared to the recursion
that is involved in RNN-based models. We evaluate the performance of our model
in a synthetic target tracking task using bearing only measurements generated
from a state-space model, a probabilistic modeling of polyphonic music
sequences problem, and a high frequency trading task using a time-series of
ask/bid quotes and their corresponding volumes. Our experimental results using
synthetic and real datasets verify the significant advantages of the UFCNN
compared to the RNN and LSTM baselines.
</summary>
    <author>
      <name>Roni Mittelman</name>
    </author>
    <link href="http://arxiv.org/abs/1508.00317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.00317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.03371v1</id>
    <updated>2015-09-11T01:20:46Z</updated>
    <published>2015-09-11T01:20:46Z</published>
    <title>Efficient Convolutional Neural Networks for Pixelwise Classification on
  Heterogeneous Hardware Systems</title>
    <summary>  This work presents and analyzes three convolutional neural network (CNN)
models for efficient pixelwise classification of images. When using
convolutional neural networks to classify single pixels in patches of a whole
image, a lot of redundant computations are carried out when using sliding
window networks. This set of new architectures solve this issue by either
removing redundant computations or using fully convolutional architectures that
inherently predict many pixels at once.
  The implementations of the three models are accessible through a new utility
on top of the Caffe library. The utility provides support for a wide range of
image input and output formats, pre-processing parameters and methods to
equalize the label histogram during training. The Caffe library has been
extended by new layers and a new backend for availability on a wider range of
hardware such as CPUs and GPUs through OpenCL.
  On AMD GPUs, speedups of $54\times$ (SK-Net), $437\times$ (U-Net) and
$320\times$ (USK-Net) have been observed, taking the SK equivalent SW (sliding
window) network as the baseline. The label throughput is up to one megapixel
per second.
  The analyzed neural networks have distinctive characteristics that apply
during training or processing, and not every data set is suitable to every
architecture. The quality of the predictions is assessed on two neural tissue
data sets, of which one is the ISBI 2012 challenge data set. Two different loss
functions, Malis loss and Softmax loss, were used during training.
  The whole pipeline, consisting of models, interface and modified Caffe
library, is available as Open Source software under the working title Project
Greentea.
</summary>
    <author>
      <name>Fabian Tschopp</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">92 pages, project source code available at
  https://github.com/naibaf7/, technical report written at ETH Z\"urich, in
  collaboration with AMD, UZH INI and HHMI Janelia</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.03371v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.03371v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.01952v1</id>
    <updated>2016-04-07T10:46:54Z</updated>
    <published>2016-04-07T10:46:54Z</published>
    <title>Deep Online Convex Optimization with Gated Games</title>
    <summary>  Methods from convex optimization are widely used as building blocks for deep
learning algorithms. However, the reasons for their empirical success are
unclear, since modern convolutional networks (convnets), incorporating
rectifier units and max-pooling, are neither smooth nor convex. Standard
guarantees therefore do not apply. This paper provides the first convergence
rates for gradient descent on rectifier convnets. The proof utilizes the
particular structure of rectifier networks which consists in binary
active/inactive gates applied on top of an underlying linear network. The
approach generalizes to max-pooling, dropout and maxout. In other words, to
precisely the neural networks that perform best empirically. The key step is to
introduce gated games, an extension of convex games with similar convergence
properties that capture the gating function of rectifiers. The main result is
that rectifier convnets converge to a critical point at a rate controlled by
the gated-regret of the units in the network. Corollaries of the main result
include: (i) a game-theoretic description of the representations learned by a
neural network; (ii) a logarithmic-regret algorithm for training neural nets;
and (iii) a formal setting for analyzing conditional computation in neural nets
that can be applied to recently developed models of attention.
</summary>
    <author>
      <name>David Balduzzi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages. This paper renders arXiv:1509.01851 obsolete. It contains
  the same basic results, with major changes to exposition and minor changes to
  terminology</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.01952v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.01952v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06758v2</id>
    <updated>2016-06-15T07:44:56Z</updated>
    <published>2016-05-22T08:31:39Z</published>
    <title>Functional methods for disordered neural networks</title>
    <summary>  Neural networks of the brain form one of the most complex systems we know.
Many qualitative features of the emerging collective phenomena, such as
correlated activity, stability, response to inputs, chaotic and regular
behavior, can, however, be understood in simple models that are accessible to a
treatment in statistical mechanics, or, more precisely, classical statistical
field theory.
  This tutorial presents the fundamentals behind contemporary developments in
the theory of neural networks of rate units that are based on methods from
statistical mechanics of classical systems with a large number of interacting
degrees of freedom. In particular we will focus on a relevant class of systems
that have quenched (time independent) disorder. In neural networks, the main
source of disorder arises from random synaptic couplings between neurons. These
systems are in many respects similar to spin glasses. The tutorial therefore
also explains the methods for these disordered systems as far as they are
applied in neuroscience.
  The presentation consists of two parts. In the first part we introduce
stochastic differential equations in the Martin - Siggia - Rose - De Dominicis
- Janssen path integral formalism. In the second part we employ this language
to derive the dynamic mean-field theory for deterministic random networks, the
basis of the seminal work by Sompolinsky, Crisanti, Sommers 1988, as well as a
recent extension to stochastic dynamics.
</summary>
    <author>
      <name>Jannis Schuecker</name>
    </author>
    <author>
      <name>Sven Goedeke</name>
    </author>
    <author>
      <name>David Dahmen</name>
    </author>
    <author>
      <name>Moritz Helias</name>
    </author>
    <link href="http://arxiv.org/abs/1605.06758v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06758v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.01926v1</id>
    <updated>2016-09-07T10:44:28Z</updated>
    <published>2016-09-07T10:44:28Z</published>
    <title>A modular architecture for transparent computation in Recurrent Neural
  Networks</title>
    <summary>  Computation is classically studied in terms of automata, formal languages and
algorithms; yet, the relation between neural dynamics and symbolic
representations and operations is still unclear in traditional eliminative
connectionism. Therefore, we suggest a unique perspective on this central
issue, to which we would like to refer as to transparent connectionism, by
proposing accounts of how symbolic computation can be implemented in neural
substrates. In this study we first introduce a new model of dynamics on a
symbolic space, the versatile shift, showing that it supports the real-time
simulation of a range of automata. We then show that the Goedelization of
versatile shifts defines nonlinear dynamical automata, dynamical systems
evolving on a vectorial space. Finally, we present a mapping between nonlinear
dynamical automata and recurrent artificial neural networks. The mapping
defines an architecture characterized by its granular modularity, where data,
symbolic operations and their control are not only distinguishable in
activation space, but also spatially localizable in the network itself, while
maintaining a distributed encoding of symbolic representations. The resulting
networks simulate automata in real-time and are programmed directly, in absence
of network training. To discuss the unique characteristics of the architecture
and their consequences, we present two examples: i) the design of a Central
Pattern Generator from a finite-state locomotive controller, and ii) the
creation of a network simulating a system of interactive automata that supports
the parsing of garden-path sentences as investigated in psycholinguistics
experiments.
</summary>
    <author>
      <name>Giovanni Sirio Carmantini</name>
    </author>
    <author>
      <name>Peter beim Graben</name>
    </author>
    <author>
      <name>Mathieu Desroches</name>
    </author>
    <author>
      <name>Serafim Rodrigues</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neunet.2016.09.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neunet.2016.09.001" rel="related"/>
    <link href="http://arxiv.org/abs/1609.01926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.01926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.05812v4</id>
    <updated>2017-04-25T19:48:41Z</updated>
    <published>2016-10-18T22:06:01Z</published>
    <title>Small-footprint Highway Deep Neural Networks for Speech Recognition</title>
    <summary>  State-of-the-art speech recognition systems typically employ neural network
acoustic models. However, compared to Gaussian mixture models, deep neural
network (DNN) based acoustic models often have many more model parameters,
making it challenging for them to be deployed on resource-constrained
platforms, such as mobile devices. In this paper, we study the application of
the recently proposed highway deep neural network (HDNN) for training
small-footprint acoustic models. HDNNs are a depth-gated feedforward neural
network, which include two types of gate functions to facilitate the
information flow through different layers. Our study demonstrates that HDNNs
are more compact than regular DNNs for acoustic modeling, i.e., they can
achieve comparable recognition accuracy with many fewer model parameters.
Furthermore, HDNNs are more controllable than DNNs: the gate functions of an
HDNN can control the behavior of the whole network using a very small number of
model parameters. Finally, we show that HDNNs are more adaptable than DNNs. For
example, simply updating the gate functions using adaptation data can result in
considerable gains in accuracy. We demonstrate these aspects by experiments
using the publicly available AMI corpus, which has around 80 hours of training
data.
</summary>
    <author>
      <name>Liang Lu</name>
    </author>
    <author>
      <name>Steve Renals</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASLP.2017.2698723</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASLP.2017.2698723" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures. Accepted to IEEE/ACM Transactions on Audio,
  Speech and Language Processing, 2017. arXiv admin note: text overlap with
  arXiv:1608.00892, arXiv:1607.01963</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.05812v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.05812v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.02666v1</id>
    <updated>2016-12-05T20:25:29Z</updated>
    <published>2016-12-05T20:25:29Z</published>
    <title>Evaluating the Performance of ANN Prediction System at Shanghai Stock
  Market in the Period 21-Sep-2016 to 11-Oct-2016</title>
    <summary>  This research evaluates the performance of an Artificial Neural Network based
prediction system that was employed on the Shanghai Stock Exchange for the
period 21-Sep-2016 to 11-Oct-2016. It is a follow-up to a previous paper in
which the prices were predicted and published before September 21. Stock market
price prediction remains an important quest for investors and researchers. This
research used an Artificial Intelligence system, being an Artificial Neural
Network that is feedforward multi-layer perceptron with error backpropagation
for prediction, unlike other methods such as technical, fundamental or time
series analysis. While these alternative methods tend to guide on trends and
not the exact likely prices, neural networks on the other hand have the ability
to predict the real value prices, as was done on this research. Nonetheless,
determination of suitable network parameters remains a challenge in neural
network design, with this research settling on a configuration of 5:21:21:1
with 80% training data or 4-year of training data as a good enough model for
stock prediction, as already determined in a previous research by the author.
The comparative results indicate that neural network can predict typical stock
market prices with mean absolute percentage errors that are as low as 1.95%
over the ten prediction instances that was studied in this research.
</summary>
    <author>
      <name>Barack Wamkaya Wanjawa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 4 figures, 8 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.02666v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.02666v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.08635v1</id>
    <updated>2017-02-28T03:52:06Z</updated>
    <published>2017-02-28T03:52:06Z</published>
    <title>Learning What Data to Learn</title>
    <summary>  Machine learning is essentially the sciences of playing with data. An
adaptive data selection strategy, enabling to dynamically choose different data
at various training stages, can reach a more effective model in a more
efficient way. In this paper, we propose a deep reinforcement learning
framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter}
(\textbf{NDF}), to explore automatic and adaptive data selection in the
training process. In particular, NDF takes advantage of a deep neural network
to adaptively select and filter important data instances from a sequential
stream of training data, such that the future accumulative reward (e.g., the
convergence speed) is maximized. In contrast to previous studies in data
selection that is mainly based on heuristic strategies, NDF is quite generic
and thus can be widely suitable for many machine learning tasks. Taking neural
network training with stochastic gradient descent (SGD) as an example,
comprehensive experiments with respect to various neural network modeling
(e.g., multi-layer perceptron networks, convolutional neural networks and
recurrent neural networks) and several applications (e.g., image classification
and text understanding) demonstrate that NDF powered SGD can achieve comparable
accuracy with standard SGD process by using less data and fewer iterations.
</summary>
    <author>
      <name>Yang Fan</name>
    </author>
    <author>
      <name>Fei Tian</name>
    </author>
    <author>
      <name>Tao Qin</name>
    </author>
    <author>
      <name>Jiang Bian</name>
    </author>
    <author>
      <name>Tie-Yan Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A preliminary version will appear in ICLR 2017, workshop track.
  https://openreview.net/forum?id=SyJNmVqgg&amp;noteId=SyJNmVqgg</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.08635v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.08635v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.04238v4</id>
    <updated>2018-01-05T12:56:42Z</updated>
    <published>2017-04-13T15:52:14Z</published>
    <title>A dynamic connectome supports the emergence of stable computational
  function of neural circuits through reward-based learning</title>
    <summary>  Synaptic connections between neurons in the brain are dynamic because of
continuously ongoing spine dynamics, axonal sprouting, and other processes. In
fact, it was recently shown that the spontaneous synapse-autonomous component
of spine dynamics is at least as large as the component that depends on the
history of pre- and postsynaptic neural activity. These data are inconsistent
with common models for network plasticity, and raise the questions how neural
circuits can maintain a stable computational function in spite of these
continuously ongoing processes, and what functional uses these ongoing
processes might have. Here, we present a rigorous theoretical framework for
these seemingly stochastic spine dynamics and rewiring processes in the context
of reward-based learning tasks. We show that spontaneous synapse-autonomous
processes, in combination with reward signals such as dopamine, can explain the
capability of networks of neurons in the brain to configure themselves for
specific computational tasks, and to compensate automatically for later changes
in the network or task. Furthermore we show theoretically and through computer
simulations that stable computational performance is compatible with
continuously ongoing synapse-autonomous changes. After reaching good
computational performance it causes primarily a slow drift of network
architecture and dynamics in task-irrelevant dimensions, as observed for neural
activity in motor cortex and other areas. On the more abstract level of
reinforcement learning the resulting model gives rise to an understanding of
reward-driven network plasticity as continuous sampling of network
configurations.
</summary>
    <author>
      <name>David Kappel</name>
    </author>
    <author>
      <name>Robert Legenstein</name>
    </author>
    <author>
      <name>Stefan Habenschuss</name>
    </author>
    <author>
      <name>Michael Hsieh</name>
    </author>
    <author>
      <name>Wolfgang Maass</name>
    </author>
    <link href="http://arxiv.org/abs/1704.04238v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.04238v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.02901v2</id>
    <updated>2018-01-13T19:38:21Z</updated>
    <published>2017-06-07T15:17:21Z</published>
    <title>Characterizing Types of Convolution in Deep Convolutional Recurrent
  Neural Networks for Robust Speech Emotion Recognition</title>
    <summary>  Deep convolutional neural networks are being actively investigated in a wide
range of speech and audio processing applications including speech recognition,
audio event detection and computational paralinguistics, owing to their ability
to reduce factors of variations, for learning from speech. However, studies
have suggested to favor a certain type of convolutional operations when
building a deep convolutional neural network for speech applications although
there has been promising results using different types of convolutional
operations. In this work, we study four types of convolutional operations on
different input features for speech emotion recognition under noisy and clean
conditions in order to derive a comprehensive understanding. Since affective
behavioral information has been shown to reflect temporally varying of mental
state and convolutional operation are applied locally in time, all deep neural
networks share a deep recurrent sub-network architecture for further temporal
modeling. We present detailed quantitative module-wise performance analysis to
gain insights into information flows within the proposed architectures. In
particular, we demonstrate the interplay of affective information and the other
irrelevant information during the progression from one module to another.
Finally we show that all of our deep neural networks provide state-of-the-art
performance on the eNTERFACE'05 corpus.
</summary>
    <author>
      <name>Che-Wei Huang</name>
    </author>
    <author>
      <name>Shrikanth. S. Narayanan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Revised Submission to IEEE Transactions</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.02901v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.02901v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07881v1</id>
    <updated>2017-06-23T22:56:40Z</updated>
    <published>2017-06-23T22:56:40Z</published>
    <title>On Sampling Strategies for Neural Network-based Collaborative Filtering</title>
    <summary>  Recent advances in neural networks have inspired people to design hybrid
recommendation algorithms that can incorporate both (1) user-item interaction
information and (2) content information including image, audio, and text.
Despite their promising results, neural network-based recommendation algorithms
pose extensive computational costs, making it challenging to scale and improve
upon. In this paper, we propose a general neural network-based recommendation
framework, which subsumes several existing state-of-the-art recommendation
algorithms, and address the efficiency issue by investigating sampling
strategies in the stochastic gradient descent training for the framework. We
tackle this issue by first establishing a connection between the loss functions
and the user-item interaction bipartite graph, where the loss function terms
are defined on links while major computation burdens are located at nodes. We
call this type of loss functions "graph-based" loss functions, for which varied
mini-batch sampling strategies can have different computational costs. Based on
the insight, three novel sampling strategies are proposed, which can
significantly improve the training efficiency of the proposed framework (up to
$\times 30$ times speedup in our experiments), as well as improving the
recommendation performance. Theoretical analysis is also provided for both the
computational cost and the convergence. We believe the study of sampling
strategies have further implications on general graph-based loss functions, and
would also enable more research under the neural network-based recommendation
framework.
</summary>
    <author>
      <name>Ting Chen</name>
    </author>
    <author>
      <name>Yizhou Sun</name>
    </author>
    <author>
      <name>Yue Shi</name>
    </author>
    <author>
      <name>Liangjie Hong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a longer version (with supplementary attached) of the KDD'17
  paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.07881v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07881v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.04780v1</id>
    <updated>2017-07-15T19:46:25Z</updated>
    <published>2017-07-15T19:46:25Z</published>
    <title>Evolutionary Training of Sparse Artificial Neural Networks: A Network
  Science Perspective</title>
    <summary>  Through the success of deep learning, Artificial Neural Networks (ANNs) are
among the most used artificial intelligence methods nowadays. ANNs have led to
major breakthroughs in various domains, such as particle physics, reinforcement
learning, speech recognition, computer vision, and so on. Taking inspiration
from the network properties of biological neural networks (e.g. sparsity,
scale-freeness), we argue that (contrary to general practice) Artificial Neural
Networks (ANN), too, should not have fully-connected layers. We show how ANNs
perform perfectly well with sparsely-connected layers. Following a Darwinian
evolutionary approach, we propose a novel algorithm which evolves an initial
random sparse topology (i.e. an Erd\H{o}s-R\'enyi random graph) of two
consecutive layers of neurons into a scale-free topology, during the ANN
training process. The resulting sparse layers can safely replace the
corresponding fully-connected layers. Our method allows to quadratically reduce
the number of parameters in the fully conencted layers of ANNs, yielding
quadratically faster computational times in both phases (i.e. training and
inference), at no decrease in accuracy. We demonstrate our claims on two
popular ANN types (restricted Boltzmann machine and multi-layer perceptron), on
two types of tasks (supervised and unsupervised learning), and on 14 benchmark
datasets. We anticipate that our approach will enable ANNs having billions of
neurons and evolved topologies to be capable of handling complex real-world
tasks that are intractable using state-of-the-art methods.
</summary>
    <author>
      <name>Decebal Constantin Mocanu</name>
    </author>
    <author>
      <name>Elena Mocanu</name>
    </author>
    <author>
      <name>Peter Stone</name>
    </author>
    <author>
      <name>Phuong H. Nguyen</name>
    </author>
    <author>
      <name>Madeleine Gibescu</name>
    </author>
    <author>
      <name>Antonio Liotta</name>
    </author>
    <link href="http://arxiv.org/abs/1707.04780v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.04780v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.03740v3</id>
    <updated>2018-02-15T20:04:02Z</updated>
    <published>2017-10-10T17:42:04Z</published>
    <title>Mixed Precision Training</title>
    <summary>  Deep neural networks have enabled progress in a wide variety of applications.
Growing the size of the neural network typically results in improved accuracy.
As model sizes grow, the memory and compute requirements for training these
models also increases. We introduce a technique to train deep neural networks
using half precision floating point numbers. In our technique, weights,
activations and gradients are stored in IEEE half-precision format.
Half-precision floating numbers have limited numerical range compared to
single-precision numbers. We propose two techniques to handle this loss of
information. Firstly, we recommend maintaining a single-precision copy of the
weights that accumulates the gradients after each optimizer step. This
single-precision copy is rounded to half-precision format during training.
Secondly, we propose scaling the loss appropriately to handle the loss of
information with half-precision gradients. We demonstrate that this approach
works for a wide variety of models including convolution neural networks,
recurrent neural networks and generative adversarial networks. This technique
works for large scale models with more than 100 million parameters trained on
large datasets. Using this approach, we can reduce the memory consumption of
deep learning models by nearly 2x. In future processors, we can also expect a
significant computation speedup using half-precision hardware units.
</summary>
    <author>
      <name>Paulius Micikevicius</name>
    </author>
    <author>
      <name>Sharan Narang</name>
    </author>
    <author>
      <name>Jonah Alben</name>
    </author>
    <author>
      <name>Gregory Diamos</name>
    </author>
    <author>
      <name>Erich Elsen</name>
    </author>
    <author>
      <name>David Garcia</name>
    </author>
    <author>
      <name>Boris Ginsburg</name>
    </author>
    <author>
      <name>Michael Houston</name>
    </author>
    <author>
      <name>Oleksii Kuchaiev</name>
    </author>
    <author>
      <name>Ganesh Venkatesh</name>
    </author>
    <author>
      <name>Hao Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at ICLR 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.03740v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.03740v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.03541v1</id>
    <updated>2017-12-10T14:50:28Z</updated>
    <published>2017-12-10T14:50:28Z</published>
    <title>An Architecture Combining Convolutional Neural Network (CNN) and Support
  Vector Machine (SVM) for Image Classification</title>
    <summary>  Convolutional neural networks (CNNs) are similar to "ordinary" neural
networks in the sense that they are made up of hidden layers consisting of
neurons with "learnable" parameters. These neurons receive inputs, performs a
dot product, and then follows it with a non-linearity. The whole network
expresses the mapping between raw image pixels and their class scores.
Conventionally, the Softmax function is the classifier used at the last layer
of this network. However, there have been studies (Alalshekmubarak and Smith,
2013; Agarap, 2017; Tang, 2013) conducted to challenge this norm. The cited
studies introduce the usage of linear support vector machine (SVM) in an
artificial neural network architecture. This project is yet another take on the
subject, and is inspired by (Tang, 2013). Empirical data has shown that the
CNN-SVM model was able to achieve a test accuracy of ~99.04% using the MNIST
dataset (LeCun, Cortes, and Burges, 2010). On the other hand, the CNN-Softmax
was able to achieve a test accuracy of ~99.23% using the same dataset. Both
models were also tested on the recently-published Fashion-MNIST dataset (Xiao,
Rasul, and Vollgraf, 2017), which is suppose to be a more difficult image
classification dataset than MNIST (Zalandoresearch, 2017). This proved to be
the case as CNN-SVM reached a test accuracy of ~90.72%, while the CNN-Softmax
reached a test accuracy of ~91.86%. The said results may be improved if data
preprocessing techniques were employed on the datasets, and if the base CNN
model was a relatively more sophisticated than the one used in this study.
</summary>
    <author>
      <name>Abien Fred Agarap</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.03541v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.03541v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.00632v2</id>
    <updated>2018-01-09T09:15:07Z</updated>
    <published>2018-01-02T12:50:12Z</published>
    <title>Character-level Recurrent Neural Networks in Practice: Comparing
  Training and Sampling Schemes</title>
    <summary>  Recurrent neural networks are nowadays successfully used in an abundance of
applications, going from text, speech and image processing to recommender
systems. Backpropagation through time is the algorithm that is commonly used to
train these networks on specific tasks. Many deep learning frameworks have
their own implementation of training and sampling procedures for recurrent
neural networks, while there are in fact multiple other possibilities to choose
from and other parameters to tune. In existing literature this is very often
overlooked or ignored. In this paper we therefore give an overview of possible
training and sampling schemes for character-level recurrent neural networks to
solve the task of predicting the next token in a given sequence. We test these
different schemes on a variety of datasets, neural network architectures and
parameter settings, and formulate a number of take-home recommendations. The
choice of training and sampling scheme turns out to be subject to a number of
trade-offs, such as training stability, sampling time, model performance and
implementation effort, but is largely independent of the data. Perhaps the most
surprising result is that transferring hidden states for correctly initializing
the model on subsequences often leads to unstable training behavior depending
on the dataset.
</summary>
    <author>
      <name>Cedric De Boom</name>
    </author>
    <author>
      <name>Thomas Demeester</name>
    </author>
    <author>
      <name>Bart Dhoedt</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00521-017-3322-z</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00521-017-3322-z" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 11 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.00632v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.00632v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.08808v1</id>
    <updated>2018-01-26T13:55:26Z</updated>
    <published>2018-01-26T13:55:26Z</published>
    <title>Learning Optimal Redistribution Mechanisms through Neural Networks</title>
    <summary>  We consider a setting where $p$ public resources are to be allocated among
$n$ competing and strategic agents so as to maximize social welfare (the
objects should be allocated to those who value them the most). This is called
allocative efficiency (AE). We need the agents to report their valuations for
obtaining these resources, truthfully referred to as dominant strategy
incentive compatibility (DSIC). We use auction-based mechanisms to achieve AE
and DSIC yet budget balance cannot be ensured, due to Green-Laffont
Impossibility Theorem. That is, the net transfer of money cannot be zero. This
problem has been addressed by designing a redistribution mechanism so as to
ensure a minimum surplus of money as well as AE and DSIC. The objective could
be to minimize surplus in expectation or in the worst case and these $p$
objects could be homogeneous or heterogeneous. Designing redistribution
mechanisms which perform well in expectation becomes analytically challenging
for heterogeneous settings. In this paper, we take a completely different,
data-driven approach. We train a neural network to determine an optimal
redistribution mechanism based on given settings with both the objectives,
optimal in expectation and optimal in the worst case. We also propose a loss
function to train a neural network to optimize worst case. We design neural
networks with the underlying rebate functions being linear as well as nonlinear
in terms of bids of the agents. Our networks' performances are same as the
theoretical guarantees for the cases where it has been solved. We observe that
a neural network based redistribution mechanism for homogeneous settings which
uses nonlinear rebate functions outperforms linear rebate functions when the
objective is optimal in expectation. Our approach also yields an optimal in
expectation redistribution mechanism for heterogeneous settings.
</summary>
    <author>
      <name>P Manisha</name>
    </author>
    <author>
      <name>C V Jawahar</name>
    </author>
    <author>
      <name>Sujit Gujar</name>
    </author>
    <link href="http://arxiv.org/abs/1801.08808v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.08808v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.00909v1</id>
    <updated>2016-01-05T17:15:37Z</updated>
    <published>2016-01-05T17:15:37Z</published>
    <title>The high-conductance state enables neural sampling in networks of LIF
  neurons</title>
    <summary>  The apparent stochasticity of in-vivo neural circuits has long been
hypothesized to represent a signature of ongoing stochastic inference in the
brain. More recently, a theoretical framework for neural sampling has been
proposed, which explains how sample-based inference can be performed by
networks of spiking neurons. One particular requirement of this approach is
that the neural response function closely follows a logistic curve.
  Analytical approaches to calculating neural response functions have been the
subject of many theoretical studies. In order to make the problem tractable,
particular assumptions regarding the neural or synaptic parameters are usually
made. However, biologically significant activity regimes exist which are not
covered by these approaches: Under strong synaptic bombardment, as is often the
case in cortex, the neuron is shifted into a high-conductance state (HCS)
characterized by a small membrane time constant. In this regime, synaptic time
constants and refractory periods dominate membrane dynamics.
  The core idea of our approach is to separately consider two different "modes"
of spiking dynamics: burst spiking and transient quiescence, in which the
neuron does not spike for longer periods. We treat the former by propagating
the PDF of the effective membrane potential from spike to spike within a burst,
while using a diffusion approximation for the latter. We find that our
prediction of the neural response function closely matches simulation data.
Moreover, in the HCS scenario, we show that the neural response function
becomes symmetric and can be well approximated by a logistic function, thereby
providing the correct dynamics in order to perform neural sampling. We hereby
provide not only a normative framework for Bayesian inference in cortex, but
also powerful applications of low-power, accelerated neuromorphic systems to
relevant machine learning tasks.
</summary>
    <author>
      <name>Mihai A. Petrovici</name>
    </author>
    <author>
      <name>Ilja Bytschok</name>
    </author>
    <author>
      <name>Johannes Bill</name>
    </author>
    <author>
      <name>Johannes Schemmel</name>
    </author>
    <author>
      <name>Karlheinz Meier</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1186/1471-2202-16-S1-O2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1186/1471-2202-16-S1-O2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.00909v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.00909v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04662v2</id>
    <updated>2017-02-27T17:30:54Z</updated>
    <published>2016-05-16T06:36:44Z</published>
    <title>Forman curvature for directed networks</title>
    <summary>  A goal in network science is the geometrical characterization of complex
networks. In this direction, we (arXiv:1603.00386; J. Stat. Mech. (2016)
P063206) have recently introduced the Forman's discretization of Ricci
curvature to the realm of undirected networks. Investigation of Forman
curvature in diverse model and real-world undirected networks revealed that
this measure captures several aspects of the organization of complex undirected
networks. However, many important real-world networks are inherently directed
in nature, and the Forman curvature for undirected networks is unsuitable for
analysis of such directed networks. Hence, we here extend the Forman curvature
for undirected networks to the case of directed networks. The simple
mathematical formula for the Forman curvature in directed networks elegantly
incorporates node weights, edge weights and edge direction. By applying the
Forman curvature for directed networks to a variety of model and real-world
directed networks, we show that the measure can be used to characterize the
structure of complex directed networks. Furthermore, our results also hold in
real directed networks which are weighted or spatial in nature. These results
in combination with our previous results suggest that the Forman curvature can
be readily employed to study the organization of both directed and undirected
complex networks.
</summary>
    <author>
      <name>R. P. Sreejith</name>
    </author>
    <author>
      <name>J√ºrgen Jost</name>
    </author>
    <author>
      <name>Emil Saucan</name>
    </author>
    <author>
      <name>Areejit Samal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 10 Main figures, 8 Supplementary Figures, Supplementary
  Tables available upon request from authors. Added new results in this version</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.04662v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04662v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/q-bio/0403011v1</id>
    <updated>2004-03-07T23:27:32Z</updated>
    <published>2004-03-07T23:27:32Z</published>
    <title>Memorization in a neural network with adjustable transfer function and
  conditional gating</title>
    <summary>  The main problem about replacing LTP as a memory mechanism has been to find
other highly abstract, easily understandable principles for induced plasticity.
In this paper we attempt to lay out such a basic mechanism, namely intrinsic
plasticity. Important empirical observations with theoretical significance are
time-layering of neural plasticity mediated by additional constraints to enter
into later stages, various manifestations of intrinsic neural properties, and
conditional gating of synaptic connections. An important consequence of the
proposed mechanism is that it can explain the usually latent nature of
memories.
</summary>
    <author>
      <name>Gabriele Scheler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/q-bio/0403011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/q-bio/0403011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.2928v1</id>
    <updated>2008-07-18T11:23:27Z</updated>
    <published>2008-07-18T11:23:27Z</published>
    <title>Visual Grouping by Neural Oscillators</title>
    <summary>  Distributed synchronization is known to occur at several scales in the brain,
and has been suggested as playing a key functional role in perceptual grouping.
State-of-the-art visual grouping algorithms, however, seem to give
comparatively little attention to neural synchronization analogies. Based on
the framework of concurrent synchronization of dynamic systems, simple networks
of neural oscillators coupled with diffusive connections are proposed to solve
visual grouping problems. Multi-layer algorithms and feedback mechanisms are
also studied. The same algorithm is shown to achieve promising results on
several classical visual grouping problems, including point clustering, contour
integration and image segmentation.
</summary>
    <author>
      <name>Guoshen Yu</name>
    </author>
    <author>
      <name>Jean-Jacques Slotine</name>
    </author>
    <link href="http://arxiv.org/abs/0807.2928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.2928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.6768v1</id>
    <updated>2014-11-25T08:52:14Z</updated>
    <published>2014-11-25T08:52:14Z</published>
    <title>Hypotheses of neural code and the information model of the
  neuron-detector</title>
    <summary>  This paper deals with the problem of neural code solving. On the basis of the
formulated hypotheses the information model of a neuron-detector is suggested,
the detector being one of the basic elements of an artificial neural network
(ANN). The paper subjects the connectionist paradigm of ANN building to
criticism and suggests a new presentation paradigm for ANN building and
neuroelements (NE) learning. The adequacy of the suggested model is proved by
the fact that is does not contradict the modern propositions of neuropsychology
and neurophysiology.
</summary>
    <author>
      <name>Yuri Parzhin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.6768v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.6768v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0; I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.5401v2</id>
    <updated>2014-12-10T16:01:39Z</updated>
    <published>2014-10-20T19:28:26Z</published>
    <title>Neural Turing Machines</title>
    <summary>  We extend the capabilities of neural networks by coupling them to external
memory resources, which they can interact with by attentional processes. The
combined system is analogous to a Turing Machine or Von Neumann architecture
but is differentiable end-to-end, allowing it to be efficiently trained with
gradient descent. Preliminary results demonstrate that Neural Turing Machines
can infer simple algorithms such as copying, sorting, and associative recall
from input and output examples.
</summary>
    <author>
      <name>Alex Graves</name>
    </author>
    <author>
      <name>Greg Wayne</name>
    </author>
    <author>
      <name>Ivo Danihelka</name>
    </author>
    <link href="http://arxiv.org/abs/1410.5401v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.5401v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.07526v3</id>
    <updated>2015-11-20T15:36:56Z</updated>
    <published>2015-10-26T16:03:27Z</published>
    <title>Empirical Study on Deep Learning Models for Question Answering</title>
    <summary>  In this paper we explore deep learning models with memory component or
attention mechanism for question answering task. We combine and compare three
models, Neural Machine Translation, Neural Turing Machine, and Memory Networks
for a simulated QA data set. This paper is the first one that uses Neural
Machine Translation and Neural Turing Machines for solving QA tasks. Our
results suggest that the combination of attention and memory have potential to
solve certain QA problem.
</summary>
    <author>
      <name>Yang Yu</name>
    </author>
    <author>
      <name>Wei Zhang</name>
    </author>
    <author>
      <name>Chung-Wei Hang</name>
    </author>
    <author>
      <name>Bing Xiang</name>
    </author>
    <author>
      <name>Bowen Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/1510.07526v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.07526v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.02433v3</id>
    <updated>2016-06-15T00:07:05Z</updated>
    <published>2015-12-08T12:42:00Z</published>
    <title>Minimum Risk Training for Neural Machine Translation</title>
    <summary>  We propose minimum risk training for end-to-end neural machine translation.
Unlike conventional maximum likelihood estimation, minimum risk training is
capable of optimizing model parameters directly with respect to arbitrary
evaluation metrics, which are not necessarily differentiable. Experiments show
that our approach achieves significant improvements over maximum likelihood
estimation on a state-of-the-art neural machine translation system across
various languages pairs. Transparent to architectures, our approach can be
applied to more neural networks and potentially benefit more NLP tasks.
</summary>
    <author>
      <name>Shiqi Shen</name>
    </author>
    <author>
      <name>Yong Cheng</name>
    </author>
    <author>
      <name>Zhongjun He</name>
    </author>
    <author>
      <name>Wei He</name>
    </author>
    <author>
      <name>Hua Wu</name>
    </author>
    <author>
      <name>Maosong Sun</name>
    </author>
    <author>
      <name>Yang Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in Proceedings of ACL 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.02433v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.02433v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.05368v1</id>
    <updated>2016-11-16T17:04:04Z</updated>
    <published>2016-11-16T17:04:04Z</published>
    <title>Neural Style Representations and the Large-Scale Classification of
  Artistic Style</title>
    <summary>  The artistic style of a painting is a subtle aesthetic judgment used by art
historians for grouping and classifying artwork. The recently introduced
`neural-style' algorithm substantially succeeds in merging the perceived
artistic style of one image or set of images with the perceived content of
another. In light of this and other recent developments in image analysis via
convolutional neural networks, we investigate the effectiveness of a
`neural-style' representation for classifying the artistic style of paintings.
</summary>
    <author>
      <name>Jeremiah Johnson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.05368v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.05368v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.00827v1</id>
    <updated>2016-12-02T20:31:44Z</updated>
    <published>2016-12-02T20:31:44Z</published>
    <title>Learning Operations on a Stack with Neural Turing Machines</title>
    <summary>  Multiple extensions of Recurrent Neural Networks (RNNs) have been proposed
recently to address the difficulty of storing information over long time
periods. In this paper, we experiment with the capacity of Neural Turing
Machines (NTMs) to deal with these long-term dependencies on well-balanced
strings of parentheses. We show that not only does the NTM emulate a stack with
its heads and learn an algorithm to recognize such words, but it is also
capable of strongly generalizing to much longer sequences.
</summary>
    <author>
      <name>Tristan Deleu</name>
    </author>
    <author>
      <name>Joseph Dureau</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">1st Workshop on Neural Abstract Machines &amp; Program Induction (NAMPI),
  NIPS 2016, Barcelona, Spain</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.00827v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.00827v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.06497v1</id>
    <updated>2017-04-21T11:56:00Z</updated>
    <published>2017-04-21T11:56:00Z</published>
    <title>Bandit Structured Prediction for Neural Sequence-to-Sequence Learning</title>
    <summary>  Bandit structured prediction describes a stochastic optimization framework
where learning is performed from partial feedback. This feedback is received in
the form of a task loss evaluation to a predicted output structure, without
having access to gold standard structures. We advance this framework by lifting
linear bandit learning to neural sequence-to-sequence learning problems using
attention-based recurrent neural networks. Furthermore, we show how to
incorporate control variates into our learning algorithms for variance
reduction and improved generalization. We present an evaluation on a neural
machine translation task that shows improvements of up to 5.89 BLEU points for
domain adaptation from simulated bandit feedback.
</summary>
    <author>
      <name>Julia Kreutzer</name>
    </author>
    <author>
      <name>Artem Sokolov</name>
    </author>
    <author>
      <name>Stefan Riezler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ACL 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.06497v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.06497v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.05367v2</id>
    <updated>2017-12-29T19:48:56Z</updated>
    <published>2017-10-15T17:43:43Z</published>
    <title>Weaving and neural complexity in symmetric quantum states</title>
    <summary>  We study the behaviour of two different measures of the complexity of
multipartite correlation patterns, weaving and neural complexity, for symmetric
quantum states. Weaving is the weighted sum of genuine multipartite
correlations of any order, where the weights are proportional to the
correlation order. The neural complexity, originally introduced to characterize
correlation patterns in classical neural networks, is here extended to the
quantum scenario. We derive closed formulas of the two quantities for GHZ
states mixed with white noise.
</summary>
    <author>
      <name>Cristian E. Susa</name>
    </author>
    <author>
      <name>Davide Girolami</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.optcom.2017.12.050</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.optcom.2017.12.050" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Contribution to a Special Issue on Quantum Correlations, close to
  published version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Optics Communications 413, 157-161 (2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1710.05367v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.05367v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.3989v1</id>
    <updated>2010-04-22T19:18:51Z</updated>
    <published>2010-04-22T19:18:51Z</published>
    <title>Interdependent networks: Reducing the coupling strength leads to a
  change from a first to second order percolation transition</title>
    <summary>  We study a system composed from two interdependent networks A and B, where a
fraction of the nodes in network A depends on the nodes of network B and a
fraction of the nodes in network B depends on the nodes of network A. Due to
the coupling between the networks when nodes in one network fail they cause
dependent nodes in the other network to also fail. This invokes an iterative
cascade of failures in both networks. When a critical fraction of nodes fail
the iterative process results in a percolation phase transition that completely
fragments both networks. We show both analytically and numerically that
reducing the coupling between the networks leads to a change from a first order
percolation phase transition to a second order percolation transition at a
critical point. The scaling of the percolation order parameter near the
critical point is characterized by the critical exponent beta=1.
</summary>
    <author>
      <name>Roni Parshani</name>
    </author>
    <author>
      <name>Sergey V. Buldyrev</name>
    </author>
    <author>
      <name>Shlomo Havlin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.105.048701</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.105.048701" rel="related"/>
    <link href="http://arxiv.org/abs/1004.3989v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.3989v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.6339v2</id>
    <updated>2012-03-28T20:47:17Z</updated>
    <published>2012-01-30T20:22:24Z</published>
    <title>Epidemics on Interconnected Networks</title>
    <summary>  Populations are seldom completely isolated from their environment.
Individuals in a particular geographic or social region may be considered a
distinct network due to strong local ties, but will also interact with
individuals in other networks. We study the susceptible-infected-recovered
(SIR) process on interconnected network systems, and find two distinct regimes.
In strongly-coupled network systems, epidemics occur simultaneously across the
entire system at a critical infection strength $\beta_c$, below which the
disease does not spread. In contrast, in weakly-coupled network systems, a
mixed phase exists below $\beta_c$ of the coupled network system, where an
epidemic occurs in one network but does not spread to the coupled network. We
derive an expression for the network and disease parameters that allow this
mixed phase and verify it numerically. Public health implications of
communities comprising these two classes of network systems are also mentioned.
</summary>
    <author>
      <name>M. Dickison</name>
    </author>
    <author>
      <name>S. Havlin</name>
    </author>
    <author>
      <name>H. E. Stanley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1201.6339v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.6339v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.0630v1</id>
    <updated>2010-11-02T14:44:59Z</updated>
    <published>2010-11-02T14:44:59Z</published>
    <title>Inter-arrival times of message propagation on directed networks</title>
    <summary>  One of the challenges in fighting cybercrime is to understand the dynamics of
message propagation on botnets, networks of infected computers used to send
viruses, unsolicited commercial emails (SPAM) or denial of service attacks. We
map this problem to the propagation of multiple random walkers on directed
networks and we evaluate the inter-arrival time distribution between successive
walkers arriving at a target. We show that the temporal organization of this
process, which models information propagation on unstructured peer to peer
networks, has the same features as SPAM arriving to a single user. We study the
behavior of the message inter-arrival time distribution on three different
network topologies using two different rules for sending messages. In all
networks the propagation is not a pure Poisson process. It shows universal
features on Poissonian networks and a more complex behavior on scale free
networks. Results open the possibility to indirectly learn about the process of
sending messages on networks with unknown topologies, by studying inter-arrival
times at any node of the network.
</summary>
    <author>
      <name>Tamara Mihaljev</name>
    </author>
    <author>
      <name>Lucilla de Arcangelis</name>
    </author>
    <author>
      <name>Hans J. Herrmann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.84.026112</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.84.026112" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1011.0630v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.0630v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.0215v3</id>
    <updated>2014-12-23T17:36:55Z</updated>
    <published>2014-02-02T17:25:58Z</published>
    <title>Mutually connected component of network of networks with replica nodes</title>
    <summary>  We describe the emergence of the giant mutually connected component in
networks of networks in which each node has a single replica node in any layer
and can be interdependent only on its replica nodes in the interdependent
layers. We prove that if in these networks, all the nodes of one network
(layer) are interdependent on the nodes of the same other interconnected layer,
then, remarkably, the mutually connected component does not depend on the
topology of the network of networks. This component coincides with the mutual
component of the fully connected network of networks constructed from the same
set of layers, i.e., a multiplex network.
</summary>
    <author>
      <name>Ginestra Bianconi</name>
    </author>
    <author>
      <name>Sergey N. Dorogovtsev</name>
    </author>
    <author>
      <name>Jos√© F. F. Mendes</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.91.012804</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.91.012804" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">(9 pages, 2 figures )</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E. 91, 012804 (2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1402.0215v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.0215v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.01053v2</id>
    <updated>2017-11-29T22:28:20Z</updated>
    <published>2017-04-04T15:23:01Z</published>
    <title>Network-ensemble comparisons with stochastic rewiring and von Neumann
  entropy</title>
    <summary>  Assessing whether a given network is typical or atypical for a random-network
ensemble (i.e., network-ensemble comparison) has widespread applications
ranging from null-model selection and hypothesis testing to clustering and
classifying networks. We develop a framework for network-ensemble comparison by
subjecting the network to stochastic rewiring. We study two rewiring processes,
uniform and degree-preserved rewiring, which yield random-network ensembles
that converge to the Erdos-Renyi and configuration-model ensembles,
respectively. We study convergence through von Neumann entropy (VNE), a network
summary statistic measuring information content based on the spectra of a
Laplacian matrix, and develop a perturbation analysis for the expected effect
of rewiring on VNE. Our analysis yields an estimate for how many rewires are
required for a given network to resemble a typical network from an ensemble,
offering a computationally efficient quantity for network-ensemble comparison
that does not require simulation of the corresponding rewiring process.
</summary>
    <author>
      <name>Zichao Li</name>
    </author>
    <author>
      <name>Peter J. Mucha</name>
    </author>
    <author>
      <name>Dane Taylor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.01053v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.01053v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.04977v1</id>
    <updated>2018-02-14T07:46:15Z</updated>
    <published>2018-02-14T07:46:15Z</published>
    <title>Paraphrasing Complex Network: Network Compression via Factor Transfer</title>
    <summary>  Deep neural networks (DNN) have recently shown promising performances in
various areas. Although DNNs are very powerful, a large number of network
parameters requires substantial storage and memory bandwidth which hinders them
from being applied to actual embedded systems. Many researchers have sought
ways of model compression to reduce the size of a network with minimal
performance degradation. Among them, a method called knowledge transfer is to
train the student network with a stronger teacher network. In this paper, we
propose a method to overcome the limitations of conventional knowledge transfer
methods and improve the performance of a student network. An auto-encoder is
used in an unsupervised manner to extract compact factors which are defined as
compressed feature maps of the teacher network. When using the factors to train
the student network, we observed that the performance of the student network
becomes better than the ones with other conventional knowledge transfer methods
because factors contain paraphrased compact information of the teacher network
that is easy for the student network to understand.
</summary>
    <author>
      <name>Jangho Kim</name>
    </author>
    <author>
      <name>SeoungUK Park</name>
    </author>
    <author>
      <name>Nojun Kwak</name>
    </author>
    <link href="http://arxiv.org/abs/1802.04977v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.04977v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cmp-lg/9709012v1</id>
    <updated>1997-09-23T14:01:27Z</updated>
    <published>1997-09-23T14:01:27Z</published>
    <title>Using Single Layer Networks for Discrete, Sequential Data: An Example
  from Natural Language Processing</title>
    <summary>  A natural language parser which has been successfully implemented is
described. This is a hybrid system, in which neural networks operate within a
rule based framework. It can be accessed via telnet for users to try on their
own text. (For details, contact the author.) Tested on technical manuals, the
parser finds the subject and head of the subject in over 90% of declarative
sentences.
  The neural processing components belong to the class of Generalized Single
Layer Networks (GSLN). In general, supervised, feed-forward networks need more
than one layer to process data. However, in some cases data can be
pre-processed with a non-linear transformation, and then presented in a
linearly separable form for subsequent processing by a single layer net. Such
networks offer advantages of functional transparency and operational speed.
  For our parser, the initial stage of processing maps linguistic data onto a
higher order representation, which can then be analysed by a single layer
network. This transformation is supported by information theoretic analysis.
</summary>
    <author>
      <name>Caroline Lyon</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Science Department, University of Hertfordshire, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Ray Frank</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Science Department, University of Hertfordshire, UK</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 9 figures, Latex format, uses epsfig, .styfile included</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Computing and Applications 5(4), 1997, 196-214</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cmp-lg/9709012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cmp-lg/9709012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cmp-lg" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cmp-lg" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9807101v1</id>
    <updated>1998-07-07T13:38:55Z</updated>
    <published>1998-07-07T13:38:55Z</published>
    <title>Attractor neural networks storing multiple space representations: a
  model for hippocampal place fields</title>
    <summary>  A recurrent neural network model storing multiple spatial maps, or
``charts'', is analyzed. A network of this type has been suggested as a model
for the origin of place cells in the hippocampus of rodents. The extremely
diluted and fully connected limits are studied, and the storage capacity and
the information capacity are found. The important parameters determining the
performance of the network are the sparsity of the spatial representations and
the degree of connectivity, as found already for the storage of individual
memory patterns in the general theory of auto-associative networks. Such
results suggest a quantitative parallel between theories of hippocampal
function in different animal species, such as primates (episodic memory) and
rodents (memory for space).
</summary>
    <author>
      <name>Francesco P. Battaglia</name>
    </author>
    <author>
      <name>Alessandro Treves</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.58.7738</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.58.7738" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 RevTeX pages, 8 pes figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9807101v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9807101v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9907390v1</id>
    <updated>1999-07-26T13:05:44Z</updated>
    <published>1999-07-26T13:05:44Z</published>
    <title>Q-Ising neural network dynamics: a comparative review of various
  architectures</title>
    <summary>  This contribution reviews the parallel dynamics of Q-Ising neural networks
for various architectures: extremely diluted asymmetric, layered feedforward,
extremely diluted symmetric, and fully connected. Using a probabilistic
signal-to-noise ratio analysis, taking into account all feedback correlations,
which are strongly dependent upon these architectures the evolution of the
distribution of the local field is found. This leads to a recursive scheme
determining the complete time evolution of the order parameters of the network.
Arbitrary Q and mainly zero temperature are considered. For the asymmetrically
diluted and the layered feedforward network a closed-form solution is obtained
while for the symmetrically diluted and fully connected architecture the
feedback correlations prevent such a closed-form solution. For these symmetric
networks equilibrium fixed-point equations can be derived under certain
conditions on the noise in the system. They are the same as those obtained in a
thermodynamic replica-symmetric mean-field theory approach.
</summary>
    <author>
      <name>D. Bolle</name>
    </author>
    <author>
      <name>G. Jongen</name>
    </author>
    <author>
      <name>G. M. Shim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the proceedings of the Int. Conf. on Math. Phys. and
  Stochastic Analysis (Lisbon, October, 1998), ed. S. Albevero et all (World
  Scientific)</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9907390v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9907390v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9910009v1</id>
    <updated>1999-10-01T13:29:24Z</updated>
    <published>1999-10-01T13:29:24Z</published>
    <title>Categorization in fully connected multi-state neural network models</title>
    <summary>  The categorization ability of fully connected neural network models, with
either discrete or continuous Q-state units, is studied in this work in replica
symmetric mean-field theory. Hierarchically correlated multi-state patterns in
a two level structure of ancestors and descendents (examples) are embedded in
the network and the categorization task consists in recognizing the ancestors
when the network is trained exclusively with their descendents. Explicit
results for the dependence of the equilibrium properties of a Q=3-state model
and a $Q=\infty$-state model are obtained in the form of phase diagrams and
categorization curves. A strong improvement of the categorization ability is
found when the network is trained with examples of low activity. The
categorization ability is found to be robust to finite threshold and synaptic
noise. The Almeida-Thouless lines that limit the validity of the
replica-symmetric results, are also obtained.
</summary>
    <author>
      <name>R. Erichsen</name>
    </author>
    <author>
      <name>W. K. Theumann</name>
    </author>
    <author>
      <name>D. R. C. Dominguez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 8 figures, to appear in Physical Review E</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9910009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9910009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0107141v2</id>
    <updated>2001-08-28T15:31:34Z</updated>
    <published>2001-07-06T15:41:32Z</published>
    <title>Retrieval behavior and thermodynamic properties of symmetrically diluted
  Q-Ising neural networks</title>
    <summary>  The retrieval behavior and thermodynamic properties of symmetrically diluted
Q-Ising neural networks are derived and studied in replica-symmetric mean-field
theory generalizing earlier works on either the fully connected or the
symmetrical extremely diluted network. Capacity-gain parameter phase diagrams
are obtained for the Q=3, Q=4 and $Q=\infty$ state networks with uniformly
distributed patterns of low activity in order to search for the effects of a
gradual dilution of the synapses. It is shown that enlarged regions of
continuous changeover into a region of optimal performance are obtained for
finite stochastic noise and small but finite connectivity. The de
Almeida-Thouless lines of stability are obtained for arbitrary connectivity,
and the resulting phase diagrams are used to draw conclusions on the behavior
of symmetrically diluted networks with other pattern distributions of either
high or low activity.
</summary>
    <author>
      <name>W. K. Theumann</name>
    </author>
    <author>
      <name>R. Erichsen Jr</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.64.061902</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.64.061902" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, revtex</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0107141v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0107141v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0202411v1</id>
    <updated>2002-02-22T22:43:27Z</updated>
    <published>2002-02-22T22:43:27Z</published>
    <title>Dynamical Phase Transition in a Neural Network Model with Noise: an
  Exact Solution</title>
    <summary>  The dynamical organization in the presence of noise of a Boolean neural
network with random connections is analyzed. For low levels of noise, the
system reaches a stationary state in which the majority of its elements acquire
the same value. It is shown that, under very general conditions, there exists a
critical value of the noise, below which the network remains organized and
above which it behaves randomly. The existence and nature of the phase
transition are computed analytically, showing that the critical exponent is
1/2. The dependence of the critical noise on the parameters of the network is
obtained. These results are then compared with two numerical realizations of
the network.
</summary>
    <author>
      <name>Cristian Huepe</name>
    </author>
    <author>
      <name>Maximino Aldana</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 6 figures, Submitted to the Jounal of Statistical Physics</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0202411v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0202411v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0305527v2</id>
    <updated>2004-11-15T14:54:22Z</updated>
    <published>2003-05-22T11:22:40Z</published>
    <title>Back-propagation of accuracy</title>
    <summary>  In this paper we solve the problem: how to determine maximal allowable
errors, possible for signals and parameters of each element of a network
proceeding from the condition that the vector of output signals of the network
should be calculated with given accuracy? "Back-propagation of accuracy" is
developed to solve this problem. The calculation of allowable errors for each
element of network by back-propagation of accuracy is surprisingly similar to a
back-propagation of error, because it is the backward signals motion, but at
the same time it is very different because the new rules of signals
transformation in the passing back through the elements are different. The
method allows us to formulate the requirements to the accuracy of calculations
and to the realization of technical devices, if the requirements to the
accuracy of output signals of the network are known.
</summary>
    <author>
      <name>M. Yu. Senashova</name>
    </author>
    <author>
      <name>A. N. Gorban</name>
    </author>
    <author>
      <name>D. C. Wunsch II</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures, The talk given on ICNN97 (The 1997 IEEE
  International Conference on Neural Networks, Houston, USA)</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0305527v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0305527v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0405051v1</id>
    <updated>2004-05-16T03:44:06Z</updated>
    <published>2004-05-16T03:44:06Z</published>
    <title>Short Term Load Forecasting Models in Czech Republic Using Soft
  Computing Paradigms</title>
    <summary>  This paper presents a comparative study of six soft computing models namely
multilayer perceptron networks, Elman recurrent neural network, radial basis
function network, Hopfield model, fuzzy inference system and hybrid fuzzy
neural network for the hourly electricity demand forecast of Czech Republic.
The soft computing models were trained and tested using the actual hourly load
data for seven years. A comparison of the proposed techniques is presented for
predicting 2 day ahead demands for electricity. Simulation results indicate
that hybrid fuzzy neural network and radial basis function networks are the
best candidates for the analysis and forecasting of electricity demand.
</summary>
    <author>
      <name>Muhammad Riaz Khan</name>
    </author>
    <author>
      <name>Ajith Abraham</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Knowledge-Based Intelligent Engineering
  Systems, IOS Press Netherlands, Volume 7, Number 4, pp. 172-179, 2003</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0405051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0405051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611079v4</id>
    <updated>2007-12-13T16:08:25Z</updated>
    <published>2006-11-16T11:26:22Z</published>
    <title>Managing network congestion with a Kohonen-based RED queue</title>
    <summary>  The behaviour of the TCP AIMD algorithm is known to cause queue length
oscillations when congestion occurs at a router output link. Indeed, due to
these queueing variations, end-to-end applications experience large delay
jitter. Many studies have proposed efficient Active Queue Management (AQM)
mechanisms in order to reduce queue oscillations and stabilize the queue
length. These AQM are mostly improvements of the Random Early Detection (RED)
model. Unfortunately, these enhancements do not react in a similar manner for
various network conditions and are strongly sensitive to their initial setting
parameters. Although this paper proposes a solution to overcome the
difficulties of setting these parameters by using a Kohonen neural network
model, another goal of this study is to investigate whether cognitive
intelligence could be placed in the core network to solve such stability
problem. In our context, we use results from the neural network area to
demonstrate that our proposal, named Kohonen-RED (KRED), enables a stable queue
length without complex parameters setting and passive measurements.
</summary>
    <author>
      <name>Emmanuel Lochin</name>
    </author>
    <author>
      <name>Bruno Talavera</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.engappai.2010.10.012</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.engappai.2010.10.012" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0611079v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611079v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/nlin/0310033v1</id>
    <updated>2003-10-23T09:22:59Z</updated>
    <published>2003-10-23T09:22:59Z</published>
    <title>Associative memory on a small-world neural network</title>
    <summary>  We study a model of associative memory based on a neural network with
small-world structure. The efficacy of the network to retrieve one of the
stored patterns exhibits a phase transition at a finite value of the disorder.
The more ordered networks are unable to recover the patterns, and are always
attracted to mixture states. Besides, for a range of the number of stored
patterns, the efficacy has a maximum at an intermediate value of the disorder.
We also give a statistical characterization of the attractors for all values of
the disorder of the network.
</summary>
    <author>
      <name>Luis G. Morelli</name>
    </author>
    <author>
      <name>Guillermo Abramson</name>
    </author>
    <author>
      <name>Marcelo N. Kuperman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1140/epjb/e2004-00144-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1140/epjb/e2004-00144-7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures (eps)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Eur. Phys. J. B 38, 495-500 (2004)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/nlin/0310033v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/nlin/0310033v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0712.1003v1</id>
    <updated>2007-12-06T18:04:19Z</updated>
    <published>2007-12-06T18:04:19Z</published>
    <title>Dynamical synapses causing self-organized criticality in neural networks</title>
    <summary>  We show that a network of spiking neurons exhibits robust self-organized
criticality if the synaptic efficacies follow realistic dynamics. Deriving
analytical expressions for the average coupling strengths and inter-spike
intervals, we demonstrate that networks with dynamical synapses exhibit
critical avalanche dynamics for a wide range of interaction parameters. We
prove that in the thermodynamical limit the network becomes critical for all
large enough coupling parameters. We thereby explain experimental observations
in which cortical neurons show avalanche activity with the total intensity of
firing events being distributed as a power-law.
</summary>
    <author>
      <name>Anna Levina</name>
    </author>
    <author>
      <name>J. Michael Herrmann</name>
    </author>
    <author>
      <name>Theo Geisel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/nphys758</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/nphys758" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">A. Levina, J. M. Herrmann, T. Geisel. Dynamical synapses causing
  self-organized criticality in neural networks, Nature Phys. 3, 857-860 (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0712.1003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.1003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.2512v1</id>
    <updated>2009-12-13T16:05:02Z</updated>
    <published>2009-12-13T16:05:02Z</published>
    <title>Communication via Quantum Neural Network</title>
    <summary>  In this study, the partially entangled neural networks is used to transfer
information between two neurons, where the original teleportation protocol is
employed this for this purpose. The effect of the network strength on the
fidelity of the transported information is investigated. We show that as the
strength of the network increases, the accuracy of the transformed information
increases. As a practical application, we consider the spread of swine flu
virus between two equivalent tranches of the community. In this treatment two
factors are considered, one for humanity and the other for influence factor.
  The likelihood of infection between different age group is investigated,
where we show that the strength of the neural network and the degree of
infection plays an important role on transferring infection between different
age group. From theoretical point of view, we show that it is possible to
control the spread of the virus by controlling the network parameter. Also, by
using local rotation, one can decrease the rate of infection between the young.
</summary>
    <author>
      <name>A. Al- Segher</name>
    </author>
    <author>
      <name>Nasser Metwally</name>
    </author>
    <link href="http://arxiv.org/abs/0912.2512v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.2512v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.gen-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.gen-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.4004v1</id>
    <updated>2010-02-21T19:41:45Z</updated>
    <published>2010-02-21T19:41:45Z</published>
    <title>Nature inspired artificial intelligence based adaptive traffic flow
  distribution in computer network</title>
    <summary>  Because of the stochastic nature of traffic requirement matrix, it is very
difficult to get the optimal traffic distribution to minimize the delay even
with adaptive routing protocol in a fixed connection network where capacity
already defined for each link. Hence there is a requirement to define such a
method, which could generate the optimal solution very quickly and efficiently.
This paper presenting a new concept to provide the adaptive optimal traffic
distribution for dynamic condition of traffic matrix using nature based
intelligence methods. With the defined load and fixed capacity of links,
average delay for packet has minimized with various variations of evolutionary
programming and particle swarm optimization. Comparative study has given over
their performance in terms of converging speed. Universal approximation
capability, the key feature of feed forward neural network has applied to
predict the flow distribution on each link to minimize the average delay for a
total load available at present on the network. For any variation in the total
load, the new flow distribution can be generated by neural network immediately,
which could generate minimum delay in the network. With the inclusion of this
information, performance of routing protocol will be improved very much.
</summary>
    <author>
      <name>Manoj Kumar Singh</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computing, Volume 2, Issue 2, February 2010,
  https://sites.google.com/site/journalofcomputing/</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1002.4004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.4004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.5191v2</id>
    <updated>2012-10-25T08:59:35Z</updated>
    <published>2011-11-22T13:25:41Z</published>
    <title>Multitasking associative networks</title>
    <summary>  We introduce a bipartite, diluted and frustrated, network as a sparse
restricted Boltzman machine and we show its thermodynamical equivalence to an
associative working memory able to retrieve multiple patterns in parallel
without falling into spurious states typical of classical neural networks. We
focus on systems processing in parallel a finite (up to logarithmic growth in
the volume) amount of patterns, mirroring the low-level storage of standard
Amit-Gutfreund-Sompolinsky theory. Results obtained trough statistical
mechanics, signal-to-noise technique and Monte Carlo simulations are overall in
perfect agreement and carry interesting biological insights. Indeed, these
associative networks pave new perspectives in the understanding of multitasking
features expressed by complex systems, e.g. neural and immune networks.
</summary>
    <author>
      <name>Elena Agliari</name>
    </author>
    <author>
      <name>Adriano Barra</name>
    </author>
    <author>
      <name>Andrea Galluzzi</name>
    </author>
    <author>
      <name>Francesco Guerra</name>
    </author>
    <author>
      <name>Francesco Moauro</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.109.268101</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.109.268101" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear on Phys.Rev.Lett</arxiv:comment>
    <link href="http://arxiv.org/abs/1111.5191v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.5191v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3605v3</id>
    <updated>2013-03-08T19:42:37Z</updated>
    <published>2013-01-16T07:23:19Z</published>
    <title>Feature Learning in Deep Neural Networks - Studies on Speech Recognition
  Tasks</title>
    <summary>  Recent studies have shown that deep neural networks (DNNs) perform
significantly better than shallow networks and Gaussian mixture models (GMMs)
on large vocabulary speech recognition tasks. In this paper, we argue that the
improved accuracy achieved by the DNNs is the result of their ability to
extract discriminative internal representations that are robust to the many
sources of variability in speech signals. We show that these representations
become increasingly insensitive to small perturbations in the input with
increasing network depth, which leads to better speech recognition performance
with deeper networks. We also show that DNNs cannot extrapolate to test samples
that are substantially different from the training examples. If the training
data are sufficiently representative, however, internal features learned by the
DNN are relatively stable with respect to speaker differences, bandwidth
differences, and environment distortion. This enables DNN-based recognizers to
perform as well or better than state-of-the-art systems based on GMMs or
shallow networks without the need for explicit model adaptation or feature
normalization.
</summary>
    <author>
      <name>Dong Yu</name>
    </author>
    <author>
      <name>Michael L. Seltzer</name>
    </author>
    <author>
      <name>Jinyu Li</name>
    </author>
    <author>
      <name>Jui-Ting Huang</name>
    </author>
    <author>
      <name>Frank Seide</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2013, 9 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.3605v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3605v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.7464v1</id>
    <updated>2013-07-29T05:21:37Z</updated>
    <published>2013-07-29T05:21:37Z</published>
    <title>Real-time Peer-to-Peer Botnet Detection Framework based on Bayesian
  Regularized Neural Network</title>
    <summary>  Over the past decade, the Cyberspace has seen an increasing number of attacks
coming from botnets using the Peer-to-Peer (P2P) architecture. Peer-to-Peer
botnets use a decentralized Command &amp; Control architecture. Moreover, a large
number of such botnets already exist, and newer versions- which significantly
differ from their parent bot- are also discovered practically every year. In
this work, the authors propose and implement a novel hybrid framework for
detecting P2P botnets in live network traffic by integrating Neural Networks
with Bayesian Regularization. Bayesian Regularization helps in achieving better
generalization of the dataset, thereby enabling the detection of botnet
activity even of those bots which were never used in training the Neural
Network. Hence such a framework is suitable for detection of newer and unseen
botnets in live traffic of a network. This was verified by testing the
Framework on test data unseen to the Detection module (using untrained botnet
dataset), and the authors were successful in detecting this activity with an
accuracy of 99.2 %.
</summary>
    <author>
      <name>Sharath Chandra Guntuku</name>
    </author>
    <author>
      <name>Pratik Narang</name>
    </author>
    <author>
      <name>Chittaranjan Hota</name>
    </author>
    <link href="http://arxiv.org/abs/1307.7464v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.7464v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.2032v1</id>
    <updated>2011-06-10T12:24:07Z</updated>
    <published>2011-06-10T12:24:07Z</published>
    <title>Storage capacity of phase-coded patterns in sparse neural networks</title>
    <summary>  We study the storage of multiple phase-coded patterns as stable dynamical
attractors in recurrent neural networks with sparse connectivity. To determine
the synaptic strength of existent connections and store the phase-coded
patterns, we introduce a learning rule inspired to the spike-timing dependent
plasticity (STDP). We find that, after learning, the spontaneous dynamics of
the network replay one of the stored dynamical patterns, depending on the
network initialization. We study the network capacity as a function of
topology, and find that a small- world-like topology may be optimal, as a
compromise between the high wiring cost of long range connections and the
capacity increase.
</summary>
    <author>
      <name>Siliva Scarpetta</name>
    </author>
    <author>
      <name>Ferdinando Giacco</name>
    </author>
    <author>
      <name>Antonio de Candia</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1209/0295-5075/95/28006</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1209/0295-5075/95/28006" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in Europhysics Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.2032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.2032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.4540v1</id>
    <updated>2014-03-18T17:15:21Z</updated>
    <published>2014-03-18T17:15:21Z</published>
    <title>Similarity networks for classification: a case study in the Horse Colic
  problem</title>
    <summary>  This paper develops a two-layer neural network in which the neuron model
computes a user-defined similarity function between inputs and weights. The
neuron transfer function is formed by composition of an adapted logistic
function with the mean of the partial input-weight similarities. The resulting
neuron model is capable of dealing directly with variables of potentially
different nature (continuous, fuzzy, ordinal, categorical). There is also
provision for missing values. The network is trained using a two-stage
procedure very similar to that used to train a radial basis function (RBF)
neural network. The network is compared to two types of RBF networks in a
non-trivial dataset: the Horse Colic problem, taken as a case study and
analyzed in detail.
</summary>
    <author>
      <name>Llu√≠s Belanche</name>
    </author>
    <author>
      <name>Jer√≥nimo Hern√°ndez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 1 figure Universitat Polit\`ecnica de Catalunya preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.4540v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.4540v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.7604v1</id>
    <updated>2014-09-26T15:39:55Z</updated>
    <published>2014-09-26T15:39:55Z</published>
    <title>Stochastic mean field formulation of the dynamics of diluted neural
  networks</title>
    <summary>  We consider pulse-coupled Leaky Integrate-and-Fire neural networks with
randomly distributed synaptic couplings. This random dilution induces
fluctuations in the evolution of the macroscopic variables and deterministic
chaos at the microscopic level. Our main aim is to mimic the effect of the
dilution as a noise source acting on the dynamics of a globally coupled
non-chaotic system. Indeed, the evolution of a diluted neural network can be
well approximated as a fully pulse coupled network, where each neuron is driven
by a mean synaptic current plus additive noise. These terms represent the
average and the fluctuations of the synaptic currents acting on the single
neurons in the diluted system. The main microscopic and macroscopic dynamical
features can be retrieved with this stochastic approximation. Furthermore, the
microscopic stability of the diluted network can be also reproduced, as
demonstrated from the almost coincidence of the measured Lyapunov exponents in
the deterministic and stochastic cases for an ample range of system sizes. Our
results strongly suggest that the fluctuations in the synaptic currents are
responsible for the emergence of chaos in this class of pulse coupled networks.
</summary>
    <author>
      <name>D. Angulo-Garcia</name>
    </author>
    <author>
      <name>A. Torcini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.91.022928</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.91.022928" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 Pages, 4 Figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 91, 022928 (2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1409.7604v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.7604v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.5068v4</id>
    <updated>2015-04-09T21:43:29Z</updated>
    <published>2014-12-11T23:03:49Z</published>
    <title>Towards Deep Neural Network Architectures Robust to Adversarial Examples</title>
    <summary>  Recent work has shown deep neural networks (DNNs) to be highly susceptible to
well-designed, small perturbations at the input layer, or so-called adversarial
examples. Taking images as an example, such distortions are often
imperceptible, but can result in 100% mis-classification for a state of the art
DNN. We study the structure of adversarial examples and explore network
topology, pre-processing and training strategies to improve the robustness of
DNNs. We perform various experiments to assess the removability of adversarial
examples by corrupting with additional noise and pre-processing with denoising
autoencoders (DAEs). We find that DAEs can remove substantial amounts of the
adversarial noise. How- ever, when stacking the DAE with the original DNN, the
resulting network can again be attacked by new adversarial examples with even
smaller distortion. As a solution, we propose Deep Contractive Network, a model
with a new end-to-end training procedure that includes a smoothness penalty
inspired by the contractive autoencoder (CAE). This increases the network
robustness to adversarial examples, without a significant performance penalty.
</summary>
    <author>
      <name>Shixiang Gu</name>
    </author>
    <author>
      <name>Luca Rigazio</name>
    </author>
    <link href="http://arxiv.org/abs/1412.5068v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.5068v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.07866v2</id>
    <updated>2016-06-29T14:11:07Z</updated>
    <published>2015-05-28T21:34:57Z</published>
    <title>Learning universal computations with spikes</title>
    <summary>  Providing the neurobiological basis of information processing in higher
animals, spiking neural networks must be able to learn a variety of complicated
computations, including the generation of appropriate, possibly delayed
reactions to inputs and the self-sustained generation of complex activity
patterns, e.g.~for locomotion. Many such computations require previous building
of intrinsic world models. Here we show how spiking neural networks may solve
these different tasks. Firstly, we derive constraints under which classes of
spiking neural networks lend themselves to substrates of powerful general
purpose computing. The networks contain dendritic or synaptic nonlinearities
and have a constrained connectivity. We then combine such networks with
learning rules for outputs or recurrent connections. We show that this allows
to learn even difficult benchmark tasks such as the self-sustained generation
of desired low-dimensional chaotic dynamics or memory-dependent computations.
Furthermore, we show how spiking networks can build models of external world
systems and use the acquired knowledge to control them.
</summary>
    <author>
      <name>Dominik Thalmeier</name>
    </author>
    <author>
      <name>Marvin Uhlmann</name>
    </author>
    <author>
      <name>Hilbert J. Kappen</name>
    </author>
    <author>
      <name>Raoul-Martin Memmesheimer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pcbi.1004895</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pcbi.1004895" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">PLoS Comput Biol 12(6): e1004895 (2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1505.07866v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.07866v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.06228v2</id>
    <updated>2015-11-23T16:25:30Z</updated>
    <published>2015-07-22T15:29:14Z</published>
    <title>Training Very Deep Networks</title>
    <summary>  Theoretical and empirical evidence indicates that the depth of neural
networks is crucial for their success. However, training becomes more difficult
as depth increases, and training of very deep networks remains an open problem.
Here we introduce a new architecture designed to overcome this. Our so-called
highway networks allow unimpeded information flow across many layers on
information highways. They are inspired by Long Short-Term Memory recurrent
networks and use adaptive gating units to regulate the information flow. Even
with hundreds of layers, highway networks can be trained directly through
simple gradient descent. This enables the study of extremely deep and efficient
architectures.
</summary>
    <author>
      <name>Rupesh Kumar Srivastava</name>
    </author>
    <author>
      <name>Klaus Greff</name>
    </author>
    <author>
      <name>J√ºrgen Schmidhuber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages. Extends arXiv:1505.00387. Project webpage is at
  http://people.idsia.ch/~rupesh/very_deep_learning/. in Advances in Neural
  Information Processing Systems 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.06228v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.06228v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; G.1.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.03776v3</id>
    <updated>2016-04-13T02:56:43Z</updated>
    <published>2015-11-12T05:06:16Z</published>
    <title>ProNet: Learning to Propose Object-specific Boxes for Cascaded Neural
  Networks</title>
    <summary>  This paper aims to classify and locate objects accurately and efficiently,
without using bounding box annotations. It is challenging as objects in the
wild could appear at arbitrary locations and in different scales. In this
paper, we propose a novel classification architecture ProNet based on
convolutional neural networks. It uses computationally efficient neural
networks to propose image regions that are likely to contain objects, and
applies more powerful but slower networks on the proposed regions. The basic
building block is a multi-scale fully-convolutional network which assigns
object confidence scores to boxes at different locations and scales. We show
that such networks can be trained effectively using image-level annotations,
and can be connected into cascades or trees for efficient object
classification. ProNet outperforms previous state-of-the-art significantly on
PASCAL VOC 2012 and MS COCO datasets for object classification and point-based
localization.
</summary>
    <author>
      <name>Chen Sun</name>
    </author>
    <author>
      <name>Manohar Paluri</name>
    </author>
    <author>
      <name>Ronan Collobert</name>
    </author>
    <author>
      <name>Ram Nevatia</name>
    </author>
    <author>
      <name>Lubomir Bourdev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2016 (fixed reference issue)</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.03776v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.03776v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.04210v3</id>
    <updated>2016-06-14T05:39:27Z</updated>
    <published>2015-11-13T09:35:34Z</published>
    <title>On the Quality of the Initial Basin in Overspecified Neural Networks</title>
    <summary>  Deep learning, in the form of artificial neural networks, has achieved
remarkable practical success in recent years, for a variety of difficult
machine learning applications. However, a theoretical explanation for this
remains a major open problem, since training neural networks involves
optimizing a highly non-convex objective function, and is known to be
computationally hard in the worst case. In this work, we study the
\emph{geometric} structure of the associated non-convex objective function, in
the context of ReLU networks and starting from a random initialization of the
network parameters. We identify some conditions under which it becomes more
favorable to optimization, in the sense of (i) High probability of initializing
at a point from which there is a monotonically decreasing path to a global
minimum; and (ii) High probability of initializing at a basin (suitably
defined) with a small minimal objective value. A common theme in our results is
that such properties are more likely to hold for larger ("overspecified")
networks, which accords with some recent empirical and theoretical
observations.
</summary>
    <author>
      <name>Itay Safran</name>
    </author>
    <author>
      <name>Ohad Shamir</name>
    </author>
    <link href="http://arxiv.org/abs/1511.04210v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.04210v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.06951v1</id>
    <updated>2015-11-22T02:33:08Z</updated>
    <published>2015-11-22T02:33:08Z</published>
    <title>Gradual DropIn of Layers to Train Very Deep Neural Networks</title>
    <summary>  We introduce the concept of dynamically growing a neural network during
training. In particular, an untrainable deep network starts as a trainable
shallow network and newly added layers are slowly, organically added during
training, thereby increasing the network's depth. This is accomplished by a new
layer, which we call DropIn. The DropIn layer starts by passing the output from
a previous layer (effectively skipping over the newly added layers), then
increasingly including units from the new layers for both feedforward and
backpropagation. We show that deep networks, which are untrainable with
conventional methods, will converge with DropIn layers interspersed in the
architecture. In addition, we demonstrate that DropIn provides regularization
during training in an analogous way as dropout. Experiments are described with
the MNIST dataset and various expanded LeNet architectures, CIFAR-10 dataset
with its architecture expanded from 3 to 11 layers, and on the ImageNet dataset
with the AlexNet architecture expanded to 13 layers and the VGG 16-layer
architecture.
</summary>
    <author>
      <name>Leslie N. Smith</name>
    </author>
    <author>
      <name>Emily M. Hand</name>
    </author>
    <author>
      <name>Timothy Doster</name>
    </author>
    <link href="http://arxiv.org/abs/1511.06951v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.06951v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.03850v1</id>
    <updated>2015-12-12T00:08:27Z</updated>
    <published>2015-12-12T00:08:27Z</published>
    <title>Minimal Perceptrons for Memorizing Complex Patterns</title>
    <summary>  Feedforward neural networks have been investigated to understand learning and
memory, as well as applied to numerous practical problems in pattern
classification. It is a rule of thumb that more complex tasks require larger
networks. However, the design of optimal network architectures for specific
tasks is still an unsolved fundamental problem. In this study, we consider
three-layered neural networks for memorizing binary patterns. We developed a
new complexity measure of binary patterns, and estimated the minimal network
size for memorizing them as a function of their complexity. We formulated the
minimal network size for regular, random, and complex patterns. In particular,
the minimal size for complex patterns, which are neither ordered nor
disordered, was predicted by measuring their Hamming distances from known
ordered patterns. Our predictions agreed with simulations based on the
back-propagation algorithm.
</summary>
    <author>
      <name>Marissa Pastor</name>
    </author>
    <author>
      <name>Juyong Song</name>
    </author>
    <author>
      <name>Danh-Tai Hoang</name>
    </author>
    <author>
      <name>Junghyo Jo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physa.2016.06.025</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physa.2016.06.025" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physica A 462:31-37 (2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1512.03850v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.03850v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.02189v1</id>
    <updated>2016-01-10T09:07:09Z</updated>
    <published>2016-01-10T09:07:09Z</published>
    <title>Mimicking Collective Firing Patterns of Hundreds of Connected Neurons
  using a Single-Neuron Experiment</title>
    <summary>  The experimental study of neural networks requires simultaneous measurements
of a massive number of neurons, while monitoring properties of the
connectivity, synaptic strengths and delays. Current technological barriers
make such a mission unachievable. In addition, as a result of the enormous
number of required measurements, the estimated network parameters would differ
from the original ones. Here we present a versatile experimental technique,
which enables the study of recurrent neural networks activity while being
capable of dictating the network connectivity and synaptic strengths. This
method is based on the observation that the response of neurons depends solely
on their recent stimulations, a short-term memory. It allows a long-term scheme
of stimulation and recording of a single neuron, to mimic simultaneous activity
measurements of neurons in a recurrent network. Utilization of this technique
demonstrates the spontaneous emergence of cooperative synchronous oscillations,
in particular the coexistence of fast Gamma and slow Delta oscillations, and
opens the horizon for the experimental study of other cooperative phenomena
within large-scale neural networks.
</summary>
    <author>
      <name>Amir Goldental</name>
    </author>
    <author>
      <name>Pinhas Sabo</name>
    </author>
    <author>
      <name>Shira Sardi</name>
    </author>
    <author>
      <name>Roni Vardi</name>
    </author>
    <author>
      <name>Ido Kanter</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3389/fnins.2015.00508</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3389/fnins.2015.00508" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages and 6 figures,
  http://journal.frontiersin.org/article/10.3389/fnins.2015.00508/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Front. Neurosci. 9:508 (2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1601.02189v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.02189v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.04485v2</id>
    <updated>2016-05-27T22:11:26Z</updated>
    <published>2016-02-14T18:36:59Z</published>
    <title>Benefits of depth in neural networks</title>
    <summary>  For any positive integer $k$, there exist neural networks with $\Theta(k^3)$
layers, $\Theta(1)$ nodes per layer, and $\Theta(1)$ distinct parameters which
can not be approximated by networks with $\mathcal{O}(k)$ layers unless they
are exponentially large --- they must possess $\Omega(2^k)$ nodes. This result
is proved here for a class of nodes termed "semi-algebraic gates" which
includes the common choices of ReLU, maximum, indicator, and piecewise
polynomial functions, therefore establishing benefits of depth against not just
standard networks with ReLU gates, but also convolutional networks with ReLU
and maximization gates, sum-product networks, and boosted decision trees (in
this last case with a stronger separation: $\Omega(2^{k^3})$ total tree nodes
are required).
</summary>
    <author>
      <name>Matus Telgarsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear, COLT 2016. For a simplified version, see
  http://arxiv.org/abs/1509.08101</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.04485v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.04485v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.01025v2</id>
    <updated>2016-03-17T03:32:30Z</updated>
    <published>2016-03-03T08:51:52Z</published>
    <title>Convolutional Neural Networks using Logarithmic Data Representation</title>
    <summary>  Recent advances in convolutional neural networks have considered model
complexity and hardware efficiency to enable deployment onto embedded systems
and mobile devices. For example, it is now well-known that the arithmetic
operations of deep networks can be encoded down to 8-bit fixed-point without
significant deterioration in performance. However, further reduction in
precision down to as low as 3-bit fixed-point results in significant losses in
performance. In this paper we propose a new data representation that enables
state-of-the-art networks to be encoded to 3 bits with negligible loss in
classification performance. To perform this, we take advantage of the fact that
the weights and activations in a trained network naturally have non-uniform
distributions. Using non-uniform, base-2 logarithmic representation to encode
weights, communicate activations, and perform dot-products enables networks to
1) achieve higher classification accuracies than fixed-point at the same
resolution and 2) eliminate bulky digital multipliers. Finally, we propose an
end-to-end training procedure that uses log representation at 5-bits, which
achieves higher final test accuracy than linear at 5-bits.
</summary>
    <author>
      <name>Daisuke Miyashita</name>
    </author>
    <author>
      <name>Edward H. Lee</name>
    </author>
    <author>
      <name>Boris Murmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.01025v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.01025v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09260v2</id>
    <updated>2016-06-03T14:45:35Z</updated>
    <published>2016-03-30T16:16:57Z</published>
    <title>Degrees of Freedom in Deep Neural Networks</title>
    <summary>  In this paper, we explore degrees of freedom in deep sigmoidal neural
networks. We show that the degrees of freedom in these models is related to the
expected optimism, which is the expected difference between test error and
training error. We provide an efficient Monte-Carlo method to estimate the
degrees of freedom for multi-class classification methods. We show degrees of
freedom are lower than the parameter count in a simple XOR network. We extend
these results to neural nets trained on synthetic and real data, and
investigate impact of network's architecture and different regularization
choices. The degrees of freedom in deep networks are dramatically smaller than
the number of parameters, in some real datasets several orders of magnitude.
Further, we observe that for fixed number of parameters, deeper networks have
less degrees of freedom exhibiting a regularization-by-depth.
</summary>
    <author>
      <name>Tianxiang Gao</name>
    </author>
    <author>
      <name>Vladimir Jojic</name>
    </author>
    <link href="http://arxiv.org/abs/1603.09260v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09260v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.03099v1</id>
    <updated>2016-04-11T05:17:09Z</updated>
    <published>2016-04-11T05:17:09Z</published>
    <title>Symbolic Knowledge Extraction using ≈Åukasiewicz Logics</title>
    <summary>  This work describes a methodology that combines logic-based systems and
connectionist systems. Our approach uses finite truth-valued {\L}ukasiewicz
logic, wherein every connective can be defined by a neuron in an artificial
network. This allowed the injection of first-order formulas into a network
architecture, and also simplified symbolic rule extraction. For that we trained
a neural networks using the Levenderg-Marquardt algorithm, where we restricted
the knowledge dissemination in the network structure. This procedure reduces
neural network plasticity without drastically damaging the learning
performance, thus making the descriptive power of produced neural networks
similar to the descriptive power of {\L}ukasiewicz logic language and
simplifying the translation between symbolic and connectionist structures. We
used this method for reverse engineering truth table and in extraction of
formulas from real data sets.
</summary>
    <author>
      <name>Carlos Leandro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages. arXiv admin note: substantial text overlap with
  arXiv:1604.02780, arXiv:1604.02774</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.03099v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03099v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="03B52, 92B20, 68T05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06929v1</id>
    <updated>2016-04-23T17:36:12Z</updated>
    <published>2016-04-23T17:36:12Z</published>
    <title>Memory and Information Processing in Recurrent Neural Networks</title>
    <summary>  Recurrent neural networks (RNN) are simple dynamical systems whose
computational power has been attributed to their short-term memory. Short-term
memory of RNNs has been previously studied analytically only for the case of
orthogonal networks, and only under annealed approximation, and uncorrelated
input. Here for the first time, we present an exact solution to the memory
capacity and the task-solving performance as a function of the structure of a
given network instance, enabling direct determination of the
function--structure relation in RNNs. We calculate the memory capacity for
arbitrary networks with exponentially correlated input and further related it
to the performance of the system on signal processing tasks in a supervised
learning setup. We compute the expected error and the worst-case error bound as
a function of the spectra of the network and the correlation structure of its
inputs and outputs. Our results give an explanation for learning and
generalization of task solving using short-term memory, which is crucial for
building alternative computer architectures using physical phenomena based on
the short-term memory principle.
</summary>
    <author>
      <name>Alireza Goudarzi</name>
    </author>
    <author>
      <name>Sarah Marzen</name>
    </author>
    <author>
      <name>Peter Banda</name>
    </author>
    <author>
      <name>Guy Feldman</name>
    </author>
    <author>
      <name>Christof Teuscher</name>
    </author>
    <author>
      <name>Darko Stefanovic</name>
    </author>
    <link href="http://arxiv.org/abs/1604.06929v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06929v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01981v1</id>
    <updated>2016-06-07T00:28:42Z</updated>
    <published>2016-06-07T00:28:42Z</published>
    <title>Deep neural networks are robust to weight binarization and other
  non-linear distortions</title>
    <summary>  Recent results show that deep neural networks achieve excellent performance
even when, during training, weights are quantized and projected to a binary
representation. Here, we show that this is just the tip of the iceberg: these
same networks, during testing, also exhibit a remarkable robustness to
distortions beyond quantization, including additive and multiplicative noise,
and a class of non-linear projections where binarization is just a special
case. To quantify this robustness, we show that one such network achieves 11%
test error on CIFAR-10 even with 0.68 effective bits per weight. Furthermore,
we find that a common training heuristic--namely, projecting quantized weights
during backpropagation--can be altered (or even removed) and networks still
achieve a base level of robustness during testing. Specifically, training with
weight projections other than quantization also works, as does simply clipping
the weights, both of which have never been reported before. We confirm our
results for CIFAR-10 and ImageNet datasets. Finally, drawing from these ideas,
we propose a stochastic projection rule that leads to a new state of the art
network with 7.64% test error on CIFAR-10 using no data augmentation.
</summary>
    <author>
      <name>Paul Merolla</name>
    </author>
    <author>
      <name>Rathinakumar Appuswamy</name>
    </author>
    <author>
      <name>John Arthur</name>
    </author>
    <author>
      <name>Steve K. Esser</name>
    </author>
    <author>
      <name>Dharmendra Modha</name>
    </author>
    <link href="http://arxiv.org/abs/1606.01981v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01981v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.00421v1</id>
    <updated>2016-11-01T23:17:55Z</updated>
    <published>2016-11-01T23:17:55Z</published>
    <title>Flood-Filling Networks</title>
    <summary>  State-of-the-art image segmentation algorithms generally consist of at least
two successive and distinct computations: a boundary detection process that
uses local image information to classify image locations as boundaries between
objects, followed by a pixel grouping step such as watershed or connected
components that clusters pixels into segments. Prior work has varied the
complexity and approach employed in these two steps, including the
incorporation of multi-layer neural networks to perform boundary prediction,
and the use of global optimizations during pixel clustering. We propose a
unified and end-to-end trainable machine learning approach, flood-filling
networks, in which a recurrent 3d convolutional network directly produces
individual segments from a raw image. The proposed approach robustly segments
images with an unknown and variable number of objects as well as highly
variable object sizes. We demonstrate the approach on a challenging 3d image
segmentation task, connectomic reconstruction from volume electron microscopy
data, on which flood-filling neural networks substantially improve accuracy
over other state-of-the-art methods. The proposed approach can replace complex
multi-step segmentation pipelines with a single neural network that is learned
end-to-end.
</summary>
    <author>
      <name>Micha≈Ç Januszewski</name>
    </author>
    <author>
      <name>Jeremy Maitin-Shepard</name>
    </author>
    <author>
      <name>Peter Li</name>
    </author>
    <author>
      <name>J√∂rgen Kornfeld</name>
    </author>
    <author>
      <name>Winfried Denk</name>
    </author>
    <author>
      <name>Viren Jain</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.00421v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.00421v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.01714v1</id>
    <updated>2016-11-06T01:32:39Z</updated>
    <published>2016-11-06T01:32:39Z</published>
    <title>Beyond Fine Tuning: A Modular Approach to Learning on Small Data</title>
    <summary>  In this paper we present a technique to train neural network models on small
amounts of data. Current methods for training neural networks on small amounts
of rich data typically rely on strategies such as fine-tuning a pre-trained
neural network or the use of domain-specific hand-engineered features. Here we
take the approach of treating network layers, or entire networks, as modules
and combine pre-trained modules with untrained modules, to learn the shift in
distributions between data sets. The central impact of using a modular approach
comes from adding new representations to a network, as opposed to replacing
representations via fine-tuning. Using this technique, we are able surpass
results using standard fine-tuning transfer learning approaches, and we are
also able to significantly increase performance over such approaches when using
smaller amounts of data.
</summary>
    <author>
      <name>Ark Anderson</name>
    </author>
    <author>
      <name>Kyle Shaffer</name>
    </author>
    <author>
      <name>Artem Yankov</name>
    </author>
    <author>
      <name>Court D. Corley</name>
    </author>
    <author>
      <name>Nathan O. Hodas</name>
    </author>
    <link href="http://arxiv.org/abs/1611.01714v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.01714v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.06937v1</id>
    <updated>2016-11-21T18:38:17Z</updated>
    <published>2016-11-21T18:38:17Z</published>
    <title>Using inspiration from synaptic plasticity rules to optimize traffic
  flow in distributed engineered networks</title>
    <summary>  Controlling the flow and routing of data is a fundamental problem in many
distributed networks, including transportation systems, integrated circuits,
and the Internet. In the brain, synaptic plasticity rules have been discovered
that regulate network activity in response to environmental inputs, which
enable circuits to be stable yet flexible. Here, we develop a new
neuro-inspired model for network flow control that only depends on modifying
edge weights in an activity-dependent manner. We show how two fundamental
plasticity rules (long-term potentiation and long-term depression) can be cast
as a distributed gradient descent algorithm for regulating traffic flow in
engineered networks. We then characterize, both via simulation and
analytically, how different forms of edge-weight update rules affect network
routing efficiency and robustness. We find a close correspondence between
certain classes of synaptic weight update rules derived experimentally in the
brain and rules commonly used in engineering, suggesting common principles to
both.
</summary>
    <author>
      <name>Jonathan Y. Suen</name>
    </author>
    <author>
      <name>Saket Navlakha</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/NECO_a_00945</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/NECO_a_00945" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">43 pages, 5 Figures. Submitted to Neural Computation</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Comput. 29(5) (2017) 1204-1228</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1611.06937v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.06937v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.08258v1</id>
    <updated>2016-11-24T17:07:48Z</updated>
    <published>2016-11-24T17:07:48Z</published>
    <title>Weakly Supervised Cascaded Convolutional Networks</title>
    <summary>  Object detection is a challenging task in visual understanding domain, and
even more so if the supervision is to be weak. Recently, few efforts to handle
the task without expensive human annotations is established by promising deep
neural network. A new architecture of cascaded networks is proposed to learn a
convolutional neural network (CNN) under such conditions. We introduce two such
architectures, with either two cascade stages or three which are trained in an
end-to-end pipeline. The first stage of both architectures extracts best
candidate of class specific region proposals by training a fully convolutional
network. In the case of the three stage architecture, the middle stage provides
object segmentation, using the output of the activation maps of first stage.
The final stage of both architectures is a part of a convolutional neural
network that performs multiple instance learning on proposals extracted in the
previous stage(s). Our experiments on the PASCAL VOC 2007, 2010, 2012 and large
scale object datasets, ILSVRC 2013, 2014 datasets show improvements in the
areas of weakly-supervised object detection, classification and localization.
</summary>
    <author>
      <name>Ali Diba</name>
    </author>
    <author>
      <name>Vivek Sharma</name>
    </author>
    <author>
      <name>Ali Pazandeh</name>
    </author>
    <author>
      <name>Hamed Pirsiavash</name>
    </author>
    <author>
      <name>Luc Van Gool</name>
    </author>
    <link href="http://arxiv.org/abs/1611.08258v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.08258v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.08588v2</id>
    <updated>2016-12-09T22:30:17Z</updated>
    <published>2016-11-23T17:43:28Z</published>
    <title>PVANet: Lightweight Deep Neural Networks for Real-time Object Detection</title>
    <summary>  In object detection, reducing computational cost is as important as improving
accuracy for most practical usages. This paper proposes a novel network
structure, which is an order of magnitude lighter than other state-of-the-art
networks while maintaining the accuracy. Based on the basic principle of more
layers with less channels, this new deep neural network minimizes its
redundancy by adopting recent innovations including C.ReLU and Inception
structure. We also show that this network can be trained efficiently to achieve
solid results on well-known object detection benchmarks: 84.9% and 84.2% mAP on
VOC2007 and VOC2012 while the required compute is less than 10% of the recent
ResNet-101.
</summary>
    <author>
      <name>Sanghoon Hong</name>
    </author>
    <author>
      <name>Byungseok Roh</name>
    </author>
    <author>
      <name>Kye-Hyeon Kim</name>
    </author>
    <author>
      <name>Yeongjae Cheon</name>
    </author>
    <author>
      <name>Minje Park</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at NIPS 2016 Workshop on Efficient Methods for Deep Neural
  Networks (EMDNN). Continuation of arXiv:1608.08021. The affiliation has been
  corrected</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.08588v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.08588v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.01396v2</id>
    <updated>2017-05-21T15:19:50Z</updated>
    <published>2017-03-04T04:31:43Z</published>
    <title>Stacking-based Deep Neural Network: Deep Analytic Network on
  Convolutional Spectral Histogram Features</title>
    <summary>  Stacking-based deep neural network (S-DNN), in general, denotes a deep neural
network (DNN) resemblance in terms of its very deep, feedforward network
architecture. The typical S-DNN aggregates a variable number of individually
learnable modules in series to assemble a DNN-alike alternative to the targeted
object recognition tasks. This work likewise devises an S-DNN instantiation,
dubbed deep analytic network (DAN), on top of the spectral histogram (SH)
features. The DAN learning principle relies on ridge regression, and some key
DNN constituents, specifically, rectified linear unit, fine-tuning, and
normalization. The DAN aptitude is scrutinized on three repositories of varying
domains, including FERET (faces), MNIST (handwritten digits), and CIFAR10
(natural objects). The empirical results unveil that DAN escalates the SH
baseline performance over a sufficiently deep layer.
</summary>
    <author>
      <name>Cheng-Yaw Low</name>
    </author>
    <author>
      <name>Andrew Beng-Jin Teoh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.01396v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.01396v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.08245v1</id>
    <updated>2017-03-23T22:25:05Z</updated>
    <published>2017-03-23T22:25:05Z</published>
    <title>On the Robustness of Convolutional Neural Networks to Internal
  Architecture and Weight Perturbations</title>
    <summary>  Deep convolutional neural networks are generally regarded as robust function
approximators. So far, this intuition is based on perturbations to external
stimuli such as the images to be classified. Here we explore the robustness of
convolutional neural networks to perturbations to the internal weights and
architecture of the network itself. We show that convolutional networks are
surprisingly robust to a number of internal perturbations in the higher
convolutional layers but the bottom convolutional layers are much more fragile.
For instance, Alexnet shows less than a 30% decrease in classification
performance when randomly removing over 70% of weight connections in the top
convolutional or dense layers but performance is almost at chance with the same
perturbation in the first convolutional layer. Finally, we suggest further
investigations which could continue to inform the robustness of convolutional
networks to internal perturbations.
</summary>
    <author>
      <name>Nicholas Cheney</name>
    </author>
    <author>
      <name>Martin Schrimpf</name>
    </author>
    <author>
      <name>Gabriel Kreiman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">under review at ICML 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.08245v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.08245v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.09202v1</id>
    <updated>2017-03-27T17:45:07Z</updated>
    <published>2017-03-27T17:45:07Z</published>
    <title>Biologically inspired protection of deep networks from adversarial
  attacks</title>
    <summary>  Inspired by biophysical principles underlying nonlinear dendritic computation
in neural circuits, we develop a scheme to train deep neural networks to make
them robust to adversarial attacks. Our scheme generates highly nonlinear,
saturated neural networks that achieve state of the art performance on gradient
based adversarial examples on MNIST, despite never being exposed to
adversarially chosen examples during training. Moreover, these networks exhibit
unprecedented robustness to targeted, iterative schemes for generating
adversarial examples, including second-order methods. We further identify
principles governing how these networks achieve their robustness, drawing on
methods from information geometry. We find these networks progressively create
highly flat and compressed internal representations that are sensitive to very
few input dimensions, while still solving the task. Moreover, they employ
highly kurtotic weight distributions, also found in the brain, and we
demonstrate how such kurtosis can protect even linear classifiers from
adversarial attack.
</summary>
    <author>
      <name>Aran Nayebi</name>
    </author>
    <author>
      <name>Surya Ganguli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.09202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.09202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.02286v1</id>
    <updated>2017-04-07T16:40:13Z</updated>
    <published>2017-04-07T16:40:13Z</published>
    <title>Threat analysis of IoT networks Using Artificial Neural Network
  Intrusion Detection System</title>
    <summary>  The Internet of things (IoT) is still in its infancy and has attracted much
interest in many industrial sectors including medical fields, logistics
tracking, smart cities and automobiles. However as a paradigm, it is
susceptible to a range of significant intrusion threats. This paper presents a
threat analysis of the IoT and uses an Artificial Neural Network (ANN) to
combat these threats. A multi-level perceptron, a type of supervised ANN, is
trained using internet packet traces, then is assessed on its ability to thwart
Distributed Denial of Service (DDoS/DoS) attacks. This paper focuses on the
classification of normal and threat patterns on an IoT Network. The ANN
procedure is validated against a simulated IoT network. The experimental
results demonstrate 99.4% accuracy and can successfully detect various DDoS/DoS
attacks.
</summary>
    <author>
      <name>Elike Hodo</name>
    </author>
    <author>
      <name>Xavier Bellekens</name>
    </author>
    <author>
      <name>Andrew Hamilton</name>
    </author>
    <author>
      <name>Pierre-louis Dubouilh</name>
    </author>
    <author>
      <name>Ephraim Iorkyase</name>
    </author>
    <author>
      <name>Christos Tachtatzis</name>
    </author>
    <author>
      <name>Robert Atkinson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ISNCC.2016.7746067</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ISNCC.2016.7746067" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in The 2016 International Symposium on Networks, Computers
  and Communications (IEEE ISNCC'16) , Hammamet, Tunisia, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.02286v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.02286v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.03993v3</id>
    <updated>2017-05-06T17:14:00Z</updated>
    <published>2017-04-13T05:04:44Z</published>
    <title>ApproxDBN: Approximate Computing for Discriminative Deep Belief Networks</title>
    <summary>  Probabilistic generative neural networks are useful for many applications,
such as image classification, speech recognition and occlusion removal.
However, the power budget for hardware implementations of neural networks can
be extremely tight. To address this challenge we describe a design methodology
for using approximate computing methods to implement Approximate Deep Belief
Networks (ApproxDBNs) by systematically exploring the use of (1) limited
precision of variables; (2) criticality analysis to identify the nodes in the
network which can operate with such limited precision while allowing the
network to maintain target accuracy levels; and (3) a greedy search methodology
with incremental retraining to determine the optimal reduction in precision to
enable maximize power savings under user-specified accuracy constraints.
Experimental results show that significant bit-length reduction can be achieved
by our ApproxDBN with constrained accuracy loss.
</summary>
    <author>
      <name>Xiaojing Xu</name>
    </author>
    <author>
      <name>Srinjoy Das</name>
    </author>
    <author>
      <name>Ken Kreutz-Delgado</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.03993v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.03993v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.07595v1</id>
    <updated>2017-04-25T09:09:00Z</updated>
    <published>2017-04-25T09:09:00Z</published>
    <title>Skeleton-based Action Recognition with Convolutional Neural Networks</title>
    <summary>  Current state-of-the-art approaches to skeleton-based action recognition are
mostly based on recurrent neural networks (RNN). In this paper, we propose a
novel convolutional neural networks (CNN) based framework for both action
classification and detection. Raw skeleton coordinates as well as skeleton
motion are fed directly into CNN for label prediction. A novel skeleton
transformer module is designed to rearrange and select important skeleton
joints automatically. With a simple 7-layer network, we obtain 89.3% accuracy
on validation set of the NTU RGB+D dataset. For action detection in untrimmed
videos, we develop a window proposal network to extract temporal segment
proposals, which are further classified within the same network. On the recent
PKU-MMD dataset, we achieve 93.7% mAP, surpassing the baseline by a large
margin.
</summary>
    <author>
      <name>Chao Li</name>
    </author>
    <author>
      <name>Qiaoyong Zhong</name>
    </author>
    <author>
      <name>Di Xie</name>
    </author>
    <author>
      <name>Shiliang Pu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LSP.2017.2678539</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LSP.2017.2678539" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICMEW 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.07595v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.07595v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.08045v2</id>
    <updated>2017-06-12T19:43:39Z</updated>
    <published>2017-04-26T10:24:54Z</published>
    <title>The loss surface of deep and wide neural networks</title>
    <summary>  While the optimization problem behind deep neural networks is highly
non-convex, it is frequently observed in practice that training deep networks
seems possible without getting stuck in suboptimal points. It has been argued
that this is the case as all local minima are close to being globally optimal.
We show that this is (almost) true, in fact almost all local minima are
globally optimal, for a fully connected network with squared loss and analytic
activation function given that the number of hidden units of one layer of the
network is larger than the number of training points and the network structure
from this layer on is pyramidal.
</summary>
    <author>
      <name>Quynh Nguyen</name>
    </author>
    <author>
      <name>Matthias Hein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2017. Main results now hold for larger classes of loss functions</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.08045v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.08045v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07706v1</id>
    <updated>2017-05-22T13:14:11Z</updated>
    <published>2017-05-22T13:14:11Z</published>
    <title>An Out-of-the-box Full-network Embedding for Convolutional Neural
  Networks</title>
    <summary>  Transfer learning for feature extraction can be used to exploit deep
representations in contexts where there is very few training data, where there
are limited computational resources, or when tuning the hyper-parameters needed
for training is not an option. While previous contributions to feature
extraction propose embeddings based on a single layer of the network, in this
paper we propose a full-network embedding which successfully integrates
convolutional and fully connected features, coming from all layers of a deep
convolutional neural network. To do so, the embedding normalizes features in
the context of the problem, and discretizes their values to reduce noise and
regularize the embedding space. Significantly, this also reduces the
computational cost of processing the resultant representations. The proposed
method is shown to outperform single layer embeddings on several image
classification tasks, while also being more robust to the choice of the
pre-trained model used for obtaining the initial features. The performance gap
in classification accuracy between thoroughly tuned solutions and the
full-network embedding is also reduced, which makes of the proposed approach a
competitive solution for a large set of applications.
</summary>
    <author>
      <name>Dario Garcia-Gasulla</name>
    </author>
    <author>
      <name>Armand Vilalta</name>
    </author>
    <author>
      <name>Ferran Par√©s</name>
    </author>
    <author>
      <name>Jonatan Moreno</name>
    </author>
    <author>
      <name>Eduard Ayguad√©</name>
    </author>
    <author>
      <name>Jesus Labarta</name>
    </author>
    <author>
      <name>Ulises Cort√©s</name>
    </author>
    <author>
      <name>Toyotaro Suzumura</name>
    </author>
    <link href="http://arxiv.org/abs/1705.07706v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07706v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.03698v2</id>
    <updated>2017-11-18T22:10:57Z</updated>
    <published>2017-09-12T05:41:13Z</published>
    <title>Reversible Architectures for Arbitrarily Deep Residual Neural Networks</title>
    <summary>  Recently, deep residual networks have been successfully applied in many
computer vision and natural language processing tasks, pushing the
state-of-the-art performance with deeper and wider architectures. In this work,
we interpret deep residual networks as ordinary differential equations (ODEs),
which have long been studied in mathematics and physics with rich theoretical
and empirical success. From this interpretation, we develop a theoretical
framework on stability and reversibility of deep neural networks, and derive
three reversible neural network architectures that can go arbitrarily deep in
theory. The reversibility property allows a memory-efficient implementation,
which does not need to store the activations for most hidden layers. Together
with the stability of our architectures, this enables training deeper networks
using only modest computational resources. We provide both theoretical analyses
and empirical results. Experimental results demonstrate the efficacy of our
architectures against several strong baselines on CIFAR-10, CIFAR-100 and
STL-10 with superior or on-par state-of-the-art performance. Furthermore, we
show our architectures yield superior results when trained using fewer training
data.
</summary>
    <author>
      <name>Bo Chang</name>
    </author>
    <author>
      <name>Lili Meng</name>
    </author>
    <author>
      <name>Eldad Haber</name>
    </author>
    <author>
      <name>Lars Ruthotto</name>
    </author>
    <author>
      <name>David Begert</name>
    </author>
    <author>
      <name>Elliot Holtham</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at AAAI 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.03698v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.03698v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.00022v1</id>
    <updated>2017-09-29T18:35:54Z</updated>
    <published>2017-09-29T18:35:54Z</published>
    <title>Learning a Structured Neural Network Policy for a Hopping Task</title>
    <summary>  In this work, we attempt to learn a neural network policy for dynamic,
underactuated locomotion tasks. Learning a policy for such a task is non
trivial due to the dynamic, fast changing, non linear and contact rich dynamics
of this task. We use existing trajectory optimization techniques to optimize a
set of policies. For this, we present a method that allows to learn contact
rich dynamics for underactuated systems in a sample efficient manner. Using a
new kind of neural network architecture, we are able to preserve more control
structure and information from the optimized policies. This way, the network
output is more interpretable. We analyze the quality of the learned dynamics
and the robustness as well as generalization of the learned network policy on a
set of tasks for a simulated hopping task. We also inspect if the network has
learned reasonable control structures from the optimized policies.
</summary>
    <author>
      <name>Julian Viereck</name>
    </author>
    <author>
      <name>Jules Kozolinsky</name>
    </author>
    <author>
      <name>Alexander Herzog</name>
    </author>
    <author>
      <name>Ludovic Righetti</name>
    </author>
    <link href="http://arxiv.org/abs/1710.00022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.00022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.04528v1</id>
    <updated>2017-11-13T11:23:36Z</updated>
    <published>2017-11-13T11:23:36Z</published>
    <title>Simple And Efficient Architecture Search for Convolutional Neural
  Networks</title>
    <summary>  Neural networks have recently had a lot of success for many tasks. However,
neural network architectures that perform well are still typically designed
manually by experts in a cumbersome trial-and-error process. We propose a new
method to automatically search for well-performing CNN architectures based on a
simple hill climbing procedure whose operators apply network morphisms,
followed by short optimization runs by cosine annealing. Surprisingly, this
simple method yields competitive results, despite only requiring resources in
the same order of magnitude as training a single network. E.g., on CIFAR-10,
our method designs and trains networks with an error rate below 6% in only 12
hours on a single GPU; training for one day reduces this error further, to
almost 5%.
</summary>
    <author>
      <name>Thomas Elsken</name>
    </author>
    <author>
      <name>Jan-Hendrik Metzen</name>
    </author>
    <author>
      <name>Frank Hutter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review as a conference paper at ICLR 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.04528v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.04528v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.09090v2</id>
    <updated>2018-02-02T07:04:51Z</updated>
    <published>2017-11-24T05:27:19Z</published>
    <title>Invariance of Weight Distributions in Rectified MLPs</title>
    <summary>  An interesting approach to analyzing and developing tools for neural networks
that has received renewed attention is to examine the equivalent kernel of the
neural network. This is based on the fact that a fully connected feedforward
network with one hidden layer, a certain weight distribution, an activation
function, and an infinite number of neurons is a mapping that can be viewed as
a projection into a Hilbert space. We show that the equivalent kernel of an MLP
with ReLU or Leaky ReLU activations for all rotationally-invariant weight
distributions is the same, generalizing a previous result that required
Gaussian weight distributions. We derive the equivalent kernel for these cases.
In deep networks, the equivalent kernel approaches a pathological fixed point,
which can be used to argue why training randomly initialized networks can be
difficult. Our results also have implications for weight initialization and the
level sets in neural network cost functions.
</summary>
    <author>
      <name>Russell Tsuchida</name>
    </author>
    <author>
      <name>Farbod Roosta-Khorasani</name>
    </author>
    <author>
      <name>Marcus Gallagher</name>
    </author>
    <link href="http://arxiv.org/abs/1711.09090v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.09090v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.00575v1</id>
    <updated>2017-12-02T09:08:05Z</updated>
    <published>2017-12-02T09:08:05Z</published>
    <title>Lecture video indexing using boosted margin maximizing neural networks</title>
    <summary>  This paper presents a novel approach for lecture video indexing using a
boosted deep convolutional neural network system. The indexing is performed by
matching high quality slide images, for which text is either known or
extracted, to lower resolution video frames with possible noise, perspective
distortion, and occlusions. We propose a deep neural network integrated with a
boosting framework composed of two sub-networks targeting feature extraction
and similarity determination to perform the matching. The trained network is
given as input a pair of slide image and a candidate video frame image and
produces the similarity between them. A boosting framework is integrated into
our proposed network during the training process. Experimental results show
that the proposed approach is much more capable of handling occlusion, spatial
transformations, and other types of noises when compared with known approaches.
</summary>
    <author>
      <name>Di Ma</name>
    </author>
    <author>
      <name>Xi Zhang</name>
    </author>
    <author>
      <name>Xu Ouyang</name>
    </author>
    <author>
      <name>Gady Agam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ICMLA 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.00575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.00575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.02854v1</id>
    <updated>2017-12-07T20:21:01Z</updated>
    <published>2017-12-07T20:21:01Z</published>
    <title>Stochastic reconstruction of an oolitic limestone by generative
  adversarial networks</title>
    <summary>  Stochastic image reconstruction is a key part of modern digital rock physics
and materials analysis that aims to create numerous representative samples of
material micro-structures for upscaling, numerical computation of effective
properties and uncertainty quantification. We present a method of
three-dimensional stochastic image reconstruction based on generative
adversarial neural networks (GANs). GANs represent a framework of unsupervised
learning methods that require no a priori inference of the probability
distribution associated with the training data. Using a fully convolutional
neural network allows fast sampling of large volumetric images.We apply a GAN
based workflow of network training and image generation to an oolitic Ketton
limestone micro-CT dataset. Minkowski functionals, effective permeability as
well as velocity distributions of simulated flow within the acquired images are
compared with the synthetic reconstructions generated by the deep neural
network. While our results show that GANs allow a fast and accurate
reconstruction of the evaluated image dataset, we address a number of open
questions and challenges involved in the evaluation of generative network-based
methods.
</summary>
    <author>
      <name>Lukas Mosser</name>
    </author>
    <author>
      <name>Olivier Dubrule</name>
    </author>
    <author>
      <name>Martin J. Blunt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.02854v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.02854v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.geo-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.05404v1</id>
    <updated>2017-12-14T12:59:26Z</updated>
    <published>2017-12-14T12:59:26Z</published>
    <title>SEE: Towards Semi-Supervised End-to-End Scene Text Recognition</title>
    <summary>  Detecting and recognizing text in natural scene images is a challenging, yet
not completely solved task. In recent years several new systems that try to
solve at least one of the two sub-tasks (text detection and text recognition)
have been proposed. In this paper we present SEE, a step towards
semi-supervised neural networks for scene text detection and recognition, that
can be optimized end-to-end. Most existing works consist of multiple deep
neural networks and several pre-processing steps. In contrast to this, we
propose to use a single deep neural network, that learns to detect and
recognize text from natural images, in a semi-supervised way. SEE is a network
that integrates and jointly learns a spatial transformer network, which can
learn to detect text regions in an image, and a text recognition network that
takes the identified text regions and recognizes their textual content. We
introduce the idea behind our novel approach and show its feasibility, by
performing a range of experiments on standard benchmark datasets, where we
achieve competitive results.
</summary>
    <author>
      <name>Christian Bartz</name>
    </author>
    <author>
      <name>Haojin Yang</name>
    </author>
    <author>
      <name>Christoph Meinel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI-18. arXiv admin note: substantial text overlap with
  arXiv:1707.08831</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.05404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.05404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.10158v1</id>
    <updated>2017-12-29T09:21:34Z</updated>
    <published>2017-12-29T09:21:34Z</published>
    <title>Non-linear motor control by local learning in spiking neural networks</title>
    <summary>  Learning weights in a spiking neural network with hidden neurons, using
local, stable and online rules, to control non-linear body dynamics is an open
problem. Here, we employ a supervised scheme, Feedback-based Online Local
Learning Of Weights (FOLLOW), to train a network of heterogeneous spiking
neurons with hidden layers, to control a two-link arm so as to reproduce a
desired state trajectory. The network first learns an inverse model of the
non-linear dynamics, i.e. from state trajectory as input to the network, it
learns to infer the continuous-time command that produced the trajectory.
Connection weights are adjusted via a local plasticity rule that involves
pre-synaptic firing and post-synaptic feedback of the error in the inferred
command. We choose a network architecture, termed differential feedforward,
that gives the lowest test error from different feedforward and recurrent
architectures. The learned inverse model is then used to generate a
continuous-time motor command to control the arm, given a desired trajectory.
</summary>
    <author>
      <name>Aditya Gilra</name>
    </author>
    <author>
      <name>Wulfram Gerstner</name>
    </author>
    <link href="http://arxiv.org/abs/1712.10158v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.10158v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.00119v1</id>
    <updated>2017-12-30T11:40:50Z</updated>
    <published>2017-12-30T11:40:50Z</published>
    <title>Towards co-evolution of fitness predictors and Deep Neural Networks</title>
    <summary>  Deep neural networks proved to be a very useful and powerful tool with many
practical applications. They especially excel at learning from large data sets
with labeled samples. However, in order to achieve good learning results, the
network architecture has to be carefully designed. Creating an optimal topology
requires a lot of experience and knowledge. Unfortunately there are no
practically applicable algorithms which could help in this situation. Using an
evolutionary process to develop new network topologies might solve this
problem. The limiting factor in this case is the speed of evaluation of a
single specimen (a single network architecture), which includes learning based
on the whole large dataset. In this paper we propose to overcome this problem
by using a fitness prediction technique: use subsets of the original training
set to conduct the training process and use its results as an approximation of
specimen's fitness. We discuss the feasibility of this approach in context of
the desired fitness predictor features and analyze whether subsets obtained in
an evolutionary process can be used to estimate the fitness of the network
topology. Finally we draw conclusions from our experiments and outline plans
for future work.
</summary>
    <author>
      <name>W≈Çodzimierz Funika</name>
    </author>
    <author>
      <name>Pawe≈Ç Koperek</name>
    </author>
    <link href="http://arxiv.org/abs/1801.00119v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.00119v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.01563v1</id>
    <updated>2018-01-04T22:17:52Z</updated>
    <published>2018-01-04T22:17:52Z</published>
    <title>DENSER: Deep Evolutionary Network Structured Representation</title>
    <summary>  Deep Evolutionary Network Structured Representation (DENSER) is a novel
approach to automatically design Artificial Neural Networks (ANNs) using
Evolutionary Computation (EC). The algorithm not only searches for the best
network topology (e.g., number of layers, type of layers), but also tunes
hyper-parameters, such as, learning parameters or data augmentation parameters.
The automatic design is achieved using a representation with two distinct
levels, where the outer level encodes the general structure of the network,
i.e., the sequence of layers, and the inner level encodes the parameters
associated with each layer. The allowed layers and hyper-parameter value ranges
are defined by means of a human-readable Context-Free Grammar. DENSER was used
to evolve ANNs for two widely used image classification benchmarks obtaining an
average accuracy result of up to 94.27% on the CIFAR-10 dataset, and of 78.75%
on the CIFAR-100. To the best of our knowledge, our CIFAR-100 results are the
highest performing models generated by methods that aim at the automatic design
of Convolutional Neural Networks (CNNs), and is amongst the best for manually
designed and fine-tuned CNNs .
</summary>
    <author>
      <name>Filipe Assun√ß√£o</name>
    </author>
    <author>
      <name>Nuno Louren√ßo</name>
    </author>
    <author>
      <name>Penousal Machado</name>
    </author>
    <author>
      <name>Bernardete Ribeiro</name>
    </author>
    <link href="http://arxiv.org/abs/1801.01563v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.01563v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.01609v1</id>
    <updated>2018-01-05T01:52:35Z</updated>
    <published>2018-01-05T01:52:35Z</published>
    <title>Learning $3$D-FilterMap for Deep Convolutional Neural Networks</title>
    <summary>  We present a novel and compact architecture for deep Convolutional Neural
Networks (CNNs) in this paper, termed $3$D-FilterMap Convolutional Neural
Networks ($3$D-FM-CNNs). The convolution layer of $3$D-FM-CNN learns a compact
representation of the filters, named $3$D-FilterMap, instead of a set of
independent filters in the conventional convolution layer. The filters are
extracted from the $3$D-FilterMap as overlapping $3$D submatrics with weight
sharing among nearby filters, and these filters are convolved with the input to
generate the output of the convolution layer for $3$D-FM-CNN. Due to the weight
sharing scheme, the parameter size of the $3$D-FilterMap is much smaller than
that of the filters to be learned in the conventional convolution layer when
$3$D-FilterMap generates the same number of filters. Our work is fundamentally
different from the network compression literature that reduces the size of a
learned large network in the sense that a small network is directly learned
from scratch. Experimental results demonstrate that $3$D-FM-CNN enjoys a small
parameter space by learning compact $3$D-FilterMaps, while achieving
performance compared to that of the baseline CNNs which learn the same number
of filters as that generated by the corresponding $3$D-FilterMap.
</summary>
    <author>
      <name>Yingzhen Yang</name>
    </author>
    <author>
      <name>Jianchao Yang</name>
    </author>
    <author>
      <name>Ning Xu</name>
    </author>
    <author>
      <name>Wei Han</name>
    </author>
    <link href="http://arxiv.org/abs/1801.01609v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.01609v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.09467v2</id>
    <updated>2018-01-30T02:33:41Z</updated>
    <published>2018-01-29T12:09:58Z</published>
    <title>Hierarchical Spatial Transformer Network</title>
    <summary>  Computer vision researchers have been expecting that neural networks have
spatial transformation ability to eliminate the interference caused by
geometric distortion for a long time. Emergence of spatial transformer network
makes dream come true. Spatial transformer network and its variants can handle
global displacement well, but lack the ability to deal with local spatial
variance. Hence how to achieve a better manner of deformation in the neural
network has become a pressing matter of the moment. To address this issue, we
analyze the advantages and disadvantages of approximation theory and optical
flow theory, then we combine them to propose a novel way to achieve image
deformation and implement it with a hierarchical convolutional neural network.
This new approach solves for a linear deformation along with an optical flow
field to model image deformation. In the experiments of cluttered MNIST
handwritten digits classification and image plane alignment, our method
outperforms baseline methods by a large margin.
</summary>
    <author>
      <name>Chang Shu</name>
    </author>
    <author>
      <name>Xi Chen</name>
    </author>
    <author>
      <name>Qiwei Xie</name>
    </author>
    <author>
      <name>Hua Han</name>
    </author>
    <link href="http://arxiv.org/abs/1801.09467v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.09467v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03875v1</id>
    <updated>2018-02-12T03:51:41Z</updated>
    <published>2018-02-12T03:51:41Z</published>
    <title>Pseudo-Recursal: Solving the Catastrophic Forgetting Problem in Deep
  Neural Networks</title>
    <summary>  In general, neural networks are not currently capable of learning tasks in a
sequential fashion. When a novel, unrelated task is learnt by a neural network,
it substantially forgets how to solve previously learnt tasks. One of the
original solutions to this problem is pseudo-rehearsal, which involves learning
the new task while rehearsing generated items representative of the previous
task/s. This is very effective for simple tasks. However, pseudo-rehearsal has
not yet been successfully applied to very complex tasks because in these tasks
it is difficult to generate representative items. We accomplish
pseudo-rehearsal by using a Generative Adversarial Network to generate items so
that our deep network can learn to sequentially classify the CIFAR-10, SVHN and
MNIST datasets. After training on all tasks, our network loses only 1.67%
absolute accuracy on CIFAR-10 and gains 0.24% absolute accuracy on SVHN. Our
model's performance is a substantial improvement compared to the current state
of the art solution.
</summary>
    <author>
      <name>Craig Atkinson</name>
    </author>
    <author>
      <name>Brendan McCane</name>
    </author>
    <author>
      <name>Lech Szymanski</name>
    </author>
    <author>
      <name>Anthony Robins</name>
    </author>
    <link href="http://arxiv.org/abs/1802.03875v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03875v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.04924v1</id>
    <updated>2018-02-14T02:00:40Z</updated>
    <published>2018-02-14T02:00:40Z</published>
    <title>Exploring Hidden Dimensions in Parallelizing Convolutional Neural
  Networks</title>
    <summary>  The past few years have witnessed growth in the size and computational
requirements for training deep convolutional neural networks. Current
approaches parallelize the training process onto multiple devices by applying a
single parallelization strategy (e.g., data or model parallelism) to all layers
in a network. Although easy to reason about, this design results in suboptimal
runtime performance in large-scale distributed training, since different layers
in a network may prefer different parallelization strategies. In this paper, we
propose layer-wise parallelism that allows each layer in a network to use an
individual parallelization strategy. We jointly optimize how each layer is
parallelized by solving a graph search problem. Our experiments show that
layer-wise parallelism outperforms current parallelization approaches by
increasing training speed, reducing communication costs, achieving better
scalability to multiple GPUs, while maintaining the same network accuracy.
</summary>
    <author>
      <name>Zhihao Jia</name>
    </author>
    <author>
      <name>Sina Lin</name>
    </author>
    <author>
      <name>Charles R. Qi</name>
    </author>
    <author>
      <name>Alex Aiken</name>
    </author>
    <link href="http://arxiv.org/abs/1802.04924v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.04924v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.05800v1</id>
    <updated>2018-02-15T23:36:56Z</updated>
    <published>2018-02-15T23:36:56Z</published>
    <title>Tree-CNN: A Deep Convolutional Neural Network for Lifelong Learning</title>
    <summary>  In recent years, Convolutional Neural Networks (CNNs) have shown remarkable
performance in many computer vision tasks such as object recognition and
detection. However, complex training issues, such as "catastrophic forgetting"
and hyper-parameter tuning, make incremental learning in CNNs a difficult
challenge. In this paper, we propose a hierarchical deep neural network, with
CNNs at multiple levels, and a corresponding training method for lifelong
learning. The network grows in a tree-like manner to accommodate the new
classes of data without losing the ability to identify the previously trained
classes. The proposed network was tested on CIFAR-10 and CIFAR-100 datasets,
and compared against the method of fine tuning specific layers of a
conventional CNN. We obtained comparable accuracies and achieved 40% and 20%
reduction in training effort in CIFAR-10 and CIFAR 100 respectively. The
network was able to organize the incoming classes of data into feature-driven
super-classes. Our model improves upon existing hierarchical CNN models by
adding the capability of self-growth and also yields important observations on
feature selective classification.
</summary>
    <author>
      <name>Deboleena Roy</name>
    </author>
    <author>
      <name>Priyadarshini Panda</name>
    </author>
    <author>
      <name>Kaushik Roy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 8 figures, 6 tables. Submitted to IEEE Transactions on
  Pattern Analysis and Machine Intelligence</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.05800v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.05800v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/adap-org/9507008v1</id>
    <updated>1995-08-02T07:43:53Z</updated>
    <published>1995-08-02T07:43:53Z</published>
    <title>Evolving Neural Networks with Iterative Learning Scheme for Associative
  Memory</title>
    <summary>  A locally iterative learning (LIL) rule is adapted to a model of the
associative memory based on the evolving recurrent-type neural networks
composed of growing neurons. There exist extremely different scale parameters
of time, the individual learning time and the generation in evolution. This
model allows us definite investigation on the interaction between learning and
evolution. And the reinforcement of the robustness against the noise is also
achieved in the evolutional scheme.
</summary>
    <author>
      <name>Sh. Fujita</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Kobe University</arxiv:affiliation>
    </author>
    <author>
      <name>H. Nishimura</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Hyogo University of Education</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, compressed and uuencoded postscript file</arxiv:comment>
    <link href="http://arxiv.org/abs/adap-org/9507008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/adap-org/9507008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="adap-org" scheme="http://arxiv.org/schemas/atom"/>
    <category term="adap-org" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0202023v1</id>
    <updated>2002-02-01T20:00:53Z</updated>
    <published>2002-02-01T20:00:53Z</published>
    <title>Independent neurons representing a finite set of stimuli: dependence of
  the mutual information on the number of units sampled</title>
    <summary>  We study the capacity with which a system of independent neuron-like units
represents a given set of stimuli. We assume that each neuron provides a fixed
amount of information, and that the information provided by different neurons
has a random overlap. We derive analytically the dependence of the mutual
information between the set of stimuli and the neural responses on the number
of units sampled. For a large set of stimuli, the mutual information rises
linearly with the number of neurons, and later saturates exponentially at its
maximum value.
</summary>
    <author>
      <name>Ines Samengo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 5 figures, Network: Computation in Neural Systems, vol 12
  (1) 21 - 31 (2000)</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0202023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0202023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0301627v1</id>
    <updated>2003-01-31T16:33:17Z</updated>
    <published>2003-01-31T16:33:17Z</published>
    <title>Combining Hebbian and reinforcement learning in a minibrain model</title>
    <summary>  A toy model of a neural network in which both Hebbian learning and
reinforcement learning occur is studied. The problem of `path interference',
which makes that the neural net quickly forgets previously learned input-output
relations is tackled by adding a Hebbian term (proportional to the learning
rate $\eta$) to the reinforcement term (proportional to $\rho$) in the learning
rule. It is shown that the number of learning steps is reduced considerably if
$1/4 &lt; \eta/\rho &lt; 1/2$, i.e., if the Hebbian term is neither too small nor too
large compared to the reinforcement term.
</summary>
    <author>
      <name>R. J. C. Bosman</name>
    </author>
    <author>
      <name>W. A. van Leeuwen</name>
    </author>
    <author>
      <name>B. Wemmenhove</name>
    </author>
    <link href="http://arxiv.org/abs/cond-mat/0301627v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0301627v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0510017v1</id>
    <updated>2005-10-01T19:14:36Z</updated>
    <published>2005-10-01T19:14:36Z</published>
    <title>Switching between memories in neural automata with synaptic noise</title>
    <summary>  We present a stochastic neural automata in which activity fluctuations and
synaptic intensities evolve at different temperature, the latter moving through
a set of stored patterns. The network thus exhibits various retrieval phases,
including one which depicts continuous switching between attractors. The
switching may be either random or more complex, depending on the system
parameters values.
</summary>
    <author>
      <name>J. M. Cortes</name>
    </author>
    <author>
      <name>P. L. Garrido</name>
    </author>
    <author>
      <name>J. Marro</name>
    </author>
    <author>
      <name>J. J. Torres</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neurocomputing 58-60: 67-71, 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0510017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0510017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0410004v1</id>
    <updated>2004-10-02T07:19:49Z</updated>
    <published>2004-10-02T07:19:49Z</published>
    <title>Applying Policy Iteration for Training Recurrent Neural Networks</title>
    <summary>  Recurrent neural networks are often used for learning time-series data. Based
on a few assumptions we model this learning task as a minimization problem of a
nonlinear least-squares cost function. The special structure of the cost
function allows us to build a connection to reinforcement learning. We exploit
this connection and derive a convergent, policy iteration-based algorithm.
Furthermore, we argue that RNN training can be fit naturally into the
reinforcement learning framework.
</summary>
    <author>
      <name>I. Szita</name>
    </author>
    <author>
      <name>A. Lorincz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Supplementary material. 17 papes, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0410004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0410004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0412103v3</id>
    <updated>2007-12-19T11:27:09Z</updated>
    <published>2004-12-23T10:33:18Z</published>
    <title>Chosen-Plaintext Cryptanalysis of a Clipped-Neural-Network-Based Chaotic
  Cipher</title>
    <summary>  In ISNN'04, a novel symmetric cipher was proposed, by combining a chaotic
signal and a clipped neural network (CNN) for encryption. The present paper
analyzes the security of this chaotic cipher against chosen-plaintext attacks,
and points out that this cipher can be broken by a chosen-plaintext attack.
Experimental analyses are given to support the feasibility of the proposed
attack.
</summary>
    <author>
      <name>Chengqing Li</name>
    </author>
    <author>
      <name>Shujun Li</name>
    </author>
    <author>
      <name>Dan Zhang</name>
    </author>
    <author>
      <name>Guanrong Chen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/11427445_103</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/11427445_103" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LNCS style, 7 pages, 1 figure (6 sub-figures)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Lecture Notes in Computer Science, vol. 3497, pp. 630-636, 2005</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0412103v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0412103v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611006v1</id>
    <updated>2006-11-02T00:47:57Z</updated>
    <published>2006-11-02T00:47:57Z</published>
    <title>Evolving controllers for simulated car racing</title>
    <summary>  This paper describes the evolution of controllers for racing a simulated
radio-controlled car around a track, modelled on a real physical track. Five
different controller architectures were compared, based on neural networks,
force fields and action sequences. The controllers use either egocentric (first
person), Newtonian (third person) or no information about the state of the car
(open-loop controller). The only controller that was able to evolve good racing
behaviour was based on a neural network acting on egocentric inputs.
</summary>
    <author>
      <name>Julian Togelius</name>
    </author>
    <author>
      <name>Simon M. Lucas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Won the CEC 2005 Best Student Paper Award</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2005 Congress on Evolutionary Computation,
  pages 1906-1913</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0611006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/nlin/0611010v1</id>
    <updated>2006-11-05T14:04:44Z</updated>
    <published>2006-11-05T14:04:44Z</published>
    <title>Anticipation of summer monsoon rainfall over India by Artificial Neural
  Network with Conjugate Gradient Descent Learning</title>
    <summary>  Present study aims to develop a predictive model for the average summer
monsoon rainfall amount over India. The dataset made available by the Indian
Institute of Tropical Meteorology, Pune, has been explored. To develop the
predictive model, Backpropagation method with Conjugate Gradient Descent
algorithm has been implemented. The Neural Net model with the said algorithm
has been learned thrice to reach a good result. After three runs of the model,
it is found that a high prediction yield is available. Ultimately, Artificial
Neural Network with Conjugate Gradient Descent based Backpropagation algorithm
is found to be skillful in predicting average summer monsoon rainfall amount
over India.
</summary>
    <author>
      <name>Surajit Chattopadhyay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 2 tables, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/nlin/0611010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/nlin/0611010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/quant-ph/0401127v2</id>
    <updated>2004-02-11T18:40:03Z</updated>
    <published>2004-01-21T10:44:57Z</published>
    <title>Quantum computing in neural networks</title>
    <summary>  According to the statistical interpretation of quantum theory, quantum
computers form a distinguished class of probabilistic machines (PMs) by
encoding n qubits in 2n pbits (random binary variables). This raises the
possibility of a large-scale quantum computing using PMs, especially with
neural networks which have the innate capability for probabilistic information
processing. Restricting ourselves to a particular model, we construct and
numerically examine the performance of neural circuits implementing universal
quantum gates. A discussion on the physiological plausibility of proposed
coding scheme is also provided.
</summary>
    <author>
      <name>P. Gralewicz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Minor changes, improved discussion, added references, 10 pages, 4 eps
  figures, LaTeX2e</arxiv:comment>
    <link href="http://arxiv.org/abs/quant-ph/0401127v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/quant-ph/0401127v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.3235v3</id>
    <updated>2009-07-02T17:24:41Z</updated>
    <published>2008-02-21T23:41:09Z</published>
    <title>Characterization of the convergence of stationary Fokker-Planck learning</title>
    <summary>  The convergence properties of the stationary Fokker-Planck algorithm for the
estimation of the asymptotic density of stochastic search processes is studied.
Theoretical and empirical arguments for the characterization of convergence of
the estimation in the case of separable and nonseparable nonlinear optimization
problems are given. Some implications of the convergence of stationary
Fokker-Planck learning for the inference of parameters in artificial neural
network models are outlined.
</summary>
    <author>
      <name>Arturo Berrones</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neucom.2008.12.042</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neucom.2008.12.042" rel="related"/>
    <link href="http://arxiv.org/abs/0802.3235v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.3235v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.4010v1</id>
    <updated>2008-02-27T13:00:38Z</updated>
    <published>2008-02-27T13:00:38Z</published>
    <title>Brain architecture: A design for natural computation</title>
    <summary>  Fifty years ago, John von Neumann compared the architecture of the brain with
that of computers that he invented and which is still in use today. In those
days, the organisation of computers was based on concepts of brain
organisation. Here, we give an update on current results on the global
organisation of neural systems. For neural systems, we outline how the spatial
and topological architecture of neuronal and cortical networks facilitates
robustness against failures, fast processing, and balanced network activation.
Finally, we discuss mechanisms of self-organization for such architectures.
After all, the organization of the brain might again inspire computer
architecture.
</summary>
    <author>
      <name>Marcus Kaiser</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1098/rsta.2007.0007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1098/rsta.2007.0007" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Philosophical Transactions of The Royal Society A, 365: 3033-3045,
  2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0802.4010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.4010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.4808v1</id>
    <updated>2008-04-30T12:23:05Z</updated>
    <published>2008-04-30T12:23:05Z</published>
    <title>Solving Time of Least Square Systems in Sigma-Pi Unit Networks</title>
    <summary>  The solving of least square systems is a useful operation in
neurocomputational modeling of learning, pattern matching, and pattern
recognition. In these last two cases, the solution must be obtained on-line,
thus the time required to solve a system in a plausible neural architecture is
critical. This paper presents a recurrent network of Sigma-Pi neurons, whose
solving time increases at most like the logarithm of the system size, and of
its condition number, which provides plausible computation times for biological
systems.
</summary>
    <author>
      <name>Pierre Courrieu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LPC</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Nombre de pages: 7</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Information Processing - Letters and Reviews 4, 3 (2004)
  39-45</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0804.4808v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.4808v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.1070v1</id>
    <updated>2008-06-05T23:03:42Z</updated>
    <published>2008-06-05T23:03:42Z</published>
    <title>A Minimal Model for the Study of Polychronous Groups</title>
    <summary>  A minimal model of polychronous groups in neural networks is presented. The
model is computationally efficient and allows the study of polychronous groups
independent of specific neuron models. Computational experiments were performed
with the model in one- and two-dimensional neural architectures to determine
the dependence of the number of polychronous groups on various connectivity
options. The possibility of using polychronous groups as computational elements
is also discussed.
</summary>
    <author>
      <name>Willard L. Maier</name>
    </author>
    <author>
      <name>Bruce N. Miller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0806.1070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.1070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.2535v1</id>
    <updated>2008-12-13T09:21:31Z</updated>
    <published>2008-12-13T09:21:31Z</published>
    <title>Pattern Recognition and Memory Mapping using Mirroring Neural Networks</title>
    <summary>  In this paper, we present a new kind of learning implementation to recognize
the patterns using the concept of Mirroring Neural Network (MNN) which can
extract information from distinct sensory input patterns and perform pattern
recognition tasks. It is also capable of being used as an advanced associative
memory wherein image data is associated with voice inputs in an unsupervised
manner. Since the architecture is hierarchical and modular it has the potential
of being used to devise learning engines of ever increasing complexity.
</summary>
    <author>
      <name>Dasika Ratna Deepthi</name>
    </author>
    <author>
      <name>K. Eswaran</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Paper No 336, IEEE, ICETiC 2009, International Conference on
  Emerging Trends in Computing</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0812.2535v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.2535v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.3902v1</id>
    <updated>2009-08-26T20:34:50Z</updated>
    <published>2009-08-26T20:34:50Z</published>
    <title>On the Expressiveness of Line Drawings</title>
    <summary>  Can expressiveness of a drawing be traced with a computer? In this study a
neural network (perceptron) and a support vector machine are used to classify
line drawings. To do this the line drawings are attributed values according to
a kinematic model and a diffusion model for the lines they consist of. The
values for both models are related to looking times. Extreme values according
to these models, that is both extremely short and extremely long looking times,
are interpreted as indicating expressiveness. The results strongly indicate
that expressiveness in this sense can be detected, at least with a neural
network.
</summary>
    <author>
      <name>Harm Hollestelle</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0908.3902v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.3902v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.0745v1</id>
    <updated>2010-02-03T14:02:39Z</updated>
    <published>2010-02-03T14:02:39Z</published>
    <title>Using CODEQ to Train Feed-forward Neural Networks</title>
    <summary>  CODEQ is a new, population-based meta-heuristic algorithm that is a hybrid of
concepts from chaotic search, opposition-based learning, differential evolution
and quantum mechanics. CODEQ has successfully been used to solve different
types of problems (e.g. constrained, integer-programming, engineering) with
excellent results. In this paper, CODEQ is used to train feed-forward neural
networks. The proposed method is compared with particle swarm optimization and
differential evolution algorithms on three data sets with encouraging results.
</summary>
    <author>
      <name>Mahamed G. H. Omran</name>
    </author>
    <author>
      <name>Faisal al-Adwani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1002.0745v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.0745v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.3274v1</id>
    <updated>2010-04-19T18:24:02Z</updated>
    <published>2010-04-19T18:24:02Z</published>
    <title>A New Approach to Keyphrase Extraction Using Neural Networks</title>
    <summary>  Keyphrases provide a simple way of describing a document, giving the reader
some clues about its contents. Keyphrases can be useful in a various
applications such as retrieval engines, browsing interfaces, thesaurus
construction, text mining etc.. There are also other tasks for which keyphrases
are useful, as we discuss in this paper. This paper describes a neural network
based approach to keyphrase extraction from scientific articles. Our results
show that the proposed method performs better than some state-of-the art
keyphrase extraction approaches.
</summary>
    <author>
      <name>Kamal Sarkar</name>
    </author>
    <author>
      <name>Mita Nasipuri</name>
    </author>
    <author>
      <name>Suranjan Ghose</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues online at
  http://ijcsi.org/articles/A-New-Approach-to-Keyphrase-Extraction-Using-Neural-Networks.php</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI, Volume 7, Issue 2, March 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.3274v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.3274v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.1211v1</id>
    <updated>2011-09-06T15:19:48Z</updated>
    <published>2011-09-06T15:19:48Z</published>
    <title>An Efficient Preprocessing Methodology for Discovering Patterns and
  Clustering of Web Users using a Dynamic ART1 Neural Network</title>
    <summary>  In this paper, a complete preprocessing methodology for discovering patterns
in web usage mining process to improve the quality of data by reducing the
quantity of data has been proposed. A dynamic ART1 neural network clustering
algorithm to group users according to their Web access patterns with its neat
architecture is also proposed. Several experiments are conducted and the
results show the proposed methodology reduces the size of Web log files down to
73-82% of the initial size and the proposed ART1 algorithm is dynamic and
learns relatively stable quality clusters.
</summary>
    <author>
      <name>C. Ramya</name>
    </author>
    <author>
      <name>G. Kavitha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages; International Conference on Information Processing,
  august-2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.1211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.1211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.2034v2</id>
    <updated>2013-08-22T12:21:24Z</updated>
    <published>2011-09-09T14:59:59Z</published>
    <title>Learning Sequence Neighbourhood Metrics</title>
    <summary>  Recurrent neural networks (RNNs) in combination with a pooling operator and
the neighbourhood components analysis (NCA) objective function are able to
detect the characterizing dynamics of sequences and embed them into a
fixed-length vector space of arbitrary dimensionality. Subsequently, the
resulting features are meaningful and can be used for visualization or nearest
neighbour classification in linear time. This kind of metric learning for
sequential data enables the use of algorithms tailored towards fixed length
vector spaces such as R^n.
</summary>
    <author>
      <name>Justin Bayer</name>
    </author>
    <author>
      <name>Christian Osendorfer</name>
    </author>
    <author>
      <name>Patrick van der Smagt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Artificial Neural Networks and Machine Learning ICANN 2012 Springer
  Berlin Heidelberg 2012. 531-538</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.2034v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.2034v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.4931v1</id>
    <updated>2012-07-20T12:15:12Z</updated>
    <published>2012-07-20T12:15:12Z</published>
    <title>Motion Planning Of an Autonomous Mobile Robot Using Artificial Neural
  Network</title>
    <summary>  The paper presents the electronic design and motion planning of a robot based
on decision making regarding its straight motion and precise turn using
Artificial Neural Network (ANN). The ANN helps in learning of robot so that it
performs motion autonomously. The weights calculated are implemented in
microcontroller. The performance has been tested to be excellent.
</summary>
    <author>
      <name>G. N. Tripathi</name>
    </author>
    <author>
      <name>V. Rihani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures, 1 table, 1 graph chart, ITCA-2012, Chennai, India</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.4931v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.4931v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.4385v1</id>
    <updated>2012-11-19T12:21:49Z</updated>
    <published>2012-11-19T12:21:49Z</published>
    <title>Artificial Neural Network Based Optical Character Recognition</title>
    <summary>  Optical Character Recognition deals in recognition and classification of
characters from an image. For the recognition to be accurate, certain
topological and geometrical properties are calculated, based on which a
character is classified and recognized. Also, the Human psychology perceives
characters by its overall shape and features such as strokes, curves,
protrusions, enclosures etc. These properties, also called Features are
extracted from the image by means of spatial pixel-based calculation. A
collection of such features, called Vectors, help in defining a character
uniquely, by means of an Artificial Neural Network that uses these Feature
Vectors.
</summary>
    <author>
      <name>Vivek Shrivastava</name>
    </author>
    <author>
      <name>Navdeep Sharma</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/sipij.2012.3506</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/sipij.2012.3506" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Signal &amp; Image Processing : An International Journal (SIPIJ) Vol.3,
  No.5, October 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.4385v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.4385v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.5217v1</id>
    <updated>2012-12-20T20:11:30Z</updated>
    <published>2012-12-20T20:11:30Z</published>
    <title>A Neural Network Approach to ECG Denoising</title>
    <summary>  We propose an ECG denoising method based on a feed forward neural network
with three hidden layers. Particulary useful for very noisy signals, this
approach uses the available ECG channels to reconstruct a noisy channel. We
tested the method, on all the records from Physionet MIT-BIH Arrhythmia
Database, adding electrode motion artifact noise. This denoising method
improved the perfomance of publicly available ECG analysis programs on noisy
ECG signals. This is an offline method that can be used to remove noise from
very corrupted Holter records.
</summary>
    <author>
      <name>Rui Rodrigues</name>
    </author>
    <author>
      <name>Paula Couto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.5217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.5217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3557v1</id>
    <updated>2013-01-16T02:12:07Z</updated>
    <published>2013-01-16T02:12:07Z</published>
    <title>Stochastic Pooling for Regularization of Deep Convolutional Neural
  Networks</title>
    <summary>  We introduce a simple and effective method for regularizing large
convolutional neural networks. We replace the conventional deterministic
pooling operations with a stochastic procedure, randomly picking the activation
within each pooling region according to a multinomial distribution, given by
the activities within the pooling region. The approach is hyper-parameter free
and can be combined with other regularization approaches, such as dropout and
data augmentation. We achieve state-of-the-art performance on four image
datasets, relative to other approaches that do not utilize data augmentation.
</summary>
    <author>
      <name>Matthew D. Zeiler</name>
    </author>
    <author>
      <name>Rob Fergus</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.3557v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3557v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.1292v1</id>
    <updated>2014-03-20T19:47:58Z</updated>
    <published>2014-03-20T19:47:58Z</published>
    <title>Review of Face Detection Systems Based Artificial Neural Networks
  Algorithms</title>
    <summary>  Face detection is one of the most relevant applications of image processing
and biometric systems. Artificial neural networks (ANN) have been used in the
field of image processing and pattern recognition. There is lack of literature
surveys which give overview about the studies and researches related to the
using of ANN in face detection. Therefore, this research includes a general
review of face detection studies and systems which based on different ANN
approaches and algorithms. The strengths and limitations of these literature
studies and systems were included also.
</summary>
    <author>
      <name>Omaima N. A. AL-Allaf</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijma.2013.6101</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijma.2013.6101" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 12 figures, 1 table, IJMA Journal</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The International Journal of Multimedia &amp; Its Applications (IJMA)
  Vol.6, No.1, February 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.1292v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.1292v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.2614v4</id>
    <updated>2015-02-26T07:06:17Z</updated>
    <published>2014-06-07T20:20:11Z</published>
    <title>Application and Verification of Algorithm Learning Based Neural Network</title>
    <summary>  This paper has been withdrawn by the author due to a crucial accuracy error
in Fig. 5. For precise performance of ALBNN please refer to Yoon et al.'s work
in the following article. Yoon, H., Park, C. S., Kim, J. S., &amp; Baek, J. G.
(2013). Algorithm learning based neural network integrating feature selection
and classification. Expert Systems with Applications, 40(1), 231-241.
http://www.sciencedirect.com/science/article/pii/S0957417412008731
</summary>
    <author>
      <name>Rizwana Kalsoom</name>
    </author>
    <author>
      <name>Moomal Qureshi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author due to a crucial accuracy
  error in Fig. 5</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.2614v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.2614v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.1610v2</id>
    <updated>2014-09-22T17:49:01Z</updated>
    <published>2014-07-07T08:00:57Z</published>
    <title>Analyzing the Performance of Multilayer Neural Networks for Object
  Recognition</title>
    <summary>  In the last two years, convolutional neural networks (CNNs) have achieved an
impressive suite of results on standard recognition datasets and tasks.
CNN-based features seem poised to quickly replace engineered representations,
such as SIFT and HOG. However, compared to SIFT and HOG, we understand much
less about the nature of the features learned by large CNNs. In this paper, we
experimentally probe several aspects of CNN feature learning in an attempt to
help practitioners gain useful, evidence-backed intuitions about how to apply
CNNs to computer vision problems.
</summary>
    <author>
      <name>Pulkit Agrawal</name>
    </author>
    <author>
      <name>Ross Girshick</name>
    </author>
    <author>
      <name>Jitendra Malik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in European Conference on Computer Vision 2014 (ECCV-2014)</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.1610v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.1610v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.5882v2</id>
    <updated>2014-09-03T03:09:02Z</updated>
    <published>2014-08-25T19:48:04Z</published>
    <title>Convolutional Neural Networks for Sentence Classification</title>
    <summary>  We report on a series of experiments with convolutional neural networks (CNN)
trained on top of pre-trained word vectors for sentence-level classification
tasks. We show that a simple CNN with little hyperparameter tuning and static
vectors achieves excellent results on multiple benchmarks. Learning
task-specific vectors through fine-tuning offers further gains in performance.
We additionally propose a simple modification to the architecture to allow for
the use of both task-specific and static vectors. The CNN models discussed
herein improve upon the state of the art on 4 out of 7 tasks, which include
sentiment analysis and question classification.
</summary>
    <author>
      <name>Yoon Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in EMNLP 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.5882v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.5882v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.1690v1</id>
    <updated>2009-02-10T16:43:06Z</updated>
    <published>2009-02-10T16:43:06Z</published>
    <title>Back analysis of microplane model parameters using soft computing
  methods</title>
    <summary>  A new procedure based on layered feed-forward neural networks for the
microplane material model parameters identification is proposed in the present
paper. Novelties are usage of the Latin Hypercube Sampling method for the
generation of training sets, a systematic employment of stochastic sensitivity
analysis and a genetic algorithm-based training of a neural network by an
evolutionary algorithm. Advantages and disadvantages of this approach together
with possible extensions are thoroughly discussed and analyzed.
</summary>
    <author>
      <name>A. Kucerova</name>
    </author>
    <author>
      <name>M. Leps</name>
    </author>
    <author>
      <name>J. Zeman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 27 figures, 7 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CAMES: Computer Assisted Mechanics and Engineering Sciences, 14
  (2), 219-242, 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0902.1690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.1690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.06046v1</id>
    <updated>2015-03-20T12:00:44Z</updated>
    <published>2015-03-20T12:00:44Z</published>
    <title>Deep Transform: Cocktail Party Source Separation via Probabilistic
  Re-Synthesis</title>
    <summary>  In cocktail party listening scenarios, the human brain is able to separate
competing speech signals. However, the signal processing implemented by the
brain to perform cocktail party listening is not well understood. Here, we
trained two separate convolutive autoencoder deep neural networks (DNN) to
separate monaural and binaural mixtures of two concurrent speech streams. We
then used these DNNs as convolutive deep transform (CDT) devices to perform
probabilistic re-synthesis. The CDTs operated directly in the time-domain. Our
simulations demonstrate that very simple neural networks are capable of
exploiting monaural and binaural information available in a cocktail party
listening scenario.
</summary>
    <author>
      <name>Andrew J. R. Simpson</name>
    </author>
    <link href="http://arxiv.org/abs/1503.06046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.06046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.4301v1</id>
    <updated>2010-01-25T02:09:42Z</updated>
    <published>2010-01-25T02:09:42Z</published>
    <title>Probabilistic Approach to Neural Networks Computation Based on Quantum
  Probability Model Probabilistic Principal Subspace Analysis Example</title>
    <summary>  In this paper, we introduce elements of probabilistic model that is suitable
for modeling of learning algorithms in biologically plausible artificial neural
networks framework. Model is based on two of the main concepts in quantum
physics - a density matrix and the Born rule. As an example, we will show that
proposed probabilistic interpretation is suitable for modeling of on-line
learning algorithms for PSA, which are preferably realized by a parallel
hardware based on very simple computational units. Proposed concept (model) can
be used in the context of improving algorithm convergence speed, learning
factor choice, or input signal scale robustness. We are going to see how the
Born rule and the Hebbian learning rule are connected
</summary>
    <author>
      <name>Marko V. Jankovic</name>
    </author>
    <link href="http://arxiv.org/abs/1001.4301v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.4301v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.8124v1</id>
    <updated>2012-10-30T19:11:06Z</updated>
    <published>2012-10-30T19:11:06Z</published>
    <title>Hierarchical Learning Algorithm for the Beta Basis Function Neural
  Network</title>
    <summary>  The paper presents a two-level learning method for the design of the Beta
Basis Function Neural Network BBFNN. A Genetic Algorithm is employed at the
upper level to construct BBFNN, while the key learning parameters :the width,
the centers and the Beta form are optimised using the gradient algorithm at the
lower level. In order to demonstrate the effectiveness of this hierarchical
learning algorithm HLABBFNN, we need to validate our algorithm for the
approximation of non-linear function.
</summary>
    <author>
      <name>Habib Dhahri</name>
    </author>
    <author>
      <name>Mohamed Adel Alimi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Third International Conference on Systems, Signals &amp; Device, March
  21-24, 2005 , Sousse, Tunisia</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1210.8124v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.8124v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.7251v1</id>
    <updated>2013-11-28T09:44:45Z</updated>
    <published>2013-11-28T09:44:45Z</published>
    <title>Spatially-Adaptive Reconstruction in Computed Tomography using Neural
  Networks</title>
    <summary>  We propose a supervised machine learning approach for boosting existing
signal and image recovery methods and demonstrate its efficacy on example of
image reconstruction in computed tomography. Our technique is based on a local
nonlinear fusion of several image estimates, all obtained by applying a chosen
reconstruction algorithm with different values of its control parameters.
Usually such output images have different bias/variance trade-off. The fusion
of the images is performed by feed-forward neural network trained on a set of
known examples. Numerical experiments show an improvement in reconstruction
quality relatively to existing direct and iterative reconstruction methods.
</summary>
    <author>
      <name>Joseph Shtok</name>
    </author>
    <author>
      <name>Michael Zibulevsky</name>
    </author>
    <author>
      <name>Michael Elad</name>
    </author>
    <link href="http://arxiv.org/abs/1311.7251v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.7251v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.6885v1</id>
    <updated>2013-12-24T20:38:18Z</updated>
    <published>2013-12-24T20:38:18Z</published>
    <title>Deep learning for class-generic object detection</title>
    <summary>  We investigate the use of deep neural networks for the novel task of class
generic object detection. We show that neural networks originally designed for
image recognition can be trained to detect objects within images, regardless of
their class, including objects for which no bounding box labels have been
provided. In addition, we show that bounding box labels yield a 1% performance
increase on the ImageNet recognition challenge.
</summary>
    <author>
      <name>Brody Huval</name>
    </author>
    <author>
      <name>Adam Coates</name>
    </author>
    <author>
      <name>Andrew Ng</name>
    </author>
    <link href="http://arxiv.org/abs/1312.6885v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.6885v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.4326v2</id>
    <updated>2015-10-20T15:08:48Z</updated>
    <published>2014-09-15T16:54:42Z</published>
    <title>Computing the Stereo Matching Cost with a Convolutional Neural Network</title>
    <summary>  We present a method for extracting depth information from a rectified image
pair. We train a convolutional neural network to predict how well two image
patches match and use it to compute the stereo matching cost. The cost is
refined by cross-based cost aggregation and semiglobal matching, followed by a
left-right consistency check to eliminate errors in the occluded regions. Our
stereo method achieves an error rate of 2.61 % on the KITTI stereo dataset and
is currently (August 2014) the top performing method on this dataset.
</summary>
    <author>
      <name>Jure ≈Ωbontar</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CVPR.2015.7298767</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CVPR.2015.7298767" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Conference on Computer Vision and Pattern Recognition (CVPR), June
  2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.4326v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.4326v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.3555v1</id>
    <updated>2014-12-11T06:46:53Z</updated>
    <published>2014-12-11T06:46:53Z</published>
    <title>Empirical Evaluation of Gated Recurrent Neural Networks on Sequence
  Modeling</title>
    <summary>  In this paper we compare different types of recurrent units in recurrent
neural networks (RNNs). Especially, we focus on more sophisticated units that
implement a gating mechanism, such as a long short-term memory (LSTM) unit and
a recently proposed gated recurrent unit (GRU). We evaluate these recurrent
units on the tasks of polyphonic music modeling and speech signal modeling. Our
experiments revealed that these advanced recurrent units are indeed better than
more traditional recurrent units such as tanh units. Also, we found GRU to be
comparable to LSTM.
</summary>
    <author>
      <name>Junyoung Chung</name>
    </author>
    <author>
      <name>Caglar Gulcehre</name>
    </author>
    <author>
      <name>KyungHyun Cho</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented in NIPS 2014 Deep Learning and Representation Learning
  Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.3555v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3555v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.04042v1</id>
    <updated>2015-02-13T16:09:41Z</updated>
    <published>2015-02-13T16:09:41Z</published>
    <title>Abstract Learning via Demodulation in a Deep Neural Network</title>
    <summary>  Inspired by the brain, deep neural networks (DNN) are thought to learn
abstract representations through their hierarchical architecture. However, at
present, how this happens is not well understood. Here, we demonstrate that DNN
learn abstract representations by a process of demodulation. We introduce a
biased sigmoid activation function and use it to show that DNN learn and
perform better when optimized for demodulation. Our findings constitute the
first unambiguous evidence that DNN perform abstract learning in practical use.
Our findings may also explain abstract learning in the human brain.
</summary>
    <author>
      <name>Andrew J. R. Simpson</name>
    </author>
    <link href="http://arxiv.org/abs/1502.04042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.04042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.00923v1</id>
    <updated>2015-04-03T19:57:06Z</updated>
    <published>2015-04-03T19:57:06Z</published>
    <title>A Unified Deep Neural Network for Speaker and Language Recognition</title>
    <summary>  Learned feature representations and sub-phoneme posteriors from Deep Neural
Networks (DNNs) have been used separately to produce significant performance
gains for speaker and language recognition tasks. In this work we show how
these gains are possible using a single DNN for both speaker and language
recognition. The unified DNN approach is shown to yield substantial performance
improvements on the the 2013 Domain Adaptation Challenge speaker recognition
task (55% reduction in EER for the out-of-domain condition) and on the NIST
2011 Language Recognition Evaluation (48% reduction in EER for the 30s test
condition).
</summary>
    <author>
      <name>Fred Richardson</name>
    </author>
    <author>
      <name>Douglas Reynolds</name>
    </author>
    <author>
      <name>Najim Dehak</name>
    </author>
    <link href="http://arxiv.org/abs/1504.00923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.00923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.03641v1</id>
    <updated>2015-04-14T17:53:51Z</updated>
    <published>2015-04-14T17:53:51Z</published>
    <title>Learning to Compare Image Patches via Convolutional Neural Networks</title>
    <summary>  In this paper we show how to learn directly from image data (i.e., without
resorting to manually-designed features) a general similarity function for
comparing image patches, which is a task of fundamental importance for many
computer vision problems. To encode such a function, we opt for a CNN-based
model that is trained to account for a wide variety of changes in image
appearance. To that end, we explore and study multiple neural network
architectures, which are specifically adapted to this task. We show that such
an approach can significantly outperform the state-of-the-art on several
problems and benchmark datasets.
</summary>
    <author>
      <name>Sergey Zagoruyko</name>
    </author>
    <author>
      <name>Nikos Komodakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.03641v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.03641v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.01011v1</id>
    <updated>2015-08-05T09:22:25Z</updated>
    <published>2015-08-05T09:22:25Z</published>
    <title>Learning from LDA using Deep Neural Networks</title>
    <summary>  Latent Dirichlet Allocation (LDA) is a three-level hierarchical Bayesian
model for topic inference. In spite of its great success, inferring the latent
topic distribution with LDA is time-consuming. Motivated by the transfer
learning approach proposed by~\newcite{hinton2015distilling}, we present a
novel method that uses LDA to supervise the training of a deep neural network
(DNN), so that the DNN can approximate the costly LDA inference with less
computation. Our experiments on a document classification task show that a
simple DNN can learn the LDA behavior pretty well, while the inference is
speeded up tens or hundreds of times.
</summary>
    <author>
      <name>Dongxu Zhang</name>
    </author>
    <author>
      <name>Tianyi Luo</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <author>
      <name>Rong Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1508.01011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.01011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.02790v1</id>
    <updated>2015-08-12T01:11:47Z</updated>
    <published>2015-08-12T01:11:47Z</published>
    <title>On the Convergence of SGD Training of Neural Networks</title>
    <summary>  Neural networks are usually trained by some form of stochastic gradient
descent (SGD)). A number of strategies are in common use intended to improve
SGD optimization, such as learning rate schedules, momentum, and batching.
These are motivated by ideas about the occurrence of local minima at different
scales, valleys, and other phenomena in the objective function. Empirical
results presented here suggest that these phenomena are not significant factors
in SGD optimization of MLP-related objective functions, and that the behavior
of stochastic gradient descent in these problems is better described as the
simultaneous convergence at different rates of many, largely non-interacting
subproblems
</summary>
    <author>
      <name>Thomas M. Breuel</name>
    </author>
    <link href="http://arxiv.org/abs/1508.02790v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.02790v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.07130v1</id>
    <updated>2015-08-28T08:50:18Z</updated>
    <published>2015-08-28T08:50:18Z</published>
    <title>Parallel Dither and Dropout for Regularising Deep Neural Networks</title>
    <summary>  Effective regularisation during training can mean the difference between
success and failure for deep neural networks. Recently, dither has been
suggested as alternative to dropout for regularisation during batch-averaged
stochastic gradient descent (SGD). In this article, we show that these methods
fail without batch averaging and we introduce a new, parallel regularisation
method that may be used without batch averaging. Our results for
parallel-regularised non-batch-SGD are substantially better than what is
possible with batch-SGD. Furthermore, our results demonstrate that dither and
dropout are complimentary.
</summary>
    <author>
      <name>Andrew J. R. Simpson</name>
    </author>
    <link href="http://arxiv.org/abs/1508.07130v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.07130v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.01126v1</id>
    <updated>2015-09-03T15:28:55Z</updated>
    <published>2015-09-03T15:28:55Z</published>
    <title>Training of CC4 Neural Network with Spread Unary Coding</title>
    <summary>  This paper adapts the corner classification algorithm (CC4) to train the
neural networks using spread unary inputs. This is an important problem as
spread unary appears to be at the basis of data representation in biological
learning. The modified CC4 algorithm is tested using the pattern classification
experiment and the results are found to be good. Specifically, we show that the
number of misclassified points is not particularly sensitive to the chosen
radius of generalization.
</summary>
    <author>
      <name>Pushpa Sree Potluri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.01126v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.01126v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.02512v1</id>
    <updated>2015-09-08T19:59:19Z</updated>
    <published>2015-09-08T19:59:19Z</published>
    <title>DeepCough: A Deep Convolutional Neural Network in A Wearable Cough
  Detection System</title>
    <summary>  In this paper, we present a system that employs a wearable acoustic sensor
and a deep convolutional neural network for detecting coughs. We evaluate the
performance of our system on 14 healthy volunteers and compare it to that of
other cough detection systems that have been reported in the literature.
Experimental results show that our system achieves a classification sensitivity
of 95.1% and a specificity of 99.5%.
</summary>
    <author>
      <name>Justice Amoh</name>
    </author>
    <author>
      <name>Kofi Odame</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">BioCAS-2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.02512v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.02512v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.05711v2</id>
    <updated>2015-10-28T08:42:54Z</updated>
    <published>2015-10-19T22:38:09Z</published>
    <title>Qualitative Projection Using Deep Neural Networks</title>
    <summary>  Deep neural networks (DNN) abstract by demodulating the output of linear
filters. In this article, we refine this definition of abstraction to show that
the inputs of a DNN are abstracted with respect to the filters. Or, to restate,
the abstraction is qualified by the filters. This leads us to introduce the
notion of qualitative projection. We use qualitative projection to abstract
MNIST hand-written digits with respect to the various dogs, horses, planes and
cars of the CIFAR dataset. We then classify the MNIST digits according to the
magnitude of their dogness, horseness, planeness and carness qualities,
illustrating the generality of qualitative projection.
</summary>
    <author>
      <name>Andrew J. R. Simpson</name>
    </author>
    <link href="http://arxiv.org/abs/1510.05711v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.05711v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.04348v1</id>
    <updated>2015-11-13T16:36:05Z</updated>
    <published>2015-11-13T16:36:05Z</published>
    <title>Large Scale Artificial Neural Network Training Using Multi-GPUs</title>
    <summary>  This paper describes a method for accelerating large scale Artificial Neural
Networks (ANN) training using multi-GPUs by reducing the forward and backward
passes to matrix multiplication. We propose an out-of-core multi-GPU matrix
multiplication and integrate the algorithm with the ANN training. The
experiments demonstrate that our matrix multiplication algorithm achieves
linear speedup on multiple inhomogeneous GPUs. The full paper of this project
can be found at [1].
</summary>
    <author>
      <name>Linnan Wang</name>
    </author>
    <author>
      <name>Wei Wu</name>
    </author>
    <author>
      <name>Jianxiong Xiao</name>
    </author>
    <author>
      <name>Yang Yi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SC 15 Poster</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.04348v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.04348v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.03809v1</id>
    <updated>2015-11-03T07:21:44Z</updated>
    <published>2015-11-03T07:21:44Z</published>
    <title>Artificial neural network approach for condition-based maintenance</title>
    <summary>  In this research, computerized maintenance management will be investigated.
The rise of maintenance cost forced the research community to look for more
effective ways to schedule maintenance operations. Using computerized models to
come up with optimal maintenance policy has led to better equipment utilization
and lower costs. This research adopts Condition-Based Maintenance model where
the maintenance decision is generated based on equipment conditions. Artificial
Neural Network technique is proposed to capture and analyze equipment condition
signals which lead to higher level of knowledge gathering. This knowledge is
used to accurately estimate equipment failure time. Based on these estimations,
an optimal maintenance management policy can be achieved.
</summary>
    <author>
      <name>Mostafa Sayyed</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">108 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.03809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.03809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.07776v4</id>
    <updated>2016-10-12T04:47:45Z</updated>
    <published>2016-02-25T02:42:58Z</published>
    <title>Recurrent Neural Network Grammars</title>
    <summary>  We introduce recurrent neural network grammars, probabilistic models of
sentences with explicit phrase structure. We explain efficient inference
procedures that allow application to both parsing and language modeling.
Experiments show that they provide better parsing in English than any single
previously published supervised generative model and better language modeling
than state-of-the-art sequential RNNs in English and Chinese.
</summary>
    <author>
      <name>Chris Dyer</name>
    </author>
    <author>
      <name>Adhiguna Kuncoro</name>
    </author>
    <author>
      <name>Miguel Ballesteros</name>
    </author>
    <author>
      <name>Noah A. Smith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of NAACL 2016 (contains corrigendum)</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.07776v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.07776v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05008v1</id>
    <updated>2016-04-18T06:29:01Z</updated>
    <published>2016-04-18T06:29:01Z</published>
    <title>Forecasting Volatility in Indian Stock Market using Artificial Neural
  Network with Multiple Inputs and Outputs</title>
    <summary>  Volatility in stock markets has been extensively studied in the applied
finance literature. In this paper, Artificial Neural Network models based on
various back propagation algorithms have been constructed to predict volatility
in the Indian stock market through volatility of NIFTY returns and volatility
of gold returns. This model considers India VIX, CBOE VIX, volatility of crude
oil returns (CRUDESDR), volatility of DJIA returns (DJIASDR), volatility of DAX
returns (DAXSDR), volatility of Hang Seng returns (HANGSDR) and volatility of
Nikkei returns (NIKKEISDR) as predictor variables. Three sets of experiments
have been performed over three time periods to judge the effectiveness of the
approach.
</summary>
    <author>
      <name>Tamal Datta Chaudhuri</name>
    </author>
    <author>
      <name>Indranil Ghosh</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/21245-4034</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/21245-4034" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1604.05008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05377v1</id>
    <updated>2016-04-18T23:18:23Z</updated>
    <published>2016-04-18T23:18:23Z</published>
    <title>Churn analysis using deep convolutional neural networks and autoencoders</title>
    <summary>  Customer temporal behavioral data was represented as images in order to
perform churn prediction by leveraging deep learning architectures prominent in
image classification. Supervised learning was performed on labeled data of over
6 million customers using deep convolutional neural networks, which achieved an
AUC of 0.743 on the test dataset using no more than 12 temporal features for
each customer. Unsupervised learning was conducted using autoencoders to better
understand the reasons for customer churn. Images that maximally activate the
hidden units of an autoencoder trained with churned customers reveal ample
opportunities for action to be taken to prevent churn among strong data, no
voice users.
</summary>
    <author>
      <name>Artit Wangperawong</name>
    </author>
    <author>
      <name>Cyrille Brun</name>
    </author>
    <author>
      <name>Olav Laudy</name>
    </author>
    <author>
      <name>Rujikorn Pavasuthipaisit</name>
    </author>
    <link href="http://arxiv.org/abs/1604.05377v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05377v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07154v1</id>
    <updated>2016-05-23T19:40:50Z</updated>
    <published>2016-05-23T19:40:50Z</published>
    <title>Path-Normalized Optimization of Recurrent Neural Networks with ReLU
  Activations</title>
    <summary>  We investigate the parameter-space geometry of recurrent neural networks
(RNNs), and develop an adaptation of path-SGD optimization method, attuned to
this geometry, that can learn plain RNNs with ReLU activations. On several
datasets that require capturing long-term dependency structure, we show that
path-SGD can significantly improve trainability of ReLU RNNs compared to RNNs
trained with SGD, even with various recently suggested initialization schemes.
</summary>
    <author>
      <name>Behnam Neyshabur</name>
    </author>
    <author>
      <name>Yuhuai Wu</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Nathan Srebro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07154v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07154v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02555v1</id>
    <updated>2016-06-08T13:47:18Z</updated>
    <published>2016-06-08T13:47:18Z</published>
    <title>Improving Recurrent Neural Networks For Sequence Labelling</title>
    <summary>  In this paper we study different types of Recurrent Neural Networks (RNN) for
sequence labeling tasks. We propose two new variants of RNNs integrating
improvements for sequence labeling, and we compare them to the more traditional
Elman and Jordan RNNs. We compare all models, either traditional or new, on
four distinct tasks of sequence labeling: two on Spoken Language Understanding
(ATIS and MEDIA); and two of POS tagging for the French Treebank (FTB) and the
Penn Treebank (PTB) corpora. The results show that our new variants of RNNs are
always more effective than the others.
</summary>
    <author>
      <name>Marco Dinarelli</name>
    </author>
    <author>
      <name>Isabelle Tellier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02555v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02555v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04930v1</id>
    <updated>2016-06-15T19:38:14Z</updated>
    <published>2016-06-15T19:38:14Z</published>
    <title>Deep Learning for Music</title>
    <summary>  Our goal is to be able to build a generative model from a deep neural network
architecture to try to create music that has both harmony and melody and is
passable as music composed by humans. Previous work in music generation has
mainly been focused on creating a single melody. More recent work on polyphonic
music modeling, centered around time series probability density estimation, has
met some partial success. In particular, there has been a lot of work based off
of Recurrent Neural Networks combined with Restricted Boltzmann Machines
(RNN-RBM) and other similar recurrent energy based models. Our approach,
however, is to perform end-to-end learning and generation with deep neural nets
alone.
</summary>
    <author>
      <name>Allen Huang</name>
    </author>
    <author>
      <name>Raymond Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, Stanford CS224D</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.04930v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04930v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05990v1</id>
    <updated>2016-06-20T07:05:14Z</updated>
    <published>2016-06-20T07:05:14Z</published>
    <title>A New Training Method for Feedforward Neural Networks Based on Geometric
  Contraction Property of Activation Functions</title>
    <summary>  We propose a new training method for a feedforward neural network having the
activation functions with the geometric contraction property. The method
consists of constructing a new functional that is less nonlinear in comparison
with the classical functional by removing the nonlinearity of the activation
functions from the output layer. We validate this new method by a series of
experiments that show an improved learning speed and also a better
classification error.
</summary>
    <author>
      <name>Petre Birtea</name>
    </author>
    <author>
      <name>Cosmin Cernazanu-Glavan</name>
    </author>
    <author>
      <name>Alexandru Sisu</name>
    </author>
    <link href="http://arxiv.org/abs/1606.05990v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05990v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92B20, 68T05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07953v2</id>
    <updated>2016-07-12T17:10:38Z</updated>
    <published>2016-06-25T19:46:28Z</published>
    <title>Bidirectional Recurrent Neural Networks for Medical Event Detection in
  Electronic Health Records</title>
    <summary>  Sequence labeling for extraction of medical events and their attributes from
unstructured text in Electronic Health Record (EHR) notes is a key step towards
semantic understanding of EHRs. It has important applications in health
informatics including pharmacovigilance and drug surveillance. The state of the
art supervised machine learning models in this domain are based on Conditional
Random Fields (CRFs) with features calculated from fixed context windows. In
this application, we explored various recurrent neural network frameworks and
show that they significantly outperformed the CRF models.
</summary>
    <author>
      <name>Abhyuday Jagannatha</name>
    </author>
    <author>
      <name>Hong Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In proceedings of NAACL HLT 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.07953v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07953v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02028v1</id>
    <updated>2016-07-06T12:23:47Z</updated>
    <published>2016-07-06T12:23:47Z</published>
    <title>Artificial neural networks and fuzzy logic for recognizing alphabet
  characters and mathematical symbols</title>
    <summary>  Optical Character Recognition software (OCR) are important tools for
obtaining accessible texts. We propose the use of artificial neural networks
(ANN) in order to develop pattern recognition algorithms capable of recognizing
both normal texts and formulae. We present an original improvement of the
backpropagation algorithm. Moreover, we describe a novel image segmentation
algorithm that exploits fuzzy logic for separating touching characters.
</summary>
    <author>
      <name>Giuseppe Air√≤ Farulla</name>
    </author>
    <author>
      <name>Tiziana Armano</name>
    </author>
    <author>
      <name>Anna Capietto</name>
    </author>
    <author>
      <name>Nadir Murru</name>
    </author>
    <author>
      <name>Rosaria Rossini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-41264-1_1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-41264-1_1" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Lecture Notes in Computer Science, Volume 9759 2016, Computers
  Helping People with Special Needs, p. 7-14</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1607.02028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.06627v1</id>
    <updated>2016-08-23T06:01:19Z</updated>
    <published>2016-08-23T06:01:19Z</published>
    <title>Artificial Neural Networks for Detection of Malaria in RBCs</title>
    <summary>  Malaria is one of the most common diseases caused by mosquitoes and is a
great public health problem worldwide. Currently, for malaria diagnosis the
standard technique is microscopic examination of a stained blood film. We
propose use of Artificial Neural Networks (ANN) for the diagnosis of the
disease in the red blood cell. For this purpose features / parameters are
computed from the data obtained by the digital holographic images of the blood
cells and is given as input to ANN which classifies the cell as the infected
one or otherwise.
</summary>
    <author>
      <name>Purnima Pandit</name>
    </author>
    <author>
      <name>A. Anand</name>
    </author>
    <link href="http://arxiv.org/abs/1608.06627v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.06627v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M45" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03348v4</id>
    <updated>2017-01-14T05:54:29Z</updated>
    <published>2016-09-12T11:23:20Z</published>
    <title>A Threshold-based Scheme for Reinforcement Learning in Neural Networks</title>
    <summary>  A generic and scalable Reinforcement Learning scheme for Artificial Neural
Networks is presented, providing a general purpose learning machine. By
reference to a node threshold three features are described 1) A mechanism for
Primary Reinforcement, capable of solving linearly inseparable problems 2) The
learning scheme is extended to include a mechanism for Conditioned
Reinforcement, capable of forming long term strategy 3) The learning scheme is
modified to use a threshold-based deep learning algorithm, providing a robust
and biologically inspired alternative to backpropagation. The model may be used
for supervised as well as unsupervised training regimes.
</summary>
    <author>
      <name>Thomas H. Ward</name>
    </author>
    <link href="http://arxiv.org/abs/1609.03348v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03348v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.09218v1</id>
    <updated>2016-09-29T06:32:45Z</updated>
    <published>2016-09-29T06:32:45Z</published>
    <title>Is cortical criticality unique?</title>
    <summary>  There are indications that for optimizing neural computation, neural networks
- including the brain - operate at criticality. Previous approaches have,
however, used diverse fingerprints of criticality, leaving open the question
whether they refer to a unique critical point or whether there could be
several. Using a recurrent spiking neural network as the model, we demonstrate
that avalanche criticality does not necessarily lie at the dynamical
edge-of-chaos and that therefore, the different fingerprints indicate distinct
phenomena with an as yet unclarified relationship.
</summary>
    <author>
      <name>Kalris Kanders</name>
    </author>
    <author>
      <name>Ruedi Stoop</name>
    </author>
    <link href="http://arxiv.org/abs/1609.09218v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09218v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.03477v1</id>
    <updated>2016-11-10T20:35:47Z</updated>
    <published>2016-11-10T20:35:47Z</published>
    <title>Song From PI: A Musically Plausible Network for Pop Music Generation</title>
    <summary>  We present a novel framework for generating pop music. Our model is a
hierarchical Recurrent Neural Network, where the layers and the structure of
the hierarchy encode our prior knowledge about how pop music is composed. In
particular, the bottom layers generate the melody, while the higher levels
produce the drums and chords. We conduct several human studies that show strong
preference of our generated music over that produced by the recent method by
Google. We additionally show two applications of our framework: neural dancing
and karaoke, as well as neural story singing.
</summary>
    <author>
      <name>Hang Chu</name>
    </author>
    <author>
      <name>Raquel Urtasun</name>
    </author>
    <author>
      <name>Sanja Fidler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">under review at ICLR 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.03477v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.03477v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.04966v1</id>
    <updated>2016-12-15T08:28:16Z</updated>
    <published>2016-12-15T08:28:16Z</published>
    <title>Design of Image Matched Non-Separable Wavelet using Convolutional Neural
  Network</title>
    <summary>  Image-matched nonseparable wavelets can find potential use in many
applications including image classification, segmen- tation, compressive
sensing, etc. This paper proposes a novel design methodology that utilizes
convolutional neural net- work (CNN) to design two-channel non-separable
wavelet matched to a given image. The design is proposed on quin- cunx lattice.
The loss function of the convolutional neural network is setup with total
squared error between the given input image to CNN and the reconstructed image
at the output of CNN, leading to perfect reconstruction at the end of train-
ing. Simulation results have been shown on some standard images.
</summary>
    <author>
      <name>Naushad Ansari</name>
    </author>
    <author>
      <name>Anubha Gupta</name>
    </author>
    <author>
      <name>Rahul Duggal</name>
    </author>
    <link href="http://arxiv.org/abs/1612.04966v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.04966v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05251v1</id>
    <updated>2016-12-15T20:57:56Z</updated>
    <published>2016-12-15T20:57:56Z</published>
    <title>Neural Networks for Joint Sentence Classification in Medical Paper
  Abstracts</title>
    <summary>  Existing models based on artificial neural networks (ANNs) for sentence
classification often do not incorporate the context in which sentences appear,
and classify sentences individually. However, traditional sentence
classification approaches have been shown to greatly benefit from jointly
classifying subsequent sentences, such as with conditional random fields. In
this work, we present an ANN architecture that combines the effectiveness of
typical ANN models to classify sentences in isolation, with the strength of
structured prediction. Our model achieves state-of-the-art results on two
different datasets for sequential sentence classification in medical abstracts.
</summary>
    <author>
      <name>Franck Dernoncourt</name>
    </author>
    <author>
      <name>Ji Young Lee</name>
    </author>
    <author>
      <name>Peter Szolovits</name>
    </author>
    <link href="http://arxiv.org/abs/1612.05251v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05251v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.01811v1</id>
    <updated>2017-01-07T09:58:49Z</updated>
    <published>2017-01-07T09:58:49Z</published>
    <title>Structural Attention Neural Networks for improved sentiment analysis</title>
    <summary>  We introduce a tree-structured attention neural network for sentences and
small phrases and apply it to the problem of sentiment classification. Our
model expands the current recursive models by incorporating structural
information around a node of a syntactic tree using both bottom-up and top-down
information propagation. Also, the model utilizes structural attention to
identify the most salient representations during the construction of the
syntactic tree. To our knowledge, the proposed models achieve state of the art
performance on the Stanford Sentiment Treebank dataset.
</summary>
    <author>
      <name>Filippos Kokkinos</name>
    </author>
    <author>
      <name>Alexandros Potamianos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to EACL2017 for review</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.01811v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.01811v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.06796v2</id>
    <updated>2017-02-28T14:17:16Z</updated>
    <published>2017-01-24T10:29:31Z</published>
    <title>Discriminative Neural Topic Models</title>
    <summary>  We propose a neural network based approach for learning topics from text and
image datasets. The model makes no assumptions about the conditional
distribution of the observed features given the latent topics. This allows us
to perform topic modelling efficiently using sentences of documents and patches
of images as observed features, rather than limiting ourselves to words.
Moreover, the proposed approach is online, and hence can be used for streaming
data. Furthermore, since the approach utilizes neural networks, it can be
implemented on GPU with ease, and hence it is very scalable.
</summary>
    <author>
      <name>Gaurav Pandey</name>
    </author>
    <author>
      <name>Ambedkar Dukkipati</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.06796v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.06796v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.02071v1</id>
    <updated>2017-04-07T02:15:42Z</updated>
    <published>2017-04-07T02:15:42Z</published>
    <title>Convolutional Neural Pyramid for Image Processing</title>
    <summary>  We propose a principled convolutional neural pyramid (CNP) framework for
general low-level vision and image processing tasks. It is based on the
essential finding that many applications require large receptive fields for
structure understanding. But corresponding neural networks for regression
either stack many layers or apply large kernels to achieve it, which is
computationally very costly. Our pyramid structure can greatly enlarge the
field while not sacrificing computation efficiency. Extra benefit includes
adaptive network depth and progressive upsampling for quasi-realtime testing on
VGA-size input. Our method profits a broad set of applications, such as
depth/RGB image restoration, completion, noise/artifact removal, edge
refinement, image filtering, image enhancement and colorization.
</summary>
    <author>
      <name>Xiaoyong Shen</name>
    </author>
    <author>
      <name>Ying-Cong Chen</name>
    </author>
    <author>
      <name>Xin Tao</name>
    </author>
    <author>
      <name>Jiaya Jia</name>
    </author>
    <link href="http://arxiv.org/abs/1704.02071v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.02071v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.00504v1</id>
    <updated>2017-06-01T21:57:32Z</updated>
    <published>2017-06-01T21:57:32Z</published>
    <title>Dynamic Stripes: Exploiting the Dynamic Precision Requirements of
  Activation Values in Neural Networks</title>
    <summary>  Stripes is a Deep Neural Network (DNN) accelerator that uses bit-serial
computation to offer performance that is proportional to the fixed-point
precision of the activation values. The fixed-point precisions are determined a
priori using profiling and are selected at a per layer granularity. This paper
presents Dynamic Stripes, an extension to Stripes that detects precision
variance at runtime and at a finer granularity. This extra level of precision
reduction increases performance by 41% over Stripes.
</summary>
    <author>
      <name>Alberto Delmas</name>
    </author>
    <author>
      <name>Patrick Judd</name>
    </author>
    <author>
      <name>Sayeh Sharify</name>
    </author>
    <author>
      <name>Andreas Moshovos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.00504v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.00504v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.00648v1</id>
    <updated>2017-05-03T13:46:05Z</updated>
    <published>2017-05-03T13:46:05Z</published>
    <title>Dataflow Matrix Machines as a Model of Computations with Linear Streams</title>
    <summary>  We overview dataflow matrix machines as a Turing complete generalization of
recurrent neural networks and as a programming platform. We describe vector
space of finite prefix trees with numerical leaves which allows us to combine
expressive power of dataflow matrix machines with simplicity of traditional
recurrent neural networks.
</summary>
    <author>
      <name>Michael Bukatin</name>
    </author>
    <author>
      <name>Jon Anthony</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, accepted for presentation at LearnAut 2017: Learning and
  Automata workshop at LICS (Logic in Computer Science) 2017 conference.
  Preprint original version: April 9, 2017; minor correction: May 1, 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.00648v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.00648v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.01265v2</id>
    <updated>2017-11-01T03:37:23Z</updated>
    <published>2017-07-05T08:55:28Z</published>
    <title>Multiple Range-Restricted Bidirectional Gated Recurrent Units with
  Attention for Relation Classification</title>
    <summary>  Most of neural approaches to relation classification have focused on finding
short patterns that represent the semantic relation using Convolutional Neural
Networks (CNNs) and those approaches have generally achieved better
performances than using Recurrent Neural Networks (RNNs). In a similar
intuition to the CNN models, we propose a novel RNN-based model that strongly
focuses on only important parts of a sentence using multiple range-restricted
bidirectional layers and attention for relation classification. Experimental
results on the SemEval-2010 relation classification task show that our model is
comparable to the state-of-the-art CNN-based and RNN-based models that use
additional linguistic information.
</summary>
    <author>
      <name>Jonggu Kim</name>
    </author>
    <author>
      <name>Jong-Hyeok Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.01265v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.01265v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.06719v1</id>
    <updated>2017-07-20T23:12:11Z</updated>
    <published>2017-07-20T23:12:11Z</published>
    <title>Generalized Convolutional Neural Networks for Point Cloud Data</title>
    <summary>  The introduction of cheap RGB-D cameras, stereo cameras, and LIDAR devices
has given the computer vision community 3D information that conventional RGB
cameras cannot provide. This data is often stored as a point cloud. In this
paper, we present a novel method to apply the concept of convolutional neural
networks to this type of data. By creating a mapping of nearest neighbors in a
dataset, and individually applying weights to spatial relationships between
points, we achieve an architecture that works directly with point clouds, but
closely resembles a convolutional neural net in both design and behavior. Such
a method bypasses the need for extensive feature engineering, while proving to
be computationally efficient and requiring few parameters.
</summary>
    <author>
      <name>Aleksandr Savchenkov</name>
    </author>
    <link href="http://arxiv.org/abs/1707.06719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.06719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.03417v1</id>
    <updated>2017-08-11T00:41:56Z</updated>
    <published>2017-08-11T00:41:56Z</published>
    <title>GlobeNet: Convolutional Neural Networks for Typhoon Eye Tracking from
  Remote Sensing Imagery</title>
    <summary>  Advances in remote sensing technologies have made it possible to use
high-resolution visual data for weather observation and forecasting tasks. We
propose the use of multi-layer neural networks for understanding complex
atmospheric dynamics based on multichannel satellite images. The capability of
our model was evaluated by using a linear regression task for single typhoon
coordinates prediction. A specific combination of models and different
activation policies enabled us to obtain an interesting prediction result in
the northeastern hemisphere (ENH).
</summary>
    <author>
      <name>Seungkyun Hong</name>
    </author>
    <author>
      <name>Seongchan Kim</name>
    </author>
    <author>
      <name>Minsu Joh</name>
    </author>
    <author>
      <name>Sa-kwang Song</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review as a workshop paper at CI 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.03417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.03417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.06257v2</id>
    <updated>2017-12-11T11:26:00Z</updated>
    <published>2017-08-21T14:30:49Z</published>
    <title>A Flow Model of Neural Networks</title>
    <summary>  Based on a natural connection between ResNet and transport equation or its
characteristic equation, we propose a continuous flow model for both ResNet and
plain net. Through this continuous model, a ResNet can be explicitly
constructed as a refinement of a plain net. The flow model provides an
alternative perspective to understand phenomena in deep neural networks, such
as why it is necessary and sufficient to use 2-layer blocks in ResNets, why
deeper is better, and why ResNets are even deeper, and so on. It also opens a
gate to bring in more tools from the huge area of differential equations.
</summary>
    <author>
      <name>Zhen Li</name>
    </author>
    <author>
      <name>Zuoqiang Shi</name>
    </author>
    <link href="http://arxiv.org/abs/1708.06257v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.06257v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.05313v1</id>
    <updated>2017-11-14T21:07:38Z</updated>
    <published>2017-11-14T21:07:38Z</published>
    <title>Simulating Action Dynamics with Neural Process Networks</title>
    <summary>  Understanding procedural language requires anticipating the causal effects of
actions, even when they are not explicitly stated. In this work, we introduce
Neural Process Networks to understand procedural text through (neural)
simulation of action dynamics. Our model complements existing memory
architectures with dynamic entity tracking by explicitly modeling actions as
state transformers. The model updates the states of the entities by executing
learned action operators. Empirical results demonstrate that our proposed model
can reason about the unstated causal effects of actions, allowing it to provide
more accurate contextual information for understanding and generating
procedural text, all while offering more interpretable internal representations
than existing alternatives.
</summary>
    <author>
      <name>Antoine Bosselut</name>
    </author>
    <author>
      <name>Omer Levy</name>
    </author>
    <author>
      <name>Ari Holtzman</name>
    </author>
    <author>
      <name>Corin Ennis</name>
    </author>
    <author>
      <name>Dieter Fox</name>
    </author>
    <author>
      <name>Yejin Choi</name>
    </author>
    <link href="http://arxiv.org/abs/1711.05313v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.05313v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.06288v1</id>
    <updated>2018-02-17T20:42:04Z</updated>
    <published>2018-02-17T20:42:04Z</published>
    <title>Implementation of Neural Network and feature extraction to classify ECG
  signals</title>
    <summary>  This paper presents a suitable and efficient implementation of a feature
extraction algorithm (Pan Tompkins algorithm) on electrocardiography (ECG)
signals, for detection and classification of four cardiac diseases: Sleep
Apnea, Arrhythmia, Supraventricular Arrhythmia and Long Term Atrial
Fibrillation (AF) and differentiating them from the normal heart beat by using
pan Tompkins RR detection followed by feature extraction for classification
purpose .The paper also presents a new approach towards signal classification
using the existing neural networks classifiers.
</summary>
    <author>
      <name>R Karthik</name>
    </author>
    <author>
      <name>Dhruv Tyagi</name>
    </author>
    <author>
      <name>Amogh Raut</name>
    </author>
    <author>
      <name>Soumya Saxena</name>
    </author>
    <author>
      <name>Rajesh Kumar M</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SPRINGER LNEE</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.06288v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06288v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.5405v2</id>
    <updated>2015-11-13T12:38:53Z</updated>
    <published>2014-08-22T18:35:27Z</published>
    <title>Recurrent Neural Network Based Hybrid Model of Gene Regulatory Network</title>
    <summary>  Systems biology is an emerging interdisciplinary area of research that
focuses on study of complex interactions in a biological system, such as gene
regulatory networks. The discovery of gene regulatory networks leads to a wide
range of applications, such as pathways related to a disease that can unveil in
what way the disease acts and provide novel tentative drug targets. In
addition, the development of biological models from discovered networks or
pathways can help to predict the responses to disease and can be much useful
for the novel drug development and treatments. The inference of regulatory
networks from biological data is still in its infancy stage. This paper
proposes a recurrent neural network (RNN) based gene regulatory network (GRN)
model hybridized with generalized extended Kalman filter for weight update in
backpropagation through time training algorithm. The RNN is a complex neural
network that gives a better settlement between the biological closeness and
mathematical flexibility to model GRN. The RNN is able to capture complex,
non-linear and dynamic relationship among variables. Gene expression data are
inherently noisy and Kalman filter performs well for estimation even in noisy
data. Hence, non-linear version of Kalman filter, i.e., generalized extended
Kalman filter has been applied for weight update during network training. The
developed model has been applied on DNA SOS repair network, IRMA network, and
two synthetic networks from DREAM Challenge. We compared our results with other
state-of-the-art techniques that show superiority of our model. Further, 5%
Gaussian noise has been added in the dataset and result of the proposed model
shows negligible effect of noise on the results.
</summary>
    <author>
      <name>Khalid Raza</name>
    </author>
    <author>
      <name>Mansaf Alam</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.compbiolchem.2016.08.002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.compbiolchem.2016.08.002" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 9 figures and 4 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computational Biology and Chemistry, 64: 322-334, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1408.5405v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.5405v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.03209v2</id>
    <updated>2016-04-14T15:54:11Z</updated>
    <published>2015-01-13T23:46:05Z</published>
    <title>Neural Implementation of Probabilistic Models of Cognition</title>
    <summary>  Bayesian models of cognition hypothesize that human brains make sense of data
by representing probability distributions and applying Bayes' rule to find the
best explanation for available data. Understanding the neural mechanisms
underlying probabilistic models remains important because Bayesian models
provide a computational framework, rather than specifying mechanistic
processes. Here, we propose a deterministic neural-network model which
estimates and represents probability distributions from observable events --- a
phenomenon related to the concept of probability matching. Our model learns to
represent probabilities without receiving any representation of them from the
external world, but rather by experiencing the occurrence patterns of
individual events. Our neural implementation of probability matching is paired
with a neural module applying Bayes' rule, forming a comprehensive neural
scheme to simulate human Bayesian learning and inference. Our model also
provides novel explanations of base-rate neglect, a notable deviation from
Bayes.
</summary>
    <author>
      <name>Milad Kharratzadeh</name>
    </author>
    <author>
      <name>Thomas R. Shultz</name>
    </author>
    <link href="http://arxiv.org/abs/1501.03209v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.03209v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.00810v3</id>
    <updated>2016-06-30T10:28:36Z</updated>
    <published>2016-03-02T18:01:57Z</published>
    <title>Character-based Neural Machine Translation</title>
    <summary>  Neural Machine Translation (MT) has reached state-of-the-art results.
However, one of the main challenges that neural MT still faces is dealing with
very large vocabularies and morphologically rich languages. In this paper, we
propose a neural MT system using character-based embeddings in combination with
convolutional and highway layers to replace the standard lookup-based word
representations. The resulting unlimited-vocabulary and affix-aware source word
embeddings are tested in a state-of-the-art neural MT based on an
attention-based bidirectional recurrent neural network. The proposed MT scheme
provides improved results even when the source language is not morphologically
rich. Improvements up to 3 BLEU points are obtained in the German-English WMT
task.
</summary>
    <author>
      <name>Marta R. Costa-Juss√†</name>
    </author>
    <author>
      <name>Jos√© A. R. Fonollosa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at ACL 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.00810v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.00810v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03592v2</id>
    <updated>2016-08-18T09:52:31Z</updated>
    <published>2016-06-11T13:39:43Z</published>
    <title>Balanced activation in a simple embodied neural simulation</title>
    <summary>  In recent years, there have been many computational simulations of
spontaneous neural dynamics. Here, we explore a model of spontaneous neural
dynamics and allow it to control a virtual agent moving in a simple
environment. This setup generates interesting brain-environment feedback
interactions that rapidly destabilize neural and behavioral dynamics and
suggest the need for homeostatic mechanisms. We investigate roles for both
local homeostatic plasticity (local inhibition adjusting over time to balance
excitatory input) as well as macroscopic task negative activity (that
compensates for task positive, sensory input) in regulating both neural
activity and resulting behavior (trajectories through the environment). Our
results suggest complementary functional roles for both local homeostatic
plasticity and balanced activity across brain regions in maintaining neural and
behavioral dynamics. These findings suggest important functional roles for
homeostatic systems in maintaining neural and behavioral dynamics and suggest a
novel functional role for frequently reported macroscopic task-negative
patterns of activity (e.g., the default mode network).
</summary>
    <author>
      <name>Peter J. Hellyer</name>
    </author>
    <author>
      <name>Claudia Clopath</name>
    </author>
    <author>
      <name>Angie A. Kehagia</name>
    </author>
    <author>
      <name>Federico E. Turkheimer</name>
    </author>
    <author>
      <name>Robert Leech</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 7 figures, associated github repository:
  https://github.com/c3nl-neuraldynamics/Avatar</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.03592v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03592v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06275v2</id>
    <updated>2016-09-01T10:56:45Z</updated>
    <published>2016-07-21T11:40:50Z</published>
    <title>Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain
  Factoid Question Answering</title>
    <summary>  While question answering (QA) with neural network, i.e. neural QA, has
achieved promising results in recent years, lacking of large scale real-word QA
dataset is still a challenge for developing and evaluating neural QA system. To
alleviate this problem, we propose a large scale human annotated real-world QA
dataset WebQA with more than 42k questions and 556k evidences. As existing
neural QA methods resolve QA either as sequence generation or
classification/ranking problem, they face challenges of expensive softmax
computation, unseen answers handling or separate candidate answer generation
component. In this work, we cast neural QA as a sequence labeling problem and
propose an end-to-end sequence labeling model, which overcomes all the above
challenges. Experimental results on WebQA show that our model outperforms the
baselines significantly with an F1 score of 74.69% with word-based input, and
the performance drops only 3.72 F1 points with more challenging character-based
input.
</summary>
    <author>
      <name>Peng Li</name>
    </author>
    <author>
      <name>Wei Li</name>
    </author>
    <author>
      <name>Zhengyan He</name>
    </author>
    <author>
      <name>Xuguang Wang</name>
    </author>
    <author>
      <name>Ying Cao</name>
    </author>
    <author>
      <name>Jie Zhou</name>
    </author>
    <author>
      <name>Wei Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 figures, withdraw experimental results on CNN/Daily Mail
  datasets</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.06275v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06275v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.04816v3</id>
    <updated>2017-06-08T14:12:35Z</updated>
    <published>2017-03-14T23:09:45Z</published>
    <title>Making Neural QA as Simple as Possible but not Simpler</title>
    <summary>  Recent development of large-scale question answering (QA) datasets triggered
a substantial amount of research into end-to-end neural architectures for QA.
Increasingly complex systems have been conceived without comparison to simpler
neural baseline systems that would justify their complexity. In this work, we
propose a simple heuristic that guides the development of neural baseline
systems for the extractive QA task. We find that there are two ingredients
necessary for building a high-performing neural QA system: first, the awareness
of question words while processing the context and second, a composition
function that goes beyond simple bag-of-words modeling, such as recurrent
neural networks. Our results show that FastQA, a system that meets these two
requirements, can achieve very competitive performance compared with existing
models. We argue that this surprising finding puts results of previous systems
and the complexity of recent QA datasets into perspective.
</summary>
    <author>
      <name>Dirk Weissenborn</name>
    </author>
    <author>
      <name>Georg Wiese</name>
    </author>
    <author>
      <name>Laura Seiffe</name>
    </author>
    <link href="http://arxiv.org/abs/1703.04816v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.04816v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.04058v1</id>
    <updated>2017-05-11T08:08:44Z</updated>
    <published>2017-05-11T08:08:44Z</published>
    <title>Neural Style Transfer: A Review</title>
    <summary>  The recent work of Gatys et al. demonstrated the power of Convolutional
Neural Networks (CNN) in creating artistic fantastic imagery by separating and
recombing the image content and style. This process of using CNN to migrate the
semantic content of one image to different styles is referred to as Neural
Style Transfer. Since then, Neural Style Transfer has become a trending topic
both in academic literature and industrial applications. It is receiving
increasing attention from computer vision researchers and several methods are
proposed to either improve or extend the original neural algorithm proposed by
Gatys et al. However, there is no comprehensive survey presenting and
summarizing recent Neural Style Transfer literature. This review aims to
provide an overview of the current progress towards Neural Style Transfer, as
well as discussing its various applications and open problems for future
research.
</summary>
    <author>
      <name>Yongcheng Jing</name>
    </author>
    <author>
      <name>Yezhou Yang</name>
    </author>
    <author>
      <name>Zunlei Feng</name>
    </author>
    <author>
      <name>Jingwen Ye</name>
    </author>
    <author>
      <name>Mingli Song</name>
    </author>
    <link href="http://arxiv.org/abs/1705.04058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06544v1</id>
    <updated>2016-05-20T21:38:32Z</updated>
    <published>2016-05-20T21:38:32Z</published>
    <title>Inference by Reparameterization in Neural Population Codes</title>
    <summary>  Behavioral experiments on humans and animals suggest that the brain performs
probabilistic inference to interpret its environment. Here we present a new
general-purpose, biologically-plausible neural implementation of approximate
inference. The neural network represents uncertainty using Probabilistic
Population Codes (PPCs), which are distributed neural representations that
naturally encode probability distributions, and support marginalization and
evidence integration in a biologically-plausible manner. By connecting multiple
PPCs together as a probabilistic graphical model, we represent multivariate
probability distributions. Approximate inference in graphical models can be
accomplished by message-passing algorithms that disseminate local information
throughout the graph. An attractive and often accurate example of such an
algorithm is Loopy Belief Propagation (LBP), which uses local marginalization
and evidence integration operations to perform approximate inference
efficiently even for complex models. Unfortunately, a subtle feature of LBP
renders it neurally implausible. However, LBP can be elegantly reformulated as
a sequence of Tree-based Reparameterizations (TRP) of the graphical model. We
re-express the TRP updates as a nonlinear dynamical system with both fast and
slow timescales, and show that this produces a neurally plausible solution. By
combining all of these ideas, we show that a network of PPCs can represent
multivariate probability distributions and implement the TRP updates to perform
probabilistic inference. Simulations with Gaussian graphical models demonstrate
that the neural network inference quality is comparable to the direct
evaluation of LBP and robust to noise, and thus provides a promising mechanism
for general probabilistic inference in the population codes of the brain.
</summary>
    <author>
      <name>Rajkumar Vasudeva Raju</name>
    </author>
    <author>
      <name>Xaq Pitkow</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures, submitted to NIPS 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.06544v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06544v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.05079v1</id>
    <updated>2017-03-15T11:05:43Z</updated>
    <published>2017-03-15T11:05:43Z</published>
    <title>Neutral theory and scale-free neural dynamics</title>
    <summary>  Avalanches of electrochemical activity in brain networks have been
empirically reported to obey scale-invariant behavior --characterized by
power-law distributions up to some upper cut-off-- both in vitro and in vivo.
Elucidating whether such scaling laws stem from the underlying neural dynamics
operating at the edge of a phase transition is a fascinating possibility, as
systems poised at criticality have been argued to exhibit a number of important
functional advantages. Here we employ a well-known model for neural dynamics
with synaptic plasticity, to elucidate an alternative scenario in which
neuronal avalanches can coexist, overlapping in time, but still remaining
scale-free. Remarkably their scale-invariance does not stem from underlying
criticality nor self-organization at the edge of a continuous phase transition.
Instead, it emerges from the fact that perturbations to the system exhibit a
neutral drift --guided by demographic fluctuations-- with respect to endogenous
spontaneous activity. Such a neutral dynamics --similar to the one in neutral
theories of population genetics-- implies marginal propagation of activity,
characterized by power-law distributed causal avalanches. Importantly, our
results underline the importance of considering causal information --on which
neuron triggers the firing of which-- to properly estimate the statistics of
avalanches of neural activity. We discuss the implications of these findings
both in modeling and to elucidate experimental observations, as well as its
possible consequences for actual neural dynamics and information processing in
actual neural networks.
</summary>
    <author>
      <name>Matteo Martinello</name>
    </author>
    <author>
      <name>Jorge Hidalgo</name>
    </author>
    <author>
      <name>Serena di Santo</name>
    </author>
    <author>
      <name>Amos Maritan</name>
    </author>
    <author>
      <name>Dietmar Plenz</name>
    </author>
    <author>
      <name>Miguel A. Mu√±oz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevX.7.041071</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevX.7.041071" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Main text: 8 pages, 3 figures. Supplementary information: 5 pages, 4
  figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. X 7, 041071 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1703.05079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.05079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.6230v2</id>
    <updated>2012-10-25T13:55:10Z</updated>
    <published>2012-10-23T13:19:08Z</published>
    <title>A Self-Organized Neural Comparator</title>
    <summary>  Learning algorithms need generally the possibility to compare several streams
of information. Neural learning architectures hence need a unit, a comparator,
able to compare several inputs encoding either internal or external
information, like for instance predictions and sensory readings. Without the
possibility of comparing the values of prediction to actual sensory inputs,
reward evaluation and supervised learning would not be possible.
  Comparators are usually not implemented explicitly, necessary comparisons are
commonly performed by directly comparing one-to-one the respective activities.
This implies that the characteristics of the two input streams (like size and
encoding) must be provided at the time of designing the system.
  It is however plausible that biological comparators emerge from
self-organizing, genetically encoded principles, which allow the system to
adapt to the changes in the input and in the organism.
  We propose an unsupervised neural circuitry, where the function of input
comparison emerges via self-organization only from the interaction of the
system with the respective inputs, without external influence or supervision.
  The proposed neural comparator adapts, unsupervised, according to the
correlations present in the input streams. The system consists of a multilayer
feed-forward neural network which follows a local output minimization
(anti-Hebbian) rule for adaptation of the synaptic weights.
  The local output minimization allows the circuit to autonomously acquire the
capability of comparing the neural activities received from different neural
populations, which may differ in the size of the population and in the neural
encoding used. The comparator is able to compare objects never encountered
before in the sensory input streams and to evaluate a measure of their
similarity, even when differently encoded.
</summary>
    <author>
      <name>Guillermo A. Ludue√±a</name>
    </author>
    <author>
      <name>Claudius Gros</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/NECO_a_00424</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/NECO_a_00424" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">G. A. Ludue\~na and C. Gros, A self-organized neural comparator,
  Neural Computation, 25, pp 1006 (2013)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1210.6230v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.6230v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/astro-ph/0503539v1</id>
    <updated>2005-03-24T14:46:32Z</updated>
    <published>2005-03-24T14:46:32Z</published>
    <title>Neural networks for gamma-hadron separation in MAGIC</title>
    <summary>  Neural networks have proved to be versatile and robust for particle
separation in many experiments related to particle astrophysics. We apply these
techniques to separate gamma rays from hadrons for the MAGIC Cerenkov
Telescope. Two types of neural network architectures have been used for the
classi cation task: one is the MultiLayer Perceptron (MLP) based on supervised
learning, and the other is the Self-Organising Tree Algorithm (SOTA), which is
based on unsupervised learning. We propose a new architecture by combining
these two neural networks types to yield better and faster classi cation
results for our classi cation problem.
</summary>
    <author>
      <name>P. Boinee</name>
    </author>
    <author>
      <name>F. Barbarino</name>
    </author>
    <author>
      <name>A. De Angelis</name>
    </author>
    <author>
      <name>A. Saggion</name>
    </author>
    <author>
      <name>M. Zacchello</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/1-4020-4339-2_41</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/1-4020-4339-2_41" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures, to be published in the Proceedings of the 6th
  International Symposium ''Frontiers of Fundamental and Computational
  Physics'' (FFP6), Udine (Italy), Sep. 26-29, 2004</arxiv:comment>
    <link href="http://arxiv.org/abs/astro-ph/0503539v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/astro-ph/0503539v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/chao-dyn/9701020v1</id>
    <updated>1997-01-22T06:15:15Z</updated>
    <published>1997-01-22T06:15:15Z</published>
    <title>Chaos and Asymptotical Stability in Discrete-time Neural Networks</title>
    <summary>  This paper aims to theoretically prove by applying Marotto's Theorem that
both transiently chaotic neural networks (TCNN) and discrete-time recurrent
neural networks (DRNN) have chaotic structure. A significant property of TCNN
and DRNN is that they have only one fixed point, when absolute values of the
self-feedback connection weights in TCNN and the difference time in DRNN are
sufficiently large. We show that this unique fixed point can actually evolve
into a snap-back repeller which generates chaotic structure, if several
conditions are satisfied. On the other hand, by using the Lyapunov functions,
we also derive sufficient conditions on asymptotical stability for symmetrical
versions of both TCNN and DRNN, under which TCNN and DRNN asymptotically
converge to a fixed point. Furthermore, generic bifurcations are also
considered in this paper. Since both of TCNN and DRNN are not special but
simple and general, the obtained theoretical results hold for a wide class of
discrete-time neural networks. To demonstrate the theoretical results of this
paper better, several numerical simulations are provided as illustrating
examples.
</summary>
    <author>
      <name>Luonan Chen</name>
    </author>
    <author>
      <name>Kazuyuki Aihara</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/S0167-2789(96)00302-8</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/S0167-2789(96)00302-8" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper will be published in Physica D. Figures should be
  requested to the first author</arxiv:comment>
    <link href="http://arxiv.org/abs/chao-dyn/9701020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/chao-dyn/9701020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="chao-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="chao-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9612122v1</id>
    <updated>1996-12-12T18:00:13Z</updated>
    <published>1996-12-12T18:00:13Z</published>
    <title>Fermion Mapping for Orthogonal and Symplectic Ensembles</title>
    <summary>  The circular orthogonal and circular symplectic ensembles are mapped onto
free, non-hermitian fermion systems. As an illustration, the two-level form
factors are calculated.
</summary>
    <author>
      <name>M. B. Hastings</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, LaTeX</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9612122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9612122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9806289v1</id>
    <updated>1998-06-24T15:07:15Z</updated>
    <published>1998-06-24T15:07:15Z</published>
    <title>Spin Glasses</title>
    <summary>  An introduction and overview is given of the theory of spin glasses and its
application.
</summary>
    <author>
      <name>David Sherrington</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">59 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9806289v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9806289v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9812198v1</id>
    <updated>1998-12-11T16:08:51Z</updated>
    <published>1998-12-11T16:08:51Z</published>
    <title>An Oscillator Neural Network Retrieving Sparsely Coded Phase Patterns</title>
    <summary>  Little is known theoretically about the associative memory capabilities of
neural networks in which information is encoded not only in the mean firing
rate but also in the timing of firings. Particularly, in the case that the
fraction of active neurons involved in memorizing patterns becomes small, it is
biologically important to consider the timings of firings and to study how such
consideration influences storage capacities and quality of recalled patterns.
For this purpose, we propose a simple extended model of oscillator neural
networks to allow for expression of non-firing state. %which is able to
memorize sparsely coded phase patterns including non-firing states. Analyzing
both equilibrium states and dynamical properties in recalling processes, we
find that the system possesses good associative memory.
</summary>
    <author>
      <name>Toshio Aoyagi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Applied Mathematics and Physics, Graduate School of Informatics, Kyoto University</arxiv:affiliation>
    </author>
    <author>
      <name>Masaki Nomura</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Applied Mathematics and Physics, Graduate School of Informatics, Kyoto University</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.83.1062</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.83.1062" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 Postscript figures, uses epsbox.sty</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9812198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9812198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9912101v1</id>
    <updated>1999-12-07T07:55:44Z</updated>
    <published>1999-12-07T07:55:44Z</published>
    <title>Mutual information and self-control of a fully-connected low-activity
  neural network</title>
    <summary>  A self-control mechanism for the dynamics of a three-state fully-connected
neural network is studied through the introduction of a time-dependent
threshold. The self-adapting threshold is a function of both the neural and the
pattern activity in the network. The time evolution of the order parameters is
obtained on the basis of a recently developed dynamical recursive scheme. In
the limit of low activity the mutual information is shown to be the relevant
parameter in order to determine the retrieval quality. Due to self-control an
improvement of this mutual information content as well as an increase of the
storage capacity and an enlargement of the basins of attraction are found.
These results are compared with numerical simulations.
</summary>
    <author>
      <name>D. Bolle'</name>
    </author>
    <author>
      <name>D. Dominguez Carreta</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/S0378-4371(00)00308-3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/S0378-4371(00)00308-3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 8 ps.figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9912101v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9912101v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0009248v1</id>
    <updated>2000-09-17T13:06:02Z</updated>
    <published>2000-09-17T13:06:02Z</published>
    <title>Random Matrix Theory with Non-integer Beta-parameter</title>
    <summary>  We show that the random matrix theory with non-integer "symmetry parameter"
beta describes the statistics of transport parameters of strongly disordered
two dimensional systems.
</summary>
    <author>
      <name>Peter Markos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 Fig</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0009248v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0009248v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0205297v1</id>
    <updated>2002-05-14T20:49:46Z</updated>
    <published>2002-05-14T20:49:46Z</published>
    <title>Complex Systems: a Physicist's Viewpoint</title>
    <summary>  I present my viewpoint on complexity, stressing general arguments and using a
rather simple language.
</summary>
    <author>
      <name>Giorgio Parisi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, no figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0205297v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0205297v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0207657v1</id>
    <updated>2002-07-27T01:34:26Z</updated>
    <published>2002-07-27T01:34:26Z</published>
    <title>Time Interval Distribution of Earthquakes</title>
    <summary>  The statistical properties of time intervals between significant earthquakes
are found to be described by the Zipf-Mandelbrot-Tsallis-type distribution.
</summary>
    <author>
      <name>Sumiyoshi Abe</name>
    </author>
    <author>
      <name>Norikazu Suzuki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0207657v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0207657v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0208590v3</id>
    <updated>2010-07-12T17:09:28Z</updated>
    <published>2002-08-30T01:50:18Z</published>
    <title>Dreams, endocannabinoids and itinerant dynamics in neural networks: re
  elaborating Crick-Mitchison unlearning hypothesis</title>
    <summary>  In this work we reevaluate and elaborate Crick-Mitchison's proposal that
REM-sleep corresponds to a self-organized process for unlearning attractors in
neural networks. This reformulation is made at the face of recent findings
concerning the intense activation of the amygdalar complex during REM-sleep,
the role of endocannabinoids in synaptic weakening and neural network models
with itinerant associative dynamics. We distinguish between a neurological
REM-sleep function and a related evolutionary/behavioral dreaming function. At
the neurological level, we propose that REM-sleep regulates excessive
plasticity and weakens over stable brain activation patterns, specially in the
amygdala, hippocampus and motor systems. At the behavioral level, we propose
that dream narrative evolved as exploratory behavior made in a virtual
environment promoting "emotional (un)learning", that is, habituation of
emotional responses, anxiety and fear. We make several experimental predictions
at variance with those of Memory Consolidation Hipothesis. We also predict that
the "replay" of cells ensembles is done at an increasing faster pace along
REM-sleep.
</summary>
    <author>
      <name>Osame Kinouchi</name>
    </author>
    <author>
      <name>Renato Rodrigues Kinouchi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 2 figures, Revised version (2010)</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0208590v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0208590v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.pop-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0305565v1</id>
    <updated>2003-05-23T15:17:02Z</updated>
    <published>2003-05-23T15:17:02Z</published>
    <title>Theories of the Structural Glass Transition</title>
    <summary>  We review phenomenological and microscopic theories of the structural glass
transition
</summary>
    <author>
      <name>Rolf Schilling</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, 10 figures. appears in "Collective Dynamics of Nonlinear
  and Disordered Systems", eds. G.Radons, W.Just and P.Haeussler,
  Spronger(2003)</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0305565v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0305565v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0306538v2</id>
    <updated>2005-01-20T21:44:11Z</updated>
    <published>2003-06-20T20:24:47Z</published>
    <title>Linear frequency locking and the structure of lasing modes in random
  lasers</title>
    <summary>  The paper has been withdrawn.
</summary>
    <author>
      <name>L. I. Deych</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, RevTex</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0306538v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0306538v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mes-hall" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0409276v3</id>
    <updated>2004-11-26T14:25:20Z</updated>
    <published>2004-09-10T18:19:03Z</published>
    <title>This paper has been withdrawn</title>
    <summary>  The effect of the random quantum transverse field $\Omega$
</summary>
    <author>
      <name>H. Ez-Zahraouy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0409276v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0409276v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0505650v1</id>
    <updated>2005-05-26T15:37:33Z</updated>
    <published>2005-05-26T15:37:33Z</published>
    <title>Reply to a comment "No robust phases in aerogel..." (cond-mat/0505281)</title>
    <summary>  The arguments of Volovik are refuted.
</summary>
    <author>
      <name>I. A. Fomin</name>
    </author>
    <link href="http://arxiv.org/abs/cond-mat/0505650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0505650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.supr-con" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.supr-con" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0512022v2</id>
    <updated>2006-06-01T08:14:49Z</updated>
    <published>2005-12-01T15:52:30Z</published>
    <title>Genetic attack on neural cryptography</title>
    <summary>  Different scaling properties for the complexity of bidirectional
synchronization and unidirectional learning are essential for the security of
neural cryptography. Incrementing the synaptic depth of the networks increases
the synchronization time only polynomially, but the success of the geometric
attack is reduced exponentially and it clearly fails in the limit of infinite
synaptic depth. This method is improved by adding a genetic algorithm, which
selects the fittest neural networks. The probability of a successful genetic
attack is calculated for different model parameters using numerical
simulations. The results show that scaling laws observed in the case of other
attacks hold for the improved algorithm, too. The number of networks needed for
an effective attack grows exponentially with increasing synaptic depth. In
addition, finite-size effects caused by Hebbian and anti-Hebbian learning are
analyzed. These learning rules converge to the random walk rule if the synaptic
depth is small compared to the square root of the system size.
</summary>
    <author>
      <name>Andreas Ruttor</name>
    </author>
    <author>
      <name>Wolfgang Kinzel</name>
    </author>
    <author>
      <name>Rivka Naeh</name>
    </author>
    <author>
      <name>Ido Kanter</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.73.036121</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.73.036121" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 12 figures; section 5 amended, typos corrected</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 73, 036121 (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0512022v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0512022v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0609713v1</id>
    <updated>2006-09-27T17:38:23Z</updated>
    <published>2006-09-27T17:38:23Z</published>
    <title>Cluster Probability in Bootstrap Percolation</title>
    <summary>  We develop a recursive formula for the probability of a k-cluster in
bootstrap percolation.
</summary>
    <author>
      <name>A. B. Harris</name>
    </author>
    <author>
      <name>Andrea J. Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, no figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0609713v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0609713v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0612571v3</id>
    <updated>2009-10-07T01:22:13Z</updated>
    <published>2006-12-21T22:19:13Z</published>
    <title>Self-destruction of the electric dipolar glass</title>
    <summary>  This paper has been withdrawn.
</summary>
    <author>
      <name>M. Schechter</name>
    </author>
    <author>
      <name>P. C. E. Stamp</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">paper withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0612571v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0612571v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0701138v1</id>
    <updated>2007-01-08T09:20:52Z</updated>
    <published>2007-01-08T09:20:52Z</published>
    <title>Effects of degree distribution in mutual synchronization of neural
  networks</title>
    <summary>  We study the effects of the degree distribution in mutual synchronization of
two-layer neural networks. We carry out three coupling strategies: large-large
coupling, random coupling, and small-small coupling. By computer simulations
and analytical methods, we find that couplings between nodes with large degree
play an important role in the synchronization. For large-large coupling, less
couplings are needed for inducing synchronization for both random and
scale-free networks. For random coupling, cutting couplings between nodes with
large degree is very efficient for preventing neural systems from
synchronization, especially when subnetworks are scale-free.
</summary>
    <author>
      <name>Sheng-Jun Wang</name>
    </author>
    <author>
      <name>Xin-Jian Xu</name>
    </author>
    <author>
      <name>Zhi-Xi Wu</name>
    </author>
    <author>
      <name>Ying-Hai Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.74.041915</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.74.041915" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physical Review E 74, 041915 (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0701138v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0701138v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0703823v1</id>
    <updated>2007-03-30T19:44:14Z</updated>
    <published>2007-03-30T19:44:14Z</published>
    <title>Note on the exponent puzzle of the Anderson-Mott transition</title>
    <summary>  The exponent puzzle of the Anderson-Mott transition is discussed on the basis
of a duality model for strongly correlated electrons.
</summary>
    <author>
      <name>O. Narikiyo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0703823v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0703823v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.str-el" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.str-el" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0405013v1</id>
    <updated>2004-05-05T00:44:12Z</updated>
    <published>2004-05-05T00:44:12Z</published>
    <title>DCT Based Texture Classification Using Soft Computing Approach</title>
    <summary>  Classification of texture pattern is one of the most important problems in
pattern recognition. In this paper, we present a classification method based on
the Discrete Cosine Transform (DCT) coefficients of texture image. As DCT works
on gray level image, the color scheme of each image is transformed into gray
levels. For classifying the images using DCT we used two popular soft computing
techniques namely neurocomputing and neuro-fuzzy computing. We used a
feedforward neural network trained using the backpropagation learning and an
evolving fuzzy neural network to classify the textures. The soft computing
models were trained using 80% of the texture data and remaining was used for
testing and validation purposes. A performance comparison was made among the
soft computing models for the texture classification problem. We also analyzed
the effects of prolonged training of neural networks. It is observed that the
proposed neuro-fuzzy model performed better than neural network.
</summary>
    <author>
      <name>Golam Sorwar</name>
    </author>
    <author>
      <name>Ajith Abraham</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Malaysian Journal of Computer Science, 2004 (forth coming)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0405013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0405013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/physics/0306075v1</id>
    <updated>2003-06-10T06:46:20Z</updated>
    <published>2003-06-10T06:46:20Z</published>
    <title>Combined first--principles calculation and neural--network correction
  approach as a powerful tool in computational physics and chemistry</title>
    <summary>  Despite of their success, the results of first-principles quantum mechanical
calculations contain inherent numerical errors caused by various
approximations. We propose here a neural-network algorithm to greatly reduce
these inherent errors. As a demonstration, this combined quantum mechanical
calculation and neural-network correction approach is applied to the evaluation
of standard heat of formation $\DelH$ and standard Gibbs energy of formation
$\DelG$ for 180 organic molecules at 298 K. A dramatic reduction of numerical
errors is clearly shown with systematic deviations being eliminated. For
examples, the root--mean--square deviation of the calculated $\DelH$ ($\DelG$)
for the 180 molecules is reduced from 21.4 (22.3) kcal$\cdotp$mol$^{-1}$ to 3.1
(3.3) kcal$\cdotp$mol$^{-1}$ for B3LYP/6-311+G({\it d,p}) and from 12.0 (12.9)
kcal$\cdotp$mol$^{-1}$ to 3.3 (3.4) kcal$\cdotp$mol$^{-1}$ for
B3LYP/6-311+G(3{\it df},2{\it p}) before and after the neural-network
correction.
</summary>
    <author>
      <name>LiHong Hu</name>
    </author>
    <author>
      <name>XiuJun Wang</name>
    </author>
    <author>
      <name>LaiHo Wong</name>
    </author>
    <author>
      <name>GuanHua Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures, REVTeX4 style, submitted to Phys.Rev.Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/physics/0306075v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/physics/0306075v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/physics/0402070v1</id>
    <updated>2004-02-16T06:57:40Z</updated>
    <published>2004-02-16T06:57:40Z</published>
    <title>Using a neural network approach for muon reconstruction and triggering</title>
    <summary>  The extremely high rate of events that will be produced in the future Large
Hadron Collider requires the triggering mechanism to take precise decisions in
a few nano-seconds. We present a study which used an artificial neural network
triggering algorithm and compared it to the performance of a dedicated
electronic muon triggering system. Relatively simple architecture was used to
solve a complicated inverse problem. A comparison with a realistic example of
the ATLAS first level trigger simulation was in favour of the neural network. A
similar architecture trained after the simulation of the electronics first
trigger stage showed a further background rejection.
</summary>
    <author>
      <name>E. Etzion</name>
    </author>
    <author>
      <name>H. Abramowicz</name>
    </author>
    <author>
      <name>Y. Benhammou</name>
    </author>
    <author>
      <name>D. Horn</name>
    </author>
    <author>
      <name>L. Levinson</name>
    </author>
    <author>
      <name>R. Livneh</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.nima.2004.07.091</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.nima.2004.07.091" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A talk given at ACAT03, KEK, Japan, November 2003. Submitted to
  Nuclear Instruments and Methods in Physics Research, Section A</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Nucl.Instrum.Meth. A534 (2004) 222-227</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/physics/0402070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/physics/0402070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/physics/0703229v1</id>
    <updated>2007-03-25T15:56:07Z</updated>
    <published>2007-03-25T15:56:07Z</published>
    <title>The Normalized Radial Basis Function Neural Network and its Relation to
  the Perceptron</title>
    <summary>  The normalized radial basis function neural network emerges in the
statistical modeling of natural laws that relate components of multivariate
data. The modeling is based on the kernel estimator of the joint probability
density function pertaining to given data. From this function a governing law
is extracted by the conditional average estimator. The corresponding
nonparametric regression represents a normalized radial basis function neural
network and can be related with the multi-layer perceptron equation. In this
article an exact equivalence of both paradigms is demonstrated for a
one-dimensional case with symmetric triangular basis functions. The
transformation provides for a simple interpretation of perceptron parameters in
terms of statistical samples of multivariate data.
</summary>
    <author>
      <name>I. Grabec</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 eps figures, uses IEEEtran.cls</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dynamics of Continuous, Discrete and Impulsive Systems, B, Special
  Volume: Advances in Neural Networks, 2007 (in print)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/physics/0703229v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/physics/0703229v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/quant-ph/0607053v2</id>
    <updated>2008-01-04T16:22:42Z</updated>
    <published>2006-07-07T13:03:28Z</published>
    <title>Error-Resistant Distributed Quantum Computation in Trapped Ion Chain</title>
    <summary>  We consider experimentally feasible chains of trapped ions with pseudo-spin
1/2, and find models that can potentially be used to implement error-resistant
quantum computation. Similar in spirit to classical neural networks, the
error-resistance of the system is achieved by encoding the qubits distributed
over the whole system. We therefore call our system a ''quantum neural
network'', and present a ''quantum neural network model of quantum
computation''. Qubits are encoded in a few quasi-degenerated low energy levels
of the whole system, separated by a large gap from the excited states, and
large energy barriers between themselves. We investigate protocols for
implementing a universal set of quantum logic gates in the system, by adiabatic
passage of a few low-lying energy levels of the whole system. Naturally
appearing and potentially dangerous distributed noise in the system leaves the
fidelity of the computation virtually unchanged, if it is not too strong. The
computation is also naturally resilient to local perturbations of the spins.
</summary>
    <author>
      <name>Sibylle Braungardt</name>
    </author>
    <author>
      <name>Aditi Sen De</name>
    </author>
    <author>
      <name>Ujjwal Sen</name>
    </author>
    <author>
      <name>Maciej Lewenstein</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevA.76.042307</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevA.76.042307" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 figures, RevTeX4; v2: another noise model analysed,
  published version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. A 76, 042307 (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/quant-ph/0607053v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/quant-ph/0607053v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.3690v2</id>
    <updated>2008-04-07T09:40:27Z</updated>
    <published>2007-05-25T04:56:59Z</published>
    <title>A mathematical analysis of the effects of Hebbian learning rules on the
  dynamics and structure of discrete-time random recurrent neural networks</title>
    <summary>  We present a mathematical analysis of the effects of Hebbian learning in
random recurrent neural networks, with a generic Hebbian learning rule
including passive forgetting and different time scales for neuronal activity
and learning dynamics. Previous numerical works have reported that Hebbian
learning drives the system from chaos to a steady state through a sequence of
bifurcations. Here, we interpret these results mathematically and show that
these effects, involving a complex coupling between neuronal dynamics and
synaptic graph structure, can be analyzed using Jacobian matrices, which
introduce both a structural and a dynamical point of view on the neural network
evolution. Furthermore, we show that the sensitivity to a learned pattern is
maximal when the largest Lyapunov exponent is close to 0. We discuss how neural
networks may take advantage of this regime of high functional interest.
</summary>
    <author>
      <name>Benoit Siri</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs</arxiv:affiliation>
    </author>
    <author>
      <name>Hugues Berry</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs</arxiv:affiliation>
    </author>
    <author>
      <name>Bruno Cessac</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INLN</arxiv:affiliation>
    </author>
    <author>
      <name>Bruno Delord</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ANIM</arxiv:affiliation>
    </author>
    <author>
      <name>Mathias Quoy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ETIS</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/0705.3690v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.3690v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0708.0328v1</id>
    <updated>2007-08-02T11:40:55Z</updated>
    <published>2007-08-02T11:40:55Z</published>
    <title>Adaptive thresholds for neural networks with synaptic noise</title>
    <summary>  The inclusion of a macroscopic adaptive threshold is studied for the
retrieval dynamics of both layered feedforward and fully connected neural
network models with synaptic noise. These two types of architectures require a
different method to be solved numerically. In both cases it is shown that, if
the threshold is chosen appropriately as a function of the cross-talk noise and
of the activity of the stored patterns, adapting itself automatically in the
course of the recall process, an autonomous functioning of the network is
guaranteed. This self-control mechanism considerably improves the quality of
retrieval, in particular the storage capacity, the basins of attraction and the
mutual information content.
</summary>
    <author>
      <name>D. Bolle</name>
    </author>
    <author>
      <name>R. Heylen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 10 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Neural Systems, Vol. 17, No. 4 (2007)
  241-252</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0708.0328v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.0328v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.1012v1</id>
    <updated>2009-03-05T14:48:10Z</updated>
    <published>2009-03-05T14:48:10Z</published>
    <title>The simulation of the activity dependent neural network growth</title>
    <summary>  It is currently accepted that cortical maps are dynamic constructions that
are altered in response to external input. Experience-dependent structural
changes in cortical microcurcuts lead to changes of activity, i.e. to changes
in information encoded. Specific patterns of external stimulation can lead to
creation of new synaptic connections between neurons. The calcium influxes
controlled by neuronal activity regulate the processes of neurotrophic factors
released by neurons, growth cones movement and synapse differentiation in
developing neural systems. We propose a model for description and investigation
of the activity dependent development of neural networks. The dynamics of the
network parameters (activity, diffusion of axon guidance chemicals, growth cone
position) is described by a closed set of differential equations. The model
presented here describes the development of neural networks under the
assumption of activity dependent axon guidance molecules. Numerical simulation
shows that morpholess neurons compromise the development of cortical
connectivity.
</summary>
    <author>
      <name>F. Gafarov</name>
    </author>
    <author>
      <name>N. Khusnutdinov</name>
    </author>
    <author>
      <name>F. Galimyanov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1142/S0219635209002058</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1142/S0219635209002058" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0903.1012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.1012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.3155v2</id>
    <updated>2009-07-06T18:52:05Z</updated>
    <published>2009-05-19T18:48:25Z</published>
    <title>Search for Standard Model Higgs Boson Production in Association with a W
  Boson using a Neural Network</title>
    <summary>  We present a search for standard model Higgs boson production in association
with a W boson in proton-antiproton collisions at a center of mass energy of
1.96 TeV. The search employs data collected with the CDF II detector that
correspond to an integrated luminosity of approximately 1.9 inverse fb. We
select events consistent with a signature of a single charged lepton, missing
transverse energy, and two jets. Jets corresponding to bottom quarks are
identified with a secondary vertex tagging method, a jet probability tagging
method, and a neural network filter. We use kinematic information in an
artificial neural network to improve discrimination between signal and
background compared to previous analyses. The observed number of events and the
neural network output distributions are consistent with the standard model
background expectations, and we set 95% confidence level upper limits on the
production cross section times branching fraction ranging from 1.2 to 1.1 pb or
7.5 to 102 times the standard model expectation for Higgs boson masses from 110
to $150 GeV/c^2, respectively.
</summary>
    <author>
      <name> CDF Collaboration</name>
    </author>
    <author>
      <name>T. Aaltonen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevD.80.012002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevD.80.012002" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Phys. Rev. D</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys.Rev.D80:012002,2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0905.3155v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.3155v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.0075v1</id>
    <updated>2009-07-01T07:35:52Z</updated>
    <published>2009-07-01T07:35:52Z</published>
    <title>XDANNG: XML based Distributed Artificial Neural Network with Globus
  Toolkit</title>
    <summary>  Artificial Neural Network is one of the most common AI application fields.
This field has direct and indirect usages most sciences. The main goal of ANN
is to imitate biological neural networks for solving scientific problems. But
the level of parallelism is the main problem of ANN systems in comparison with
biological systems. To solve this problem, we have offered a XML-based
framework for implementing ANN on the Globus Toolkit Platform. Globus Toolkit
is well known management software for multipurpose Grids. Using the Grid for
simulating the neuron network will lead to a high degree of parallelism in the
implementation of ANN. We have used the XML for improving flexibility and
scalability in our framework.
</summary>
    <author>
      <name>Hamidreza Mahini</name>
    </author>
    <author>
      <name>Alireza Mahini</name>
    </author>
    <author>
      <name>Javad Ghofrani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, international journal of computer science and information
  security</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSIS June 2009 Issue, Vol. 2 No. 1</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0907.0075v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.0075v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.3368v1</id>
    <updated>2012-07-13T21:28:17Z</updated>
    <published>2012-07-13T21:28:17Z</published>
    <title>Learning the Pseudoinverse Solution to Network Weights</title>
    <summary>  The last decade has seen the parallel emergence in computational neuroscience
and machine learning of neural network structures which spread the input signal
randomly to a higher dimensional space; perform a nonlinear activation; and
then solve for a regression or classification output by means of a mathematical
pseudoinverse operation. In the field of neuromorphic engineering, these
methods are increasingly popular for synthesizing biologically plausible neural
networks, but the "learning method" - computation of the pseudoinverse by
singular value decomposition - is problematic both for biological plausibility
and because it is not an online or an adaptive method. We present an online or
incremental method of computing the pseudoinverse, which we argue is
biologically plausible as a learning method, and which can be made adaptable
for non-stationary data streams. The method is significantly more
memory-efficient than the conventional computation of pseudoinverses by
singular value decomposition.
</summary>
    <author>
      <name>Jonathan Tapson</name>
    </author>
    <author>
      <name>Andre van Schaik</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neunet.2013.02.008</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neunet.2013.02.008" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 3 figures; in submission to Neural Networks</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.3368v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.3368v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.6107v1</id>
    <updated>2013-01-24T18:20:26Z</updated>
    <published>2013-01-24T18:20:26Z</published>
    <title>On the correction of anomalous phase oscillation in entanglement
  witnesses using quantum neural networks</title>
    <summary>  Entanglement of a quantum system depends upon relative phase in complicated
ways, which no single measurement can reflect. Because of this, entanglement
witnesses are necessarily limited in applicability and/or utility. We propose
here a solution to the problem using quantum neural networks. A quantum system
contains the information of its entanglement; thus, if we are clever, we can
extract that information efficiently. As proof of concept, we show how this can
be done for the case of pure states of a two-qubit system, using an
entanglement indicator corrected for the anomalous phase oscillation. Both the
entanglement indicator and the phase correction are calculated by the quantum
system itself acting as a neural network.
</summary>
    <author>
      <name>E. C. Behrman</name>
    </author>
    <author>
      <name>R. E. F. Bonde</name>
    </author>
    <author>
      <name>J. E. Steck</name>
    </author>
    <author>
      <name>J. F. Behrman</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE-Transactions on Neural Networks and Learning Systems 25,
  1696-1703 (2014)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1301.6107v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.6107v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.5207v1</id>
    <updated>2013-04-12T22:30:20Z</updated>
    <published>2013-04-12T22:30:20Z</published>
    <title>Case's Eigenvalues by Markel's Matrix</title>
    <summary>  A method of computing Case's eigenvalues is proposed. The eigenvalues are
obtained as eigenvalues of a tridiagonal matrix.
</summary>
    <author>
      <name>Manabu Machida</name>
    </author>
    <link href="http://arxiv.org/abs/1304.5207v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.5207v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.0159v1</id>
    <updated>2014-04-01T08:23:10Z</updated>
    <published>2014-04-01T08:23:10Z</published>
    <title>Quantum walks on graphs representing the firing patterns of a quantum
  neural network</title>
    <summary>  Quantum walks have been shown to be fruitful tools in analysing the dynamic
properties of quantum systems. This article proposes to use quantum walks as an
approach to Quantum Neural Networks (QNNs). QNNs replace binary McCulloch-Pitts
neurons with a qubit in order to use the advantages of quantum computing in
neural networks. A quantum walk on the firing states of such a QNN is supposed
to simulate central properties of the dynamics of classical neural networks,
such as associative memory. It is shown that a biased discrete Hadamard walk
derived from the updating process of a biological neuron does not lead to a
unitary walk. However, a Stochastic Quantum Walk between the global firing
states of a QNN can be constructed and it is shown that it contains the feature
of associative memory. The quantum contribution to the walk accounts for a
modest speed-up in some regimes.
</summary>
    <author>
      <name>Maria Schuld</name>
    </author>
    <author>
      <name>Ilya Sinayskiy</name>
    </author>
    <author>
      <name>Francesco Petruccione</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevA.89.032333</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevA.89.032333" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. A 89 032333 (2014)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.0159v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.0159v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.7828v4</id>
    <updated>2014-10-08T10:00:38Z</updated>
    <published>2014-04-30T18:39:00Z</published>
    <title>Deep Learning in Neural Networks: An Overview</title>
    <summary>  In recent years, deep artificial neural networks (including recurrent ones)
have won numerous contests in pattern recognition and machine learning. This
historical survey compactly summarises relevant work, much of it from the
previous millennium. Shallow and deep learners are distinguished by the depth
of their credit assignment paths, which are chains of possibly learnable,
causal links between actions and effects. I review deep supervised learning
(also recapitulating the history of backpropagation), unsupervised learning,
reinforcement learning &amp; evolutionary computation, and indirect search for
short programs encoding deep and large networks.
</summary>
    <author>
      <name>Juergen Schmidhuber</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neunet.2014.09.003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neunet.2014.09.003" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">88 pages, 888 references</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Networks, Vol 61, pp 85-117, Jan 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.7828v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.7828v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.6247v1</id>
    <updated>2014-06-24T14:16:56Z</updated>
    <published>2014-06-24T14:16:56Z</published>
    <title>Recurrent Models of Visual Attention</title>
    <summary>  Applying convolutional neural networks to large images is computationally
expensive because the amount of computation scales linearly with the number of
image pixels. We present a novel recurrent neural network model that is capable
of extracting information from an image or video by adaptively selecting a
sequence of regions or locations and only processing the selected regions at
high resolution. Like convolutional neural networks, the proposed model has a
degree of translation invariance built-in, but the amount of computation it
performs can be controlled independently of the input image size. While the
model is non-differentiable, it can be trained using reinforcement learning
methods to learn task-specific policies. We evaluate our model on several image
classification tasks, where it significantly outperforms a convolutional neural
network baseline on cluttered images, and on a dynamic visual control problem,
where it learns to track a simple object without an explicit training signal
for doing so.
</summary>
    <author>
      <name>Volodymyr Mnih</name>
    </author>
    <author>
      <name>Nicolas Heess</name>
    </author>
    <author>
      <name>Alex Graves</name>
    </author>
    <author>
      <name>Koray Kavukcuoglu</name>
    </author>
    <link href="http://arxiv.org/abs/1406.6247v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.6247v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.7690v1</id>
    <updated>2014-06-30T12:08:38Z</updated>
    <published>2014-06-30T12:08:38Z</published>
    <title>A neural network clustering algorithm for the ATLAS silicon pixel
  detector</title>
    <summary>  A novel technique to identify and split clusters created by multiple charged
particles in the ATLAS pixel detector using a set of artificial neural networks
is presented. Such merged clusters are a common feature of tracks originating
from highly energetic objects, such as jets. Neural networks are trained using
Monte Carlo samples produced with a detailed detector simulation. This
technique replaces the former clustering approach based on a connected
component analysis and charge interpolation. The performance of the neural
network splitting technique is quantified using data from proton--proton
collisions at the LHC collected by the ATLAS detector in 2011 and from Monte
Carlo simulations. This technique reduces the number of clusters shared between
tracks in highly energetic jets by up to a factor of three. It also provides
more precise position and error estimates of the clusters in both the
transverse and longitudinal impact parameter resolution.
</summary>
    <author>
      <name> ATLAS collaboration</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1748-0221/9/09/P09009</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1748-0221/9/09/P09009" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages plus author list + cover pages (38 pages total), 10 figures,
  0 tables, submitted to JINST, All figures including auxiliary figures are
  available at
  http://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/PERF-2012-05</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">JINST 9 (2014) P09009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1406.7690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.7690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.2873v2</id>
    <updated>2014-12-08T20:21:52Z</updated>
    <published>2014-08-12T22:40:21Z</published>
    <title>First-Pass Large Vocabulary Continuous Speech Recognition using
  Bi-Directional Recurrent DNNs</title>
    <summary>  We present a method to perform first-pass large vocabulary continuous speech
recognition using only a neural network and language model. Deep neural network
acoustic models are now commonplace in HMM-based speech recognition systems,
but building such systems is a complex, domain-specific task. Recent work
demonstrated the feasibility of discarding the HMM sequence modeling framework
by directly predicting transcript text from audio. This paper extends this
approach in two ways. First, we demonstrate that a straightforward recurrent
neural network architecture can achieve a high level of accuracy. Second, we
propose and evaluate a modified prefix-search decoding algorithm. This approach
to decoding enables first-pass speech recognition with a language model,
completely unaided by the cumbersome infrastructure of HMM-based systems.
Experiments on the Wall Street Journal corpus demonstrate fairly competitive
word error rates, and the importance of bi-directional network recurrence.
</summary>
    <author>
      <name>Awni Y. Hannun</name>
    </author>
    <author>
      <name>Andrew L. Maas</name>
    </author>
    <author>
      <name>Daniel Jurafsky</name>
    </author>
    <author>
      <name>Andrew Y. Ng</name>
    </author>
    <link href="http://arxiv.org/abs/1408.2873v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.2873v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.6369v1</id>
    <updated>2014-11-24T07:28:21Z</updated>
    <published>2014-11-24T07:28:21Z</published>
    <title>Scale-Invariant Convolutional Neural Networks</title>
    <summary>  Even though convolutional neural networks (CNN) has achieved near-human
performance in various computer vision tasks, its ability to tolerate scale
variations is limited. The popular practise is making the model bigger first,
and then train it with data augmentation using extensive scale-jittering. In
this paper, we propose a scaleinvariant convolutional neural network (SiCNN), a
modeldesigned to incorporate multi-scale feature exaction and classification
into the network structure. SiCNN uses a multi-column architecture, with each
column focusing on a particular scale. Unlike previous multi-column strategies,
these columns share the same set of filter parameters by a scale transformation
among them. This design deals with scale variation without blowing up the model
size. Experimental results show that SiCNN detects features at various scales,
and the classification result exhibits strong robustness against object scale
variations.
</summary>
    <author>
      <name>Yichong Xu</name>
    </author>
    <author>
      <name>Tianjun Xiao</name>
    </author>
    <author>
      <name>Jiaxing Zhang</name>
    </author>
    <author>
      <name>Kuiyuan Yang</name>
    </author>
    <author>
      <name>Zheng Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is submitted for CVPR2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.6369v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.6369v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.5778v1</id>
    <updated>2013-03-22T20:55:48Z</updated>
    <published>2013-03-22T20:55:48Z</published>
    <title>Speech Recognition with Deep Recurrent Neural Networks</title>
    <summary>  Recurrent neural networks (RNNs) are a powerful model for sequential data.
End-to-end training methods such as Connectionist Temporal Classification make
it possible to train RNNs for sequence labelling problems where the
input-output alignment is unknown. The combination of these methods with the
Long Short-term Memory RNN architecture has proved particularly fruitful,
delivering state-of-the-art results in cursive handwriting recognition. However
RNN performance in speech recognition has so far been disappointing, with
better results returned by deep feedforward networks. This paper investigates
\emph{deep recurrent neural networks}, which combine the multiple levels of
representation that have proved so effective in deep networks with the flexible
use of long range context that empowers RNNs. When trained end-to-end with
suitable regularisation, we find that deep Long Short-term Memory RNNs achieve
a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to
our knowledge is the best recorded score.
</summary>
    <author>
      <name>Alex Graves</name>
    </author>
    <author>
      <name>Abdel-rahman Mohamed</name>
    </author>
    <author>
      <name>Geoffrey Hinton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ICASSP 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.5778v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.5778v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.3096v1</id>
    <updated>2009-11-16T17:47:14Z</updated>
    <published>2009-11-16T17:47:14Z</published>
    <title>The replica symmetric behavior of the analogical neural network</title>
    <summary>  In this paper we continue our investigation of the analogical neural network,
paying interest to its replica symmetric behavior in the absence of external
fields of any type. Bridging the neural network to a bipartite spin-glass, we
introduce and apply a new interpolation scheme to its free energy that
naturally extends the interpolation via cavity fields or stochastic
perturbations to these models. As a result we obtain the free energy of the
system as a sum rule, which, at least at the replica symmetric level, can be
solved exactly. As a next step we study its related self-consistent equations
for the order parameters and their rescaled fluctuations, found to diverge on
the same critical line of the standard Amit-Gutfreund-Sompolinsky theory.
</summary>
    <author>
      <name>Adriano Barra</name>
    </author>
    <author>
      <name>Giuseppe Genovese</name>
    </author>
    <author>
      <name>Francesco Guerra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10955-010-0020-y</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10955-010-0020-y" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0911.3096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.3096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.6014v1</id>
    <updated>2011-05-30T15:19:55Z</updated>
    <published>2011-05-30T15:19:55Z</published>
    <title>Neural Networks for Emotion Classification</title>
    <summary>  It is argued that for the computer to be able to interact with humans, it
needs to have the communication skills of humans. One of these skills is the
ability to understand the emotional state of the person. This thesis describes
a neural network-based approach for emotion classification. We learn a
classifier that can recognize six basic emotions with an average accuracy of
77% over the Cohn-Kanade database. The novelty of this work is that instead of
empirically selecting the parameters of the neural network, i.e. the learning
rate, activation function parameter, momentum number, the number of nodes in
one layer, etc. we developed a strategy that can automatically select
comparatively better combination of these parameters. We also introduce another
way to perform back propagation. Instead of using the partial differential of
the error function, we use optimal algorithm; namely Powell's direction set to
minimize the error function. We were also interested in construction an
authentic emotion databases. This is a very important task because nowadays
there is no such database available. Finally, we perform several experiments
and show that our neural network approach can be successfully used for emotion
recognition.
</summary>
    <author>
      <name>Yafei Sun</name>
    </author>
    <link href="http://arxiv.org/abs/1105.6014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.6014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.6531v1</id>
    <updated>2012-08-31T15:39:17Z</updated>
    <published>2012-08-31T15:39:17Z</published>
    <title>Comment: Characterizing dynamic length scales in glass forming liquids</title>
    <summary>  Comment on Nature Physics 8, 164 (2012) by Kob, Roldan-Vargas and Berthier
</summary>
    <author>
      <name>Elijah Flenner</name>
    </author>
    <author>
      <name>Grzegorz Szamel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/nphys2437</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/nphys2437" rel="related"/>
    <link href="http://arxiv.org/abs/1208.6531v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.6531v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.7956v1</id>
    <updated>2012-10-30T10:41:30Z</updated>
    <published>2012-10-30T10:41:30Z</published>
    <title>Implementation of a Vision System for a Landmine Detecting Robot Using
  Artificial Neural Network</title>
    <summary>  Landmines, specifically anti-tank mines, cluster bombs, and unexploded
ordnance form a serious problem in many countries. Several landmine sweeping
techniques are used for minesweeping. This paper presents the design and the
implementation of the vision system of an autonomous robot for landmines
localization. The proposed work develops state-of-the-art techniques in digital
image processing for pre-processing captured images of the contaminated area.
After enhancement, Artificial Neural Network (ANN) is used in order to
identify, recognize and classify the landmines' make and model. The
Back-Propagation algorithm is used for training the network. The proposed work
proved to be able to identify and classify different types of landmines under
various conditions (rotated landmine, partially covered landmine) with a
success rate of up to 90%.
</summary>
    <author>
      <name>Roger Achkar</name>
    </author>
    <author>
      <name>Michel Owayjan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijaia.2012.3507</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijaia.2012.3507" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 14 figures, 4 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Achkar R, Owayjan M. Implementation of a Vision System for a
  Landmine Detecting Robot Using Artificial Neural Network. International
  Journal of Artificial Intelligence &amp; Applications (IJAIA), Vol 3, No. 5, pp.
  73-92, September 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1210.7956v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.7956v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T45" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.6091v3</id>
    <updated>2014-03-06T03:06:36Z</updated>
    <published>2013-11-24T08:04:41Z</published>
    <title>A Primal-Dual Method for Training Recurrent Neural Networks Constrained
  by the Echo-State Property</title>
    <summary>  We present an architecture of a recurrent neural network (RNN) with a
fully-connected deep neural network (DNN) as its feature extractor. The RNN is
equipped with both causal temporal prediction and non-causal look-ahead, via
auto-regression (AR) and moving-average (MA), respectively. The focus of this
paper is a primal-dual training method that formulates the learning of the RNN
as a formal optimization problem with an inequality constraint that provides a
sufficient condition for the stability of the network dynamics. Experimental
results demonstrate the effectiveness of this new method, which achieves 18.86%
phone recognition error on the TIMIT benchmark for the core test set. The
result approaches the best result of 17.7%, which was obtained by using RNN
with long short-term memory (LSTM). The results also show that the proposed
primal-dual training method produces lower recognition errors than the popular
RNN methods developed earlier based on the carefully tuned threshold parameter
that heuristically prevents the gradient from exploding.
</summary>
    <author>
      <name>Jianshu Chen</name>
    </author>
    <author>
      <name>Li Deng</name>
    </author>
    <link href="http://arxiv.org/abs/1311.6091v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.6091v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.1287v1</id>
    <updated>2013-12-04T19:21:02Z</updated>
    <published>2013-12-04T19:21:02Z</published>
    <title>Using neural networks to estimate redshift distributions. An application
  to CFHTLenS</title>
    <summary>  We present a novel way of using neural networks (NN) to estimate the redshift
distribution of a galaxy sample. We are able to obtain a probability density
function (PDF) for each galaxy using a classification neural network. The
method is applied to 58714 galaxies in CFHTLenS that have spectroscopic
redshifts from DEEP2, VVDS and VIPERS. Using this data we show that the stacked
PDF's give an excellent representation of the true $N(z)$ using information
from 5, 4 or 3 photometric bands. We show that the fractional error due to
using N(z_(phot)) instead of N(z_(truth)) is &lt;=1 on the lensing power spectrum
P_(kappa) in several tomographic bins. Further we investigate how well this
method performs when few training samples are available and show that in this
regime the neural network slightly overestimates the N(z) at high z. Finally
the case where the training sample is not representative of the full data set
is investigated. An IPython notebook accompanying this paper is made available
here: https://bitbucket.org/christopher_bonnett/nn_notebook
</summary>
    <author>
      <name>Christopher Bonnett</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/mnras/stv230</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/mnras/stv230" rel="related"/>
    <link href="http://arxiv.org/abs/1312.1287v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.1287v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.5419v3</id>
    <updated>2014-05-15T11:32:03Z</updated>
    <published>2013-12-19T06:53:24Z</published>
    <title>Large-scale Multi-label Text Classification - Revisiting Neural Networks</title>
    <summary>  Neural networks have recently been proposed for multi-label classification
because they are able to capture and model label dependencies in the output
layer. In this work, we investigate limitations of BP-MLL, a neural network
(NN) architecture that aims at minimizing pairwise ranking error. Instead, we
propose to use a comparably simple NN approach with recently proposed learning
techniques for large-scale multi-label text classification tasks. In
particular, we show that BP-MLL's ranking loss minimization can be efficiently
and effectively replaced with the commonly used cross entropy error function,
and demonstrate that several advances in neural network training that have been
developed in the realm of deep learning can be effectively employed in this
setting. Our experimental results show that simple NN models equipped with
advanced techniques such as rectified linear units, dropout, and AdaGrad
perform as well as or even outperform state-of-the-art approaches on six
large-scale textual datasets with diverse characteristics.
</summary>
    <author>
      <name>Jinseok Nam</name>
    </author>
    <author>
      <name>Jungi Kim</name>
    </author>
    <author>
      <name>Eneldo Loza Menc√≠a</name>
    </author>
    <author>
      <name>Iryna Gurevych</name>
    </author>
    <author>
      <name>Johannes F√ºrnkranz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 4 figures, submitted to ECML 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.5419v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.5419v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.5813v2</id>
    <updated>2014-06-09T08:39:37Z</updated>
    <published>2013-12-20T05:22:20Z</published>
    <title>Unsupervised Pretraining Encourages Moderate-Sparseness</title>
    <summary>  It is well known that direct training of deep neural networks will generally
lead to poor results. A major progress in recent years is the invention of
various pretraining methods to initialize network parameters and it was shown
that such methods lead to good prediction performance. However, the reason for
the success of pretraining has not been fully understood, although it was
argued that regularization and better optimization play certain roles. This
paper provides another explanation for the effectiveness of pretraining, where
we show pretraining leads to a sparseness of hidden unit activation in the
resulting neural networks. The main reason is that the pretraining models can
be interpreted as an adaptive sparse coding. Compared to deep neural network
with sigmoid function, our experimental results on MNIST and Birdsong further
support this sparseness observation.
</summary>
    <author>
      <name>Jun Li</name>
    </author>
    <author>
      <name>Wei Luo</name>
    </author>
    <author>
      <name>Jian Yang</name>
    </author>
    <author>
      <name>Xiaotong Yuan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures, (to appear) ICML-Workshop on Unsupervised
  Learning from Bioacoustic Big Data (uLearnBio) 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.5813v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.5813v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.6186v1</id>
    <updated>2013-12-21T00:56:56Z</updated>
    <published>2013-12-21T00:56:56Z</published>
    <title>GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network
  Training</title>
    <summary>  The ability to train large-scale neural networks has resulted in
state-of-the-art performance in many areas of computer vision. These results
have largely come from computational break throughs of two forms: model
parallelism, e.g. GPU accelerated training, which has seen quick adoption in
computer vision circles, and data parallelism, e.g. A-SGD, whose large scale
has been used mostly in industry. We report early experiments with a system
that makes use of both model parallelism and data parallelism, we call GPU
A-SGD. We show using GPU A-SGD it is possible to speed up training of large
convolutional neural networks useful for computer vision. We believe GPU A-SGD
will make it possible to train larger networks on larger training sets in a
reasonable amount of time.
</summary>
    <author>
      <name>Thomas Paine</name>
    </author>
    <author>
      <name>Hailin Jin</name>
    </author>
    <author>
      <name>Jianchao Yang</name>
    </author>
    <author>
      <name>Zhe Lin</name>
    </author>
    <author>
      <name>Thomas Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.6186v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.6186v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.5206v1</id>
    <updated>2014-05-20T19:52:43Z</updated>
    <published>2014-05-20T19:52:43Z</published>
    <title>Application of Multilayer Feedforward Neural Networks in Predicting Tree
  Height and Forest Stock Volume of Chinese Fir</title>
    <summary>  Wood increment is critical information in forestry management. Previous
studies used mathematics models to describe complex growing pattern of forest
stand, in order to determine the dynamic status of growing forest stand in
multiple conditions. In our research, we aimed at studying non-linear
relationships to establish precise and robust Artificial Neural Networks (ANN)
models to predict the precise values of tree height and forest stock volume
based on data of Chinese fir. Results show that Multilayer Feedforward Neural
Networks with 4 nodes (MLFN-4) can predict the tree height with the lowest RMS
error (1.77); Multilayer Feedforward Neural Networks with 7 nodes (MLFN-7) can
predict the forest stock volume with the lowest RMS error (4.95). The training
and testing process have proved that our models are precise and robust.
</summary>
    <author>
      <name>Xiaohui Huang</name>
    </author>
    <author>
      <name>Xing Hu</name>
    </author>
    <author>
      <name>Weichang Jiang</name>
    </author>
    <author>
      <name>Zhi Yang</name>
    </author>
    <author>
      <name>Hao Li</name>
    </author>
    <link href="http://arxiv.org/abs/1405.5206v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.5206v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.7063v5</id>
    <updated>2015-04-15T20:07:50Z</updated>
    <published>2014-12-22T17:19:56Z</published>
    <title>Diverse Embedding Neural Network Language Models</title>
    <summary>  We propose Diverse Embedding Neural Network (DENN), a novel architecture for
language models (LMs). A DENNLM projects the input word history vector onto
multiple diverse low-dimensional sub-spaces instead of a single
higher-dimensional sub-space as in conventional feed-forward neural network
LMs. We encourage these sub-spaces to be diverse during network training
through an augmented loss function. Our language modeling experiments on the
Penn Treebank data set show the performance benefit of using a DENNLM.
</summary>
    <author>
      <name>Kartik Audhkhasi</name>
    </author>
    <author>
      <name>Abhinav Sethy</name>
    </author>
    <author>
      <name>Bhuvana Ramabhadran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review as workshop contribution at ICLR 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.7063v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.7063v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.05700v2</id>
    <updated>2015-07-13T15:47:13Z</updated>
    <published>2015-02-19T20:51:27Z</published>
    <title>Scalable Bayesian Optimization Using Deep Neural Networks</title>
    <summary>  Bayesian optimization is an effective methodology for the global optimization
of functions with expensive evaluations. It relies on querying a distribution
over functions defined by a relatively cheap surrogate model. An accurate model
for this distribution over functions is critical to the effectiveness of the
approach, and is typically fit using Gaussian processes (GPs). However, since
GPs scale cubically with the number of observations, it has been challenging to
handle objectives whose optimization requires many evaluations, and as such,
massively parallelizing the optimization.
  In this work, we explore the use of neural networks as an alternative to GPs
to model distributions over functions. We show that performing adaptive basis
function regression with a neural network as the parametric form performs
competitively with state-of-the-art GP-based approaches, but scales linearly
with the number of data rather than cubically. This allows us to achieve a
previously intractable degree of parallelism, which we apply to large scale
hyperparameter optimization, rapidly finding competitive models on benchmark
object recognition tasks using convolutional networks, and image caption
generation using neural language models.
</summary>
    <author>
      <name>Jasper Snoek</name>
    </author>
    <author>
      <name>Oren Rippel</name>
    </author>
    <author>
      <name>Kevin Swersky</name>
    </author>
    <author>
      <name>Ryan Kiros</name>
    </author>
    <author>
      <name>Nadathur Satish</name>
    </author>
    <author>
      <name>Narayanan Sundaram</name>
    </author>
    <author>
      <name>Md. Mostofa Ali Patwary</name>
    </author>
    <author>
      <name> Prabhat</name>
    </author>
    <author>
      <name>Ryan P. Adams</name>
    </author>
    <link href="http://arxiv.org/abs/1502.05700v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.05700v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.07540v1</id>
    <updated>2015-02-26T13:18:09Z</updated>
    <published>2015-02-26T13:18:09Z</published>
    <title>A hypothesize-and-verify framework for Text Recognition using Deep
  Recurrent Neural Networks</title>
    <summary>  Deep LSTM is an ideal candidate for text recognition. However text
recognition involves some initial image processing steps like segmentation of
lines and words which can induce error to the recognition system. Without
segmentation, learning very long range context is difficult and becomes
computationally intractable. Therefore, alternative soft decisions are needed
at the pre-processing level. This paper proposes a hybrid text recognizer using
a deep recurrent neural network with multiple layers of abstraction and long
range context along with a language model to verify the performance of the deep
neural network. In this paper we construct a multi-hypotheses tree architecture
with candidate segments of line sequences from different segmentation
algorithms at its different branches. The deep neural network is trained on
perfectly segmented data and tests each of the candidate segments, generating
unicode sequences. In the verification step, these unicode sequences are
validated using a sub-string match with the language model and best first
search is used to find the best possible combination of alternative hypothesis
from the tree structure. Thus the verification framework using language models
eliminates wrong segmentation outputs and filters recognition errors.
</summary>
    <author>
      <name>Anupama Ray</name>
    </author>
    <author>
      <name>Sai Rajeswar</name>
    </author>
    <author>
      <name>Santanu Chaudhury</name>
    </author>
    <link href="http://arxiv.org/abs/1502.07540v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.07540v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.04788v1</id>
    <updated>2015-04-19T04:24:15Z</updated>
    <published>2015-04-19T04:24:15Z</published>
    <title>Compressing Neural Networks with the Hashing Trick</title>
    <summary>  As deep nets are increasingly used in applications suited for mobile devices,
a fundamental dilemma becomes apparent: the trend in deep learning is to grow
models to absorb ever-increasing data set sizes; however mobile devices are
designed with very little memory and cannot store such large models. We present
a novel network architecture, HashedNets, that exploits inherent redundancy in
neural networks to achieve drastic reductions in model sizes. HashedNets uses a
low-cost hash function to randomly group connection weights into hash buckets,
and all connections within the same hash bucket share a single parameter value.
These parameters are tuned to adjust to the HashedNets weight sharing
architecture with standard backprop during training. Our hashing procedure
introduces no additional memory overhead, and we demonstrate on several
benchmark data sets that HashedNets shrink the storage requirements of neural
networks substantially while mostly preserving generalization performance.
</summary>
    <author>
      <name>Wenlin Chen</name>
    </author>
    <author>
      <name>James T. Wilson</name>
    </author>
    <author>
      <name>Stephen Tyree</name>
    </author>
    <author>
      <name>Kilian Q. Weinberger</name>
    </author>
    <author>
      <name>Yixin Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1504.04788v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.04788v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.06678v1</id>
    <updated>2015-04-25T03:59:14Z</updated>
    <published>2015-04-25T03:59:14Z</published>
    <title>Differential Recurrent Neural Networks for Action Recognition</title>
    <summary>  The long short-term memory (LSTM) neural network is capable of processing
complex sequential information since it utilizes special gating schemes for
learning representations from long input sequences. It has the potential to
model any sequential time-series data, where the current hidden state has to be
considered in the context of the past hidden states. This property makes LSTM
an ideal choice to learn the complex dynamics of various actions.
Unfortunately, the conventional LSTMs do not consider the impact of
spatio-temporal dynamics corresponding to the given salient motion patterns,
when they gate the information that ought to be memorized through time. To
address this problem, we propose a differential gating scheme for the LSTM
neural network, which emphasizes on the change in information gain caused by
the salient motions between the successive frames. This change in information
gain is quantified by Derivative of States (DoS), and thus the proposed LSTM
model is termed as differential Recurrent Neural Network (dRNN). We demonstrate
the effectiveness of the proposed model by automatically recognizing actions
from the real-world 2D and 3D human action datasets. Our study is one of the
first works towards demonstrating the potential of learning complex time-series
representations via high-order derivatives of states.
</summary>
    <author>
      <name>Vivek Veeriah</name>
    </author>
    <author>
      <name>Naifan Zhuang</name>
    </author>
    <author>
      <name>Guo-Jun Qi</name>
    </author>
    <link href="http://arxiv.org/abs/1504.06678v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.06678v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.04304v3</id>
    <updated>2016-09-23T20:47:55Z</updated>
    <published>2015-06-13T18:23:42Z</published>
    <title>Combinatorial Energy Learning for Image Segmentation</title>
    <summary>  We introduce a new machine learning approach for image segmentation that uses
a neural network to model the conditional energy of a segmentation given an
image. Our approach, combinatorial energy learning for image segmentation
(CELIS) places a particular emphasis on modeling the inherent combinatorial
nature of dense image segmentation problems. We propose efficient algorithms
for learning deep neural networks to model the energy function, and for local
optimization of this energy in the space of supervoxel agglomerations. We
extensively evaluate our method on a publicly available 3-D microscopy dataset
with 25 billion voxels of ground truth data. On an 11 billion voxel test set,
we find that our method improves volumetric reconstruction accuracy by more
than 20% as compared to two state-of-the-art baseline methods: graph-based
segmentation of the output of a 3-D convolutional neural network trained to
predict boundaries, as well as a random forest classifier trained to
agglomerate supervoxels that were generated by a 3-D convolutional neural
network.
</summary>
    <author>
      <name>Jeremy Maitin-Shepard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UC Berkeley</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Google</arxiv:affiliation>
    </author>
    <author>
      <name>Viren Jain</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Google</arxiv:affiliation>
    </author>
    <author>
      <name>Michal Januszewski</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Google</arxiv:affiliation>
    </author>
    <author>
      <name>Peter Li</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Google</arxiv:affiliation>
    </author>
    <author>
      <name>Pieter Abbeel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UC Berkeley</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1506.04304v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.04304v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.04701v3</id>
    <updated>2015-06-22T16:22:23Z</updated>
    <published>2015-06-15T18:51:35Z</published>
    <title>Multi-path Convolutional Neural Networks for Complex Image
  Classification</title>
    <summary>  Convolutional Neural Networks demonstrate high performance on ImageNet
Large-Scale Visual Recognition Challenges contest. Nevertheless, the published
results only show the overall performance for all image classes. There is no
further analysis why certain images get worse results and how they could be
improved. In this paper, we provide deep performance analysis based on
different types of images and point out the weaknesses of convolutional neural
networks through experiment. We design a novel multiple paths convolutional
neural network, which feeds different versions of images into separated paths
to learn more comprehensive features. This model has better presentation for
image than the traditional single path model. We acquire better classification
results on complex validation set on both top 1 and top 5 scores than the best
ILSVRC 2013 classification model.
</summary>
    <author>
      <name>Mingming Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1506.04701v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.04701v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.08995v2</id>
    <updated>2015-07-22T09:09:44Z</updated>
    <published>2015-06-30T09:04:25Z</published>
    <title>The complexity of dynamics in small neural circuits</title>
    <summary>  Mean-field theory is a powerful tool for studying large neural networks.
However, when the system is composed of a few neurons, macroscopic differences
between the mean-field approximation and the real behavior of the network can
arise. Here we introduce a study of the dynamics of a small firing-rate network
with excitatory and inhibitory populations, in terms of local and global
bifurcations of the neural activity. Our approach is analytically tractable in
many respects, and sheds new light on the finite-size effects of the system. In
particular, we focus on the formation of multiple branching solutions of the
neural equations through spontaneous symmetry-breaking, since this phenomenon
increases considerably the complexity of the dynamical behavior of the network.
For these reasons, branching points may reveal important mechanisms through
which neurons interact and process information, which are not accounted for by
the mean-field approximation.
</summary>
    <author>
      <name>Diego Fasoli</name>
    </author>
    <author>
      <name>Anna Cattani</name>
    </author>
    <author>
      <name>Stefano Panzeri</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pcbi.1004992</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pcbi.1004992" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages, 11 figures. Supplementary materials added, colors of
  figures 8 and 9 fixed, results unchanged</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.08995v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.08995v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.01053v1</id>
    <updated>2015-07-04T01:06:16Z</updated>
    <published>2015-07-04T01:06:16Z</published>
    <title>Describing Multimedia Content using Attention-based Encoder--Decoder
  Networks</title>
    <summary>  Whereas deep neural networks were first mostly used for classification tasks,
they are rapidly expanding in the realm of structured output problems, where
the observed target is composed of multiple random variables that have a rich
joint distribution, given the input. We focus in this paper on the case where
the input also has a rich structure and the input and output structures are
somehow related. We describe systems that learn to attend to different places
in the input, for each element of the output, for a variety of tasks: machine
translation, image caption generation, video clip description and speech
recognition. All these systems are based on a shared set of building blocks:
gated recurrent neural networks and convolutional neural networks, along with
trained attention mechanisms. We report on experimental results with these
systems, showing impressively good performance and the advantage of the
attention mechanism.
</summary>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <author>
      <name>Aaron Courville</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TMM.2015.2477044</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TMM.2015.2477044" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE Transactions on Multimedia Special Issue on Deep
  Learning for Multimedia Computing</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.01053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.01053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.05331v1</id>
    <updated>2015-07-19T20:30:10Z</updated>
    <published>2015-07-19T20:30:10Z</published>
    <title>Fast Adaptive Weight Noise</title>
    <summary>  Marginalising out uncertain quantities within the internal representations or
parameters of neural networks is of central importance for a wide range of
learning techniques, such as empirical, variational or full Bayesian methods.
We set out to generalise fast dropout (Wang &amp; Manning, 2013) to cover a wider
variety of noise processes in neural networks. This leads to an efficient
calculation of the marginal likelihood and predictive distribution which evades
sampling and the consequential increase in training time due to highly variant
gradient estimates. This allows us to approximate variational Bayes for the
parameters of feed-forward neural networks. Inspired by the minimum description
length principle, we also propose and experimentally verify the direct
optimisation of the regularised predictive distribution. The methods yield
results competitive with previous neural network based approaches and Gaussian
processes on a wide range of regression tasks.
</summary>
    <author>
      <name>Justin Bayer</name>
    </author>
    <author>
      <name>Maximilian Karl</name>
    </author>
    <author>
      <name>Daniela Korhammer</name>
    </author>
    <author>
      <name>Patrick van der Smagt</name>
    </author>
    <link href="http://arxiv.org/abs/1507.05331v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.05331v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.08750v2</id>
    <updated>2015-12-22T04:26:54Z</updated>
    <published>2015-07-31T04:43:30Z</published>
    <title>Action-Conditional Video Prediction using Deep Networks in Atari Games</title>
    <summary>  Motivated by vision-based reinforcement learning (RL) problems, in particular
Atari games from the recent benchmark Aracade Learning Environment (ALE), we
consider spatio-temporal prediction problems where future (image-)frames are
dependent on control variables or actions as well as previous frames. While not
composed of natural scenes, frames in Atari games are high-dimensional in size,
can involve tens of objects with one or more objects being controlled by the
actions directly and many other objects being influenced indirectly, can
involve entry and departure of objects, and can involve deep partial
observability. We propose and evaluate two deep neural network architectures
that consist of encoding, action-conditional transformation, and decoding
layers based on convolutional neural networks and recurrent neural networks.
Experimental results show that the proposed architectures are able to generate
visually-realistic frames that are also useful for control over approximately
100-step action-conditional futures in some games. To the best of our
knowledge, this paper is the first to make and evaluate long-term predictions
on high-dimensional video conditioned by control inputs.
</summary>
    <author>
      <name>Junhyuk Oh</name>
    </author>
    <author>
      <name>Xiaoxiao Guo</name>
    </author>
    <author>
      <name>Honglak Lee</name>
    </author>
    <author>
      <name>Richard Lewis</name>
    </author>
    <author>
      <name>Satinder Singh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at NIPS 2015 (Advances in Neural Information Processing
  Systems 28)</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.08750v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.08750v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.00021v2</id>
    <updated>2015-09-21T15:09:35Z</updated>
    <published>2015-07-31T20:24:20Z</published>
    <title>Artificial Neural Networks Applied to Taxi Destination Prediction</title>
    <summary>  We describe our first-place solution to the ECML/PKDD discovery challenge on
taxi destination prediction. The task consisted in predicting the destination
of a taxi based on the beginning of its trajectory, represented as a
variable-length sequence of GPS points, and diverse associated
meta-information, such as the departure time, the driver id and client
information. Contrary to most published competitor approaches, we used an
almost fully automated approach based on neural networks and we ranked first
out of 381 teams. The architectures we tried use multi-layer perceptrons,
bidirectional recurrent neural networks and models inspired from recently
introduced memory networks. Our approach could easily be adapted to other
applications in which the goal is to predict a fixed-length output from a
variable-length sequence.
</summary>
    <author>
      <name>Alexandre de Br√©bisson</name>
    </author>
    <author>
      <name>√âtienne Simon</name>
    </author>
    <author>
      <name>Alex Auvolat</name>
    </author>
    <author>
      <name>Pascal Vincent</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECML/PKDD discovery challenge</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.00021v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.00021v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.00818v1</id>
    <updated>2015-08-04T16:28:07Z</updated>
    <published>2015-08-04T16:28:07Z</published>
    <title>Pseudo almost periodic solutions for neutral type high-order Hopfield
  neural networks with mixed time-varying delays and leakage delays on time
  scales</title>
    <summary>  In this paper, a class of neutral type high-order Hopfield neural networks
with mixed time-varying delays and leakage delays on time scales is proposed.
Based on the exponential dichotomy of linear dynamic equations on time scales,
Banach's fixed point theorem and the theory of calculus on time scales, some
sufficient conditions are obtained for the existence and global exponential
stability of pseudo almost periodic solutions for this class of neural
networks. Our results are completely new. Finally, we present an example to
illustrate our results are effective. Our example also shows that the
continuous-time neural network and its discrete-time analogue have the same
dynamical behaviors for the pseudo almost periodicity.
</summary>
    <author>
      <name>Yongkun Li</name>
    </author>
    <author>
      <name>Xiaofang Meng</name>
    </author>
    <author>
      <name>Lianglin Xiong</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s13042-016-0570-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s13042-016-0570-7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.00818v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.00818v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.09199v1</id>
    <updated>2015-09-30T14:46:44Z</updated>
    <published>2015-09-30T14:46:44Z</published>
    <title>Fault Tolerance in Distributed Neural Computing</title>
    <summary>  With the increasing complexity of computing systems, complete hardware
reliability can no longer be guaranteed. We need, however, to ensure overall
system reliability. One of the most important features of artificial neural
networks is their intrinsic fault-tolerance. The aim of this work is to
investigate whether such networks have features that can be applied to wider
computational systems. This paper presents an analysis, in both the learning
and operational phases, of a distributed feed-forward neural network with
decentralised event-driven time management, which is insensitive to
intermittent faults caused by unreliable communication or faulty hardware
components. The learning rules used in the model are local in space and time,
which allows efficient scalable distributed implementation. We investigate the
overhead caused by injected faults and analyse the sensitivity to limited
failures in the computational hardware in different areas of the network.
</summary>
    <author>
      <name>Anton Kulakov</name>
    </author>
    <author>
      <name>Mark Zwolinski</name>
    </author>
    <author>
      <name>Jeff Reeve</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.1.1387.0800</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.1.1387.0800" rel="related"/>
    <link href="http://arxiv.org/abs/1509.09199v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.09199v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.05970v2</id>
    <updated>2016-05-18T19:53:41Z</updated>
    <published>2015-10-20T17:15:05Z</published>
    <title>Stereo Matching by Training a Convolutional Neural Network to Compare
  Image Patches</title>
    <summary>  We present a method for extracting depth information from a rectified image
pair. Our approach focuses on the first stage of many stereo algorithms: the
matching cost computation. We approach the problem by learning a similarity
measure on small image patches using a convolutional neural network. Training
is carried out in a supervised manner by constructing a binary classification
data set with examples of similar and dissimilar pairs of patches. We examine
two network architectures for this task: one tuned for speed, the other for
accuracy. The output of the convolutional neural network is used to initialize
the stereo matching cost. A series of post-processing steps follow: cross-based
cost aggregation, semiglobal matching, a left-right consistency check, subpixel
enhancement, a median filter, and a bilateral filter. We evaluate our method on
the KITTI 2012, KITTI 2015, and Middlebury stereo data sets and show that it
outperforms other approaches on all three data sets.
</summary>
    <author>
      <name>Jure ≈Ωbontar</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">JMLR 17(65):1-32, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1510.05970v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.05970v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.01427v1</id>
    <updated>2015-11-04T18:40:46Z</updated>
    <published>2015-11-04T18:40:46Z</published>
    <title>Turing Computation with Recurrent Artificial Neural Networks</title>
    <summary>  We improve the results by Siegelmann &amp; Sontag (1995) by providing a novel and
parsimonious constructive mapping between Turing Machines and Recurrent
Artificial Neural Networks, based on recent developments of Nonlinear Dynamical
Automata. The architecture of the resulting R-ANNs is simple and elegant,
stemming from its transparent relation with the underlying NDAs. These
characteristics yield promise for developments in machine learning methods and
symbolic computation with continuous time dynamical systems. A framework is
provided to directly program the R-ANNs from Turing Machine descriptions, in
absence of network training. At the same time, the network can potentially be
trained to perform algorithmic tasks, with exciting possibilities in the
integration of approaches akin to Google DeepMind's Neural Turing Machines.
</summary>
    <author>
      <name>Giovanni S Carmantini</name>
    </author>
    <author>
      <name>Peter beim Graben</name>
    </author>
    <author>
      <name>Mathieu Desroches</name>
    </author>
    <author>
      <name>Serafim Rodrigues</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.01427v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.01427v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.01813v1</id>
    <updated>2015-11-05T16:51:54Z</updated>
    <published>2015-11-05T16:51:54Z</published>
    <title>Percolation and Random Walks</title>
    <summary>  The fourfold research proposal regards in particular: critical oriented
percolation; random walk limit laws; neural networks with long-range
connections; the ant in a labyrinth.
</summary>
    <author>
      <name>Achillefs Tzioufas</name>
    </author>
    <link href="http://arxiv.org/abs/1511.01813v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.01813v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.05493v4</id>
    <updated>2017-09-22T21:36:00Z</updated>
    <published>2015-11-17T18:10:12Z</published>
    <title>Gated Graph Sequence Neural Networks</title>
    <summary>  Graph-structured data appears frequently in domains including chemistry,
natural language semantics, social networks, and knowledge bases. In this work,
we study feature learning techniques for graph-structured inputs. Our starting
point is previous work on Graph Neural Networks (Scarselli et al., 2009), which
we modify to use gated recurrent units and modern optimization techniques and
then extend to output sequences. The result is a flexible and broadly useful
class of neural network models that has favorable inductive biases relative to
purely sequence-based models (e.g., LSTMs) when the problem is
graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and
graph algorithm learning tasks. We then show it achieves state-of-the-art
performance on a problem from program verification, in which subgraphs need to
be matched to abstract data structures.
</summary>
    <author>
      <name>Yujia Li</name>
    </author>
    <author>
      <name>Daniel Tarlow</name>
    </author>
    <author>
      <name>Marc Brockschmidt</name>
    </author>
    <author>
      <name>Richard Zemel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper in ICLR 2016. Fixed a typo</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.05493v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.05493v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.03965v4</id>
    <updated>2016-05-09T02:16:54Z</updated>
    <published>2015-12-12T21:41:24Z</published>
    <title>The Power of Depth for Feedforward Neural Networks</title>
    <summary>  We show that there is a simple (approximately radial) function on $\reals^d$,
expressible by a small 3-layer feedforward neural networks, which cannot be
approximated by any 2-layer network, to more than a certain constant accuracy,
unless its width is exponential in the dimension. The result holds for
virtually all known activation functions, including rectified linear units,
sigmoids and thresholds, and formally demonstrates that depth -- even if
increased by 1 -- can be exponentially more valuable than width for standard
feedforward neural networks. Moreover, compared to related results in the
context of Boolean functions, our result requires fewer assumptions, and the
proof techniques and construction are very different.
</summary>
    <author>
      <name>Ronen Eldan</name>
    </author>
    <author>
      <name>Ohad Shamir</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to COLT 2016; Fixed a bug in the proof of claim 2 (now
  requiring the mild assumption that the activations are polynomially bounded);
  Other minor revisions</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.03965v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.03965v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.04280v4</id>
    <updated>2017-06-14T15:17:27Z</updated>
    <published>2015-12-14T12:29:32Z</published>
    <title>Small-footprint Deep Neural Networks with Highway Connections for Speech
  Recognition</title>
    <summary>  For speech recognition, deep neural networks (DNNs) have significantly
improved the recognition accuracy in most of benchmark datasets and application
domains. However, compared to the conventional Gaussian mixture models,
DNN-based acoustic models usually have much larger number of model parameters,
making it challenging for their applications in resource constrained platforms,
e.g., mobile devices. In this paper, we study the application of the recently
proposed highway network to train small-footprint DNNs, which are {\it thinner}
and {\it deeper}, and have significantly smaller number of model parameters
compared to conventional DNNs. We investigated this approach on the AMI meeting
speech transcription corpus which has around 70 hours of audio data. The
highway neural networks constantly outperformed their plain DNN counterparts,
and the number of model parameters can be reduced significantly without
sacrificing the recognition accuracy.
</summary>
    <author>
      <name>Liang Lu</name>
    </author>
    <author>
      <name>Steve Renals</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures, fixed typo, accepted by Interspeech 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.04280v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.04280v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.01100v1</id>
    <updated>2016-01-06T07:36:15Z</updated>
    <published>2016-01-06T07:36:15Z</published>
    <title>Memory Matters: Convolutional Recurrent Neural Network for Scene Text
  Recognition</title>
    <summary>  Text recognition in natural scene is a challenging problem due to the many
factors affecting text appearance. In this paper, we presents a method that
directly transcribes scene text images to text without needing of sophisticated
character segmentation. We leverage recent advances of deep neural networks to
model the appearance of scene text images with temporal dynamics. Specifically,
we integrates convolutional neural network (CNN) and recurrent neural network
(RNN) which is motivated by observing the complementary modeling capabilities
of the two models. The main contribution of this work is investigating how
temporal memory helps in an segmentation free fashion for this specific
problem. By using long short-term memory (LSTM) blocks as hidden units, our
model can retain long-term memory compared with HMMs which only maintain
short-term state dependences. We conduct experiments on Street View House
Number dataset containing highly variable number images. The results
demonstrate the superiority of the proposed method over traditional HMM based
methods.
</summary>
    <author>
      <name>Guo Qiang</name>
    </author>
    <author>
      <name>Tu Dan</name>
    </author>
    <author>
      <name>Li Guohui</name>
    </author>
    <author>
      <name>Lei Jun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.01100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.01100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.04902v1</id>
    <updated>2016-01-19T13:06:16Z</updated>
    <published>2016-01-19T13:06:16Z</published>
    <title>PupilNet: Convolutional Neural Networks for Robust Pupil Detection</title>
    <summary>  Real-time, accurate, and robust pupil detection is an essential prerequisite
for pervasive video-based eye-tracking. However, automated pupil detection in
real-world scenarios has proven to be an intricate challenge due to fast
illumination changes, pupil occlusion, non centered and off-axis eye recording,
and physiological eye characteristics. In this paper, we propose and evaluate a
method based on a novel dual convolutional neural network pipeline. In its
first stage the pipeline performs coarse pupil position identification using a
convolutional neural network and subregions from a downscaled input image to
decrease computational costs. Using subregions derived from a small window
around the initial pupil position estimate, the second pipeline stage employs
another convolutional neural network to refine this position, resulting in an
increased pupil detection rate up to 25% in comparison with the best performing
state-of-the-art algorithm. Annotated data sets can be made available upon
request.
</summary>
    <author>
      <name>Wolfgang Fuhl</name>
    </author>
    <author>
      <name>Thiago Santini</name>
    </author>
    <author>
      <name>Gjergji Kasneci</name>
    </author>
    <author>
      <name>Enkelejda Kasneci</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.04902v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.04902v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.02660v2</id>
    <updated>2016-05-26T11:47:18Z</updated>
    <published>2016-02-08T17:37:16Z</published>
    <title>Exploiting Cyclic Symmetry in Convolutional Neural Networks</title>
    <summary>  Many classes of images exhibit rotational symmetry. Convolutional neural
networks are sometimes trained using data augmentation to exploit this, but
they are still required to learn the rotation equivariance properties from the
data. Encoding these properties into the network architecture, as we are
already used to doing for translation equivariance by using convolutional
layers, could result in a more efficient use of the parameter budget by
relieving the model from learning them. We introduce four operations which can
be inserted into neural network models as layers, and which can be combined to
make these models partially equivariant to rotations. They also enable
parameter sharing across different orientations. We evaluate the effect of
these architectural modifications on three datasets which exhibit rotational
symmetry and demonstrate improved performance with smaller models.
</summary>
    <author>
      <name>Sander Dieleman</name>
    </author>
    <author>
      <name>Jeffrey De Fauw</name>
    </author>
    <author>
      <name>Koray Kavukcuoglu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures, accepted for publication at ICML 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.02660v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.02660v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.02830v3</id>
    <updated>2016-03-17T14:54:25Z</updated>
    <published>2016-02-09T01:01:59Z</published>
    <title>Binarized Neural Networks: Training Deep Neural Networks with Weights
  and Activations Constrained to +1 or -1</title>
    <summary>  We introduce a method to train Binarized Neural Networks (BNNs) - neural
networks with binary weights and activations at run-time. At training-time the
binary weights and activations are used for computing the parameters gradients.
During the forward pass, BNNs drastically reduce memory size and accesses, and
replace most arithmetic operations with bit-wise operations, which is expected
to substantially improve power-efficiency. To validate the effectiveness of
BNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On
both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10
and SVHN datasets. Last but not least, we wrote a binary matrix multiplication
GPU kernel with which it is possible to run our MNIST BNN 7 times faster than
with an unoptimized GPU kernel, without suffering any loss in classification
accuracy. The code for training and running our BNNs is available on-line.
</summary>
    <author>
      <name>Matthieu Courbariaux</name>
    </author>
    <author>
      <name>Itay Hubara</name>
    </author>
    <author>
      <name>Daniel Soudry</name>
    </author>
    <author>
      <name>Ran El-Yaniv</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages and 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.02830v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.02830v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.02867v4</id>
    <updated>2017-03-20T21:41:51Z</updated>
    <published>2016-02-09T05:44:36Z</published>
    <title>Value Iteration Networks</title>
    <summary>  We introduce the value iteration network (VIN): a fully differentiable neural
network with a `planning module' embedded within. VINs can learn to plan, and
are suitable for predicting outcomes that involve planning-based reasoning,
such as policies for reinforcement learning. Key to our approach is a novel
differentiable approximation of the value-iteration algorithm, which can be
represented as a convolutional neural network, and trained end-to-end using
standard backpropagation. We evaluate VIN based policies on discrete and
continuous path-planning domains, and on a natural-language based search task.
We show that by learning an explicit planning computation, VIN policies
generalize better to new, unseen domains.
</summary>
    <author>
      <name>Aviv Tamar</name>
    </author>
    <author>
      <name>Yi Wu</name>
    </author>
    <author>
      <name>Garrett Thomas</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
    <author>
      <name>Pieter Abbeel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fixed missing table values</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in Neural Information Processing Systems 29 pages
  2154--2162, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.02867v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.02867v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08332v2</id>
    <updated>2016-05-23T15:51:07Z</updated>
    <published>2016-02-26T14:15:03Z</published>
    <title>Bounded Rational Decision-Making in Feedforward Neural Networks</title>
    <summary>  Bounded rational decision-makers transform sensory input into motor output
under limited computational resources. Mathematically, such decision-makers can
be modeled as information-theoretic channels with limited transmission rate.
Here, we apply this formalism for the first time to multilayer feedforward
neural networks. We derive synaptic weight update rules for two scenarios,
where either each neuron is considered as a bounded rational decision-maker or
the network as a whole. In the update rules, bounded rationality translates
into information-theoretically motivated types of regularization in weight
space. In experiments on the MNIST benchmark classification task for
handwritten digits, we show that such information-theoretic regularization
successfully prevents overfitting across different architectures and attains
results that are competitive with other recent techniques like dropout,
dropconnect and Bayes by backprop, for both ordinary and convolutional neural
networks.
</summary>
    <author>
      <name>Felix Leibfried</name>
    </author>
    <author>
      <name>Daniel Alexander Braun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 32nd Conference on Uncertainty in Artificial
  Intelligence (UAI), New York City, NY, USA, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.08332v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08332v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.00806v3</id>
    <updated>2016-07-19T08:10:08Z</updated>
    <published>2016-03-02T17:48:25Z</published>
    <title>Hybrid Collaborative Filtering with Autoencoders</title>
    <summary>  Collaborative Filtering aims at exploiting the feedback of users to provide
personalised recommendations. Such algorithms look for latent variables in a
large sparse matrix of ratings. They can be enhanced by adding side information
to tackle the well-known cold start problem. While Neu-ral Networks have
tremendous success in image and speech recognition, they have received less
attention in Collaborative Filtering. This is all the more surprising that
Neural Networks are able to discover latent variables in large and
heterogeneous datasets. In this paper, we introduce a Collaborative Filtering
Neural network architecture aka CFN which computes a non-linear Matrix
Factorization from sparse rating inputs and side information. We show
experimentally on the MovieLens and Douban dataset that CFN outper-forms the
state of the art and benefits from side information. We provide an
implementation of the algorithm as a reusable plugin for Torch, a popular
Neural Network framework.
</summary>
    <author>
      <name>Florian Strub</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SEQUEL, CRIStAL</arxiv:affiliation>
    </author>
    <author>
      <name>Jeremie Mary</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRIStAL, SEQUEL</arxiv:affiliation>
    </author>
    <author>
      <name>Romaric Gaudel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIFL</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1603.00806v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.00806v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09727v1</id>
    <updated>2016-03-31T19:16:54Z</updated>
    <published>2016-03-31T19:16:54Z</published>
    <title>Neural Language Correction with Character-Based Attention</title>
    <summary>  Natural language correction has the potential to help language learners
improve their writing skills. While approaches with separate classifiers for
different error types have high precision, they do not flexibly handle errors
such as redundancy or non-idiomatic phrasing. On the other hand, word and
phrase-based machine translation methods are not designed to cope with
orthographic errors, and have recently been outpaced by neural models.
Motivated by these issues, we present a neural network-based approach to
language correction. The core component of our method is an encoder-decoder
recurrent neural network with an attention mechanism. By operating at the
character level, the network avoids the problem of out-of-vocabulary words. We
illustrate the flexibility of our approach on dataset of noisy, user-generated
text collected from an English learner forum. When combined with a language
model, our method achieves a state-of-the-art $F_{0.5}$-score on the CoNLL 2014
Shared Task. We further demonstrate that training the network on additional
data with synthesized errors can improve performance.
</summary>
    <author>
      <name>Ziang Xie</name>
    </author>
    <author>
      <name>Anand Avati</name>
    </author>
    <author>
      <name>Naveen Arivazhagan</name>
    </author>
    <author>
      <name>Dan Jurafsky</name>
    </author>
    <author>
      <name>Andrew Y. Ng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.09727v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09727v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00825v1</id>
    <updated>2016-04-04T11:52:07Z</updated>
    <published>2016-04-04T11:52:07Z</published>
    <title>Layer-wise Relevance Propagation for Neural Networks with Local
  Renormalization Layers</title>
    <summary>  Layer-wise relevance propagation is a framework which allows to decompose the
prediction of a deep neural network computed over a sample, e.g. an image, down
to relevance scores for the single input dimensions of the sample such as
subpixels of an image. While this approach can be applied directly to
generalized linear mappings, product type non-linearities are not covered. This
paper proposes an approach to extend layer-wise relevance propagation to neural
networks with local renormalization layers, which is a very common product-type
non-linearity in convolutional neural networks. We evaluate the proposed method
for local renormalization layers on the CIFAR-10, Imagenet and MIT Places
datasets.
</summary>
    <author>
      <name>Alexander Binder</name>
    </author>
    <author>
      <name>Gr√©goire Montavon</name>
    </author>
    <author>
      <name>Sebastian Bach</name>
    </author>
    <author>
      <name>Klaus-Robert M√ºller</name>
    </author>
    <author>
      <name>Wojciech Samek</name>
    </author>
    <link href="http://arxiv.org/abs/1604.00825v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00825v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06737v1</id>
    <updated>2016-04-22T16:34:30Z</updated>
    <published>2016-04-22T16:34:30Z</published>
    <title>Entity Embeddings of Categorical Variables</title>
    <summary>  We map categorical variables in a function approximation problem into
Euclidean spaces, which are the entity embeddings of the categorical variables.
The mapping is learned by a neural network during the standard supervised
training process. Entity embedding not only reduces memory usage and speeds up
neural networks compared with one-hot encoding, but more importantly by mapping
similar values close to each other in the embedding space it reveals the
intrinsic properties of the categorical variables. We applied it successfully
in a recent Kaggle competition and were able to reach the third position with
relative simple features. We further demonstrate in this paper that entity
embedding helps the neural network to generalize better when the data is sparse
and statistics is unknown. Thus it is especially useful for datasets with lots
of high cardinality features, where other methods tend to overfit. We also
demonstrate that the embeddings obtained from the trained neural network boost
the performance of all tested machine learning methods considerably when used
as the input features instead. As entity embedding defines a distance measure
for categorical variables it can be used for visualizing categorical data and
for data clustering.
</summary>
    <author>
      <name>Cheng Guo</name>
    </author>
    <author>
      <name>Felix Berkhahn</name>
    </author>
    <link href="http://arxiv.org/abs/1604.06737v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06737v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.00329v1</id>
    <updated>2016-05-02T01:37:57Z</updated>
    <published>2016-05-02T01:37:57Z</published>
    <title>Some Insights into the Geometry and Training of Neural Networks</title>
    <summary>  Neural networks have been successfully used for classification tasks in a
rapidly growing number of practical applications. Despite their popularity and
widespread use, there are still many aspects of training and classification
that are not well understood. In this paper we aim to provide some new insights
into training and classification by analyzing neural networks from a
feature-space perspective. We review and explain the formation of decision
regions and study some of their combinatorial aspects. We place a particular
emphasis on the connections between the neural network weight and bias terms
and properties of decision boundaries and other regions that exhibit varying
levels of classification confidence. We show how the error backpropagates in
these regions and emphasize the important role they have in the formation of
gradients. These findings expose the connections between scaling of the weight
parameters and the density of the training samples. This sheds more light on
the vanishing gradient problem, explains the need for regularization, and
suggests an approach for subsampling training data to improve performance.
</summary>
    <author>
      <name>Ewout van den Berg</name>
    </author>
    <link href="http://arxiv.org/abs/1605.00329v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.00329v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01735v1</id>
    <updated>2016-05-05T20:00:04Z</updated>
    <published>2016-05-05T20:00:04Z</published>
    <title>Machine learning phases of matter</title>
    <summary>  Neural networks can be used to identify phases and phase transitions in
condensed matter systems via supervised machine learning. Readily programmable
through modern software libraries, we show that a standard feed-forward neural
network can be trained to detect multiple types of order parameter directly
from raw state configurations sampled with Monte Carlo. In addition, they can
detect highly non-trivial states such as Coulomb phases, and if modified to a
convolutional neural network, topological phases with no conventional order
parameter. We show that this classification occurs within the neural network
without knowledge of the Hamiltonian or even the general locality of
interactions. These results demonstrate the power of machine learning as a
basic research tool in the field of condensed matter and statistical physics.
</summary>
    <author>
      <name>Juan Carrasquilla</name>
    </author>
    <author>
      <name>Roger G. Melko</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/nphys4035</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/nphys4035" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 8 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.01735v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01735v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.str-el" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.str-el" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05216v1</id>
    <updated>2016-05-17T15:45:08Z</updated>
    <published>2016-05-17T15:45:08Z</published>
    <title>Combinatorially Generated Piecewise Activation Functions</title>
    <summary>  In the neuroevolution literature, research has primarily focused on evolving
the number of nodes, connections, and weights in artificial neural networks.
Few attempts have been made to evolve activation functions. Research in
evolving activation functions has mainly focused on evolving function
parameters, and developing heterogeneous networks by selecting from a fixed
pool of activation functions. This paper introduces a novel technique for
evolving heterogeneous artificial neural networks through combinatorially
generating piecewise activation functions to enhance expressive power. I
demonstrate this technique on NeuroEvolution of Augmenting Topologies using
ArcTan and Sigmoid, and show that it outperforms the original algorithm on
non-Markovian double pole balancing. This technique expands the landscape of
unconventional activation functions by demonstrating that they are competitive
with canonical choices, and introduces a purview for further exploration of
automatic model selection for artificial neural networks.
</summary>
    <author>
      <name>Justin Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1605.05216v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05216v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06065v1</id>
    <updated>2016-05-19T17:44:51Z</updated>
    <published>2016-05-19T17:44:51Z</published>
    <title>One-shot Learning with Memory-Augmented Neural Networks</title>
    <summary>  Despite recent breakthroughs in the applications of deep neural networks, one
setting that presents a persistent challenge is that of "one-shot learning."
Traditional gradient-based networks require a lot of data to learn, often
through extensive iterative training. When new data is encountered, the models
must inefficiently relearn their parameters to adequately incorporate the new
information without catastrophic interference. Architectures with augmented
memory capacities, such as Neural Turing Machines (NTMs), offer the ability to
quickly encode and retrieve new information, and hence can potentially obviate
the downsides of conventional models. Here, we demonstrate the ability of a
memory-augmented neural network to rapidly assimilate new data, and leverage
this data to make accurate predictions after only a few samples. We also
introduce a new method for accessing an external memory that focuses on memory
content, unlike previous methods that additionally use memory location-based
focusing mechanisms.
</summary>
    <author>
      <name>Adam Santoro</name>
    </author>
    <author>
      <name>Sergey Bartunov</name>
    </author>
    <author>
      <name>Matthew Botvinick</name>
    </author>
    <author>
      <name>Daan Wierstra</name>
    </author>
    <author>
      <name>Timothy Lillicrap</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.06065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06437v1</id>
    <updated>2016-05-20T17:02:40Z</updated>
    <published>2016-05-20T17:02:40Z</published>
    <title>Learning shape correspondence with anisotropic convolutional neural
  networks</title>
    <summary>  Establishing correspondence between shapes is a fundamental problem in
geometry processing, arising in a wide variety of applications. The problem is
especially difficult in the setting of non-isometric deformations, as well as
in the presence of topological noise and missing parts, mainly due to the
limited capability to model such deformations axiomatically. Several recent
works showed that invariance to complex shape transformations can be learned
from examples. In this paper, we introduce an intrinsic convolutional neural
network architecture based on anisotropic diffusion kernels, which we term
Anisotropic Convolutional Neural Network (ACNN). In our construction, we
generalize convolutions to non-Euclidean domains by constructing a set of
oriented anisotropic diffusion kernels, creating in this way a local intrinsic
polar representation of the data (`patch'), which is then correlated with a
filter. Several cascades of such filters, linear, and non-linear operators are
stacked to form a deep neural network whose parameters are learned by
minimizing a task-specific cost. We use ACNNs to effectively learn intrinsic
dense correspondences between deformable shapes in very challenging settings,
achieving state-of-the-art results on some of the most difficult recent
correspondence benchmarks.
</summary>
    <author>
      <name>Davide Boscaini</name>
    </author>
    <author>
      <name>Jonathan Masci</name>
    </author>
    <author>
      <name>Emanuele Rodol√†</name>
    </author>
    <author>
      <name>Michael M. Bronstein</name>
    </author>
    <link href="http://arxiv.org/abs/1605.06437v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06437v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04750v1</id>
    <updated>2016-06-15T13:14:05Z</updated>
    <published>2016-06-15T13:14:05Z</published>
    <title>Multi-Modal Hybrid Deep Neural Network for Speech Enhancement</title>
    <summary>  Deep Neural Networks (DNN) have been successful in en- hancing noisy speech
signals. Enhancement is achieved by learning a nonlinear mapping function from
the features of the corrupted speech signal to that of the reference clean
speech signal. The quality of predicted features can be improved by providing
additional side channel information that is robust to noise, such as visual
cues. In this paper we propose a novel deep learning model inspired by insights
from human audio visual perception. In the proposed unified hybrid
architecture, features from a Convolution Neural Network (CNN) that processes
the visual cues and features from a fully connected DNN that processes the
audio signal are integrated using a Bidirectional Long Short-Term Memory
(BiLSTM) network. The parameters of the hybrid model are jointly learned using
backpropagation. We compare the quality of enhanced speech from the hybrid
models with those from traditional DNN and BiLSTM models.
</summary>
    <author>
      <name>Zhenzhou Wu</name>
    </author>
    <author>
      <name>Sunil Sivadas</name>
    </author>
    <author>
      <name>Yong Kiam Tan</name>
    </author>
    <author>
      <name>Ma Bin</name>
    </author>
    <author>
      <name>Rick Siow Mong Goh</name>
    </author>
    <link href="http://arxiv.org/abs/1606.04750v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04750v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05029v2</id>
    <updated>2017-07-28T15:28:01Z</updated>
    <published>2016-06-16T02:20:04Z</published>
    <title>No Need to Pay Attention: Simple Recurrent Neural Networks Work! (for
  Answering "Simple" Questions)</title>
    <summary>  First-order factoid question answering assumes that the question can be
answered by a single fact in a knowledge base (KB). While this does not seem
like a challenging task, many recent attempts that apply either complex
linguistic reasoning or deep neural networks achieve 65%-76% accuracy on
benchmark sets. Our approach formulates the task as two machine learning
problems: detecting the entities in the question, and classifying the question
as one of the relation types in the KB. We train a recurrent neural network to
solve each problem. On the SimpleQuestions dataset, our approach yields
substantial improvements over previously published results --- even neural
networks based on much more complex architectures. The simplicity of our
approach also has practical advantages, such as efficiency and modularity, that
are valuable especially in an industry setting. In fact, we present a
preliminary analysis of the performance of our model on real queries from
Comcast's X1 entertainment platform with millions of users every day.
</summary>
    <author>
      <name>Ferhan Ture</name>
    </author>
    <author>
      <name>Oliver Jojic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, to appear in EMNLP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.05029v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05029v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06329v2</id>
    <updated>2016-06-22T14:07:56Z</updated>
    <published>2016-06-20T20:56:47Z</published>
    <title>Recognizing Surgical Activities with Recurrent Neural Networks</title>
    <summary>  We apply recurrent neural networks to the task of recognizing surgical
activities from robot kinematics. Prior work in this area focuses on
recognizing short, low-level activities, or gestures, and has been based on
variants of hidden Markov models and conditional random fields. In contrast, we
work on recognizing both gestures and longer, higher-level activites, or
maneuvers, and we model the mapping from kinematics to gestures/maneuvers with
recurrent neural networks. To our knowledge, we are the first to apply
recurrent neural networks to this task. Using a single model and a single set
of hyperparameters, we match state-of-the-art performance for gesture
recognition and advance state-of-the-art performance for maneuver recognition,
in terms of both accuracy and edit distance. Code is available at
https://github.com/rdipietro/miccai-2016-surgical-activity-rec .
</summary>
    <author>
      <name>Robert DiPietro</name>
    </author>
    <author>
      <name>Colin Lea</name>
    </author>
    <author>
      <name>Anand Malpani</name>
    </author>
    <author>
      <name>Narges Ahmidi</name>
    </author>
    <author>
      <name>S. Swaroop Vedula</name>
    </author>
    <author>
      <name>Gyusung I. Lee</name>
    </author>
    <author>
      <name>Mija R. Lee</name>
    </author>
    <author>
      <name>Gregory D. Hager</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Conditionally accepted at MICCAI 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.06329v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06329v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07461v2</id>
    <updated>2017-10-30T15:11:54Z</updated>
    <published>2016-06-23T20:20:39Z</published>
    <title>LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in
  Recurrent Neural Networks</title>
    <summary>  Recurrent neural networks, and in particular long short-term memory (LSTM)
networks, are a remarkably effective tool for sequence modeling that learn a
dense black-box hidden representation of their sequential input. Researchers
interested in better understanding these models have studied the changes in
hidden state representations over time and noticed some interpretable patterns
but also significant noise. In this work, we present LSTMVIS, a visual analysis
tool for recurrent neural networks with a focus on understanding these hidden
state dynamics. The tool allows users to select a hypothesis input range to
focus on local state changes, to match these states changes to similar patterns
in a large data set, and to align these results with structural annotations
from their domain. We show several use cases of the tool for analyzing specific
hidden state properties on dataset containing nesting, phrase structure, and
chord progressions, and demonstrate how the tool can be used to isolate
patterns for further statistical analysis. We characterize the domain, the
different stakeholders, and their goals and tasks.
</summary>
    <author>
      <name>Hendrik Strobelt</name>
    </author>
    <author>
      <name>Sebastian Gehrmann</name>
    </author>
    <author>
      <name>Hanspeter Pfister</name>
    </author>
    <author>
      <name>Alexander M. Rush</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">InfoVis 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.07461v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07461v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07470v1</id>
    <updated>2016-06-23T20:37:06Z</updated>
    <published>2016-06-23T20:37:06Z</published>
    <title>NN-grams: Unifying neural network and n-gram language models for Speech
  Recognition</title>
    <summary>  We present NN-grams, a novel, hybrid language model integrating n-grams and
neural networks (NN) for speech recognition. The model takes as input both word
histories as well as n-gram counts. Thus, it combines the memorization capacity
and scalability of an n-gram model with the generalization ability of neural
networks. We report experiments where the model is trained on 26B words.
NN-grams are efficient at run-time since they do not include an output soft-max
layer. The model is trained using noise contrastive estimation (NCE), an
approach that transforms the estimation problem of neural networks into one of
binary classification between data samples and noise samples. We present
results with noise samples derived from either an n-gram distribution or from
speech recognition lattices. NN-grams outperforms an n-gram model on an Italian
speech recognition dictation task.
</summary>
    <author>
      <name>Babak Damavandi</name>
    </author>
    <author>
      <name>Shankar Kumar</name>
    </author>
    <author>
      <name>Noam Shazeer</name>
    </author>
    <author>
      <name>Antoine Bruguier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in the proceedings of INTERSPEECH 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.07470v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07470v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.01977v1</id>
    <updated>2016-07-07T12:01:59Z</updated>
    <published>2016-07-07T12:01:59Z</published>
    <title>Deep Depth Super-Resolution : Learning Depth Super-Resolution using Deep
  Convolutional Neural Network</title>
    <summary>  Depth image super-resolution is an extremely challenging task due to the
information loss in sub-sampling. Deep convolutional neural network have been
widely applied to color image super-resolution. Quite surprisingly, this
success has not been matched to depth super-resolution. This is mainly due to
the inherent difference between color and depth images. In this paper, we
bridge up the gap and extend the success of deep convolutional neural network
to depth super-resolution. The proposed deep depth super-resolution method
learns the mapping from a low-resolution depth image to a high resolution one
in an end-to-end style. Furthermore, to better regularize the learned depth
map, we propose to exploit the depth field statistics and the local correlation
between depth image and color image. These priors are integrated in an energy
minimization formulation, where the deep neural network learns the unary term,
the depth field statistics works as global model constraint and the color-depth
correlation is utilized to enforce the local structure in depth images.
Extensive experiments on various depth super-resolution benchmark datasets show
that our method outperforms the state-of-the-art depth image super-resolution
methods with a margin.
</summary>
    <author>
      <name>Xibin Song</name>
    </author>
    <author>
      <name>Yuchao Dai</name>
    </author>
    <author>
      <name>Xueying Qin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.01977v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.01977v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.04311v1</id>
    <updated>2016-07-14T20:44:27Z</updated>
    <published>2016-07-14T20:44:27Z</published>
    <title>Defensive Distillation is Not Robust to Adversarial Examples</title>
    <summary>  We show that defensive distillation is not secure: it is no more resistant to
targeted misclassification attacks than unprotected neural networks.
</summary>
    <author>
      <name>Nicholas Carlini</name>
    </author>
    <author>
      <name>David Wagner</name>
    </author>
    <link href="http://arxiv.org/abs/1607.04311v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.04311v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04369v2</id>
    <updated>2016-10-13T15:03:38Z</updated>
    <published>2016-08-15T19:21:58Z</published>
    <title>Star-galaxy Classification Using Deep Convolutional Neural Networks</title>
    <summary>  Most existing star-galaxy classifiers use the reduced summary information
from catalogs, requiring careful feature extraction and selection. The latest
advances in machine learning that use deep convolutional neural networks allow
a machine to automatically learn the features directly from data, minimizing
the need for input from human experts. We present a star-galaxy classification
framework that uses deep convolutional neural networks (ConvNets) directly on
the reduced, calibrated pixel values. Using data from the Sloan Digital Sky
Survey (SDSS) and the Canada-France-Hawaii Telescope Lensing Survey (CFHTLenS),
we demonstrate that ConvNets are able to produce accurate and well-calibrated
probabilistic classifications that are competitive with conventional machine
learning techniques. Future advances in deep learning may bring more success
with current and forthcoming photometric surveys, such as the Dark Energy
Survey (DES) and the Large Synoptic Survey Telescope (LSST), because deep
neural networks require very little, manual feature engineering.
</summary>
    <author>
      <name>Edward J. Kim</name>
    </author>
    <author>
      <name>Robert J. Brunner</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/mnras/stw2672</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/mnras/stw2672" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 page, 13 figures. Accepted for publication in the MNRAS. Code
  available at https://github.com/EdwardJKim/dl4astro</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.04369v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04369v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08782v1</id>
    <updated>2016-08-31T09:21:17Z</updated>
    <published>2016-08-31T09:21:17Z</published>
    <title>Training Deep Spiking Neural Networks using Backpropagation</title>
    <summary>  Deep spiking neural networks (SNNs) hold great potential for improving the
latency and energy efficiency of deep neural networks through event-based
computation. However, training such networks is difficult due to the
non-differentiable nature of asynchronous spike events. In this paper, we
introduce a novel technique, which treats the membrane potentials of spiking
neurons as differentiable signals, where discontinuities at spike times are
only considered as noise. This enables an error backpropagation mechanism for
deep SNNs, which works directly on spike signals and membrane potentials. Thus,
compared with previous methods relying on indirect training and conversion, our
technique has the potential to capture the statics of spikes more precisely.
Our novel framework outperforms all previously reported results for SNNs on the
permutation invariant MNIST benchmark, as well as the N-MNIST benchmark
recorded with event-based vision sensors.
</summary>
    <author>
      <name>Jun Haeng Lee</name>
    </author>
    <author>
      <name>Tobi Delbruck</name>
    </author>
    <author>
      <name>Michael Pfeiffer</name>
    </author>
    <link href="http://arxiv.org/abs/1608.08782v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08782v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.01704v7</id>
    <updated>2017-03-09T05:22:52Z</updated>
    <published>2016-09-06T19:37:57Z</published>
    <title>Hierarchical Multiscale Recurrent Neural Networks</title>
    <summary>  Learning both hierarchical and temporal representation has been among the
long-standing challenges of recurrent neural networks. Multiscale recurrent
neural networks have been considered as a promising approach to resolve this
issue, yet there has been a lack of empirical evidence showing that this type
of models can actually capture the temporal dependencies by discovering the
latent hierarchical structure of the sequence. In this paper, we propose a
novel multiscale approach, called the hierarchical multiscale recurrent neural
networks, which can capture the latent hierarchical structure in the sequence
by encoding the temporal dependencies with different timescales using a novel
update mechanism. We show some evidence that our proposed multiscale
architecture can discover underlying hierarchical structure in the sequences
without using explicit boundary information. We evaluate our proposed model on
character-level language modelling and handwriting sequence modelling.
</summary>
    <author>
      <name>Junyoung Chung</name>
    </author>
    <author>
      <name>Sungjin Ahn</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <link href="http://arxiv.org/abs/1609.01704v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.01704v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07378v1</id>
    <updated>2016-09-23T14:24:44Z</updated>
    <published>2016-09-23T14:24:44Z</published>
    <title>Multi-Output Artificial Neural Network for Storm Surge Prediction in
  North Carolina</title>
    <summary>  During hurricane seasons, emergency managers and other decision makers need
accurate and `on-time' information on potential storm surge impacts. Fully
dynamical computer models, such as the ADCIRC tide, storm surge, and wind-wave
model take several hours to complete a forecast when configured at high spatial
resolution. Additionally, statically meaningful ensembles of high-resolution
models (needed for uncertainty estimation) cannot easily be computed in near
real-time. This paper discusses an artificial neural network model for storm
surge prediction in North Carolina. The network model provides fast, real-time
storm surge estimates at coastal locations in North Carolina. The paper studies
the performance of the neural network model vs. other models on synthetic and
real hurricane data.
</summary>
    <author>
      <name>Anton Bezuglov</name>
    </author>
    <author>
      <name>Brian Blanton</name>
    </author>
    <author>
      <name>Reinaldo Santiago</name>
    </author>
    <link href="http://arxiv.org/abs/1609.07378v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07378v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ao-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.01946v1</id>
    <updated>2016-10-06T17:01:07Z</updated>
    <published>2016-10-06T17:01:07Z</published>
    <title>Efficient Valuation of SCR via a Neural Network Approach</title>
    <summary>  As part of the new regulatory framework of Solvency II, introduced by the
European Union, insurance companies are required to monitor their solvency by
computing a key risk metric called the Solvency Capital Requirement (SCR). The
official description of the SCR is not rigorous and has lead researchers to
develop their own mathematical frameworks for calculation of the SCR. These
frameworks are complex and are difficult to implement. Recently, Bauer et al.
suggested a nested Monte Carlo (MC) simulation framework to calculate the SCR.
But the proposed MC framework is computationally expensive even for a simple
insurance product. In this paper, we propose incorporating a neural network
approach into the nested simulation framework to significantly reduce the
computational complexity in the calculation. We study the performance of our
neural network approach in estimating the SCR for a large portfolio of an
important class of insurance products called Variable Annuities (VAs). Our
experiments show that the proposed neural network approach is both efficient
and accurate.
</summary>
    <author>
      <name>Seyed Amir Hejazi</name>
    </author>
    <author>
      <name>Kenneth R. Jackson</name>
    </author>
    <link href="http://arxiv.org/abs/1610.01946v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.01946v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.02050v1</id>
    <updated>2016-01-13T06:27:05Z</updated>
    <published>2016-01-13T06:27:05Z</published>
    <title>Artificial Neural Network Based Power System Stabilizer on a Single
  Machine Infinite Bus Modelled in Digsilent Powerfactory and MATLAB</title>
    <summary>  In this paper the use of artificial neural network in power system stability
is studied. A predictive controller based on two neural networks is designed
and tested on a single machine infinite bus system which is used to replace
conventional power system stabilizers. They have been used for decades in power
system to dampen small amplitude low frequency oscillation in power systems.
The increases in size and complexity of power systems have cast a shadow on
efficiency of conventional method. New control strategies have been proposed in
many researches. Artificial Neural Networks have been studied in many
publications but lack of assurance of their functionality has hindered the
practical usage of them in utilities. The proposed control structure is
modelled using a novel data exchange established between MATLAB and
</summary>
    <author>
      <name>Ali Kharrazi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 16 figures, Electrical Engineering: An International
  Journal (EEIJ), Vol. 2, No. 2/3/4, December 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.02050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.02050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.04989v1</id>
    <updated>2016-10-17T07:28:06Z</updated>
    <published>2016-10-17T07:28:06Z</published>
    <title>Cached Long Short-Term Memory Neural Networks for Document-Level
  Sentiment Classification</title>
    <summary>  Recently, neural networks have achieved great success on sentiment
classification due to their ability to alleviate feature engineering. However,
one of the remaining challenges is to model long texts in document-level
sentiment classification under a recurrent architecture because of the
deficiency of the memory unit. To address this problem, we present a Cached
Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic
information in long texts. CLSTM introduces a cache mechanism, which divides
memory into several groups with different forgetting rates and thus enables the
network to keep sentiment information better within a recurrent unit. The
proposed CLSTM outperforms the state-of-the-art models on three publicly
available document-level sentiment analysis datasets.
</summary>
    <author>
      <name>Jiacheng Xu</name>
    </author>
    <author>
      <name>Danlu Chen</name>
    </author>
    <author>
      <name>Xipeng Qiu</name>
    </author>
    <author>
      <name>Xuangjing Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as long paper of EMNLP2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.04989v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.04989v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.08927v1</id>
    <updated>2016-10-27T18:58:06Z</updated>
    <published>2016-10-27T18:58:06Z</published>
    <title>Voice Conversion using Convolutional Neural Networks</title>
    <summary>  The human auditory system is able to distinguish the vocal source of
thousands of speakers, yet not much is known about what features the auditory
system uses to do this. Fourier Transforms are capable of capturing the pitch
and harmonic structure of the speaker but this alone proves insufficient at
identifying speakers uniquely. The remaining structure, often referred to as
timbre, is critical to identifying speakers but we understood little about it.
In this paper we use recent advances in neural networks in order to manipulate
the voice of one speaker into another by transforming not only the pitch of the
speaker, but the timbre. We review generative models built with neural networks
as well as architectures for creating neural networks that learn analogies. Our
preliminary results converting voices from one speaker to another are
encouraging.
</summary>
    <author>
      <name>Shariq Mobin</name>
    </author>
    <author>
      <name>Joan Bruna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 2016 Machine Learning Summer School (MLSS) in Cadiz,
  Spain</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.08927v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.08927v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.00468v1</id>
    <updated>2016-11-02T04:42:40Z</updated>
    <published>2016-11-02T04:42:40Z</published>
    <title>CRF-CNN: Modeling Structured Information in Human Pose Estimation</title>
    <summary>  Deep convolutional neural networks (CNN) have achieved great success. On the
other hand, modeling structural information has been proved critical in many
vision problems. It is of great interest to integrate them effectively. In a
classical neural network, there is no message passing between neurons in the
same layer. In this paper, we propose a CRF-CNN framework which can
simultaneously model structural information in both output and hidden feature
layers in a probabilistic way, and it is applied to human pose estimation. A
message passing scheme is proposed, so that in various layers each body joint
receives messages from all the others in an efficient way. Such message passing
can be implemented with convolution between features maps in the same layer,
and it is also integrated with feedforward propagation in neural networks.
Finally, a neural network implementation of end-to-end learning CRF-CNN is
provided. Its effectiveness is demonstrated through experiments on two
benchmark datasets.
</summary>
    <author>
      <name>Xiao Chu</name>
    </author>
    <author>
      <name>Wanli Ouyang</name>
    </author>
    <author>
      <name>Hongsheng Li</name>
    </author>
    <author>
      <name>Xiaogang Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NIPS</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.00468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.00468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.07743v1</id>
    <updated>2016-11-23T11:14:01Z</updated>
    <published>2016-11-23T11:14:01Z</published>
    <title>Tunable Sensitivity to Large Errors in Neural Network Training</title>
    <summary>  When humans learn a new concept, they might ignore examples that they cannot
make sense of at first, and only later focus on such examples, when they are
more useful for learning. We propose incorporating this idea of tunable
sensitivity for hard examples in neural network learning, using a new
generalization of the cross-entropy gradient step, which can be used in place
of the gradient in any gradient-based training method. The generalized gradient
is parameterized by a value that controls the sensitivity of the training
process to harder training examples. We tested our method on several benchmark
datasets. We propose, and corroborate in our experiments, that the optimal
level of sensitivity to hard example is positively correlated with the depth of
the network. Moreover, the test prediction error obtained by our method is
generally lower than that of the vanilla cross-entropy gradient learner. We
therefore conclude that tunable sensitivity can be helpful for neural network
learning.
</summary>
    <author>
      <name>Gil Keren</name>
    </author>
    <author>
      <name>Sivan Sabato</name>
    </author>
    <author>
      <name>Bj√∂rn Schuller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper is accepted to the AAAI 2017 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.07743v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.07743v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.01340v1</id>
    <updated>2016-12-05T13:18:48Z</updated>
    <published>2016-12-05T13:18:48Z</published>
    <title>We used Neural Networks to Detect Clickbaits: You won't believe what
  happened Next!</title>
    <summary>  Online content publishers often use catchy headlines for their articles in
order to attract users to their websites. These headlines, popularly known as
clickbaits, exploit a user's curiosity gap and lure them to click on links that
often disappoint them. Existing methods for automatically detecting clickbaits
rely on heavy feature engineering and domain knowledge. Here, we introduce a
neural network architecture based on Recurrent Neural Networks for detecting
clickbaits. Our model relies on distributed word representations learned from a
large unannotated corpora, and character embeddings learned via Convolutional
Neural Networks. Experimental results on a dataset of news headlines show that
our model outperforms existing techniques for clickbait detection with an
accuracy of 0.98 with F1-score of 0.98 and ROC-AUC of 0.99.
</summary>
    <author>
      <name>Ankesh Anand</name>
    </author>
    <author>
      <name>Tanmoy Chakraborty</name>
    </author>
    <author>
      <name>Noseong Park</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the European Conference on Information Retrieval (ECIR),
  2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.01340v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.01340v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.04876v1</id>
    <updated>2016-12-14T23:13:26Z</updated>
    <published>2016-12-14T23:13:26Z</published>
    <title>Collaborative creativity with Monte-Carlo Tree Search and Convolutional
  Neural Networks</title>
    <summary>  We investigate a human-machine collaborative drawing environment in which an
autonomous agent sketches images while optionally allowing a user to directly
influence the agent's trajectory. We combine Monte Carlo Tree Search with image
classifiers and test both shallow models (e.g. multinomial logistic regression)
and deep Convolutional Neural Networks (e.g. LeNet, Inception v3). We found
that using the shallow model, the agent produces a limited variety of images,
which are noticably recogonisable by humans. However, using the deeper models,
the agent produces a more diverse range of images, and while the agent remains
very confident (99.99%) in having achieved its objective, to humans they mostly
resemble unrecognisable 'random' noise. We relate this to recent research which
also discovered that 'deep neural networks are easily fooled' \cite{Nguyen2015}
and we discuss possible solutions and future directions for the research.
</summary>
    <author>
      <name>Memo Akten</name>
    </author>
    <author>
      <name>Mick Grierson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the Constructive Machine Learning workshop at NIPS 2016
  as a poster and spotlight talk. 8 pages including 2 page references, 2 page
  appendix, 3 figures. Blog post (including videos) at
  https://medium.com/@memoakten/collaborative-creativity-with-monte-carlo-tree-search-and-convolutional-neural-networks-and-other-69d7107385a0</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.04876v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.04876v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.00185v1</id>
    <updated>2017-01-01T01:57:59Z</updated>
    <published>2017-01-01T01:57:59Z</published>
    <title>Self-Taught Convolutional Neural Networks for Short Text Clustering</title>
    <summary>  Short text clustering is a challenging problem due to its sparseness of text
representation. Here we propose a flexible Self-Taught Convolutional neural
network framework for Short Text Clustering (dubbed STC^2), which can flexibly
and successfully incorporate more useful semantic features and learn non-biased
deep text representation in an unsupervised manner. In our framework, the
original raw text features are firstly embedded into compact binary codes by
using one existing unsupervised dimensionality reduction methods. Then, word
embeddings are explored and fed into convolutional neural networks to learn
deep feature representations, meanwhile the output units are used to fit the
pre-trained binary codes in the training process. Finally, we get the optimal
clusters by employing K-means to cluster the learned representations. Extensive
experimental results demonstrate that the proposed framework is effective,
flexible and outperform several popular clustering methods when tested on three
public short text datasets.
</summary>
    <author>
      <name>Jiaming Xu</name>
    </author>
    <author>
      <name>Bo Xu</name>
    </author>
    <author>
      <name>Peng Wang</name>
    </author>
    <author>
      <name>Suncong Zheng</name>
    </author>
    <author>
      <name>Guanhua Tian</name>
    </author>
    <author>
      <name>Jun Zhao</name>
    </author>
    <author>
      <name>Bo Xu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neunet.2016.12.008</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neunet.2016.12.008" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, accepted for publication in Neural Networks</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.00185v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.00185v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.01358v1</id>
    <updated>2017-01-05T15:40:44Z</updated>
    <published>2017-01-05T15:40:44Z</published>
    <title>NeuroRule: A Connectionist Approach to Data Mining</title>
    <summary>  Classification, which involves finding rules that partition a given data set
into disjoint groups, is one class of data mining problems. Approaches proposed
so far for mining classification rules for large databases are mainly decision
tree based symbolic learning methods. The connectionist approach based on
neural networks has been thought not well suited for data mining. One of the
major reasons cited is that knowledge generated by neural networks is not
explicitly represented in the form of rules suitable for verification or
interpretation by humans. This paper examines this issue. With our newly
developed algorithms, rules which are similar to, or more concise than those
generated by the symbolic methods can be extracted from the neural networks.
The data mining process using neural networks with the emphasis on rule
extraction is described. Experimental results and comparison with previously
published works are presented.
</summary>
    <author>
      <name>Hongjun Lu</name>
    </author>
    <author>
      <name>Rudy Setiono</name>
    </author>
    <author>
      <name>Huan Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">VLDB1995</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.01358v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.01358v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.07474v1</id>
    <updated>2017-01-25T20:25:29Z</updated>
    <published>2017-01-25T20:25:29Z</published>
    <title>Exploiting Convolutional Neural Network for Risk Prediction with Medical
  Feature Embedding</title>
    <summary>  The widespread availability of electronic health records (EHRs) promises to
usher in the era of personalized medicine. However, the problem of extracting
useful clinical representations from longitudinal EHR data remains challenging.
In this paper, we explore deep neural network models with learned medical
feature embedding to deal with the problems of high dimensionality and
temporality. Specifically, we use a multi-layer convolutional neural network
(CNN) to parameterize the model and is thus able to capture complex non-linear
longitudinal evolution of EHRs. Our model can effectively capture local/short
temporal dependency in EHRs, which is beneficial for risk prediction. To
account for high dimensionality, we use the embedding medical features in the
CNN model which hold the natural medical concepts. Our initial experiments
produce promising results and demonstrate the effectiveness of both the medical
feature embedding and the proposed convolutional neural network in risk
prediction on cohorts of congestive heart failure and diabetes patients
compared with several strong baselines.
</summary>
    <author>
      <name>Zhengping Che</name>
    </author>
    <author>
      <name>Yu Cheng</name>
    </author>
    <author>
      <name>Zhaonan Sun</name>
    </author>
    <author>
      <name>Yan Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NIPS 2016 Workshop on Machine Learning for Health (ML4HC)</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.07474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.07474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.03418v1</id>
    <updated>2017-02-11T12:26:25Z</updated>
    <published>2017-02-11T12:26:25Z</published>
    <title>Exponential distance distribution of connected neurons in simulations of
  two-dimensional in vitro neural network development</title>
    <summary>  The distribution of the geometric distances of connected neurons is a
practical factor underlying neural networks in the brain. It can affect the
brain\'s dynamic properties at the ground level. Karbowski derived a power-law
decay distribution that has not yet been verified by experiment. In this work,
we check its validity using simulations with a phenomenological model. Based on
the in vitro two-dimensional development of neural networks in culture vessels
by Ito, we match the synapse number saturation time to obtain suitable
parameters for the development process, then determine the distribution of
distances between connected neurons under such conditions. Our simulations
obtain a clear exponential distribution instead of a power-law one, which
indicates that Karbowski's conclusion is invalid, at least for the case of in
vitro neural network development in two-dimensional culture vessels.
</summary>
    <author>
      <name>Zhi-Song lv</name>
    </author>
    <author>
      <name>Chen-Ping Zhu</name>
    </author>
    <author>
      <name>Pei Nie</name>
    </author>
    <author>
      <name>Jing Zhao</name>
    </author>
    <author>
      <name>Hui-Jie Yang</name>
    </author>
    <author>
      <name>Yan-Jun Wang</name>
    </author>
    <author>
      <name>Chin-Kun Hu</name>
    </author>
    <link href="http://arxiv.org/abs/1702.03418v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.03418v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.06286v1</id>
    <updated>2017-02-21T07:37:59Z</updated>
    <published>2017-02-21T07:37:59Z</published>
    <title>Convolutional Recurrent Neural Networks for Polyphonic Sound Event
  Detection</title>
    <summary>  Sound events often occur in unstructured environments where they exhibit wide
variations in their frequency content and temporal structure. Convolutional
neural networks (CNN) are able to extract higher level features that are
invariant to local spectral and temporal variations. Recurrent neural networks
(RNNs) are powerful in learning the longer term temporal context in the audio
signals. CNNs and RNNs as classifiers have recently shown improved performances
over established methods in various sound recognition tasks. We combine these
two approaches in a Convolutional Recurrent Neural Network (CRNN) and apply it
on a polyphonic sound event detection task. We compare the performance of the
proposed CRNN method with CNN, RNN, and other established methods, and observe
a considerable improvement for four different datasets consisting of everyday
sound events.
</summary>
    <author>
      <name>Emre √áakƒ±r</name>
    </author>
    <author>
      <name>Giambattista Parascandolo</name>
    </author>
    <author>
      <name>Toni Heittola</name>
    </author>
    <author>
      <name>Heikki Huttunen</name>
    </author>
    <author>
      <name>Tuomas Virtanen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASLP.2017.2690575</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASLP.2017.2690575" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for IEEE Transactions on Audio, Speech and Language
  Processing, Special Issue on Sound Scene and Event Analysis</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.06286v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.06286v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00311v3</id>
    <updated>2017-03-21T08:20:05Z</updated>
    <published>2017-03-01T14:24:42Z</published>
    <title>Multi-stage Neural Networks with Single-sided Classifiers for False
  Positive Reduction and its Evaluation using Lung X-ray CT Images</title>
    <summary>  Lung nodule classification is a class imbalanced problem because nodules are
found with much lower frequency than non-nodules. In the class imbalanced
problem, conventional classifiers tend to be overwhelmed by the majority class
and ignore the minority class. We therefore propose cascaded convolutional
neural networks to cope with the class imbalanced problem. In the proposed
approach, multi-stage convolutional neural networks that perform as
single-sided classifiers filter out obvious non-nodules. Successively, a
convolutional neural network trained with a balanced data set calculates nodule
probabilities. The proposed method achieved the sensitivity of 92.4\% and 94.5%
at 4 and 8 false positives per scan in Free Receiver Operating Characteristics
(FROC) curve analysis, respectively.
</summary>
    <author>
      <name>Masaharu Sakamoto</name>
    </author>
    <author>
      <name>Hiroki Nakano</name>
    </author>
    <author>
      <name>Kun Zhao</name>
    </author>
    <author>
      <name>Taro Sekiyama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1611.07136</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.00311v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00311v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.01024v1</id>
    <updated>2017-03-03T03:14:28Z</updated>
    <published>2017-03-03T03:14:28Z</published>
    <title>Exponential Moving Average Model in Parallel Speech Recognition Training</title>
    <summary>  As training data rapid growth, large-scale parallel training with multi-GPUs
cluster is widely applied in the neural network model learning currently.We
present a new approach that applies exponential moving average method in
large-scale parallel training of neural network model. It is a non-interference
strategy that the exponential moving average model is not broadcasted to
distributed workers to update their local models after model synchronization in
the training process, and it is implemented as the final model of the training
system. Fully-connected feed-forward neural networks (DNNs) and deep
unidirectional Long short-term memory (LSTM) recurrent neural networks (RNNs)
are successfully trained with proposed method for large vocabulary continuous
speech recognition on Shenma voice search data in Mandarin. The character error
rate (CER) of Mandarin speech recognition further degrades than
state-of-the-art approaches of parallel training.
</summary>
    <author>
      <name>Xu Tian</name>
    </author>
    <author>
      <name>Jun Zhang</name>
    </author>
    <author>
      <name>Zejun Ma</name>
    </author>
    <author>
      <name>Yi He</name>
    </author>
    <author>
      <name>Juan Wei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.01024v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.01024v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.04845v1</id>
    <updated>2017-03-15T00:15:01Z</updated>
    <published>2017-03-15T00:15:01Z</published>
    <title>Skin lesion segmentation based on preprocessing, thresholding and neural
  networks</title>
    <summary>  This abstract describes the segmentation system used to participate in the
challenge ISIC 2017: Skin Lesion Analysis Towards Melanoma Detection. Several
preprocessing techniques have been tested for three color representations (RGB,
YCbCr and HSV) of 392 images. Results have been used to choose the better
preprocessing for each channel. In each case a neural network is trained to
predict the Jaccard Index based on object characteristics. The system includes
black frames and reference circle detection algorithms but no special treatment
is done for hair removal. Segmentation is performed in two steps first the best
channel to be segmented is chosen by selecting the best neural network output.
If this output does not predict a Jaccard Index over 0.5 a more aggressive
preprocessing is performed using open and close morphological operations and
the segmentation of the channel that obtains the best output from the neural
networks is selected as the lesion.
</summary>
    <author>
      <name>Juana M. Guti√©rrez-Arriola</name>
    </author>
    <author>
      <name>Marta G√≥mez-√Ålvarez</name>
    </author>
    <author>
      <name>Victor Osma-Ruiz</name>
    </author>
    <author>
      <name>Nicol√°s S√°enz-Lech√≥n</name>
    </author>
    <author>
      <name>Rub√©n Fraile</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures, abstract submitted to participate in the
  challenge ISIC 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.04845v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.04845v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.05499v1</id>
    <updated>2017-03-16T08:19:48Z</updated>
    <published>2017-03-16T08:19:48Z</published>
    <title>Gr√ºneisen model for melts</title>
    <summary>  The Gr\"uneisen relation is shown to be important for the thermodynamics of
dense liquids.
</summary>
    <author>
      <name>U. Buchenau</name>
    </author>
    <link href="http://arxiv.org/abs/1703.05499v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.05499v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.soft" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.08497v1</id>
    <updated>2017-03-24T16:41:19Z</updated>
    <published>2017-03-24T16:41:19Z</published>
    <title>Local Deep Neural Networks for Age and Gender Classification</title>
    <summary>  Local deep neural networks have been recently introduced for gender
recognition. Although, they achieve very good performance they are very
computationally expensive to train. In this work, we introduce a simplified
version of local deep neural networks which significantly reduces the training
time. Instead of using hundreds of patches per image, as suggested by the
original method, we propose to use 9 overlapping patches per image which cover
the entire face region. This results in a much reduced training time, since
just 9 patches are extracted per image instead of hundreds, at the expense of a
slightly reduced performance. We tested the proposed modified local deep neural
networks approach on the LFW and Adience databases for the task of gender and
age classification. For both tasks and both databases the performance is up to
1% lower compared to the original version of the algorithm. We have also
investigated which patches are more discriminative for age and gender
classification. It turns out that the mouth and eyes regions are useful for age
classification, whereas just the eye region is useful for gender
classification.
</summary>
    <author>
      <name>Zukang Liao</name>
    </author>
    <author>
      <name>Stavros Petridis</name>
    </author>
    <author>
      <name>Maja Pantic</name>
    </author>
    <link href="http://arxiv.org/abs/1703.08497v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.08497v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.08987v2</id>
    <updated>2017-04-03T13:00:02Z</updated>
    <published>2017-03-27T09:51:55Z</published>
    <title>LIDAR-based Driving Path Generation Using Fully Convolutional Neural
  Networks</title>
    <summary>  In this work, a novel learning-based approach has been developed to generate
driving paths by integrating LIDAR point clouds, GPS-IMU information, and
Google driving directions. The system is based on a fully convolutional neural
network that jointly learns to carry out perception and path generation from
real-world driving sequences and that is trained using automatically generated
training examples. Several combinations of input data were tested in order to
assess the performance gain provided by specific information modalities. The
fully convolutional neural network trained using all the available sensors
together with driving directions achieved the best MaxF score of 88.13% when
considering a region of interest of 60x60 meters. By considering a smaller
region of interest, the agreement between predicted paths and ground-truth
increased to 92.60%. The positive results obtained in this work indicate that
the proposed system may help fill the gap between low-level scene parsing and
behavior-reflex approaches by generating outputs that are close to vehicle
control and at the same time human-interpretable.
</summary>
    <author>
      <name>Luca Caltagirone</name>
    </author>
    <author>
      <name>Mauro Bellone</name>
    </author>
    <author>
      <name>Lennart Svensson</name>
    </author>
    <author>
      <name>Mattias Wahde</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Changed title, formerly "Simultaneous Perception and Path Generation
  Using Fully Convolutional Neural Networks"</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.08987v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.08987v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.00063v1</id>
    <updated>2017-03-31T21:25:20Z</updated>
    <published>2017-03-31T21:25:20Z</published>
    <title>TopologyNet: Topology based deep convolutional neural networks for
  biomolecular property predictions</title>
    <summary>  Although deep learning approaches have had tremendous success in image, video
and audio processing, computer vision, and speech recognition, their
applications to three-dimensional (3D) biomolecular structural data sets have
been hindered by the entangled geometric complexity and biological complexity.
We introduce topology, i.e., element specific persistent homology (ESPH), to
untangle geometric complexity and biological complexity. ESPH represents 3D
complex geometry by one-dimensional (1D) topological invariants and retains
crucial biological information via a multichannel image representation. It is
able to reveal hidden structure-function relationships in biomolecules. We
further integrate ESPH and convolutional neural networks to construct a
multichannel topological neural network (TopologyNet) for the predictions of
protein-ligand binding affinities and protein stability changes upon mutation.
To overcome the limitations to deep learning arising from small and noisy
training sets, we present a multitask topological convolutional neural network
(MT-TCNN). We demonstrate that the present TopologyNet architectures outperform
other state-of-the-art methods in the predictions of protein-ligand binding
affinities, globular protein mutation impacts, and membrane protein mutation
impacts.
</summary>
    <author>
      <name>Zixuan Cang</name>
    </author>
    <author>
      <name>Guo-Wei Wei</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pcbi.1005690</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pcbi.1005690" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 8 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.00063v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.00063v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.00774v2</id>
    <updated>2017-04-15T02:59:14Z</updated>
    <published>2017-04-03T19:17:58Z</published>
    <title>Restricted Recurrent Neural Tensor Networks: Exploiting Word Frequency
  and Compositionality for Increased Model Capacity and Performance With No
  Computational Overhead</title>
    <summary>  Increasing the capacity of recurrent neural networks (RNN) usually involves
augmenting the size of the hidden layer, resulting in a significant increase of
computational cost. An alternative is the recurrent neural tensor network
(RNTN), which increases capacity by employing distinct hidden layer weights for
each vocabulary word. However, memory usage scales linearly with vocabulary
size, which can reach millions for word-level language models. In this paper,
we introduce restricted recurrent neural tensor networks (r-RNTN) which reserve
distinct hidden layer weights for frequent vocabulary words while sharing a
single set of weights for infrequent words. Perplexity evaluations show that
r-RNTNs improve language model performance over standard RNNs using only a
small fraction of the parameters of unrestricted RNTNs.
</summary>
    <author>
      <name>Alexandre Salle</name>
    </author>
    <author>
      <name>Aline Villavicencio</name>
    </author>
    <link href="http://arxiv.org/abs/1704.00774v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.00774v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.02798v2</id>
    <updated>2017-04-11T17:25:08Z</updated>
    <published>2017-04-10T10:59:05Z</published>
    <title>Bayesian Recurrent Neural Networks</title>
    <summary>  In this work we explore a straightforward variational Bayes scheme for
Recurrent Neural Networks. Firstly, we show that a simple adaptation of
truncated backpropagation through time can yield good quality uncertainty
estimates and superior regularisation at only a small extra computational cost
during training. Secondly, we demonstrate how a novel kind of posterior
approximation yields further improvements to the performance of Bayesian RNNs.
We incorporate local gradient information into the approximate posterior to
sharpen it around the current batch statistics. This technique is not exclusive
to recurrent neural networks and can be applied more widely to train Bayesian
neural networks. We also empirically demonstrate how Bayesian RNNs are superior
to traditional RNNs on a language modelling benchmark and an image captioning
task, as well as showing how each of these methods improve our model over a
variety of other schemes for training them. We also introduce a new benchmark
for studying uncertainty for language models so future methods can be easily
compared.
</summary>
    <author>
      <name>Meire Fortunato</name>
    </author>
    <author>
      <name>Charles Blundell</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <link href="http://arxiv.org/abs/1704.02798v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.02798v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.04095v1</id>
    <updated>2017-02-13T22:42:52Z</updated>
    <published>2017-02-13T22:42:52Z</published>
    <title>Training Neural Networks Based on Imperialist Competitive Algorithm for
  Predicting Earthquake Intensity</title>
    <summary>  In this study we determined neural network weights and biases by Imperialist
Competitive Algorithm (ICA) in order to train network for predicting earthquake
intensity in Richter. For this reason, we used dependent parameters like
earthquake occurrence time, epicenter's latitude and longitude in degree, focal
depth in kilometer, and the seismological center distance from epicenter and
earthquake focal center in kilometer which has been provided by Berkeley data
base. The studied neural network has two hidden layer: its first layer has 16
neurons and the second layer has 24 neurons. By using ICA algorithm, average
error for testing data is 0.0007 with a variance equal to 0.318. The earthquake
prediction error in Richter by MSE criteria for ICA algorithm is 0.101, but by
using GA, the MSE value is 0.115.
</summary>
    <author>
      <name>Mohsen Moradi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.04095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.04095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.06803v1</id>
    <updated>2017-04-22T14:02:01Z</updated>
    <published>2017-04-22T14:02:01Z</published>
    <title>Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks</title>
    <summary>  Matrix completion models are among the most common formulations of
recommender systems. Recent works have showed a boost of performance of these
techniques when introducing the pairwise relationships between users/items in
the form of graphs, and imposing smoothness priors on these graphs. However,
such techniques do not fully exploit the local stationarity structures of
user/item graphs, and the number of parameters to learn is linear w.r.t. the
number of users and items. We propose a novel approach to overcome these
limitations by using geometric deep learning on graphs. Our matrix completion
architecture combines graph convolutional neural networks and recurrent neural
networks to learn meaningful statistical graph-structured patterns and the
non-linear diffusion process that generates the known ratings. This neural
network system requires a constant number of parameters independent of the
matrix size. We apply our method on both synthetic and real datasets, showing
that it outperforms state-of-the-art techniques.
</summary>
    <author>
      <name>Federico Monti</name>
    </author>
    <author>
      <name>Michael M. Bronstein</name>
    </author>
    <author>
      <name>Xavier Bresson</name>
    </author>
    <link href="http://arxiv.org/abs/1704.06803v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.06803v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.07503v1</id>
    <updated>2017-04-25T01:10:09Z</updated>
    <published>2017-04-25T01:10:09Z</published>
    <title>Learning of Human-like Algebraic Reasoning Using Deep Feedforward Neural
  Networks</title>
    <summary>  There is a wide gap between symbolic reasoning and deep learning. In this
research, we explore the possibility of using deep learning to improve symbolic
reasoning. Briefly, in a reasoning system, a deep feedforward neural network is
used to guide rewriting processes after learning from algebraic reasoning
examples produced by humans. To enable the neural network to recognise patterns
of algebraic expressions with non-deterministic sizes, reduced partial trees
are used to represent the expressions. Also, to represent both top-down and
bottom-up information of the expressions, a centralisation technique is used to
improve the reduced partial trees. Besides, symbolic association vectors and
rule application records are used to improve the rewriting processes.
Experimental results reveal that the algebraic reasoning examples can be
accurately learnt only if the feedforward neural network has enough hidden
layers. Also, the centralisation technique, the symbolic association vectors
and the rule application records can reduce error rates of reasoning. In
particular, the above approaches have led to 4.6% error rate of reasoning on a
dataset of linear equations, differentials and integrals.
</summary>
    <author>
      <name>Cheng-Hao Cai</name>
    </author>
    <author>
      <name>Dengfeng Ke</name>
    </author>
    <author>
      <name>Yanyan Xu</name>
    </author>
    <author>
      <name>Kaile Su</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.07503v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.07503v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0; I.2.3; I.2.4; I.2.6; I.2.8; I.5.0; I.5.1; I.5.2; I.5.4; F.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.05992v1</id>
    <updated>2017-05-17T02:34:27Z</updated>
    <published>2017-05-17T02:34:27Z</published>
    <title>Frame Stacking and Retaining for Recurrent Neural Network Acoustic Model</title>
    <summary>  Frame stacking is broadly applied in end-to-end neural network training like
connectionist temporal classification (CTC), and it leads to more accurate
models and faster decoding. However, it is not well-suited to conventional
neural network based on context-dependent state acoustic model, if the decoder
is unchanged. In this paper, we propose a novel frame retaining method which is
applied in decoding. The system which combined frame retaining with frame
stacking could reduces the time consumption of both training and decoding. Long
short-term memory (LSTM) recurrent neural networks (RNNs) using it achieve
almost linear training speedup and reduces relative 41\% real time factor
(RTF). At the same time, recognition performance is no degradation or improves
sightly on Shenma voice search dataset in Mandarin.
</summary>
    <author>
      <name>Xu Tian</name>
    </author>
    <author>
      <name>Jun Zhang</name>
    </author>
    <author>
      <name>Zejun Ma</name>
    </author>
    <author>
      <name>Yi He</name>
    </author>
    <author>
      <name>Juan Wei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.05992v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.05992v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.06264v2</id>
    <updated>2017-06-06T16:56:23Z</updated>
    <published>2017-05-17T17:17:07Z</published>
    <title>Deep Diagnostics: Applying Convolutional Neural Networks for Vessels
  Defects Detection</title>
    <summary>  Coronary angiography is considered to be a safe tool for the evaluation of
coronary artery disease and perform in approximately 12 million patients each
year worldwide. [1] In most cases, angiograms are manually analyzed by a
cardiologist. Actually, there are no clinical practice algorithms which could
improve and automate this work. Neural networks show high efficiency in tasks
of image analysis and they can be used for the analysis of angiograms and
facilitate diagnostics. We have developed an algorithm based on Convolutional
Neural Network and Neural Network U-Net [2] for vessels segmentation and
defects detection such as stenosis. For our research we used anonymized
angiography data obtained from one of the city's hospitals and augmented them
to improve learning efficiency. U-Net usage provided high quality segmentation
and the combination of our algorithm with an ensemble of classifiers shows a
good accuracy in the task of ischemia evaluation on test data. Subsequently,
this approach can be served as a basis for the creation of an analytical system
that could speed up the diagnosis of cardiovascular diseases and greatly
facilitate the work of a specialist.
</summary>
    <author>
      <name>Stanislav Filippov</name>
    </author>
    <author>
      <name>Arsenii Moiseev</name>
    </author>
    <author>
      <name>Andronenko Andrey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Complaint to the article due to low research quality</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.06264v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.06264v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.08843v1</id>
    <updated>2017-05-24T16:22:13Z</updated>
    <published>2017-05-24T16:22:13Z</published>
    <title>Parsing with CYK over Distributed Representations: "Classical" Syntactic
  Parsing in the Novel Era of Neural Networks</title>
    <summary>  Syntactic parsing is a key task in natural language processing which has been
dominated by symbolic, grammar-based syntactic parsers. Neural networks, with
their distributed representations, are challenging these methods.
  In this paper, we want to show that existing parsing algorithms can cross the
border and be defined over distributed representations. We then define D-CYK: a
version of the traditional CYK algorithm defined over distributed
representations. Our D-CYK operates as the original CYK but uses matrix
multiplications. These operations are compatible with traditional neural
networks. Experiments show that D-CYK approximates the original CYK. By showing
that CYK can be performed on distributed representations, our D-CYK opens the
possibility of defining recurrent layers of CYK-informed neural networks.
</summary>
    <author>
      <name>Fabio Massimo Zanzotto</name>
    </author>
    <author>
      <name>Giordano Cristini</name>
    </author>
    <link href="http://arxiv.org/abs/1705.08843v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.08843v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.09864v1</id>
    <updated>2017-05-27T20:52:10Z</updated>
    <published>2017-05-27T20:52:10Z</published>
    <title>BMXNet: An Open-Source Binary Neural Network Implementation Based on
  MXNet</title>
    <summary>  Binary Neural Networks (BNNs) can drastically reduce memory size and accesses
by applying bit-wise operations instead of standard arithmetic operations.
Therefore it could significantly improve the efficiency and lower the energy
consumption at runtime, which enables the application of state-of-the-art deep
learning models on low power devices. BMXNet is an open-source BNN library
based on MXNet, which supports both XNOR-Networks and Quantized Neural
Networks. The developed BNN layers can be seamlessly applied with other
standard library components and work in both GPU and CPU mode. BMXNet is
maintained and developed by the multimedia research group at Hasso Plattner
Institute and released under Apache license. Extensive experiments validate the
efficiency and effectiveness of our implementation. The BMXNet library, several
sample projects, and a collection of pre-trained binary deep models are
available for download at https://github.com/hpi-xnor
</summary>
    <author>
      <name>Haojin Yang</name>
    </author>
    <author>
      <name>Martin Fritzsche</name>
    </author>
    <author>
      <name>Christian Bartz</name>
    </author>
    <author>
      <name>Christoph Meinel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.09864v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.09864v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.03686v3</id>
    <updated>2017-10-17T02:24:34Z</updated>
    <published>2017-06-12T15:29:48Z</published>
    <title>Image Crowd Counting Using Convolutional Neural Network and Markov
  Random Field</title>
    <summary>  In this paper, we propose a method called Convolutional Neural Network-Markov
Random Field (CNN-MRF) to estimate the crowd count in a still image. We first
divide the dense crowd visible image into overlapping patches and then use a
deep convolutional neural network to extract features from each patch image,
followed by a fully connected neural network to regress the local patch crowd
count. Since the local patches have overlapping portions, the crowd count of
the adjacent patches has a high correlation. We use this correlation and the
Markov random field to smooth the counting results of the local patches.
Experiments show that our approach significantly outperforms the
state-of-the-art methods on UCF and Shanghaitech crowd counting datasets.
</summary>
    <author>
      <name>Kang Han</name>
    </author>
    <author>
      <name>Wanggen Wan</name>
    </author>
    <author>
      <name>Haiyan Yao</name>
    </author>
    <author>
      <name>Li Hou</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.20965/jaciii.2017.p0632(2017)</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.20965/jaciii.2017.p0632(2017)" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures, JACIII Vol.21 No.4</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">JACIII Vol.21 No.4 2017 pp. 632-638</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.03686v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.03686v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.06749v1</id>
    <updated>2017-06-21T06:11:59Z</updated>
    <published>2017-06-21T06:11:59Z</published>
    <title>Cross-language Learning with Adversarial Neural Networks: Application to
  Community Question Answering</title>
    <summary>  We address the problem of cross-language adaptation for question-question
similarity reranking in community question answering, with the objective to
port a system trained on one input language to another input language given
labeled training data for the first language and only unlabeled data for the
second language. In particular, we propose to use adversarial training of
neural networks to learn high-level features that are discriminative for the
main learning task, and at the same time are invariant across the input
languages. The evaluation results show sizable improvements for our
cross-language adversarial neural network (CLANN) model over a strong
non-adversarial system.
</summary>
    <author>
      <name>Shafiq Joty</name>
    </author>
    <author>
      <name>Preslav Nakov</name>
    </author>
    <author>
      <name>Llu√≠s M√†rquez</name>
    </author>
    <author>
      <name>Israa Jaradat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CoNLL-2017: The SIGNLL Conference on Computational Natural Language
  Learning; cross-language adversarial neural network (CLANN) model;
  adversarial training; cross-language adaptation; community question
  answering; question-question similarity</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.06749v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.06749v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07206v2</id>
    <updated>2017-08-04T20:01:33Z</updated>
    <published>2017-06-22T08:24:59Z</published>
    <title>Explaining Recurrent Neural Network Predictions in Sentiment Analysis</title>
    <summary>  Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown
to deliver insightful explanations in the form of input space relevances for
understanding feed-forward neural network classification decisions. In the
present work, we extend the usage of LRP to recurrent neural networks. We
propose a specific propagation rule applicable to multiplicative connections as
they arise in recurrent network architectures such as LSTMs and GRUs. We apply
our technique to a word-based bi-directional LSTM model on a five-class
sentiment prediction task, and evaluate the resulting LRP relevances both
qualitatively and quantitatively, obtaining better results than a
gradient-based related method which was used in previous work.
</summary>
    <author>
      <name>Leila Arras</name>
    </author>
    <author>
      <name>Gr√©goire Montavon</name>
    </author>
    <author>
      <name>Klaus-Robert M√ºller</name>
    </author>
    <author>
      <name>Wojciech Samek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures, accepted for EMNLP'17 Workshop on Computational
  Approaches to Subjectivity, Sentiment &amp; Social Media Analysis (WASSA)</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.07206v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07206v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08466v1</id>
    <updated>2017-06-26T16:37:26Z</updated>
    <published>2017-06-26T16:37:26Z</published>
    <title>Inverse Ising inference by combining Ornstein-Zernike theory with deep
  learning</title>
    <summary>  Inferring a generative model from data is a fundamental problem in machine
learning. It is well-known that the Ising model is the maximum entropy model
for binary variables which reproduces the sample mean and pairwise
correlations. Learning the parameters of the Ising model from data is the
challenge. We establish an analogy between the inverse Ising problem and the
Ornstein-Zernike formalism in liquid state physics. Rather than analytically
deriving the closure relation, we use a deep neural network to learn the
closure from simulations of the Ising model. We show, using simulations as well
as biochemical datasets, that the deep neural network model outperforms
systematic field-theoretic expansions and can generalize well beyond the
parameter regime of the training data. The neural network is able to learn from
synthetic data, which can be generated with relative ease, to give accurate
predictions on real world datasets.
</summary>
    <author>
      <name>Alpha A. Lee</name>
    </author>
    <link href="http://arxiv.org/abs/1706.08466v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08466v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08838v1</id>
    <updated>2017-06-23T19:06:13Z</updated>
    <published>2017-06-23T19:06:13Z</published>
    <title>TimeNet: Pre-trained deep recurrent neural network for time series
  classification</title>
    <summary>  Inspired by the tremendous success of deep Convolutional Neural Networks as
generic feature extractors for images, we propose TimeNet: a deep recurrent
neural network (RNN) trained on diverse time series in an unsupervised manner
using sequence to sequence (seq2seq) models to extract features from time
series. Rather than relying on data from the problem domain, TimeNet attempts
to generalize time series representation across domains by ingesting time
series from several domains simultaneously. Once trained, TimeNet can be used
as a generic off-the-shelf feature extractor for time series. The
representations or embeddings given by a pre-trained TimeNet are found to be
useful for time series classification (TSC). For several publicly available
datasets from UCR TSC Archive and an industrial telematics sensor data from
vehicles, we observe that a classifier learned over the TimeNet embeddings
yields significantly better performance compared to (i) a classifier learned
over the embeddings given by a domain-specific RNN, as well as (ii) a nearest
neighbor classifier based on Dynamic Time Warping.
</summary>
    <author>
      <name>Pankaj Malhotra</name>
    </author>
    <author>
      <name>Vishnu TV</name>
    </author>
    <author>
      <name>Lovekesh Vig</name>
    </author>
    <author>
      <name>Puneet Agarwal</name>
    </author>
    <author>
      <name>Gautam Shroff</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25th European Symposium on Artificial Neural Networks, Computational
  Intelligence and Machine Learning, 2017, Bruges, Belgium</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.08838v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08838v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.02746v2</id>
    <updated>2017-07-12T20:08:12Z</updated>
    <published>2017-07-10T08:44:46Z</published>
    <title>Backpropagation in matrix notation</title>
    <summary>  In this note we calculate the gradient of the network function in matrix
notation.
</summary>
    <author>
      <name>N. M. Mishachev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, Remark 6 added</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.02746v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.02746v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.03880v1</id>
    <updated>2017-08-13T09:51:07Z</updated>
    <published>2017-08-13T09:51:07Z</published>
    <title>Image Quality Assessment Guided Deep Neural Networks Training</title>
    <summary>  For many computer vision problems, the deep neural networks are trained and
validated based on the assumption that the input images are pristine (i.e.,
artifact-free). However, digital images are subject to a wide range of
distortions in real application scenarios, while the practical issues regarding
image quality in high level visual information understanding have been largely
ignored. In this paper, in view of the fact that most widely deployed deep
learning models are susceptible to various image distortions, the distorted
images are involved for data augmentation in the deep neural network training
process to learn a reliable model for practical applications. In particular, an
image quality assessment based label smoothing method, which aims at
regularizing the label distribution of training images, is further proposed to
tune the objective functions in learning the neural network. Experimental
results show that the proposed method is effective in dealing with both low and
high quality images in the typical image classification task.
</summary>
    <author>
      <name>Zhuo Chen</name>
    </author>
    <author>
      <name>Weisi Lin</name>
    </author>
    <author>
      <name>Shiqi Wang</name>
    </author>
    <author>
      <name>Long Xu</name>
    </author>
    <author>
      <name>Leida Li</name>
    </author>
    <link href="http://arxiv.org/abs/1708.03880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.03880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.04485v1</id>
    <updated>2017-05-23T22:11:11Z</updated>
    <published>2017-05-23T22:11:11Z</published>
    <title>SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks</title>
    <summary>  Convolutional Neural Networks (CNNs) have emerged as a fundamental technology
for machine learning. High performance and extreme energy efficiency are
critical for deployments of CNNs in a wide range of situations, especially
mobile platforms such as autonomous vehicles, cameras, and electronic personal
assistants. This paper introduces the Sparse CNN (SCNN) accelerator
architecture, which improves performance and energy efficiency by exploiting
the zero-valued weights that stem from network pruning during training and
zero-valued activations that arise from the common ReLU operator applied during
inference. Specifically, SCNN employs a novel dataflow that enables maintaining
the sparse weights and activations in a compressed encoding, which eliminates
unnecessary data transfers and reduces storage requirements. Furthermore, the
SCNN dataflow facilitates efficient delivery of those weights and activations
to the multiplier array, where they are extensively reused. In addition, the
accumulation of multiplication products are performed in a novel accumulator
array. Our results show that on contemporary neural networks, SCNN can improve
both performance and energy by a factor of 2.7x and 2.3x, respectively, over a
comparably provisioned dense CNN accelerator.
</summary>
    <author>
      <name>Angshuman Parashar</name>
    </author>
    <author>
      <name>Minsoo Rhu</name>
    </author>
    <author>
      <name>Anurag Mukkara</name>
    </author>
    <author>
      <name>Antonio Puglielli</name>
    </author>
    <author>
      <name>Rangharajan Venkatesan</name>
    </author>
    <author>
      <name>Brucek Khailany</name>
    </author>
    <author>
      <name>Joel Emer</name>
    </author>
    <author>
      <name>Stephen W. Keckler</name>
    </author>
    <author>
      <name>William J. Dally</name>
    </author>
    <link href="http://arxiv.org/abs/1708.04485v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.04485v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.06250v1</id>
    <updated>2017-08-18T07:51:43Z</updated>
    <published>2017-08-18T07:51:43Z</published>
    <title>Pillar Networks++: Distributed non-parametric deep and wide networks</title>
    <summary>  In recent work, it was shown that combining multi-kernel based support vector
machines (SVMs) can lead to near state-of-the-art performance on an action
recognition dataset (HMDB-51 dataset). This was 0.4\% lower than frameworks
that used hand-crafted features in addition to the deep convolutional feature
extractors. In the present work, we show that combining distributed Gaussian
Processes with multi-stream deep convolutional neural networks (CNN) alleviate
the need to augment a neural network with hand-crafted features. In contrast to
prior work, we treat each deep neural convolutional network as an expert
wherein the individual predictions (and their respective uncertainties) are
combined into a Product of Experts (PoE) framework.
</summary>
    <author>
      <name>Biswa Sengupta</name>
    </author>
    <author>
      <name>Yu Qian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1707.06923</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.06250v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.06250v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.06686v1</id>
    <updated>2017-08-17T19:43:56Z</updated>
    <published>2017-08-17T19:43:56Z</published>
    <title>Extensive deep neural networks</title>
    <summary>  We present a procedure for training and evaluating a deep neural network
which can efficiently infer extensive parameters of arbitrarily large systems,
doing so with O(N) complexity. We use a form of domain decomposition for
training and inference, where each sub-domain (tile) is comprised of a
non-overlapping focus region surrounded by an overlapping context region. The
relative sizes of focus and context are physically motivated and depend on the
locality length scale of the problem. Extensive deep neural networks (EDNN) are
a formulation of convolutional neural networks which provide a flexible and
general approach, based on physical constraints, to describe multi-scale
interactions. They are well suited to massively parallel inference, as no
inter-thread communication is necessary during evaluation. Example uses for
learning simple spin models, Laplacian (derivative) operator, and approximating
many-body quantum mechanical operators (within the density functional theory
approach) are demonstrated.
</summary>
    <author>
      <name>Iryna Luchak</name>
    </author>
    <author>
      <name>Kyle Mills</name>
    </author>
    <author>
      <name>Kevin Ryczko</name>
    </author>
    <author>
      <name>Adam Domurad</name>
    </author>
    <author>
      <name>Isaac Tamblyn</name>
    </author>
    <link href="http://arxiv.org/abs/1708.06686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.06686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.06822v2</id>
    <updated>2017-09-08T13:47:53Z</updated>
    <published>2017-08-22T21:13:18Z</published>
    <title>Deep EndoVO: A Recurrent Convolutional Neural Network (RCNN) based
  Visual Odometry Approach for Endoscopic Capsule Robots</title>
    <summary>  Ingestible wireless capsule endoscopy is an emerging minimally invasive
diagnostic technology for inspection of the GI tract and diagnosis of a wide
range of diseases and pathologies. Medical device companies and many research
groups have recently made substantial progresses in converting passive capsule
endoscopes to active capsule robots, enabling more accurate, precise, and
intuitive detection of the location and size of the diseased areas. Since a
reliable real time pose estimation functionality is crucial for actively
controlled endoscopic capsule robots, in this study, we propose a monocular
visual odometry (VO) method for endoscopic capsule robot operations. Our method
lies on the application of the deep Recurrent Convolutional Neural Networks
(RCNNs) for the visual odometry task, where Convolutional Neural Networks
(CNNs) and Recurrent Neural Networks (RNNs) are used for the feature extraction
and inference of dynamics across the frames, respectively. Detailed analyses
and evaluations made on a real pig stomach dataset proves that our system
achieves high translational and rotational accuracies for different types of
endoscopic capsule robot trajectories.
</summary>
    <author>
      <name>Mehmet Turan</name>
    </author>
    <author>
      <name>Yasin Almalioglu</name>
    </author>
    <author>
      <name>Helder Araujo</name>
    </author>
    <author>
      <name>Ender Konukoglu</name>
    </author>
    <author>
      <name>Metin Sitti</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neucom.2017.10.014</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neucom.2017.10.014" rel="related"/>
    <link href="http://arxiv.org/abs/1708.06822v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.06822v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.07403v1</id>
    <updated>2017-08-24T13:40:06Z</updated>
    <published>2017-08-24T13:40:06Z</published>
    <title>CloudScan - A configuration-free invoice analysis system using recurrent
  neural networks</title>
    <summary>  We present CloudScan; an invoice analysis system that requires zero
configuration or upfront annotation. In contrast to previous work, CloudScan
does not rely on templates of invoice layout, instead it learns a single global
model of invoices that naturally generalizes to unseen invoice layouts. The
model is trained using data automatically extracted from end-user provided
feedback. This automatic training data extraction removes the requirement for
users to annotate the data precisely. We describe a recurrent neural network
model that can capture long range context and compare it to a baseline logistic
regression model corresponding to the current CloudScan production system. We
train and evaluate the system on 8 important fields using a dataset of 326,471
invoices. The recurrent neural network and baseline model achieve 0.891 and
0.887 average F1 scores respectively on seen invoice layouts. For the harder
task of unseen invoice layouts, the recurrent neural network model outperforms
the baseline with 0.840 average F1 compared to 0.788.
</summary>
    <author>
      <name>Rasmus Berg Palm</name>
    </author>
    <author>
      <name>Ole Winther</name>
    </author>
    <author>
      <name>Florian Laws</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at ICDAR 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.07403v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.07403v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.07469v3</id>
    <updated>2017-12-16T19:19:32Z</updated>
    <published>2017-08-24T15:50:24Z</published>
    <title>DGM: A deep learning algorithm for solving partial differential
  equations</title>
    <summary>  High-dimensional PDEs have been a longstanding computational challenge. We
propose to solve high-dimensional PDEs by approximating the solution with a
deep neural network which is trained to satisfy the differential operator,
initial condition, and boundary conditions. We prove that the neural network
converges to the solution of the partial differential equation as the number of
hidden units increases. Our algorithm is meshfree, which is key since meshes
become infeasible in higher dimensions. Instead of forming a mesh, the neural
network is trained on batches of randomly sampled time and space points. We
implement the approach for American options (a type of free-boundary PDE which
is widely used in finance) in up to $200$ dimensions. We call the algorithm a
"Deep Galerkin Method (DGM)" since it is similar in spirit to Galerkin methods,
with the solution approximated by a neural network instead of a linear
combination of basis functions.
</summary>
    <author>
      <name>Justin Sirignano</name>
    </author>
    <author>
      <name>Konstantinos Spiliopoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Deep learning, machine learning, partial differential equations</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.07469v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.07469v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.MF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.MF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.09534v1</id>
    <updated>2017-08-31T02:14:59Z</updated>
    <published>2017-08-31T02:14:59Z</published>
    <title>Towards On-Chip Optical FFTs for Convolutional Neural Networks</title>
    <summary>  Convolutional neural networks have become an essential element of spatial
deep learning systems. In the prevailing architecture, the convolution
operation is performed with Fast Fourier Transforms (FFT) electronically in
GPUs. The parallelism of GPUs provides an efficiency over CPUs, however both
approaches being electronic are bound by the speed and power limits of the
interconnect delay inside the circuits. Here we present a silicon photonics
based architecture for convolutional neural networks that harnesses the phase
property of light to perform FFTs efficiently. Our all-optical FFT is based on
nested Mach-Zender Interferometers, directional couplers, and phase shifters,
with backend electro-optic modulators for sampling. The FFT delay depends only
on the propagation delay of the optical signal through the silicon photonics
structures. Designing and analyzing the performance of a convolutional neural
network deployed with our on-chip optical FFT, we find dramatic improvements by
up to 10^4 when compared to state-of-the-art GPUs when exploring a compounded
figure-of-merit given by power per convolution over area. At a high level, this
performance is enabled by mapping the desired mathematical function, an FFT,
synergistically onto hardware, in this case optical delay interferometers.
</summary>
    <author>
      <name>Jonathan George</name>
    </author>
    <author>
      <name>Hani Nejadriahi</name>
    </author>
    <author>
      <name>Volker Sorger</name>
    </author>
    <link href="http://arxiv.org/abs/1708.09534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.09534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.00583v1</id>
    <updated>2017-09-02T13:59:39Z</updated>
    <published>2017-09-02T13:59:39Z</published>
    <title>Training Spiking Neural Networks for Cognitive Tasks: A Versatile
  Framework Compatible to Various Temporal Codes</title>
    <summary>  Conventional modeling approaches have found limitations in matching the
increasingly detailed neural network structures and dynamics recorded in
experiments to the diverse brain functionalities. On another approach, studies
have demonstrated to train spiking neural networks for simple functions using
supervised learning. Here, we introduce a modified SpikeProp learning
algorithm, which achieved better learning stability in different activity
states. In addition, we show biological realistic features such as lateral
connections and sparse activities can be included in the network. We
demonstrate the versatility of this framework by implementing three well-known
temporal codes for different types of cognitive tasks, which are MNIST digits
recognition, spatial coordinate transformation, and motor sequence generation.
Moreover, we find several characteristic features have evolved alongside the
task training, such as selective activity, excitatory-inhibitory balance, and
weak pair-wise correlation. The coincidence between the self-evolved and
experimentally observed features indicates their importance on the brain
functionality. Our results suggest a unified setting in which diverse cognitive
computations and mechanisms can be studied.
</summary>
    <author>
      <name>Chaofei Hong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version will be
  superseded</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.00583v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.00583v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.00812v2</id>
    <updated>2017-10-27T09:01:34Z</updated>
    <published>2017-09-04T05:23:42Z</published>
    <title>Phase Diagrams of Three-Dimensional Anderson and Quantum Percolation
  Models using Deep Three-Dimensional Convolutional Neural Network</title>
    <summary>  The three-dimensional Anderson model is a well-studied model of disordered
electron systems that shows the delocalization--localization transition. As in
our previous papers on two- and three-dimensional (2D, 3D) quantum phase
transitions [J. Phys. Soc. Jpn. {\bf 85}, 123706 (2016), {\bf 86}, 044708
(2017)], we used an image recognition algorithm based on a multilayered
convolutional neural network. However, in contrast to previous papers in which
2D image recognition was used, we applied 3D image recognition to analyze
entire 3D wave functions. We show that a full phase diagram of the
disorder-energy plane is obtained once the 3D convolutional neural network has
been trained at the band center. We further demonstrate that the full phase
diagram for 3D quantum bond and site percolations can be drawn by training the
3D Anderson model at the band center.
</summary>
    <author>
      <name>Tomohiro Mano</name>
    </author>
    <author>
      <name>Tomi Ohtsuki</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.7566/JPSJ.86.113704</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.7566/JPSJ.86.113704" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 figures. Published version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J. Phys. Soc. Jpn. 86, 113704 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1709.00812v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.00812v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.04060v1</id>
    <updated>2017-09-12T21:14:57Z</updated>
    <published>2017-09-12T21:14:57Z</published>
    <title>Streamlined Deployment for Quantized Neural Networks</title>
    <summary>  Running Deep Neural Network (DNN) models on devices with limited
computational capability is a challenge due to large compute and memory
requirements. Quantized Neural Networks (QNNs) have emerged as a potential
solution to this problem, promising to offer most of the DNN accuracy benefits
with much lower computational cost. However, harvesting these benefits on
existing mobile CPUs is a challenge since operations on highly quantized
datatypes are not natively supported in most instruction set architectures
(ISAs). In this work, we first describe a streamlining flow to convert all QNN
inference operations to integer ones. Afterwards, we provide techniques based
on processing one bit position at a time (bit-serial) to show how QNNs can be
efficiently deployed using common bitwise operations. We demonstrate the
potential of QNNs on mobile CPUs with microbenchmarks and on a quantized
AlexNet, which is 3.5x faster than an optimized 8-bit baseline.
</summary>
    <author>
      <name>Yaman Umuroglu</name>
    </author>
    <author>
      <name>Magnus Jahre</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at the International Workshop on Highly Efficient Neural
  Networks Design (HENND) co-located with CASES'17</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.04060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.04060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.09212v2</id>
    <updated>2018-02-26T10:52:04Z</updated>
    <published>2017-09-26T18:28:47Z</published>
    <title>Deep convolutional neural networks for estimating porous material
  parameters with ultrasound tomography</title>
    <summary>  We study the feasibility of data based machine learning applied to ultrasound
tomography to estimate water-saturated porous material parameters. In this
work, the data to train the neural networks is simulated by solving wave
propagation in coupled poroviscoelastic-viscoelastic-acoustic media. As the
forward model, we consider a high-order discontinuous Galerkin method while
deep convolutional neural networks are used to solve the parameter estimation
problem. In the numerical experiment, we estimate the material porosity and
tortuosity while the remaining parameters which are of less interest are
successfully marginalized in the neural networks-based inversion. Computational
examples confirms the feasibility and accuracy of this approach.
</summary>
    <author>
      <name>Timo L√§hivaara</name>
    </author>
    <author>
      <name>Leo K√§rkk√§inen</name>
    </author>
    <author>
      <name>Janne M. J. Huttunen</name>
    </author>
    <author>
      <name>Jan S. Hesthaven</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1121/1.5024341</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1121/1.5024341" rel="related"/>
    <link href="http://arxiv.org/abs/1709.09212v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.09212v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.01507v3</id>
    <updated>2018-02-09T13:49:13Z</updated>
    <published>2017-10-04T08:53:12Z</published>
    <title>A Neural Clickbait Detection Engine</title>
    <summary>  In an age where people are becoming increasing likely to trust information
found through online media, journalists have begun employing techniques to lure
readers to articles by using catchy headlines, called clickbait. These
headlines entice the user into clicking through the article whilst not
providing information relevant to the headline itself. Previous methods of
detecting clickbait have explored techniques heavily dependent on feature
engineering, with little experimentation having been tried with neural network
architectures. We introduce a novel model combining recurrent neural networks,
attention layers and image embeddings. Our model uses a combination of
distributed word embeddings derived from unannotated corpora, character level
embeddings calculated through Convolutional Neural Networks. These
representations are passed through a bidirectional LSTM with an attention
layer. The image embeddings are also learnt from large data using CNNs.
Experimental results show that our model achieves an F1 score of 65.37% beating
the previous benchmark of 55.21%.
</summary>
    <author>
      <name>Yash Kumar Lal</name>
    </author>
    <author>
      <name>Siddhartha Gairola</name>
    </author>
    <author>
      <name>Vaibhav Kumar</name>
    </author>
    <author>
      <name>Dhruv Khattar</name>
    </author>
    <link href="http://arxiv.org/abs/1710.01507v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.01507v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.02633v1</id>
    <updated>2017-10-07T04:11:38Z</updated>
    <published>2017-10-07T04:11:38Z</published>
    <title>Radiation Pattern Synthesis Using Hybrid Fourier- Woodward-Lawson-Neural
  Networks for Reliable MIMO Antenna Systems</title>
    <summary>  In this paper, we implement hybrid Woodward-Lawson-Neural Networks and
weighted Fourier method to synthesize antenna arrays. The neural networks (NN)
is applied here to simplify the modeling of MIMO antenna arrays by assessing
phases. The main problem is obviously to find optimal weights of the linear
antenna array elements giving radiation pattern with minimum sidelobe level
(SLL) and hence ameliorating the antenna array performance. To attain this
purpose, an antenna array for reliable Multiple-Input Multiple-Output (MIMO)
applications with frequency at 2.45 GHz is implemented. To validate the
suggested method, many examples of uniformly excited array patterns with the
main beam are put in the direction of the useful signal. The
Woodward-Lawson-Neural Networks synthesis method permits to find out
interesting analytical equations for the synthesis of an antenna array and
highlights the flexibility between the system parameters in input and those in
output. The performance of this hybrid optimization underlines how well the
system is suitable for a wireless communication and how it participates in
reducing interference, as well.
</summary>
    <author>
      <name>Elies Ghayoula</name>
    </author>
    <author>
      <name>Ridha Ghayoula</name>
    </author>
    <author>
      <name>Jaouhar Fattahi</name>
    </author>
    <author>
      <name>Emil Pricop</name>
    </author>
    <author>
      <name>Jean-Yves Chouinard</name>
    </author>
    <author>
      <name>Ammar Bouallegue</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the IEEE SMC 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.02633v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.02633v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.04288v1</id>
    <updated>2017-10-11T20:07:28Z</updated>
    <published>2017-10-11T20:07:28Z</published>
    <title>Audio Concept Classification with Hierarchical Deep Neural Networks</title>
    <summary>  Audio-based multimedia retrieval tasks may identify semantic information in
audio streams, i.e., audio concepts (such as music, laughter, or a revving
engine). Conventional Gaussian-Mixture-Models have had some success in
classifying a reduced set of audio concepts. However, multi-class
classification can benefit from context window analysis and the discriminating
power of deeper architectures. Although deep learning has shown promise in
various applications such as speech and object recognition, it has not yet met
the expectations for other fields such as audio concept classification. This
paper explores, for the first time, the potential of deep learning in
classifying audio concepts on User-Generated Content videos. The proposed
system is comprised of two cascaded neural networks in a hierarchical
configuration to analyze the short- and long-term context information. Our
system outperforms a GMM approach by a relative 54%, a Neural Network by 33%,
and a Deep Neural Network by 12% on the TRECVID-MED database
</summary>
    <author>
      <name>Mirco Ravanelli</name>
    </author>
    <author>
      <name>Benjamin Elizalde</name>
    </author>
    <author>
      <name>Karl Ni</name>
    </author>
    <author>
      <name>Gerald Friedland</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EUSIPCO 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1710.04288v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.04288v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.08637v1</id>
    <updated>2017-10-24T07:46:57Z</updated>
    <published>2017-10-24T07:46:57Z</published>
    <title>Improving Accuracy of Nonparametric Transfer Learning via Vector
  Segmentation</title>
    <summary>  Transfer learning using deep neural networks as feature extractors has become
increasingly popular over the past few years. It allows to obtain
state-of-the-art accuracy on datasets too small to train a deep neural network
on its own, and it provides cutting edge descriptors that, combined with
nonparametric learning methods, allow rapid and flexible deployment of
performing solutions in computationally restricted settings. In this paper, we
are interested in showing that the features extracted using deep neural
networks have specific properties which can be used to improve accuracy of
downstream nonparametric learning methods. Namely, we demonstrate that for some
distributions where information is embedded in a few coordinates, segmenting
feature vectors can lead to better accuracy. We show how this model can be
applied to real datasets by performing experiments using three mainstream deep
neural network feature extractors and four databases, in vision and audio.
</summary>
    <author>
      <name>Vincent Gripon</name>
    </author>
    <author>
      <name>Ghouthi B. Hacene</name>
    </author>
    <author>
      <name>Matthias L√∂we</name>
    </author>
    <author>
      <name>Franck Vermet</name>
    </author>
    <link href="http://arxiv.org/abs/1710.08637v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.08637v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.09489v2</id>
    <updated>2017-11-07T16:15:44Z</updated>
    <published>2017-10-25T22:45:04Z</published>
    <title>Scalable Neural Network Decoders for Higher Dimensional Quantum Codes</title>
    <summary>  Machine learning has the potential to become an important tool in quantum
error correction as it allows the decoder to adapt to the error distribution of
a quantum chip. An additional motivation for using neural networks is the fact
that they can be evaluated by dedicated hardware which is very fast and
consumes little power. Machine learning has been previously applied to decode
the surface code. However, these approaches are not scalable as the training
has to be redone for every system size which becomes increasingly difficult. In
this work the existence of local decoders for higher dimensional codes leads us
to use a low-depth convolutional neural network to locally assign a likelihood
of error on each qubit. For noiseless syndrome measurements, numerical
simulations show that the decoder has a threshold of around $7.1\%$ when
applied to the 4D toric code. When the syndrome measurements are noisy, the
decoder performs better for larger code sizes when the error probability is
low. We also give theoretical and numerical analysis to show how a
convolutional neural network is different from the 1-nearest neighbor
algorithm, which is a baseline machine learning method.
</summary>
    <author>
      <name>Nikolas P. Breuckmann</name>
    </author>
    <author>
      <name>Xiaotong Ni</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 17 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.09489v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.09489v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.09511v2</id>
    <updated>2017-11-16T21:25:25Z</updated>
    <published>2017-10-26T02:01:12Z</published>
    <title>InterpNET: Neural Introspection for Interpretable Deep Learning</title>
    <summary>  Humans are able to explain their reasoning. On the contrary, deep neural
networks are not. This paper attempts to bridge this gap by introducing a new
way to design interpretable neural networks for classification, inspired by
physiological evidence of the human visual system's inner-workings. This paper
proposes a neural network design paradigm, termed InterpNET, which can be
combined with any existing classification architecture to generate natural
language explanations of the classifications. The success of the module relies
on the assumption that the network's computation and reasoning is represented
in its internal layer activations. While in principle InterpNET could be
applied to any existing classification architecture, it is evaluated via an
image classification and explanation task. Experiments on a CUB bird
classification and explanation dataset show qualitatively and quantitatively
that the model is able to generate high-quality explanations. While the current
state-of-the-art METEOR score on this dataset is 29.2, InterpNET achieves a
much higher METEOR score of 37.9.
</summary>
    <author>
      <name>Shane Barratt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at NIPS 2017 Symposium on Interpretable Machine Learning</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.09511v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.09511v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.10403v1</id>
    <updated>2017-10-28T06:59:18Z</updated>
    <published>2017-10-28T06:59:18Z</published>
    <title>Trainable back-propagated functional transfer matrices</title>
    <summary>  Connections between nodes of fully connected neural networks are usually
represented by weight matrices. In this article, functional transfer matrices
are introduced as alternatives to the weight matrices: Instead of using real
weights, a functional transfer matrix uses real functions with trainable
parameters to represent connections between nodes. Multiple functional transfer
matrices are then stacked together with bias vectors and activations to form
deep functional transfer neural networks. These neural networks can be trained
within the framework of back-propagation, based on a revision of the delta
rules and the error transmission rule for functional connections. In
experiments, it is demonstrated that the revised rules can be used to train a
range of functional connections: 20 different functions are applied to neural
networks with up to 10 hidden layers, and most of them gain high test
accuracies on the MNIST database. It is also demonstrated that a functional
transfer matrix with a memory function can roughly memorise a non-cyclical
sequence of 400 digits.
</summary>
    <author>
      <name>Cheng-Hao Cai</name>
    </author>
    <author>
      <name>Yanyan Xu</name>
    </author>
    <author>
      <name>Dengfeng Ke</name>
    </author>
    <author>
      <name>Kaile Su</name>
    </author>
    <author>
      <name>Jing Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">39 pages, 4 figures, submitted as a journal article</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.10403v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.10403v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.11272v1</id>
    <updated>2017-10-30T23:45:57Z</updated>
    <published>2017-10-30T23:45:57Z</published>
    <title>Empirical analysis of non-linear activation functions for Deep Neural
  Networks in classification tasks</title>
    <summary>  We provide an overview of several non-linear activation functions in a neural
network architecture that have proven successful in many machine learning
applications. We conduct an empirical analysis on the effectiveness of using
these function on the MNIST classification task, with the aim of clarifying
which functions produce the best results overall. Based on this first set of
results, we examine the effects of building deeper architectures with an
increasing number of hidden layers. We also survey the impact of using, on the
same task, different initialisation schemes for the weights of our neural
network. Using these sets of experiments as a base, we conclude by providing a
optimal neural network architecture that yields impressive results in accuracy
on the MNIST classification task.
</summary>
    <author>
      <name>Giovanni Alcantara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 23 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.11272v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.11272v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.00449v2</id>
    <updated>2018-01-31T17:03:31Z</updated>
    <published>2017-11-01T17:28:26Z</published>
    <title>Attacking Binarized Neural Networks</title>
    <summary>  Neural networks with low-precision weights and activations offer compelling
efficiency advantages over their full-precision equivalents. The two most
frequently discussed benefits of quantization are reduced memory consumption,
and a faster forward pass when implemented with efficient bitwise operations.
We propose a third benefit of very low-precision neural networks: improved
robustness against some adversarial attacks, and in the worst case, performance
that is on par with full-precision models. We focus on the very low-precision
case where weights and activations are both quantized to $\pm$1, and note that
stochastically quantizing weights in just one layer can sharply reduce the
impact of iterative attacks. We observe that non-scaled binary neural networks
exhibit a similar effect to the original defensive distillation procedure that
led to gradient masking, and a false notion of security. We address this by
conducting both black-box and white-box experiments with binary models that do
not artificially mask gradients.
</summary>
    <author>
      <name>Angus Galloway</name>
    </author>
    <author>
      <name>Graham W. Taylor</name>
    </author>
    <author>
      <name>Medhat Moussa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at ICLR 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.00449v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.00449v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.02704v2</id>
    <updated>2018-02-21T18:32:54Z</updated>
    <published>2017-11-08T16:38:23Z</published>
    <title>Vowel recognition with four coupled spin-torque nano-oscillators</title>
    <summary>  Substantial evidence indicates that the brain uses principles of non-linear
dynamics in neural processes, providing inspiration for computing with
nanoelectronic devices. However, training neural networks composed of dynamical
nanodevices requires finely controlling and tuning their coupled oscillations.
In this work, we show that the outstanding tunability of spintronic
nano-oscillators can solve this challenge. We successfully train a hardware
network of four spin-torque nano-oscillators to recognize spoken vowels by
tuning their frequencies according to an automatic real-time learning rule. We
show that the high experimental recognition rates stem from the high frequency
tunability of the oscillators and their mutual coupling. Our results
demonstrate that non-trivial pattern classification tasks can be achieved with
small hardware neural networks by endowing them with non-linear dynamical
features: here, oscillations and synchronization. This demonstration is a
milestone for spintronics-based neuromorphic computing.
</summary>
    <author>
      <name>Miguel Romera</name>
    </author>
    <author>
      <name>Philippe Talatchian</name>
    </author>
    <author>
      <name>Sumito Tsunegi</name>
    </author>
    <author>
      <name>Flavio Abreu Araujo</name>
    </author>
    <author>
      <name>Vincent Cros</name>
    </author>
    <author>
      <name>Paolo Bortolotti</name>
    </author>
    <author>
      <name>Kay Yakushiji</name>
    </author>
    <author>
      <name>Akio Fukushima</name>
    </author>
    <author>
      <name>Hitoshi Kubota</name>
    </author>
    <author>
      <name>Shinji Yuasa</name>
    </author>
    <author>
      <name>Damir Vodenicarevic</name>
    </author>
    <author>
      <name>Nicolas Locatelli</name>
    </author>
    <author>
      <name>Damien Querlioz</name>
    </author>
    <author>
      <name>Julie Grollier</name>
    </author>
    <link href="http://arxiv.org/abs/1711.02704v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.02704v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.03278v1</id>
    <updated>2017-11-09T07:32:30Z</updated>
    <published>2017-11-09T07:32:30Z</published>
    <title>Feed Forward and Backward Run in Deep Convolution Neural Network</title>
    <summary>  Convolution Neural Networks (CNN), known as ConvNets are widely used in many
visual imagery application, object classification, speech recognition. After
the implementation and demonstration of the deep convolution neural network in
Imagenet classification in 2012 by krizhevsky, the architecture of deep
Convolution Neural Network is attracted many researchers. This has led to the
major development in Deep learning frameworks such as Tensorflow, caffe, keras,
theno. Though the implementation of deep learning is quite possible by
employing deep learning frameworks, mathematical theory and concepts are harder
to understand for new learners and practitioners. This article is intended to
provide an overview of ConvNets architecture and to explain the mathematical
theory behind it including activation function, loss function, feedforward and
backward propagation. In this article, grey scale image is taken as input
information image, ReLU and Sigmoid activation function are considered for
developing the architecture and cross-entropy loss function are used for
computing the difference between predicted value and actual value. The
architecture is developed in such a way that it can contain one convolution
layer, one pooling layer, and multiple dense layers
</summary>
    <author>
      <name>Pushparaja Murugan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 20th International Conference on Computer Vision and Image
  Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.03278v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.03278v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.09064v1</id>
    <updated>2017-11-24T18:19:08Z</updated>
    <published>2017-11-24T18:19:08Z</published>
    <title>Efficient and Invariant Convolutional Neural Networks for Dense
  Prediction</title>
    <summary>  Convolutional neural networks have shown great success on feature extraction
from raw input data such as images. Although convolutional neural networks are
invariant to translations on the inputs, they are not invariant to other
transformations, including rotation and flip. Recent attempts have been made to
incorporate more invariance in image recognition applications, but they are not
applicable to dense prediction tasks, such as image segmentation. In this
paper, we propose a set of methods based on kernel rotation and flip to enable
rotation and flip invariance in convolutional neural networks. The kernel
rotation can be achieved on kernels of 3 $\times$ 3, while kernel flip can be
applied on kernels of any size. By rotating in eight or four angles, the
convolutional layers could produce the corresponding number of feature maps
based on eight or four different kernels. By using flip, the convolution layer
can produce three feature maps. By combining produced feature maps using
maxout, the resource requirement could be significantly reduced while still
retain the invariance properties. Experimental results demonstrate that the
proposed methods can achieve various invariance at reasonable resource
requirements in terms of both memory and time.
</summary>
    <author>
      <name>Hongyang Gao</name>
    </author>
    <author>
      <name>Shuiwang Ji</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, ICDM2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.09064v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.09064v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.10683v3</id>
    <updated>2018-02-27T03:08:14Z</updated>
    <published>2017-11-29T05:13:32Z</published>
    <title>Do Convolutional Neural Networks act as Compositional Nearest Neighbors?</title>
    <summary>  We present a simple approach based on pixel-wise nearest neighbors to
understand and interpret the internal operations of state-of-the-art neural
networks for pixel-level tasks. Specifically, we aim to understand the
synthesis and prediction mechanisms of state-of-the-art convolutional neural
networks for pixel-level tasks. To this end, we primarily analyze the synthesis
process of generative models and the prediction mechanism of discriminative
models. The main hypothesis of this work is that convolutional neural networks
for pixel-level tasks learn a fast compositional nearest neighbor synthesis or
prediction function. Our experiments on semantic segmentation and
image-to-image translation show qualitative and quantitative evidence
supporting this hypothesis.
</summary>
    <author>
      <name>Chunhui Liu</name>
    </author>
    <author>
      <name>Aayush Bansal</name>
    </author>
    <author>
      <name>Victor Fragoso</name>
    </author>
    <author>
      <name>Deva Ramanan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">statement too strong</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.10683v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.10683v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.00170v1</id>
    <updated>2017-12-01T03:14:51Z</updated>
    <published>2017-12-01T03:14:51Z</published>
    <title>Text Generation Based on Generative Adversarial Nets with Latent
  Variable</title>
    <summary>  In this paper, we propose a model using generative adversarial net (GAN) to
generate realistic text. Instead of using standard GAN, we combine variational
autoencoder (VAE) with generative adversarial net. The use of high-level latent
random variables is helpful to learn the data distribution and solve the
problem that generative adversarial net always emits the similar data. We
propose the VGAN model where the generative model is composed of recurrent
neural network and VAE. The discriminative model is a convolutional neural
network. We train the model via policy gradient. We apply the proposed model to
the task of text generation and compare it to other recent neural network based
models, such as recurrent neural network language model and SeqGAN. We evaluate
the performance of the model by calculating negative log-likelihood and the
BLEU score. We conduct experiments on three benchmark datasets, and results
show that our model outperforms other previous models.
</summary>
    <author>
      <name>Heng Wang</name>
    </author>
    <author>
      <name>Zengchang Qin</name>
    </author>
    <author>
      <name>Tao Wan</name>
    </author>
    <link href="http://arxiv.org/abs/1712.00170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.00170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.00371v1</id>
    <updated>2017-12-01T15:37:43Z</updated>
    <published>2017-12-01T15:37:43Z</published>
    <title>Deep Neural Network Detects Quantum Phase Transition</title>
    <summary>  We detect the quantum phase transition of a quantum many-body system by
mapping the observed results of the quantum state onto a neural network. In the
present study, we utilized the simplest case of a quantum many-body system,
namely a one-dimensional chain of Ising spins with the transverse Ising model.
We prepared several spin configurations, which were obtained using repeated
observations of the model for a particular strength of the transverse field, as
input data for the neural network. Although the proposed method can be employed
using experimental observations of quantum many-body systems, we tested our
technique with spin configurations generated by a quantum Monte Carlo
simulation without initial relaxation. The neural network successfully
classified the strength of transverse field only from the spin configurations,
leading to consistent estimations of the critical point of our model $\Gamma_c
=J$.
</summary>
    <author>
      <name>Shunta Arai</name>
    </author>
    <author>
      <name>Masayuki Ohzeki</name>
    </author>
    <author>
      <name>Kazuyuki Tanaka</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.7566/JPSJ.87.033001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.7566/JPSJ.87.033001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4pages,3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J. Phys. Soc. Jpn., Vol.87, No.3,2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1712.00371v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.00371v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.01340v1</id>
    <updated>2017-12-04T20:39:05Z</updated>
    <published>2017-12-04T20:39:05Z</published>
    <title>Precision Scaling of Neural Networks for Efficient Audio Processing</title>
    <summary>  While deep neural networks have shown powerful performance in many audio
applications, their large computation and memory demand has been a challenge
for real-time processing. In this paper, we study the impact of scaling the
precision of neural networks on the performance of two common audio processing
tasks, namely, voice-activity detection and single-channel speech enhancement.
We determine the optimal pair of weight/neuron bit precision by exploring its
impact on both the performance and processing time. Through experiments
conducted with real user data, we demonstrate that deep neural networks that
use lower bit precision significantly reduce the processing time (up to 30x).
However, their performance impact is low (&lt; 3.14%) only in the case of
classification tasks such as those present in voice activity detection.
</summary>
    <author>
      <name>Jong Hwan Ko</name>
    </author>
    <author>
      <name>Josh Fromm</name>
    </author>
    <author>
      <name>Matthai Philipose</name>
    </author>
    <author>
      <name>Ivan Tashev</name>
    </author>
    <author>
      <name>Shuayb Zarar</name>
    </author>
    <link href="http://arxiv.org/abs/1712.01340v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.01340v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.01639v1</id>
    <updated>2017-11-15T00:06:54Z</updated>
    <published>2017-11-15T00:06:54Z</published>
    <title>Can CNNs Construct Highly Accurate Model Efficiently with Limited
  Training Samples?</title>
    <summary>  It is well known that metamodel or surrogate modeling techniques have been
widely applied in engineering problems due to their higher efficiency. However,
with the increase of the linearity and dimensions, it is difficult for the
present popular metamodeling techniques to construct reliable metamodel and
apply to more and more complicated high dimensional problems. Recently, neural
networks (NNs), especially deep neural networks (DNNs) have been widely
recognized as feasible and effective tools for multidiscipline. Actually, some
popular NNs, such as back propagation neural networks (BPNNs) can be regarded
as a kind of metamodeling techniques. However, for high dimensional problems,
it seems difficult for a BPNN to construct a metamodel. In this study, to
construct the high accurate metamodel efficiently, another powerful NN,
convolutional neural networks (CNNs) are introduced to construct metamodels.
Considering the distinctive characteristic of the CNNs, the CNNs are considered
to be a potential modeling tool to handle highly nonlinear and dimensional
problems with the limited training samples.
</summary>
    <author>
      <name>Yu Li</name>
    </author>
    <author>
      <name>Hu Wang</name>
    </author>
    <author>
      <name>Juanjuan Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1712.01639v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.01639v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.02198v2</id>
    <updated>2017-12-18T01:37:37Z</updated>
    <published>2017-11-19T06:22:20Z</published>
    <title>Lung Nodule Classification by the Combination of Fusion Classifier and
  Cascaded Convolutional Neural Networks</title>
    <summary>  Lung nodule classification is a class imbalanced problem, as nodules are
found with much lower frequency than non-nodules. In the class imbalanced
problem, conventional classifiers tend to be overwhelmed by the majority class
and ignore the minority class. We showed that cascaded convolutional neural
networks can classify the nodule candidates precisely for a class imbalanced
nodule candidate data set in our previous study. In this paper, we propose
Fusion classifier in conjunction with the cascaded convolutional neural network
models. To fuse the models, nodule probabilities are calculated by using the
convolutional neural network models at first. Then, Fusion classifier is
trained and tested by the nodule probabilities. The proposed method achieved
the sensitivity of 94.4% and 95.9% at 4 and 8 false positives per scan in Free
Receiver Operating Characteristics (FROC) curve analysis, respectively.
</summary>
    <author>
      <name>Masaharu Sakamoto</name>
    </author>
    <author>
      <name>Hiroki Nakano</name>
    </author>
    <author>
      <name>Kun Zhao</name>
    </author>
    <author>
      <name>Taro Sekiyama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Draft of ISBI2018. arXiv admin note: text overlap with
  arXiv:1703.00311</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.02198v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.02198v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.02734v1</id>
    <updated>2017-12-07T17:25:48Z</updated>
    <published>2017-12-07T17:25:48Z</published>
    <title>ChemNet: A Transferable and Generalizable Deep Neural Network for
  Small-Molecule Property Prediction</title>
    <summary>  With access to large datasets, deep neural networks (DNN) have achieved
human-level accuracy in image and speech recognition tasks. However, in
chemistry, availability of large standardized and labelled datasets is scarce,
and many chemical properties of research interest, chemical data is inherently
small and fragmented. In this work, we explore transfer learning techniques in
conjunction with the existing Chemception CNN model, to create a transferable
and generalizable deep neural network for small-molecule property prediction.
Our latest model, ChemNet learns in a semi-supervised manner from inexpensive
labels computed from the ChEMBL database. When fine-tuned to the Tox21, HIV and
FreeSolv dataset, which are 3 separate chemical properties that ChemNet was not
originally trained on, we demonstrate that ChemNet exceeds the performance of
existing Chemception models and other contemporary DNN models. Furthermore, as
ChemNet has been pre-trained on a large diverse chemical database, it can be
used as a general-purpose plug-and-play deep neural network for the prediction
of novel small-molecule chemical properties.
</summary>
    <author>
      <name>Garrett B. Goh</name>
    </author>
    <author>
      <name>Charles Siegel</name>
    </author>
    <author>
      <name>Abhinav Vishnu</name>
    </author>
    <author>
      <name>Nathan O. Hodas</name>
    </author>
    <link href="http://arxiv.org/abs/1712.02734v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.02734v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.03049v1</id>
    <updated>2017-12-08T13:12:04Z</updated>
    <published>2017-12-08T13:12:04Z</published>
    <title>Artificial Neural Networks that Learn to Satisfy Logic Constraints</title>
    <summary>  Logic-based problems such as planning, theorem proving, or puzzles, typically
involve combinatoric search and structured knowledge representation. Artificial
neural networks are very successful statistical learners, however, for many
years, they have been criticized for their weaknesses in representing and in
processing complex structured knowledge which is crucial for combinatoric
search and symbol manipulation. Two neural architectures are presented, which
can encode structured relational knowledge in neural activation, and store
bounded First Order Logic constraints in connection weights. Both architectures
learn to search for a solution that satisfies the constraints. Learning is done
by unsupervised practicing on problem instances from the same domain, in a way
that improves the network-solving speed. No teacher exists to provide answers
for the problem instances of the training and test sets. However, the domain
constraints are provided as prior knowledge to a loss function that measures
the degree of constraint violations. Iterations of activation calculation and
learning are executed until a solution that maximally satisfies the constraints
emerges on the output units. As a test case, block-world planning problems are
used to train networks that learn to plan in that domain, but the techniques
proposed could be used more generally as in integrating prior symbolic
knowledge with statistical learning
</summary>
    <author>
      <name>Gadi Pinkas</name>
    </author>
    <author>
      <name>Shimon Cohen</name>
    </author>
    <link href="http://arxiv.org/abs/1712.03049v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.03049v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.03249v1</id>
    <updated>2017-12-08T19:05:50Z</updated>
    <published>2017-12-08T19:05:50Z</published>
    <title>Social Emotion Mining Techniques for Facebook Posts Reaction Prediction</title>
    <summary>  As of February 2016 Facebook allows users to express their experienced
emotions about a post by using five so-called `reactions'. This research paper
proposes and evaluates alternative methods for predicting these reactions to
user posts on public pages of firms/companies (like supermarket chains). For
this purpose, we collected posts (and their reactions) from Facebook pages of
large supermarket chains and constructed a dataset which is available for other
researches. In order to predict the distribution of reactions of a new post,
neural network architectures (convolutional and recurrent neural networks) were
tested using pretrained word embeddings. Results of the neural networks were
improved by introducing a bootstrapping approach for sentiment and emotion
mining on the comments for each post. The final model (a combination of neural
network and a baseline emotion miner) is able to predict the reaction
distribution on Facebook posts with a mean squared error (or misclassification
rate) of 0.135.
</summary>
    <author>
      <name>Florian Krebs</name>
    </author>
    <author>
      <name>Bruno Lubascher</name>
    </author>
    <author>
      <name>Tobias Moers</name>
    </author>
    <author>
      <name>Pieter Schaap</name>
    </author>
    <author>
      <name>Gerasimos Spanakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 13 figures and accepted at ICAART 2018. (Dataset:
  https://github.com/jerryspan/FacebookR)</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.03249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.03249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.05304v1</id>
    <updated>2017-12-14T15:57:56Z</updated>
    <published>2017-12-14T15:57:56Z</published>
    <title>A quantum algorithm to train neural networks using low-depth circuits</title>
    <summary>  The question has remained open if near-term gate model quantum computers will
offer a quantum advantage for practical applications in the pre-fault tolerance
noise regime. A class of algorithms which have shown some promise in this
regard are the so-called classical-quantum hybrid variational algorithms. Here
we develop a low-depth quantum algorithm to train quantum Boltzmann machine
neural networks using such variational methods. We introduce a method which
employs the quantum approximate optimization algorithm as a subroutine in order
to approximately sample from Gibbs states of Ising Hamiltonians. We use this
approximate Gibbs sampling to train neural networks for which we demonstrate
training convergence for numerically simulated noisy circuits with depolarizing
errors of rates of up to 4%.
</summary>
    <author>
      <name>Guillaume Verdon</name>
    </author>
    <author>
      <name>Michael Broughton</name>
    </author>
    <author>
      <name>Jacob Biamonte</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.05304v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.05304v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.07107v2</id>
    <updated>2018-01-05T15:51:54Z</updated>
    <published>2017-12-19T18:44:07Z</published>
    <title>Adversarial Examples: Attacks and Defenses for Deep Learning</title>
    <summary>  With rapid progress and great successes in a wide spectrum of applications,
deep learning is being applied in many safety-critical environments. However,
deep neural networks have been recently found vulnerable to well-designed input
samples, called \textit{adversarial examples}. Adversarial examples are
imperceptible to human but can easily fool deep neural networks in the
testing/deploying stage. The vulnerability to adversarial examples becomes one
of the major risks for applying deep neural networks in safety-critical
scenarios. Therefore, the attacks and defenses on adversarial examples draw
great attention.
  In this paper, we review recent findings on adversarial examples against deep
neural networks, summarize the methods for generating adversarial examples, and
propose a taxonomy of these methods. Under the taxonomy, applications and
countermeasures for adversarial examples are investigated. We further elaborate
on adversarial examples and explore the challenges and the potential solutions.
</summary>
    <author>
      <name>Xiaoyong Yuan</name>
    </author>
    <author>
      <name>Pan He</name>
    </author>
    <author>
      <name>Qile Zhu</name>
    </author>
    <author>
      <name>Rajendra Rana Bhat</name>
    </author>
    <author>
      <name>Xiaolin Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 13 figures, Github:
  https://github.com/chbrian/awesome-adversarial-examples-dl</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.07107v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.07107v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.09203v3</id>
    <updated>2018-02-22T03:50:08Z</updated>
    <published>2017-12-26T08:04:43Z</published>
    <title>Algorithmic Regularization in Over-parameterized Matrix Sensing and
  Neural Networks with Quadratic Activations</title>
    <summary>  We show that the (stochastic) gradient descent algorithm provides an implicit
regularization effect in the learning of over-parameterized matrix
factorization models and one-hidden-layer neural networks with quadratic
activations. Concretely, we show that given $\tilde{O}(dr^{2})$ random linear
measurements of a rank $r$ positive semidefinite matrix $X^{\star}$, we can
recover $X^{\star}$ by parameterizing it by $UU^\top$ with $U\in
\mathbb{R}^{d\times d}$ and minimizing the squared loss, even if $r \ll d$. We
prove that starting from a small initialization, gradient descent recovers
$X^{\star}$ in $\tilde{O}(\sqrt{r})$ iterations approximately. The results
solve the conjecture of Gunasekar et al.'17 under the restricted isometry
property. The technique can be applied to analyzing neural networks with
quadratic activations with some technical modifications.
</summary>
    <author>
      <name>Yuanzhi Li</name>
    </author>
    <author>
      <name>Tengyu Ma</name>
    </author>
    <author>
      <name>Hongyang Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">changed the title and added new results on neural networks with
  quadratic activations</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.09203v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.09203v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.09592v1</id>
    <updated>2017-12-27T14:45:40Z</updated>
    <published>2017-12-27T14:45:40Z</published>
    <title>An Artificial Neural Network-based Stock Trading System Using Technical
  Analysis and Big Data Framework</title>
    <summary>  In this paper, a neural network-based stock price prediction and trading
system using technical analysis indicators is presented. The model developed
first converts the financial time series data into a series of buy-sell-hold
trigger signals using the most commonly preferred technical analysis
indicators. Then, a Multilayer Perceptron (MLP) artificial neural network (ANN)
model is trained in the learning stage on the daily stock prices between 1997
and 2007 for all of the Dow30 stocks. Apache Spark big data framework is used
in the training stage. The trained model is then tested with data from 2007 to
2017. The results indicate that by choosing the most appropriate technical
indicators, the neural network model can achieve comparable results against the
Buy and Hold strategy in most of the cases. Furthermore, fine tuning the
technical indicators and/or optimization strategy can enhance the overall
trading performance.
</summary>
    <author>
      <name>O. B. Sezer</name>
    </author>
    <author>
      <name>M. Ozbayoglu</name>
    </author>
    <author>
      <name>E. Dogdu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3077286.3077294</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3077286.3077294" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Southeast Conference, ACMSE 2017, Kennesaw State University, GA,
  U.S.A., 13-15 April, 2017</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Southeast Conference, ACMSE 2017, Kennesaw State University,
  GA, U.S.A., 13-15 April, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1712.09592v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.09592v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.TR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68TXX" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.04326v2</id>
    <updated>2018-01-29T22:20:26Z</updated>
    <published>2018-01-12T21:43:56Z</published>
    <title>Not All Ops Are Created Equal!</title>
    <summary>  Efficient and compact neural network models are essential for enabling the
deployment on mobile and embedded devices. In this work, we point out that
typical design metrics for gauging the efficiency of neural network
architectures -- total number of operations and parameters -- are not
sufficient. These metrics may not accurately correlate with the actual
deployment metrics such as energy and memory footprint. We show that throughput
and energy varies by up to 5X across different neural network operation types
on an off-the-shelf Arm Cortex-M7 microcontroller. Furthermore, we show that
the memory required for activation data also need to be considered, apart from
the model parameters, for network architecture exploration studies.
</summary>
    <author>
      <name>Liangzhen Lai</name>
    </author>
    <author>
      <name>Naveen Suda</name>
    </author>
    <author>
      <name>Vikas Chandra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at SysML Conference 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.04326v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.04326v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.08267v1</id>
    <updated>2018-01-25T03:37:02Z</updated>
    <published>2018-01-25T03:37:02Z</published>
    <title>Visual Weather Temperature Prediction</title>
    <summary>  In this paper, we attempt to employ convolutional recurrent neural networks
for weather temperature estimation using only image data. We study ambient
temperature estimation based on deep neural networks in two scenarios a)
estimating temperature of a single outdoor image, and b) predicting temperature
of the last image in an image sequence. In the first scenario, visual features
are extracted by a convolutional neural network trained on a large-scale image
dataset. We demonstrate that promising performance can be obtained, and analyze
how volume of training data influences performance. In the second scenario, we
consider the temporal evolution of visual appearance, and construct a recurrent
neural network to predict the temperature of the last image in a given image
sequence. We obtain better prediction accuracy compared to the state-of-the-art
models. Further, we investigate how performance varies when information is
extracted from different scene regions, and when images are captured in
different daytime hours. Our approach further reinforces the idea of using only
visual information for cost efficient weather prediction in the future.
</summary>
    <author>
      <name>Wei-Ta Chu</name>
    </author>
    <author>
      <name>Kai-Chia Ho</name>
    </author>
    <author>
      <name>Ali Borji</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, accepted to WACV 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.08267v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.08267v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.09655v2</id>
    <updated>2018-02-12T09:03:00Z</updated>
    <published>2018-01-29T18:16:14Z</published>
    <title>Real space mapping of topological invariants using artificial neural
  networks</title>
    <summary>  Topological invariants allow to characterize Hamiltonians, predicting the
existence of topologically protected in-gap modes. Those invariants are
generically computed for infinite systems with trans- lational symmetry, by
tracing the evolution of the occupied wavefunctions under twisted boundary
conditions. However, those procedures do not allow to calculate a topological
invariant by evaluating the system locally, nor allow to determine it in the
absence of translational symmetry. Here we show that artificial neural networks
can be trained to identify the topological order by evaluating a local
projection of the density matrix. We demonstrate this for two different models,
a 1-D topological su- perconductor and a 2-D quantum anomalous Hall state, both
with spatially modulated parameters. Our neural network correctly identifies
the different topological domains in real space, predicting the location of
in-gap states. By combining a neural network with a calculation of the
electronic states that uses the Kernel Polynomial Method, we show that the
local evaluation of the invariant can be carried out for systems without
translational symmetry consisting of tens of thousands of atoms. Our results
show that supervised learning is an efficient methodology to characterize the
local topology of a system.
</summary>
    <author>
      <name>D. Carvalho</name>
    </author>
    <author>
      <name>N. A. Garcia-Martinez</name>
    </author>
    <author>
      <name>J. L. Lado</name>
    </author>
    <author>
      <name>J. Fernandez-Rossier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.09655v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.09655v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.mes-hall" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mes-hall" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.09782v2</id>
    <updated>2018-01-31T12:58:43Z</updated>
    <published>2018-01-29T22:14:57Z</published>
    <title>Prediction of Pedestrian Speed with Artificial Neural Networks</title>
    <summary>  Pedestrian behaviours tend to depend on the type of facility. Therefore
accurate predictions of pedestrians movements in complex geometries (including
corridor, bottleneck or intersection) are difficult to achieve for classical
models with few parameters. Artificial neural networks have multiple parameters
and are able to identify various types of patterns. They could be a suitable
alternative for forecasts. We aim in this paper to present first steps testing
this approach. We compare estimations of pedestrian speed with a classical
model and a neural network for combinations of corridor and bottleneck
experiments. The results show that the neural network is able to differentiate
the two geometries and to improve the estimation of pedestrian speeds when the
geometries are mixed.
</summary>
    <author>
      <name>Antoine Tordeux</name>
    </author>
    <author>
      <name>Mohcine Chraibi</name>
    </author>
    <author>
      <name>Armin Seyfried</name>
    </author>
    <author>
      <name>Andreas Schadschneider</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures; Traffic and Granular Flow 2017 Conference
  (TGF'17)</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.09782v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.09782v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00560v1</id>
    <updated>2018-02-02T05:09:10Z</updated>
    <published>2018-02-02T05:09:10Z</published>
    <title>Interpretable Deep Convolutional Neural Networks via Meta-learning</title>
    <summary>  Model interpretability is a requirement in many applications in which crucial
decisions are made by users relying on a model's outputs. The recent movement
for "algorithmic fairness" also stipulates explainability, and therefore
interpretability of learning models. And yet the most successful contemporary
Machine Learning approaches, the Deep Neural Networks, produce models that are
highly non-interpretable. We attempt to address this challenge by proposing a
technique called CNN-INTE to interpret deep Convolutional Neural Networks (CNN)
via meta-learning. In this work, we interpret a specific hidden layer of the
deep CNN model on the MNIST image dataset. We use a clustering algorithm in a
two-level structure to find the meta-level training data and Random Forest as
base learning algorithms to generate the meta-level test data. The
interpretation results are displayed visually via diagrams, which clearly
indicates how a specific test instance is classified. Our method achieves
global interpretation for all the test instances without sacrificing the
accuracy obtained by the original deep CNN model. This means our model is
faithful to the deep CNN model, which leads to reliable interpretations.
</summary>
    <author>
      <name>Xuan Liu</name>
    </author>
    <author>
      <name>Xiaoguang Wang</name>
    </author>
    <author>
      <name>Stan Matwin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 9 figures, submitted to the 2018 International Joint
  Conference on Neural Networks</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.00560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.06259v1</id>
    <updated>2018-02-17T16:47:32Z</updated>
    <published>2018-02-17T16:47:32Z</published>
    <title>Exact and Consistent Interpretation for Piecewise Linear Neural
  Networks: A Closed Form Solution</title>
    <summary>  Strong intelligent machines powered by deep neural networks are increasingly
deployed as black boxes to make decisions in risk-sensitive domains, such as
finance and medical. To reduce potential risk and build trust with users, it is
critical to interpret how such machines make their decisions. Existing works
interpret a pre-trained neural network by analyzing hidden neurons, mimicking
pre-trained models or approximating local predictions. However, these methods
do not provide a guarantee on the exactness and consistency of their
interpretation. In this paper, we propose an elegant closed form solution named
$OpenBox$ to compute exact and consistent interpretations for the family of
Piecewise Linear Neural Networks (PLNN). The major idea is to first transform a
PLNN into a mathematically equivalent set of linear classifiers, then interpret
each linear classifier by the features that dominate its prediction. We further
apply $OpenBox$ to demonstrate the effectiveness of non-negative and sparse
constraints on improving the interpretability of PLNNs. The extensive
experiments on both synthetic and real world data sets clearly demonstrate the
exactness and consistency of our interpretation.
</summary>
    <author>
      <name>Lingyang Chu</name>
    </author>
    <author>
      <name>Xia Hu</name>
    </author>
    <author>
      <name>Juhua Hu</name>
    </author>
    <author>
      <name>Lanjun Wang</name>
    </author>
    <author>
      <name>Jian Pei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review of KDD 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.06259v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06259v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.06463v1</id>
    <updated>2018-02-18T22:49:56Z</updated>
    <published>2018-02-18T22:49:56Z</published>
    <title>Local Geometry of One-Hidden-Layer Neural Networks for Logistic
  Regression</title>
    <summary>  We study the local geometry of a one-hidden-layer fully-connected neural
network where the training samples are generated from a multi-neuron logistic
regression model. We prove that under Gaussian input, the empirical risk
function employing quadratic loss exhibits strong convexity and smoothness
uniformly in a local neighborhood of the ground truth, for a class of smooth
activation functions satisfying certain properties, including sigmoid and tanh,
as soon as the sample complexity is sufficiently large. This implies that if
initialized in this neighborhood, gradient descent converges linearly to a
critical point that is provably close to the ground truth without requiring a
fresh set of samples at each iteration. This significantly improves upon prior
results on learning shallow neural networks with multiple neurons. To the best
of our knowledge, this is the first global convergence guarantee for
one-hidden-layer neural networks using gradient descent over the empirical risk
function without resampling at the near-optimal sampling and computational
complexity.
</summary>
    <author>
      <name>Haoyu Fu</name>
    </author>
    <author>
      <name>Yuejie Chi</name>
    </author>
    <author>
      <name>Yingbin Liang</name>
    </author>
    <link href="http://arxiv.org/abs/1802.06463v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06463v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.07260v1</id>
    <updated>2018-02-20T13:21:16Z</updated>
    <published>2018-02-20T13:21:16Z</published>
    <title>Deep Learning Classification in Asteroseismology Using an Improved
  Neural Network: Results on 15000 Kepler Red Giants and Applications to K2 and
  TESS Data</title>
    <summary>  Deep learning in the form of 1D convolutional neural networks have previously
been shown to be capable of efficiently classifying the evolutionary state of
oscillating red giants into red giant branch stars and helium-core burning
stars by recognizing visual features in their asteroseismic frequency spectra.
We elaborate further on the deep learning method by developing an improved
convolutional neural network classifier. To make our method useful for current
and future space missions such as K2, TESS and PLATO, we train classifiers that
are able to classify the evolutionary states of lower frequency resolution
spectra expected from these missions. Additionally, we provide new
classifications for 8633 Kepler red giants, out of which 426 have previously
not been classified using asteroseismology. This brings the total to 14983
Kepler red giants classified with our new neural network. We also verify that
our classifiers are remarkably robust to suboptimal data, including low
signal-to-noise and incorrect training truth labels.
</summary>
    <author>
      <name>Marc Hon</name>
    </author>
    <author>
      <name>Dennis Stello</name>
    </author>
    <author>
      <name>Jie Yu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/mnras/sty483</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/mnras/sty483" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 14 figures, 4 tables. Accepted for publication in the Main
  Journal of MNRAS. The catalogue containing updated evolutionary state
  classifications of ~16000 Kepler red giants is available as an ancillary file</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.07260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.07260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.09750v1</id>
    <updated>2018-02-27T07:25:32Z</updated>
    <published>2018-02-27T07:25:32Z</published>
    <title>Train Feedfoward Neural Network with Layer-wise Adaptive Rate via
  Approximating Back-matching Propagation</title>
    <summary>  Stochastic gradient descent (SGD) has achieved great success in training deep
neural network, where the gradient is computed through back-propagation.
However, the back-propagated values of different layers vary dramatically. This
inconsistence of gradient magnitude across different layers renders
optimization of deep neural network with a single learning rate problematic. We
introduce the back-matching propagation which computes the backward values on
the layer's parameter and the input by matching backward values on the layer's
output. This leads to solving a bunch of least-squares problems, which requires
high computational cost. We then reduce the back-matching propagation with
approximations and propose an algorithm that turns to be the regular SGD with a
layer-wise adaptive learning rate strategy. This allows an easy implementation
of our algorithm in current machine learning frameworks equipped with
auto-differentiation. We apply our algorithm in training modern deep neural
networks and achieve favorable results over SGD.
</summary>
    <author>
      <name>Huishuai Zhang</name>
    </author>
    <author>
      <name>Wei Chen</name>
    </author>
    <author>
      <name>Tie-Yan Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.09750v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.09750v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.07543v3</id>
    <updated>2016-02-28T22:04:54Z</updated>
    <published>2015-11-24T02:31:46Z</published>
    <title>Convergent Learning: Do different neural networks learn the same
  representations?</title>
    <summary>  Recent success in training deep neural networks have prompted active
investigation into the features learned on their intermediate layers. Such
research is difficult because it requires making sense of non-linear
computations performed by millions of parameters, but valuable because it
increases our ability to understand current models and create improved versions
of them. In this paper we investigate the extent to which neural networks
exhibit what we call convergent learning, which is when the representations
learned by multiple nets converge to a set of features which are either
individually similar between networks or where subsets of features span similar
low-dimensional spaces. We propose a specific method of probing
representations: training multiple networks and then comparing and contrasting
their individual, learned representations at the level of neurons or groups of
neurons. We begin research into this question using three techniques to
approximately align different neural networks on a feature level: a bipartite
matching approach that makes one-to-one assignments between neurons, a sparse
prediction approach that finds one-to-many mappings, and a spectral clustering
approach that finds many-to-many mappings. This initial investigation reveals a
few previously unknown properties of neural networks, and we argue that future
research into the question of convergent learning will yield many more. The
insights described here include (1) that some features are learned reliably in
multiple networks, yet other features are not consistently learned; (2) that
units learn to span low-dimensional subspaces and, while these subspaces are
common to multiple networks, the specific basis vectors learned are not; (3)
that the representation codes show evidence of being a mix between a local code
and slightly, but not fully, distributed codes across multiple units.
</summary>
    <author>
      <name>Yixuan Li</name>
    </author>
    <author>
      <name>Jason Yosinski</name>
    </author>
    <author>
      <name>Jeff Clune</name>
    </author>
    <author>
      <name>Hod Lipson</name>
    </author>
    <author>
      <name>John Hopcroft</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at ICLR 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.07543v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.07543v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.05695v1</id>
    <updated>2015-07-21T03:48:04Z</updated>
    <published>2015-07-21T03:48:04Z</published>
    <title>A neuromorphic hardware architecture using the Neural Engineering
  Framework for pattern recognition</title>
    <summary>  We present a hardware architecture that uses the Neural Engineering Framework
(NEF) to implement large-scale neural networks on Field Programmable Gate
Arrays (FPGAs) for performing pattern recognition in real time. NEF is a
framework that is capable of synthesising large-scale cognitive systems from
subnetworks. We will first present the architecture of the proposed neural
network implemented using fixed-point numbers and demonstrate a routine that
computes the decoding weights by using the online pseudoinverse update method
(OPIUM) in a parallel and distributed manner. The proposed system is
efficiently implemented on a compact digital neural core. This neural core
consists of 64 neurons that are instantiated by a single physical neuron using
a time-multiplexing approach. As a proof of concept, we combined 128 identical
neural cores together to build a handwritten digit recognition system using the
MNIST database and achieved a recognition rate of 96.55%. The system is
implemented on a state-of-the-art FPGA and can process 5.12 million digits per
second. The architecture is not limited to handwriting recognition, but is
generally applicable as an extremely fast pattern recognition processor for
various kinds of patterns such as speech and images.
</summary>
    <author>
      <name>Runchun Wang</name>
    </author>
    <author>
      <name>Chetan Singh Thakur</name>
    </author>
    <author>
      <name>Tara Julia Hamilton</name>
    </author>
    <author>
      <name>Jonathan Tapson</name>
    </author>
    <author>
      <name>Andre van Schaik</name>
    </author>
    <link href="http://arxiv.org/abs/1507.05695v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.05695v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.07469v1</id>
    <updated>2017-03-21T23:29:47Z</updated>
    <published>2017-03-21T23:29:47Z</published>
    <title>RobustFill: Neural Program Learning under Noisy I/O</title>
    <summary>  The problem of automatically generating a computer program from some
specification has been studied since the early days of AI. Recently, two
competing approaches for automatic program learning have received significant
attention: (1) neural program synthesis, where a neural network is conditioned
on input/output (I/O) examples and learns to generate a program, and (2) neural
program induction, where a neural network generates new outputs directly using
a latent program representation.
  Here, for the first time, we directly compare both approaches on a
large-scale, real-world learning task. We additionally contrast to rule-based
program synthesis, which uses hand-crafted semantics to guide the program
generation. Our neural models use a modified attention RNN to allow encoding of
variable-sized sets of I/O pairs. Our best synthesis model achieves 92%
accuracy on a real-world test set, compared to the 34% accuracy of the previous
best neural synthesis approach. The synthesis model also outperforms a
comparable induction model on this task, but we more importantly demonstrate
that the strength of each approach is highly dependent on the evaluation metric
and end-user application. Finally, we show that we can train our neural models
to remain very robust to the type of noise expected in real-world data (e.g.,
typos), while a highly-engineered rule-based system fails entirely.
</summary>
    <author>
      <name>Jacob Devlin</name>
    </author>
    <author>
      <name>Jonathan Uesato</name>
    </author>
    <author>
      <name>Surya Bhupatiraju</name>
    </author>
    <author>
      <name>Rishabh Singh</name>
    </author>
    <author>
      <name>Abdel-rahman Mohamed</name>
    </author>
    <author>
      <name>Pushmeet Kohli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages + 9 pages of supplementary material</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.07469v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.07469v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.7916v4</id>
    <updated>2016-06-09T14:23:52Z</updated>
    <published>2014-11-28T15:51:59Z</published>
    <title>The effect of heterogeneity on decorrelation mechanisms in spiking
  neural networks: a neuromorphic-hardware study</title>
    <summary>  High-level brain function such as memory, classification or reasoning can be
realized by means of recurrent networks of simplified model neurons. Analog
neuromorphic hardware constitutes a fast and energy efficient substrate for the
implementation of such neural computing architectures in technical applications
and neuroscientific research. The functional performance of neural networks is
often critically dependent on the level of correlations in the neural activity.
In finite networks, correlations are typically inevitable due to shared
presynaptic input. Recent theoretical studies have shown that inhibitory
feedback, abundant in biological neural networks, can actively suppress these
shared-input correlations and thereby enable neurons to fire nearly
independently. For networks of spiking neurons, the decorrelating effect of
inhibitory feedback has so far been explicitly demonstrated only for
homogeneous networks of neurons with linear sub-threshold dynamics. Theory,
however, suggests that the effect is a general phenomenon, present in any
system with sufficient inhibitory feedback, irrespective of the details of the
network structure or the neuronal and synaptic properties. Here, we investigate
the effect of network heterogeneity on correlations in sparse, random networks
of inhibitory neurons with non-linear, conductance-based synapses. Emulations
of these networks on the analog neuromorphic hardware system Spikey allow us to
test the efficiency of decorrelation by inhibitory feedback in the presence of
hardware-specific heterogeneities. The configurability of the hardware
substrate enables us to modulate the extent of heterogeneity in a systematic
manner. We selectively study the effects of shared input and recurrent
connections on correlations in membrane potentials and spike trains. Our
results confirm ...
</summary>
    <author>
      <name>Thomas Pfeil</name>
    </author>
    <author>
      <name>Jakob Jordan</name>
    </author>
    <author>
      <name>Tom Tetzlaff</name>
    </author>
    <author>
      <name>Andreas Gr√ºbl</name>
    </author>
    <author>
      <name>Johannes Schemmel</name>
    </author>
    <author>
      <name>Markus Diesmann</name>
    </author>
    <author>
      <name>Karlheinz Meier</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevX.6.021023</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevX.6.021023" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 10 figures, supplements</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. X 6, 021023 (2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1411.7916v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.7916v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.6120v3</id>
    <updated>2014-02-19T17:26:57Z</updated>
    <published>2013-12-20T20:24:00Z</published>
    <title>Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks</title>
    <summary>  Despite the widespread practical success of deep learning methods, our
theoretical understanding of the dynamics of learning in deep neural networks
remains quite sparse. We attempt to bridge the gap between the theory and
practice of deep learning by systematically analyzing learning dynamics for the
restricted case of deep linear neural networks. Despite the linearity of their
input-output map, such networks have nonlinear gradient descent dynamics on
weights that change with the addition of each new hidden layer. We show that
deep linear networks exhibit nonlinear learning phenomena similar to those seen
in simulations of nonlinear networks, including long plateaus followed by rapid
transitions to lower error solutions, and faster convergence from greedy
unsupervised pretraining initial conditions than from random initial
conditions. We provide an analytical description of these phenomena by finding
new exact solutions to the nonlinear dynamics of deep learning. Our theoretical
analysis also reveals the surprising finding that as the depth of a network
approaches infinity, learning speed can nevertheless remain finite: for a
special class of initial conditions on the weights, very deep networks incur
only a finite, depth independent, delay in learning speed relative to shallow
networks. We show that, under certain conditions on the training data,
unsupervised pretraining can find this special class of initial conditions,
while scaled random Gaussian initializations cannot. We further exhibit a new
class of random orthogonal initial conditions on weights that, like
unsupervised pre-training, enjoys depth independent learning times. We further
show that these initial conditions also lead to faithful propagation of
gradients even in deep nonlinear networks, as long as they operate in a special
regime known as the edge of chaos.
</summary>
    <author>
      <name>Andrew M. Saxe</name>
    </author>
    <author>
      <name>James L. McClelland</name>
    </author>
    <author>
      <name>Surya Ganguli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submission to ICLR2014. Revised based on reviewer feedback</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.6120v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.6120v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07383v2</id>
    <updated>2016-05-26T15:42:02Z</updated>
    <published>2016-05-24T11:22:08Z</published>
    <title>From Local Chaos to Critical Slowing Down: A Theory of the Functional
  Connectivity of Small Neural Circuits</title>
    <summary>  Functional connectivity is a fundamental property of neural networks that
quantifies the segregation and integration of information between cortical
areas. Due to mathematical complexity, a theory that could explain how the
parameters of mesoscopic networks composed of a few tens of neurons affect the
functional connectivity is still to be formulated. Yet, many interesting
problems in neuroscience involve the study of networks composed of a small
number of neurons. Based on a recent study of the dynamics of small neural
circuits, we combine the analysis of local bifurcations of multi-population
neural networks of arbitrary size with the analytical calculation of the
functional connectivity. We study the functional connectivity in different
regimes, showing that external stimuli cause the network to switch from
asynchronous states characterized by weak correlation and low variability
(local chaos), to synchronous states characterized by strong correlations and
wide temporal fluctuations (critical slowing down). Local chaos typically
occurs in large networks, but here we show that it can also be generated by
strong stimuli in small neural circuits. On the other side, critical slowing
down is expected to occur when the stimulus moves the network close to a local
bifurcation. In particular, strongly positive correlations occur at the
saddle-node and Andronov-Hopf bifurcations of the network, while strongly
negative correlations occur when the network undergoes a spontaneous
symmetry-breaking at the branching-point bifurcations. These results prove that
the functional connectivity of firing-rate network models is strongly affected
by the external stimuli even if the anatomical connections are fixed, and
suggest an effective mechanism through which biological networks can
dynamically modulate the encoding and integration of sensory information.
</summary>
    <author>
      <name>Diego Fasoli</name>
    </author>
    <author>
      <name>Anna Cattani</name>
    </author>
    <author>
      <name>Stefano Panzeri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 5 figures; compiled version of the Supplementary Materials
  added (12 pages)</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07383v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07383v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00825v2</id>
    <updated>2016-07-20T22:42:41Z</updated>
    <published>2016-06-02T19:48:22Z</published>
    <title>Training a Hidden Markov Model with a Bayesian Spiking Neural Network</title>
    <summary>  It is of some interest to understand how statistically based mechanisms for
signal processing might be integrated with biologically motivated mechanisms
such as neural networks. This paper explores a novel hybrid approach for
classifying segments of sequential data, such as individual spoken works. The
approach combines a hidden Markov model (HMM) with a spiking neural network
(SNN). The HMM, consisting of states and transitions, forms a fixed backbone
with nonadaptive transition probabilities. The SNN, however, implements a
biologically based Bayesian computation that derives from the spike
timing-dependent plasticity (STDP) learning rule. The emission (observation)
probabilities of the HMM are represented in the SNN and trained with the STDP
rule. A separate SNN, each with the same architecture, is associated with each
of the states of the HMM. Because of the STDP training, each SNN implements an
expectation maximization algorithm to learn the emission probabilities for one
HMM state. The model was studied on synthesized spike-train data and also on
spoken word data. Preliminary results suggest its performance compares
favorably with other biologically motivated approaches. Because of the model's
uniqueness and initial promise, it warrants further study. It provides some new
ideas on how the brain might implement the equivalent of an HMM in a neural
circuit.
</summary>
    <author>
      <name>Amirhossein Tavanaei</name>
    </author>
    <author>
      <name>Anthony S Maida</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Bayesian Spiking Neural Network, Revision submitted: April-27-2016</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Signal Processing Systems, (2016), 1-10</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.00825v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00825v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00522v1</id>
    <updated>2017-03-01T21:41:09Z</updated>
    <published>2017-03-01T21:41:09Z</published>
    <title>Understanding Synthetic Gradients and Decoupled Neural Interfaces</title>
    <summary>  When training neural networks, the use of Synthetic Gradients (SG) allows
layers or modules to be trained without update locking - without waiting for a
true error gradient to be backpropagated - resulting in Decoupled Neural
Interfaces (DNIs). This unlocked ability of being able to update parts of a
neural network asynchronously and with only local information was demonstrated
to work empirically in Jaderberg et al (2016). However, there has been very
little demonstration of what changes DNIs and SGs impose from a functional,
representational, and learning dynamics point of view. In this paper, we study
DNIs through the use of synthetic gradients on feed-forward networks to better
understand their behaviour and elucidate their effect on optimisation. We show
that the incorporation of SGs does not affect the representational strength of
the learning system for a neural network, and prove the convergence of the
learning system for linear and deep linear models. On practical problems we
investigate the mechanism by which synthetic gradient estimators approximate
the true loss, and, surprisingly, how that leads to drastically different
layer-wise representations. Finally, we also expose the relationship of using
synthetic gradients to other error approximation techniques and find a unifying
language for discussion and comparison.
</summary>
    <author>
      <name>Wojciech Marian Czarnecki</name>
    </author>
    <author>
      <name>Grzegorz ≈öwirszcz</name>
    </author>
    <author>
      <name>Max Jaderberg</name>
    </author>
    <author>
      <name>Simon Osindero</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Koray Kavukcuoglu</name>
    </author>
    <link href="http://arxiv.org/abs/1703.00522v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00522v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09046v1</id>
    <updated>2017-07-25T23:35:17Z</updated>
    <published>2017-07-25T23:35:17Z</published>
    <title>Time Divergence-Convergence Learning Scheme in Multi-Layer Dynamic
  Synapse Neural Networks</title>
    <summary>  A new learning scheme called time divergence-convergence (TDC) is proposed
for two-layer dynamic synapse neural networks (DSNN). DSNN is an artificial
neural network model, in which the synaptic transmission is modeled by a
dynamic process and the information between neurons are transmitted through
spike timing. In TDC, the intra-layer neurons of a DSNN are trained to map
input spike trains to a higher dimension of spike trains called a
feature-domain, and the output neurons are trained to build the desired spike
trains by processing the spike timing of intralayer neurons. The DSNN
performance was examined in a jittered spike train classification task which
shows more than 92\% accuracy in classifying different spike trains. The DSNN
performance is comparable with the recurrent multi-layer neural networks and
surpasses a single-layer DSNN with a 22\% margin. Synaptic dynamics have been
proposed as the neural substrate for sub-second temporal processing; we can
utilize TDC to train a DSNN to perform diverse forms of sub-second temporal
processing. The TDC learning proposed here is scalable in terms of the synaptic
adaptation of deeper layers of multi-layer DSNNs. The DSNN along with TDC
learning proposed here can be used in to replicate the processing observed in
neural circuitry and in pattern recognition tasks.
</summary>
    <author>
      <name>Ali Yousefi</name>
    </author>
    <author>
      <name>Theodore W. Berger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.09046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="00-11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.0980v1</id>
    <updated>2008-11-06T16:04:08Z</updated>
    <published>2008-11-06T16:04:08Z</published>
    <title>Self-organized criticality and adaptation in discrete dynamical networks</title>
    <summary>  It has been proposed that adaptation in complex systems is optimized at the
critical boundary between ordered and disordered dynamical regimes. Here, we
review models of evolving dynamical networks that lead to self-organization of
network topology based on a local coupling between a dynamical order parameter
and rewiring of network connectivity, with convergence towards criticality in
the limit of large network size $N$. In particular, two adaptive schemes are
discussed and compared in the context of Boolean Networks and Threshold
Networks: 1) Active nodes loose links, frozen nodes aquire new links, 2) Nodes
with correlated activity connect, de-correlated nodes disconnect. These simple
local adaptive rules lead to co-evolution of network topology and -dynamics.
Adaptive networks are strikingly different from random networks: They evolve
inhomogeneous topologies and broad plateaus of homeostatic regulation,
dynamical activity exhibits $1/f$ noise and attractor periods obey a scale-free
distribution. The proposed co-evolutionary mechanism of topological
self-organization is robust against noise and does not depend on the details of
dynamical transition rules. Using finite-size scaling, it is shown that
networks converge to a self-organized critical state in the thermodynamic
limit. Finally, we discuss open questions and directions for future research,
and outline possible applications of these models to adaptive systems in
diverse areas.
</summary>
    <author>
      <name>Thimo Rohlf</name>
    </author>
    <author>
      <name>Stefan Bornholdt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 19 figures. Review article to appear in: "Adaptive Networks
  - Theory, Models and Applications", eds. T. Gross and H. Sayama, Springer
  2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0811.0980v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.0980v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.01238v4</id>
    <updated>2018-02-23T17:58:41Z</updated>
    <published>2015-11-04T08:25:56Z</published>
    <title>The wisdom of networks: A general adaptation and learning mechanism of
  complex systems: The network core triggers fast responses to known stimuli;
  innovations require the slow network periphery and are encoded by
  core-remodeling</title>
    <summary>  I hypothesize that re-occurring prior experience of complex systems mobilizes
a fast response, whose attractor is encoded by their strongly connected network
core. In contrast, responses to novel stimuli are often slow and require the
weakly connected network periphery. Upon repeated stimulus, peripheral network
nodes remodel the network core that encodes the attractor of the new response.
This "core-periphery learning" theory reviews and generalizes the heretofore
fragmented knowledge on attractor formation by neural networks,
periphery-driven innovation and a number of recent reports on the adaptation of
protein, neuronal and social networks. The coreperiphery learning theory may
increase our understanding of signaling, memory formation, information encoding
and decision-making processes. Moreover, the power of network periphery-related
'wisdom of crowds' inventing creative, novel responses indicates that
deliberative democracy is a slow yet efficient learning strategy developed as
the success of a billion-year evolution.
</summary>
    <author>
      <name>Peter Csermely</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1002/bies.201700150</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1002/bies.201700150" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 2015 preliminary version can be downloaded as an earlier version
  of the final paper here. Please find illustrative videos here:
  http://networkdecisions.linkgroup.hu and a video abstract here:
  https://youtu.be/IIjP7zWGjVE</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">BioEssays 40, 1700150 (2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1511.01238v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.01238v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.06611v1</id>
    <updated>2018-01-20T01:28:20Z</updated>
    <published>2018-01-20T01:28:20Z</published>
    <title>Multiple Description Convolutional Neural Networks for Image Compression</title>
    <summary>  Multiple description coding (MDC) is able to stably transmit the signal in
the un-reliable and non-prioritized networks, which has been broadly studied
for several decades. However, the traditional MDC doesn't well leverage image's
context features to generate multiple descriptions. In this paper, we propose a
novel standard-compliant convolutional neural network-based MDC framework in
term of image's context features. Firstly, multiple description generator
network (MDGN) is designed to produce appearance-similar yet feature-different
multiple descriptions automatically according to image's content, which are
compressed by standard codec. Secondly, we present multiple description
reconstruction network (MDRN) including side reconstruction network (SRN) and
central reconstruction network (CRN). When any one of two lossy descriptions is
received at the decoder, SRN network is used to improve the quality of this
decoded lossy description by removing the compression artifact and up-sampling
simultaneously. Meanwhile, we utilize CRN network with two decoded descriptions
as inputs for better reconstruction, if both of lossy descriptions are
available. Thirdly, multiple description virtual codec network (MDVCN) is
proposed to bridge the gap between MDGN network and MDRN network in order to
train an end-to-end MDC framework. Here, two learning algorithms are provided
to train our whole framework. In addition to structural similarity loss
function, the produced descriptions are used as opposing labels with multiple
description distance loss function to regularize the training of MDGN network.
These losses guarantee that the generated description images are structurally
similar yet finely diverse. Experimental results show a great deal of objective
and subjective quality measurements to validate the efficiency of the proposed
method.
</summary>
    <author>
      <name>Lijun Zhao</name>
    </author>
    <author>
      <name>Huihui Bai</name>
    </author>
    <author>
      <name>Anhong Wang</name>
    </author>
    <author>
      <name>Yao Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 3 tables, and 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.06611v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.06611v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.5112v1</id>
    <updated>2011-03-26T06:57:37Z</updated>
    <published>2011-03-26T06:57:37Z</published>
    <title>Weight-conserving characterization of complex functional brain networks</title>
    <summary>  Complex functional brain networks are large networks of brain regions and
functional brain connections. Statistical characterizations of these networks
aim to quantify global and local properties of brain activity with a small
number of network measures. Important functional network measures include
measures of modularity (measures of the goodness with which a network is
optimally partitioned into functional subgroups) and measures of centrality
(measures of the functional influence of individual brain regions).
Characterizations of functional networks are increasing in popularity, but are
associated with several important methodological problems. These problems
include the inability to characterize densely connected and weighted functional
networks, the neglect of degenerate topologically distinct high-modularity
partitions of these networks, and the absence of a network null model for
testing hypotheses of association between observed nontrivial network
properties and simple weighted connectivity properties. In this study we
describe a set of methods to overcome these problems. Specifically, we
generalize measures of modularity and centrality to fully connected and
weighted complex networks, describe the detection of degenerate high-modularity
partitions of these networks, and introduce a weighted-connectivity null model
of these networks. We illustrate our methods by demonstrating degenerate
high-modularity partitions and strong correlations between two complementary
measures of centrality in resting-state functional magnetic resonance imaging
(MRI) networks from the 1000 Functional Connectomes Project, an open-access
repository of resting-state functional MRI datasets. Our methods may allow more
sound and reliable characterizations and comparisons of functional brain
networks across conditions and subjects.
</summary>
    <author>
      <name>Mikail Rubinov</name>
    </author>
    <author>
      <name>Olaf Sporns</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neuroimage.2011.03.069</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neuroimage.2011.03.069" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NeuroImage, in press</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neuroimage. 2011 Jun 15;56(4):2068-79. Epub 2011 Apr 1</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1103.5112v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.5112v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.5054v2</id>
    <updated>2013-01-15T14:50:55Z</updated>
    <published>2012-12-20T14:30:48Z</published>
    <title>Generating functionals for autonomous latching dynamics in attractor
  relict networks</title>
    <summary>  Well characterized sequences of dynamical states play an important role for
motor control and associative neural computation in the brain. Autonomous
dynamics involving sequences of transiently stable states have been termed
associative latching in the context of grammar generation. We propose that
generating functionals allow for a systematic construction of dynamical
networks with well characterized dynamical behavior, such as regular or
intermittent bursting latching dynamics.
  Coupling local, slowly adapting variables to an attractor network allows to
destabilize all attractors, turning them into attractor ruins. The resulting
attractor relict network may show ongoing autonomous latching dynamics. We
propose to use two generating functionals for the construction of attractor
relict networks. The first functional is a simple Hopfield energy functional,
known to generate a neural attractor network. The second generating functional,
which we denote polyhomeostatic optimization, is based on
information-theoretical principles, encoding the information content of the
neural firing statistics. Polyhomeostatic optimization destabilizes the
attractors of the Hopfield network inducing latching dynamics.
  We investigate the influence of stress, in terms of conflicting optimization
targets, on the resulting dynamics. Objective function stress is absent when
the target level for the mean of neural activities is identical for the two
generating functionals and the resulting latching dynamics is then found to be
regular. Objective function stress is present when the respective target
activity levels differ, inducing intermittent bursting latching dynamics. We
propose that generating functionals may be useful quite generally for the
controlled construction of complex dynamical systems.
</summary>
    <author>
      <name>Mathias Linkerhand</name>
    </author>
    <author>
      <name>Claudius Gros</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">replaced two figures which were not showing up properly</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Scientific Reports, Vol 3, 2042 (2013)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1212.5054v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.5054v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.0985v3</id>
    <updated>2014-03-24T15:39:23Z</updated>
    <published>2013-10-03T14:03:51Z</published>
    <title>Average synaptic activity and neural networks topology: a global inverse
  problem</title>
    <summary>  The dynamics of neural networks is often characterized by collective behavior
and quasi-synchronous events, where a large fraction of neurons fire in short
time intervals, separated by uncorrelated firing activity. These global
temporal signals are crucial for brain functioning. They strongly depend on the
topology of the network and on the fluctuations of the connectivity. We propose
a heterogeneous mean--field approach to neural dynamics on random networks,
that explicitly preserves the disorder in the topology at growing network
sizes, and leads to a set of self-consistent equations. Within this approach,
we provide an effective description of microscopic and large scale temporal
signals in a leaky integrate-and-fire model with short term plasticity, where
quasi-synchronous events arise. Our equations provide a clear analytical
picture of the dynamics, evidencing the contributions of both periodic (locked)
and aperiodic (unlocked) neurons to the measurable average signal. In
particular, we formulate and solve a global inverse problem of reconstructing
the in-degree distribution from the knowledge of the average activity field.
Our method is very general and applies to a large class of dynamical models on
dense random networks.
</summary>
    <author>
      <name>Raffaella Burioni</name>
    </author>
    <author>
      <name>Mario Casartelli</name>
    </author>
    <author>
      <name>Matteo di Volo</name>
    </author>
    <author>
      <name>Roberto Livi</name>
    </author>
    <author>
      <name>Alessandro Vezzani</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Scientific Reports 4, 4336 (2014)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1310.0985v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.0985v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.03240v3</id>
    <updated>2016-04-13T23:26:45Z</updated>
    <published>2015-02-11T10:02:50Z</published>
    <title>Conditional Random Fields as Recurrent Neural Networks</title>
    <summary>  Pixel-level labelling tasks, such as semantic segmentation, play a central
role in image understanding. Recent approaches have attempted to harness the
capabilities of deep learning techniques for image recognition to tackle
pixel-level labelling tasks. One central issue in this methodology is the
limited capacity of deep learning techniques to delineate visual objects. To
solve this problem, we introduce a new form of convolutional neural network
that combines the strengths of Convolutional Neural Networks (CNNs) and
Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To
this end, we formulate mean-field approximate inference for the Conditional
Random Fields with Gaussian pairwise potentials as Recurrent Neural Networks.
This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a
deep network that has desirable properties of both CNNs and CRFs. Importantly,
our system fully integrates CRF modelling with CNNs, making it possible to
train the whole deep network end-to-end with the usual back-propagation
algorithm, avoiding offline post-processing methods for object delineation. We
apply the proposed method to the problem of semantic image segmentation,
obtaining top results on the challenging Pascal VOC 2012 segmentation
benchmark.
</summary>
    <author>
      <name>Shuai Zheng</name>
    </author>
    <author>
      <name>Sadeep Jayasumana</name>
    </author>
    <author>
      <name>Bernardino Romera-Paredes</name>
    </author>
    <author>
      <name>Vibhav Vineet</name>
    </author>
    <author>
      <name>Zhizhong Su</name>
    </author>
    <author>
      <name>Dalong Du</name>
    </author>
    <author>
      <name>Chang Huang</name>
    </author>
    <author>
      <name>Philip H. S. Torr</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICCV.2015.179</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICCV.2015.179" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is published in IEEE ICCV 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.03240v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.03240v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.00175v2</id>
    <updated>2016-01-08T06:50:36Z</updated>
    <published>2015-10-31T21:13:30Z</published>
    <title>FireCaffe: near-linear acceleration of deep neural network training on
  compute clusters</title>
    <summary>  Long training times for high-accuracy deep neural networks (DNNs) impede
research into new DNN architectures and slow the development of high-accuracy
DNNs. In this paper we present FireCaffe, which successfully scales deep neural
network training across a cluster of GPUs. We also present a number of best
practices to aid in comparing advancements in methods for scaling and
accelerating the training of deep neural networks. The speed and scalability of
distributed algorithms is almost always limited by the overhead of
communicating between servers; DNN training is not an exception to this rule.
Therefore, the key consideration here is to reduce communication overhead
wherever possible, while not degrading the accuracy of the DNN models that we
train. Our approach has three key pillars. First, we select network hardware
that achieves high bandwidth between GPU servers -- Infiniband or Cray
interconnects are ideal for this. Second, we consider a number of communication
algorithms, and we find that reduction trees are more efficient and scalable
than the traditional parameter server approach. Third, we optionally increase
the batch size to reduce the total quantity of communication during DNN
training, and we identify hyperparameters that allow us to reproduce the
small-batch accuracy while training with large batch sizes. When training
GoogLeNet and Network-in-Network on ImageNet, we achieve a 47x and 39x speedup,
respectively, when training on a cluster of 128 GPUs.
</summary>
    <author>
      <name>Forrest N. Iandola</name>
    </author>
    <author>
      <name>Khalid Ashraf</name>
    </author>
    <author>
      <name>Matthew W. Moskewicz</name>
    </author>
    <author>
      <name>Kurt Keutzer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Version 2: Added results on 128 GPUs</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.00175v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.00175v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.02780v1</id>
    <updated>2016-04-11T03:23:21Z</updated>
    <published>2016-04-11T03:23:21Z</published>
    <title>Knowledge Extraction and Knowledge Integration governed by
  ≈Åukasiewicz Logics</title>
    <summary>  The development of machine learning in particular and artificial intelligent
in general has been strongly conditioned by the lack of an appropriate
interface layer between deduction, abduction and induction. In this work we
extend traditional algebraic specification methods in this direction. Here we
assume that such interface for AI emerges from an adequate Neural-Symbolic
integration. This integration is made for universe of discourse described on a
Topos governed by a many-valued {\L}ukasiewicz logic. Sentences are integrated
in a symbolic knowledge base describing the problem domain, codified using a
graphic-based language, wherein every logic connective is defined by a neuron
in an artificial network. This allows the integration of first-order formulas
into a network architecture as background knowledge, and simplifies symbolic
rule extraction from trained networks. For the train of such neural networks we
changed the Levenderg-Marquardt algorithm, restricting the knowledge
dissemination in the network structure using soft crystallization. This
procedure reduces neural network plasticity without drastically damaging the
learning performance, allowing the emergence of symbolic patterns. This makes
the descriptive power of produced neural networks similar to the descriptive
power of {\L}ukasiewicz logic language, reducing the information lost on
translation between symbolic and connectionist structures. We tested this
method on the extraction of knowledge from specified structures. For it, we
present the notion of fuzzy state automata, and we use automata behaviour to
infer its structure. We use this type of automata on the generation of models
for relations specified as symbolic background knowledge.
</summary>
    <author>
      <name>Carlos Leandro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.02780v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.02780v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="03D05, 03B52, 92B20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04801v2</id>
    <updated>2016-06-16T06:53:55Z</updated>
    <published>2016-06-15T14:56:42Z</published>
    <title>A Powerful Generative Model Using Random Weights for the Deep Image
  Representation</title>
    <summary>  To what extent is the success of deep visualization due to the training?
Could we do deep visualization using untrained, random weight networks? To
address this issue, we explore new and powerful generative models for three
popular deep visualization tasks using untrained, random weight convolutional
neural networks. First we invert representations in feature spaces and
reconstruct images from white noise inputs. The reconstruction quality is
statistically higher than that of the same method applied on well trained
networks with the same architecture. Next we synthesize textures using scaled
correlations of representations in multiple layers and our results are almost
indistinguishable with the original natural texture and the synthesized
textures based on the trained network. Third, by recasting the content of an
image in the style of various artworks, we create artistic images with high
perceptual quality, highly competitive to the prior work of Gatys et al. on
pretrained networks. To our knowledge this is the first demonstration of image
representations using untrained deep neural networks. Our work provides a new
and fascinating tool to study the representation of deep network architecture
and sheds light on new understandings on deep visualization.
</summary>
    <author>
      <name>Kun He</name>
    </author>
    <author>
      <name>Yan Wang</name>
    </author>
    <author>
      <name>John Hopcroft</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 10 figures, submited to NIPS 2016 conference. Computer
  Vision and Pattern Recognition, Neurons and Cognition, Neural and
  Evolutionary Computing</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.04801v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04801v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00607v2</id>
    <updated>2016-10-14T11:39:17Z</updated>
    <published>2016-09-02T14:11:53Z</published>
    <title>Parton Shower Uncertainties in Jet Substructure Analyses with Deep
  Neural Networks</title>
    <summary>  Machine learning methods incorporating deep neural networks have been the
subject of recent proposals for new hadronic resonance taggers. These methods
require training on a dataset produced by an event generator where the true
class labels are known. However, training a network on a specific event
generator may bias the network towards learning features associated with the
approximations to QCD used in that generator which are not present in real
data. We therefore investigate the effects of variations in the modelling of
the parton shower on the performance of deep neural network taggers using jet
images from hadronic W-bosons at the LHC, including detector-related effects.
By investigating network performance on samples from the Pythia, Herwig and
Sherpa generators, we find differences of up to fifty percent in background
rejection for fixed signal efficiency. We also introduce and study a method,
which we dub zooming, for implementing scale-invariance in neural network-based
taggers. We find that this leads to an improvement in performance across a wide
range of jet transverse momenta. Our results emphasise the importance gaining a
detailed understanding what aspects of jet physics these methods are
exploiting.
</summary>
    <author>
      <name>James Barnard</name>
    </author>
    <author>
      <name>Edmund Noel Dawe</name>
    </author>
    <author>
      <name>Matthew J. Dolan</name>
    </author>
    <author>
      <name>Nina Rajcic</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevD.95.014018</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevD.95.014018" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures; v2: plots updated, references added</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. D 95, 014018 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1609.00607v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00607v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.06733v1</id>
    <updated>2017-08-22T17:31:54Z</updated>
    <published>2017-08-22T17:31:54Z</published>
    <title>BadNets: Identifying Vulnerabilities in the Machine Learning Model
  Supply Chain</title>
    <summary>  Deep learning-based techniques have achieved state-of-the-art performance on
a wide variety of recognition and classification tasks. However, these networks
are typically computationally expensive to train, requiring weeks of
computation on many GPUs; as a result, many users outsource the training
procedure to the cloud or rely on pre-trained models that are then fine-tuned
for a specific task. In this paper we show that outsourced training introduces
new security risks: an adversary can create a maliciously trained network (a
backdoored neural network, or a \emph{BadNet}) that has state-of-the-art
performance on the user's training and validation samples, but behaves badly on
specific attacker-chosen inputs. We first explore the properties of BadNets in
a toy example, by creating a backdoored handwritten digit classifier. Next, we
demonstrate backdoors in a more realistic scenario by creating a U.S. street
sign classifier that identifies stop signs as speed limits when a special
sticker is added to the stop sign; we then show in addition that the backdoor
in our US street sign detector can persist even if the network is later
retrained for another task and cause a drop in accuracy of {25}\% on average
when the backdoor trigger is present. These results demonstrate that backdoors
in neural networks are both powerful and---because the behavior of neural
networks is difficult to explicate---stealthy. This work provides motivation
for further research into techniques for verifying and inspecting neural
networks, just as we have developed tools for verifying and debugging software.
</summary>
    <author>
      <name>Tianyu Gu</name>
    </author>
    <author>
      <name>Brendan Dolan-Gavitt</name>
    </author>
    <author>
      <name>Siddharth Garg</name>
    </author>
    <link href="http://arxiv.org/abs/1708.06733v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.06733v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.07949v2</id>
    <updated>2018-03-04T18:10:00Z</updated>
    <published>2017-08-26T08:07:57Z</published>
    <title>TraNNsformer: Neural network transformation for memristive crossbar
  based neuromorphic system design</title>
    <summary>  Implementation of Neuromorphic Systems using post Complementary
Metal-Oxide-Semiconductor (CMOS) technology based Memristive Crossbar Array
(MCA) has emerged as a promising solution to enable low-power acceleration of
neural networks. However, the recent trend to design Deep Neural Networks
(DNNs) for achieving human-like cognitive abilities poses significant
challenges towards the scalable design of neuromorphic systems (due to the
increase in computation/storage demands). Network pruning [7] is a powerful
technique to remove redundant connections for designing optimally connected
(maximally sparse) DNNs. However, such pruning techniques induce irregular
connections that are incoherent to the crossbar structure. Eventually they
produce DNNs with highly inefficient hardware realizations (in terms of area
and energy). In this work, we propose TraNNsformer - an integrated training
framework that transforms DNNs to enable their efficient realization on
MCA-based systems. TraNNsformer first prunes the connectivity matrix while
forming clusters with the remaining connections. Subsequently, it retrains the
network to fine tune the connections and reinforce the clusters. This is done
iteratively to transform the original connectivity into an optimally pruned and
maximally clustered mapping. Without accuracy loss, TraNNsformer reduces the
area (energy) consumption by 28% - 55% (49% - 67%) with respect to the original
network. Compared to network pruning, TraNNsformer achieves 28% - 49% (15% -
29%) area (energy) savings. Furthermore, TraNNsformer is a technology-aware
framework that allows mapping a given DNN to any MCA size permissible by the
memristive technology for reliable operations.
</summary>
    <author>
      <name>Aayush Ankit</name>
    </author>
    <author>
      <name>Abhronil Sengupta</name>
    </author>
    <author>
      <name>Kaushik Roy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICCAD.2017.8203823</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICCAD.2017.8203823" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">(8 pages, 9 figures) Published in Computer-Aided Design (ICCAD), 2017
  IEEE/ACM International Conference on</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.07949v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.07949v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.03637v1</id>
    <updated>2017-11-09T23:01:42Z</updated>
    <published>2017-11-09T23:01:42Z</published>
    <title>Learning and Real-time Classification of Hand-written Digits With
  Spiking Neural Networks</title>
    <summary>  We describe a novel spiking neural network (SNN) for automated, real-time
handwritten digit classification and its implementation on a GP-GPU platform.
Information processing within the network, from feature extraction to
classification is implemented by mimicking the basic aspects of neuronal spike
initiation and propagation in the brain. The feature extraction layer of the
SNN uses fixed synaptic weight maps to extract the key features of the image
and the classifier layer uses the recently developed NormAD approximate
gradient descent based supervised learning algorithm for spiking neural
networks to adjust the synaptic weights. On the standard MNIST database images
of handwritten digits, our network achieves an accuracy of 99.80% on the
training set and 98.06% on the test set, with nearly 7x fewer parameters
compared to the state-of-the-art spiking networks. We further use this network
in a GPU based user-interface system demonstrating real-time SNN simulation to
infer digits written by different users. On a test set of 500 such images, this
real-time platform achieves an accuracy exceeding 97% while making a prediction
within an SNN emulation time of less than 100ms.
</summary>
    <author>
      <name>Shruti R. Kulkarni</name>
    </author>
    <author>
      <name>John M. Alexiades</name>
    </author>
    <author>
      <name>Bipin Rajendran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures, 1 table, accepted at ICECS 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.03637v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.03637v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.07480v1</id>
    <updated>2017-11-20T17:58:10Z</updated>
    <published>2017-11-20T17:58:10Z</published>
    <title>E-PUR: An Energy-Efficient Processing Unit for Recurrent Neural Networks</title>
    <summary>  Recurrent Neural Networks (RNNs) are a key technology for emerging
applications such as automatic speech recognition, machine translation or image
description. Long Short Term Memory (LSTM) networks are the most successful RNN
implementation, as they can learn long term dependencies to achieve high
accuracy. Unfortunately, the recurrent nature of LSTM networks significantly
constrains the amount of parallelism and, hence, multicore CPUs and many-core
GPUs exhibit poor efficiency for RNN inference. In this paper, we present
E-PUR, an energy-efficient processing unit tailored to the requirements of LSTM
computation. The main goal of E-PUR is to support large recurrent neural
networks for low-power mobile devices. E-PUR provides an efficient hardware
implementation of LSTM networks that is flexible to support diverse
applications. One of its main novelties is a technique that we call Maximizing
Weight Locality (MWL), which improves the temporal locality of the memory
accesses for fetching the synaptic weights, reducing the memory requirements by
a large extent. Our experimental results show that E-PUR achieves real-time
performance for different LSTM networks, while reducing energy consumption by
orders of magnitude with respect to general-purpose processors and GPUs, and it
requires a very small chip area. Compared to a modern mobile SoC, an NVIDIA
Tegra X1, E-PUR provides an average energy reduction of 92x.
</summary>
    <author>
      <name>Franyell Silfa</name>
    </author>
    <author>
      <name>Gem Dot</name>
    </author>
    <author>
      <name>Jose-Maria Arnau</name>
    </author>
    <author>
      <name>Antonio Gonzalez</name>
    </author>
    <link href="http://arxiv.org/abs/1711.07480v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.07480v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.00557v1</id>
    <updated>2017-12-02T06:08:35Z</updated>
    <published>2017-12-02T06:08:35Z</published>
    <title>Recurrent Neural Network Language Models for Open Vocabulary Event-Level
  Cyber Anomaly Detection</title>
    <summary>  Automated analysis methods are crucial aids for monitoring and defending a
network to protect the sensitive or confidential data it hosts. This work
introduces a flexible, powerful, and unsupervised approach to detecting
anomalous behavior in computer and network logs, one that largely eliminates
domain-dependent feature engineering employed by existing methods. By treating
system logs as threads of interleaved "sentences" (event log lines) to train
online unsupervised neural network language models, our approach provides an
adaptive model of normal network behavior. We compare the effectiveness of both
standard and bidirectional recurrent neural network language models at
detecting malicious activity within network log data. Extending these models,
we introduce a tiered recurrent architecture, which provides context by
modeling sequences of users' actions over time. Compared to Isolation Forest
and Principal Components Analysis, two popular anomaly detection algorithms, we
observe superior performance on the Los Alamos National Laboratory Cyber
Security dataset. For log-line-level red team detection, our best performing
character-based model provides test set area under the receiver operator
characteristic curve of 0.98, demonstrating the strong fine-grained anomaly
detection performance of this approach on open vocabulary logging sources.
</summary>
    <author>
      <name>Aaron Tuor</name>
    </author>
    <author>
      <name>Ryan Baerwolf</name>
    </author>
    <author>
      <name>Nicolas Knowles</name>
    </author>
    <author>
      <name>Brian Hutchinson</name>
    </author>
    <author>
      <name>Nicole Nichols</name>
    </author>
    <author>
      <name>Rob Jasper</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, To appear in proceedings of AAAI-2018 Artificial
  Intelligence in Cyber Security Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.00557v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.00557v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.07130v2</id>
    <updated>2018-02-24T03:15:40Z</updated>
    <published>2018-01-22T14:59:18Z</published>
    <title>Computational Protein Design with Deep Learning Neural Networks</title>
    <summary>  Computational protein design has a wide variety of applications. Despite its
remarkable success, designing a protein for a given structure and function is
still a challenging task. On the other hand, the number of solved protein
structures is rapidly increasing while the number of unique protein folds has
reached a steady number, suggesting more structural information is being
accumulated on each fold. Deep learning neural network is a powerful method to
learn such big data set and has shown superior performance in many machine
learning fields. In this study, we applied the deep learning neural network
approach to computational protein design for predicting the probability of 20
natural amino acids on each residue in a protein. A large set of protein
structures was collected and a multi-layer neural network was constructed. A
number of structural properties were extracted as input features and the best
network achieved an accuracy of 38.3%. Using the network output as residue type
restraints was able to improve the average sequence identity in designing three
natural proteins using Rosetta. Moreover, the predictions from our network show
~3% higher sequence identity than a previous method. Results from this study
may benefit further development of computational protein design methods.
</summary>
    <author>
      <name>Jingxue Wang</name>
    </author>
    <author>
      <name>Huali Cao</name>
    </author>
    <author>
      <name>John Z. H. Zhang</name>
    </author>
    <author>
      <name>Yifei Qi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 5 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.07130v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.07130v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.BM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.10578v1</id>
    <updated>2018-01-31T17:51:32Z</updated>
    <published>2018-01-31T17:51:32Z</published>
    <title>Evaluating the Robustness of Neural Networks: An Extreme Value Theory
  Approach</title>
    <summary>  The robustness of neural networks to adversarial examples has received great
attention due to security implications. Despite various attack approaches to
crafting visually imperceptible adversarial examples, little has been developed
towards a comprehensive measure of robustness. In this paper, we provide a
theoretical justification for converting robustness analysis into a local
Lipschitz constant estimation problem, and propose to use the Extreme Value
Theory for efficient evaluation. Our analysis yields a novel robustness metric
called CLEVER, which is short for Cross Lipschitz Extreme Value for nEtwork
Robustness. The proposed CLEVER score is attack-agnostic and computationally
feasible for large neural networks. Experimental results on various networks,
including ResNet, Inception-v3 and MobileNet, show that (i) CLEVER is aligned
with the robustness indication measured by the $\ell_2$ and $\ell_\infty$ norms
of adversarial examples from powerful attacks, and (ii) defended networks using
defensive distillation or bounded ReLU indeed achieve better CLEVER scores. To
the best of our knowledge, CLEVER is the first attack-independent robustness
metric that can be applied to any neural network classifier.
</summary>
    <author>
      <name>Tsui-Wei Weng</name>
    </author>
    <author>
      <name>Huan Zhang</name>
    </author>
    <author>
      <name>Pin-Yu Chen</name>
    </author>
    <author>
      <name>Jinfeng Yi</name>
    </author>
    <author>
      <name>Dong Su</name>
    </author>
    <author>
      <name>Yupeng Gao</name>
    </author>
    <author>
      <name>Cho-Jui Hsieh</name>
    </author>
    <author>
      <name>Luca Daniel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by Sixth International Conference on Learning
  Representations (ICLR 2018). Tsui-Wei Weng and Huan Zhang contributed equally</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.10578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.10578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.09888v1</id>
    <updated>2017-12-28T15:08:14Z</updated>
    <published>2017-12-28T15:08:14Z</published>
    <title>Improved Inception-Residual Convolutional Neural Network for Object
  Recognition</title>
    <summary>  Machine learning and computer vision have driven many of the greatest
advances in the modeling of Deep Convolutional Neural Networks (DCNNs).
Nowadays, most of the research has been focused on improving recognition
accuracy with better DCNN models and learning approaches. The recurrent
convolutional approach is not applied very much, other than in a few DCNN
architectures. On the other hand, Inception-v4 and Residual networks have
promptly become popular among computer the vision community. In this paper, we
introduce a new DCNN model called the Inception Recurrent Residual
Convolutional Neural Network (IRRCNN), which utilizes the power of the
Recurrent Convolutional Neural Network (RCNN), the Inception network, and the
Residual network. This approach improves the recognition accuracy of the
Inception-residual network with same number of network parameters. In addition,
this proposed architecture generalizes the Inception network, the RCNN, and the
Residual network with significantly improved training accuracy. We have
empirically evaluated the performance of the IRRCNN model on different
benchmarks including CIFAR-10, CIFAR-100, TinyImageNet-200, and CU3D-100. The
experimental results show higher recognition accuracy against most of the
popular DCNN models including the RCNN. We have also investigated the
performance of the IRRCNN approach against the Equivalent Inception Network
(EIN) and the Equivalent Inception Residual Network (EIRN) counterpart on the
CIFAR-100 dataset. We report around 4.53%, 4.49% and 3.56% improvement in
classification accuracy compared with the RCNN, EIN, and EIRN on the CIFAR-100
dataset respectively. Furthermore, the experiment has been conducted on the
TinyImageNet-200 and CU3D-100 datasets where the IRRCNN provides better testing
accuracy compared to the Inception Recurrent CNN (IRCNN), the EIN, and the
EIRN.
</summary>
    <author>
      <name>Md Zahangir Alom</name>
    </author>
    <author>
      <name>Mahmudul Hasan</name>
    </author>
    <author>
      <name>Chris Yakopcic</name>
    </author>
    <author>
      <name>Tarek M. Taha</name>
    </author>
    <author>
      <name>Vijayan K. Asari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 15 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.09888v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.09888v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0203568v1</id>
    <updated>2002-03-27T19:55:54Z</updated>
    <published>2002-03-27T19:55:54Z</published>
    <title>Bistable Gradient Networks in the Thermodynamic Limit</title>
    <summary>  We examine the large-network, low-loading behaviour of an attractor neural
network, the so-called bistable gradient network (BGN). We use analytical and
numerical methods to characterize the attractor states of the network and their
basins of attraction. The energy landscape is more complex than that of the
Hopfield network and depends on the strength of the coupling among units. At
weak coupling, the BGN acts as a highly selective associative memory; the input
must be close to the one of the stored patterns in order to be recognized. A
category of spurious attractors occurs which is not present in the Hopfield
network. Stronger coupling results in a transition to a more Hopfield-like
regime with large basins of attraction. The basins of attraction for spurious
attractors are noticeably suppressed compared to the Hopfield case, even though
the Hebbian synaptic structure is the same and there is no stochastic noise.
</summary>
    <author>
      <name>Patrick N. McGraw</name>
    </author>
    <author>
      <name>Michael Menzinger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">RevTeX, 18 pages including 12 figures. Submitted to Phys. Rev. E</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0203568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0203568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0306625v2</id>
    <updated>2003-07-06T21:39:13Z</updated>
    <published>2003-06-24T22:39:05Z</published>
    <title>Heterogeneity in oscillator networks: Are smaller worlds easier to
  synchronize?</title>
    <summary>  Small-world and scale-free networks are known to be more easily synchronized
than regular lattices, which is usually attributed to the smaller network
distance between oscillators. Surprisingly, we find that networks with a
homogeneous distribution of connectivity are more synchronizable than
heterogeneous ones, even though the average network distance is larger. We
present numerical computations and analytical estimates on synchronizability of
the network in terms of its heterogeneity parameters. Our results suggest that
some degree of homogeneity is expected in naturally evolved structures, such as
neural networks, where synchronizability is desirable.
</summary>
    <author>
      <name>Takashi Nishikawa</name>
    </author>
    <author>
      <name>Adilson E. Motter</name>
    </author>
    <author>
      <name>Ying-Cheng Lai</name>
    </author>
    <author>
      <name>Frank C. Hoppensteadt</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.91.014101</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.91.014101" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures, REVTeX4</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. Lett. 91, 014101 (2003)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0306625v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0306625v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0703178v1</id>
    <updated>2007-03-07T10:06:31Z</updated>
    <published>2007-03-07T10:06:31Z</published>
    <title>On the stationary state of a network of inhibitory spiking neurons</title>
    <summary>  The background activity of a cortical neural network is modeled by a
homogeneous integrate-and-fire network with unreliable inhibitory synapses.
Numerical and analytical calculations show that the network relaxes into a
stationary state of high attention. The majority of the neurons has a membrane
potential just below the threshold; as a consequence the network can react
immediately - on the time scale of synaptic transmission- on external pulses.
The neurons fire with a low rate and with a broad distribution of interspike
intervals. Firing events of the total network are correlated over short time
periods. The firing rate increases linearly with external stimuli. In the limit
of infinitely large networks, the synaptic noise decreases to zero.
Nevertheless, the distribution of interspike intervals remains broad.
</summary>
    <author>
      <name>Wolfgang Kinzel</name>
    </author>
    <link href="http://arxiv.org/abs/cond-mat/0703178v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0703178v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0410036v2</id>
    <updated>2005-09-09T17:48:53Z</updated>
    <published>2004-10-15T20:25:24Z</published>
    <title>Self-Organised Factorial Encoding of a Toroidal Manifold</title>
    <summary>  It is shown analytically how a neural network can be used optimally to encode
input data that is derived from a toroidal manifold. The case of a 2-layer
network is considered, where the output is assumed to be a set of discrete
neural firing events. The network objective function measures the average
Euclidean error that occurs when the network attempts to reconstruct its input
from its output. This optimisation problem is solved analytically for a
toroidal input manifold, and two types of solution are obtained: a joint
encoder in which the network acts as a soft vector quantiser, and a factorial
encoder in which the network acts as a pair of soft vector quantisers (one for
each of the circular subspaces of the torus). The factorial encoder is favoured
for small network sizes when the number of observed firing events is large.
Such self-organised factorial encoding may be used to restrict the size of
network that is required to perform a given encoding task, and will decompose
an input manifold into its constituent submanifolds.
</summary>
    <author>
      <name>Stephen Luttrell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">46 pages, 11 figures, corrected equation 39</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0410036v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0410036v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/q-bio/0603035v1</id>
    <updated>2006-03-29T21:35:42Z</updated>
    <published>2006-03-29T21:35:42Z</published>
    <title>Synchronization in Electrically Coupled Neural Networks</title>
    <summary>  In this report, we investigate the synchronization of temporal activity in an
electrically coupled neural network model. The electrical coupling is
established by homotypic static gap-junctions (Connexin 43). Two distinct
network topologies, namely: {\em sparse random network, (SRN)} and {\em fully
connected network, (FCN)} are used to establish the connectivity. The strength
of connectivity in the FCN is governed by the {\em mean gap junctional
conductance} ($\mu$). In the case of the SRN, the overall strength of
connectivity is governed by the {\em density of connections} ($\delta$) and the
connection strength between two neurons ($S_0$). The synchronization of the
network with increasing gap junctional strength and varying population sizes is
investigated. It was observed that the network {\em abruptly} makes a
transition from a weakly synchronized to a well synchronized regime when
($\delta$) or ($\mu$) exceeds a critical value. It was also observed that the
($\delta$, $\mu$) values used to achieve synchronization decreases with
increasing network size.
</summary>
    <author>
      <name>Rajesh G Kavasseri</name>
    </author>
    <author>
      <name>Radhakrishnan Nagarajan</name>
    </author>
    <link href="http://arxiv.org/abs/q-bio/0603035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/q-bio/0603035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.0429v1</id>
    <updated>2008-07-02T19:32:59Z</updated>
    <published>2008-07-02T19:32:59Z</published>
    <title>The phase diagram of random threshold networks</title>
    <summary>  Threshold networks are used as models for neural or gene regulatory networks.
They show a rich dynamical behaviour with a transition between a frozen and a
chaotic phase. We investigate the phase diagram of randomly connected threshold
networks with real-valued thresholds h and a fixed number of inputs per node.
The nodes are updated according to the same rules as in a model of the
cell-cycle network of Saccharomyces cereviseae [PNAS 101, 4781 (2004)]. Using
the annealed approximation, we derive expressions for the time evolution of the
proportion of nodes in the "on" and "off" state, and for the sensitivity
$\lambda$. The results are compared with simulations of quenched networks. We
find that for integer values of h the simulations show marked deviations from
the annealed approximation even for large networks. This can be attributed to
the particular choice of the updating rule.
</summary>
    <author>
      <name>Agnes Szejka</name>
    </author>
    <author>
      <name>Tamara Mihaljev</name>
    </author>
    <author>
      <name>Barbara Drossel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1367-2630/10/6/063009</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1367-2630/10/6/063009" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">New J. Phys. 10 (2008) 063009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0807.0429v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.0429v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.CB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.1943v1</id>
    <updated>2011-07-11T06:51:16Z</updated>
    <published>2011-07-11T06:51:16Z</published>
    <title>Enhanced Genetic Algorithm approach for Solving Dynamic Shortest Path
  Routing Problems using Immigrants and Memory Schemes</title>
    <summary>  In Internet Routing, the static shortest path (SP) problem has been addressed
using well known intelligent optimization techniques like artificial neural
networks, genetic algorithms (GAs) and particle swarm optimization. Advancement
in wireless communication lead more and more mobile wireless networks, such as
mobile networks [mobile ad hoc networks (MANETs)] and wireless sensor networks.
Dynamic nature of the network is the main characteristic of MANET. Therefore,
the SP routing problem in MANET turns into dynamic optimization problem (DOP).
Here the nodes ae made aware of the environmental condition, thereby making it
intelligent, which goes as the input for GA. The implementation then uses GAs
with immigrants and memory schemes to solve the dynamic SP routing problem
(DSPRP) in MANETS. In our paper, once the network topology changes, the optimal
solutions in the new environment can be searched using the new immigrants or
the useful information stored in the memory. Results shows GA with new
immigrants shows better convergence result than GA with memory scheme.
</summary>
    <author>
      <name>T. R. Gopalakrishnan Nair</name>
    </author>
    <author>
      <name>Kavitha Sooda</name>
    </author>
    <author>
      <name>M. B. Yashoda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages,6 figures, International Conference on Frontiers of Computer
  Science, 7TH TO 9TH August 2011, JN Tata Convention Centre, IISc,Bangalore,
  India</arxiv:comment>
    <link href="http://arxiv.org/abs/1107.1943v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.1943v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.8260v2</id>
    <updated>2013-03-13T09:08:43Z</updated>
    <published>2012-10-31T08:22:48Z</published>
    <title>Mean Field Theory of Dynamical Systems Driven by External Signals</title>
    <summary>  Dynamical systems driven by strong external signals are ubiquituous in nature
and engineering. Here we study "echo state networks", networks of a large
number of randomly connected nodes, which represent a simple model of a neural
network, and have important applications in machine learning. We develop a mean
field theory of echo state networks. The dynamics of the network is captured by
the evolution law, similar to a logistic map, for a single collective variable.
When the network is driven by many independent external signals, this
collective variable reaches a steady state. But when the network is driven by a
single external signal, the collective variable is nonstationnary but can be
characterised by its time averaged distribution. The predictions of the mean
field theory, including the value of the largest Lyaponuov exponent, are
compared with the numerical integration of the equations of motion.
</summary>
    <author>
      <name>Marc Massar</name>
    </author>
    <author>
      <name>Serge Massar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.87.042809</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.87.042809" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physical Review E 87, 042809 (2013)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1210.8260v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.8260v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.6041v1</id>
    <updated>2013-06-25T17:28:53Z</updated>
    <published>2013-06-25T17:28:53Z</published>
    <title>Learning, Generalization, and Functional Entropy in Random Automata
  Networks</title>
    <summary>  It has been shown \citep{broeck90:physicalreview,patarnello87:europhys} that
feedforward Boolean networks can learn to perform specific simple tasks and
generalize well if only a subset of the learning examples is provided for
learning. Here, we extend this body of work and show experimentally that random
Boolean networks (RBNs), where both the interconnections and the Boolean
transfer functions are chosen at random initially, can be evolved by using a
state-topology evolution to solve simple tasks. We measure the learning and
generalization performance, investigate the influence of the average node
connectivity $K$, the system size $N$, and introduce a new measure that allows
to better describe the network's learning and generalization behavior. We show
that the connectivity of the maximum entropy networks scales as a power-law of
the system size $N$. Our results show that networks with higher average
connectivity $K$ (supercritical) achieve higher memorization and partial
generalization. However, near critical connectivity, the networks show a higher
perfect generalization on the even-odd task.
</summary>
    <author>
      <name>Alireza Goudarzi</name>
    </author>
    <author>
      <name>Christof Teuscher</name>
    </author>
    <author>
      <name>Natali Gulbahce</name>
    </author>
    <author>
      <name>Thimo Rohlf</name>
    </author>
    <link href="http://arxiv.org/abs/1306.6041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.6041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO, nlin.CD, q-bio.NC, physics.bio-ph, cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; I.2.6; I.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.1909v1</id>
    <updated>2013-11-18T17:56:11Z</updated>
    <published>2013-11-18T17:56:11Z</published>
    <title>From Maxout to Channel-Out: Encoding Information on Sparse Pathways</title>
    <summary>  Motivated by an important insight from neural science, we propose a new
framework for understanding the success of the recently proposed "maxout"
networks. The framework is based on encoding information on sparse pathways and
recognizing the correct pathway at inference time. Elaborating further on this
insight, we propose a novel deep network architecture, called "channel-out"
network, which takes a much better advantage of sparse pathway encoding. In
channel-out networks, pathways are not only formed a posteriori, but they are
also actively selected according to the inference outputs from the lower
layers. From a mathematical perspective, channel-out networks can represent a
wider class of piece-wise continuous functions, thereby endowing the network
with more expressive power than that of maxout networks. We test our
channel-out networks on several well-known image classification benchmarks,
setting new state-of-the-art performance on CIFAR-100 and STL-10, which
represent some of the "harder" image classification benchmarks.
</summary>
    <author>
      <name>Qi Wang</name>
    </author>
    <author>
      <name>Joseph JaJa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages including the appendix, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.1909v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.1909v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.05756v1</id>
    <updated>2015-11-18T12:30:57Z</updated>
    <published>2015-11-18T12:30:57Z</published>
    <title>Image Question Answering using Convolutional Neural Network with Dynamic
  Parameter Prediction</title>
    <summary>  We tackle image question answering (ImageQA) problem by learning a
convolutional neural network (CNN) with a dynamic parameter layer whose weights
are determined adaptively based on questions. For the adaptive parameter
prediction, we employ a separate parameter prediction network, which consists
of gated recurrent unit (GRU) taking a question as its input and a
fully-connected layer generating a set of candidate weights as its output.
However, it is challenging to construct a parameter prediction network for a
large number of parameters in the fully-connected dynamic parameter layer of
the CNN. We reduce the complexity of this problem by incorporating a hashing
technique, where the candidate weights given by the parameter prediction
network are selected using a predefined hash function to determine individual
weights in the dynamic parameter layer. The proposed network---joint network
with the CNN for ImageQA and the parameter prediction network---is trained
end-to-end through back-propagation, where its weights are initialized using a
pre-trained CNN and GRU. The proposed algorithm illustrates the
state-of-the-art performance on all available public ImageQA benchmarks.
</summary>
    <author>
      <name>Hyeonwoo Noh</name>
    </author>
    <author>
      <name>Paul Hongsuck Seo</name>
    </author>
    <author>
      <name>Bohyung Han</name>
    </author>
    <link href="http://arxiv.org/abs/1511.05756v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.05756v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07427v1</id>
    <updated>2016-05-24T12:48:19Z</updated>
    <published>2016-05-24T12:48:19Z</published>
    <title>Hierarchical Memory Networks</title>
    <summary>  Memory networks are neural networks with an explicit memory component that
can be both read and written to by the network. The memory is often addressed
in a soft way using a softmax function, making end-to-end training with
backpropagation possible. However, this is not computationally scalable for
applications which require the network to read from extremely large memories.
On the other hand, it is well known that hard attention mechanisms based on
reinforcement learning are challenging to train successfully. In this paper, we
explore a form of hierarchical memory network, which can be considered as a
hybrid between hard and soft attention memory networks. The memory is organized
in a hierarchical structure such that reading from it is done with less
computation than soft attention over a flat memory, while also being easier to
train than hard attention over a flat memory. Specifically, we propose to
incorporate Maximum Inner Product Search (MIPS) in the training and inference
procedures for our hierarchical memory network. We explore the use of various
state-of-the art approximate MIPS techniques and report results on
SimpleQuestions, a challenging large scale factoid question answering task.
</summary>
    <author>
      <name>Sarath Chandar</name>
    </author>
    <author>
      <name>Sungjin Ahn</name>
    </author>
    <author>
      <name>Hugo Larochelle</name>
    </author>
    <author>
      <name>Pascal Vincent</name>
    </author>
    <author>
      <name>Gerald Tesauro</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07427v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07427v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.00710v1</id>
    <updated>2016-11-02T18:22:33Z</updated>
    <published>2016-11-02T18:22:33Z</published>
    <title>Deep counter networks for asynchronous event-based processing</title>
    <summary>  Despite their advantages in terms of computational resources, latency, and
power consumption, event-based implementations of neural networks have not been
able to achieve the same performance figures as their equivalent
state-of-the-art deep network models. We propose counter neurons as minimal
spiking neuron models which only require addition and comparison operations,
thus avoiding costly multiplications. We show how inference carried out in deep
counter networks converges to the same accuracy levels as are achieved with
state-of-the-art conventional networks. As their event-based style of
computation leads to reduced latency and sparse updates, counter networks are
ideally suited for efficient compact and low-power hardware implementation. We
present theory and training methods for counter networks, and demonstrate on
the MNIST benchmark that counter networks converge quickly, both in terms of
time and number of operations required, to state-of-the-art classification
accuracy.
</summary>
    <author>
      <name>Jonathan Binas</name>
    </author>
    <author>
      <name>Giacomo Indiveri</name>
    </author>
    <author>
      <name>Michael Pfeiffer</name>
    </author>
    <link href="http://arxiv.org/abs/1611.00710v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.00710v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.01775v1</id>
    <updated>2017-03-06T09:21:35Z</updated>
    <published>2017-03-06T09:21:35Z</published>
    <title>Building a Regular Decision Boundary with Deep Networks</title>
    <summary>  In this work, we build a generic architecture of Convolutional Neural
Networks to discover empirical properties of neural networks. Our first
contribution is to introduce a state-of-the-art framework that depends upon few
hyper parameters and to study the network when we vary them. It has no max
pooling, no biases, only 13 layers, is purely convolutional and yields up to
95.4% and 79.6% accuracy respectively on CIFAR10 and CIFAR100. We show that the
nonlinearity of a deep network does not need to be continuous, non expansive or
point-wise, to achieve good performance. We show that increasing the width of
our network permits being competitive with very deep networks. Our second
contribution is an analysis of the contraction and separation properties of
this network. Indeed, a 1-nearest neighbor classifier applied on deep features
progressively improves with depth, which indicates that the representation is
progressively more regular. Besides, we defined and analyzed local support
vectors that separate classes locally. All our experiments are reproducible and
code is available online, based on TensorFlow.
</summary>
    <author>
      <name>Edouard Oyallon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2017, 8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.01775v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.01775v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.08383v1</id>
    <updated>2017-03-24T12:07:34Z</updated>
    <published>2017-03-24T12:07:34Z</published>
    <title>Smart Augmentation - Learning an Optimal Data Augmentation Strategy</title>
    <summary>  A recurring problem faced when training neural networks is that there is
typically not enough data to maximize the generalization capability of deep
neural networks(DNN). There are many techniques to address this, including data
augmentation, dropout, and transfer learning. In this paper, we introduce an
additional method which we call Smart Augmentation and we show how to use it to
increase the accuracy and reduce overfitting on a target network. Smart
Augmentation works by creating a network that learns how to generate augmented
data during the training process of a target network in a way that reduces that
networks loss. This allows us to learn augmentations that minimize the error of
that network.
  Smart Augmentation has shown the potential to increase accuracy by
demonstrably significant measures on all datasets tested. In addition, it has
shown potential to achieve similar or improved performance levels with
significantly smaller network sizes in a number of tested cases.
</summary>
    <author>
      <name>Joseph Lemley</name>
    </author>
    <author>
      <name>Shabab Bazrafkan</name>
    </author>
    <author>
      <name>Peter Corcoran</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ACCESS.2017.2696121</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ACCESS.2017.2696121" rel="related"/>
    <link href="http://arxiv.org/abs/1703.08383v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.08383v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.05152v1</id>
    <updated>2017-05-15T10:44:26Z</updated>
    <published>2017-05-15T10:44:26Z</published>
    <title>Sensitivity of directed networks to the addition and pruning of edges
  and vertices</title>
    <summary>  We study the sensitivity of directed complex networks to the addition and
pruning of edges and vertices and introduce the susceptibility, which
quantifies this sensitivity. We show that topologically different parts of a
directed network have different sensitivity to the addition and pruning of
edges and vertices and, therefore, they are characterized by different
susceptibilities. These susceptibilities diverge at the critical point of the
directed percolation transition, signaling the appearance (or disappearance) of
the giant strongly connected component in the infinite size limit. We
demonstrate this behavior in randomly damaged real and synthetic directed
complex networks, such as the World Wide Web, Twitter, the \emph{Caenorhabditis
elegans} neural network, directed Erd\H{o}s-R\'enyi graphs, and others. We
reveal a non-monotonous dependence of the sensitivity to random pruning of
edges or vertices in the case of \emph{Caenorhabditis elegans} and Twitter that
manifests specific structural peculiarities of these networks. We propose the
measurements of the susceptibilities during the addition or pruning of edges
and vertices as a new method for studying structural peculiarities of directed
networks.
</summary>
    <author>
      <name>A. V. Goltsev</name>
    </author>
    <author>
      <name>G. Tim√°r</name>
    </author>
    <author>
      <name>J. F. F. Mendes</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.96.022317</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.96.022317" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 96, 022317 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1705.05152v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.05152v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.05157v1</id>
    <updated>2017-06-16T06:42:15Z</updated>
    <published>2017-06-16T06:42:15Z</published>
    <title>A Fully Trainable Network with RNN-based Pooling</title>
    <summary>  Pooling is an important component in convolutional neural networks (CNNs) for
aggregating features and reducing computational burden. Compared with other
components such as convolutional layers and fully connected layers which are
completely learned from data, the pooling component is still handcrafted such
as max pooling and average pooling. This paper proposes a learnable pooling
function using recurrent neural networks (RNN) so that the pooling can be fully
adapted to data and other components of the network, leading to an improved
performance. Such a network with learnable pooling function is referred to as a
fully trainable network (FTN). Experimental results have demonstrated that the
proposed RNN-based pooling can well approximate the existing pooling functions
and improve the performance of the network. Especially for small networks, the
proposed FTN can improve the performance by seven percentage points in terms of
error rate on the CIFAR-10 dataset compared with the traditional CNN.
</summary>
    <author>
      <name>Shuai Li</name>
    </author>
    <author>
      <name>Wanqing Li</name>
    </author>
    <author>
      <name>Chris Cook</name>
    </author>
    <author>
      <name>Ce Zhu</name>
    </author>
    <author>
      <name>Yanbo Gao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 5 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.05157v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.05157v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.11383v1</id>
    <updated>2017-11-30T13:32:45Z</updated>
    <published>2017-11-30T13:32:45Z</published>
    <title>Learning to Learn from Weak Supervision by Full Supervision</title>
    <summary>  In this paper, we propose a method for training neural networks when we have
a large set of data with weak labels and a small amount of data with true
labels. In our proposed model, we train two neural networks: a target network,
the learner and a confidence network, the meta-learner. The target network is
optimized to perform a given task and is trained using a large set of unlabeled
data that are weakly annotated. We propose to control the magnitude of the
gradient updates to the target network using the scores provided by the second
confidence network, which is trained on a small amount of supervised data. Thus
we avoid that the weight updates computed from noisy labels harm the quality of
the target network model.
</summary>
    <author>
      <name>Mostafa Dehghani</name>
    </author>
    <author>
      <name>Aliaksei Severyn</name>
    </author>
    <author>
      <name>Sascha Rothe</name>
    </author>
    <author>
      <name>Jaap Kamps</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at NIPS Workshop on Meta-Learning (MetaLearn 2017), Long
  Beach, CA, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.11383v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.11383v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.00289v1</id>
    <updated>2017-12-31T14:32:27Z</updated>
    <published>2017-12-31T14:32:27Z</published>
    <title>Deep Stacked Networks with Residual Polishing for Image Inpainting</title>
    <summary>  Deep neural networks have shown promising results in image inpainting even if
the missing area is relatively large. However, most of the existing inpainting
networks introduce undesired artifacts and noise to the repaired regions. To
solve this problem, we present a novel framework which consists of two stacked
convolutional neural networks that inpaint the image and remove the artifacts,
respectively. The first network considers the global structure of the damaged
image and coarsely fills the blank area. Then the second network modifies the
repaired image to cancel the noise introduced by the first network. The
proposed framework splits the problem into two distinct partitions that can be
optimized separately, therefore it can be applied to any inpainting algorithm
by changing the first network. Second stage in our framework which aims at
polishing the inpainted images can be treated as a denoising problem where a
wide range of algorithms can be employed. Our results demonstrate that the
proposed framework achieves significant improvement on both visual and
quantitative evaluations.
</summary>
    <author>
      <name>Ugur Demir</name>
    </author>
    <author>
      <name>Gozde Unal</name>
    </author>
    <link href="http://arxiv.org/abs/1801.00289v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.00289v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.02642v3</id>
    <updated>2018-01-23T10:24:09Z</updated>
    <published>2018-01-08T19:02:44Z</published>
    <title>Boundary Optimizing Network (BON)</title>
    <summary>  Despite all the success that deep neural networks have seen in classifying
certain datasets, the challenge of finding optimal solutions that generalize
still remains. In this paper, we propose the Boundary Optimizing Network (BON),
a new approach to generalization for deep neural networks when used for
supervised learning. Given a classification network, we propose to use a
collaborative generative network that produces new synthetic data points in the
form of perturbations of original data points. In this way, we create a data
support around each original data point which prevents decision boundaries from
passing too close to the original data points, i.e. prevents overfitting. We
show that BON improves convergence on CIFAR-10 using the state-of-the-art
Densenet. We do however observe that the generative network suffers from
catastrophic forgetting during training, and we therefore propose to use a
variation of Memory Aware Synapses to optimize the generative network (called
BON++). On the Iris dataset, we visualize the effect of BON++ when the
generator does not suffer from catastrophic forgetting and conclude that the
approach has the potential to create better boundaries in a higher dimensional
space.
</summary>
    <author>
      <name>Marco Singh</name>
    </author>
    <author>
      <name>Akshay Pai</name>
    </author>
    <link href="http://arxiv.org/abs/1801.02642v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.02642v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.06105v1</id>
    <updated>2018-01-18T15:54:55Z</updated>
    <published>2018-01-18T15:54:55Z</published>
    <title>Overcoming the vanishing gradient problem in plain recurrent networks</title>
    <summary>  Plain recurrent networks greatly suffer from the vanishing gradient problem
while Gated Neural Networks (GNNs) such as Long-short Term Memory (LSTM) and
Gated Recurrent Unit (GRU) deliver promising results in many sequence learning
tasks through sophisticated network designs. This paper shows how we can
address this problem in a plain recurrent network by analyzing the gating
mechanisms in GNNs. We propose a novel network called the Recurrent Identity
Network (RIN) which allows a plain recurrent network to overcome the vanishing
gradient problem while training very deep models without the use of gates. We
compare this model with IRNNs and LSTMs on multiple sequence modeling
benchmarks. The RINs demonstrate competitive performance and converge faster in
all tasks. Notably, small RIN models produce 12%--67% higher accuracy on the
Sequential and Permuted MNIST datasets and reach state-of-the-art performance
on the bAbI question answering dataset.
</summary>
    <author>
      <name>Yuhuang Hu</name>
    </author>
    <author>
      <name>Adrian Huber</name>
    </author>
    <author>
      <name>Jithendar Anumula</name>
    </author>
    <author>
      <name>Shih-Chii Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.06105v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.06105v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.01588v1</id>
    <updated>2018-03-05T10:17:01Z</updated>
    <published>2018-03-05T10:17:01Z</published>
    <title>N-body Networks: a Covariant Hierarchical Neural Network Architecture
  for Learning Atomic Potentials</title>
    <summary>  We describe N-body networks, a neural network architecture for learning the
behavior and properties of complex many body physical systems. Our specific
application is to learn atomic potential energy surfaces for use in molecular
dynamics simulations. Our architecture is novel in that (a) it is based on a
hierarchical decomposition of the many body system into subsytems, (b) the
activations of the network correspond to the internal state of each subsystem,
(c) the "neurons" in the network are constructed explicitly so as to guarantee
that each of the activations is covariant to rotations, (d) the neurons operate
entirely in Fourier space, and the nonlinearities are realized by tensor
products followed by Clebsch-Gordan decompositions. As part of the description
of our network, we give a characterization of what way the weights of the
network may interact with the activations so as to ensure that the covariance
property is maintained.
</summary>
    <author>
      <name>Risi Kondor</name>
    </author>
    <link href="http://arxiv.org/abs/1803.01588v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.01588v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
