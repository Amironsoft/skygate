<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Aneural%20networks%26id_list%3D%26start%3D0%26max_results%3D2000" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:neural networks&amp;id_list=&amp;start=0&amp;max_results=2000</title>
  <id>http://arxiv.org/api/TfcOcUOuideOGiKyrdODbHq950w</id>
  <updated>2018-03-07T00:00:00-05:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">90292</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">2000</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/cs/0504056v1</id>
    <updated>2005-04-13T13:59:55Z</updated>
    <published>2005-04-13T13:59:55Z</published>
    <title>Self-Organizing Multilayered Neural Networks of Optimal Complexity</title>
    <summary>  The principles of self-organizing the neural networks of optimal complexity
is considered under the unrepresentative learning set. The method of
self-organizing the multi-layered neural networks is offered and used to train
the logical neural networks which were applied to the medical diagnostics.
</summary>
    <author>
      <name>V. Schetinin</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0504056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0608073v1</id>
    <updated>2006-08-18T08:28:23Z</updated>
    <published>2006-08-18T08:28:23Z</published>
    <title>Parametrical Neural Networks and Some Other Similar Architectures</title>
    <summary>  A review of works on associative neural networks accomplished during last
four years in the Institute of Optical Neural Technologies RAS is given. The
presentation is based on description of parametrical neural networks (PNN). For
today PNN have record recognizing characteristics (storage capacity, noise
immunity and speed of operation). Presentation of basic ideas and principles is
accentuated.
</summary>
    <author>
      <name>Leonid B. Litinskii</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 2 figures, accepted for publication in "Optical Memory &amp;
  Neural Networks" (2006)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0608073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0608073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.06246v1</id>
    <updated>2017-01-23T01:26:24Z</updated>
    <published>2017-01-23T01:26:24Z</published>
    <title>Neural network representation of tensor network and chiral states</title>
    <summary>  We study the representational power of a Boltzmann machine (a type of neural
network) in quantum many-body systems. We prove that any (local) tensor network
state has a (local) neural network representation. The construction is almost
optimal in the sense that the number of parameters in the neural network
representation is almost linear in the number of nonzero parameters in the
tensor network representation. Despite the difficulty of representing (gapped)
chiral topological states with local tensor networks, we construct a
quasi-local neural network representation for a chiral p-wave superconductor.
This demonstrates the power of Boltzmann machines.
</summary>
    <author>
      <name>Yichen Huang</name>
    </author>
    <author>
      <name>Joel E. Moore</name>
    </author>
    <link href="http://arxiv.org/abs/1701.06246v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.06246v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07333v1</id>
    <updated>2016-05-24T08:20:12Z</updated>
    <published>2016-05-24T08:20:12Z</published>
    <title>Combining Recurrent and Convolutional Neural Networks for Relation
  Classification</title>
    <summary>  This paper investigates two different neural architectures for the task of
relation classification: convolutional neural networks and recurrent neural
networks. For both models, we demonstrate the effect of different architectural
choices. We present a new context representation for convolutional neural
networks for relation classification (extended middle context). Furthermore, we
propose connectionist bi-directional recurrent neural networks and introduce
ranking loss for their optimization. Finally, we show that combining
convolutional and recurrent neural networks using a simple voting scheme is
accurate enough to improve results. Our neural models achieve state-of-the-art
results on the SemEval 2010 relation classification task.
</summary>
    <author>
      <name>Ngoc Thang Vu</name>
    </author>
    <author>
      <name>Heike Adel</name>
    </author>
    <author>
      <name>Pankaj Gupta</name>
    </author>
    <author>
      <name>Hinrich Sch√ºtze</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NAACL 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07333v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07333v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.5997v2</id>
    <updated>2014-04-26T23:10:51Z</updated>
    <published>2014-04-23T22:37:56Z</published>
    <title>One weird trick for parallelizing convolutional neural networks</title>
    <summary>  I present a new way to parallelize the training of convolutional neural
networks across multiple GPUs. The method scales significantly better than all
alternatives when applied to modern convolutional neural networks.
</summary>
    <author>
      <name>Alex Krizhevsky</name>
    </author>
    <link href="http://arxiv.org/abs/1404.5997v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.5997v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.01439v1</id>
    <updated>2016-10-05T14:26:27Z</updated>
    <published>2016-10-05T14:26:27Z</published>
    <title>Nonlinear Systems Identification Using Deep Dynamic Neural Networks</title>
    <summary>  Neural networks are known to be effective function approximators. Recently,
deep neural networks have proven to be very effective in pattern recognition,
classification tasks and human-level control to model highly nonlinear
realworld systems. This paper investigates the effectiveness of deep neural
networks in the modeling of dynamical systems with complex behavior. Three deep
neural network structures are trained on sequential data, and we investigate
the effectiveness of these networks in modeling associated characteristics of
the underlying dynamical systems. We carry out similar evaluations on select
publicly available system identification datasets. We demonstrate that deep
neural networks are effective model estimators from input-output data
</summary>
    <author>
      <name>Olalekan Ogunmolu</name>
    </author>
    <author>
      <name>Xuejun Gu</name>
    </author>
    <author>
      <name>Steve Jiang</name>
    </author>
    <author>
      <name>Nicholas Gans</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">American Control Conference, 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.01439v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.01439v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.02522v1</id>
    <updated>2016-12-08T03:28:10Z</updated>
    <published>2016-12-08T03:28:10Z</published>
    <title>Geometric Decomposition of Feed Forward Neural Networks</title>
    <summary>  There have been several attempts to mathematically understand neural networks
and many more from biological and computational perspectives. The field has
exploded in the last decade, yet neural networks are still treated much like a
black box. In this work we describe a structure that is inherent to a feed
forward neural network. This will provide a framework for future work on neural
networks to improve training algorithms, compute the homology of the network,
and other applications. Our approach takes a more geometric point of view and
is unlike other attempts to mathematically understand neural networks that rely
on a functional perspective.
</summary>
    <author>
      <name>Sven Cattell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.02522v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.02522v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92B20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.04759v1</id>
    <updated>2017-11-13T18:50:04Z</updated>
    <published>2017-11-13T18:50:04Z</published>
    <title>Neural Networks Architecture Evaluation in a Quantum Computer</title>
    <summary>  In this work, we propose a quantum algorithm to evaluate neural networks
architectures named Quantum Neural Network Architecture Evaluation (QNNAE). The
proposed algorithm is based on a quantum associative memory and the learning
algorithm for artificial neural networks. Unlike conventional algorithms for
evaluating neural network architectures, QNNAE does not depend on
initialization of weights. The proposed algorithm has a binary output and
results in 0 with probability proportional to the performance of the network.
And its computational cost is equal to the computational cost to train a neural
network.
</summary>
    <author>
      <name>Adenilton Jos√© da Silva</name>
    </author>
    <author>
      <name>Rodolfo Luan F. de Oliveira</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/BRACIS.2017.33</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/BRACIS.2017.33" rel="related"/>
    <link href="http://arxiv.org/abs/1711.04759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.04759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.2853v1</id>
    <updated>2013-12-10T16:15:48Z</updated>
    <published>2013-12-10T16:15:48Z</published>
    <title>Performance Analysis Of Neural Network Models For Oxazolines And
  Oxazoles Derivatives Descriptor Dataset</title>
    <summary>  Neural networks have been used successfully to a broad range of areas such as
business, data mining, drug discovery and biology. In medicine, neural networks
have been applied widely in medical diagnosis, detection and evaluation of new
drugs and treatment cost estimation. In addition, neural networks have begin
practice in data mining strategies for the aim of prediction, knowledge
discovery. This paper will present the application of neural networks for the
prediction and analysis of antitubercular activity of Oxazolines and Oxazoles
derivatives. This study presents techniques based on the development of Single
hidden layer neural network (SHLFFNN), Gradient Descent Back propagation neural
network (GDBPNN), Gradient Descent Back propagation with momentum neural
network (GDBPMNN), Back propagation with Weight decay neural network (BPWDNN)
and Quantile regression neural network (QRNN) of artificial neural network
(ANN) models Here, we comparatively evaluate the performance of five neural
network techniques. The evaluation of the efficiency of each model by ways of
benchmark experiments is an accepted application. Cross-validation and
resampling techniques are commonly used to derive point estimates of the
performances which are compared to identify methods with good properties.
Predictive accuracy was evaluated using the root mean squared error (RMSE),
Coefficient determination(???), mean absolute error(MAE), mean percentage
error(MPE) and relative square error(RSE). We found that all five neural
network models were able to produce feasible models. QRNN model is outperforms
with all statistical tests amongst other four models.
</summary>
    <author>
      <name> Doreswamy</name>
    </author>
    <author>
      <name>Chanabasayya . M. Vastrad</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijist.2013.3601</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijist.2013.3601" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">published International Journal of Information Sciences and
  Techniques (IJIST) Vol.3, No.6, November 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1312.2853v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.2853v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.4495v1</id>
    <updated>2010-09-22T22:32:37Z</updated>
    <published>2010-09-22T22:32:37Z</published>
    <title>Unary Coding for Neural Network Learning</title>
    <summary>  This paper presents some properties of unary coding of significance for
biological learning and instantaneously trained neural networks.
</summary>
    <author>
      <name>Subhash Kak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1009.4495v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.4495v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05549v1</id>
    <updated>2017-01-19T18:43:56Z</updated>
    <published>2017-01-19T18:43:56Z</published>
    <title>Deep Neural Networks - A Brief History</title>
    <summary>  Introduction to deep neural networks and their history.
</summary>
    <author>
      <name>Krzysztof J. Cios</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.05549v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05549v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08551v1</id>
    <updated>2016-03-28T20:28:09Z</updated>
    <published>2016-03-28T20:28:09Z</published>
    <title>Genetic cellular neural networks for generating three-dimensional
  geometry</title>
    <summary>  There are a number of ways to procedurally generate interesting
three-dimensional shapes, and a method where a cellular neural network is
combined with a mesh growth algorithm is presented here. The aim is to create a
shape from a genetic code in such a way that a crude search can find
interesting shapes. Identical neural networks are placed at each vertex of a
mesh which can communicate with neural networks on neighboring vertices. The
output of the neural networks determine how the mesh grows, allowing
interesting shapes to be produced emergently, mimicking some of the complexity
of biological organism development. Since the neural networks' parameters can
be freely mutated, the approach is amenable for use in a genetic algorithm.
</summary>
    <author>
      <name>Hugo Martay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.08551v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08551v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92B20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.0324v1</id>
    <updated>2013-02-02T00:43:42Z</updated>
    <published>2013-02-02T00:43:42Z</published>
    <title>A New Constructive Method to Optimize Neural Network Architecture and
  Generalization</title>
    <summary>  In this paper, after analyzing the reasons of poor generalization and
overfitting in neural networks, we consider some noise data as a singular value
of a continuous function - jump discontinuity point. The continuous part can be
approximated with the simplest neural networks, which have good generalization
performance and optimal network architecture, by traditional algorithms such as
constructive algorithm for feed-forward neural networks with incremental
training, BP algorithm, ELM algorithm, various constructive algorithm, RBF
approximation and SVM. At the same time, we will construct RBF neural networks
to fit the singular value with every error in, and we prove that a function
with jumping discontinuity points can be approximated by the simplest neural
networks with a decay RBF neural networks in by each error, and a function with
jumping discontinuity point can be constructively approximated by a decay RBF
neural networks in by each error and the constructive part have no
generalization influence to the whole machine learning system which will
optimize neural network architecture and generalization performance, reduce the
overfitting phenomenon by avoid fitting the noisy data.
</summary>
    <author>
      <name>Hou Muzhou</name>
    </author>
    <author>
      <name>Moon Ho Lee</name>
    </author>
    <link href="http://arxiv.org/abs/1302.0324v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.0324v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="41A99, 65D15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.0930v1</id>
    <updated>2007-07-06T14:18:02Z</updated>
    <published>2007-07-06T14:18:02Z</published>
    <title>Bayesian Learning of Neural Networks for Signal/Background
  Discrimination in Particle Physics</title>
    <summary>  Neural networks are used extensively in classification problems in particle
physics research. Since the training of neural networks can be viewed as a
problem of inference, Bayesian learning of neural networks can provide more
optimal and robust results than conventional learning methods. We have
investigated the use of Bayesian neural networks for signal/background
discrimination in the search for second generation leptoquarks at the Tevatron,
as an example. We present a comparison of the results obtained from the
conventional training of feedforward neural networks and networks trained with
Bayesian methods.
</summary>
    <author>
      <name>Michael Pogwizd</name>
    </author>
    <author>
      <name>Laura Jane Elgass</name>
    </author>
    <author>
      <name>Pushpalatha C. Bhat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 4 figures, conference proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/0707.0930v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.0930v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0308503v1</id>
    <updated>2003-08-25T15:11:24Z</updated>
    <published>2003-08-25T15:11:24Z</published>
    <title>Neural network learning dynamics in a path integral framework</title>
    <summary>  A path-integral formalism is proposed for studying the dynamical evolution in
time of patterns in an artificial neural network in the presence of noise. An
effective cost function is constructed which determines the unique global
minimum of the neural network system. The perturbative method discussed also
provides a way for determining the storage capacity of the network.
</summary>
    <author>
      <name>J. Balakrishnan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s100510051172</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s100510051172" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Eur.Phys.J.B15, 679 (2000)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0308503v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0308503v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.3714v2</id>
    <updated>2014-12-13T00:57:57Z</updated>
    <published>2014-12-11T16:35:27Z</published>
    <title>Feature Weight Tuning for Recursive Neural Networks</title>
    <summary>  This paper addresses how a recursive neural network model can automatically
leave out useless information and emphasize important evidence, in other words,
to perform "weight tuning" for higher-level representation acquisition. We
propose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural
Network (BENN), which automatically control how much one specific unit
contributes to the higher-level representation. The proposed model can be
viewed as incorporating a more powerful compositional function for embedding
acquisition in recursive neural networks. Experimental results demonstrate the
significant improvement over standard neural models.
</summary>
    <author>
      <name>Jiwei Li</name>
    </author>
    <link href="http://arxiv.org/abs/1412.3714v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3714v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0407436v1</id>
    <updated>2004-07-16T12:50:21Z</updated>
    <published>2004-07-16T12:50:21Z</published>
    <title>Neural Networks Processing Mean Values of Random Variables</title>
    <summary>  We introduce a class of neural networks derived from probabilistic models in
the form of Bayesian belief networks. By imposing additional assumptions about
the nature of the probabilistic models represented in the belief networks, we
derive neural networks with standard dynamics that require no training to
determine the synaptic weights, that can pool multiple sources of evidence, and
that deal cleanly and consistently with inconsistent or contradictory evidence.
The presented neural networks capture many properties of Bayesian belief
networks, providing distributed versions of probabilistic models.
</summary>
    <author>
      <name>M. J. Barber</name>
    </author>
    <author>
      <name>J. W. Clark</name>
    </author>
    <author>
      <name>C. H. Anderson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures, 1 table, submitted to Phys Rev E</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0407436v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0407436v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0410492v1</id>
    <updated>2004-10-19T23:08:45Z</updated>
    <published>2004-10-19T23:08:45Z</published>
    <title>Stability of a neural network model with small-world connections</title>
    <summary>  Small-world networks are highly clustered networks with small distances among
the nodes. There are many biological neural networks that present this kind of
connections. There are no special weightings in the connections of most
existing small-world network models. However, this kind of simply-connected
models cannot characterize biological neural networks, in which there are
different weights in synaptic connections. In this paper, we present a neural
network model with weighted small-world connections, and further investigate
the stability of this model.
</summary>
    <author>
      <name>Chunguang Li</name>
    </author>
    <author>
      <name>Guanrong Chen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.68.052901</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.68.052901" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physical Review E, Vol. 68, 052901, 2003</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0410492v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0410492v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.4610v1</id>
    <updated>2010-04-26T19:18:48Z</updated>
    <published>2010-04-26T19:18:48Z</published>
    <title>Mobility Prediction in Wireless Ad Hoc Networks using Neural Networks</title>
    <summary>  Mobility prediction allows estimating the stability of paths in a mobile
wireless Ad Hoc networks. Identifying stable paths helps to improve routing by
reducing the overhead and the number of connection interruptions. In this
paper, we introduce a neural network based method for mobility prediction in Ad
Hoc networks. This method consists of a multi-layer and recurrent neural
network using back propagation through time algorithm for training.
</summary>
    <author>
      <name>Heni Kaaniche</name>
    </author>
    <author>
      <name>Farouk Kamoun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Heni Kaaniche and Farouk Kamoun, "Mobility Prediction in Wireless Ad
  Hoc Networks using Neural Networks", Journal of Telecommunications, Volume 2,
  Issue 1, p95-101, April 2010</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Telecommunications, Volume 2, Issue 1, p95-101, April
  2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.4610v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.4610v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.5326v1</id>
    <updated>2010-04-29T15:35:32Z</updated>
    <published>2010-04-29T15:35:32Z</published>
    <title>Designing neural networks that process mean values of random variables</title>
    <summary>  We introduce a class of neural networks derived from probabilistic models in
the form of Bayesian networks. By imposing additional assumptions about the
nature of the probabilistic models represented in the networks, we derive
neural networks with standard dynamics that require no training to determine
the synaptic weights, that perform accurate calculation of the mean values of
the random variables, that can pool multiple sources of evidence, and that deal
cleanly and consistently with inconsistent or contradictory evidence. The
presented neural networks capture many properties of Bayesian networks,
providing distributed versions of probabilistic models.
</summary>
    <author>
      <name>Michael J. Barber</name>
    </author>
    <author>
      <name>John W. Clark</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, elsarticle</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.5326v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.5326v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.00726v1</id>
    <updated>2015-10-02T20:17:33Z</updated>
    <published>2015-10-02T20:17:33Z</published>
    <title>A Primer on Neural Network Models for Natural Language Processing</title>
    <summary>  Over the past few years, neural networks have re-emerged as powerful
machine-learning models, yielding state-of-the-art results in fields such as
image recognition and speech processing. More recently, neural network models
started to be applied also to textual natural language signals, again with very
promising results. This tutorial surveys neural network models from the
perspective of natural language processing research, in an attempt to bring
natural-language researchers up to speed with the neural techniques. The
tutorial covers input encoding for natural language tasks, feed-forward
networks, convolutional networks, recurrent networks and recursive networks, as
well as the computation graph abstraction for automatic gradient computation.
</summary>
    <author>
      <name>Yoav Goldberg</name>
    </author>
    <link href="http://arxiv.org/abs/1510.00726v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.00726v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.08985v1</id>
    <updated>2015-10-30T06:42:03Z</updated>
    <published>2015-10-30T06:42:03Z</published>
    <title>Prediction-Adaptation-Correction Recurrent Neural Networks for
  Low-Resource Language Speech Recognition</title>
    <summary>  In this paper, we investigate the use of prediction-adaptation-correction
recurrent neural networks (PAC-RNNs) for low-resource speech recognition. A
PAC-RNN is comprised of a pair of neural networks in which a {\it correction}
network uses auxiliary information given by a {\it prediction} network to help
estimate the state probability. The information from the correction network is
also used by the prediction network in a recurrent loop. Our model outperforms
other state-of-the-art neural networks (DNNs, LSTMs) on IARPA-Babel tasks.
Moreover, transfer learning from a language that is similar to the target
language can help improve performance further.
</summary>
    <author>
      <name>Yu Zhang</name>
    </author>
    <author>
      <name>Ekapol Chuangsuwanich</name>
    </author>
    <author>
      <name>James Glass</name>
    </author>
    <author>
      <name>Dong Yu</name>
    </author>
    <link href="http://arxiv.org/abs/1510.08985v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.08985v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00095v1</id>
    <updated>2017-07-01T04:56:08Z</updated>
    <published>2017-07-01T04:56:08Z</published>
    <title>Exploring the Imposition of Synaptic Precision Restrictions For
  Evolutionary Synthesis of Deep Neural Networks</title>
    <summary>  A key contributing factor to incredible success of deep neural networks has
been the significant rise on massively parallel computing devices allowing
researchers to greatly increase the size and depth of deep neural networks,
leading to significant improvements in modeling accuracy. Although deeper,
larger, or complex deep neural networks have shown considerable promise, the
computational complexity of such networks is a major barrier to utilization in
resource-starved scenarios. We explore the synaptogenesis of deep neural
networks in the formation of efficient deep neural network architectures within
an evolutionary deep intelligence framework, where a probabilistic generative
modeling strategy is introduced to stochastically synthesize increasingly
efficient yet effective offspring deep neural networks over generations,
mimicking evolutionary processes such as heredity, random mutation, and natural
selection in a probabilistic manner. In this study, we primarily explore the
imposition of synaptic precision restrictions and its impact on the
evolutionary synthesis of deep neural networks to synthesize more efficient
network architectures tailored for resource-starved scenarios. Experimental
results show significant improvements in synaptic efficiency (~10X decrease for
GoogLeNet-based DetectNet) and inference speed (&gt;5X increase for
GoogLeNet-based DetectNet) while preserving modeling accuracy.
</summary>
    <author>
      <name>Mohammad Javad Shafiee</name>
    </author>
    <author>
      <name>Francis Li</name>
    </author>
    <author>
      <name>Alexander Wong</name>
    </author>
    <link href="http://arxiv.org/abs/1707.00095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.02693v1</id>
    <updated>2015-10-09T15:04:11Z</updated>
    <published>2015-10-09T15:04:11Z</published>
    <title>Feedforward Sequential Memory Neural Networks without Recurrent Feedback</title>
    <summary>  We introduce a new structure for memory neural networks, called feedforward
sequential memory networks (FSMN), which can learn long-term dependency without
using recurrent feedback. The proposed FSMN is a standard feedforward neural
networks equipped with learnable sequential memory blocks in the hidden layers.
In this work, we have applied FSMN to several language modeling (LM) tasks.
Experimental results have shown that the memory blocks in FSMN can learn
effective representations of long history. Experiments have shown that FSMN
based language models can significantly outperform not only feedforward neural
network (FNN) based LMs but also the popular recurrent neural network (RNN)
LMs.
</summary>
    <author>
      <name>ShiLiang Zhang</name>
    </author>
    <author>
      <name>Hui Jiang</name>
    </author>
    <author>
      <name>Si Wei</name>
    </author>
    <author>
      <name>LiRong Dai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.02693v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.02693v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06220v1</id>
    <updated>2016-03-20T14:39:27Z</updated>
    <published>2016-03-20T14:39:27Z</published>
    <title>Flow of Information in Feed-Forward Deep Neural Networks</title>
    <summary>  Feed-forward deep neural networks have been used extensively in various
machine learning applications. Developing a precise understanding of the
underling behavior of neural networks is crucial for their efficient
deployment. In this paper, we use an information theoretic approach to study
the flow of information in a neural network and to determine how entropy of
information changes between consecutive layers. Moreover, using the Information
Bottleneck principle, we develop a constrained optimization problem that can be
used in the training process of a deep neural network. Furthermore, we
determine a lower bound for the level of data representation that can be
achieved in a deep neural network with an acceptable level of distortion.
</summary>
    <author>
      <name>Pejman Khadivi</name>
    </author>
    <author>
      <name>Ravi Tandon</name>
    </author>
    <author>
      <name>Naren Ramakrishnan</name>
    </author>
    <link href="http://arxiv.org/abs/1603.06220v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06220v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.08171v1</id>
    <updated>2017-02-27T08:00:58Z</updated>
    <published>2017-02-27T08:00:58Z</published>
    <title>Fixed-point optimization of deep neural networks with adaptive step size
  retraining</title>
    <summary>  Fixed-point optimization of deep neural networks plays an important role in
hardware based design and low-power implementations. Many deep neural networks
show fairly good performance even with 2- or 3-bit precision when quantized
weights are fine-tuned by retraining. We propose an improved fixedpoint
optimization algorithm that estimates the quantization step size dynamically
during the retraining. In addition, a gradual quantization scheme is also
tested, which sequentially applies fixed-point optimizations from high- to
low-precision. The experiments are conducted for feed-forward deep neural
networks (FFDNNs), convolutional neural networks (CNNs), and recurrent neural
networks (RNNs).
</summary>
    <author>
      <name>Sungho Shin</name>
    </author>
    <author>
      <name>Yoonho Boo</name>
    </author>
    <author>
      <name>Wonyong Sung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is accepted in ICASSP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.08171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.08171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505016v1</id>
    <updated>2005-05-07T20:56:58Z</updated>
    <published>2005-05-07T20:56:58Z</published>
    <title>Visual Character Recognition using Artificial Neural Networks</title>
    <summary>  The recognition of optical characters is known to be one of the earliest
applications of Artificial Neural Networks, which partially emulate human
thinking in the domain of artificial intelligence. In this paper, a simplified
neural approach to recognition of optical or visual characters is portrayed and
discussed. The document is expected to serve as a resource for learners and
amateur investigators in pattern recognition, neural networking and related
disciplines.
</summary>
    <author>
      <name>Shashank Araokar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, tutorial resource</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0406047v1</id>
    <updated>2004-06-24T13:14:58Z</updated>
    <published>2004-06-24T13:14:58Z</published>
    <title>Self-organizing neural networks in classification and image recognition</title>
    <summary>  Self-organizing neural networks are used for brick finding in OPERA
experiment. Self-organizing neural networks and wavelet analysis used for
recognition and extraction of car numbers from images.
</summary>
    <author>
      <name>G. A. Ososkov</name>
    </author>
    <author>
      <name>S. G. Dmitrievskiy</name>
    </author>
    <author>
      <name>A. V. Stadnik</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0406047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0406047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.5081v2</id>
    <updated>2011-06-28T21:32:26Z</updated>
    <published>2011-03-25T20:59:13Z</published>
    <title>Using Variable Threshold to Increase Capacity in a Feedback Neural
  Network</title>
    <summary>  The article presents new results on the use of variable thresholds to
increase the capacity of a feedback neural network. Non-binary networks are
also considered in this analysis.
</summary>
    <author>
      <name>Praveen Kuruvada</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1103.5081v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.5081v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.00712v1</id>
    <updated>2016-12-02T15:46:09Z</updated>
    <published>2016-12-02T15:46:09Z</published>
    <title>Probabilistic Neural Programs</title>
    <summary>  We present probabilistic neural programs, a framework for program induction
that permits flexible specification of both a computational model and inference
algorithm while simultaneously enabling the use of deep neural networks.
Probabilistic neural programs combine a computation graph for specifying a
neural network with an operator for weighted nondeterministic choice. Thus, a
program describes both a collection of decisions as well as the neural network
architecture used to make each one. We evaluate our approach on a challenging
diagram question answering task where probabilistic neural programs correctly
execute nearly twice as many programs as a baseline model.
</summary>
    <author>
      <name>Kenton W. Murray</name>
    </author>
    <author>
      <name>Jayant Krishnamurthy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in NAMPI workshop at NIPS 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.00712v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.00712v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.3115v1</id>
    <updated>2014-03-12T21:19:26Z</updated>
    <published>2014-03-12T21:19:26Z</published>
    <title>Memory Capacity of Neural Networks using a Circulant Weight Matrix</title>
    <summary>  This paper presents results on the memory capacity of a generalized feedback
neural network using a circulant matrix. Children are capable of learning soon
after birth which indicates that the neural networks of the brain have prior
learnt capacity that is a consequence of the regular structures in the brain's
organization. Motivated by this idea, we consider the capacity of circulant
matrices as weight matrices in a feedback network.
</summary>
    <author>
      <name>Vamsi Sashank Kotagiri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.3115v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.3115v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04682v1</id>
    <updated>2016-08-16T17:38:45Z</updated>
    <published>2016-08-16T17:38:45Z</published>
    <title>Extent of error control in neural networks</title>
    <summary>  The article sets and solves the task to control an error of the artificial
neural network with variable signal conductivity. This kind of neural networks
was especially developed to construct timetables. Behavior of such a neural
network can be described as dynamic system control problem. The authors gave as
the results of the solving the ANN feedback control problem.
</summary>
    <author>
      <name>Alexander Ignatenkov</name>
    </author>
    <author>
      <name>Alexey Olshansky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure, 11 references</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.04682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="49L20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.01430v2</id>
    <updated>2016-10-06T13:28:41Z</updated>
    <published>2016-10-05T14:14:51Z</published>
    <title>LAYERS: Yet another Neural Network toolkit</title>
    <summary>  Layers is an open source neural network toolkit aim at providing an easy way
to implement modern neural networks. The main user target are students and to
this end layers provides an easy scriptting language that can be early adopted.
The user has to focus only on design details as network totpology and parameter
tunning.
</summary>
    <author>
      <name>Roberto Paredes</name>
    </author>
    <author>
      <name>Jos√©-Miguel Bened√≠</name>
    </author>
    <link href="http://arxiv.org/abs/1610.01430v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.01430v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.06511v2</id>
    <updated>2017-10-02T22:45:53Z</updated>
    <published>2017-06-20T15:17:37Z</published>
    <title>Optimal modularity and memory capacity of neural networks</title>
    <summary>  The neural network is a powerful computing framework that has been exploited
by biological evolution and by humans for solving diverse problems. Although
the computational capabilities of neural networks are determined by their
structure, the current understanding of the relationships between a neural
network's architecture and function is still primitive. Here we reveal that
neural network's modular architecture plays a vital role in determining the
neural dynamics and memory performance of the network. In particular, we
demonstrate that there exists an optimal modularity for memory performance,
where a balance between local cohesion and global connectivity is established,
allowing optimally modular networks to remember longer. Our results suggest
that insights from dynamical analysis of neural networks and information
spreading processes can be leveraged to better design neural networks and may
shed light on the brain's modular organization.
</summary>
    <author>
      <name>Nathaniel Rodriguez</name>
    </author>
    <author>
      <name>Eduardo Izquierdo</name>
    </author>
    <author>
      <name>Yong-Yeol Ahn</name>
    </author>
    <link href="http://arxiv.org/abs/1706.06511v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.06511v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.07252v1</id>
    <updated>2017-08-24T02:14:50Z</updated>
    <published>2017-08-24T02:14:50Z</published>
    <title>A Study on Neural Network Language Modeling</title>
    <summary>  An exhaustive study on neural network language modeling (NNLM) is performed
in this paper. Different architectures of basic neural network language models
are described and examined. A number of different improvements over basic
neural network language models, including importance sampling, word classes,
caching and bidirectional recurrent neural network (BiRNN), are studied
separately, and the advantages and disadvantages of every technique are
evaluated. Then, the limits of neural network language modeling are explored
from the aspects of model architecture and knowledge representation. Part of
the statistical information from a word sequence will loss when it is processed
word by word in a certain order, and the mechanism of training neural network
by updating weight matrixes and vectors imposes severe restrictions on any
significant enhancement of NNLM. For knowledge representation, the knowledge
represented by neural network language models is the approximate probabilistic
distribution of word sequences from a certain training data set rather than the
knowledge of a language itself or the information conveyed by word sequences in
a natural language. Finally, some directions for improving neural network
language modeling further is discussed.
</summary>
    <author>
      <name>Dengliang Shi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.07252v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.07252v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.04035v1</id>
    <updated>2016-12-13T05:40:20Z</updated>
    <published>2016-12-13T05:40:20Z</published>
    <title>DizzyRNN: Reparameterizing Recurrent Neural Networks for Norm-Preserving
  Backpropagation</title>
    <summary>  The vanishing and exploding gradient problems are well-studied obstacles that
make it difficult for recurrent neural networks to learn long-term time
dependencies. We propose a reparameterization of standard recurrent neural
networks to update linear transformations in a provably norm-preserving way
through Givens rotations. Additionally, we use the absolute value function as
an element-wise non-linearity to preserve the norm of backpropagated signals
over the entire network. We show that this reparameterization reduces the
number of parameters and maintains the same algorithmic complexity as a
standard recurrent neural network, while outperforming standard recurrent
neural networks with orthogonal initializations and Long Short-Term Memory
networks on the copy problem.
</summary>
    <author>
      <name>Victor Dorobantu</name>
    </author>
    <author>
      <name>Per Andre Stromhaug</name>
    </author>
    <author>
      <name>Jess Renteria</name>
    </author>
    <link href="http://arxiv.org/abs/1612.04035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.04035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.06729v1</id>
    <updated>2017-07-21T00:50:46Z</updated>
    <published>2017-07-21T00:50:46Z</published>
    <title>Predictive networking and optimization for flow-based networks</title>
    <summary>  Artificial Neural Networks (ANNs) were used to classify neural network flows
by flow size. After training the neural network was able to predict the size of
a flows with 87% accuracy with a Feed Forward Neural Network. This demonstrates
that flow based routers can prioritize candidate flows with a predicted large
number of packets for priority insertion into hardware content-addressable
memory.
</summary>
    <author>
      <name>Michael Arnold</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A thesis submitted for the Master of Science Degree at The University
  of Alabama in Huntsville</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.06729v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.06729v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.01427v3</id>
    <updated>2017-03-30T19:51:47Z</updated>
    <published>2016-11-04T15:47:32Z</published>
    <title>Sparsely-Connected Neural Networks: Towards Efficient VLSI
  Implementation of Deep Neural Networks</title>
    <summary>  Recently deep neural networks have received considerable attention due to
their ability to extract and represent high-level abstractions in data sets.
Deep neural networks such as fully-connected and convolutional neural networks
have shown excellent performance on a wide range of recognition and
classification tasks. However, their hardware implementations currently suffer
from large silicon area and high power consumption due to the their high degree
of complexity. The power/energy consumption of neural networks is dominated by
memory accesses, the majority of which occur in fully-connected networks. In
fact, they contain most of the deep neural network parameters. In this paper,
we propose sparsely-connected networks, by showing that the number of
connections in fully-connected networks can be reduced by up to 90% while
improving the accuracy performance on three popular datasets (MNIST, CIFAR10
and SVHN). We then propose an efficient hardware architecture based on
linear-feedback shift registers to reduce the memory requirements of the
proposed sparsely-connected networks. The proposed architecture can save up to
90% of memory compared to the conventional implementations of fully-connected
neural networks. Moreover, implementation results show up to 84% reduction in
the energy consumption of a single neuron of the proposed sparsely-connected
networks compared to a single neuron of fully-connected neural networks.
</summary>
    <author>
      <name>Arash Ardakani</name>
    </author>
    <author>
      <name>Carlo Condo</name>
    </author>
    <author>
      <name>Warren J. Gross</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at ICLR 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.01427v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.01427v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9811031v1</id>
    <updated>1998-11-24T23:33:12Z</updated>
    <published>1998-11-24T23:33:12Z</published>
    <title>Speech Synthesis with Neural Networks</title>
    <summary>  Text-to-speech conversion has traditionally been performed either by
concatenating short samples of speech or by using rule-based systems to convert
a phonetic representation of speech into an acoustic representation, which is
then converted into speech. This paper describes a system that uses a
time-delay neural network (TDNN) to perform this phonetic-to-acoustic mapping,
with another neural network to control the timing of the generated speech. The
neural network system requires less memory than a concatenation system, and
performed well in tests comparing it to commercial systems using other
technologies.
</summary>
    <author>
      <name>Orhan Karaali</name>
    </author>
    <author>
      <name>Gerald Corrigan</name>
    </author>
    <author>
      <name>Ira Gerson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, PostScript</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">World Congress on Neural Networks (1996) 45-50. San Diego</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9811031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9811031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/nlin/0202038v1</id>
    <updated>2002-02-19T12:39:23Z</updated>
    <published>2002-02-19T12:39:23Z</published>
    <title>On model selection and the disability of neural networks to decompose
  tasks</title>
    <summary>  A neural network with fixed topology can be regarded as a parametrization of
functions, which decides on the correlations between functional variations when
parameters are adapted. We propose an analysis, based on a differential
geometry point of view, that allows to calculate these correlations. In
practise, this describes how one response is unlearned while another is
trained. Concerning conventional feed-forward neural networks we find that they
generically introduce strong correlations, are predisposed to forgetting, and
inappropriate for task decomposition. Perspectives to solve these problems are
discussed.
</summary>
    <author>
      <name>Marc Toussaint</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LaTeX, 7 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Joint Conference on Neural
  Networks (IJCNN 2002), 245-250.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/nlin/0202038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/nlin/0202038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.4170v1</id>
    <updated>2012-02-19T16:56:45Z</updated>
    <published>2012-02-19T16:56:45Z</published>
    <title>Classification by Ensembles of Neural Networks</title>
    <summary>  We introduce a new procedure for training of artificial neural networks by
using the approximation of an objective function by arithmetic mean of an
ensemble of selected randomly generated neural networks, and apply this
procedure to the classification (or pattern recognition) problem. This approach
differs from the standard one based on the optimization theory. In particular,
any neural network from the mentioned ensemble may not be an approximation of
the objective function.
</summary>
    <author>
      <name>S. V. Kozyrev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, LaTeX</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">p-Adic Numbers, Ultrametric Analysis and Applications, 2012, Vol.
  4, No. 1, pp. 27-33</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1202.4170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.4170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.2468v1</id>
    <updated>2014-01-10T21:09:36Z</updated>
    <published>2014-01-10T21:09:36Z</published>
    <title>N2Sky - Neural Networks as Services in the Clouds</title>
    <summary>  We present the N2Sky system, which provides a framework for the exchange of
neural network specific knowledge, as neural network paradigms and objects, by
a virtual organization environment. It follows the sky computing paradigm
delivering ample resources by the usage of federated Clouds. N2Sky is a novel
Cloud-based neural network simulation environment, which follows a pure service
oriented approach. The system implements a transparent environment aiming to
enable both novice and experienced users to do neural network research easily
and comfortably. N2Sky is built using the RAVO reference architecture of
virtual organizations which allows itself naturally integrating into the Cloud
service stack (SaaS, PaaS, and IaaS) of service oriented architectures.
</summary>
    <author>
      <name>Erich Schikuta</name>
    </author>
    <author>
      <name>Erwin Mann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">extended version of paper published at IJCNN 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.2468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.2468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.5; I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.06662v1</id>
    <updated>2017-09-19T22:21:49Z</updated>
    <published>2017-09-19T22:21:49Z</published>
    <title>Verifying Properties of Binarized Deep Neural Networks</title>
    <summary>  Understanding properties of deep neural networks is an important challenge in
deep learning. In this paper, we take a step in this direction by proposing a
rigorous way of verifying properties of a popular class of neural networks,
Binarized Neural Networks, using the well-developed means of Boolean
satisfiability. Our main contribution is a construction that creates a
representation of a binarized neural network as a Boolean formula. Our encoding
is the first exact Boolean representation of a deep neural network. Using this
encoding, we leverage the power of modern SAT solvers along with a proposed
counterexample-guided search procedure to verify various properties of these
networks. A particular focus will be on the critical property of robustness to
adversarial perturbations. For this property, our experimental results
demonstrate that our approach scales to medium-size deep neural networks used
in image classification tasks. To the best of our knowledge, this is the first
work on verifying properties of deep neural networks using an exact Boolean
encoding of the network.
</summary>
    <author>
      <name>Nina Narodytska</name>
    </author>
    <author>
      <name>Shiva Prasad Kasiviswanathan</name>
    </author>
    <author>
      <name>Leonid Ryzhyk</name>
    </author>
    <author>
      <name>Mooly Sagiv</name>
    </author>
    <author>
      <name>Toby Walsh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.06662v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.06662v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.1164v1</id>
    <updated>2010-02-05T09:20:51Z</updated>
    <published>2010-02-05T09:20:51Z</published>
    <title>Existence and Global Logarithmic Stability of Impulsive Neural Networks
  with Time Delay</title>
    <summary>  The stability and convergence of the neural networks are the fundamental
characteristics in the Hopfield type networks. Since time delay is ubiquitous
in most physical and biological systems, more attention is being made for the
delayed neural networks. The inclusion of time delay into a neural model is
natural due to the finite transmission time of the interactions. The stability
analysis of the neural networks depends on the Lyapunov function and hence it
must be constructed for the given system. In this paper we have made an attempt
to establish the logarithmic stability of the impulsive delayed neural networks
by constructing suitable Lyapunov function.
</summary>
    <author>
      <name>A. K. Ojha</name>
    </author>
    <author>
      <name>Dushmanta Mallick</name>
    </author>
    <author>
      <name>C. Mallick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues, IJCSI, Vol. 7,
  Issue 1, No. 2, January 2010,
  http://ijcsi.org/articles/Existence-and-Global-Logarithmic-Stability-of-Impulsive-Neural-Networks-with-Time-Delay.php</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues, IJCSI, Vol. 7,
  Issue 1, No. 2, January 2010,
  http://ijcsi.org/articles/Existence-and-Global-Logarithmic-Stability-of-Impulsive-Neural-Networks-with-Time-Delay.php</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1002.1164v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.1164v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.0215v1</id>
    <updated>2012-12-02T15:07:56Z</updated>
    <published>2012-12-02T15:07:56Z</published>
    <title>Artificial Neural Network for Performance Modeling and Optimization of
  CMOS Analog Circuits</title>
    <summary>  This paper presents an implementation of multilayer feed forward neural
networks (NN) to optimize CMOS analog circuits. For modeling and design
recently neural network computational modules have got acceptance as an
unorthodox and useful tool. To achieve high performance of active or passive
circuit component neural network can be trained accordingly. A well trained
neural network can produce more accurate outcome depending on its learning
capability. Neural network model can replace empirical modeling solutions
limited by range and accuracy.[2] Neural network models are easy to obtain for
new circuits or devices which can replace analytical methods. Numerical
modeling methods can also be replaced by neural network model due to their
computationally expansive behavior.[2][10][20]. The pro- posed implementation
is aimed at reducing resource requirement, without much compromise on the
speed. The NN ensures proper functioning by assigning the appropriate inputs,
weights, biases, and excitation function of the layer that is currently being
computed. The concept used is shown to be very effective in reducing resource
requirements and enhancing speed.
</summary>
    <author>
      <name>Mriganka Chakraborty</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/9380-3731</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/9380-3731" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications November 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.0215v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.0215v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.4855v1</id>
    <updated>2012-09-20T14:14:59Z</updated>
    <published>2012-09-20T14:14:59Z</published>
    <title>The Future of Neural Networks</title>
    <summary>  The paper describes some recent developments in neural networks and discusses
the applicability of neural networks in the development of a machine that
mimics the human brain. The paper mentions a new architecture, the pulsed
neural network that is being considered as the next generation of neural
networks. The paper also explores the use of memristors in the development of a
brain-like computer called the MoNETA. A new model, multi/infinite dimensional
neural networks, are a recent development in the area of advanced neural
networks. The paper concludes that the need of neural networks in the
development of human-like technology is essential and may be non-expendable for
it.
</summary>
    <author>
      <name>Sachin Lakra</name>
    </author>
    <author>
      <name>T. V. Prasad</name>
    </author>
    <author>
      <name>G. Ramakrishna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Published in proceedings of 6th National Conference on Computing
  for Nation Development, INDIACom 2012, New Delhi, India, 23-24 February,
  2012, pp. 481-486</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1209.4855v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.4855v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.05683v1</id>
    <updated>2017-06-18T16:30:25Z</updated>
    <published>2017-06-18T16:30:25Z</published>
    <title>Sparse Neural Networks Topologies</title>
    <summary>  We propose Sparse Neural Network architectures that are based on random or
structured bipartite graph topologies. Sparse architectures provide compression
of the models learned and speed-ups of computations, they can also surpass
their unstructured or fully connected counterparts. As we show, even more
compact topologies of the so-called SNN (Sparse Neural Network) can be achieved
with the use of structured graphs of connections between consecutive layers of
neurons. In this paper, we investigate how the accuracy and training speed of
the models depend on the topology and sparsity of the neural network. Previous
approaches using sparcity are all based on fully connected neural network
models and create sparcity during training phase, instead we explicitly define
a sparse architectures of connections before the training. Building compact
neural network models is coherent with empirical observations showing that
there is much redundancy in learned neural network models. We show
experimentally that the accuracy of the models learned with neural networks
depends on expander-like properties of the underlying topologies such as the
spectral gap and algebraic connectivity rather than the density of the graphs
of connections.
</summary>
    <author>
      <name>Alfred Bourely</name>
    </author>
    <author>
      <name>John Patrick Boueri</name>
    </author>
    <author>
      <name>Krzysztof Choromonski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.05683v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.05683v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.07373v1</id>
    <updated>2016-02-24T02:39:47Z</updated>
    <published>2016-02-24T02:39:47Z</published>
    <title>On Study of the Binarized Deep Neural Network for Image Classification</title>
    <summary>  Recently, the deep neural network (derived from the artificial neural
network) has attracted many researchers' attention by its outstanding
performance. However, since this network requires high-performance GPUs and
large storage, it is very hard to use it on individual devices. In order to
improve the deep neural network, many trials have been made by refining the
network structure or training strategy. Unlike those trials, in this paper, we
focused on the basic propagation function of the artificial neural network and
proposed the binarized deep neural network. This network is a pure binary
system, in which all the values and calculations are binarized. As a result,
our network can save a lot of computational resource and storage. Therefore, it
is possible to use it on various devices. Moreover, the experimental results
proved the feasibility of the proposed network.
</summary>
    <author>
      <name>Song Wang</name>
    </author>
    <author>
      <name>Dongchun Ren</name>
    </author>
    <author>
      <name>Li Chen</name>
    </author>
    <author>
      <name>Wei Fan</name>
    </author>
    <author>
      <name>Jun Sun</name>
    </author>
    <author>
      <name>Satoshi Naoi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures. Rejected conference (CVPR 2015) submission.
  Submission date: November, 2014. This work is patented in China (NO.
  201410647710.3)</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.07373v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.07373v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09444v1</id>
    <updated>2016-11-29T00:39:45Z</updated>
    <published>2016-11-29T00:39:45Z</published>
    <title>The empirical size of trained neural networks</title>
    <summary>  ReLU neural networks define piecewise linear functions of their inputs.
However, initializing and training a neural network is very different from
fitting a linear spline. In this paper, we expand empirically upon previous
theoretical work to demonstrate features of trained neural networks. Standard
network initialization and training produce networks vastly simpler than a
naive parameter count would suggest and can impart odd features to the trained
network. However, we also show the forced simplicity is beneficial and, indeed,
critical for the wide success of these networks.
</summary>
    <author>
      <name>Kevin K. Chen</name>
    </author>
    <author>
      <name>Anthony Gamst</name>
    </author>
    <author>
      <name>Alden Walker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.09444v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09444v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.04747v1</id>
    <updated>2017-08-16T02:17:59Z</updated>
    <published>2017-08-16T02:17:59Z</published>
    <title>An Improved Neural Segmentation Method Based on U-NET</title>
    <summary>  Neural segmentation has a great impact on the smooth implementation of local
anesthesia surgery. At present, the network for the segmentation includes U-NET
[1] and SegNet [2]. U-NET network has short training time and less training
parameters, but the depth is not deep enough. SegNet network has deeper
structure, but it needs longer training time, and more training samples. In
this paper, we propose an improved U-NET neural network for the segmentation.
This network deepens the original structure through importing residual network.
Compared with U-NET and SegNet, the improved U-NET network has fewer training
parameters, shorter training time and get a great improvement in segmentation
effect. The improved U-NET network structure has a good application scene in
neural segmentation.
</summary>
    <author>
      <name>Chenyang Xu</name>
    </author>
    <author>
      <name>Mengxin Li</name>
    </author>
    <link href="http://arxiv.org/abs/1708.04747v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.04747v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/chao-dyn/9701021v1</id>
    <updated>1997-01-22T08:41:37Z</updated>
    <published>1997-01-22T08:41:37Z</published>
    <title>Chaotic Simulated Annealing by A Neural Network Model with Transient
  Chaos</title>
    <summary>  We propose a neural network model with transient chaos, or a transiently
chaotic neural network (TCNN) as an approximation method for combinatorial
optimization problem, by introducing transiently chaotic dynamics into neural
networks. Unlike conventional neural networks only with point attractors, the
proposed neural network has richer and more flexible dynamics, so that it can
be expected to have higher ability of searching for globally optimal or
near-optimal solutions. A significant property of this model is that the
chaotic neurodynamics is temporarily generated for searching and
self-organizing, and eventually vanishes with autonomous decreasing of a
bifurcation parameter corresponding to the "temperature" in usual annealing
process. Therefore, the neural network gradually approaches, through the
transient chaos, to dynamical structure similar to such conventional models as
the Hopfield neural network which converges to a stable equilibrium point.
Since the optimization process of the transiently chaotic neural network is
similar to simulated annealing, not in a stochastic way but in a
deterministically chaotic way, the new method is regarded as chaotic simulated
annealing (CSA). Fundamental characteristics of the transiently chaotic
neurodynamics are numerically investigated with examples of a single neuron
model and the Traveling Salesman Problem (TSP). Moreover, a maintenance
scheduling problem for generators in a practical power system is also analysed
to verify practical efficiency of this new method.
</summary>
    <author>
      <name>Luonan Chen</name>
    </author>
    <author>
      <name>Kazuyuki Aihara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">the theoretical results related to this paper should be referred to
  "Chaos and Asymptotical Stability in Discrete-time Neural Networks" by L.Chen
  and K.Aihara, Physica D (in press). Journal ref.: Neural Networks, Vol.8,
  No.6, pp.915-930, 1995</arxiv:comment>
    <link href="http://arxiv.org/abs/chao-dyn/9701021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/chao-dyn/9701021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="chao-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="chao-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.0213v1</id>
    <updated>2007-10-01T06:51:42Z</updated>
    <published>2007-10-01T06:51:42Z</published>
    <title>Optimising the topology of complex neural networks</title>
    <summary>  In this paper, we study instances of complex neural networks, i.e. neural
netwo rks with complex topologies. We use Self-Organizing Map neural networks
whose n eighbourhood relationships are defined by a complex network, to
classify handwr itten digits. We show that topology has a small impact on
performance and robus tness to neuron failures, at least at long learning
times. Performance may howe ver be increased (by almost 10%) by artificial
evolution of the network topo logy. In our experimental conditions, the evolved
networks are more random than their parents, but display a more heterogeneous
degree distribution.
</summary>
    <author>
      <name>Fei Jiang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs, INRIA Futurs</arxiv:affiliation>
    </author>
    <author>
      <name>Hugues Berry</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs</arxiv:affiliation>
    </author>
    <author>
      <name>Marc Schoenauer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans ECCS'07 (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.0213v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.0213v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.1141v2</id>
    <updated>2014-10-28T19:14:37Z</updated>
    <published>2014-10-05T10:54:07Z</published>
    <title>On the Computational Efficiency of Training Neural Networks</title>
    <summary>  It is well-known that neural networks are computationally hard to train. On
the other hand, in practice, modern day neural networks are trained efficiently
using SGD and a variety of tricks that include different activation functions
(e.g. ReLU), over-specification (i.e., train networks which are larger than
needed), and regularization. In this paper we revisit the computational
complexity of training neural networks from a modern perspective. We provide
both positive and negative results, some of them yield new provably efficient
and practical algorithms for training certain types of neural networks.
</summary>
    <author>
      <name>Roi Livni</name>
    </author>
    <author>
      <name>Shai Shalev-Shwartz</name>
    </author>
    <author>
      <name>Ohad Shamir</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Section 2 is revised due to a mistake</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.1141v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.1141v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.01000v1</id>
    <updated>2016-09-04T23:57:43Z</updated>
    <published>2016-09-04T23:57:43Z</published>
    <title>Convexified Convolutional Neural Networks</title>
    <summary>  We describe the class of convexified convolutional neural networks (CCNNs),
which capture the parameter sharing of convolutional neural networks in a
convex manner. By representing the nonlinear convolutional filters as vectors
in a reproducing kernel Hilbert space, the CNN parameters can be represented as
a low-rank matrix, which can be relaxed to obtain a convex optimization
problem. For learning two-layer convolutional neural networks, we prove that
the generalization error obtained by a convexified CNN converges to that of the
best possible CNN. For learning deeper networks, we train CCNNs in a layer-wise
manner. Empirically, CCNNs achieve performance competitive with CNNs trained by
backpropagation, SVMs, fully-connected neural networks, stacked denoising
auto-encoders, and other baseline methods.
</summary>
    <author>
      <name>Yuchen Zhang</name>
    </author>
    <author>
      <name>Percy Liang</name>
    </author>
    <author>
      <name>Martin J. Wainwright</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.01000v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.01000v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.06570v1</id>
    <updated>2017-10-18T03:05:47Z</updated>
    <published>2017-10-18T03:05:47Z</published>
    <title>A Correspondence Between Random Neural Networks and Statistical Field
  Theory</title>
    <summary>  A number of recent papers have provided evidence that practical design
questions about neural networks may be tackled theoretically by studying the
behavior of random networks. However, until now the tools available for
analyzing random neural networks have been relatively ad-hoc. In this work, we
show that the distribution of pre-activations in random neural networks can be
exactly mapped onto lattice models in statistical physics. We argue that
several previous investigations of stochastic networks actually studied a
particular factorial approximation to the full lattice model. For random linear
networks and random rectified linear networks we show that the corresponding
lattice models in the wide network limit may be systematically approximated by
a Gaussian distribution with covariance between the layers of the network. In
each case, the approximate distribution can be diagonalized by Fourier
transformation. We show that this approximation accurately describes the
results of numerical simulations of wide random neural networks. Finally, we
demonstrate that in each case the large scale behavior of the random networks
can be approximated by an effective field theory.
</summary>
    <author>
      <name>Samuel S. Schoenholz</name>
    </author>
    <author>
      <name>Jeffrey Pennington</name>
    </author>
    <author>
      <name>Jascha Sohl-Dickstein</name>
    </author>
    <link href="http://arxiv.org/abs/1710.06570v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.06570v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.05267v1</id>
    <updated>2016-09-16T18:10:18Z</updated>
    <published>2016-09-16T18:10:18Z</published>
    <title>Rule Extraction Algorithm for Deep Neural Networks: A Review</title>
    <summary>  Despite the highest classification accuracy in wide varieties of application
areas, artificial neural network has one disadvantage. The way this Network
comes to a decision is not easily comprehensible. The lack of explanation
ability reduces the acceptability of neural network in data mining and decision
system. This drawback is the reason why researchers have proposed many rule
extraction algorithms to solve the problem. Recently, Deep Neural Network (DNN)
is achieving a profound result over the standard neural network for
classification and recognition problems. It is a hot machine learning area
proven both useful and innovative. This paper has thoroughly reviewed various
rule extraction algorithms, considering the classification scheme:
decompositional, pedagogical, and eclectics. It also presents the evaluation of
these algorithms based on the neural network structure with which the algorithm
is intended to work. The main contribution of this review is to show that there
is a limited study of rule extraction algorithm from DNN.
</summary>
    <author>
      <name>Tameru Hailesilassie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages,2 figures,IEEE Publication format, Keywords- Artificial
  neural network; Deep neural network; Rule extraction; Decompositional;
  Pedagogical; Eclectic</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">(IJCSIS) International Journal of Computer Science and Information
  Security,Vol. 14, No. 7, July 2016, page 371-381</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.05267v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.05267v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.0936v1</id>
    <updated>2009-12-04T21:33:10Z</updated>
    <published>2009-12-04T21:33:10Z</published>
    <title>Neural-estimator for the surface emission rate of atmospheric gases</title>
    <summary>  The emission rate of minority atmospheric gases is inferred by a new approach
based on neural networks. The neural network applied is the multi-layer
perceptron with backpropagation algorithm for learning. The identification of
these surface fluxes is an inverse problem. A comparison between the new
neural-inversion and regularized inverse solution id performed. The results
obtained from the neural networks are significantly better. In addition, the
inversion with the neural netwroks is fster than regularized approaches, after
training.
</summary>
    <author>
      <name>F. F. Paes</name>
    </author>
    <author>
      <name>H. F. Campos Velho</name>
    </author>
    <link href="http://arxiv.org/abs/0912.0936v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.0936v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.05955v1</id>
    <updated>2017-03-17T10:46:23Z</updated>
    <published>2017-03-17T10:46:23Z</published>
    <title>Implicit Gradient Neural Networks with a Positive-Definite Mass Matrix
  for Online Linear Equations Solving</title>
    <summary>  Motivated by the advantages achieved by implicit analogue net for solving
online linear equations, a novel implicit neural model is designed based on
conventional explicit gradient neural networks in this letter by introducing a
positive-definite mass matrix. In addition to taking the advantages of the
implicit neural dynamics, the proposed implicit gradient neural networks can
still achieve globally exponential convergence to the unique theoretical
solution of linear equations and also global stability even under no-solution
and multi-solution situations. Simulative results verify theoretical
convergence analysis on the proposed neural dynamics.
</summary>
    <author>
      <name>Ke Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Information Processing Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.05955v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.05955v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.1231v1</id>
    <updated>2014-06-04T23:00:05Z</updated>
    <published>2014-06-04T23:00:05Z</published>
    <title>Multi-task Neural Networks for QSAR Predictions</title>
    <summary>  Although artificial neural networks have occasionally been used for
Quantitative Structure-Activity/Property Relationship (QSAR/QSPR) studies in
the past, the literature has of late been dominated by other machine learning
techniques such as random forests. However, a variety of new neural net
techniques along with successful applications in other domains have renewed
interest in network approaches. In this work, inspired by the winning team's
use of neural networks in a recent QSAR competition, we used an artificial
neural network to learn a function that predicts activities of compounds for
multiple assays at the same time. We conducted experiments leveraging recent
methods for dealing with overfitting in neural networks as well as other tricks
from the neural networks literature. We compared our methods to alternative
methods reported to perform well on these tasks and found that our neural net
methods provided superior performance.
</summary>
    <author>
      <name>George E. Dahl</name>
    </author>
    <author>
      <name>Navdeep Jaitly</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <link href="http://arxiv.org/abs/1406.1231v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.1231v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04153v1</id>
    <updated>2016-04-14T13:48:26Z</updated>
    <published>2016-04-14T13:48:26Z</published>
    <title>Learning to Generate Genotypes with Neural Networks</title>
    <summary>  Neural networks and evolutionary computation have a rich intertwined history.
They most commonly appear together when an evolutionary algorithm optimises the
parameters and topology of a neural network for reinforcement learning
problems, or when a neural network is applied as a surrogate fitness function
to aid the evolutionary optimisation of expensive fitness functions. In this
paper we take a different approach, asking the question of whether a neural
network can be used to provide a mutation distribution for an evolutionary
algorithm, and what advantages this approach may offer? Two modern neural
network models are investigated, a Denoising Autoencoder modified to produce
stochastic outputs and the Neural Autoregressive Distribution Estimator.
Results show that the neural network approach to learning genotypes is able to
solve many difficult discrete problems, such as MaxSat and HIFF, and regularly
outperforms other evolutionary techniques.
</summary>
    <author>
      <name>Alexander W. Churchill</name>
    </author>
    <author>
      <name>Siddharth Sigtia</name>
    </author>
    <author>
      <name>Chrisantha Fernando</name>
    </author>
    <link href="http://arxiv.org/abs/1604.04153v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04153v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.04781v2</id>
    <updated>2015-11-06T19:41:13Z</updated>
    <published>2015-10-16T05:37:06Z</published>
    <title>A Survey: Time Travel in Deep Learning Space: An Introduction to Deep
  Learning Models and How Deep Learning Models Evolved from the Initial Ideas</title>
    <summary>  This report will show the history of deep learning evolves. It will trace
back as far as the initial belief of connectionism modelling of brain, and come
back to look at its early stage realization: neural networks. With the
background of neural network, we will gradually introduce how convolutional
neural network, as a representative of deep discriminative models, is developed
from neural networks, together with many practical techniques that can help in
optimization of neural networks. On the other hand, we will also trace back to
see the evolution history of deep generative models, to see how researchers
balance the representation power and computation complexity to reach Restricted
Boltzmann Machine and eventually reach Deep Belief Nets. Further, we will also
look into the development history of modelling time series data with neural
networks. We start with Time Delay Neural Networks and move further to
currently famous model named Recurrent Neural Network and its extension Long
Short Term Memory. We will also briefly look into how to construct deep
recurrent neural networks. Finally, we will conclude this report with some
interesting open-ended questions of deep neural networks.
</summary>
    <author>
      <name>Haohan Wang</name>
    </author>
    <author>
      <name>Bhiksha Raj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">43 pages, 31 figures. Fix typos in abstract</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.04781v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.04781v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00168v2</id>
    <updated>2017-10-04T06:55:48Z</updated>
    <published>2017-03-01T07:58:29Z</published>
    <title>Modular Representation of Layered Neural Networks</title>
    <summary>  Layered neural networks have greatly improved the performance of various
applications including image processing, speech recognition, natural language
processing, and bioinformatics. However, it is still difficult to discover or
interpret knowledge from the inference provided by a layered neural network,
since its internal representation has many nonlinear and complex parameters
embedded in hierarchical layers. Therefore, it becomes important to establish a
new methodology by which layered neural networks can be understood.
  In this paper, we propose a new method for extracting a global and simplified
structure from a layered neural network. Based on network analysis, the
proposed method detects communities or clusters of units with similar
connection patterns. We show its effectiveness by applying it to three use
cases. (1) Network decomposition: it can decompose a trained neural network
into multiple small independent networks thus dividing the problem and reducing
the computation time. (2) Training assessment: the appropriateness of a trained
result with a given hyperparameter or randomly chosen initial parameters can be
evaluated by using a modularity index. And (3) data analysis: in practical data
it reveals the community structure in the input, hidden, and output layers,
which serves as a clue for discovering knowledge from a trained neural network.
</summary>
    <author>
      <name>Chihiro Watanabe</name>
    </author>
    <author>
      <name>Kaoru Hiramatsu</name>
    </author>
    <author>
      <name>Kunio Kashino</name>
    </author>
    <link href="http://arxiv.org/abs/1703.00168v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00168v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0208453v1</id>
    <updated>2002-08-23T11:35:30Z</updated>
    <published>2002-08-23T11:35:30Z</published>
    <title>Neural Cryptography</title>
    <summary>  Two neural networks which are trained on their mutual output bits show a
novel phenomenon: The networks synchronize to a state with identical time
dependent weights. It is shown how synchronization by mutual learning can be
applied to cryptography: secret key exchange over a public channel.
</summary>
    <author>
      <name>Wolfgang Kinzel</name>
    </author>
    <author>
      <name>Ido Kanter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9th International Conference on Neural Information Processing,
  Singapore, Nov. 2002</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0208453v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0208453v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.0417v1</id>
    <updated>2010-07-02T18:05:29Z</updated>
    <published>2010-07-02T18:05:29Z</published>
    <title>Delta Learning Rule for the Active Sites Model</title>
    <summary>  This paper reports the results on methods of comparing the memory retrieval
capacity of the Hebbian neural network which implements the B-Matrix approach,
by using the Widrow-Hoff rule of learning. We then, extend the recently
proposed Active Sites model by developing a delta rule to increase memory
capacity. Also, this paper extends the binary neural network to a multi-level
(non-binary) neural network.
</summary>
    <author>
      <name>Krishna Chaithanya Lingashetty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 Pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1007.0417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.0417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.01809v1</id>
    <updated>2017-05-04T12:20:56Z</updated>
    <published>2017-05-04T12:20:56Z</published>
    <title>Pixel Normalization from Numeric Data as Input to Neural Networks</title>
    <summary>  Text to image transformation for input to neural networks requires
intermediate steps. This paper attempts to present a new approach to pixel
normalization so as to convert textual data into image, suitable as input for
neural networks. This method can be further improved by its Graphics Processing
Unit (GPU) implementation to provide significant speedup in computational time.
</summary>
    <author>
      <name>Parth Sane</name>
    </author>
    <author>
      <name>Ravindra Agrawal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE WiSPNET 2017 conference in Chennai</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.01809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504054v1</id>
    <updated>2005-04-13T13:40:38Z</updated>
    <published>2005-04-13T13:40:38Z</published>
    <title>Learning from Web: Review of Approaches</title>
    <summary>  Knowledge discovery is defined as non-trivial extraction of implicit,
previously unknown and potentially useful information from given data.
Knowledge extraction from web documents deals with unstructured, free-format
documents whose number is enormous and rapidly growing. The artificial neural
networks are well suitable to solve a problem of knowledge discovery from web
documents because trained networks are able more accurately and easily to
classify the learning and testing examples those represent the text mining
domain. However, the neural networks that consist of large number of weighted
connections and activation units often generate the incomprehensible and
hard-to-understand models of text classification. This problem may be also
addressed to most powerful recurrent neural networks that employ the feedback
links from hidden or output units to their input units. Due to feedback links,
recurrent neural networks are able take into account of a context in document.
To be useful for data mining, self-organizing neural network techniques of
knowledge extraction have been explored and developed. Self-organization
principles were used to create an adequate neural-network structure and reduce
a dimensionality of features used to describe text documents. The use of these
principles seems interesting because ones are able to reduce a neural-network
redundancy and considerably facilitate the knowledge representation.
</summary>
    <author>
      <name>Vitaly Schetinin</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0504054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.07588v2</id>
    <updated>2016-11-25T18:02:21Z</updated>
    <published>2016-11-23T00:11:41Z</published>
    <title>A Neural Network Model to Classify Liver Cancer Patients Using Data
  Expansion and Compression</title>
    <summary>  We develop a neural network model to classify liver cancer patients into
high-risk and low-risk groups using genomic data. Our approach provides a novel
technique to classify big data sets using neural network models. We preprocess
the data before training the neural network models. We first expand the data
using wavelet analysis. We then compress the wavelet coefficients by mapping
them onto a new scaled orthonormal coordinate system. Then the data is used to
train a neural network model that enables us to classify cancer patients into
two different classes of high-risk and low-risk patients. We use the
leave-one-out approach to build a neural network model. This neural network
model enables us to classify a patient using genomic data as a high-risk or
low-risk patient without any information about the survival time of the
patient. The results from genomic data analysis are compared with survival time
analysis. It is shown that the expansion and compression of data using wavelet
analysis and singular value decomposition (SVD) is essential to train the
neural network model.
</summary>
    <author>
      <name>Ashkan Zeinalzadeh</name>
    </author>
    <author>
      <name>Tom Wenska</name>
    </author>
    <author>
      <name>Gordon Okimoto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.07588v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.07588v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00144v4</id>
    <updated>2017-09-22T01:53:39Z</updated>
    <published>2017-03-01T05:38:16Z</published>
    <title>Theoretical Properties for Neural Networks with Weight Matrices of Low
  Displacement Rank</title>
    <summary>  Recently low displacement rank (LDR) matrices, or so-called structured
matrices, have been proposed to compress large-scale neural networks. Empirical
results have shown that neural networks with weight matrices of LDR matrices,
referred as LDR neural networks, can achieve significant reduction in space and
computational complexity while retaining high accuracy. We formally study LDR
matrices in deep learning. First, we prove the universal approximation property
of LDR neural networks with a mild condition on the displacement operators. We
then show that the error bounds of LDR neural networks are as efficient as
general neural networks with both single-layer and multiple-layer structure.
Finally, we propose back-propagation based training algorithm for general LDR
neural networks.
</summary>
    <author>
      <name>Liang Zhao</name>
    </author>
    <author>
      <name>Siyu Liao</name>
    </author>
    <author>
      <name>Yanzhi Wang</name>
    </author>
    <author>
      <name>Zhe Li</name>
    </author>
    <author>
      <name>Jian Tang</name>
    </author>
    <author>
      <name>Victor Pan</name>
    </author>
    <author>
      <name>Bo Yuan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.00144v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00144v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.07592v1</id>
    <updated>2017-11-21T01:11:00Z</updated>
    <published>2017-11-21T01:11:00Z</published>
    <title>Sparse-Input Neural Networks for High-dimensional Nonparametric
  Regression and Classification</title>
    <summary>  Neural networks are usually not the tool of choice for nonparametric
high-dimensional problems where the number of input features is much larger
than the number of observations. Though neural networks can approximate complex
multivariate functions, they generally require a large number of training
observations to obtain reasonable fits, unless one can learn the appropriate
network structure. In this manuscript, we show that neural networks can be
applied successfully to high-dimensional settings if the true function falls in
a low dimensional subspace, and proper regularization is used. We propose
fitting a neural network with a sparse group lasso penalty on the first-layer
input weights, which results in a neural net that only uses a small subset of
the original features. In addition, we characterize the statistical convergence
of the penalized empirical risk minimizer to the optimal neural network: we
show that the excess risk of this penalized estimator only grows with the
logarithm of the number of input features; and we show that the weights of
irrelevant features converge to zero. Via simulation studies and data analyses,
we show that these sparse-input neural networks outperform existing
nonparametric high-dimensional estimation methods when the data has complex
higher-order interactions.
</summary>
    <author>
      <name>Jean Feng</name>
    </author>
    <author>
      <name>Noah Simon</name>
    </author>
    <link href="http://arxiv.org/abs/1711.07592v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.07592v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.08934v1</id>
    <updated>2017-12-24T14:54:41Z</updated>
    <published>2017-12-24T14:54:41Z</published>
    <title>A Survey of FPGA Based Neural Network Accelerator</title>
    <summary>  Recent researches on neural network have shown great advantage in computer
vision over traditional algorithms based on handcrafted features and models.
Neural network is now widely adopted in regions like image, speech and video
recognition. But the great computation and storage complexity of neural network
based algorithms poses great difficulty on its application. CPU platforms are
hard to offer enough computation capacity. GPU platforms are the first choice
for neural network process because of its high computation capacity and easy to
use development frameworks.
  On the other hand, FPGA based neural network accelerator is becoming a
research topic. Because specific designed hardware is the next possible
solution to surpass GPU in speed and energy efficiency. Various FPGA based
accelerator designs have been proposed with software and hardware optimization
techniques to achieve high speed and energy efficiency. In this paper, we give
an overview of previous work on neural network accelerators based on FPGA and
summarize the main techniques used. Investigation from software to hardware,
from circuit level to system level is carried out to complete analysis of FPGA
based neural network accelerator design and serves as a guide to future work.
</summary>
    <author>
      <name>Kaiyuan Guo</name>
    </author>
    <author>
      <name>Shulin Zeng</name>
    </author>
    <author>
      <name>Jincheng Yu</name>
    </author>
    <author>
      <name>Yu Wang</name>
    </author>
    <author>
      <name>Huazhong Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1712.08934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.08934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04393v3</id>
    <updated>2017-02-06T22:51:33Z</updated>
    <published>2016-06-14T14:36:55Z</published>
    <title>Deep Learning with Darwin: Evolutionary Synthesis of Deep Neural
  Networks</title>
    <summary>  Taking inspiration from biological evolution, we explore the idea of "Can
deep neural networks evolve naturally over successive generations into highly
efficient deep neural networks?" by introducing the notion of synthesizing new
highly efficient, yet powerful deep neural networks over successive generations
via an evolutionary process from ancestor deep neural networks. The
architectural traits of ancestor deep neural networks are encoded using
synaptic probability models, which can be viewed as the `DNA' of these
networks. New descendant networks with differing network architectures are
synthesized based on these synaptic probability models from the ancestor
networks and computational environmental factor models, in a random manner to
mimic heredity, natural selection, and random mutation. These offspring
networks are then trained into fully functional networks, like one would train
a newborn, and have more efficient, more diverse network architectures than
their ancestor networks, while achieving powerful modeling capabilities.
Experimental results for the task of visual saliency demonstrated that the
synthesized `evolved' offspring networks can achieve state-of-the-art
performance while having network architectures that are significantly more
efficient (with a staggering $\sim$48-fold decrease in synapses by the fourth
generation) compared to the original ancestor network.
</summary>
    <author>
      <name>Mohammad Javad Shafiee</name>
    </author>
    <author>
      <name>Akshaya Mishra</name>
    </author>
    <author>
      <name>Alexander Wong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.04393v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04393v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09598v1</id>
    <updated>2017-06-29T07:13:42Z</updated>
    <published>2017-06-29T07:13:42Z</published>
    <title>CS591 Report: Application of siamesa network in 2D transformation</title>
    <summary>  Deep learning has been extensively used various aspects of computer vision
area. Deep learning separate itself from traditional neural network by having a
much deeper and complicated network layers in its network structures.
Traditionally, deep neural network is abundantly used in computer vision tasks
including classification and detection and has achieve remarkable success and
set up a new state of the art results in these fields. Instead of using neural
network for vision recognition and detection. I will show the ability of neural
network to do image registration, synthesis of images and image retrieval in
this report.
</summary>
    <author>
      <name>Dorothy Chang</name>
    </author>
    <link href="http://arxiv.org/abs/1706.09598v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09598v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0212486v1</id>
    <updated>2002-12-19T14:46:01Z</updated>
    <published>2002-12-19T14:46:01Z</published>
    <title>Neural Networks, Game Theory and Time Series Generation</title>
    <summary>  This dissertation highlights connections between the fields of neural
networks, game theory and time series generation. The concept of
antipredictability is explained, and the properties of time series that are
antipredictable for several prototypical prediction algorithms (neural
networks, Boolean funtions etc.) are studied. The Minority Game provides a
framework in which antipredictability arises naturally. Several variations of
the MG are introduced and compared, including extensions to more than two
choices, and the properties of the generated time series are analysed. A
learning algorithm is presented by which a neural network can find a good mixed
strategy in zero-sum matrix games. In a certain limit, this algorithm is a
stochastic variation of the "fictitious play" learning algorithm.
</summary>
    <author>
      <name>Richard Metzler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Dissertation. 130 pages, quite a few figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0212486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0212486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0603396v1</id>
    <updated>2006-03-15T00:19:58Z</updated>
    <published>2006-03-15T00:19:58Z</published>
    <title>Periodic Neural Activity Induced by Network Complexity</title>
    <summary>  We study a model for neural activity on the small-world topology of Watts and
Strogatz and on the scale-free topology of Barab\'asi and Albert. We find that
the topology of the network connections may spontaneously induce periodic
neural activity, contrasting with chaotic neural activities exhibited by
regular topologies. Periodic activity exists only for relatively small networks
and occurs with higher probability when the rewiring probability is larger. The
average length of the periods increases with the square root of the network
size.
</summary>
    <author>
      <name>D. R. Paula</name>
    </author>
    <author>
      <name>A. D. Araujo</name>
    </author>
    <author>
      <name>J. S. Andrade Jr</name>
    </author>
    <author>
      <name>H. J. Herrmann</name>
    </author>
    <author>
      <name>J. A. C. Gallas</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.74.017102</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.74.017102" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0603396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0603396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504053v1</id>
    <updated>2005-04-13T13:28:15Z</updated>
    <published>2005-04-13T13:28:15Z</published>
    <title>A Neural-Network Technique for Recognition of Filaments in Solar Images</title>
    <summary>  We describe a new neural-network technique developed for an automated
recognition of solar filaments visible in the hydrogen H-alpha line full disk
spectroheliograms. This technique allows neural networks learn from a few image
fragments labelled manually to recognize the single filaments depicted on a
local background. The trained network is able to recognize filaments depicted
on the backgrounds with variations in brightness caused by atmospherics
distortions. Despite the difference in backgrounds in our experiments the
neural network has properly recognized filaments in the testing image
fragments. Using a parabolic activation function we extend this technique to
recognize multiple solar filaments which may appear in one fragment.
</summary>
    <author>
      <name>V. V. Zharkova</name>
    </author>
    <author>
      <name>V. Schetinin</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0504053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.4351v1</id>
    <updated>2011-01-23T09:16:35Z</updated>
    <published>2011-01-23T09:16:35Z</published>
    <title>Building a Chaotic Proved Neural Network</title>
    <summary>  Chaotic neural networks have received a great deal of attention these last
years. In this paper we establish a precise correspondence between the
so-called chaotic iterations and a particular class of artificial neural
networks: global recurrent multi-layer perceptrons. We show formally that it is
possible to make these iterations behave chaotically, as defined by Devaney,
and thus we obtain the first neural networks proven chaotic. Several neural
networks with different architectures are trained to exhibit a chaotical
behavior.
</summary>
    <author>
      <name>Jacques M. Bahi</name>
    </author>
    <author>
      <name>Christophe Guyeux</name>
    </author>
    <author>
      <name>Michel Salomon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, submitted to ICCANS 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1101.4351v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.4351v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.06602v2</id>
    <updated>2016-03-07T16:36:33Z</updated>
    <published>2015-06-22T13:52:01Z</published>
    <title>Context-dependent representation in recurrent neural networks</title>
    <summary>  In order to assess the short-term memory performance of non-linear random
neural networks, we introduce a measure to quantify the dependence of a neural
representation upon the past context. We study this measure both numerically
and theoretically using the mean-field theory for random neural networks,
showing the existence of an optimal level of synaptic weights heterogeneity. We
further investigate the influence of the network topology, in particular the
symmetry of reciprocal synaptic connections, on this measure of context
dependence, revealing the importance of considering the interplay between
non-linearities and connectivity structure.
</summary>
    <author>
      <name>Gilles Wainrib</name>
    </author>
    <link href="http://arxiv.org/abs/1506.06602v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.06602v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.01321v1</id>
    <updated>2016-02-03T14:46:35Z</updated>
    <published>2016-02-03T14:46:35Z</published>
    <title>A continuum among logarithmic, linear, and exponential functions, and
  its potential to improve generalization in neural networks</title>
    <summary>  We present the soft exponential activation function for artificial neural
networks that continuously interpolates between logarithmic, linear, and
exponential functions. This activation function is simple, differentiable, and
parameterized so that it can be trained as the rest of the network is trained.
We hypothesize that soft exponential has the potential to improve neural
network learning, as it can exactly calculate many natural operations that
typical neural networks can only approximate, including addition,
multiplication, inner product, distance, polynomials, and sinusoids.
</summary>
    <author>
      <name>Luke B. Godfrey</name>
    </author>
    <author>
      <name>Michael S. Gashler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 8 figures, conference, In Proceedings of Knowledge Discovery
  and Information Retrieval (KDIR) 2015, Lisbon, Portugal, December 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.01321v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.01321v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.00857v1</id>
    <updated>2017-05-02T08:45:54Z</updated>
    <published>2017-05-02T08:45:54Z</published>
    <title>Decoding Small Surface Codes with Feedforward Neural Networks</title>
    <summary>  Surface codes reach high error thresholds when decoded with known algorithms,
but the decoding time will likely exceed the available time budget, especially
for near-term implementations. To decrease the decoding time, we reduce the
decoding problem to a classification problem that a feedforward neural network
can solve. We investigate quantum error correction and fault tolerance at small
code distances using neural network-based decoders, demonstrating that the
neural network can generalize to inputs that were not provided during training
and that they can reach similar or better decoding performance compared to
previous algorithms. We conclude by discussing the time required by a
feedforward neural network decoder in hardware.
</summary>
    <author>
      <name>Savvas Varsamopoulos</name>
    </author>
    <author>
      <name>Ben Criger</name>
    </author>
    <author>
      <name>Koen Bertels</name>
    </author>
    <link href="http://arxiv.org/abs/1705.00857v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.00857v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.07790v1</id>
    <updated>2017-12-21T04:50:03Z</updated>
    <published>2017-12-21T04:50:03Z</published>
    <title>The use of adversaries for optimal neural network training</title>
    <summary>  B-decay data from the Belle experiment at the KEKB collider have a
substantial background from $e^{+}e^{-}\to q \bar{q}$ events. To suppress this
we employ deep neural network algorithms. These provide improved signal from
background discrimination. However, the deep neural network develops a
substantial correlation with the $\Delta E$ kinematic variable used to
distinguish signal from background in the final fit due to its relationship
with input variables. The effect of this correlation is counter-acted by
deploying an adversarial neural network. Overall the adversarial deep neural
network performs better than an unoptimised commercial package, NeuroBayes.
</summary>
    <author>
      <name>Anton Hawthorne-Gonzalvez</name>
    </author>
    <author>
      <name>Martin Sevior</name>
    </author>
    <link href="http://arxiv.org/abs/1712.07790v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.07790v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.09685v1</id>
    <updated>2017-12-27T21:05:42Z</updated>
    <published>2017-12-27T21:05:42Z</published>
    <title>Neural network augmented inverse problems for PDEs</title>
    <summary>  In this paper we show how to augment classical methods for inverse problems
with artificial neural networks. The neural network acts as a parametric
container for the coefficient to be estimated from noisy data. Neural networks
are global, smooth function approximators and as such they do not require
regularization of the error functional to recover smooth solutions and
coefficients. We give detailed examples using the Poisson equation in 1, 2, and
3 space dimensions and show that the neural network augmentation is robust with
respect to noisy data, mesh, and geometry.
</summary>
    <author>
      <name>Jens Berg</name>
    </author>
    <author>
      <name>Kaj Nystr√∂m</name>
    </author>
    <link href="http://arxiv.org/abs/1712.09685v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.09685v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.00905v1</id>
    <updated>2018-01-03T05:52:52Z</updated>
    <published>2018-01-03T05:52:52Z</published>
    <title>Neural Networks in Adversarial Setting and Ill-Conditioned Weight Space</title>
    <summary>  Recently, Neural networks have seen a huge surge in its adoption due to their
ability to provide high accuracy on various tasks. On the other hand, the
existence of adversarial examples have raised suspicions regarding the
generalization capabilities of neural networks. In this work, we focus on the
weight matrix learnt by the neural networks and hypothesize that ill
conditioned weight matrix is one of the contributing factors in neural
network's susceptibility towards adversarial examples. For ensuring that the
learnt weight matrix's condition number remains sufficiently low, we suggest
using orthogonal regularizer. We show that this indeed helps in increasing the
adversarial accuracy on MNIST and F-MNIST datasets.
</summary>
    <author>
      <name>Mayank Singh</name>
    </author>
    <author>
      <name>Abhishek Sinha</name>
    </author>
    <author>
      <name>Balaji Krishnamurthy</name>
    </author>
    <link href="http://arxiv.org/abs/1801.00905v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.00905v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.01117v1</id>
    <updated>2018-01-05T02:10:32Z</updated>
    <published>2018-01-05T02:10:32Z</published>
    <title>Learning from Pseudo-Randomness With an Artificial Neural Network - Does
  God Play Pseudo-Dice?</title>
    <summary>  Inspired by the fact that the neural network, as the mainstream for machine
learning, has brought successes in many application areas, here we propose to
use this approach for decoding hidden correlation among pseudo-random data and
predicting events accordingly. With a simple neural network structure and a
typical training procedure, we demonstrate the learning and prediction power of
the neural network in extremely random environment. Finally, we postulate that
the high sensitivity and efficiency of the neural network may allow to
critically test if there could be any fundamental difference between quantum
randomness and pseudo randomness, which is equivalent to the question: Does God
play dice?
</summary>
    <author>
      <name>Fenglei Fan</name>
    </author>
    <author>
      <name>Ge Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures, 22 references</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.01117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.01117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.04435v1</id>
    <updated>2018-01-13T13:17:31Z</updated>
    <published>2018-01-13T13:17:31Z</published>
    <title>Neural network forecast of the sunspot diagram</title>
    <summary>  We attempt to forecast the Sun's sunspot butterfly diagram using neural
networks as a prediction method. We use this approach to forecast in both
latitude (space) and time, using a full spatial-temporal series of the sunspot
diagram from 1874 to 2015 that trains the neural network. The analysis of the
results show that it is indeed possible to reconstruct the overall shape and
amplitude of the spatial-temporal pattern of sunspots using these feed-forward
neural networks. However, we conclude that more data and/or improved neural
network techniques are probably necessary for this approach to have real
predictive power.
</summary>
    <author>
      <name>Eurico Covas</name>
    </author>
    <author>
      <name>Nuno Peixinho</name>
    </author>
    <author>
      <name>Joao Fernandes</name>
    </author>
    <link href="http://arxiv.org/abs/1801.04435v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.04435v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.09608v1</id>
    <updated>2016-10-30T06:34:19Z</updated>
    <published>2016-10-30T06:34:19Z</published>
    <title>A Theoretical Study of The Relationship Between Whole An ELM Network and
  Its Subnetworks</title>
    <summary>  A biological neural network is constituted by numerous subnetworks and
modules with different functionalities. For an artificial neural network, the
relationship between a network and its subnetworks is also important and useful
for both theoretical and algorithmic research, i.e. it can be exploited to
develop incremental network training algorithm or parallel network training
algorithm. In this paper we explore the relationship between an ELM neural
network and its subnetworks. To the best of our knowledge, we are the first to
prove a theorem that shows an ELM neural network can be scattered into
subnetworks and its optimal solution can be constructed recursively by the
optimal solutions of these subnetworks. Based on the theorem we also present
two algorithms to train a large ELM neural network efficiently: one is a
parallel network training algorithm and the other is an incremental network
training algorithm. The experimental results demonstrate the usefulness of the
theorem and the validity of the developed algorithms.
</summary>
    <author>
      <name>Enmei Tu</name>
    </author>
    <author>
      <name>Guanghao Zhang</name>
    </author>
    <author>
      <name>Lily Rachmawati</name>
    </author>
    <author>
      <name>Eshan Rajabally</name>
    </author>
    <author>
      <name>Guang-Bin Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.09608v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.09608v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.03918v1</id>
    <updated>2018-01-11T18:43:10Z</updated>
    <published>2018-01-11T18:43:10Z</published>
    <title>Black Holes as Brains: Neural Networks with Area Law Entropy</title>
    <summary>  Motivated by the potential similarities between the underlying mechanisms of
the enhanced memory storage capacity in black holes and in brain networks, we
construct an artificial quantum neural network based on gravity-like synaptic
connections and a symmetry structure that allows to describe the network in
terms of geometry of a d-dimensional space. We show that the network possesses
a critical state in which the gapless neurons emerge that appear to inhabit a
(d-1)-dimensional surface, with their number given by the surface area. In the
excitations of these neurons, the network can store and retrieve an
exponentially large number of patterns within an arbitrarily narrow energy gap.
The corresponding micro-state entropy of the brain network exhibits an area
law. The neural network can be described in terms of a quantum field, via
identifying the different neurons with the different momentum modes of the
field, while identifying the synaptic connections among the neurons with the
interactions among the corresponding momentum modes. Such a mapping allows to
attribute a well-defined sense of geometry to an intrinsically non-local
system, such as the neural network, and vice versa, it allows to represent the
quantum field model as a neural network.
</summary>
    <author>
      <name>Gia Dvali</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.03918v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.03918v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.06186v2</id>
    <updated>2017-03-02T09:36:24Z</updated>
    <published>2017-02-14T17:24:04Z</published>
    <title>Survey of reasoning using Neural networks</title>
    <summary>  Reason and inference require process as well as memory skills by humans.
Neural networks are able to process tasks like image recognition (better than
humans) but in memory aspects are still limited (by attention mechanism, size).
Recurrent Neural Network (RNN) and it's modified version LSTM are able to solve
small memory contexts, but as context becomes larger than a threshold, it is
difficult to use them. The Solution is to use large external memory. Still, it
poses many challenges like, how to train neural networks for discrete memory
representation, how to describe long term dependencies in sequential data etc.
Most prominent neural architectures for such tasks are Memory networks:
inference components combined with long term memory and Neural Turing Machines:
neural networks using external memory resources. Also, additional techniques
like attention mechanism, end to end gradient descent on discrete memory
representation are needed to support these solutions. Preliminary results of
above neural architectures on simple algorithms (sorting, copying) and Question
Answering (based on story, dialogs) application are comparable with the state
of the art. In this paper, I explain these architectures (in general), the
additional techniques used and the results of their application.
</summary>
    <author>
      <name>Amit Sahu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.06186v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.06186v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.0213v1</id>
    <updated>2011-12-01T15:37:09Z</updated>
    <published>2011-12-01T15:37:09Z</published>
    <title>Supervised Learning of Logical Operations in Layered Spiking Neural
  Networks with Spike Train Encoding</title>
    <summary>  Few algorithms for supervised training of spiking neural networks exist that
can deal with patterns of multiple spikes, and their computational properties
are largely unexplored. We demonstrate in a set of simulations that the ReSuMe
learning algorithm can be successfully applied to layered neural networks.
Input and output patterns are encoded as spike trains of multiple precisely
timed spikes, and the network learns to transform the input trains into target
output trains. This is done by combining the ReSuMe learning algorithm with
multiplicative scaling of the connections of downstream neurons.
  We show in particular that layered networks with one hidden layer can learn
the basic logical operations, including Exclusive-Or, while networks without
hidden layer cannot, mirroring an analogous result for layered networks of rate
neurons.
  While supervised learning in spiking neural networks is not yet fit for
technical purposes, exploring computational properties of spiking neural
networks advances our understanding of how computations can be done with spike
trains.
</summary>
    <author>
      <name>Andr√© Gr√ºning</name>
    </author>
    <author>
      <name>Ioana Sporea</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11063-012-9225-1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11063-012-9225-1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Processing Letters October 2012, Volume 36, Issue 2, pp
  117-134</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1112.0213v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.0213v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.06539v1</id>
    <updated>2016-11-20T16:05:07Z</updated>
    <published>2016-11-20T16:05:07Z</published>
    <title>Efficient Stochastic Inference of Bitwise Deep Neural Networks</title>
    <summary>  Recently published methods enable training of bitwise neural networks which
allow reduced representation of down to a single bit per weight. We present a
method that exploits ensemble decisions based on multiple stochastically
sampled network models to increase performance figures of bitwise neural
networks in terms of classification accuracy at inference. Our experiments with
the CIFAR-10 and GTSRB datasets show that the performance of such network
ensembles surpasses the performance of the high-precision base model. With this
technique we achieve 5.81% best classification error on CIFAR-10 test set using
bitwise networks. Concerning inference on embedded systems we evaluate these
bitwise networks using a hardware efficient stochastic rounding procedure. Our
work contributes to efficient embedded bitwise neural networks.
</summary>
    <author>
      <name>Sebastian Vogel</name>
    </author>
    <author>
      <name>Christoph Schorn</name>
    </author>
    <author>
      <name>Andre Guntoro</name>
    </author>
    <author>
      <name>Gerd Ascheid</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, Workshop on Efficient Methods for Deep Neural
  Networks at Neural Information Processing Systems Conference 2016, NIPS 2016,
  EMDNN 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.06539v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.06539v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.1; C.1.3; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.00891v1</id>
    <updated>2017-06-03T03:08:34Z</updated>
    <published>2017-06-03T03:08:34Z</published>
    <title>Spectrum-based deep neural networks for fraud detection</title>
    <summary>  In this paper, we focus on fraud detection on a signed graph with only a
small set of labeled training data. We propose a novel framework that combines
deep neural networks and spectral graph analysis. In particular, we use the
node projection (called as spectral coordinate) in the low dimensional spectral
space of the graph's adjacency matrix as input of deep neural networks.
Spectral coordinates in the spectral space capture the most useful topology
information of the network. Due to the small dimension of spectral coordinates
(compared with the dimension of the adjacency matrix derived from a graph),
training deep neural networks becomes feasible. We develop and evaluate two
neural networks, deep autoencoder and convolutional neural network, in our
fraud detection framework. Experimental results on a real signed graph show
that our spectrum based deep neural networks are effective in fraud detection.
</summary>
    <author>
      <name>Shuhan Yuan</name>
    </author>
    <author>
      <name>Xintao Wu</name>
    </author>
    <author>
      <name>Jun Li</name>
    </author>
    <author>
      <name>Aidong Lu</name>
    </author>
    <link href="http://arxiv.org/abs/1706.00891v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.00891v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.01911v1</id>
    <updated>2017-08-06T17:10:38Z</updated>
    <published>2017-08-06T17:10:38Z</published>
    <title>Training of Deep Neural Networks based on Distance Measures using
  RMSProp</title>
    <summary>  The vanishing gradient problem was a major obstacle for the success of deep
learning. In recent years it was gradually alleviated through multiple
different techniques. However the problem was not really overcome in a
fundamental way, since it is inherent to neural networks with activation
functions based on dot products. In a series of papers, we are going to analyze
alternative neural network structures which are not based on dot products. In
this first paper, we revisit neural networks built up of layers based on
distance measures and Gaussian activation functions. These kinds of networks
were only sparsely used in the past since they are hard to train when using
plain stochastic gradient descent methods. We show that by using Root Mean
Square Propagation (RMSProp) it is possible to efficiently learn multi-layer
neural networks. Furthermore we show that when appropriately initialized these
kinds of neural networks suffer much less from the vanishing and exploding
gradient problem than traditional neural networks even for deep networks.
</summary>
    <author>
      <name>Thomas Kurbiel</name>
    </author>
    <author>
      <name>Shahrzad Khaleghian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 14 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.01911v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.01911v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.05792v1</id>
    <updated>2018-02-15T23:24:39Z</updated>
    <published>2018-02-15T23:24:39Z</published>
    <title>Masked Conditional Neural Networks for Automatic Sound Events
  Recognition</title>
    <summary>  Deep neural network architectures designed for application domains other than
sound, especially image recognition, may not optimally harness the
time-frequency representation when adapted to the sound recognition problem. In
this work, we explore the ConditionaL Neural Network (CLNN) and the Masked
ConditionaL Neural Network (MCLNN) for multi-dimensional temporal signal
recognition. The CLNN considers the inter-frame relationship, and the MCLNN
enforces a systematic sparseness over the network's links to enable learning in
frequency bands rather than bins allowing the network to be frequency shift
invariant mimicking a filterbank. The mask also allows considering several
combinations of features concurrently, which is usually handcrafted through
exhaustive manual search. We applied the MCLNN to the environmental sound
recognition problem using the ESC-10 and ESC-50 datasets. MCLNN achieved
competitive performance, using 12% of the parameters and without augmentation,
compared to state-of-the-art Convolutional Neural Networks.
</summary>
    <author>
      <name>Fady Medhat</name>
    </author>
    <author>
      <name>David Chesmore</name>
    </author>
    <author>
      <name>John Robinson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DSAA.2017.43</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DSAA.2017.43" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Restricted Boltzmann Machine, RBM, Conditional RBM, CRBM, Deep Belief
  Net, DBN, Conditional Neural Network, CLNN, Masked Conditional Neural
  Network, MCLNN, Environmental Sound Recognition, ESR</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Data Science and Advanced
  Analytics (DSAA) Year: 2017, Pages: 389 - 394</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1802.05792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.05792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.06432v1</id>
    <updated>2018-02-18T19:55:09Z</updated>
    <published>2018-02-18T19:55:09Z</published>
    <title>Music Genre Classification using Masked Conditional Neural Networks</title>
    <summary>  The ConditionaL Neural Networks (CLNN) and the Masked ConditionaL Neural
Networks (MCLNN) exploit the nature of multi-dimensional temporal signals. The
CLNN captures the conditional temporal influence between the frames in a window
and the mask in the MCLNN enforces a systematic sparseness that follows a
filterbank-like pattern over the network links. The mask induces the network to
learn about time-frequency representations in bands, allowing the network to
sustain frequency shifts. Additionally, the mask in the MCLNN automates the
exploration of a range of feature combinations, usually done through an
exhaustive manual search. We have evaluated the MCLNN performance using the
Ballroom and Homburg datasets of music genres. MCLNN has achieved accuracies
that are competitive to state-of-the-art handcrafted attempts in addition to
models based on Convolutional Neural Networks.
</summary>
    <author>
      <name>Fady Medhat</name>
    </author>
    <author>
      <name>David Chesmore</name>
    </author>
    <author>
      <name>John Robinson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-70096-0_49</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-70096-0_49" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Conditional Neural Networks (CLNN), Masked Conditional Neural
  Networks (MCLNN), Conditional Restricted Boltzmann Machine (CRBM), Deep
  Belief Nets (DBN), Music Information Retrieval (MIR)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Neural Information Processing (ICONIP)
  Year: 2017, Pages: 470-481</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1802.06432v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06432v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.02129v1</id>
    <updated>2018-03-06T11:40:46Z</updated>
    <published>2018-03-06T11:40:46Z</published>
    <title>A Non-Technical Survey on Deep Convolutional Neural Network
  Architectures</title>
    <summary>  Artificial neural networks have recently shown great results in many
disciplines and a variety of applications, including natural language
understanding, speech processing, games and image data generation. One
particular application in which the strong performance of artificial neural
networks was demonstrated is the recognition of objects in images, where deep
convolutional neural networks are commonly applied. In this survey, we give a
comprehensive introduction to this topic (object recognition with deep
convolutional neural networks), with a strong focus on the evolution of network
architectures. Therefore, we aim to compress the most important concepts in
this field in a simple and non-technical manner to allow for future researchers
to have a quick general understanding.
  This work is structured as follows:
  1. We will explain the basic ideas of (convolutional) neural networks and
deep learning and examine their usage for three object recognition tasks: image
classification, object localization and object detection.
  2. We give a review on the evolution of deep convolutional neural networks by
providing an extensive overview of the most important network architectures
presented in chronological order of their appearances.
</summary>
    <author>
      <name>Felix Altenberger</name>
    </author>
    <author>
      <name>Claus Lenz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages (incl. references), 23 Postscript figures, uses IEEEtran</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.02129v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.02129v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06216v3</id>
    <updated>2016-08-06T23:45:51Z</updated>
    <published>2016-06-20T17:29:01Z</published>
    <title>Neural networks with differentiable structure</title>
    <summary>  While gradient descent has proven highly successful in learning connection
weights for neural networks, the actual structure of these networks is usually
determined by hand, or by other optimization algorithms. Here we describe a
simple method to make network structure differentiable, and therefore
accessible to gradient descent. We test this method on recurrent neural
networks applied to simple sequence prediction problems. Starting with initial
networks containing only one node, the method automatically builds networks
that successfully solve the tasks. The number of nodes in the final network
correlates with task difficulty. The method can dynamically increase network
size in response to an abrupt complexification in the task; however, reduction
in network size in response to task simplification is not evident for
reasonable meta-parameters. The method does not penalize network performance
for these test tasks: variable-size networks actually reach better performance
than fixed-size networks of higher, lower or identical size. We conclude by
discussing how this method could be applied to more complex networks, such as
feedforward layered networks, or multiple-area networks of arbitrary shape.
</summary>
    <author>
      <name>Thomas Miconi</name>
    </author>
    <link href="http://arxiv.org/abs/1606.06216v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06216v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0406268v2</id>
    <updated>2004-10-05T15:16:48Z</updated>
    <published>2004-06-11T14:26:53Z</published>
    <title>Networking genetic regulation and neural computation: Directed network
  topology and its effect on the dynamics</title>
    <summary>  Two different types of directed networks are investigated, transcriptional
regulation networks and neural networks. The directed network structure are
studied and also shown to reflect the different processes taking place on the
networks. The distribution of influence, identified as the the number of
downstream vertices, are used as a tool for investigating random vertex
removal. In the transcriptional regulation networks we observe that only a
small number of vertices have a large influence. The small influences of most
vertices limit the effect of a random removal to in most cases only a small
fraction of vertices in the network. The neural network has a rather different
topology with respect to the influence, which are large for most vertices. To
further investigate the effect of vertex removal we simulate the biological
processes taking place on the networks. Opposed to the presumpted large effect
of random vertex removal in the neural network, the high density of edges in
conjunction with the dynamics used makes the change in the state of the system
to be highly localized around the removed vertex.
</summary>
    <author>
      <name>Andreas Gr√∂nlund</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.70.061908</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.70.061908" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0406268v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0406268v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.4524v1</id>
    <updated>2007-07-31T02:27:10Z</updated>
    <published>2007-07-31T02:27:10Z</published>
    <title>Image Authentication Based on Neural Networks</title>
    <summary>  Neural network has been attracting more and more researchers since the past
decades. The properties, such as parameter sensitivity, random similarity,
learning ability, etc., make it suitable for information protection, such as
data encryption, data authentication, intrusion detection, etc. In this paper,
by investigating neural networks' properties, the low-cost authentication
method based on neural networks is proposed and used to authenticate images or
videos. The authentication method can detect whether the images or videos are
modified maliciously. Firstly, this chapter introduces neural networks'
properties, such as parameter sensitivity, random similarity, diffusion
property, confusion property, one-way property, etc. Secondly, the chapter
gives an introduction to neural network based protection methods. Thirdly, an
image or video authentication scheme based on neural networks is presented, and
its performances, including security, robustness and efficiency, are analyzed.
Finally, conclusions are drawn, and some open issues in this field are
presented.
</summary>
    <author>
      <name>Shiguo Lian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages,10 figures, submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/0707.4524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.4524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; E.3.x; F.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.10642v1</id>
    <updated>2017-03-30T19:04:55Z</updated>
    <published>2017-03-30T19:04:55Z</published>
    <title>Deep Neural Network Optimized to Resistive Memory with Nonlinear
  Current-Voltage Characteristics</title>
    <summary>  Artificial Neural Network computation relies on intensive vector-matrix
multiplications. Recently, the emerging nonvolatile memory (NVM) crossbar array
showed a feasibility of implementing such operations with high energy
efficiency, thus there are many works on efficiently utilizing emerging NVM
crossbar array as analog vector-matrix multiplier. However, its nonlinear I-V
characteristics restrain critical design parameters, such as the read voltage
and weight range, resulting in substantial accuracy loss. In this paper,
instead of optimizing hardware parameters to a given neural network, we propose
a methodology of reconstructing a neural network itself optimized to resistive
memory crossbar arrays. To verify the validity of the proposed method, we
simulated various neural network with MNIST and CIFAR-10 dataset using two
different specific Resistive Random Access Memory (RRAM) model. Simulation
results show that our proposed neural network produces significantly higher
inference accuracies than conventional neural network when the synapse devices
have nonlinear I-V characteristics.
</summary>
    <author>
      <name>Hyungjun Kim</name>
    </author>
    <author>
      <name>Taesu Kim</name>
    </author>
    <author>
      <name>Jinseok Kim</name>
    </author>
    <author>
      <name>Jae-Joon Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.10642v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.10642v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9412030v1</id>
    <updated>1994-12-06T09:26:29Z</updated>
    <published>1994-12-06T09:26:29Z</published>
    <title>Attractor Neural Networks</title>
    <summary>  In this lecture I will present some models of neural networks that have been
developed in the recent years. The aim is to construct neural networks which
work as associative memories. Different attractors of the network will be
identified as different internal representations of different objects. At the
end of the lecture I will present a comparison among the theoretical results
and some of the experiments done on real mammal brains.
</summary>
    <author>
      <name>Giorgio Parisi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dipart. Fisica, Universita Roma I</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/cond-mat/9412030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9412030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9808043v1</id>
    <updated>1998-08-04T19:04:49Z</updated>
    <published>1998-08-04T19:04:49Z</published>
    <title>Dynamics of networks and applications</title>
    <summary>  A survey is made of several aspects of the dynamics of networks, with special
emphasis on unsupervised learning processes, non-Gaussian data analysis and
pattern recognition in networks with complex nodes.
</summary>
    <author>
      <name>R. Vilela Mendes</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Grupo de Fisica-Matematica, Lisboa, Portugal</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Talk at the Heraeus Seminar "Scientific Applications of Neural Nets",
  to appear in the proceedings (Springer LNP)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">in "Scientific Applic. of Neural Nets", Springer Lect. Notes in
  Physics 522, 1999</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/9808043v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9808043v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.1612v1</id>
    <updated>2014-11-06T13:48:41Z</updated>
    <published>2014-11-06T13:48:41Z</published>
    <title>Effect of Activity and Inter-Cluster Correlations on
  Information-Theoretic Properties of Neural Networks</title>
    <summary>  On the basis of solutions of the master equation for networks with a small
number of neurons it is shown that the conditional entropy and integrated
information of neural networks depend on their average activity and
inter-cluster correlations.
</summary>
    <author>
      <name>Andrey Demichev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.1612v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.1612v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.00404v1</id>
    <updated>2016-05-02T09:33:46Z</updated>
    <published>2016-05-02T09:33:46Z</published>
    <title>Simple2Complex: Global Optimization by Gradient Descent</title>
    <summary>  A method named simple2complex for modeling and training deep neural networks
is proposed. Simple2complex train deep neural networks by smoothly adding more
and more layers to the shallow networks, as the learning procedure going on,
the network is just like growing. Compared with learning by end2end,
simple2complex is with less possibility trapping into local minimal, namely,
owning ability for global optimization. Cifar10 is used for verifying the
superiority of simple2complex.
</summary>
    <author>
      <name>Ming Li</name>
    </author>
    <link href="http://arxiv.org/abs/1605.00404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.00404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.04515v1</id>
    <updated>2017-10-12T13:40:43Z</updated>
    <published>2017-10-12T13:40:43Z</published>
    <title>Convolutional Attention-based Seq2Seq Neural Network for End-to-End ASR</title>
    <summary>  This thesis introduces the sequence to sequence model with Luong's attention
mechanism for end-to-end ASR. It also describes various neural network
algorithms including Batch normalization, Dropout and Residual network which
constitute the convolutional attention-based seq2seq neural network. Finally
the proposed model proved its effectiveness for speech recognition achieving
15.8% phoneme error rate on TIMIT dataset.
</summary>
    <author>
      <name>Dan Lim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Masters thesis, Korea Univ</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.04515v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.04515v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.05029v3</id>
    <updated>2018-02-18T18:42:40Z</updated>
    <published>2017-08-16T18:28:22Z</published>
    <title>Deep Neural Network Capacity</title>
    <summary>  In recent years, deep neural network exhibits its powerful superiority on
information discrimination in many computer vision applications. However, the
capacity of deep neural network architecture is still a mystery to the
researchers. Intuitively, larger capacity of neural network can always deposit
more information to improve the discrimination ability of the model. But, the
learnable parameter scale is not feasible to estimate the capacity of deep
neural network. Due to the overfitting, directly increasing hidden nodes number
and hidden layer number are already demonstrated not necessary to effectively
increase the network discrimination ability.
  In this paper, we propose a novel measurement, named "total valid bits", to
evaluate the capacity of deep neural networks for exploring how to
quantitatively understand the deep learning and the insights behind its super
performance. Specifically, our scheme to retrieve the total valid bits
incorporates the skilled techniques in both training phase and inference phase.
In the network training, we design decimal weight regularization and 8-bit
forward quantization to obtain the integer-oriented network representations.
Moreover, we develop adaptive-bitwidth and non-uniform quantization strategy in
the inference phase to find the neural network capacity, total valid bits. By
allowing zero bitwidth, our adaptive-bitwidth quantization can execute the
model reduction and valid bits finding simultaneously. In our extensive
experiments, we first demonstrate that our total valid bits is a good indicator
of neural network capacity. We also analyze the impact on network capacity from
the network architecture and advanced training skills, such as dropout and
batch normalization.
</summary>
    <author>
      <name>Aosen Wang</name>
    </author>
    <author>
      <name>Hua Zhou</name>
    </author>
    <author>
      <name>Wenyao Xu</name>
    </author>
    <author>
      <name>Xin Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">There is an error in Average Valid Bits computation in figure 1 in
  page 2</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.05029v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.05029v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.00165v3</id>
    <updated>2018-03-03T00:45:00Z</updated>
    <published>2017-11-01T02:13:25Z</published>
    <title>Deep Neural Networks as Gaussian Processes</title>
    <summary>  It has long been known that a single-layer fully-connected neural network
with an i.i.d. prior over its parameters is equivalent to a Gaussian process
(GP), in the limit of infinite network width. This correspondence enables exact
Bayesian inference for infinite width neural networks on regression tasks by
means of evaluating the corresponding GP. Recently, kernel functions which
mimic multi-layer random neural networks have been developed, but only outside
of a Bayesian framework. As such, previous work has not identified that these
kernels can be used as covariance functions for GPs and allow fully Bayesian
prediction with a deep neural network.
  In this work, we derive the exact equivalence between infinitely wide deep
networks and GPs. We further develop a computationally efficient pipeline to
compute the covariance function for these GPs. We then use the resulting GPs to
perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10.
We observe that trained neural network accuracy approaches that of the
corresponding GP with increasing layer width, and that the GP uncertainty is
strongly correlated with trained network prediction error. We further find that
test performance increases as finite-width trained networks are made wider and
more similar to a GP, and thus that GP predictions typically outperform those
of finite-width networks. Finally we connect the performance of these GPs to
the recent theory of signal propagation in random neural networks.
</summary>
    <author>
      <name>Jaehoon Lee</name>
    </author>
    <author>
      <name>Yasaman Bahri</name>
    </author>
    <author>
      <name>Roman Novak</name>
    </author>
    <author>
      <name>Samuel S. Schoenholz</name>
    </author>
    <author>
      <name>Jeffrey Pennington</name>
    </author>
    <author>
      <name>Jascha Sohl-Dickstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published version in ICLR 2018. 10 pages + appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.00165v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.00165v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02053v1</id>
    <updated>2016-09-07T16:30:01Z</updated>
    <published>2016-09-07T16:30:01Z</published>
    <title>Fast and Efficient Asynchronous Neural Computation with Adapting Spiking
  Neural Networks</title>
    <summary>  Biological neurons communicate with a sparing exchange of pulses - spikes. It
is an open question how real spiking neurons produce the kind of powerful
neural computation that is possible with deep artificial neural networks, using
only so very few spikes to communicate. Building on recent insights in
neuroscience, we present an Adapting Spiking Neural Network (ASNN) based on
adaptive spiking neurons. These spiking neurons efficiently encode information
in spike-trains using a form of Asynchronous Pulsed Sigma-Delta coding while
homeostatically optimizing their firing rate. In the proposed paradigm of
spiking neuron computation, neural adaptation is tightly coupled to synaptic
plasticity, to ensure that downstream neurons can correctly decode upstream
spiking neurons. We show that this type of network is inherently able to carry
out asynchronous and event-driven neural computation, while performing
identical to corresponding artificial neural networks (ANNs). In particular, we
show that these adaptive spiking neurons can be drop in replacements for ReLU
neurons in standard feedforward ANNs comprised of such units. We demonstrate
that this can also be successfully applied to a ReLU based deep convolutional
neural network for classifying the MNIST dataset. The ASNN thus outperforms
current Spiking Neural Networks (SNNs) implementations, while responding (up
to) an order of magnitude faster and using an order of magnitude fewer spikes.
Additionally, in a streaming setting where frames are continuously classified,
we show that the ASNN requires substantially fewer network updates as compared
to the corresponding ANN.
</summary>
    <author>
      <name>Davide Zambrano</name>
    </author>
    <author>
      <name>Sander M. Bohte</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.02053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.0573v1</id>
    <updated>2014-05-03T11:58:27Z</updated>
    <published>2014-05-03T11:58:27Z</published>
    <title>Spatial Neural Networks and their Functional Samples: Similarities and
  Differences</title>
    <summary>  Models of neural networks have proven their utility in the development of
learning algorithms in computer science and in the theoretical study of brain
dynamics in computational neuroscience. We propose in this paper a spatial
neural network model to analyze the important class of functional networks,
which are commonly employed in computational studies of clinical brain imaging
time series. We developed a simulation framework inspired by multichannel brain
surface recordings (more specifically, EEG -- electroencephalogram) in order to
link the mesoscopic network dynamics (represented by sampled functional
networks) and the microscopic network structure (represented by an
integrate-and-fire neural network located in a 3D space -- hence the term
spatial neural network). Functional networks are obtained by computing pairwise
correlations between time-series of mesoscopic electric potential dynamics,
which allows the construction of a graph where each node represents one
time-series. The spatial neural network model is central in this study in the
sense that it allowed us to characterize sampled functional networks in terms
of what features they are able to reproduce from the underlying spatial
network. Our modeling approach shows that, in specific conditions of sample
size and edge density, it is possible to precisely estimate several network
measurements of spatial networks by just observing functional samples.
</summary>
    <author>
      <name>Lucas Antiqueira</name>
    </author>
    <author>
      <name>Liang Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures (submitted)</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.0573v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.0573v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.10761v1</id>
    <updated>2017-11-29T10:28:02Z</updated>
    <published>2017-11-29T10:28:02Z</published>
    <title>Transfer Learning with Binary Neural Networks</title>
    <summary>  Previous work has shown that it is possible to train deep neural networks
with low precision weights and activations. In the extreme case it is even
possible to constrain the network to binary values. The costly floating point
multiplications are then reduced to fast logical operations. High end smart
phones such as Google's Pixel 2 and Apple's iPhone X are already equipped with
specialised hardware for image processing and it is very likely that other
future consumer hardware will also have dedicated accelerators for deep neural
networks. Binary neural networks are attractive in this case because the
logical operations are very fast and efficient when implemented in hardware. We
propose a transfer learning based architecture where we first train a binary
network on Imagenet and then retrain part of the network for different tasks
while keeping most of the network fixed. The fixed binary part could be
implemented in a hardware accelerator while the last layers of the network are
evaluated in software. We show that a single binary neural network trained on
the Imagenet dataset can indeed be used as a feature extractor for other
datasets.
</summary>
    <author>
      <name>Sam Leroux</name>
    </author>
    <author>
      <name>Steven Bohez</name>
    </author>
    <author>
      <name>Tim Verbelen</name>
    </author>
    <author>
      <name>Bert Vankeirsbilck</name>
    </author>
    <author>
      <name>Pieter Simoens</name>
    </author>
    <author>
      <name>Bart Dhoedt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Learning on the Phone and other Consumer Devices, NIPS2017
  Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.10761v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.10761v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04374v2</id>
    <updated>2016-10-05T17:45:06Z</updated>
    <published>2016-08-15T19:38:35Z</published>
    <title>A Geometric Framework for Convolutional Neural Networks</title>
    <summary>  In this paper, a geometric framework for neural networks is proposed. This
framework uses the inner product space structure underlying the parameter set
to perform gradient descent not in a component-based form, but in a
coordinate-free manner. Convolutional neural networks are described in this
framework in a compact form, with the gradients of standard --- and
higher-order --- loss functions calculated for each layer of the network. This
approach can be applied to other network structures and provides a basis on
which to create new networks.
</summary>
    <author>
      <name>Anthony L. Caterini</name>
    </author>
    <author>
      <name>Dong Eui Chang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Added proofs and algorithms that were missing from previous version</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.04374v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04374v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.1; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.09876v1</id>
    <updated>2018-02-27T13:35:44Z</updated>
    <published>2018-02-27T13:35:44Z</published>
    <title>Parameter diagnostics of phases and phase transition learning by neural
  networks</title>
    <summary>  We present an analysis of neural network-based machine learning schemes for
phases and phase transitions in theoretical condensed matter research, focusing
on neural networks with a single hidden layer. Such shallow neural networks
were previously found to be efficient in classifying phases and locating phase
transitions of various basic model systems. In order to rationalize the
emergence of the classification process and for identifying any underlying
physical quantities, it is feasible to examine the weight matrices and the
convolutional filter kernels that result from the learning process of such
shallow networks. Furthermore, we demonstrate how the learning-by-confusing
scheme can be used, in combination with a simple threshold-value classification
method, to diagnose the learning parameters of neural networks. In particular,
we study the classification process of both fully-connected and convolutional
neural networks for the two-dimensional Ising model with extended domain wall
configurations included in the low-temperature regime. Moreover, we consider
the two-dimensional XY model and contrast the performance of the
learning-by-confusing scheme and convolutional neural networks trained on bare
spin configurations to the case of preprocessed samples with respect to vortex
configurations. We discuss these findings in relation to similar recent
investigations and possible further applications.
</summary>
    <author>
      <name>Philippe Suchsland</name>
    </author>
    <author>
      <name>Stefan Wessel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 23 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.09876v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.09876v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.08254v3</id>
    <updated>2017-05-23T11:45:31Z</updated>
    <published>2016-05-26T12:19:09Z</published>
    <title>Robust Large Margin Deep Neural Networks</title>
    <summary>  The generalization error of deep neural networks via their classification
margin is studied in this work. Our approach is based on the Jacobian matrix of
a deep neural network and can be applied to networks with arbitrary
non-linearities and pooling layers, and to networks with different
architectures such as feed forward networks and residual networks. Our analysis
leads to the conclusion that a bounded spectral norm of the network's Jacobian
matrix in the neighbourhood of the training samples is crucial for a deep
neural network of arbitrary depth and width to generalize well. This is a
significant improvement over the current bounds in the literature, which imply
that the generalization error grows with either the width or the depth of the
network. Moreover, it shows that the recently proposed batch normalization and
weight normalization re-parametrizations enjoy good generalization properties,
and leads to a novel network regularizer based on the network's Jacobian
matrix. The analysis is supported with experimental results on the MNIST,
CIFAR-10, LaRED and ImageNet datasets.
</summary>
    <author>
      <name>Jure Sokolic</name>
    </author>
    <author>
      <name>Raja Giryes</name>
    </author>
    <author>
      <name>Guillermo Sapiro</name>
    </author>
    <author>
      <name>Miguel R. D. Rodrigues</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TSP.2017.2708039</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TSP.2017.2708039" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted to IEEE Transactions on Signal Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.08254v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08254v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9812006v1</id>
    <updated>1998-12-05T00:00:57Z</updated>
    <published>1998-12-05T00:00:57Z</published>
    <title>A High Quality Text-To-Speech System Composed of Multiple Neural
  Networks</title>
    <summary>  While neural networks have been employed to handle several different
text-to-speech tasks, ours is the first system to use neural networks
throughout, for both linguistic and acoustic processing. We divide the
text-to-speech task into three subtasks, a linguistic module mapping from text
to a linguistic representation, an acoustic module mapping from the linguistic
representation to speech, and a video module mapping from the linguistic
representation to animated images. The linguistic module employs a
letter-to-sound neural network and a postlexical neural network. The acoustic
module employs a duration neural network and a phonetic neural network. The
visual neural network is employed in parallel to the acoustic module to drive a
talking head. The use of neural networks that can be retrained on the
characteristics of different voices and languages affords our system a degree
of adaptability and naturalness heretofore unavailable.
</summary>
    <author>
      <name>Orhan Karaali</name>
    </author>
    <author>
      <name>Gerald Corrigan</name>
    </author>
    <author>
      <name>Noel Massey</name>
    </author>
    <author>
      <name>Corey Miller</name>
    </author>
    <author>
      <name>Otto Schnurr</name>
    </author>
    <author>
      <name>Andrew Mackie</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.1998.675495</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.1998.675495" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Source link (9812006.tar.gz) contains: 1 PostScript file (4 pages)
  and 3 WAV audio files. If your system does not support Windows WAV files, try
  a tool like "sox" to translate the audio into a format of your choice</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the IEEE International Conference on Acoustics,
  Speech and Signal Processing (1998) 2:1237-1240. Seattle, Washington</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9812006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9812006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.02676v1</id>
    <updated>2017-02-09T02:02:27Z</updated>
    <published>2017-02-09T02:02:27Z</published>
    <title>Energy Saving Additive Neural Network</title>
    <summary>  In recent years, machine learning techniques based on neural networks for
mobile computing become increasingly popular. Classical multi-layer neural
networks require matrix multiplications at each stage. Multiplication operation
is not an energy efficient operation and consequently it drains the battery of
the mobile device. In this paper, we propose a new energy efficient neural
network with the universal approximation property over space of Lebesgue
integrable functions. This network, called, additive neural network, is very
suitable for mobile computing. The neural structure is based on a novel vector
product definition, called ef-operator, that permits a multiplier-free
implementation. In ef-operation, the "product" of two real numbers is defined
as the sum of their absolute values, with the sign determined by the sign of
the product of the numbers. This "product" is used to construct a vector
product in $R^N$. The vector product induces the $l_1$ norm. The proposed
additive neural network successfully solves the XOR problem. The experiments on
MNIST dataset show that the classification performances of the proposed
additive neural networks are very similar to the corresponding multi-layer
perceptron and convolutional neural networks (LeNet).
</summary>
    <author>
      <name>Arman Afrasiyabi</name>
    </author>
    <author>
      <name>Ozan Yildiz</name>
    </author>
    <author>
      <name>Baris Nasir</name>
    </author>
    <author>
      <name>Fatos T. Yarman Vural</name>
    </author>
    <author>
      <name>A. Enis Cetin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages (double column), 2 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.02676v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.02676v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0603015v1</id>
    <updated>2006-03-02T23:59:19Z</updated>
    <published>2006-03-02T23:59:19Z</published>
    <title>The Basic Kak Neural Network with Complex Inputs</title>
    <summary>  The Kak family of neural networks is able to learn patterns quickly, and this
speed of learning can be a decisive advantage over other competing models in
many applications. Amongst the implementations of these networks are those
using reconfigurable networks, FPGAs and optical networks. In some
applications, it is useful to use complex data, and it is with that in mind
that this introduction to the basic Kak network with complex inputs is being
presented. The training algorithm is prescriptive and the network weights are
assigned simply upon examining the inputs. The input is mapped using quaternary
encoding for purpose of efficienty. This network family is part of a larger
hierarchy of learning schemes that include quantum models.
</summary>
    <author>
      <name>Pritam Rajagopal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 9 figures, 7 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0603015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0603015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.05440v1</id>
    <updated>2017-12-14T20:31:29Z</updated>
    <published>2017-12-14T20:31:29Z</published>
    <title>Nonparametric Neural Networks</title>
    <summary>  Automatically determining the optimal size of a neural network for a given
task without prior information currently requires an expensive global search
and training many networks from scratch. In this paper, we address the problem
of automatically finding a good network size during a single training cycle. We
introduce *nonparametric neural networks*, a non-probabilistic framework for
conducting optimization over all possible network sizes and prove its soundness
when network growth is limited via an L_p penalty. We train networks under this
framework by continuously adding new units while eliminating redundant units
via an L_2 penalty. We employ a novel optimization algorithm, which we term
*adaptive radial-angular gradient descent* or *AdaRad*, and obtain promising
results.
</summary>
    <author>
      <name>George Philipp</name>
    </author>
    <author>
      <name>Jaime G. Carbonell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.05440v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.05440v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.4071v1</id>
    <updated>2008-04-25T09:30:28Z</updated>
    <published>2008-04-25T09:30:28Z</published>
    <title>Logic Mining Using Neural Networks</title>
    <summary>  Knowledge could be gained from experts, specialists in the area of interest,
or it can be gained by induction from sets of data. Automatic induction of
knowledge from data sets, usually stored in large databases, is called data
mining. Data mining methods are important in the management of complex systems.
There are many technologies available to data mining practitioners, including
Artificial Neural Networks, Regression, and Decision Trees. Neural networks
have been successfully applied in wide range of supervised and unsupervised
learning applications. Neural network methods are not commonly used for data
mining tasks, because they often produce incomprehensible models, and require
long training times. One way in which the collective properties of a neural
network may be used to implement a computational task is by way of the concept
of energy minimization. The Hopfield network is well-known example of such an
approach. The Hopfield network is useful as content addressable memory or an
analog computer for solving combinatorial-type optimization problems. Wan
Abdullah [1] proposed a method of doing logic programming on a Hopfield neural
network. Optimization of logical inconsistency is carried out by the network
after the connection strengths are defined from the logic program; the network
relaxes to neural states corresponding to a valid interpretation. In this
article, we describe how Hopfield network is able to induce logical rules from
large database by using reverse analysis method: given the values of the
connections of a network, we can hope to know what logical rules are entrenched
in the database.
</summary>
    <author>
      <name>Saratha Sathasivam</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">USM</arxiv:affiliation>
    </author>
    <author>
      <name>Wan Ahmad Tajuddin Wan Abdullah</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Univ Malaya</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on Intelligent Systems
  2005 (ICIS 2005), Kuala Lumpur, 1-3 December 2005</arxiv:comment>
    <link href="http://arxiv.org/abs/0804.4071v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.4071v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.0798v1</id>
    <updated>2009-06-03T23:10:25Z</updated>
    <published>2009-06-03T23:10:25Z</published>
    <title>Single Neuron Memories and the Network's Proximity Matrix</title>
    <summary>  This paper extends the treatment of single-neuron memories obtained by the
B-matrix approach. The spreading of the activity within the network is
determined by the network's proximity matrix which represents the separations
amongst the neurons through the neural pathways.
</summary>
    <author>
      <name>Subhash Kak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0906.0798v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.0798v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06154v1</id>
    <updated>2016-04-21T01:47:33Z</updated>
    <published>2016-04-21T01:47:33Z</published>
    <title>Deep Adaptive Network: An Efficient Deep Neural Network with Sparse
  Binary Connections</title>
    <summary>  Deep neural networks are state-of-the-art models for understanding the
content of images, video and raw input data. However, implementing a deep
neural network in embedded systems is a challenging task, because a typical
deep neural network, such as a Deep Belief Network using 128x128 images as
input, could exhaust Giga bytes of memory and result in bandwidth and computing
bottleneck. To address this challenge, this paper presents a hardware-oriented
deep learning algorithm, named as the Deep Adaptive Network, which attempts to
exploit the sparsity in the neural connections. The proposed method adaptively
reduces the weights associated with negligible features to zero, leading to
sparse feedforward network architecture. Furthermore, since the small
proportion of important weights are significantly larger than zero, they can be
robustly thresholded and represented using single-bit integers (-1 and +1),
leading to implementations of deep neural networks with sparse and binary
connections. Our experiments showed that, for the application of recognizing
MNIST handwritten digits, the features extracted by a two-layer Deep Adaptive
Network with about 25% reserved important connections achieved 97.2%
classification accuracy, which was almost the same with the standard Deep
Belief Network (97.3%). Furthermore, for efficient hardware implementations,
the sparse-and-binary-weighted deep neural network could save about 99.3%
memory and 99.9% computation units without significant loss of classification
accuracy for pattern recognition applications.
</summary>
    <author>
      <name>Xichuan Zhou</name>
    </author>
    <author>
      <name>Shengli Li</name>
    </author>
    <author>
      <name>Kai Qin</name>
    </author>
    <author>
      <name>Kunping Li</name>
    </author>
    <author>
      <name>Fang Tang</name>
    </author>
    <author>
      <name>Shengdong Hu</name>
    </author>
    <author>
      <name>Shujun Liu</name>
    </author>
    <author>
      <name>Zhi Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, extended and submitted to IEEE Transactions of Systems,
  Man, and Cybernetics</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.06154v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06154v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.00630v2</id>
    <updated>2017-08-09T10:05:09Z</updated>
    <published>2017-08-02T07:58:45Z</published>
    <title>ProjectionNet: Learning Efficient On-Device Deep Networks Using Neural
  Projections</title>
    <summary>  Deep neural networks have become ubiquitous for applications related to
visual recognition and language understanding tasks. However, it is often
prohibitive to use typical neural networks on devices like mobile phones or
smart watches since the model sizes are huge and cannot fit in the limited
memory available on such devices. While these devices could make use of machine
learning models running on high-performance data centers with CPUs or GPUs,
this is not feasible for many applications because data can be privacy
sensitive and inference needs to be performed directly "on" device.
  We introduce a new architecture for training compact neural networks using a
joint optimization framework. At its core lies a novel objective that jointly
trains using two different types of networks--a full trainer neural network
(using existing architectures like Feed-forward NNs or LSTM RNNs) combined with
a simpler "projection" network that leverages random projections to transform
inputs or intermediate representations into bits. The simpler network encodes
lightweight and efficient-to-compute operations in bit space with a low memory
footprint. The two networks are trained jointly using backpropagation, where
the projection network learns from the full network similar to apprenticeship
learning. Once trained, the smaller network can be used directly for inference
at low memory and computation cost. We demonstrate the effectiveness of the new
approach at significantly shrinking the memory requirements of different types
of neural networks while preserving good accuracy on visual recognition and
text classification tasks. We also study the question "how many neural bits are
required to solve a given task?" using the new framework and show empirical
results contrasting model predictive capacity (in bits) versus accuracy on
several datasets.
</summary>
    <author>
      <name>Sujith Ravi</name>
    </author>
    <link href="http://arxiv.org/abs/1708.00630v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00630v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.09856v1</id>
    <updated>2018-01-30T05:47:01Z</updated>
    <published>2018-01-30T05:47:01Z</published>
    <title>ReNN: Rule-embedded Neural Networks</title>
    <summary>  The artificial neural network shows powerful ability of inference, but it is
still criticized for lack of interpretability and prerequisite needs of big
dataset. This paper proposes the Rule-embedded Neural Network (ReNN) to
overcome the shortages. ReNN first makes local-based inferences to detect local
patterns, and then uses rules based on domain knowledge about the local
patterns to generate rule-modulated map. After that, ReNN makes global-based
inferences that synthesizes the local patterns and the rule-modulated map. To
solve the optimization problem caused by rules, we use a two-stage optimization
strategy to train the ReNN model. By introducing rules into ReNN, we can
strengthen traditional neural networks with long-term dependencies which are
difficult to learn with limited empirical dataset, thus improving inference
accuracy. The complexity of neural networks can be reduced since long-term
dependencies are not modeled with neural connections, and thus the amount of
data needed to optimize the neural networks can be reduced. Besides, inferences
from ReNN can be analyzed with both local patterns and rules, and thus have
better interpretability. In this paper, ReNN has been validated with a
time-series detection problem.
</summary>
    <author>
      <name>Hu Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">conference paper, 6 pages, 4 figures, and 1 Table</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.09856v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.09856v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504058v1</id>
    <updated>2005-04-13T14:06:32Z</updated>
    <published>2005-04-13T14:06:32Z</published>
    <title>Polynomial Neural Networks Learnt to Classify EEG Signals</title>
    <summary>  A neural network based technique is presented, which is able to successfully
extract polynomial classification rules from labeled electroencephalogram (EEG)
signals. To represent the classification rules in an analytical form, we use
the polynomial neural networks trained by a modified Group Method of Data
Handling (GMDH). The classification rules were extracted from clinical EEG data
that were recorded from an Alzheimer patient and the sudden death risk
patients. The third data is EEG recordings that include the normal and artifact
segments. These EEG data were visually identified by medical experts. The
extracted polynomial rules verified on the testing EEG data allow to correctly
classify 72% of the risk group patients and 96.5% of the segments. These rules
performs slightly better than standard feedforward neural networks.
</summary>
    <author>
      <name>Vitaly Schetinin</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0504058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0607090v1</id>
    <updated>2006-07-18T21:01:43Z</updated>
    <published>2006-07-18T21:01:43Z</published>
    <title>Neural Networks with Complex and Quaternion Inputs</title>
    <summary>  This article investigates Kak neural networks, which can be instantaneously
trained, for complex and quaternion inputs. The performance of the basic
algorithm has been analyzed and shown how it provides a plausible model of
human perception and understanding of images. The motivation for studying
quaternion inputs is their use in representing spatial rotations that find
applications in computer graphics, robotics, global navigation, computer vision
and the spatial orientation of instruments. The problem of efficient mapping of
data in quaternion neural networks is examined. Some problems that need to be
addressed before quaternion neural networks find applications are identified.
</summary>
    <author>
      <name>Adityan Rishiyur</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0607090v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0607090v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.6921v1</id>
    <updated>2013-07-26T04:42:49Z</updated>
    <published>2013-07-26T04:42:49Z</published>
    <title>Memcapacitive neural networks</title>
    <summary>  We show that memcapacitive (memory capacitive) systems can be used as
synapses in artificial neural networks. As an example of our approach, we
discuss the architecture of an integrate-and-fire neural network based on
memcapacitive synapses. Moreover, we demonstrate that the
spike-timing-dependent plasticity can be simply realized with some of these
devices. Memcapacitive synapses are a low-energy alternative to memristive
synapses for neuromorphic computation.
</summary>
    <author>
      <name>Y. V. Pershin</name>
    </author>
    <author>
      <name>M. Di Ventra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1049/el.2013.2463</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1049/el.2013.2463" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Electronics Letters 50, 141 (2014)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1307.6921v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.6921v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.6413v1</id>
    <updated>2014-10-23T16:54:39Z</updated>
    <published>2014-10-23T16:54:39Z</published>
    <title>Initialization of multilayer forecasting artifical neural networks</title>
    <summary>  In this paper, a new method was developed for initialising artificial neural
networks predicting dynamics of time series. Initial weighting coefficients
were determined for neurons analogously to the case of a linear prediction
filter. Moreover, to improve the accuracy of the initialization method for a
multilayer neural network, some variants of decomposition of the transformation
matrix corresponding to the linear prediction filter were suggested. The
efficiency of the proposed neural network prediction method by forecasting
solutions of the Lorentz chaotic system is shown in this paper.
</summary>
    <author>
      <name>Vladimir V. Bochkarev</name>
    </author>
    <author>
      <name>Yulia S. Maslennikova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Uchenye Zapiski Kazanskogo Universiteta. Seriya
  Fiziko-Matematicheskie Nauki, 2010, vol. 152, no. 1, pp. 7-14. (In Russian)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1410.6413v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.6413v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M45, 62M10, 68T05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.3684v1</id>
    <updated>2014-12-10T18:23:13Z</updated>
    <published>2014-12-10T18:23:13Z</published>
    <title>Object Recognition Using Deep Neural Networks: A Survey</title>
    <summary>  Recognition of objects using Deep Neural Networks is an active area of
research and many breakthroughs have been made in the last few years. The paper
attempts to indicate how far this field has progressed. The paper briefly
describes the history of research in Neural Networks and describe several of
the recent advances in this field. The performances of recently developed
Neural Network Algorithm over benchmark datasets have been tabulated. Finally,
some the applications of this field have been provided.
</summary>
    <author>
      <name>Soren Goyal</name>
    </author>
    <author>
      <name>Paul Benjamin</name>
    </author>
    <link href="http://arxiv.org/abs/1412.3684v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3684v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.00193v1</id>
    <updated>2015-02-01T04:39:30Z</updated>
    <published>2015-02-01T04:39:30Z</published>
    <title>Evolutionary Artificial Neural Network Based on Chemical Reaction
  Optimization</title>
    <summary>  Evolutionary algorithms (EAs) are very popular tools to design and evolve
artificial neural networks (ANNs), especially to train them. These methods have
advantages over the conventional backpropagation (BP) method because of their
low computational requirement when searching in a large solution space. In this
paper, we employ Chemical Reaction Optimization (CRO), a newly developed global
optimization method, to replace BP in training neural networks. CRO is a
population-based metaheuristics mimicking the transition of molecules and their
interactions in a chemical reaction. Simulation results show that CRO
outperforms many EA strategies commonly used to train neural networks.
</summary>
    <author>
      <name>James J. Q. Yu</name>
    </author>
    <author>
      <name>Albert Y. S. Lam</name>
    </author>
    <author>
      <name>Victor O. K. Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CEC.2011.5949872</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CEC.2011.5949872" rel="related"/>
    <link href="http://arxiv.org/abs/1502.00193v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.00193v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.03827v1</id>
    <updated>2016-03-12T00:02:51Z</updated>
    <published>2016-03-12T00:02:51Z</published>
    <title>Sequential Short-Text Classification with Recurrent and Convolutional
  Neural Networks</title>
    <summary>  Recent approaches based on artificial neural networks (ANNs) have shown
promising results for short-text classification. However, many short texts
occur in sequences (e.g., sentences in a document or utterances in a dialog),
and most existing ANN-based systems do not leverage the preceding short texts
when classifying a subsequent one. In this work, we present a model based on
recurrent neural networks and convolutional neural networks that incorporates
the preceding short texts. Our model achieves state-of-the-art results on three
different datasets for dialog act prediction.
</summary>
    <author>
      <name>Ji Young Lee</name>
    </author>
    <author>
      <name>Franck Dernoncourt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as a conference paper at NAACL 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.03827v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.03827v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07724v1</id>
    <updated>2016-09-25T10:18:19Z</updated>
    <published>2016-09-25T10:18:19Z</published>
    <title>The RNN-ELM Classifier</title>
    <summary>  In this paper we examine learning methods combining the Random Neural
Network, a biologically inspired neural network and the Extreme Learning
Machine that achieve state of the art classification performance while
requiring much shorter training time. The Random Neural Network is a integrate
and fire computational model of a neural network whose mathematical structure
permits the efficient analysis of large ensembles of neurons. An activation
function is derived from the RNN and used in an Extreme Learning Machine. We
compare the performance of this combination against the ELM with various
activation functions, we reduce the input dimensionality via PCA and compare
its performance vs. autoencoder based versions of the RNN-ELM.
</summary>
    <author>
      <name>Athanasios Vlontzos</name>
    </author>
    <link href="http://arxiv.org/abs/1609.07724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.01589v1</id>
    <updated>2016-12-05T23:28:54Z</updated>
    <published>2016-12-05T23:28:54Z</published>
    <title>Improving the Performance of Neural Networks in Regression Tasks Using
  Drawering</title>
    <summary>  The method presented extends a given regression neural network to make its
performance improve. The modification affects the learning procedure only,
hence the extension may be easily omitted during evaluation without any change
in prediction. It means that the modified model may be evaluated as quickly as
the original one but tends to perform better.
  This improvement is possible because the modification gives better expressive
power, provides better behaved gradients and works as a regularization. The
knowledge gained by the temporarily extended neural network is contained in the
parameters shared with the original neural network.
  The only cost is an increase in learning time.
</summary>
    <author>
      <name>Konrad Zolna</name>
    </author>
    <link href="http://arxiv.org/abs/1612.01589v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.01589v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.07655v1</id>
    <updated>2017-11-21T07:23:32Z</updated>
    <published>2017-11-21T07:23:32Z</published>
    <title>Genetic Algorithms for Evolving Deep Neural Networks</title>
    <summary>  In recent years, deep learning methods applying unsupervised learning to
train deep layers of neural networks have achieved remarkable results in
numerous fields. In the past, many genetic algorithms based methods have been
successfully applied to training neural networks. In this paper, we extend
previous work and propose a GA-assisted method for deep learning. Our
experimental results indicate that this GA-assisted approach improves the
performance of a deep autoencoder, producing a sparser neural network.
</summary>
    <author>
      <name>Eli David</name>
    </author>
    <author>
      <name>Iddo Greental</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2598394.2602287</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2598394.2602287" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Genetic and Evolutionary Computation Conference (GECCO), pages
  1451-1452, Vancouver, Canada, July 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1711.07655v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.07655v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.06601v1</id>
    <updated>2018-01-19T23:39:15Z</updated>
    <published>2018-01-19T23:39:15Z</published>
    <title>CMSIS-NN: Efficient Neural Network Kernels for Arm Cortex-M CPUs</title>
    <summary>  Deep Neural Networks are becoming increasingly popular in always-on IoT edge
devices performing data analytics right at the source, reducing latency as well
as energy consumption for data communication. This paper presents CMSIS-NN,
efficient kernels developed to maximize the performance and minimize the memory
footprint of neural network (NN) applications on Arm Cortex-M processors
targeted for intelligent IoT edge devices. Neural network inference based on
CMSIS-NN kernels achieves 4.6X improvement in runtime/throughput and 4.9X
improvement in energy efficiency.
</summary>
    <author>
      <name>Liangzhen Lai</name>
    </author>
    <author>
      <name>Naveen Suda</name>
    </author>
    <author>
      <name>Vikas Chandra</name>
    </author>
    <link href="http://arxiv.org/abs/1801.06601v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.06601v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.01670v2</id>
    <updated>2016-03-08T16:36:00Z</updated>
    <published>2016-03-05T02:06:43Z</published>
    <title>Network Morphism</title>
    <summary>  We present in this paper a systematic study on how to morph a well-trained
neural network to a new one so that its network function can be completely
preserved. We define this as \emph{network morphism} in this research. After
morphing a parent network, the child network is expected to inherit the
knowledge from its parent network and also has the potential to continue
growing into a more powerful one with much shortened training time. The first
requirement for this network morphism is its ability to handle diverse morphing
types of networks, including changes of depth, width, kernel size, and even
subnet. To meet this requirement, we first introduce the network morphism
equations, and then develop novel morphing algorithms for all these morphing
types for both classic and convolutional neural networks. The second
requirement for this network morphism is its ability to deal with non-linearity
in a network. We propose a family of parametric-activation functions to
facilitate the morphing of any continuous non-linear activation neurons.
Experimental results on benchmark datasets and typical neural networks
demonstrate the effectiveness of the proposed network morphism scheme.
</summary>
    <author>
      <name>Tao Wei</name>
    </author>
    <author>
      <name>Changhu Wang</name>
    </author>
    <author>
      <name>Yong Rui</name>
    </author>
    <author>
      <name>Chang Wen Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review for ICML 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.01670v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.01670v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.1127v3</id>
    <updated>2009-05-20T11:23:05Z</updated>
    <published>2008-09-06T03:44:09Z</published>
    <title>Self-organization of feedforward structure and entrainment in excitatory
  neural networks with spike-timing-dependent plasticity</title>
    <summary>  Spike-timing dependent plasticity (STDP) is an organizing principle of
biological neural networks. While synchronous firing of neurons is considered
to be an important functional block in the brain, how STDP shapes neural
networks possibly toward synchrony is not entirely clear. We examine relations
between STDP and synchronous firing in spontaneously firing neural populations.
Using coupled heterogeneous phase oscillators placed on initial networks, we
show numerically that STDP prunes some synapses and promotes formation of a
feedforward network. Eventually a pacemaker, which is the neuron with the
fastest inherent frequency in our numerical simulations, emerges at the root of
the feedforward network. In each oscillatory cycle, a packet of neural activity
is propagated from the pacemaker to downstream neurons along layers of the
feedforward network. This event occurs above a clear-cut threshold value of the
initial synaptic weight. Below the threshold, neurons are self-organized into
separate clusters each of which is a feedforward network.
</summary>
    <author>
      <name>Yuko K. Takahashi</name>
    </author>
    <author>
      <name>Hiroshi Kori</name>
    </author>
    <author>
      <name>Naoki Masuda</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.79.051904</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.79.051904" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 figures, 1 table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physical Review E, 79, 051904 (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0809.1127v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.1127v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05916v1</id>
    <updated>2016-08-21T10:48:36Z</updated>
    <published>2016-08-21T10:48:36Z</published>
    <title>Neural Networks and Chaos: Construction, Evaluation of Chaotic Networks,
  and Prediction of Chaos with Multilayer Feedforward Networks</title>
    <summary>  Many research works deal with chaotic neural networks for various fields of
application. Unfortunately, up to now these networks are usually claimed to be
chaotic without any mathematical proof. The purpose of this paper is to
establish, based on a rigorous theoretical framework, an equivalence between
chaotic iterations according to Devaney and a particular class of neural
networks. On the one hand we show how to build such a network, on the other
hand we provide a method to check if a neural network is a chaotic one.
Finally, the ability of classical feedforward multilayer perceptrons to learn
sets of data obtained from a dynamical system is regarded. Various Boolean
functions are iterated on finite states. Iterations of some of them are proven
to be chaotic as it is defined by Devaney. In that context, important
differences occur in the training process, establishing with various neural
networks that chaotic behaviors are far more difficult to learn.
</summary>
    <author>
      <name>Jacques M. Bahi</name>
    </author>
    <author>
      <name>Jean-Fran√ßois Couchot</name>
    </author>
    <author>
      <name>Christophe Guyeux</name>
    </author>
    <author>
      <name>Michel Salomon</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">AIP Chaos, An Interdisciplinary Journal of Nonlinear Science.
  22(1), 013122 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1608.05916v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05916v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.06464v1</id>
    <updated>2017-11-17T09:29:52Z</updated>
    <published>2017-11-17T09:29:52Z</published>
    <title>A unified deep artificial neural network approach to partial
  differential equations in complex geometries</title>
    <summary>  We use deep feedforward artificial neural networks to approximate solutions
of partial differential equations of advection and diffusion type in complex
geometries. We derive analytical expressions of the gradients of the cost
function with respect to the network parameters, as well as the gradient of the
network itself with respect to the input, for arbitrarily deep networks. The
method is based on an ansatz for the solution, which requires nothing but
feedforward neural networks, and an unconstrained gradient based optimization
method such as gradient descent or quasi-Newton methods.
  We provide detailed examples on how to use deep feedforward neural networks
as a basis for further work on deep neural network approximations to partial
differential equations. We highlight the benefits of deep compared to shallow
neural networks and other convergence enhancing techniques.
</summary>
    <author>
      <name>Jens Berg</name>
    </author>
    <author>
      <name>Kaj Nystr√∂m</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.06464v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.06464v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.04161v2</id>
    <updated>2017-03-03T20:43:04Z</updated>
    <published>2016-10-13T16:34:30Z</published>
    <title>Why Deep Neural Networks for Function Approximation?</title>
    <summary>  Recently there has been much interest in understanding why deep neural
networks are preferred to shallow networks. We show that, for a large class of
piecewise smooth functions, the number of neurons needed by a shallow network
to approximate a function is exponentially larger than the corresponding number
of neurons needed by a deep network for a given degree of function
approximation. First, we consider univariate functions on a bounded interval
and require a neural network to achieve an approximation error of $\varepsilon$
uniformly over the interval. We show that shallow networks (i.e., networks
whose depth does not depend on $\varepsilon$) require
$\Omega(\text{poly}(1/\varepsilon))$ neurons while deep networks (i.e.,
networks whose depth grows with $1/\varepsilon$) require
$\mathcal{O}(\text{polylog}(1/\varepsilon))$ neurons. We then extend these
results to certain classes of important multivariate functions. Our results are
derived for neural networks which use a combination of rectifier linear units
(ReLUs) and binary step units, two of the most popular type of activation
functions. Our analysis builds on a simple observation: the multiplication of
two bits can be represented by a ReLU.
</summary>
    <author>
      <name>Shiyu Liang</name>
    </author>
    <author>
      <name>R. Srikant</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper is published at the 5th International Conference on
  Learning Representations (ICLR)</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.04161v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.04161v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0470v1</id>
    <updated>2014-09-01T16:20:41Z</updated>
    <published>2014-09-01T16:20:41Z</published>
    <title>Neural coordination can be enhanced by occasional interruption of normal
  firing patterns: A self-optimizing spiking neural network model</title>
    <summary>  The state space of a conventional Hopfield network typically exhibits many
different attractors of which only a small subset satisfy constraints between
neurons in a globally optimal fashion. It has recently been demonstrated that
combining Hebbian learning with occasional alterations of normal neural states
avoids this problem by means of self-organized enlargement of the best basins
of attraction. However, so far it is not clear to what extent this process of
self-optimization is also operative in real brains. Here we demonstrate that it
can be transferred to more biologically plausible neural networks by
implementing a self-optimizing spiking neural network model. In addition, by
using this spiking neural network to emulate a Hopfield network with Hebbian
learning, we attempt to make a connection between rate-based and temporal
coding based neural systems. Although further work is required to make this
model more realistic, it already suggests that the efficacy of the
self-optimizing process is independent from the simplifying assumptions of a
conventional Hopfield network. We also discuss natural and cultural processes
that could be responsible for occasional alteration of neural firing patterns
in actual brains
</summary>
    <author>
      <name>Alexander Woodward</name>
    </author>
    <author>
      <name>Tom Froese</name>
    </author>
    <author>
      <name>Takashi Ikegami</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 6 figures; Neural Networks, in press</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.0470v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0470v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92B20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; F.1.1; C.1.3; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.04818v1</id>
    <updated>2017-03-14T23:10:57Z</updated>
    <published>2017-03-14T23:10:57Z</published>
    <title>Neural Graph Machines: Learning Neural Networks Using Graphs</title>
    <summary>  Label propagation is a powerful and flexible semi-supervised learning
technique on graphs. Neural networks, on the other hand, have proven track
records in many supervised learning tasks. In this work, we propose a training
framework with a graph-regularised objective, namely "Neural Graph Machines",
that can combine the power of neural networks and label propagation. This work
generalises previous literature on graph-augmented training of neural networks,
enabling it to be applied to multiple neural architectures (Feed-forward NNs,
CNNs and LSTM RNNs) and a wide range of graphs. The new objective allows the
neural networks to harness both labeled and unlabeled data by: (a) allowing the
network to train using labeled data as in the supervised setting, (b) biasing
the network to learn similar hidden representations for neighboring nodes on a
graph, in the same vein as label propagation. Such architectures with the
proposed objective can be trained efficiently using stochastic gradient descent
and scaled to large graphs, with a runtime that is linear in the number of
edges. The proposed joint training approach convincingly outperforms many
existing methods on a wide range of tasks (multi-label classification on social
graphs, news categorization, document classification and semantic intent
classification), with multiple forms of graph inputs (including graphs with and
without node-level features) and using different types of neural networks.
</summary>
    <author>
      <name>Thang D. Bui</name>
    </author>
    <author>
      <name>Sujith Ravi</name>
    </author>
    <author>
      <name>Vivek Ramavajjala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.04818v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.04818v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0405024v1</id>
    <updated>2004-05-06T13:44:20Z</updated>
    <published>2004-05-06T13:44:20Z</published>
    <title>Meta-Learning Evolutionary Artificial Neural Networks</title>
    <summary>  In this paper, we present MLEANN (Meta-Learning Evolutionary Artificial
Neural Network), an automatic computational framework for the adaptive
optimization of artificial neural networks wherein the neural network
architecture, activation function, connection weights; learning algorithm and
its parameters are adapted according to the problem. We explored the
performance of MLEANN and conventionally designed artificial neural networks
for function approximation problems. To evaluate the comparative performance,
we used three different well-known chaotic time series. We also present the
state of the art popular neural network learning algorithms and some
experimentation results related to convergence speed and generalization
performance. We explored the performance of backpropagation algorithm;
conjugate gradient algorithm, quasi-Newton algorithm and Levenberg-Marquardt
algorithm for the three chaotic time series. Performances of the different
learning algorithms were evaluated when the activation functions and
architecture were changed. We further present the theoretical background,
algorithm, design strategy and further demonstrate how effective and inevitable
is the proposed MLEANN framework to design a neural network, which is smaller,
faster and with a better generalization performance.
</summary>
    <author>
      <name>Ajith Abraham</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neurocomputing Journal, Elsevier Science, Netherlands, Vol. 56c,
  pp. 1-38, 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0405024v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0405024v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/hep-ph/0606257v1</id>
    <updated>2006-06-24T00:42:19Z</updated>
    <published>2006-06-24T00:42:19Z</published>
    <title>An Empirical Study of Boosted Neural Network for Particle Classification
  in High Energy Collisions</title>
    <summary>  The possible application of boosted neural network to particle classification
in high energy physics is discussed. A two-dimensional toy model, where the
boundary between signal and background is irregular but not overlapping, is
constructed to show how boosting technique works with neural network. It is
found that boosted neural network not only decreases the error rate of
classification significantly but also increases the efficiency and
signal-background ratio. Besides, boosted neural network can avoid the
disadvantage aspects of single neural network design. The boosted neural
network is also applied to the classification of quark- and gluon- jet samples
from Monte Carlo \EE collisions, where the two samples show significant
overlapping. The performance of boosting technique for the two different
boundary cases -- with and without overlapping is discussed.
</summary>
    <author>
      <name>Yu Meiling</name>
    </author>
    <author>
      <name>Xu Mingmei</name>
    </author>
    <author>
      <name>Liu Lianshou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/hep-ph/0606257v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/hep-ph/0606257v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06905v2</id>
    <updated>2016-08-02T16:17:05Z</updated>
    <published>2016-06-22T11:30:47Z</published>
    <title>Learning text representation using recurrent convolutional neural
  network with highway layers</title>
    <summary>  Recently, the rapid development of word embedding and neural networks has
brought new inspiration to various NLP and IR tasks. In this paper, we describe
a staged hybrid model combining Recurrent Convolutional Neural Networks (RCNN)
with highway layers. The highway network module is incorporated in the middle
takes the output of the bi-directional Recurrent Neural Network (Bi-RNN) module
in the first stage and provides the Convolutional Neural Network (CNN) module
in the last stage with the input. The experiment shows that our model
outperforms common neural network models (CNN, RNN, Bi-RNN) on a sentiment
analysis task. Besides, the analysis of how sequence length influences the RCNN
with highway layers shows that our model could learn good representation for
the long text.
</summary>
    <author>
      <name>Ying Wen</name>
    </author>
    <author>
      <name>Weinan Zhang</name>
    </author>
    <author>
      <name>Rui Luo</name>
    </author>
    <author>
      <name>Jun Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Neu-IR '16 SIGIR Workshop on Neural Information Retrieval</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.06905v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06905v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09448v2</id>
    <updated>2016-11-30T02:00:48Z</updated>
    <published>2016-11-29T01:07:58Z</published>
    <title>The Upper Bound on Knots in Neural Networks</title>
    <summary>  Neural networks with rectified linear unit activations are essentially
multivariate linear splines. As such, one of many ways to measure the
"complexity" or "expressivity" of a neural network is to count the number of
knots in the spline model. We study the number of knots in fully-connected
feedforward neural networks with rectified linear unit activation functions. We
intentionally keep the neural networks very simple, so as to make theoretical
analyses more approachable. An induction on the number of layers $l$ reveals a
tight upper bound on the number of knots in $\mathbb{R} \to \mathbb{R}^p$ deep
neural networks. With $n_i \gg 1$ neurons in layer $i = 1, \dots, l$, the upper
bound is approximately $n_1 \dots n_l$. We then show that the exact upper bound
is tight, and we demonstrate the upper bound with an example. The purpose of
these analyses is to pave a path for understanding the behavior of general
$\mathbb{R}^q \to \mathbb{R}^p$ neural networks.
</summary>
    <author>
      <name>Kevin K. Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.09448v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09448v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07199v1</id>
    <updated>2017-05-19T21:33:00Z</updated>
    <published>2017-05-19T21:33:00Z</published>
    <title>The High-Dimensional Geometry of Binary Neural Networks</title>
    <summary>  Recent research has shown that one can train a neural network with binary
weights and activations at train time by augmenting the weights with a
high-precision continuous latent variable that accumulates small changes from
stochastic gradient descent. However, there is a dearth of theoretical analysis
to explain why we can effectively capture the features in our data with binary
weights and activations. Our main result is that the neural networks with
binary weights and activations trained using the method of Courbariaux, Hubara
et al. (2016) work because of the high-dimensional geometry of binary vectors.
In particular, the ideal continuous vectors that extract out features in the
intermediate representations of these BNNs are well-approximated by binary
vectors in the sense that dot products are approximately preserved. Compared to
previous research that demonstrated the viability of such BNNs, our work
explains why these BNNs work in terms of the HD geometry. Our theory serves as
a foundation for understanding not only BNNs but a variety of methods that seek
to compress traditional neural networks. Furthermore, a better understanding of
multilayer binary neural networks serves as a starting point for generalizing
BNNs to other neural network architectures such as recurrent neural networks.
</summary>
    <author>
      <name>Alexander G. Anderson</name>
    </author>
    <author>
      <name>Cory P. Berg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.07199v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07199v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09779v1</id>
    <updated>2017-06-29T14:38:13Z</updated>
    <published>2017-06-29T14:38:13Z</published>
    <title>Deep neural networks for direct, featureless learning through
  observation: the case of 2d spin models</title>
    <summary>  We train a deep convolutional neural network to accurately predict the
energies and magnetizations of Ising model configurations, using both the
traditional nearest-neighbour Hamiltonian, as well as a long-range screened
Coulomb Hamiltonian. We demonstrate the capability of a convolutional deep
neural network in predicting the nearest-neighbour energy of the 4x4 Ising
model. Using its success at this task, we motivate the study of the larger 8x8
Ising model, showing that the deep neural network can learn the
nearest-neighbour Ising Hamiltonian after only seeing a vanishingly small
fraction of configuration space. Additionally, we show that the neural network
has learned both the energy and magnetization operators with sufficient
accuracy to replicate the low-temperature Ising phase transition. Finally, we
teach the convolutional deep neural network to accurately predict a long-range
interaction through a screened Coulomb Hamiltonian. In this case, the benefits
of the neural network become apparent; it is able to make predictions with a
high degree of accuracy, 1600 times faster than a CUDA-optimized "exact"
calculation.
</summary>
    <author>
      <name>K. Mills</name>
    </author>
    <author>
      <name>I. Tamblyn</name>
    </author>
    <link href="http://arxiv.org/abs/1706.09779v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09779v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.02196v1</id>
    <updated>2017-10-05T20:04:10Z</updated>
    <published>2017-10-05T20:04:10Z</published>
    <title>Porcupine Neural Networks: (Almost) All Local Optima are Global</title>
    <summary>  Neural networks have been used prominently in several machine learning and
statistics applications. In general, the underlying optimization of neural
networks is non-convex which makes their performance analysis challenging. In
this paper, we take a novel approach to this problem by asking whether one can
constrain neural network weights to make its optimization landscape have good
theoretical properties while at the same time, be a good approximation for the
unconstrained one. For two-layer neural networks, we provide affirmative
answers to these questions by introducing Porcupine Neural Networks (PNNs)
whose weight vectors are constrained to lie over a finite set of lines. We show
that most local optima of PNN optimizations are global while we have a
characterization of regions where bad local optimizers may exist. Moreover, our
theoretical and empirical results suggest that an unconstrained neural network
can be approximated using a polynomially-large PNN.
</summary>
    <author>
      <name>Soheil Feizi</name>
    </author>
    <author>
      <name>Hamid Javadi</name>
    </author>
    <author>
      <name>Jesse Zhang</name>
    </author>
    <author>
      <name>David Tse</name>
    </author>
    <link href="http://arxiv.org/abs/1710.02196v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.02196v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.04481v1</id>
    <updated>2017-11-13T09:16:09Z</updated>
    <published>2017-11-13T09:16:09Z</published>
    <title>An Automatic Diagnosis Method of Facial Acne Vulgaris Based on
  Convolutional Neural Network</title>
    <summary>  In this paper, we present a new automatic diagnosis method of facial acne
vulgaris based on convolutional neural network. This method is proposed to
overcome the shortcoming of classification types in previous methods. The core
of our method is to extract features of images based on convolutional neural
network and achieve classification by classifier. We design a binary classifier
of skin-and-non-skin to detect skin area and a seven-classifier to achieve the
classification of facial acne vulgaris and healthy skin. In the experiment, we
compared the effectiveness of our convolutional neural network and the
pre-trained VGG16 neural network on the ImageNet dataset. And we use the ROC
curve and normal confusion matrix to evaluate the performance of the binary
classifier and the seven-classifier. The results of our experiment show that
the pre-trained VGG16 neural network is more effective in extracting image
features. The classifiers based on the pre-trained VGG16 neural network achieve
the skin detection and acne classification and have good robustness.
</summary>
    <author>
      <name>Xiaolei Shen</name>
    </author>
    <author>
      <name>Jiachi Zhang</name>
    </author>
    <author>
      <name>Chenjun Yan</name>
    </author>
    <author>
      <name>Hong Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 7 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.04481v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.04481v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.08163v1</id>
    <updated>2017-12-21T08:57:06Z</updated>
    <published>2017-12-21T08:57:06Z</published>
    <title>Reachable Set Computation and Safety Verification for Neural Networks
  with ReLU Activations</title>
    <summary>  Neural networks have been widely used to solve complex real-world problems.
Due to the complicate, nonlinear, non-convex nature of neural networks, formal
safety guarantees for the output behaviors of neural networks will be crucial
for their applications in safety-critical systems.In this paper, the output
reachable set computation and safety verification problems for a class of
neural networks consisting of Rectified Linear Unit (ReLU) activation functions
are addressed. A layer-by-layer approach is developed to compute output
reachable set. The computation is formulated in the form of a set of
manipulations for a union of polyhedra, which can be efficiently applied with
the aid of polyhedron computation tools. Based on the output reachable set
computation results, the safety verification for a ReLU neural network can be
performed by checking the intersections of unsafe regions and output reachable
set described by a union of polyhedra. A numerical example of a randomly
generated ReLU neural network is provided to show the effectiveness of the
approach developed in this paper.
</summary>
    <author>
      <name>Weiming Xiang</name>
    </author>
    <author>
      <name>Hoang-Dung Tran</name>
    </author>
    <author>
      <name>Taylor T. Johnson</name>
    </author>
    <link href="http://arxiv.org/abs/1712.08163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.08163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03557v1</id>
    <updated>2018-02-10T09:26:56Z</updated>
    <published>2018-02-10T09:26:56Z</published>
    <title>Reachable Set Estimation and Verification for Neural Network Models of
  Nonlinear Dynamic Systems</title>
    <summary>  Neural networks have been widely used to solve complex real-world problems.
Due to the complicate, nonlinear, non-convex nature of neural networks, formal
safety guarantees for the behaviors of neural network systems will be crucial
for their applications in safety-critical systems. In this paper, the reachable
set estimation and verification problems for Nonlinear Autoregressive-Moving
Average (NARMA) models in the forms of neural networks are addressed. The
neural network involved in the model is a class of feed-forward neural networks
called Multi-Layer Perceptron (MLP). By partitioning the input set of an MLP
into a finite number of cells, a layer-by-layer computation algorithm is
developed for reachable set estimation for each individual cell. The union of
estimated reachable sets of all cells forms an over-approximation of reachable
set of the MLP. Furthermore, an iterative reachable set estimation algorithm
based on reachable set estimation for MLPs is developed for NARMA models. The
safety verification can be performed by checking the existence of intersections
of unsafe regions and estimated reachable set. Several numerical examples are
provided to illustrate our approach.
</summary>
    <author>
      <name>Weiming Xiang</name>
    </author>
    <author>
      <name>Diego Manzanas Lopez</name>
    </author>
    <author>
      <name>Patrick Musau</name>
    </author>
    <author>
      <name>Taylor T. Johnson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.03557v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03557v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03930v1</id>
    <updated>2018-02-12T08:25:55Z</updated>
    <published>2018-02-12T08:25:55Z</published>
    <title>Visualizing Neural Network Developing Perturbation Theory</title>
    <summary>  In this letter, motivated by the question that whether the empirical fitting
of data by neural network can yield the same structure of physical laws, we
apply the neural network to a simple quantum mechanical two-body scattering
problem with short-range potentials, which by itself also plays an important
role in many branches of physics. We train a neural network to accurately
predict $ s $-wave scattering length, which governs the low-energy scattering
physics, directly from the scattering potential without solving Schr\"odinger
equation or obtaining the wavefunction. After analyzing the neural network, it
is shown that the neural network develops perturbation theory order by order
when the potential increases. This provides an important benchmark to the
machine-assisted physics research or even automated machine learning physics
laws.
</summary>
    <author>
      <name>Yadong Wu</name>
    </author>
    <author>
      <name>Pengfei Zhang</name>
    </author>
    <author>
      <name>Huitao Shen</name>
    </author>
    <author>
      <name>Hui Zhai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.03930v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03930v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.02501v1</id>
    <updated>2016-10-08T08:57:36Z</updated>
    <published>2016-10-08T08:57:36Z</published>
    <title>Revisiting Multiple Instance Neural Networks</title>
    <summary>  Recently neural networks and multiple instance learning are both attractive
topics in Artificial Intelligence related research fields. Deep neural networks
have achieved great success in supervised learning problems, and multiple
instance learning as a typical weakly-supervised learning method is effective
for many applications in computer vision, biometrics, nature language
processing, etc. In this paper, we revisit the problem of solving multiple
instance learning problems using neural networks. Neural networks are appealing
for solving multiple instance learning problem. The multiple instance neural
networks perform multiple instance learning in an end-to-end way, which take a
bag with various number of instances as input and directly output bag label.
All of the parameters in a multiple instance network are able to be optimized
via back-propagation. We propose a new multiple instance neural network to
learn bag representations, which is different from the existing multiple
instance neural networks that focus on estimating instance label. In addition,
recent tricks developed in deep learning have been studied in multiple instance
networks, we find deep supervision is effective for boosting bag classification
accuracy. In the experiments, the proposed multiple instance networks achieve
state-of-the-art or competitive performance on several MIL benchmarks.
Moreover, it is extremely fast for both testing and training, e.g., it takes
only 0.0003 second to predict a bag and a few seconds to train on a MIL
datasets on a moderate CPU.
</summary>
    <author>
      <name>Xinggang Wang</name>
    </author>
    <author>
      <name>Yongluan Yan</name>
    </author>
    <author>
      <name>Peng Tang</name>
    </author>
    <author>
      <name>Xiang Bai</name>
    </author>
    <author>
      <name>Wenyu Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1610.02501v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.02501v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9704098v1</id>
    <updated>1997-04-11T09:26:38Z</updated>
    <published>1997-04-11T09:26:38Z</published>
    <title>Phase Transitions of Neural Networks</title>
    <summary>  The cooperative behaviour of interacting neurons and synapses is studied
using models and methods from statistical physics. The competition between
training error and entropy may lead to discontinuous properties of the neural
network. This is demonstrated for a few examples: Perceptron, associative
memory, learning from examples, generalization, multilayer networks, structure
recognition, Bayesian estimate, on-line training, noise estimation and time
series generation.
</summary>
    <author>
      <name>Wolfgang Kinzel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/13642819808205038</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/13642819808205038" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Plenary talk for MINERVA workshop on mesoscopics, fractals and neural
  networks, Eilat, March 1997 Postscript File</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9704098v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9704098v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9808041v1</id>
    <updated>1998-08-04T18:38:03Z</updated>
    <published>1998-08-04T18:38:03Z</published>
    <title>Neural networks and logical reasoning systems. A translation table</title>
    <summary>  A correspondence is established between the elements of logic reasoning
systems (knowledge bases, rules, inference and queries) and the hardware and
dynamical operations of neural networks. The correspondence is framed as a
general translation dictionary which, hopefully, will allow to go back and
forth between symbolic and network formulations, a desirable step in
learning-oriented systems and multicomputer networks. In the framework of Horn
clause logics it is found that atomic propositions with n arguments correspond
to nodes with n-th order synapses, rules to synaptic intensity constraints,
forward chaining to synaptic dynamics and queries either to simple node
activation or to a query tensor dynamics.
</summary>
    <author>
      <name>Joao Martins</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Laboratorio de Mecatronica, DEEC, IST, Portugal</arxiv:affiliation>
    </author>
    <author>
      <name>R. Vilela Mendes</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Laboratorio de Mecatronica, DEEC, IST, Portugal</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Neural Systems 11 (2001) 179-186</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/9808041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9808041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9909443v1</id>
    <updated>1999-09-30T08:17:21Z</updated>
    <published>1999-09-30T08:17:21Z</published>
    <title>Thresholds in layered neural networks with variable activity</title>
    <summary>  The inclusion of a threshold in the dynamics of layered neural networks with
variable activity is studied at arbitrary temperature. In particular, the
effects on the retrieval quality of a self-controlled threshold obtained by
forcing the neural activity to stay equal to the activity of the stored paterns
during the whole retrieval process, are compared with those of a threshold
chosen externally for every loading and every temperature through optimisation
of the mutual information content of the network. Numerical results, mostly
concerning low activity networks are discussed.
</summary>
    <author>
      <name>D. Bolle'</name>
    </author>
    <author>
      <name>G. Massolo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/0305-4470/33/14/301</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/0305-4470/33/14/301" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, Latex2e, 6 eps figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J. Phys. A 33, 2597-2609 (2000).</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/9909443v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9909443v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0405505v1</id>
    <updated>2004-05-21T04:49:41Z</updated>
    <published>2004-05-21T04:49:41Z</published>
    <title>Analyzing Stability of Equilibrium Points in Neural Networks: A General
  Approach</title>
    <summary>  Networks of coupled neural systems represent an important class of models in
computational neuroscience. In some applications it is required that
equilibrium points in these networks remain stable under parameter variations.
Here we present a general methodology to yield explicit constraints on the
coupling strengths to ensure the stability of the equilibrium point. Two models
of coupled excitatory-inhibitory oscillators are used to illustrate the
approach.
</summary>
    <author>
      <name>Wilson A. Truccolo</name>
    </author>
    <author>
      <name>Govindan Rangarajan</name>
    </author>
    <author>
      <name>Yonghong Chen</name>
    </author>
    <author>
      <name>Mingzhou Ding</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Networks, vol. 16, 1453-1460 (2003)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0405505v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0405505v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0412680v1</id>
    <updated>2004-12-24T11:38:34Z</updated>
    <published>2004-12-24T11:38:34Z</published>
    <title>Vector-Neuron Models of Associative Memory</title>
    <summary>  We consider two models of Hopfield-like associative memory with $q$-valued
neurons: Potts-glass neural network (PGNN) and parametrical neural network
(PNN). In these models neurons can be in more than two different states. The
models have the record characteristics of its storage capacity and noise
immunity, and significantly exceed the Hopfield model. We present a uniform
formalism allowing us to describe both PNN and PGNN. This networks inherent
mechanisms, responsible for outstanding recognizing properties, are clarified.
</summary>
    <author>
      <name>B. V. Kryzhanovsky</name>
    </author>
    <author>
      <name>L. B. Litinskii</name>
    </author>
    <author>
      <name>A. L. Mikaelian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, Lecture on International Joint Conference on Neural Networks
  IJCNN-2004</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0412680v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0412680v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/q-bio/0601003v2</id>
    <updated>2006-10-24T14:14:07Z</updated>
    <published>2006-01-03T18:54:23Z</published>
    <title>Designing the Dynamics of Spiking Neural Networks</title>
    <summary>  Precise timing of spikes and temporal locking are key elements of neural
computation. Here we demonstrate how even strongly heterogeneous, deterministic
neural networks with delayed interactions and complex topology can exhibit
periodic patterns of spikes that are precisely timed. We develop an analytical
method to find the set of all networks exhibiting a predefined pattern
dynamics. Such patterns may be arbitrarily long and of complicated temporal
structure. We point out that the same pattern can exist in very different
networks and have different stability properties.
</summary>
    <author>
      <name>Raoul-Martin Memmesheimer</name>
    </author>
    <author>
      <name>Marc Timme</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.97.188101</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.97.188101" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/q-bio/0601003v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/q-bio/0601003v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.3451v1</id>
    <updated>2012-11-14T22:28:56Z</updated>
    <published>2012-11-14T22:28:56Z</published>
    <title>Memory Capacity of a Random Neural Network</title>
    <summary>  This paper considers the problem of information capacity of a random neural
network. The network is represented by matrices that are square and
symmetrical. The matrices have a weight which determines the highest and lowest
possible value found in the matrix. The examined matrices are randomly
generated and analyzed by a computer program. We find the surprising result
that the capacity of the network is a maximum for the binary random neural
network and it does not change as the number of quantization levels associated
with the weights increases.
</summary>
    <author>
      <name>Matt Stowe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.3451v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.3451v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.05133v2</id>
    <updated>2015-09-02T18:27:36Z</updated>
    <published>2015-08-20T21:35:52Z</published>
    <title>Steps Toward Deep Kernel Methods from Infinite Neural Networks</title>
    <summary>  Contemporary deep neural networks exhibit impressive results on practical
problems. These networks generalize well although their inherent capacity may
extend significantly beyond the number of training examples. We analyze this
behavior in the context of deep, infinite neural networks. We show that deep
infinite layers are naturally aligned with Gaussian processes and kernel
methods, and devise stochastic kernels that encode the information of these
networks. We show that stability results apply despite the size, offering an
explanation for their empirical success.
</summary>
    <author>
      <name>Tamir Hazan</name>
    </author>
    <author>
      <name>Tommi Jaakkola</name>
    </author>
    <link href="http://arxiv.org/abs/1508.05133v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.05133v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.00062v1</id>
    <updated>2015-08-28T16:28:27Z</updated>
    <published>2015-08-28T16:28:27Z</published>
    <title>The Kinetic Energy of Hydrocarbons as a Function of Electron Density and
  Convolutional Neural Networks</title>
    <summary>  We demonstrate a convolutional neural network trained to reproduce the
Kohn-Sham kinetic energy of hydrocarbons from electron density. The output of
the network is used as a non-local correction to the conventional local and
semi-local kinetic functionals. We show that this approximation qualitatively
reproduces Kohn-Sham potential energy surfaces when used with conventional
exchange correlation functionals. Numerical noise inherited from the
non-linearity of the neural network is identified as the major challenge for
the model. Finally we examine the features in the density learned by the neural
network to anticipate the prospects of generalizing these models.
</summary>
    <author>
      <name>Kun Yao</name>
    </author>
    <author>
      <name>John Parkhill</name>
    </author>
    <link href="http://arxiv.org/abs/1509.00062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.00062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.09887v2</id>
    <updated>2017-03-09T18:07:37Z</updated>
    <published>2016-10-31T12:08:46Z</published>
    <title>Depth-Width Tradeoffs in Approximating Natural Functions with Neural
  Networks</title>
    <summary>  We provide several new depth-based separation results for feed-forward neural
networks, proving that various types of simple and natural functions can be
better approximated using deeper networks than shallower ones, even if the
shallower networks are much larger. This includes indicators of balls and
ellipses; non-linear functions which are radial with respect to the $L_1$ norm;
and smooth non-linear functions. We also show that these gaps can be observed
experimentally: Increasing the depth indeed allows better learning than
increasing width, when training neural networks to learn an indicator of a unit
ball.
</summary>
    <author>
      <name>Itay Safran</name>
    </author>
    <author>
      <name>Ohad Shamir</name>
    </author>
    <link href="http://arxiv.org/abs/1610.09887v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.09887v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.06694v1</id>
    <updated>2016-11-21T09:24:24Z</updated>
    <published>2016-11-21T09:24:24Z</published>
    <title>Training Sparse Neural Networks</title>
    <summary>  Deep neural networks with lots of parameters are typically used for
large-scale computer vision tasks such as image classification. This is a
result of using dense matrix multiplications and convolutions. However, sparse
computations are known to be much more efficient. In this work, we train and
build neural networks which implicitly use sparse computations. We introduce
additional gate variables to perform parameter selection and show that this is
equivalent to using a spike-and-slab prior. We experimentally validate our
method on both small and large networks and achieve state-of-the-art
compression results for sparse neural network models.
</summary>
    <author>
      <name>Suraj Srinivas</name>
    </author>
    <author>
      <name>Akshayvarun Subramanya</name>
    </author>
    <author>
      <name>R. Venkatesh Babu</name>
    </author>
    <link href="http://arxiv.org/abs/1611.06694v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.06694v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00534v1</id>
    <updated>2017-03-01T22:21:21Z</updated>
    <published>2017-03-01T22:21:21Z</published>
    <title>Skin cancer reorganization and classification with deep neural network</title>
    <summary>  As one kind of skin cancer, melanoma is very dangerous. Dermoscopy based
early detection and recarbonization strategy is critical for melanoma therapy.
However, well-trained dermatologists dominant the diagnostic accuracy. In order
to solve this problem, many effort focus on developing automatic image analysis
systems. Here we report a novel strategy based on deep learning technique, and
achieve very high skin lesion segmentation and melanoma diagnosis accuracy: 1)
we build a segmentation neural network (skin_segnn), which achieved very high
lesion boundary detection accuracy; 2) We build another very deep neural
network based on Google inception v3 network (skin_recnn) and its well-trained
weight. The novel designed transfer learning based deep neural network
skin_inceptions_v3_nn helps to achieve a high prediction accuracy.
</summary>
    <author>
      <name>Hao Chang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures. ISIC2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.00534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00784v1</id>
    <updated>2017-07-03T23:45:54Z</updated>
    <published>2017-07-03T23:45:54Z</published>
    <title>Deep Jointly-Informed Neural Networks</title>
    <summary>  In this work a novel, automated process for determining an appropriate deep
neural network architecture and weight initialization based on decision trees
is presented. The method maps a collection of decision trees trained on the
data into a collection of initialized neural networks, with the structure of
the network determined by the structure of the tree. These models, referred to
as "deep jointly-informed neural networks", demonstrate high predictive
performance for a variety of datasets. Furthermore, the algorithm is readily
cast into a Bayesian framework, resulting in accurate and scalable models that
provide quantified uncertainties on predictions.
</summary>
    <author>
      <name>K. D. Humbird</name>
    </author>
    <author>
      <name>J. L. Peterson</name>
    </author>
    <author>
      <name>R. G. McClarren</name>
    </author>
    <link href="http://arxiv.org/abs/1707.00784v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00784v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.03414v1</id>
    <updated>2017-10-10T06:14:58Z</updated>
    <published>2017-10-10T06:14:58Z</published>
    <title>Network of Recurrent Neural Networks</title>
    <summary>  We describe a class of systems theory based neural networks called "Network
Of Recurrent neural networks" (NOR), which introduces a new structure level to
RNN related models. In NOR, RNNs are viewed as the high-level neurons and are
used to build the high-level layers. More specifically, we propose several
methodologies to design different NOR topologies according to the theory of
system evolution. Then we carry experiments on three different tasks to
evaluate our implementations. Experimental results show our models outperform
simple RNN remarkably under the same number of parameters, and sometimes
achieve even better results than GRU and LSTM.
</summary>
    <author>
      <name>Chao-Ming Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review as a conference paper at AAAI 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.03414v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.03414v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03646v1</id>
    <updated>2018-02-10T19:43:42Z</updated>
    <published>2018-02-10T19:43:42Z</published>
    <title>On the Universal Approximability of Quantized ReLU Neural Networks</title>
    <summary>  Compression is a key step to deploy large neural networks on
resource-constrained platforms. As a popular compression technique,
quantization constrains the number of distinct weight values and thus reducing
the number of bits required to represent and store each weight. In this paper,
we study the representation power of quantized neural networks. First, we prove
the universal approximability of quantized ReLU networks. Then we provide upper
bounds of storage size given the approximation error bound and the bit-width of
weights for function-independent and function-dependent structures. To the best
of the authors' knowledge, this is the first work on the universal
approximability as well as the associated storage size bound of quantized
neural networks.
</summary>
    <author>
      <name>Yukun Ding</name>
    </author>
    <author>
      <name>Jinglan Liu</name>
    </author>
    <author>
      <name>Yiyu Shi</name>
    </author>
    <link href="http://arxiv.org/abs/1802.03646v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03646v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08265v1</id>
    <updated>2016-08-29T22:02:39Z</updated>
    <published>2016-08-29T22:02:39Z</published>
    <title>About Learning in Recurrent Bistable Gradient Networks</title>
    <summary>  Recurrent Bistable Gradient Networks are attractor based neural networks
characterized by bistable dynamics of each single neuron. Coupled together
using linear interaction determined by the interconnection weights, these
networks do not suffer from spurious states or very limited capacity anymore.
Vladimir Chinarov and Michael Menzinger, who invented these networks, trained
them using Hebb's learning rule. We show, that this way of computing the
weights leads to unwanted behaviour and limitations of the networks
capabilities. Furthermore we evince, that using the first order of Hintons
Contrastive Divergence algorithm leads to a quite promising recurrent neural
network. These findings are tested by learning images of the MNIST database for
handwritten numbers.
</summary>
    <author>
      <name>J. Fischer</name>
    </author>
    <author>
      <name>S. Lackner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.08265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.01145v3</id>
    <updated>2017-05-01T14:01:32Z</updated>
    <published>2016-10-03T23:08:22Z</published>
    <title>Error bounds for approximations with deep ReLU networks</title>
    <summary>  We study expressive power of shallow and deep neural networks with piece-wise
linear activation functions. We establish new rigorous upper and lower bounds
for the network complexity in the setting of approximations in Sobolev spaces.
In particular, we prove that deep ReLU networks more efficiently approximate
smooth functions than shallow networks. In the case of approximations of 1D
Lipschitz functions we describe adaptive depth-6 network architectures more
efficient than the standard shallow architecture.
</summary>
    <author>
      <name>Dmitry Yarotsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages; major revision in v3; submitted to Neural Networks</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.01145v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.01145v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.05731v1</id>
    <updated>2018-01-17T16:17:28Z</updated>
    <published>2018-01-17T16:17:28Z</published>
    <title>In-network Neural Networks</title>
    <summary>  We present N2Net, a system that implements binary neural networks using
commodity switching chips deployed in network switches and routers. Our system
shows that these devices can run simple neural network models, whose input is
encoded in the network packets' header, at packet processing speeds (billions
of packets per second). Furthermore, our experience highlights that switching
chips could support even more complex models, provided that some minor and
cheap modifications to the chip's design are applied. We believe N2Net provides
an interesting building block for future end-to-end networked systems.
</summary>
    <author>
      <name>Giuseppe Siracusano</name>
    </author>
    <author>
      <name>Roberto Bifulco</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at SysML 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.05731v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.05731v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.01963v1</id>
    <updated>2017-11-06T15:36:32Z</updated>
    <published>2017-11-06T15:36:32Z</published>
    <title>Semi-Parallel Deep Neural Networks (SPDNN), Convergence and
  Generalization</title>
    <summary>  The Semi-Parallel Deep Neural Network (SPDNN) idea is explained in this
article and it has been shown that the convergence of the mixed network is very
close to the best network in the set and the generalization of SPDNN is better
than all the parent networks.
</summary>
    <author>
      <name>Shabab Bazrafkan</name>
    </author>
    <author>
      <name>Peter Corcoran</name>
    </author>
    <link href="http://arxiv.org/abs/1711.01963v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.01963v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0401633v2</id>
    <updated>2005-07-15T16:07:12Z</updated>
    <published>2004-01-30T12:53:47Z</published>
    <title>Self-organized annealing in laterally inhibited neural networks shows
  power law decay</title>
    <summary>  In this paper we present a method which assigns to each layer of a multilayer
neural network, whose network dynamics is governed by a noisy winner-take-all
mechanism, a neural temperature. This neural temperature is obtained by a least
mean square fit of the probability distribution of the noisy winner-take-all
mechanism to the distribution of a softmax mechanism, which has a well defined
temperature as free parameter. We call this approximated temperature resulting
from the optimization step the neural temperature. We apply this method to a
multilayer neural network during learning the XOR-problem with a Hebb-like
learning rule and show that after a transient the neural temperature decreases
in each layer according to a power law. This indicates a self-organized
annealing behavior induced by the learning rule itself instead of an external
adjustment of a control parameter as in physically motivated optimization
methods e.g. simulated annealing.
</summary>
    <author>
      <name>Frank Emmert-Streib</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0401633v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0401633v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0706.1051v1</id>
    <updated>2007-06-07T18:13:59Z</updated>
    <published>2007-06-07T18:13:59Z</published>
    <title>Improved Neural Modeling of Real-World Systems Using Genetic Algorithm
  Based Variable Selection</title>
    <summary>  Neural network models of real-world systems, such as industrial processes,
made from sensor data must often rely on incomplete data. System states may not
all be known, sensor data may be biased or noisy, and it is not often known
which sensor data may be useful for predictive modelling. Genetic algorithms
may be used to help to address this problem by determining the near optimal
subset of sensor variables most appropriate to produce good models. This paper
describes the use of genetic search to optimize variable selection to determine
inputs into the neural network model. We discuss genetic algorithm
implementation issues including data representation types and genetic operators
such as crossover and mutation. We present the use of this technique for neural
network modelling of a typical industrial application, a liquid fed ceramic
melter, and detail the results of the genetic search to optimize the neural
network model for this application.
</summary>
    <author>
      <name>Donald A. Sofge</name>
    </author>
    <author>
      <name>David L. Elliott</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">D. Sofge and D. Elliott, "Improved Neural Modeling of Real-World
  Systems Using Genetic Algorithm Based Variable Selection," In Int'l Conf. on
  Neural Networks and Brain (ICNN&amp;B'98-Beijing), 1998</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0706.1051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0706.1051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.06594v3</id>
    <updated>2015-09-28T10:15:35Z</updated>
    <published>2015-07-23T18:18:49Z</published>
    <title>Neural NILM: Deep Neural Networks Applied to Energy Disaggregation</title>
    <summary>  Energy disaggregation estimates appliance-by-appliance electricity
consumption from a single meter that measures the whole home's electricity
demand. Recently, deep neural networks have driven remarkable improvements in
classification performance in neighbouring machine learning fields such as
image classification and automatic speech recognition. In this paper, we adapt
three deep neural network architectures to energy disaggregation: 1) a form of
recurrent neural network called `long short-term memory' (LSTM); 2) denoising
autoencoders; and 3) a network which regresses the start time, end time and
average power demand of each appliance activation. We use seven metrics to test
the performance of these algorithms on real aggregate power data from five
appliances. Tests are performed against a house not seen during training and
against houses seen during training. We find that all three neural nets achieve
better F1 scores (averaged over all five appliances) than either combinatorial
optimisation or factorial hidden Markov models and that our neural net
algorithms generalise well to an unseen house.
</summary>
    <author>
      <name>Jack Kelly</name>
    </author>
    <author>
      <name>William Knottenbelt</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2821650.2821672</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2821650.2821672" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ACM BuildSys'15, November 4--5, 2015, Seoul</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.06594v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.06594v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9705270v1</id>
    <updated>1997-05-27T12:16:43Z</updated>
    <published>1997-05-27T12:16:43Z</published>
    <title>Neural Networks</title>
    <summary>  We review the theory of neural networks, as it has emerged in the last ten
years or so within the physics community, emphasizing questions of biological
relevance over those of importance in mathematical statistics and machine
learning theory.
</summary>
    <author>
      <name>Heinz Horner</name>
    </author>
    <author>
      <name>Reimer Kuehn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, LaTeX, 1 eps figure included, uses epsf and Springer's
  lamuphys style-file, review paper to appear in: "Intelligence and Artificial
  Intelligence" (provisional title), edited by U. Ratsch, M. Richter and I.O.
  Stamatescu (Springer, Heidelberg, 1997)</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9705270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9705270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0006010v1</id>
    <updated>2000-06-01T09:42:45Z</updated>
    <published>2000-06-01T09:42:45Z</published>
    <title>Statistical Mechanics of Recurrent Neural Networks I. Statics</title>
    <summary>  A lecture notes style review of the equilibrium statistical mechanics of
recurrent neural networks with discrete and continuous neurons (e.g. Ising,
coupled-oscillators). To be published in the Handbook of Biological Physics
(North-Holland). Accompanied by a similar review (part II) dealing with the
dynamics.
</summary>
    <author>
      <name>A. C. C. Coolen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">49 pages, LaTeX</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0006010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0006010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0006011v1</id>
    <updated>2000-06-01T09:43:46Z</updated>
    <published>2000-06-01T09:43:46Z</published>
    <title>Statistical Mechanics of Recurrent Neural Networks II. Dynamics</title>
    <summary>  A lecture notes style review of the non-equilibrium statistical mechanics of
recurrent neural networks with discrete and continuous neurons (e.g. Ising,
graded-response, coupled-oscillators). To be published in the Handbook of
Biological Physics (North-Holland). Accompanied by a similar review (part I)
dealing with the statics.
</summary>
    <author>
      <name>A. C. C. Coolen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">49 pages, LaTeX</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0006011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0006011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0010343v1</id>
    <updated>2000-10-23T13:01:14Z</updated>
    <published>2000-10-23T13:01:14Z</published>
    <title>Predicting and generating time series by neural networks: An
  investigation using statistical physics</title>
    <summary>  An overview is given about the statistical physics of neural networks
generating and analysing time series. Storage capacity, bit and sequence
generation, prediction error, antipredictable sequences, interacting
perceptrons and the application on the minority game are discussed. Finally, as
a demonstration a perceptron predicts bit sequences produced by human beings.
</summary>
    <author>
      <name>Wolfgang Kinzel</name>
    </author>
    <link href="http://arxiv.org/abs/cond-mat/0010343v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0010343v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0106242v1</id>
    <updated>2001-06-13T13:48:35Z</updated>
    <published>2001-06-13T13:48:35Z</published>
    <title>Local field dynamics in symmetric Q-Ising neural networks</title>
    <summary>  The time evolution of the local field in symmetric Q-Ising neural networks is
studied for arbitrary Q. In particular, the structure of the noise and the
appearance of gaps in the probability distribution are discussed. Results are
presented for several values of Q and compared with numerical simulations.
</summary>
    <author>
      <name>D. Bolle'</name>
    </author>
    <author>
      <name>G. M. Shim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages Latex, 6 eps figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0106242v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0106242v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0601129v1</id>
    <updated>2006-01-30T22:02:47Z</updated>
    <published>2006-01-30T22:02:47Z</published>
    <title>Instantaneously Trained Neural Networks</title>
    <summary>  This paper presents a review of instantaneously trained neural networks
(ITNNs). These networks trade learning time for size and, in the basic model, a
new hidden node is created for each training sample. Various versions of the
corner-classification family of ITNNs, which have found applications in
artificial intelligence (AI), are described. Implementation issues are also
considered.
</summary>
    <author>
      <name>Abhilash Ponnath</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0601129v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0601129v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/q-bio/0410004v1</id>
    <updated>2004-10-04T10:29:49Z</updated>
    <published>2004-10-04T10:29:49Z</published>
    <title>Can Neural Networks Recognize Parts?</title>
    <summary>  We have demonstrated neural networks can recognize parts by visual images.
Input signals are gray scale photographs of objects consisting of some parts
and output signals are their shapes. By training neural networks by a few set
of images, without any supervision they become to be able to recognize the
boundary between parts.
</summary>
    <author>
      <name>Koji Matsumura</name>
    </author>
    <author>
      <name>Y-h. Taguchi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to J. Phys. Soc. Jpn</arxiv:comment>
    <link href="http://arxiv.org/abs/q-bio/0410004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/q-bio/0410004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/quant-ph/0107012v2</id>
    <updated>2001-07-05T07:58:54Z</updated>
    <published>2001-07-03T13:16:38Z</published>
    <title>Quantum neural network</title>
    <summary>  It is suggested that a quantum neural network (QNN), a type of artificial
neural network, can be built using the principles of quantum information
processing. The input and output qubits in the QNN can be implemented by
optical modes with different polarization, the weights of the QNN can be
implemented by optical beam splitters and phase shifters
</summary>
    <author>
      <name>M. V. Altaisky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LaTeX, 4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/quant-ph/0107012v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/quant-ph/0107012v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/quant-ph/0305072v1</id>
    <updated>2003-05-13T12:28:52Z</updated>
    <published>2003-05-13T12:28:52Z</published>
    <title>A neural-network-like quantum information processing system</title>
    <summary>  The Hopfield neural networks and the holographic neural networks are models
which were successfully simulated on conventional computers. Starting with
these models, an analogous fundamental quantum information processing system is
developed in this article. Neuro-quantum interaction can regulate the
"collapse"-readout of quantum computation results. This paper is a
comprehensive introduction into associative processing and memory-storage in
quantum-physical framework.
</summary>
    <author>
      <name>Mitja Perus</name>
    </author>
    <author>
      <name>Horst Bischof</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, an introductory summary of essentials</arxiv:comment>
    <link href="http://arxiv.org/abs/quant-ph/0305072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/quant-ph/0305072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.1931v1</id>
    <updated>2012-07-09T01:26:53Z</updated>
    <published>2012-07-09T01:26:53Z</published>
    <title>An Augmented Smoothing Method of L1 -norm Minimization and Its
  Implementation by Neural Network Model</title>
    <summary>  In this paper we propose an augmented smoothing function for nonlinear L1
-norm minimization problem and consider a global stability of a gradient-based
neural network model to minimize the smoothing function. The numerical
simulations show that our smoothing neural network finds successfully the
global solution of the L1 -norm minimization problems considered in the
simulation.
</summary>
    <author>
      <name>Yunchol Jong</name>
    </author>
    <link href="http://arxiv.org/abs/1207.1931v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.1931v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90C30, 90C59, 65K10, 92B20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.6290v1</id>
    <updated>2013-07-24T03:48:25Z</updated>
    <published>2013-07-24T03:48:25Z</published>
    <title>Neural Network Model of Pricing Health Care Insurance</title>
    <summary>  To pricing health insurance plan, statisticians use mathematical models to
predict customers' future health condition. General Addictive Model (GAM) is a
wide accepted method for this problem. However, it have several limitations. To
solve this problem, a new method named neural network model is implemented.
Compare with GAM model, neural network provide a more accurate predicting
result.
</summary>
    <author>
      <name>Guanxi Zhuang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.6290v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.6290v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.4009v1</id>
    <updated>2012-08-20T13:54:26Z</updated>
    <published>2012-08-20T13:54:26Z</published>
    <title>Learning sparse messages in networks of neural cliques</title>
    <summary>  An extension to a recently introduced binary neural network is proposed in
order to allow the learning of sparse messages, in large numbers and with high
memory efficiency. This new network is justified both in biological and
informational terms. The learning and retrieval rules are detailed and
illustrated by various simulation results.
</summary>
    <author>
      <name>Behrooz Kamary Aliabadi</name>
    </author>
    <author>
      <name>Claude Berrou</name>
    </author>
    <author>
      <name>Vincent Gripon</name>
    </author>
    <author>
      <name>Xiaoran Jiang</name>
    </author>
    <link href="http://arxiv.org/abs/1208.4009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.4009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.6082v1</id>
    <updated>2012-10-22T22:59:58Z</updated>
    <published>2012-10-22T22:59:58Z</published>
    <title>Interplay: Dispersed Activation in Neural Networks</title>
    <summary>  This paper presents a multi-point stimulation of a Hebbian neural network
with investigation of the interplay between the stimulus waves through the
neurons of the network. Equilibrium of the resulting memory is achieved for
recall of specific memory data at a rate faster than single point stimulus. The
interplay of the intersecting stimuli appears to parallel the clarification
process of recall in biological systems.
</summary>
    <author>
      <name>Richard L. Churchill</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.6082v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.6082v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.1198v1</id>
    <updated>2008-12-05T18:18:02Z</updated>
    <published>2008-12-05T18:18:02Z</published>
    <title>A chilean seismic regionalization through a Kohonen neural network</title>
    <summary>  A study of seismic regionalization for central Chile based on a neural
network is presented. A scenario with six seismic regions is obtained,
independently of the size of the neighborhood or the reach of the correlation
between the cells of the grid. The high correlation between the spatial
distribution of the seismic zones and geographical data confirm our election of
the training vectors of the neural network.
</summary>
    <author>
      <name>Jorge Reyes</name>
    </author>
    <author>
      <name>Victor H. Cardenas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0812.1198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.1198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.geo-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.geo-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.3191v2</id>
    <updated>2014-12-14T03:18:33Z</updated>
    <published>2014-12-10T04:06:38Z</published>
    <title>Bach in 2014: Music Composition with Recurrent Neural Network</title>
    <summary>  We propose a framework for computer music composition that uses resilient
propagation (RProp) and long short term memory (LSTM) recurrent neural network.
In this paper, we show that LSTM network learns the structure and
characteristics of music pieces properly by demonstrating its ability to
recreate music. We also show that predicting existing music using RProp
outperforms Back propagation through time (BPTT).
</summary>
    <author>
      <name>I-Ting Liu</name>
    </author>
    <author>
      <name>Bhiksha Ramakrishnan</name>
    </author>
    <link href="http://arxiv.org/abs/1412.3191v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3191v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.04110v1</id>
    <updated>2016-07-14T12:45:07Z</updated>
    <published>2016-07-14T12:45:07Z</published>
    <title>Using Recurrent Neural Network for Learning Expressive Ontologies</title>
    <summary>  Recently, Neural Networks have been proven extremely effective in many
natural language processing tasks such as sentiment analysis, question
answering, or machine translation. Aiming to exploit such advantages in the
Ontology Learning process, in this technical report we present a detailed
description of a Recurrent Neural Network based system to be used to pursue
such goal.
</summary>
    <author>
      <name>Giulio Petrucci</name>
    </author>
    <author>
      <name>Chiara Ghidini</name>
    </author>
    <author>
      <name>Marco Rospocher</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.04110v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.04110v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.04968v1</id>
    <updated>2017-01-18T06:49:03Z</updated>
    <published>2017-01-18T06:49:03Z</published>
    <title>Multilayer Perceptron Algebra</title>
    <summary>  Artificial Neural Networks(ANN) has been phenomenally successful on various
pattern recognition tasks. However, the design of neural networks rely heavily
on the experience and intuitions of individual developers. In this article, the
author introduces a mathematical structure called MLP algebra on the set of all
Multilayer Perceptron Neural Networks(MLP), which can serve as a guiding
principle to build MLPs accommodating to the particular data sets, and to build
complex MLPs from simpler ones.
</summary>
    <author>
      <name>Zhao Peng</name>
    </author>
    <link href="http://arxiv.org/abs/1701.04968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.04968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.06749v2</id>
    <updated>2017-04-22T13:03:20Z</updated>
    <published>2017-03-20T14:02:11Z</published>
    <title>Efficient variational Bayesian neural network ensembles for outlier
  detection</title>
    <summary>  In this work we perform outlier detection using ensembles of neural networks
obtained by variational approximation of the posterior in a Bayesian neural
network setting. The variational parameters are obtained by sampling from the
true posterior by gradient descent. We show our outlier detection results are
comparable to those obtained using other efficient ensembling methods.
</summary>
    <author>
      <name>Nick Pawlowski</name>
    </author>
    <author>
      <name>Miguel Jaques</name>
    </author>
    <author>
      <name>Ben Glocker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at Workshop track - ICLR 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.06749v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.06749v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.08531v1</id>
    <updated>2017-04-27T12:27:25Z</updated>
    <published>2017-04-27T12:27:25Z</published>
    <title>A Survey of Neural Network Techniques for Feature Extraction from Text</title>
    <summary>  This paper aims to catalyze the discussions about text feature extraction
techniques using neural network architectures. The research questions discussed
in the paper focus on the state-of-the-art neural network techniques that have
proven to be useful tools for language processing, language generation, text
classification and other computational linguistics tasks.
</summary>
    <author>
      <name>Vineet John</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.08531v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.08531v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07101v1</id>
    <updated>2017-06-21T19:51:43Z</updated>
    <published>2017-06-21T19:51:43Z</published>
    <title>The energy landscape of a simple neural network</title>
    <summary>  We explore the energy landscape of a simple neural network. In particular, we
expand upon previous work demonstrating that the empirical complexity of fitted
neural networks is vastly less than a naive parameter count would suggest and
that this implicit regularization is actually beneficial for generalization
from fitted models.
</summary>
    <author>
      <name>Anthony Collins Gamst</name>
    </author>
    <author>
      <name>Alden Walker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.07101v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07101v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.05969v7</id>
    <updated>2018-01-16T10:02:55Z</updated>
    <published>2017-12-16T14:55:13Z</published>
    <title>Learning a Virtual Codec Based on Deep Convolutional Neural Network to
  Compress Image</title>
    <summary>  Although deep convolutional neural network has been proved to efficiently
eliminate coding artifacts caused by the coarse quantization of traditional
codec, it's difficult to train any neural network in front of the encoder for
gradient's back-propagation. In this paper, we propose an end-to-end image
compression framework based on convolutional neural network to resolve the
problem of non-differentiability of the quantization function in the standard
codec. First, the feature description neural network is used to get a valid
description in the low-dimension space with respect to the ground-truth image
so that the amount of image data is greatly reduced for storage or
transmission. After image's valid description, standard image codec such as
JPEG is leveraged to further compress image, which leads to image's great
distortion and compression artifacts, especially blocking artifacts, detail
missing, blurring, and ringing artifacts. Then, we use a post-processing neural
network to remove these artifacts. Due to the challenge of directly learning a
non-linear function for a standard codec based on convolutional neural network,
we propose to learn a virtual codec neural network to approximate the
projection from the valid description image to the post-processed compressed
image, so that the gradient could be efficiently back-propagated from the
post-processing neural network to the feature description neural network during
training. Meanwhile, an advanced learning algorithm is proposed to train our
deep neural networks for compression. Obviously, the priority of the proposed
method is compatible with standard existing codecs and our learning strategy
can be easily extended into these codecs based on convolutional neural network.
Experimental results have demonstrated the advances of the proposed method as
compared to several state-of-the-art approaches, especially at very low
bit-rate.
</summary>
    <author>
      <name>Lijun Zhao</name>
    </author>
    <author>
      <name>Huihui Bai</name>
    </author>
    <author>
      <name>Anhong Wang</name>
    </author>
    <author>
      <name>Yao Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.05969v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.05969v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0510169v1</id>
    <updated>2005-10-07T05:44:37Z</updated>
    <published>2005-10-07T05:44:37Z</published>
    <title>Theory of Recurrent Neural Network with Common Synaptic Inputs</title>
    <summary>  We discuss the effects of common synaptic inputs in a recurrent neural
network. Because of the effects of these common synaptic inputs, the
correlation between neural inputs cannot be ignored, and thus the network
exhibits sample dependence. Networks of this type do not have well-defined
thermodynamic limits, and self-averaging breaks down. We therefore need to
develop a suitable theory without relying on these common properties. While the
effects of the common synaptic inputs have been analyzed in layered neural
networks, it was apparently difficult to analyze these effects in recurrent
neural networks due to feedback connections. We investigated a sequential
associative memory model as an example of recurrent networks and succeeded in
deriving a macroscopic dynamical description as a recurrence relation form of a
probability density function.
</summary>
    <author>
      <name>Masaki Kawamura</name>
    </author>
    <author>
      <name>Michiko Yamana</name>
    </author>
    <author>
      <name>Masato Okada</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1143/JPSJ.74.2961</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1143/JPSJ.74.2961" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of the Physical Society of Japan, vol.74, no.11, Nov.
  2005, pp.2961-2965</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0510169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0510169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504064v1</id>
    <updated>2005-04-14T10:27:55Z</updated>
    <published>2005-04-14T10:27:55Z</published>
    <title>Neural-Network Techniques for Visual Mining Clinical
  Electroencephalograms</title>
    <summary>  In this chapter we describe new neural-network techniques developed for
visual mining clinical electroencephalograms (EEGs), the weak electrical
potentials invoked by brain activity. These techniques exploit fruitful ideas
of Group Method of Data Handling (GMDH). Section 2 briefly describes the
standard neural-network techniques which are able to learn well-suited
classification modes from data presented by relevant features. Section 3
introduces an evolving cascade neural network technique which adds new input
nodes as well as new neurons to the network while the training error decreases.
This algorithm is applied to recognize artifacts in the clinical EEGs. Section
4 presents the GMDH-type polynomial networks learnt from data. We applied this
technique to distinguish the EEGs recorded from an Alzheimer and a healthy
patient as well as recognize EEG artifacts. Section 5 describes the new
neural-network technique developed to induce multi-class concepts from data. We
used this technique for inducing a 16-class concept from the large-scale
clinical EEG data. Finally we discuss perspectives of applying the
neural-network techniques to clinical EEGs.
</summary>
    <author>
      <name>Vitaly Schetinin</name>
    </author>
    <author>
      <name>Joachim Schult</name>
    </author>
    <author>
      <name>Anatoly Brazhnikov</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0504064v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504064v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.5317v1</id>
    <updated>2012-06-22T20:36:56Z</updated>
    <published>2012-06-22T20:36:56Z</published>
    <title>Eigenvalue spectra of asymmetric random matrices for multi-component
  neural networks</title>
    <summary>  This paper focuses on large neural networks whose synaptic connectivity
matrices are randomly chosen from certain random matrix ensembles. The dynamics
of these networks can be characterized by the eigenvalue spectra of their
connectivity matrices. In reality, neurons in a network do not necessarily
behave in a similar way, but may belong to several different categories. The
first study of the spectra of two-component neural networks was carried out by
Rajan and Abbott. In their model, neurons are either 'excitatory' or
'inhibitory', and strengths of synapses from different types of neurons have
Gaussian distributions with different means and variances. A surprising finding
by Rajan and Abbott is that the eigenvalue spectra of these types of random
synaptic matrices do not depend on the mean values of their elements. In this
paper we prove that this is true even for a much more general type of random
neural network, where there is a finite number of types of neurons, and their
synaptic strengths have correlated distributions. Furthermore, using the
diagrammatic techniques, we calculate the explicit formula for the spectra of
synaptic matrices of multi-component neural networks.
</summary>
    <author>
      <name>Yi Wei</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.85.066116</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.85.066116" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 85, 066116 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1206.5317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.5317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.2173v1</id>
    <updated>2014-08-07T08:30:34Z</updated>
    <published>2014-08-07T08:30:34Z</published>
    <title>Face Detection Using Radial Basis Functions Neural Networks With Fixed
  Spread</title>
    <summary>  This paper presented a face detection system using Radial Basis Function
Neural Networks With Fixed Spread Value. Face detection is the first step in
face recognition system. The purpose is to localize and extract the face region
from the background that will be fed into the face recognition system for
identification. General preprocessing approach was used for normalizing the
image and Radial Basis Function (RBF) Neural Network was used to distinguish
between face and non-face. RBF Neural Networks offer several advantages
compared to other neural network architecture such as they can be trained using
fast two stages training algorithm and the network possesses the property of
best approximation. The output of the network can be optimized by setting
suitable value of center and spread of the RBF. In this paper, fixed spread
value will be used. The Radial Basis Function Neural Network (RBFNN) used to
distinguish faces and non-faces and the evaluation of the system will be the
performance of detection, False Acceptance Rate (FAR), False Rejection Rate
(FRR) and the discriminative properties.
</summary>
    <author>
      <name>K. A. A. Aziz</name>
    </author>
    <author>
      <name>S. S. Abdullah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, The Second International Conference on Control,
  Instrumentation and Mechatronic Engineering (CIM09) Malacca, Malaysia, June
  2-3, 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.2173v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.2173v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.00393v3</id>
    <updated>2015-07-23T17:11:04Z</updated>
    <published>2015-05-03T04:58:53Z</published>
    <title>ReNet: A Recurrent Neural Network Based Alternative to Convolutional
  Networks</title>
    <summary>  In this paper, we propose a deep neural network architecture for object
recognition based on recurrent neural networks. The proposed network, called
ReNet, replaces the ubiquitous convolution+pooling layer of the deep
convolutional neural network with four recurrent neural networks that sweep
horizontally and vertically in both directions across the image. We evaluate
the proposed ReNet on three widely-used benchmark datasets; MNIST, CIFAR-10 and
SVHN. The result suggests that ReNet is a viable alternative to the deep
convolutional neural network, and that further investigation is needed.
</summary>
    <author>
      <name>Francesco Visin</name>
    </author>
    <author>
      <name>Kyle Kastner</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <author>
      <name>Matteo Matteucci</name>
    </author>
    <author>
      <name>Aaron Courville</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <link href="http://arxiv.org/abs/1505.00393v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.00393v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.06342v1</id>
    <updated>2016-11-19T11:21:25Z</updated>
    <published>2016-11-19T11:21:25Z</published>
    <title>Quantized neural network design under weight capacity constraint</title>
    <summary>  The complexity of deep neural network algorithms for hardware implementation
can be lowered either by scaling the number of units or reducing the
word-length of weights. Both approaches, however, can accompany the performance
degradation although many types of research are conducted to relieve this
problem. Thus, it is an important question which one, between the network size
scaling and the weight quantization, is more effective for hardware
optimization. For this study, the performances of fully-connected deep neural
networks (FCDNNs) and convolutional neural networks (CNNs) are evaluated while
changing the network complexity and the word-length of weights. Based on these
experiments, we present the effective compression ratio (ECR) to guide the
trade-off between the network size and the precision of weights when the
hardware resource is limited.
</summary>
    <author>
      <name>Sungho Shin</name>
    </author>
    <author>
      <name>Kyuyeon Hwang</name>
    </author>
    <author>
      <name>Wonyong Sung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is accepted at NIPS 2016 workshop on Efficient Methods for
  Deep Neural Networks (EMDNN). arXiv admin note: text overlap with
  arXiv:1511.06488</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.06342v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.06342v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.01135v2</id>
    <updated>2017-05-19T04:50:29Z</updated>
    <published>2017-02-03T19:26:01Z</published>
    <title>Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks</title>
    <summary>  Deep neural networks have emerged as a widely used and effective means for
tackling complex, real-world problems. However, a major obstacle in applying
them to safety-critical systems is the great difficulty in providing formal
guarantees about their behavior. We present a novel, scalable, and efficient
technique for verifying properties of deep neural networks (or providing
counter-examples). The technique is based on the simplex method, extended to
handle the non-convex Rectified Linear Unit (ReLU) activation function, which
is a crucial ingredient in many modern neural networks. The verification
procedure tackles neural networks as a whole, without making any simplifying
assumptions. We evaluated our technique on a prototype deep neural network
implementation of the next-generation airborne collision avoidance system for
unmanned aircraft (ACAS Xu). Results show that our technique can successfully
prove properties of networks that are an order of magnitude larger than the
largest networks verified using existing methods.
</summary>
    <author>
      <name>Guy Katz</name>
    </author>
    <author>
      <name>Clark Barrett</name>
    </author>
    <author>
      <name>David Dill</name>
    </author>
    <author>
      <name>Kyle Julian</name>
    </author>
    <author>
      <name>Mykel Kochenderfer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the extended version of a paper with the same title that
  appeared at CAV 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.01135v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.01135v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0703405v3</id>
    <updated>2008-05-11T02:15:16Z</updated>
    <published>2007-03-15T10:42:09Z</published>
    <title>Topology and Dynamics of Attractor Neural Networks: The Role of
  Loopiness</title>
    <summary>  We derive an exact representation of the topological effect on the dynamics
of sequence processing neural networks within signal-to-noise analysis. A new
network structure parameter, loopiness coefficient, is introduced to
quantitatively study the loop effect on network dynamics. The large loopiness
coefficient means the large probability of finding loops in the networks. We
develop the recursive equations for the overlap parameters of neural networks
in the term of the loopiness. It was found that the large loopiness increases
the correlations among the network states at different times, and eventually it
reduces the performance of neural networks. The theory is applied to several
network topological structures, including fully-connected, densely-connected
random, densely-connected regular, and densely-connected small-world, where
encouraging results are obtained.
</summary>
    <author>
      <name>Pan Zhang</name>
    </author>
    <author>
      <name>Yong Chen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physa.2008.02.073</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physa.2008.02.073" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures, comments are favored</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physica A 387, (2008) 4411-4416</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0703405v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0703405v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.08512v1</id>
    <updated>2016-05-27T06:02:48Z</updated>
    <published>2016-05-27T06:02:48Z</published>
    <title>SNN: Stacked Neural Networks</title>
    <summary>  It has been proven that transfer learning provides an easy way to achieve
state-of-the-art accuracies on several vision tasks by training a simple
classifier on top of features obtained from pre-trained neural networks. The
goal of this work is to generate better features for transfer learning from
multiple publicly available pre-trained neural networks. To this end, we
propose a novel architecture called Stacked Neural Networks which leverages the
fast training time of transfer learning while simultaneously being much more
accurate. We show that using a stacked NN architecture can result in up to 8%
improvements in accuracy over state-of-the-art techniques using only one
pre-trained network for transfer learning. A second aim of this work is to make
network fine- tuning retain the generalizability of the base network to unseen
tasks. To this end, we propose a new technique called "joint fine-tuning" that
is able to give accuracies comparable to finetuning the same network
individually over two datasets. We also show that a jointly finetuned network
generalizes better to unseen tasks when compared to a network finetuned over a
single task.
</summary>
    <author>
      <name>Milad Mohammadi</name>
    </author>
    <author>
      <name>Subhasis Das</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.08512v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08512v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.07553v1</id>
    <updated>2017-11-20T21:28:40Z</updated>
    <published>2017-11-20T21:28:40Z</published>
    <title>Residual Gated Graph ConvNets</title>
    <summary>  Graph-structured data such as functional brain networks, social networks,
gene regulatory networks, communications networks have brought the interest in
generalizing neural networks to graph domains. In this paper, we are interested
to de- sign efficient neural network architectures for graphs with variable
length. Several existing works such as Scarselli et al. (2009); Li et al.
(2016) have focused on recurrent neural networks (RNNs) to solve this task. A
recent different approach was proposed in Sukhbaatar et al. (2016), where a
vanilla graph convolutional neural network (ConvNets) was introduced. We
believe the latter approach to be a better paradigm to solve graph learning
problems because ConvNets are more pruned to deep networks than RNNs. For this
reason, we propose the most generic class of residual multi-layer graph
ConvNets that make use of an edge gating mechanism, as proposed in Marcheggiani
&amp; Titov (2017). Gated edges appear to be a natural property in the context of
graph learning tasks, as the system has the ability to learn which edges are
important or not for the task to solve. We apply several graph neural models to
two basic network science tasks; subgraph matching and semi-supervised
clustering for graphs with variable length. Numerical results show the
performances of the new model.
</summary>
    <author>
      <name>Xavier Bresson</name>
    </author>
    <author>
      <name>Thomas Laurent</name>
    </author>
    <link href="http://arxiv.org/abs/1711.07553v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.07553v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/q-bio/0607034v1</id>
    <updated>2006-07-21T17:05:25Z</updated>
    <published>2006-07-21T17:05:25Z</published>
    <title>Nonoptimal Component Placement, but Short Processing Paths, due to
  Long-Distance Projections in Neural Systems</title>
    <summary>  It has been suggested that neural systems across several scales of
organization show optimal component placement, in which any spatial
rearrangement of the components would lead to an increase of total wiring.
Using extensive connectivity datasets for diverse neural networks combined with
spatial coordinates for network nodes, we applied an optimization algorithm to
the network layouts, in order to search for wire-saving component
rearrangements. We found that optimized component rearrangements could
substantially reduce total wiring length in all tested neural networks.
Specifically, total wiring among 95 primate (Macaque) cortical areas could be
decreased by 32%, and wiring of neuronal networks in the nematode
Caenorhabditis elegans could be reduced by 48% on the global level, and by 49%
for neurons within frontal ganglia. Wiring length reductions were possible due
to the existence of long-distance projections in neural networks. We explored
the role of these projections by comparing the original networks with minimally
rewired networks of the same size, which possessed only the shortest possible
connections. In the minimally rewired networks, the number of processing steps
along the shortest paths between components was significantly increased
compared to the original networks. Additional benchmark comparisons also
indicated that neural networks are more similar to network layouts that
minimize the length of processing paths, rather than wiring length. These
findings suggest that neural systems are not exclusively optimized for minimal
global wiring, but for a variety of factors including the minimization of
processing steps.
</summary>
    <author>
      <name>Marcus Kaiser</name>
    </author>
    <author>
      <name>Claus C. Hilgetag</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pcbi.0020095</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pcbi.0020095" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">PLoS Comput Biol 2(7): e95 (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/q-bio/0607034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/q-bio/0607034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.05463v4</id>
    <updated>2015-11-10T20:30:05Z</updated>
    <published>2015-08-22T03:36:43Z</published>
    <title>StochasticNet: Forming Deep Neural Networks via Stochastic Connectivity</title>
    <summary>  Deep neural networks is a branch in machine learning that has seen a meteoric
rise in popularity due to its powerful abilities to represent and model
high-level abstractions in highly complex data. One area in deep neural
networks that is ripe for exploration is neural connectivity formation. A
pivotal study on the brain tissue of rats found that synaptic formation for
specific functional connectivity in neocortical neural microcircuits can be
surprisingly well modeled and predicted as a random formation. Motivated by
this intriguing finding, we introduce the concept of StochasticNet, where deep
neural networks are formed via stochastic connectivity between neurons. As a
result, any type of deep neural networks can be formed as a StochasticNet by
allowing the neuron connectivity to be stochastic. Stochastic synaptic
formations, in a deep neural network architecture, can allow for efficient
utilization of neurons for performing specific tasks. To evaluate the
feasibility of such a deep neural network architecture, we train a
StochasticNet using four different image datasets (CIFAR-10, MNIST, SVHN, and
STL-10). Experimental results show that a StochasticNet, using less than half
the number of neural connections as a conventional deep neural network,
achieves comparable accuracy and reduces overfitting on the CIFAR-10, MNIST and
SVHN dataset. Interestingly, StochasticNet with less than half the number of
neural connections, achieved a higher accuracy (relative improvement in test
error rate of ~6% compared to ConvNet) on the STL-10 dataset than a
conventional deep neural network. Finally, StochasticNets have faster
operational speeds while achieving better or similar accuracy performances.
</summary>
    <author>
      <name>Mohammad Javad Shafiee</name>
    </author>
    <author>
      <name>Parthipan Siva</name>
    </author>
    <author>
      <name>Alexander Wong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.05463v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.05463v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.04082v2</id>
    <updated>2017-04-20T17:54:13Z</updated>
    <published>2017-01-15T17:32:02Z</published>
    <title>Embedding Watermarks into Deep Neural Networks</title>
    <summary>  Deep neural networks have recently achieved significant progress. Sharing
trained models of these deep neural networks is very important in the rapid
progress of researching or developing deep neural network systems. At the same
time, it is necessary to protect the rights of shared trained models. To this
end, we propose to use a digital watermarking technology to protect
intellectual property or detect intellectual property infringement of trained
models. Firstly, we formulate a new problem: embedding watermarks into deep
neural networks. We also define requirements, embedding situations, and attack
types for watermarking to deep neural networks. Secondly, we propose a general
framework to embed a watermark into model parameters using a parameter
regularizer. Our approach does not hurt the performance of networks into which
a watermark is embedded. Finally, we perform comprehensive experiments to
reveal the potential of watermarking to deep neural networks as a basis of this
new problem. We show that our framework can embed a watermark in the situations
of training a network from scratch, fine-tuning, and distilling without hurting
the performance of a deep neural network. The embedded watermark does not
disappear even after fine-tuning or parameter pruning; the watermark completely
remains even after removing 65% of parameters were pruned. The implementation
of this research is: https://github.com/yu4u/dnn-watermark
</summary>
    <author>
      <name>Yusuke Uchida</name>
    </author>
    <author>
      <name>Yuki Nagai</name>
    </author>
    <author>
      <name>Shigeyuki Sakazawa</name>
    </author>
    <author>
      <name>Shin'ichi Satoh</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3078971.3078974</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3078971.3078974" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICMR '17 Proceedings of the 2017 ACM on International Conference
  on Multimedia Retrieval Pages 269-277</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1701.04082v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.04082v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.02617v1</id>
    <updated>2018-02-07T19:58:50Z</updated>
    <published>2018-02-07T19:58:50Z</published>
    <title>Recognition of Acoustic Events Using Masked Conditional Neural Networks</title>
    <summary>  Automatic feature extraction using neural networks has accomplished
remarkable success for images, but for sound recognition, these models are
usually modified to fit the nature of the multi-dimensional temporal
representation of the audio signal in spectrograms. This may not efficiently
harness the time-frequency representation of the signal. The ConditionaL Neural
Network (CLNN) takes into consideration the interrelation between the temporal
frames, and the Masked ConditionaL Neural Network (MCLNN) extends upon the CLNN
by forcing a systematic sparseness over the network's weights using a binary
mask. The masking allows the network to learn about frequency bands rather than
bins, mimicking a filterbank used in signal transformations such as MFCC.
Additionally, the Mask is designed to consider various combinations of
features, which automates the feature hand-crafting process. We applied the
MCLNN for the Environmental Sound Recognition problem using the Urbansound8k,
YorNoise, ESC-10 and ESC-50 datasets. The MCLNN have achieved competitive
performance compared to state-of-the-art Convolutional Neural Networks and
hand-crafted attempts.
</summary>
    <author>
      <name>Fady Medhat</name>
    </author>
    <author>
      <name>David Chesmore</name>
    </author>
    <author>
      <name>John Robinson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICMLA.2017.0-158</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICMLA.2017.0-158" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Restricted Boltzmann Machine, RBM, Conditional Restricted Boltzmann
  Machine, CRBM, Conditional Neural Networks, CLNN, Masked Conditional Neural
  Networks, MCLNN, Deep Neural Network, Environmental Sound Recognition, ESR</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Machine Learning and Applications
  (ICMLA) Year: 2017 Pages: 199 - 206</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1802.02617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.02617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.01509v1</id>
    <updated>2017-05-03T17:08:05Z</updated>
    <published>2017-05-03T17:08:05Z</published>
    <title>Neural Models for Information Retrieval</title>
    <summary>  Neural ranking models for information retrieval (IR) use shallow or deep
neural networks to rank search results in response to a query. Traditional
learning to rank models employ machine learning techniques over hand-crafted IR
features. By contrast, neural models learn representations of language from raw
text that can bridge the gap between query and document vocabulary. Unlike
classical IR models, these new machine learning based approaches are
data-hungry, requiring large scale training data before they can be deployed.
This tutorial introduces basic concepts and intuitions behind neural IR models,
and places them in the context of traditional retrieval models. We begin by
introducing fundamental concepts of IR and different neural and non-neural
approaches to learning vector representations of text. We then review shallow
neural IR methods that employ pre-trained neural term embeddings without
learning the IR task end-to-end. We introduce deep neural networks next,
discussing popular deep architectures. Finally, we review the current DNN
models for information retrieval. We conclude with a discussion on potential
future directions for neural IR.
</summary>
    <author>
      <name>Bhaskar Mitra</name>
    </author>
    <author>
      <name>Nick Craswell</name>
    </author>
    <link href="http://arxiv.org/abs/1705.01509v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01509v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.4572v1</id>
    <updated>2010-09-23T10:44:24Z</updated>
    <published>2010-09-23T10:44:24Z</published>
    <title>Medical diagnosis using neural network</title>
    <summary>  This research is to search for alternatives to the resolution of complex
medical diagnosis where human knowledge should be apprehended in a general
fashion. Successful application examples show that human diagnostic
capabilities are significantly worse than the neural diagnostic system. This
paper describes a modified feedforward neural network constructive algorithm
(MFNNCA), a new algorithm for medical diagnosis. The new constructive algorithm
with backpropagation; offer an approach for the incremental construction of
near-minimal neural network architectures for pattern classification. The
algorithm starts with minimal number of hidden units in the single hidden
layer; additional units are added to the hidden layer one at a time to improve
the accuracy of the network and to get an optimal size of a neural network. The
MFNNCA was tested on several benchmarking classification problems including the
cancer, heart disease and diabetes. Experimental results show that the MFNNCA
can produce optimal neural network architecture with good generalization
ability.
</summary>
    <author>
      <name>S. M. Kamruzzaman</name>
    </author>
    <author>
      <name>Ahmed Ryadh Hasan</name>
    </author>
    <author>
      <name>Abu Bakar Siddiquee</name>
    </author>
    <author>
      <name>Md. Ehsanul Hoque Mazumder</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, International Conference</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 3rd International Conference on Electrical &amp; Computer
  Engineering (ICECE 2004), Dhaka Bangladesh, pp. 537-540, Dec. 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.4572v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.4572v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.01847v1</id>
    <updated>2015-03-06T04:48:00Z</updated>
    <published>2015-03-06T04:48:00Z</published>
    <title>Estimation of the parameters of an infectious disease model using neural
  networks</title>
    <summary>  In this paper, we propose a realistic mathematical model taking into account
the mutual interference among the interacting populations. This model attempts
to describe the control (vaccination) function as a function of the number of
infective individuals, which is an improvement over the existing susceptible
infective epidemic models. Regarding the growth of the epidemic as a nonlinear
phenomenon we have developed a neural network architecture to estimate the
vital parameters associated with this model. This architecture is based on a
recently developed new class of neural networks known as co-operative and
supportive neural networks. The application of this architecture to the present
study involves preprocessing of the input data, and this renders an efficient
estimation of the rate of spread of the epidemic. It is observed that the
proposed new neural network outperforms a simple feed-forward neural network
and polynomial regression.
</summary>
    <author>
      <name>V. Sree Hari Rao</name>
    </author>
    <author>
      <name>M. Naresh Kumar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.nonrwa.2009.04.006</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.nonrwa.2009.04.006" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 11 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Nonlinear Analysis: Real World Applications 11(2010) 1810-1818</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1503.01847v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.01847v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.00709v1</id>
    <updated>2016-01-29T13:53:42Z</updated>
    <published>2016-01-29T13:53:42Z</published>
    <title>Quantum perceptron over a field and neural network architecture
  selection in a quantum computer</title>
    <summary>  In this work, we propose a quantum neural network named quantum perceptron
over a field (QPF). Quantum computers are not yet a reality and the models and
algorithms proposed in this work cannot be simulated in actual (or classical)
computers. QPF is a direct generalization of a classical perceptron and solves
some drawbacks found in previous models of quantum perceptrons. We also present
a learning algorithm named Superposition based Architecture Learning algorithm
(SAL) that optimizes the neural network weights and architectures. SAL searches
for the best architecture in a finite set of neural network architectures with
linear time over the number of patterns in the training set. SAL is the first
learning algorithm to determine neural network architectures in polynomial
time. This speedup is obtained by the use of quantum parallelism and a
non-linear quantum operator.
</summary>
    <author>
      <name>Adenilton J. da Silva</name>
    </author>
    <author>
      <name>Teresa B. Ludermir</name>
    </author>
    <author>
      <name>Wilson R. de Oliveira</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neunet.2016.01.002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neunet.2016.01.002" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Networks, Volume 76, April 2016, Pages 55-64</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.00709v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.00709v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08275v1</id>
    <updated>2016-04-28T00:35:32Z</updated>
    <published>2016-04-28T00:35:32Z</published>
    <title>Crafting Adversarial Input Sequences for Recurrent Neural Networks</title>
    <summary>  Machine learning models are frequently used to solve complex security
problems, as well as to make decisions in sensitive situations like guiding
autonomous vehicles or predicting financial market behaviors. Previous efforts
have shown that numerous machine learning models were vulnerable to adversarial
manipulations of their inputs taking the form of adversarial samples. Such
inputs are crafted by adding carefully selected perturbations to legitimate
inputs so as to force the machine learning model to misbehave, for instance by
outputting a wrong class if the machine learning task of interest is
classification. In fact, to the best of our knowledge, all previous work on
adversarial samples crafting for neural network considered models used to solve
classification tasks, most frequently in computer vision applications. In this
paper, we contribute to the field of adversarial machine learning by
investigating adversarial input sequences for recurrent neural networks
processing sequential data. We show that the classes of algorithms introduced
previously to craft adversarial samples misclassified by feed-forward neural
networks can be adapted to recurrent neural networks. In a experiment, we show
that adversaries can craft adversarial sequences misleading both categorical
and sequential recurrent neural networks.
</summary>
    <author>
      <name>Nicolas Papernot</name>
    </author>
    <author>
      <name>Patrick McDaniel</name>
    </author>
    <author>
      <name>Ananthram Swami</name>
    </author>
    <author>
      <name>Richard Harang</name>
    </author>
    <link href="http://arxiv.org/abs/1604.08275v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08275v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07373v1</id>
    <updated>2016-08-26T07:14:37Z</updated>
    <published>2016-08-26T07:14:37Z</published>
    <title>Applying Topological Persistence in Convolutional Neural Network for
  Music Audio Signals</title>
    <summary>  Recent years have witnessed an increased interest in the application of
persistent homology, a topological tool for data analysis, to machine learning
problems. Persistent homology is known for its ability to numerically
characterize the shapes of spaces induced by features or functions. On the
other hand, deep neural networks have been shown effective in various tasks. To
our best knowledge, however, existing neural network models seldom exploit
shape information. In this paper, we investigate a way to use persistent
homology in the framework of deep neural networks. Specifically, we propose to
embed the so-called "persistence landscape," a rather new topological summary
for data, into a convolutional neural network (CNN) for dealing with audio
signals. Our evaluation on automatic music tagging, a multi-label
classification task, shows that the resulting persistent convolutional neural
network (PCNN) model can perform significantly better than state-of-the-art
models in prediction accuracy. We also discuss the intuition behind the design
of the proposed model, and offer insights into the features that it learns.
</summary>
    <author>
      <name>Jen-Yu Liu</name>
    </author>
    <author>
      <name>Shyh-Kang Jeng</name>
    </author>
    <author>
      <name>Yi-Hsuan Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1608.07373v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07373v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.06918v1</id>
    <updated>2016-10-21T19:58:29Z</updated>
    <published>2016-10-21T19:58:29Z</published>
    <title>Learning to Protect Communications with Adversarial Neural Cryptography</title>
    <summary>  We ask whether neural networks can learn to use secret keys to protect
information from other neural networks. Specifically, we focus on ensuring
confidentiality properties in a multiagent system, and we specify those
properties in terms of an adversary. Thus, a system may consist of neural
networks named Alice and Bob, and we aim to limit what a third neural network
named Eve learns from eavesdropping on the communication between Alice and Bob.
We do not prescribe specific cryptographic algorithms to these neural networks;
instead, we train end-to-end, adversarially. We demonstrate that the neural
networks can learn how to perform forms of encryption and decryption, and also
how to apply these operations selectively in order to meet confidentiality
goals.
</summary>
    <author>
      <name>Mart√≠n Abadi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Google Brain</arxiv:affiliation>
    </author>
    <author>
      <name>David G. Andersen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Google Brain</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.06918v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.06918v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.10458v1</id>
    <updated>2017-03-30T13:18:35Z</updated>
    <published>2017-03-30T13:18:35Z</published>
    <title>Application of a Shallow Neural Network to Short-Term Stock Trading</title>
    <summary>  Machine learning is increasingly prevalent in stock market trading. Though
neural networks have seen success in computer vision and natural language
processing, they have not been as useful in stock market trading. To
demonstrate the applicability of a neural network in stock trading, we made a
single-layer neural network that recommends buying or selling shares of a stock
by comparing the highest high of 10 consecutive days with that of the next 10
days, a process repeated for the stock's year-long historical data. A
chi-squared analysis found that the neural network can accurately and
appropriately decide whether to buy or sell shares for a given stock, showing
that a neural network can make simple decisions about the stock market.
</summary>
    <author>
      <name>Abhinav Madahar</name>
    </author>
    <author>
      <name>Yuze Ma</name>
    </author>
    <author>
      <name>Kunal Patel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.10458v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.10458v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.00727v2</id>
    <updated>2018-01-31T08:43:37Z</updated>
    <published>2017-11-01T10:21:02Z</published>
    <title>Performance Evaluation of Channel Decoding With Deep Neural Networks</title>
    <summary>  With the demand of high data rate and low latency in fifth generation (5G),
deep neural network decoder (NND) has become a promising candidate due to its
capability of one-shot decoding and parallel computing. In this paper, three
types of NND, i.e., multi-layer perceptron (MLP), convolution neural network
(CNN) and recurrent neural network (RNN), are proposed with the same parameter
magnitude. The performance of these deep neural networks are evaluated through
extensive simulation. Numerical results show that RNN has the best decoding
performance, yet at the price of the highest computational overhead. Moreover,
we find there exists a saturation length for each type of neural network, which
is caused by their restricted learning abilities.
</summary>
    <author>
      <name>Wei Lyu</name>
    </author>
    <author>
      <name>Zhaoyang Zhang</name>
    </author>
    <author>
      <name>Chunxu Jiao</name>
    </author>
    <author>
      <name>Kangjian Qin</name>
    </author>
    <author>
      <name>Huazi Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 11 figures, Latex; typos corrected; IEEE ICC 2018 to appear</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.00727v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.00727v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.05860v1</id>
    <updated>2017-11-06T19:17:58Z</updated>
    <published>2017-11-06T19:17:58Z</published>
    <title>A General Neural Network Hardware Architecture on FPGA</title>
    <summary>  Field Programmable Gate Arrays (FPGAs) plays an increasingly important role
in data sampling and processing industries due to its highly parallel
architecture, low power consumption, and flexibility in custom algorithms.
Especially, in the artificial intelligence field, for training and implement
the neural networks and machine learning algorithms, high energy efficiency
hardware implement and massively parallel computing capacity are heavily
demanded. Therefore, many global companies have applied FPGAs into AI and
Machine learning fields such as autonomous driving and Automatic Spoken
Language Recognition (Baidu) [1] [2] and Bing search (Microsoft) [3].
Considering the FPGAs great potential in these fields, we tend to implement a
general neural network hardware architecture on XILINX ZU9CG System On Chip
(SOC) platform [4], which contains abundant hardware resource and powerful
processing capacity. The general neural network architecture on the FPGA SOC
platform can perform forward and backward algorithms in deep neural networks
(DNN) with high performance and easily be adjusted according to the type and
scale of the neural networks.
</summary>
    <author>
      <name>Yufeng Hao</name>
    </author>
    <link href="http://arxiv.org/abs/1711.05860v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.05860v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.08831v1</id>
    <updated>2018-01-26T14:45:24Z</updated>
    <published>2018-01-26T14:45:24Z</published>
    <title>A Multilayer Convolutional Encoder-Decoder Neural Network for
  Grammatical Error Correction</title>
    <summary>  We improve automatic correction of grammatical, orthographic, and collocation
errors in text using a multilayer convolutional encoder-decoder neural network.
The network is initialized with embeddings that make use of character N-gram
information to better suit this task. When evaluated on common benchmark test
data sets (CoNLL-2014 and JFLEG), our model substantially outperforms all prior
neural approaches on this task as well as strong statistical machine
translation-based systems with neural and task-specific features trained on the
same data. Our analysis shows the superiority of convolutional neural networks
over recurrent neural networks such as long short-term memory (LSTM) networks
in capturing the local context via attention, and thereby improving the
coverage in correcting grammatical errors. By ensembling multiple models, and
incorporating an N-gram language model and edit features via rescoring, our
novel method becomes the first neural approach to outperform the current
state-of-the-art statistical machine translation-based approach, both in terms
of grammaticality and fluency.
</summary>
    <author>
      <name>Shamil Chollampatt</name>
    </author>
    <author>
      <name>Hwee Tou Ng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures, In Proceedings of AAAI 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.08831v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.08831v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.07850v1</id>
    <updated>2017-10-21T20:14:00Z</updated>
    <published>2017-10-21T20:14:00Z</published>
    <title>Deep Neural Network Approximation using Tensor Sketching</title>
    <summary>  Deep neural networks are powerful learning models that achieve
state-of-the-art performance on many computer vision, speech, and language
processing tasks. In this paper, we study a fundamental question that arises
when designing deep network architectures: Given a target network architecture
can we design a smaller network architecture that approximates the operation of
the target network? The question is, in part, motivated by the challenge of
parameter reduction (compression) in modern deep neural networks, as the ever
increasing storage and memory requirements of these networks pose a problem in
resource constrained environments.
  In this work, we focus on deep convolutional neural network architectures,
and propose a novel randomized tensor sketching technique that we utilize to
develop a unified framework for approximating the operation of both the
convolutional and fully connected layers. By applying the sketching technique
along different tensor dimensions, we design changes to the convolutional and
fully connected layers that substantially reduce the number of effective
parameters in a network. We show that the resulting smaller network can be
trained directly, and has a classification accuracy that is comparable to the
original network.
</summary>
    <author>
      <name>Shiva Prasad Kasiviswanathan</name>
    </author>
    <author>
      <name>Nina Narodytska</name>
    </author>
    <author>
      <name>Hongxia Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.07850v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.07850v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.0929v1</id>
    <updated>2007-09-06T18:00:15Z</updated>
    <published>2007-09-06T18:00:15Z</published>
    <title>Analysis of network by generalized mutual entropies</title>
    <summary>  Generalized mutual entropy is defined for networks and applied for analysis
of complex network structures. The method is tested for the case of computer
simulated scale free networks, random networks, and their mixtures. The
possible applications for real network analysis are discussed.
</summary>
    <author>
      <name>V. Gudkov</name>
    </author>
    <author>
      <name>V. Montealegre</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physa.2008.01.005</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physa.2008.01.005" rel="related"/>
    <link href="http://arxiv.org/abs/0709.0929v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.0929v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0511042v1</id>
    <updated>2005-11-10T10:00:46Z</updated>
    <published>2005-11-10T10:00:46Z</published>
    <title>Dimensions of Neural-symbolic Integration - A Structured Survey</title>
    <summary>  Research on integrated neural-symbolic systems has made significant progress
in the recent past. In particular the understanding of ways to deal with
symbolic knowledge within connectionist systems (also called artificial neural
networks) has reached a critical mass which enables the community to strive for
applicable implementations and use cases. Recent work has covered a great
variety of logics used in artificial intelligence and provides a multitude of
techniques for dealing with them within the context of artificial neural
networks. We present a comprehensive survey of the field of neural-symbolic
integration, including a new classification of system according to their
architectures and abilities.
</summary>
    <author>
      <name>Sebastian Bader</name>
    </author>
    <author>
      <name>Pascal Hitzler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0511042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0511042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08571v1</id>
    <updated>2016-02-27T08:45:35Z</updated>
    <published>2016-02-27T08:45:35Z</published>
    <title>Towards Neural Knowledge DNA</title>
    <summary>  In this paper, we propose the Neural Knowledge DNA, a framework that tailors
the ideas underlying the success of neural networks to the scope of knowledge
representation. Knowledge representation is a fundamental field that dedicate
to representing information about the world in a form that computer systems can
utilize to solve complex tasks. The proposed Neural Knowledge DNA is designed
to support discovering, storing, reusing, improving, and sharing knowledge
among machines and organisation. It is constructed in a similar fashion of how
DNA formed: built up by four essential elements. As the DNA produces
phenotypes, the Neural Knowledge DNA carries information and knowledge via its
four essential elements, namely, Networks, Experiences, States, and Actions.
</summary>
    <author>
      <name>Haoxi Zhang</name>
    </author>
    <author>
      <name>Cesar Sanin</name>
    </author>
    <author>
      <name>Edward Szczerbicki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.08571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.02540v3</id>
    <updated>2017-11-01T08:50:32Z</updated>
    <published>2017-09-08T05:00:20Z</published>
    <title>The Expressive Power of Neural Networks: A View from the Width</title>
    <summary>  The expressive power of neural networks is important for understanding deep
learning. Most existing works consider this problem from the view of the depth
of a network. In this paper, we study how width affects the expressiveness of
neural networks. Classical results state that depth-bounded (e.g. depth-$2$)
networks with suitable activation functions are universal approximators. We
show a universal approximation theorem for width-bounded ReLU networks:
width-$(n+4)$ ReLU networks, where $n$ is the input dimension, are universal
approximators. Moreover, except for a measure zero set, all functions cannot be
approximated by width-$n$ ReLU networks, which exhibits a phase transition.
Several recent works demonstrate the benefits of depth by proving the
depth-efficiency of neural networks. That is, there are classes of deep
networks which cannot be realized by any shallow network whose size is no more
than an exponential bound. Here we pose the dual question on the
width-efficiency of ReLU networks: Are there wide networks that cannot be
realized by narrow networks whose size is not substantially larger? We show
that there exist classes of wide networks which cannot be realized by any
narrow network whose depth is no more than a polynomial bound. On the other
hand, we demonstrate by extensive experiments that narrow networks whose size
exceed the polynomial bound by a constant factor can approximate wide and
shallow network with high accuracy. Our results provide more comprehensive
evidence that depth is more effective than width for the expressiveness of ReLU
networks.
</summary>
    <author>
      <name>Zhou Lu</name>
    </author>
    <author>
      <name>Hongming Pu</name>
    </author>
    <author>
      <name>Feicheng Wang</name>
    </author>
    <author>
      <name>Zhiqiang Hu</name>
    </author>
    <author>
      <name>Liwei Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted by NIPS 2017 ( with some typos fixed)</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.02540v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.02540v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504055v1</id>
    <updated>2005-04-13T13:57:56Z</updated>
    <published>2005-04-13T13:57:56Z</published>
    <title>A Learning Algorithm for Evolving Cascade Neural Networks</title>
    <summary>  A new learning algorithm for Evolving Cascade Neural Networks (ECNNs) is
described. An ECNN starts to learn with one input node and then adding new
inputs as well as new hidden neurons evolves it. The trained ECNN has a nearly
minimal number of input and hidden neurons as well as connections. The
algorithm was successfully applied to classify artifacts and normal segments in
clinical electroencephalograms (EEGs). The EEG segments were visually labeled
by EEG-viewer. The trained ECNN has correctly classified 96.69% of the testing
segments. It is slightly better than a standard fully connected neural network.
</summary>
    <author>
      <name>Vitaly Schetinin</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Processing Letter 17:21-31, 2003. Kluwer</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0504055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/nlin/0202039v1</id>
    <updated>2002-02-19T13:11:42Z</updated>
    <published>2002-02-19T13:11:42Z</published>
    <title>A neural model for multi-expert architectures</title>
    <summary>  We present a generalization of conventional artificial neural networks that
allows for a functional equivalence to multi-expert systems. The new model
provides an architectural freedom going beyond existing multi-expert models and
an integrative formalism to compare and combine various techniques of learning.
(We consider gradient, EM, reinforcement, and unsupervised learning.) Its
uniform representation aims at a simple genetic encoding and evolutionary
structure optimization of multi-expert systems. This paper contains a detailed
description of the model and learning rules, empirically validates its
functionality, and discusses future perspectives.
</summary>
    <author>
      <name>Marc Toussaint</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LaTeX, 8 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Joint Conference on Neural
  Networks (IJCNN 2002), 2755-2760.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/nlin/0202039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/nlin/0202039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.03592v1</id>
    <updated>2017-02-12T23:12:01Z</updated>
    <published>2017-02-12T23:12:01Z</published>
    <title>Graph Neural Networks and Boolean Satisfiability</title>
    <summary>  In this paper we explore whether or not deep neural architectures can learn
to classify Boolean satisfiability (SAT). We devote considerable time to
discussing the theoretical properties of SAT. Then, we define a graph
representation for Boolean formulas in conjunctive normal form, and train
neural classifiers over general graph structures called Graph Neural Networks,
or GNNs, to recognize features of satisfiability. To the best of our knowledge
this has never been tried before. Our preliminary findings are potentially
profound. In a weakly-supervised setting, that is, without problem specific
feature engineering, Graph Neural Networks can learn features of
satisfiability.
</summary>
    <author>
      <name>Benedikt B√ºnz</name>
    </author>
    <author>
      <name>Matthew Lamm</name>
    </author>
    <link href="http://arxiv.org/abs/1702.03592v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.03592v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.01991v2</id>
    <updated>2017-06-22T04:11:21Z</updated>
    <published>2017-06-06T21:58:50Z</published>
    <title>Unsupervised Neural-Symbolic Integration</title>
    <summary>  Symbolic has been long considered as a language of human intelligence while
neural networks have advantages of robust computation and dealing with noisy
data. The integration of neural-symbolic can offer better learning and
reasoning while providing a means for interpretability through the
representation of symbolic knowledge. Although previous works focus intensively
on supervised feedforward neural networks, little has been done for the
unsupervised counterparts. In this paper we show how to integrate symbolic
knowledge into unsupervised neural networks. We exemplify our approach with
knowledge in different forms, including propositional logic for DNA promoter
prediction and first-order logic for understanding family relationship.
</summary>
    <author>
      <name>Son N. Tran</name>
    </author>
    <link href="http://arxiv.org/abs/1706.01991v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.01991v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.4984v1</id>
    <updated>2010-09-25T07:05:29Z</updated>
    <published>2010-09-25T07:05:29Z</published>
    <title>Rule Extraction using Artificial Neural Networks</title>
    <summary>  Artificial neural networks have been successfully applied to a variety of
business application problems involving classification and regression. Although
backpropagation neural networks generally predict better than decision trees do
for pattern classification problems, they are often regarded as black boxes,
i.e., their predictions are not as interpretable as those of decision trees. In
many applications, it is desirable to extract knowledge from trained neural
networks so that the users can gain a better understanding of the solution.
This paper presents an efficient algorithm to extract rules from artificial
neural networks. We use two-phase training algorithm for backpropagation
learning. In the first phase, the number of hidden nodes of the network is
determined automatically in a constructive fashion by adding nodes one after
another based on the performance of the network on training data. In the second
phase, the number of relevant input units of the network is determined using
pruning algorithm. The pruning process attempts to eliminate as many
connections as possible from the network. Relevant and irrelevant attributes of
the data are distinguished during the training process. Those that are relevant
will be kept and others will be automatically discarded. From the simplified
networks having small number of connections and nodes we may easily able to
extract symbolic rules using the proposed algorithm. Extensive experimental
results on several benchmarks problems in neural networks demonstrate the
effectiveness of the proposed approach with good generalization ability.
</summary>
    <author>
      <name>S. M. Kamruzzaman</name>
    </author>
    <author>
      <name>Ahmed Ryadh Hasan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 Pages, International Conference</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. International Conference on Information and Communication
  Technology in Management (ICTM 2005), Multimedia University, Malaysia, May
  2005</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.4984v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.4984v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.6310v1</id>
    <updated>2013-02-26T04:22:44Z</updated>
    <published>2013-02-26T04:22:44Z</published>
    <title>Estimating Sectoral Pollution Load in Lagos, Nigeria Using Data Mining
  Techniques</title>
    <summary>  Industrial pollution is often considered to be one of the prime factors
contributing to air, water and soil pollution. Sectoral pollution loads
(ton/yr) into different media (i.e. air, water and land) in Lagos were
estimated using Industrial Pollution Projected System (IPPS). These were
further studied using Artificial neural Networks (ANNs), a data mining
technique that has the ability of detecting and describing patterns in large
data sets with variables that are non- linearly related. Time Lagged Recurrent
Network (TLRN) appeared as the best Neural Network model among all the neural
networks considered which includes Multilayer Perceptron (MLP) Network,
Generalized Feed Forward Neural Network (GFNN), Radial Basis Function (RBF)
Network and Recurrent Network (RN). TLRN modelled the data-sets better than the
others in terms of the mean average error (MAE) (0.14), time (39 s) and linear
correlation coefficient (0.84). The results showed that Artificial Neural
Networks (ANNs) technique (i.e., Time Lagged Recurrent Network) is also
applicable and effective in environmental assessment study. Keywords:
Artificial Neural Networks (ANNs), Data Mining Techniques, Industrial Pollution
Projection System (IPPS), Pollution load, Pollution Intensity.
</summary>
    <author>
      <name>Adesesan . B Adeyemo</name>
    </author>
    <author>
      <name>Adebola A. Oketola</name>
    </author>
    <author>
      <name>Emmanuel O. Adetula</name>
    </author>
    <author>
      <name>O. Osibanjo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 11 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">A.B ADEYEMO, A.A OKETOLA, E.O ADETULA, O.OSIBANJO (2012):
  Estimating Sectoral Pollution Load in Lagos, Nigeria Using Data Mining
  Techniques,International-Journal-of-Computer-Science-Issues, Volume 9, Issue
  6, November 2012 pages 465-475</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1302.6310v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.6310v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.03176v2</id>
    <updated>2016-11-21T16:22:35Z</updated>
    <published>2015-05-12T21:49:36Z</published>
    <title>Firing Rate Dynamics in Recurrent Spiking Neural Networks with Intrinsic
  and Network Heterogeneity</title>
    <summary>  Heterogeneity of neural attributes has recently gained a lot of attention and
is increasing recognized as a crucial feature in neural processing. Despite its
importance, this physiological feature has traditionally been neglected in
theoretical studies of cortical neural networks. Thus, there is still a lot
unknown about the consequences of cellular and circuit heterogeneity in spiking
neural networks. In particular, combining network or synaptic heterogeneity and
intrinsic heterogeneity has yet to be considered systematically despite the
fact that both are known to exist and likely have significant roles in neural
network dynamics. In a canonical recurrent spiking neural network model, we
study how these two forms of heterogeneity lead to different distributions of
excitatory firing rates. To analytically characterize how these types of
heterogeneities affect the network, we employ a dimension reduction method that
relies on a combination of Monte Carlo simulations and probability density
function equations. We find that the relationship between intrinsic and network
heterogeneity has a strong effect on the overall level of heterogeneity of the
firing rates. Specifically, this relationship can lead to amplification or
attenuation of firing rate heterogeneity, and these effects depend on whether
the recurrent network is firing asynchronously or rhythmically firing. These
observations are captured with the aforementioned reduction method, and
furthermore simpler analytic descriptions based on this dimension reduction
method are developed. The final analytic descriptions provide compact and
descriptive formulas for how the relationship between intrinsic and network
heterogeneity determines the firing rate heterogeneity dynamics in various
settings.
</summary>
    <author>
      <name>Cheng Ly</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10827-015-0578-0</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10827-015-0578-0" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.03176v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.03176v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.5472v2</id>
    <updated>2012-06-18T15:48:31Z</updated>
    <published>2011-07-27T13:48:22Z</published>
    <title>Neural Relax</title>
    <summary>  We present an algorithm for data preprocessing of an associative memory
inspired to an electrostatic problem that turns out to have intimate relations
with information maximization.
</summary>
    <author>
      <name>Elisa Benedetti</name>
    </author>
    <author>
      <name>Marco Budinich</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/NECO_a_00359</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/NECO_a_00359" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Computation, November 2012, Vol. 24, No. 11, pp. 3091 3110</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1107.5472v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.5472v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.04503v1</id>
    <updated>2017-01-17T01:15:14Z</updated>
    <published>2017-01-17T01:15:14Z</published>
    <title>Deep Learning for Computational Chemistry</title>
    <summary>  The rise and fall of artificial neural networks is well documented in the
scientific literature of both computer science and computational chemistry. Yet
almost two decades later, we are now seeing a resurgence of interest in deep
learning, a machine learning algorithm based on multilayer neural networks.
Within the last few years, we have seen the transformative impact of deep
learning in many domains, particularly in speech recognition and computer
vision, to the extent that the majority of expert practitioners in those field
are now regularly eschewing prior established models in favor of deep learning
models. In this review, we provide an introductory overview into the theory of
deep neural networks and their unique properties that distinguish them from
traditional machine learning algorithms used in cheminformatics. By providing
an overview of the variety of emerging applications of deep neural networks, we
highlight its ubiquity and broad applicability to a wide range of challenges in
the field, including QSAR, virtual screening, protein structure prediction,
quantum chemistry, materials design and property prediction. In reviewing the
performance of deep neural networks, we observed a consistent outperformance
against non-neural networks state-of-the-art models across disparate research
topics, and deep neural network based models often exceeded the "glass ceiling"
expectations of their respective tasks. Coupled with the maturity of
GPU-accelerated computing for training deep neural networks and the exponential
growth of chemical data on which to train these networks on, we anticipate that
deep learning algorithms will be a valuable tool for computational chemistry.
</summary>
    <author>
      <name>Garrett B. Goh</name>
    </author>
    <author>
      <name>Nathan O. Hodas</name>
    </author>
    <author>
      <name>Abhinav Vishnu</name>
    </author>
    <link href="http://arxiv.org/abs/1701.04503v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.04503v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.3563v1</id>
    <updated>2010-04-20T20:24:51Z</updated>
    <published>2010-04-20T20:24:51Z</published>
    <title>A QoS Provisioning Recurrent Neural Network based Call Admission Control
  for beyond 3G Networks</title>
    <summary>  The Call admission control (CAC) is one of the Radio Resource Management
(RRM) techniques that plays influential role in ensuring the desired Quality of
Service (QoS) to the users and applications in next generation networks. This
paper proposes a fuzzy neural approach for making the call admission control
decision in multi class traffic based Next Generation Wireless Networks (NGWN).
The proposed Fuzzy Neural call admission control (FNCAC) scheme is an
integrated CAC module that combines the linguistic control capabilities of the
fuzzy logic controller and the learning capabilities of the neural networks.
The model is based on recurrent radial basis function networks which have
better learning and adaptability that can be used to develop intelligent system
to handle the incoming traffic in an heterogeneous network environment. The
simulation results are optimistic and indicates that the proposed FNCAC
algorithm performs better than the other two methods and the call blocking
probability is minimal when compared to other two methods.
</summary>
    <author>
      <name>Ramesh Babu H. S.</name>
    </author>
    <author>
      <name> Gowrishankar</name>
    </author>
    <author>
      <name>Satyanarayana P. S</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues online at
  http://ijcsi.org/articles/A-QoS-Provisioning-Recurrent-Neural-Network-based-Call-Admission-Control-for-beyond-3G-Networks.php</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI, Volume 7, Issue 2, March 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.3563v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.3563v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.06910v2</id>
    <updated>2017-10-20T14:49:30Z</updated>
    <published>2017-10-18T19:53:57Z</published>
    <title>Characterization of Gradient Dominance and Regularity Conditions for
  Neural Networks</title>
    <summary>  The past decade has witnessed a successful application of deep learning to
solving many challenging problems in machine learning and artificial
intelligence. However, the loss functions of deep neural networks (especially
nonlinear networks) are still far from being well understood from a theoretical
aspect. In this paper, we enrich the current understanding of the landscape of
the square loss functions for three types of neural networks. Specifically,
when the parameter matrices are square, we provide an explicit characterization
of the global minimizers for linear networks, linear residual networks, and
nonlinear networks with one hidden layer. Then, we establish two quadratic
types of landscape properties for the square loss of these neural networks,
i.e., the gradient dominance condition within the neighborhood of their full
rank global minimizers, and the regularity condition along certain directions
and within the neighborhood of their global minimizers. These two landscape
properties are desirable for the optimization around the global minimizers of
the loss function for these neural networks.
</summary>
    <author>
      <name>Yi Zhou</name>
    </author>
    <author>
      <name>Yingbin Liang</name>
    </author>
    <link href="http://arxiv.org/abs/1710.06910v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.06910v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03308v1</id>
    <updated>2018-02-09T15:35:41Z</updated>
    <published>2018-02-09T15:35:41Z</published>
    <title>Predictive Neural Networks</title>
    <summary>  Recurrent neural networks are a powerful means to cope with time series. We
show that already linearly activated recurrent neural networks can approximate
any time-dependent function f(t) given by a number of function values. The
approximation can effectively be learned by simply solving a linear equation
system; no backpropagation or similar methods are needed. Furthermore the
network size can be reduced by taking only the most relevant components of the
network. Thus, in contrast to others, our approach not only learns network
weights but also the network architecture. The networks have interesting
properties: In the stationary case they end up in ellipse trajectories in the
long run, and they allow the prediction of further values and compact
representations of functions. We demonstrate this by several experiments, among
them multiple superimposed oscillators (MSO) and robotic soccer. Predictive
neural networks outperform the previous state-of-the-art for the MSO task with
a minimal number of units.
</summary>
    <author>
      <name>Frieder Stolzenburg</name>
    </author>
    <author>
      <name>Olivia Michael</name>
    </author>
    <author>
      <name>Oliver Obst</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 8 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.03308v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03308v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.7085v1</id>
    <updated>2012-05-31T19:38:08Z</updated>
    <published>2012-05-31T19:38:08Z</published>
    <title>Long-term memory stabilized by noise-induced rehearsal</title>
    <summary>  Cortical networks can maintain memories for decades despite the short
lifetime of synaptic strength. Can a neural network store long-lasting memories
in unstable synapses? Here, we study the effects of random noise on the
stability of memory stored in synapses of an attractor neural network. The
model includes ongoing spike timing dependent plasticity (STDP). We show that
certain classes of STDP rules can lead to the stabilization of memory patterns
stored in the network. The stabilization results from rehearsals induced by
noise. We show that unstructured neural noise, after passing through the
recurrent network weights, carries the imprint of all memory patterns in
temporal correlations. Under certain conditions, STDP combined with these
correlations, can lead to reinforcement of all existing patterns, even those
that are never explicitly visited. Thus, unstructured neural noise can
stabilize the existing structure of synaptic connectivity. Our findings may
provide the functional reason for highly irregular spiking displayed by
cortical neurons and provide justification for models of system memory
consolidation. Therefore, we propose that irregular neural activity is the
feature that helps cortical networks maintain stable connections.
</summary>
    <author>
      <name>Yi Wei</name>
    </author>
    <author>
      <name>Alexei A. Koulakov</name>
    </author>
    <link href="http://arxiv.org/abs/1205.7085v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.7085v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.3817v1</id>
    <updated>2011-07-19T19:38:51Z</updated>
    <published>2011-07-19T19:38:51Z</published>
    <title>Multiscale approach for bone remodeling simulation based on finite
  element and neural network computation</title>
    <summary>  The aim of this paper is to develop a multiscale hierarchical hybrid model
based on finite element analysis and neural network computation to link
mesoscopic scale (trabecular network level) and macroscopic (whole bone level)
to simulate bone remodelling process. Because whole bone simulation considering
the 3D trabecular level is time consuming, the finite element calculation is
performed at macroscopic level and a trained neural network are employed as
numerical devices for substituting the finite element code needed for the
mesoscale prediction. The bone mechanical properties are updated at macroscopic
scale depending on the morphological organization at the mesoscopic computed by
the trained neural network. The digital image-based modeling technique using
m-CT and voxel finite element mesh is used to capture 2 mm3 Representative
Volume Elements at mesoscale level in a femur head. The input data for the
artificial neural network are a set of bone material parameters, boundary
conditions and the applied stress. The output data is the updated bone
properties and some trabecular bone factors. The presented approach, to our
knowledge, is the first model incorporating both FE analysis and neural network
computation to simulate the multilevel bone adaptation in rapid way.
</summary>
    <author>
      <name>Ridha Hambli</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Prisme</arxiv:affiliation>
    </author>
    <author>
      <name>Abdelwahed Barkaoui</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Prisme</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference Multiscale Materials Modeling - MMM2010,
  Freiburg : Germany (2010)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1107.3817v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.3817v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.TO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.03626v1</id>
    <updated>2015-06-11T11:10:25Z</updated>
    <published>2015-06-11T11:10:25Z</published>
    <title>Margin-Based Feed-Forward Neural Network Classifiers</title>
    <summary>  Margin-Based Principle has been proposed for a long time, it has been proved
that this principle could reduce the structural risk and improve the
performance in both theoretical and practical aspects. Meanwhile, feed-forward
neural network is a traditional classifier, which is very hot at present with a
deeper architecture. However, the training algorithm of feed-forward neural
network is developed and generated from Widrow-Hoff Principle that means to
minimize the squared error. In this paper, we propose a new training algorithm
for feed-forward neural networks based on Margin-Based Principle, which could
effectively promote the accuracy and generalization ability of neural network
classifiers with less labelled samples and flexible network. We have conducted
experiments on four UCI open datasets and achieved good results as expected. In
conclusion, our model could handle more sparse labelled and more high-dimension
dataset in a high accuracy while modification from old ANN method to our method
is easy and almost free of work.
</summary>
    <author>
      <name>Han Xiao</name>
    </author>
    <author>
      <name>Xiaoyan Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been published in ICANN 2015: International Conference
  on Artificial Neural Networks, Amsterdam, The Netherlands, (May 14-15, 2015)</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.03626v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.03626v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04859v1</id>
    <updated>2016-05-16T18:12:45Z</updated>
    <published>2016-05-16T18:12:45Z</published>
    <title>Reducing the Model Order of Deep Neural Networks Using Information
  Theory</title>
    <summary>  Deep neural networks are typically represented by a much larger number of
parameters than shallow models, making them prohibitive for small footprint
devices. Recent research shows that there is considerable redundancy in the
parameter space of deep neural networks. In this paper, we propose a method to
compress deep neural networks by using the Fisher Information metric, which we
estimate through a stochastic optimization method that keeps track of
second-order information in the network. We first remove unimportant parameters
and then use non-uniform fixed point quantization to assign more bits to
parameters with higher Fisher Information estimates. We evaluate our method on
a classification task with a convolutional neural network trained on the MNIST
data set. Experimental results show that our method outperforms existing
methods for both network pruning and quantization.
</summary>
    <author>
      <name>Ming Tu</name>
    </author>
    <author>
      <name>Visar Berisha</name>
    </author>
    <author>
      <name>Yu Cao</name>
    </author>
    <author>
      <name>Jae-sun Seo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ISVLSI 2016 special session</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.04859v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04859v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07326v3</id>
    <updated>2016-07-03T09:39:30Z</updated>
    <published>2016-06-23T14:30:36Z</published>
    <title>DropNeuron: Simplifying the Structure of Deep Neural Networks</title>
    <summary>  Deep learning using multi-layer neural networks (NNs) architecture manifests
superb power in modern machine learning systems. The trained Deep Neural
Networks (DNNs) are typically large. The question we would like to address is
whether it is possible to simplify the NN during training process to achieve a
reasonable performance within an acceptable computational time. We presented a
novel approach of optimising a deep neural network through regularisation of
net- work architecture. We proposed regularisers which support a simple
mechanism of dropping neurons during a network training process. The method
supports the construction of a simpler deep neural networks with compatible
performance with its simplified version. As a proof of concept, we evaluate the
proposed method with examples including sparse linear regression, deep
autoencoder and convolutional neural network. The valuations demonstrate
excellent performance.
  The code for this work can be found in
http://www.github.com/panweihit/DropNeuron
</summary>
    <author>
      <name>Wei Pan</name>
    </author>
    <author>
      <name>Hao Dong</name>
    </author>
    <author>
      <name>Yike Guo</name>
    </author>
    <link href="http://arxiv.org/abs/1606.07326v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07326v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.03165v1</id>
    <updated>2016-10-11T02:48:13Z</updated>
    <published>2016-10-11T02:48:13Z</published>
    <title>Long Short-Term Memory based Convolutional Recurrent Neural Networks for
  Large Vocabulary Speech Recognition</title>
    <summary>  Long short-term memory (LSTM) recurrent neural networks (RNNs) have been
shown to give state-of-the-art performance on many speech recognition tasks, as
they are able to provide the learned dynamically changing contextual window of
all sequence history. On the other hand, the convolutional neural networks
(CNNs) have brought significant improvements to deep feed-forward neural
networks (FFNNs), as they are able to better reduce spectral variation in the
input signal. In this paper, a network architecture called as convolutional
recurrent neural network (CRNN) is proposed by combining the CNN and LSTM RNN.
In the proposed CRNNs, each speech frame, without adjacent context frames, is
organized as a number of local feature patches along the frequency axis, and
then a LSTM network is performed on each feature patch along the time axis. We
train and compare FFNNs, LSTM RNNs and the proposed LSTM CRNNs at various
number of configurations. Experimental results show that the LSTM CRNNs can
exceed state-of-the-art speech recognition performance.
</summary>
    <author>
      <name>Xiangang Li</name>
    </author>
    <author>
      <name>Xihong Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in INTERSPEECH 2015, September 6-10, 2015, Dresden, Germany</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.03165v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.03165v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.09463v1</id>
    <updated>2016-10-29T06:12:03Z</updated>
    <published>2016-10-29T06:12:03Z</published>
    <title>Sparse Signal Recovery for Binary Compressed Sensing by Majority Voting
  Neural Networks</title>
    <summary>  In this paper, we propose majority voting neural networks for sparse signal
recovery in binary compressed sensing. The majority voting neural network is
composed of several independently trained feedforward neural networks employing
the sigmoid function as an activation function. Our empirical study shows that
a choice of a loss function used in training processes for the network is of
prime importance. We found a loss function suitable for sparse signal recovery,
which includes a cross entropy-like term and an $L_1$ regularized term. From
the experimental results, we observed that the majority voting neural network
achieves excellent recovery performance, which is approaching the optimal
performance as the number of component nets grows. The simple architecture of
the majority voting neural networks would be beneficial for both software and
hardware implementations.
</summary>
    <author>
      <name>Daisuke Ito</name>
    </author>
    <author>
      <name>Tadashi Wadayama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.09463v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.09463v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.04959v1</id>
    <updated>2017-04-17T13:23:36Z</updated>
    <published>2017-04-17T13:23:36Z</published>
    <title>Introspection: Accelerating Neural Network Training By Learning Weight
  Evolution</title>
    <summary>  Neural Networks are function approximators that have achieved
state-of-the-art accuracy in numerous machine learning tasks. In spite of their
great success in terms of accuracy, their large training time makes it
difficult to use them for various tasks. In this paper, we explore the idea of
learning weight evolution pattern from a simple network for accelerating
training of novel neural networks. We use a neural network to learn the
training pattern from MNIST classification and utilize it to accelerate
training of neural networks used for CIFAR-10 and ImageNet classification. Our
method has a low memory footprint and is computationally efficient. This method
can also be used with other optimizers to give faster convergence. The results
indicate a general trend in the weight evolution during training of neural
networks.
</summary>
    <author>
      <name>Abhishek Sinha</name>
    </author>
    <author>
      <name>Mausoom Sarkar</name>
    </author>
    <author>
      <name>Aahitagni Mukherjee</name>
    </author>
    <author>
      <name>Balaji Krishnamurthy</name>
    </author>
    <link href="http://arxiv.org/abs/1704.04959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.04959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.09401v3</id>
    <updated>2018-01-19T23:13:36Z</updated>
    <published>2017-08-30T18:00:51Z</published>
    <title>Machine Learning Topological Invariants with Neural Networks</title>
    <summary>  In this Letter we supervisedly train neural networks to distinguish different
topological phases in the context of topological band insulators. After
training with Hamiltonians of one-dimensional insulators with chiral symmetry,
the neural network can predict their topological winding numbers with nearly
100% accuracy, even for Hamiltonians with larger winding numbers that are not
included in the training data. These results show a remarkable success that the
neural network can capture the global and nonlinear topological features of
quantum phases from local inputs. By opening up the neural network, we confirm
that the network does learn the discrete version of the winding number formula.
We also make a couple of remarks regarding the role of the symmetry and the
opposite effect of regularization techniques when applying machine learning to
physical systems.
</summary>
    <author>
      <name>Pengfei Zhang</name>
    </author>
    <author>
      <name>Huitao Shen</name>
    </author>
    <author>
      <name>Hui Zhai</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.120.066401</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.120.066401" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures and 1 table + 2 pages of supplemental material</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. Lett. 120, 066401 (2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1708.09401v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.09401v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.mes-hall" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mes-hall" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.str-el" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.08862v1</id>
    <updated>2017-12-24T00:27:09Z</updated>
    <published>2017-12-24T00:27:09Z</published>
    <title>Neural Network Multitask Learning for Traffic Flow Forecasting</title>
    <summary>  Traditional neural network approaches for traffic flow forecasting are
usually single task learning (STL) models, which do not take advantage of the
information provided by related tasks. In contrast to STL, multitask learning
(MTL) has the potential to improve generalization by transferring information
in training signals of extra tasks. In this paper, MTL based neural networks
are used for traffic flow forecasting. For neural network MTL, a
backpropagation (BP) network is constructed by incorporating traffic flows at
several contiguous time instants into an output layer. Nodes in the output
layer can be seen as outputs of different but closely related STL tasks.
Comprehensive experiments on urban vehicular traffic flow data and comparisons
with STL show that MTL in BP neural networks is a promising and effective
approach for traffic flow forecasting.
</summary>
    <author>
      <name>Feng Jin</name>
    </author>
    <author>
      <name>Shiliang Sun</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Joint Conference on Neural
  Networks (IJCNN), 2008. pp. 1898-1902</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1712.08862v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.08862v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.09327v1</id>
    <updated>2017-12-26T18:52:41Z</updated>
    <published>2017-12-26T18:52:41Z</published>
    <title>Building Robust Deep Neural Networks for Road Sign Detection</title>
    <summary>  Deep Neural Networks are built to generalize outside of training set in mind
by using techniques such as regularization, early stopping and dropout. But
considerations to make them more resilient to adversarial examples are rarely
taken. As deep neural networks become more prevalent in mission-critical and
real-time systems, miscreants start to attack them by intentionally making deep
neural networks to misclassify an object of one type to be seen as another
type. This can be catastrophic in some scenarios where the classification of a
deep neural network can lead to a fatal decision by a machine. In this work, we
used GTSRB dataset to craft adversarial samples by Fast Gradient Sign Method
and Jacobian Saliency Method, used those crafted adversarial samples to attack
another Deep Convolutional Neural Network and built the attacked network to be
more resilient against adversarial attacks by making it more robust by
Defensive Distillation and Adversarial Training
</summary>
    <author>
      <name>Arkar Min Aung</name>
    </author>
    <author>
      <name>Yousef Fadila</name>
    </author>
    <author>
      <name>Radian Gondokaryono</name>
    </author>
    <author>
      <name>Luis Gonzalez</name>
    </author>
    <link href="http://arxiv.org/abs/1712.09327v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.09327v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00614v2</id>
    <updated>2018-02-07T08:02:59Z</updated>
    <published>2018-02-02T09:39:40Z</published>
    <title>Visual Interpretability for Deep Learning: a Survey</title>
    <summary>  This paper reviews recent studies in understanding neural-network
representations and learning neural networks with interpretable/disentangled
middle-layer representations. Although deep neural networks have exhibited
superior performance in various tasks, the interpretability is always the
Achilles' heel of deep neural networks. At present, deep neural networks obtain
high discrimination power at the cost of low interpretability of their
black-box representations. We believe that high model interpretability may help
people to break several bottlenecks of deep learning, e.g., learning from very
few annotations, learning via human-computer communications at the semantic
level, and semantically debugging network representations. We focus on
convolutional neural networks (CNNs), and we revisit the visualization of CNN
representations, methods of diagnosing representations of pre-trained CNNs,
approaches for disentangling pre-trained CNN representations, learning of CNNs
with disentangled representations, and middle-to-end learning based on model
interpretability. Finally, we discuss prospective trends in explainable
artificial intelligence.
</summary>
    <author>
      <name>Quanshi Zhang</name>
    </author>
    <author>
      <name>Song-Chun Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/1802.00614v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00614v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00891v1</id>
    <updated>2018-02-03T01:42:32Z</updated>
    <published>2018-02-03T01:42:32Z</published>
    <title>Joint Binary Neural Network for Multi-label Learning with Applications
  to Emotion Classification</title>
    <summary>  Recently the deep learning techniques have achieved success in multi-label
classification due to its automatic representation learning ability and the
end-to-end learning framework. Existing deep neural networks in multi-label
classification can be divided into two kinds: binary relevance neural network
(BRNN) and threshold dependent neural network (TDNN). However, the former needs
to train a set of isolate binary networks which ignore dependencies between
labels and have heavy computational load, while the latter needs an additional
threshold function mechanism to transform the multi-class probabilities to
multi-label outputs. In this paper, we propose a joint binary neural network
(JBNN), to address these shortcomings. In JBNN, the representation of the text
is fed to a set of logistic functions instead of a softmax function, and the
multiple binary classifications are carried out synchronously in one neural
network framework. Moreover, the relations between labels are captured via
training on a joint binary cross entropy (JBCE) loss. To better meet
multi-label emotion classification, we further proposed to incorporate the
prior label relations into the JBCE loss. The experimental results on the
benchmark dataset show that our model performs significantly better than the
state-of-the-art multi-label emotion classification methods, in both
classification performance and computational efficiency.
</summary>
    <author>
      <name>Huihui He</name>
    </author>
    <author>
      <name>Rui Xia</name>
    </author>
    <link href="http://arxiv.org/abs/1802.00891v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00891v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0511235v1</id>
    <updated>2005-11-09T17:02:37Z</updated>
    <published>2005-11-09T17:02:37Z</published>
    <title>Feed-forward chains of recurrent attractor neural networks with finite
  dilution near saturation</title>
    <summary>  A stationary state replica analysis for a dual neural network model that
interpolates between a fully recurrent symmetric attractor network and a
strictly feed-forward layered network, studied by Coolen and Viana, is extended
in this work to account for finite dilution of the recurrent Hebbian
interactions between binary Ising units within each layer. Gradual dilution is
found to suppress part of the phase transitions that arise from the competition
between recurrent and feed-forward operation modes of the network. Despite
that, a long chain of layers still exhibits a relatively good performance under
finite dilution for a balanced ratio between inter-layer and intra-layer
interactions.
</summary>
    <author>
      <name>F. L. Metz</name>
    </author>
    <author>
      <name>W. K. Theumann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physa.2005.11.049</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physa.2005.11.049" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Physica A</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0511235v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0511235v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/physics/0402076v2</id>
    <updated>2005-01-06T19:37:23Z</updated>
    <published>2004-02-16T21:31:09Z</published>
    <title>Fastest learning in small world neural networks</title>
    <summary>  We investigate supervised learning in neural networks. We consider a
multi-layered feed-forward network with back propagation. We find that the
network of small-world connectivity reduces the learning error and learning
time when compared to the networks of regular or random connectivity. Our study
has potential applications in the domain of data-mining, image processing,
speech recognition, and pattern recognition.
</summary>
    <author>
      <name>D. Simard</name>
    </author>
    <author>
      <name>L. Nadeau</name>
    </author>
    <author>
      <name>H. Kr√∂ger</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physleta.2004.12.078</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physleta.2004.12.078" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Text completely revised (14 pages), all new figures (7 figs)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Lett. A336 (2005) 8-15.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/physics/0402076v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/physics/0402076v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.3679v2</id>
    <updated>2008-01-31T04:18:42Z</updated>
    <published>2007-05-24T23:55:34Z</published>
    <title>Transient dynamics for sequence processing neural networks: effect of
  degree distributions</title>
    <summary>  We derive a analytic evolution equation for overlap parameters including the
effect of degree distribution on the transient dynamics of sequence processing
neural networks. In the special case of globally coupled networks, the
precisely retrieved critical loading ratio $\alpha_c = N ^{-1/2}$ is obtained,
where $N$ is the network size. In the presence of random networks, our
theoretical predictions agree quantitatively with the numerical experiments for
delta, binomial, and power-law degree distributions.
</summary>
    <author>
      <name>Yong Chen</name>
    </author>
    <author>
      <name>Pan Zhang</name>
    </author>
    <author>
      <name>Lianchun Yu</name>
    </author>
    <author>
      <name>Shengli Zhang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.77.016110</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.77.016110" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 77, 016110 (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0705.3679v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.3679v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.0060v1</id>
    <updated>2010-02-27T04:38:43Z</updated>
    <published>2010-02-27T04:38:43Z</published>
    <title>Comment on "Fastest learning in small-world neural networks"</title>
    <summary>  This comment reexamines Simard et al.'s work in [D. Simard, L. Nadeau, H.
Kroger, Phys. Lett. A 336 (2005) 8-15]. We found that Simard et al. calculated
mistakenly the local connectivity lengths Dlocal of networks. The right results
of Dlocal are presented and the supervised learning performance of feedforward
neural networks (FNNs) with different rewirings are re-investigated in this
comment. This comment discredits Simard et al's work by two conclusions: 1)
Rewiring connections of FNNs cannot generate networks with small-world
connectivity; 2) For different training sets, there do not exist networks with
a certain number of rewirings generating reduced learning errors than networks
with other numbers of rewiring.
</summary>
    <author>
      <name>Z. X. Guo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1003.0060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.0060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.8104v1</id>
    <updated>2013-07-30T19:51:12Z</updated>
    <published>2013-07-30T19:51:12Z</published>
    <title>Neural Network Capacity for Multilevel Inputs</title>
    <summary>  This paper examines the memory capacity of generalized neural networks.
Hopfield networks trained with a variety of learning techniques are
investigated for their capacity both for binary and non-binary alphabets. It is
shown that the capacity can be much increased when multilevel inputs are used.
New learning strategies are proposed to increase Hopfield network capacity, and
the scalability of these methods is also examined in respect to size of the
network. The ability to recall entire patterns from stimulation of a single
neuron is examined for the increased capacity networks.
</summary>
    <author>
      <name>Matt Stowe</name>
    </author>
    <author>
      <name>Subhash Kak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages,17 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.8104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.8104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.2297v1</id>
    <updated>2014-07-08T23:30:19Z</updated>
    <published>2014-07-08T23:30:19Z</published>
    <title>Transition to chaos in random networks with cell-type-specific
  connectivity</title>
    <summary>  In neural circuits, statistical connectivity rules strongly depend on
neuronal type. Here we study dynamics of neural networks with cell-type
specific connectivity by extending the dynamic mean field method, and find that
these networks exhibit a phase transition between silent and chaotic activity.
By analyzing the locus of this transition, we derive a new result in random
matrix theory: the spectral radius of a random connectivity matrix with
block-structured variances. We apply our results to show how a small group of
hyper-excitable neurons within the network can significantly increase the
network's computational capacity.
</summary>
    <author>
      <name>Johnatan Aljadeff</name>
    </author>
    <author>
      <name>Merav Stern</name>
    </author>
    <author>
      <name>Tatyana O. Sharpee</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.114.088101</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.114.088101" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. Lett. 114, 088101 (2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1407.2297v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.2297v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07866v1</id>
    <updated>2016-03-25T10:27:00Z</updated>
    <published>2016-03-25T10:27:00Z</published>
    <title>The Asymptotic Performance of Linear Echo State Neural Networks</title>
    <summary>  In this article, a study of the mean-square error (MSE) performance of linear
echo-state neural networks is performed, both for training and testing tasks.
Considering the realistic setting of noise present at the network nodes, we
derive deterministic equivalents for the aforementioned MSE in the limit where
the number of input data $T$ and network size $n$ both grow large. Specializing
then the network connectivity matrix to specific random settings, we further
obtain simple formulas that provide new insights on the performance of such
networks.
</summary>
    <author>
      <name>Romain Couillet</name>
    </author>
    <author>
      <name>Gilles Wainrib</name>
    </author>
    <author>
      <name>Harry Sevi</name>
    </author>
    <author>
      <name>Hafiz Tiomoko Ali</name>
    </author>
    <link href="http://arxiv.org/abs/1603.07866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04428v2</id>
    <updated>2016-07-14T15:18:56Z</updated>
    <published>2016-04-15T11:07:45Z</published>
    <title>The Artificial Mind's Eye: Resisting Adversarials for Convolutional
  Neural Networks using Internal Projection</title>
    <summary>  We introduce a novel artificial neural network architecture that integrates
robustness to adversarial input in the network structure. The main idea of our
approach is to force the network to make predictions on what the given instance
of the class under consideration would look like and subsequently test those
predictions. By forcing the network to redraw the relevant parts of the image
and subsequently comparing this new image to the original, we are having the
network give a "proof" of the presence of the object.
</summary>
    <author>
      <name>Harm Berntsen</name>
    </author>
    <author>
      <name>Wouter Kuijper</name>
    </author>
    <author>
      <name>Tom Heskes</name>
    </author>
    <link href="http://arxiv.org/abs/1604.04428v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04428v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.06217v2</id>
    <updated>2017-09-12T22:14:36Z</updated>
    <published>2017-03-17T23:52:14Z</published>
    <title>Deciding How to Decide: Dynamic Routing in Artificial Neural Networks</title>
    <summary>  We propose and systematically evaluate three strategies for training
dynamically-routed artificial neural networks: graphs of learned
transformations through which different input signals may take different paths.
Though some approaches have advantages over others, the resulting networks are
often qualitatively similar. We find that, in dynamically-routed networks
trained to classify images, layers and branches become specialized to process
distinct categories of images. Additionally, given a fixed computational
budget, dynamically-routed networks tend to perform better than comparable
statically-routed networks.
</summary>
    <author>
      <name>Mason McGill</name>
    </author>
    <author>
      <name>Pietro Perona</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2017. Code at https://github.com/MasonMcGill/multipath-nn Video
  abstract at https://youtu.be/NHQsDaycwyQ</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.06217v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.06217v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0204054v1</id>
    <updated>2002-04-02T13:17:58Z</updated>
    <published>2002-04-02T13:17:58Z</published>
    <title>Theory of Interacting Neural Networks</title>
    <summary>  In this contribution we give an overview over recent work on the theory of
interacting neural networks. The model is defined in Section 2. The typical
teacher/student scenario is considered in Section 3. A static teacher network
is presenting training examples for an adaptive student network. In the case of
multilayer networks, the student shows a transition from a symmetric state to
specialisation. Neural networks can also generate a time series. Training on
time series and predicting it are studied in Section 4. When a network is
trained on its own output, it is interacting with itself. Such a scenario has
implications on the theory of prediction algorithms, as discussed in Section 5.
When a system of networks is trained on its minority decisions, it may be
considered as a model for competition in closed markets, see Section 6. In
Section 7 we consider two mutually interacting networks. A novel phenomenon is
observed: synchronisation by mutual learning. In Section 8 it is shown, how
this phenomenon can be applied to cryptography: Generation of a secret key over
a public channel.
</summary>
    <author>
      <name>Wolfgang Kinzel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Contribution to Networks, ed. by H.G. Schuster and S. Bornholdt, to
  be published by Wiley VCH</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0204054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0204054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0702055v1</id>
    <updated>2007-02-09T13:16:14Z</updated>
    <published>2007-02-09T13:16:14Z</published>
    <title>On the possibility of making the complete computer model of a human
  brain</title>
    <summary>  The development of the algorithm of a neural network building by the
corresponding parts of a DNA code is discussed.
</summary>
    <author>
      <name>A. V. Paraskevov</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0702055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0702055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.3161v1</id>
    <updated>2011-10-14T09:42:06Z</updated>
    <published>2011-10-14T09:42:06Z</published>
    <title>Intrinsic adaptation in autonomous recurrent neural networks</title>
    <summary>  A massively recurrent neural network responds on one side to input stimuli
and is autonomously active, on the other side, in the absence of sensory
inputs. Stimuli and information processing depends crucially on the qualia of
the autonomous-state dynamics of the ongoing neural activity. This default
neural activity may be dynamically structured in time and space, showing
regular, synchronized, bursting or chaotic activity patterns.
  We study the influence of non-synaptic plasticity on the default dynamical
state of recurrent neural networks. The non-synaptic adaption considered acts
on intrinsic neural parameters, such as the threshold and the gain, and is
driven by the optimization of the information entropy. We observe, in the
presence of the intrinsic adaptation processes, three distinct and globally
attracting dynamical regimes, a regular synchronized, an overall chaotic and an
intermittent bursting regime. The intermittent bursting regime is characterized
by intervals of regular flows, which are quite insensitive to external stimuli,
interseeded by chaotic bursts which respond sensitively to input signals. We
discuss these finding in the context of self-organized information processing
and critical brain dynamics.
</summary>
    <author>
      <name>Dimitrije Markovic</name>
    </author>
    <author>
      <name>Claudius Gros</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/NECO_a_00232</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/NECO_a_00232" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 8 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Computation February 2012, Vol. 24, No. 2: 523-540</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1110.3161v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.3161v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.7136v1</id>
    <updated>2014-02-28T05:34:36Z</updated>
    <published>2014-02-28T05:34:36Z</published>
    <title>Neural Network Approach to Railway Stand Lateral Skew Control</title>
    <summary>  The paper presents a study of an adaptive approach to lateral skew control
for an experimental railway stand. The preliminary experiments with the real
experimental railway stand and simulations with its 3-D mechanical model,
indicates difficulties of model-based control of the device. Thus, use of
neural networks for identification and control of lateral skew shall be
investigated. This paper focuses on real-data based modeling of the railway
stand by various neural network models, i.e; linear neural unit and quadratic
neural unit architectures. Furthermore, training methods of these neural
architectures as such, real-time-recurrent-learning and a variation of
back-propagation-through-time are examined, accompanied by a discussion of the
produced experimental results.
</summary>
    <author>
      <name>Peter Mark Benes</name>
    </author>
    <author>
      <name>Ivo Bukovsky</name>
    </author>
    <author>
      <name>Matous Cejnek</name>
    </author>
    <author>
      <name>Jan Kalivoda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">P. M. Benes et al., "Neural Network Approach to Railway Stand Lateral
  Skew Control" in Computer Science &amp; Information Technology (CS&amp; IT), Sydney,
  NSW, Australia, AIRCC, 2014, pp. 327-339</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.7136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.7136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.06046v1</id>
    <updated>2018-01-18T14:40:27Z</updated>
    <published>2018-01-18T14:40:27Z</published>
    <title>Conditions for traveling waves in spiking neural networks</title>
    <summary>  Spatiotemporal patterns such as traveling waves are frequently observed in
recordings of neural activity. The mechanisms underlying the generation of such
patterns are largely unknown. Previous studies have investigated the existence
and uniqueness of different types of waves or bumps of activity using
neural-field models, phenomenological coarse-grained descriptions of
neural-network dynamics. But it remains unclear how these insights can be
transferred to more biologically realistic networks of spiking neurons, where
individual neurons fire irregularly. Here, we employ mean-field theory to
reduce a microscopic model of leaky integrate-and-fire (LIF) neurons with
distance-dependent connectivity to an effective neural-field model. In contrast
to existing phenomenological descriptions, the dynamics in this neural-field
model depends on the mean and the variance in the synaptic input, both
determining the amplitude and the temporal structure of the resulting effective
coupling kernel. For the neural-field model we derive conditions for the
existence of spatial and temporal oscillations and periodic traveling waves
using linear stability analysis. We first prove that periodic traveling waves
cannot occur in a single homogeneous population of neurons, irrespective of the
form of distance dependence of the connection probability. Compatible with the
architecture of cortical neural networks, traveling waves emerge in
two-population networks of excitatory and inhibitory neurons as a combination
of delay-induced temporal oscillations and spatial oscillations due to
distance-dependent connectivity profiles. Finally, we demonstrate quantitative
agreement between predictions of the analytically tractable neural-field model
and numerical simulations of both networks of nonlinear rate-based units and
networks of LIF neurons.
</summary>
    <author>
      <name>Johanna Senk</name>
    </author>
    <author>
      <name>Karol√≠na Korvasov√°</name>
    </author>
    <author>
      <name>Jannis Schuecker</name>
    </author>
    <author>
      <name>Espen Hagen</name>
    </author>
    <author>
      <name>Tom Tetzlaff</name>
    </author>
    <author>
      <name>Markus Diesmann</name>
    </author>
    <author>
      <name>Moritz Helias</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.06046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.06046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/adap-org/9411003v1</id>
    <updated>1994-11-30T11:22:32Z</updated>
    <published>1994-11-30T11:22:32Z</published>
    <title>An Evolutionary Approach to Associative Memory in Recurrent Neural
  Networks</title>
    <summary>  In this paper, we investigate the associative memory in recurrent neural
networks, based on the model of evolving neural networks proposed by Nolfi,
Miglino and Parisi. Experimentally developed network has highly asymmetric
synaptic weights and dilute connections, quite different from those of the
Hopfield model. Some results on the effect of learning efficiency on the
evolution are also presented.
</summary>
    <author>
      <name>Sh. Fujita</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Kobe University</arxiv:affiliation>
    </author>
    <author>
      <name>H. Nishimura</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Hyogo University of Education</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, compressed and uuencoded postscript file</arxiv:comment>
    <link href="http://arxiv.org/abs/adap-org/9411003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/adap-org/9411003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="adap-org" scheme="http://arxiv.org/schemas/atom"/>
    <category term="adap-org" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9712132v1</id>
    <updated>1997-12-12T08:05:55Z</updated>
    <published>1997-12-12T08:05:55Z</published>
    <title>Threshold Noise as a Source of Volatility in Random Synchronous
  Asymmetric Neural Networks</title>
    <summary>  We study the diversity of complex spatio-temporal patterns of random
synchronous asymmetric neural networks (RSANNs). Specifically, we investigate
the impact of noisy thresholds on network performance and find that there is a
narrow and interesting region of noise parameters where RSANNs display specific
features of behavior desired for rapidly `thinking' systems: accessibility to a
large set of distinct, complex patterns.
</summary>
    <author>
      <name>Henrik Bohr</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Arizona</arxiv:affiliation>
    </author>
    <author>
      <name>Patrick McGuire</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Arizona</arxiv:affiliation>
    </author>
    <author>
      <name>Chris Pershing</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Arizona</arxiv:affiliation>
    </author>
    <author>
      <name>Johann Rafelski</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Arizona</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 11 figures, submitted to Neural Computation</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9712132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9712132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="adap-org" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9803386v1</id>
    <updated>1998-03-31T15:19:11Z</updated>
    <published>1998-03-31T15:19:11Z</published>
    <title>Finite Size Effects in Separable Recurrent Neural Networks</title>
    <summary>  We perform a systematic analytical study of finite size effects in separable
recurrent neural network models with sequential dynamics, away from saturation.
We find two types of finite size effects: thermal fluctuations, and
disorder-induced `frozen' corrections to the mean-field laws. The finite size
effects are described by equations that correspond to a time-dependent
Ornstein-Uhlenbeck process. We show how the theory can be used to understand
and quantify various finite size phenomena in recurrent neural networks, with
and without detailed balance.
</summary>
    <author>
      <name>A. Castellanos</name>
    </author>
    <author>
      <name>A. C. C. Coolen</name>
    </author>
    <author>
      <name>L. Viana</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/0305-4470/31/31/009</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/0305-4470/31/31/009" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages LaTex, with 4 postscript figures included</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9803386v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9803386v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0103092v1</id>
    <updated>2001-03-05T04:34:04Z</updated>
    <published>2001-03-05T04:34:04Z</published>
    <title>Reconstructing signal from fiber-optic measuring system with non-linear
  perceptron</title>
    <summary>  A computer model of the feed-forward neural network with the hidden layer is
developed to reconstruct physical field investigated by the fiber-optic
measuring system. The Gaussian distributions of some physical quantity are
selected as learning patterns. Neural network is learned by error
back-propagation using the conjugate gradient and coordinate descent
minimization of deviation. Learned neural network reconstructs the
two-dimensional scalar physical field with distribution having one or two
Gaussian peaks.
</summary>
    <author>
      <name>A. V. Panov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, Latex, 1 postscript figure</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0103092v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0103092v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0103585v2</id>
    <updated>2001-04-10T16:25:45Z</updated>
    <published>2001-03-28T15:33:40Z</published>
    <title>On the equivalence of the Ashkin-Teller and the four-state Potts-glass
  models of neural networks</title>
    <summary>  We show that for a particular choice of the coupling parameters the
Ashkin-Teller spin-glass neural network model with the Hebb learning rule and
one condensed pattern yields the same thermodynamic properties as the
four-state anisotropic Potts-glass neural network model. This equivalence is
not seen at the level of the Hamiltonians.
</summary>
    <author>
      <name>D. Bolle'</name>
    </author>
    <author>
      <name>P. Kozlowski</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.64.067102</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.64.067102" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, revtex, additional arguments presented</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 64, 067102 (2001)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0103585v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0103585v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0412023v1</id>
    <updated>2004-12-06T20:23:15Z</updated>
    <published>2004-12-06T20:23:15Z</published>
    <title>Multidimensional data classification with artificial neural networks</title>
    <summary>  Multi-dimensional data classification is an important and challenging problem
in many astro-particle experiments. Neural networks have proved to be versatile
and robust in multi-dimensional data classification. In this article we shall
study the classification of gamma from the hadrons for the MAGIC Experiment.
Two neural networks have been used for the classification task. One is
Multi-Layer Perceptron based on supervised learning and other is
Self-Organising Map (SOM), which is based on unsupervised learning technique.
The results have been shown and the possible ways of combining these networks
have been proposed to yield better and faster classification results.
</summary>
    <author>
      <name>P. Boinee</name>
    </author>
    <author>
      <name>F. Barbarino</name>
    </author>
    <author>
      <name>A. De Angelis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures, Submitted to EURASIP Journal on Applied Signal
  Processing, 2004</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0412023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0412023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; K.3.2; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/hep-ph/9607460v1</id>
    <updated>1996-07-29T16:07:20Z</updated>
    <published>1996-07-29T16:07:20Z</published>
    <title>Neural network analysis for gamma gamma -&gt; 3pi at Daphne</title>
    <summary>  We consider the possibility of using neural networks in experimental data
analysis in Daphne. We analyze the process $\gamma\gamma\to \pi^+ \pi^- \pi^0$
and its backgrounds using neural networks and we compare their performances
with traditional methods of applying cuts on several kinematical variables. We
find that the neural networks are more efficient and can be of great help for
processes with small number of produced events.
</summary>
    <author>
      <name>Ll. Ametller</name>
    </author>
    <author>
      <name>Ll. Garrido</name>
    </author>
    <author>
      <name>P. Talavera</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/S0370-2693(97)00124-X</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/S0370-2693(97)00124-X" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, latex, 2 figures.</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys.Lett. B396 (1997) 280-286</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/hep-ph/9607460v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/hep-ph/9607460v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/hep-ph/0607122v1</id>
    <updated>2006-07-11T17:16:35Z</updated>
    <published>2006-07-11T17:16:35Z</published>
    <title>The neural network approach to parton distribution functions</title>
    <summary>  We introduce the neural network approach to the parametrization of parton
distributions. After a general introduction, we present in detail our approach
to parametrize experimental data, based on a combination of Monte Carlo methods
and neural networks. We apply this strategy first in three different cases: the
proton structure function, hadronic tau decays and B meson decay spectra.
Finally we describe the neural network approach applied to the parametrization
of parton distribution functions, and present results on the nonsinglet parton
distribution.
</summary>
    <author>
      <name>Joan Rojo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Ph. D. Thesis, 163 pages, version with higher resolution figures
  available from the following website:
  http://www.ecm.ub.es/~joanrojo/thesis.pdf</arxiv:comment>
    <link href="http://arxiv.org/abs/hep-ph/0607122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/hep-ph/0607122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/nlin/0003003v1</id>
    <updated>2000-03-01T11:38:02Z</updated>
    <published>2000-03-01T11:38:02Z</published>
    <title>Coherent Response in a Chaotic Neural Network</title>
    <summary>  We set up a signal-driven scheme of the chaotic neural network with the
coupling constants corresponding to certain information, and investigate the
stochastic resonance-like effects under its deterministic dynamics, comparing
with the conventional case of Hopfield network with stochastic noise. It is
shown that the chaotic neural network can enhance weak subthreshold signals and
have higher coherence abilities between stimulus and response than those
attained by the conventional stochastic model.
</summary>
    <author>
      <name>Haruhiko Nishimura</name>
    </author>
    <author>
      <name>Naofumi Katada</name>
    </author>
    <author>
      <name>Kazuyuki Aihara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures, to be published in Neural Processing Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/nlin/0003003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/nlin/0003003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/nlin/0609014v1</id>
    <updated>2006-09-06T10:39:30Z</updated>
    <published>2006-09-06T10:39:30Z</published>
    <title>Multilayered feed forward Artificial Neural Network model to predict the
  average summer-monsoon rainfall in India</title>
    <summary>  In the present research, possibility of predicting average summer-monsoon
rainfall over India has been analyzed through Artificial Neural Network models.
In formulating the Artificial Neural Network based predictive model, three
layered networks have been constructed with sigmoid non-linearity. The models
under study are different in the number of hidden neurons. After a thorough
training and test procedure, neural net with three nodes in the hidden layer is
found to be the best predictive model.
</summary>
    <author>
      <name>Surajit Chattopadhyay</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2478/s11600-007-0020-8</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2478/s11600-007-0020-8" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 1 table, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/nlin/0609014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/nlin/0609014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/physics/9712035v1</id>
    <updated>1997-12-17T17:12:29Z</updated>
    <published>1997-12-17T17:12:29Z</published>
    <title>Characteristic functions and process identification by neural networks</title>
    <summary>  Principal component analysis (PCA) algorithms use neural networks to extract
the eigenvectors of the correlation matrix from the data. However, if the
process is non-Gaussian, PCA algorithms or their higher order generalisations
provide only incomplete or misleading information on the statistical properties
of the data. To handle such situations we propose neural network algorithms,
with an hybrid (supervised and unsupervised) learning scheme, which constructs
the characteristic function of the probability distribution and the transition
functions of the stochastic process. Illustrative examples are presented, which
include Cauchy and Levy-type processes
</summary>
    <author>
      <name>Joaquim A. Dente</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Laboratorio de Mecatronica, DEEC, IST, Lisboa, Portugal</arxiv:affiliation>
    </author>
    <author>
      <name>R. Vilela Mendes</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Laboratorio de Mecatronica, DEEC, IST, Lisboa, Portugal</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages Latex, 12 figures in a combined ps-file</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Networks,10 (1997) 1465-1471</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/physics/9712035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/physics/9712035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/physics/0703041v1</id>
    <updated>2007-03-05T07:18:44Z</updated>
    <published>2007-03-05T07:18:44Z</published>
    <title>A Hardware Implementation of Artificial Neural Network Using Field
  Programmable Gate Arrays</title>
    <summary>  An artificial neural network algorithm is implemented using a field
programmable gate array hardware. One hidden layer is used in the feed-forward
neural network structure in order to discriminate one class of patterns from
the other class in real time. With five 8-bit input patterns, six hidden nodes,
and one 8-bit output, the implemented hardware neural network makes decision on
a set of input patterns in 11 clocks and the result is identical to what to
expect from off-line computation. This implementation may be used in level 1
hardware triggers in high energy physics experiments
</summary>
    <author>
      <name>E. Won</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.nima.2007.08.163</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.nima.2007.08.163" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 4 figures, submitted to Nucl. Instr. Meth. A</arxiv:comment>
    <link href="http://arxiv.org/abs/physics/0703041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/physics/0703041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/quant-ph/0408148v2</id>
    <updated>2004-08-26T01:33:47Z</updated>
    <published>2004-08-23T22:55:59Z</published>
    <title>Indispensable Role of Quantum Theory in the Brain Dynamics</title>
    <summary>  Recently, Tegmark pointed out that the superposition of ion states involved
in the superposition of firing and resting states of a neuron quickly decohere.
It undoubtedly indicates that neural networks cannot work as quantum computers,
or computers taking advantage of coherent states. Does it also mean that the
brain can be modeled as a neural network obeying classical physics? Here we
show that it does not mean that the brain can be modeled as a neural network
obeying classical physics. A brand new perspective in research of neural
networks from quantum theoretical aspect is presented.
</summary>
    <author>
      <name>Yukinari Kurita</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15pages</arxiv:comment>
    <link href="http://arxiv.org/abs/quant-ph/0408148v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/quant-ph/0408148v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.1214v1</id>
    <updated>2007-05-09T07:08:58Z</updated>
    <published>2007-05-09T07:08:58Z</published>
    <title>Control of Complex Systems Using Bayesian Networks and Genetic Algorithm</title>
    <summary>  A method based on Bayesian neural networks and genetic algorithm is proposed
to control the fermentation process. The relationship between input and output
variables is modelled using Bayesian neural network that is trained using
hybrid Monte Carlo method. A feedback loop based on genetic algorithm is used
to change input variables so that the output variables are as close to the
desired target as possible without the loss of confidence level on the
prediction that the neural network gives. The proposed procedure is found to
reduce the distance between the desired target and measured outputs
significantly.
</summary>
    <author>
      <name>Tshilidzi Marwala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0705.1214v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.1214v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.2265v1</id>
    <updated>2010-02-11T03:35:38Z</updated>
    <published>2010-02-11T03:35:38Z</published>
    <title>Sequential optimizing investing strategy with neural networks</title>
    <summary>  In this paper we propose an investing strategy based on neural network models
combined with ideas from game-theoretic probability of Shafer and Vovk. Our
proposed strategy uses parameter values of a neural network with the best
performance until the previous round (trading day) for deciding the investment
in the current round. We compare performance of our proposed strategy with
various strategies including a strategy based on supervised neural network
models and show that our procedure is competitive with other strategies.
</summary>
    <author>
      <name>Ryo Adachi</name>
    </author>
    <author>
      <name>Akimichi Takemura</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Expert Systems with Applications 38 (2011) 12991-12998</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1002.2265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.2265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.2406v1</id>
    <updated>2010-09-13T14:35:41Z</updated>
    <published>2010-09-13T14:35:41Z</published>
    <title>Adaptation of the neural network-based IDS to new attacks detection</title>
    <summary>  In this paper we report our experiment concerning new attacks detection by a
neural network-based Intrusion Detection System. What is crucial for this topic
is the adaptation of the neural network that is already in use to correct
classification of a new "normal traffic" and of an attack representation not
presented during the network training process. When it comes to the new attack
it should also be easy to obtain vectors to test and to retrain the neural
classifier. We describe the proposal of an algorithm and a distributed IDS
architecture that could achieve the goals mentioned above.
</summary>
    <author>
      <name>Przemyslaw Kukielka</name>
    </author>
    <author>
      <name>Zbigniew Kotulski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1009.2406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.2406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.0265v1</id>
    <updated>2014-07-01T14:50:36Z</updated>
    <published>2014-07-01T14:50:36Z</published>
    <title>Supervised learning in Spiking Neural Networks with Limited Precision:
  SNN/LP</title>
    <summary>  A new supervised learning algorithm, SNN/LP, is proposed for Spiking Neural
Networks. This novel algorithm uses limited precision for both synaptic weights
and synaptic delays; 3 bits in each case. Also a genetic algorithm is used for
the supervised training. The results are comparable or better than previously
published work. The results are applicable to the realization of large scale
hardware neural networks. One of the trained networks is implemented in
programmable hardware.
</summary>
    <author>
      <name>Evangelos Stromatias</name>
    </author>
    <author>
      <name>John Marsland</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, originally submitted to IJCNN 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.0265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.0265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.2626v1</id>
    <updated>2011-10-12T10:56:29Z</updated>
    <published>2011-10-12T10:56:29Z</published>
    <title>Analysis of Heart Diseases Dataset using Neural Network Approach</title>
    <summary>  One of the important techniques of Data mining is Classification. Many real
world problems in various fields such as business, science, industry and
medicine can be solved by using classification approach. Neural Networks have
emerged as an important tool for classification. The advantages of Neural
Networks helps for efficient classification of given data. In this study a
Heart diseases dataset is analyzed using Neural Network approach. To increase
the efficiency of the classification process parallel approach is also adopted
in the training phase.
</summary>
    <author>
      <name>K. Usha Rani</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijdkp.2011.1501</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijdkp.2011.1501" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures, 1 table; International Journal of Data Mining &amp;
  Knowledge Management Process (IJDKP) Vol.1, No.5, September 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.2626v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.2626v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.6185v1</id>
    <updated>2011-06-30T11:08:05Z</updated>
    <published>2011-06-30T11:08:05Z</published>
    <title>Effects of Compensation, Connectivity and Tau in a Computational Model
  of Alzheimer's Disease</title>
    <summary>  This work updates an existing, simplistic computational model of Alzheimer's
Disease (AD) to investigate the behaviour of synaptic compensatory mechanisms
in neural networks with small-world connectivity, and varying methods of
calculating compensation. It additionally introduces a method for simulating
tau neurofibrillary pathology, resulting in a more dramatic damage profile.
Small-world connectivity is shown to have contrasting effects on capacity,
retrieval time, and robustness to damage, whilst the use of more
easily-obtained remote memories rather than recent memories for synaptic
compensation is found to lead to rapid network damage.
</summary>
    <author>
      <name>Mark Rowan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, submitted to International Joint Conference on Neural
  Networks 2011</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The 2011 International Joint Conference on Neural Networks
  (IJCNN), (2011) 543--550</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1106.6185v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.6185v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.2013v1</id>
    <updated>2013-11-08T16:14:14Z</updated>
    <published>2013-11-08T16:14:14Z</published>
    <title>Google matrix analysis of C.elegans neural network</title>
    <summary>  We study the structural properties of the neural network of the C.elegans
(worm) from a directed graph point of view. The Google matrix analysis is used
to characterize the neuron connectivity structure and node classifications are
discussed and compared with physiological properties of the cells. Our results
are obtained by a proper definition of neural directed network and subsequent
eigenvector analysis which recovers some results of previous studies. Our
analysis highlights particular sets of important neurons constituting the core
of the neural system. The applications of PageRank, CheiRank and ImpactRank to
characterization of interdependency of neurons are discussed.
</summary>
    <author>
      <name>Vivek Kandiah</name>
    </author>
    <author>
      <name>Dima L. Shepelyansky</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physleta.2014.04.045</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physleta.2014.04.045" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Additional information on the webpage
  http://www.quantware.ups-tlse.fr/QWLIB/wormgooglematrix/index.html</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.2013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.2013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.04899v2</id>
    <updated>2015-08-21T01:09:51Z</updated>
    <published>2015-08-20T07:17:25Z</published>
    <title>Sparse selection of bases in neural-network potential for crystalline
  and liquid Si</title>
    <summary>  The neural-network interatomic potential for crystalline and liquid Si has
been developed using the forward stepwise regression technique to reduce the
number of bases with keeping the accuracy of the potential. This approach of
making the neural-network potential enables us to construct the accurate
interatomic potentials with less and important bases selected systematically
and less heuristically. The evaluation of bulk crystalline properties, and
dynamic properties of liquid Si show good agreements between the neural-network
potential and ab-initio results.
</summary>
    <author>
      <name>Ryo Kobayashi</name>
    </author>
    <author>
      <name>Tomoyuki Tamura</name>
    </author>
    <author>
      <name>Ichiro Takeuchi</name>
    </author>
    <author>
      <name>Shuji Ogata</name>
    </author>
    <link href="http://arxiv.org/abs/1508.04899v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.04899v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.07385v3</id>
    <updated>2016-03-28T13:46:06Z</updated>
    <published>2015-09-24T14:20:29Z</published>
    <title>Provable approximation properties for deep neural networks</title>
    <summary>  We discuss approximation of functions using deep neural nets. Given a
function $f$ on a $d$-dimensional manifold $\Gamma \subset \mathbb{R}^m$, we
construct a sparsely-connected depth-4 neural network and bound its error in
approximating $f$. The size of the network depends on dimension and curvature
of the manifold $\Gamma$, the complexity of $f$, in terms of its wavelet
description, and only weakly on the ambient dimension $m$. Essentially, our
network computes wavelet functions, which are computed from Rectified Linear
Units (ReLU)
</summary>
    <author>
      <name>Uri Shaham</name>
    </author>
    <author>
      <name>Alexander Cloninger</name>
    </author>
    <author>
      <name>Ronald R. Coifman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.acha.2016.04.003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.acha.2016.04.003" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted for publication in Applied and Computational Harmonic
  Analysis</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.07385v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.07385v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.00513v1</id>
    <updated>2015-11-02T14:23:22Z</updated>
    <published>2015-11-02T14:23:22Z</published>
    <title>Pixel-wise Segmentation of Street with Neural Networks</title>
    <summary>  Pixel-wise street segmentation of photographs taken from a drivers
perspective is important for self-driving cars and can also support other
object recognition tasks. A framework called SST was developed to examine the
accuracy and execution time of different neural networks. The best neural
network achieved an $F_1$-score of 89.5% with a simple feedforward neural
network which trained to solve a regression task.
</summary>
    <author>
      <name>Sebastian Bittel</name>
    </author>
    <author>
      <name>Vitali Kaiser</name>
    </author>
    <author>
      <name>Marvin Teichmann</name>
    </author>
    <author>
      <name>Martin Thoma</name>
    </author>
    <link href="http://arxiv.org/abs/1511.00513v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.00513v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.01705v4</id>
    <updated>2016-06-07T23:25:51Z</updated>
    <published>2016-01-07T21:21:59Z</published>
    <title>Learning to Compose Neural Networks for Question Answering</title>
    <summary>  We describe a question answering model that applies to both images and
structured knowledge bases. The model uses natural language strings to
automatically assemble neural networks from a collection of composable modules.
Parameters for these modules are learned jointly with network-assembly
parameters via reinforcement learning, with only (world, question, answer)
triples as supervision. Our approach, which we term a dynamic neural model
network, achieves state-of-the-art results on benchmark datasets in both visual
and structured domains.
</summary>
    <author>
      <name>Jacob Andreas</name>
    </author>
    <author>
      <name>Marcus Rohrbach</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <author>
      <name>Dan Klein</name>
    </author>
    <link href="http://arxiv.org/abs/1601.01705v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.01705v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.05744v1</id>
    <updated>2016-10-18T19:03:02Z</updated>
    <published>2016-10-18T19:03:02Z</published>
    <title>A neural network approach to predicting and computing knot invariants</title>
    <summary>  In this paper we use artificial neural networks to predict and help compute
the values of certain knot invariants. In particular, we show that neural
networks are able to predict when a knot is quasipositive with a high degree of
accuracy. Given a knot with unknown quasipositivity we use these predictions to
identify braid representatives that are likely to be quasipositive, which we
then subject to further testing to verify. Using these techniques we identify
84 new quasipositive 11 and 12-crossing knots. Furthermore, we show that neural
networks are also able to predict and help compute the slice genus and
Ozsv\'{a}th-Szab\'{o} $\tau$-invariant of knots.
</summary>
    <author>
      <name>Mark C. Hughes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 1 figure, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.05744v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.05744v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="57M25, 57M27" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.06151v1</id>
    <updated>2016-10-19T18:49:50Z</updated>
    <published>2016-10-19T18:49:50Z</published>
    <title>Neural Networks for Modeling and Control of Particle Accelerators</title>
    <summary>  We describe some of the challenges of particle accelerator control, highlight
recent advances in neural network techniques, discuss some promising avenues
for incorporating neural networks into particle accelerator control systems,
and describe a neural network-based control system that is being developed for
resonance control of an RF electron gun at the Fermilab Accelerator Science and
Technology (FAST) facility, including initial experimental results from a
benchmark controller.
</summary>
    <author>
      <name>A. L. Edelen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Colorado State U.</arxiv:affiliation>
    </author>
    <author>
      <name>S. G. Biedron</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Colorado State U. and Ljubljana U.</arxiv:affiliation>
    </author>
    <author>
      <name>B. E. Chase</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Fermilab</arxiv:affiliation>
    </author>
    <author>
      <name>D. Edstrom</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Fermilab</arxiv:affiliation>
    </author>
    <author>
      <name>S. V. Milton</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Colorado State U.</arxiv:affiliation>
    </author>
    <author>
      <name>P. Stabile</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Geneva, ADAM</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TNS.2016.2543203</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TNS.2016.2543203" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pp</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Nuclear Science, Volume: 63, Issue: 2, 20
  April 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.06151v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.06151v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.acc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.acc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.06665v4</id>
    <updated>2017-04-21T00:00:05Z</updated>
    <published>2016-11-21T06:23:50Z</published>
    <title>A New System of Global Fractional-order Interval Implicit Projection
  Neural Networks</title>
    <summary>  The purpose of this paper is to introduce and investigate a new system of
global fractional-order interval implicit projection neural networks. An
existence and uniqueness theorem of the equilibrium point for such kind of
global fractional-order interval implicit projection neural networks is
obtained under some suitable assumptions. Moreover, Mittag-Leffler stability of
the global fractional-order interval implicit projection neural networks is
also proved. Finally, two numerical examples are given to illustrate the
validity of our results.
</summary>
    <author>
      <name>Zeng-bao Wu</name>
    </author>
    <author>
      <name>Jin-dong Li</name>
    </author>
    <author>
      <name>Nan-jing Huang</name>
    </author>
    <link href="http://arxiv.org/abs/1611.06665v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.06665v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.07136v1</id>
    <updated>2016-11-22T03:21:05Z</updated>
    <published>2016-11-22T03:21:05Z</published>
    <title>Cascaded Neural Networks with Selective Classifiers and its evaluation
  using Lung X-ray CT Images</title>
    <summary>  Lung nodule detection is a class imbalanced problem because nodules are found
with much lower frequency than non-nodules. In the class imbalanced problem,
conventional classifiers tend to be overwhelmed by the majority class and
ignore the minority class. We therefore propose cascaded convolutional neural
networks to cope with the class imbalanced problem. In the proposed approach,
cascaded convolutional neural networks that perform as selective classifiers
filter out obvious non-nodules. Successively, a convolutional neural network
trained with a balanced data set calculates nodule probabilities. The proposed
method achieved the detection sensitivity of 85.3% and 90.7% at 1 and 4 false
positives per scan in FROC curve, respectively.
</summary>
    <author>
      <name>Masaharu Sakamoto</name>
    </author>
    <author>
      <name>Hiroki Nakano</name>
    </author>
    <link href="http://arxiv.org/abs/1611.07136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.07136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.07324v1</id>
    <updated>2017-02-23T18:19:35Z</updated>
    <published>2017-02-23T18:19:35Z</published>
    <title>Inherent Biases of Recurrent Neural Networks for Phonological
  Assimilation and Dissimilation</title>
    <summary>  A recurrent neural network model of phonological pattern learning is
proposed. The model is a relatively simple neural network with one recurrent
layer, and displays biases in learning that mimic observed biases in human
learning. Single-feature patterns are learned faster than two-feature patterns,
and vowel or consonant-only patterns are learned faster than patterns involving
vowels and consonants, mimicking the results of laboratory learning
experiments. In non-recurrent models, capturing these biases requires the use
of alpha features or some other representation of repeated features, but with a
recurrent neural network, these elaborations are not necessary.
</summary>
    <author>
      <name>Amanda Doucette</name>
    </author>
    <link href="http://arxiv.org/abs/1702.07324v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.07324v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.01961v2</id>
    <updated>2017-06-12T21:05:58Z</updated>
    <published>2017-03-06T16:39:16Z</published>
    <title>Multiplicative Normalizing Flows for Variational Bayesian Neural
  Networks</title>
    <summary>  We reinterpret multiplicative noise in neural networks as auxiliary random
variables that augment the approximate posterior in a variational setting for
Bayesian neural networks. We show that through this interpretation it is both
efficient and straightforward to improve the approximation by employing
normalizing flows while still allowing for local reparametrizations and a
tractable lower bound. In experiments we show that with this new approximation
we can significantly improve upon classical mean field for Bayesian neural
networks on both predictive accuracy as well as predictive uncertainty.
</summary>
    <author>
      <name>Christos Louizos</name>
    </author>
    <author>
      <name>Max Welling</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appearing at the International Conference on Machine Learning (ICML)
  2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.01961v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.01961v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.03360v1</id>
    <updated>2017-05-09T14:43:52Z</updated>
    <published>2017-05-09T14:43:52Z</published>
    <title>Skin lesion detection based on an ensemble of deep convolutional neural
  network</title>
    <summary>  Skin cancer is a major public health problem, with over 5 million newly
diagnosed cases in the United States each year. Melanoma is the deadliest form
of skin cancer, responsible for over 9,000 deaths each year. In this paper, we
propose an ensemble of deep convolutional neural networks to classify
dermoscopy images into three classes. To achieve the highest classification
accuracy, we fuse the outputs of the softmax layers of four different neural
architectures. For aggregation, we consider the individual accuracies of the
networks weighted by the confidence values provided by their final softmax
layers. This fusion-based approach outperformed all the individual neural
networks regarding classification accuracy.
</summary>
    <author>
      <name>Balazs Harangi</name>
    </author>
    <link href="http://arxiv.org/abs/1705.03360v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.03360v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09723v1</id>
    <updated>2017-07-31T05:00:32Z</updated>
    <published>2017-07-31T05:00:32Z</published>
    <title>Solving the Bose-Hubbard model with machine learning</title>
    <summary>  Motivated by the recent successful application of artificial neural networks
to quantum many-body problems [G. Carleo and M. Troyer, Science {\bf 355}, 602
(2017)], a method to calculate the ground state of the Bose-Hubbard model using
a feedforward neural network is proposed. The results are in good agreement
with those obtained by exact diagonalization and the Gutzwiller approximation.
The method of neural-network quantum states is promising for solving quantum
many-body problems of ultracold atoms in optical lattices.
</summary>
    <author>
      <name>Hiroki Saito</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.7566/JPSJ.86.093001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.7566/JPSJ.86.093001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J. Phys. Soc. Jpn. 86, 093001 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1707.09723v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09723v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.quant-gas" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09775v1</id>
    <updated>2017-07-31T09:14:14Z</updated>
    <published>2017-07-31T09:14:14Z</published>
    <title>Capacity limitations of visual search in deep convolutional neural
  network</title>
    <summary>  Deep convolutional neural networks follow roughly the architecture of
biological visual systems, and have shown a performance comparable to human
observers in object recognition tasks. In this study, I test a pre-trained deep
neural network in some classic visual search tasks. The results reveal a
qualitative difference from human performance. It appears that there is no
difference between searches for simple features that pop out in experiments
with humans, and for feature configurations that exhibit strict capacity
limitations in human vision. Both types of stimuli reveal moderate capacity
limitations in the neural network tested here.
</summary>
    <author>
      <name>Endel Poder</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.09775v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09775v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.00077v1</id>
    <updated>2017-07-31T21:33:42Z</updated>
    <published>2017-07-31T21:33:42Z</published>
    <title>Bayesian Sparsification of Recurrent Neural Networks</title>
    <summary>  Recurrent neural networks show state-of-the-art results in many text analysis
tasks but often require a lot of memory to store their weights. Recently
proposed Sparse Variational Dropout eliminates the majority of the weights in a
feed-forward neural network without significant loss of quality. We apply this
technique to sparsify recurrent neural networks. To account for recurrent
specifics we also rely on Binary Variational Dropout for RNN. We report 99.5%
sparsity level on sentiment analysis task without a quality drop and up to 87%
sparsity level on language modeling task with slight loss of accuracy.
</summary>
    <author>
      <name>Ekaterina Lobacheva</name>
    </author>
    <author>
      <name>Nadezhda Chirkova</name>
    </author>
    <author>
      <name>Dmitry Vetrov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Workshop on Learning to Generate Natural Language, ICML,
  2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.00077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.00129v1</id>
    <updated>2017-08-01T02:09:12Z</updated>
    <published>2017-08-01T02:09:12Z</published>
    <title>Deep Generative Adversarial Neural Networks for Realistic Prostate
  Lesion MRI Synthesis</title>
    <summary>  Generative Adversarial Neural Networks (GANs) are applied to the synthetic
generation of prostate lesion MRI images. GANs have been applied to a variety
of natural images, is shown show that the same techniques can be used in the
medical domain to create realistic looking synthetic lesion images. 16mm x 16mm
patches are extracted from 330 MRI scans from the SPIE ProstateX Challenge 2016
and used to train a Deep Convolutional Generative Adversarial Neural Network
(DCGAN) utilizing cutting edge techniques. Synthetic outputs are compared to
real images and the implicit latent representations induced by the GAN are
explored. Training techniques and successful neural network architectures are
explained in detail.
</summary>
    <author>
      <name>Andy Kitchen</name>
    </author>
    <author>
      <name>Jarrel Seah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.00129v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00129v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.10; I.4.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.07908v1</id>
    <updated>2017-09-20T20:45:53Z</updated>
    <published>2017-09-20T20:45:53Z</published>
    <title>Neural Network Alternatives to Convolutive Audio Models for Source
  Separation</title>
    <summary>  Convolutive Non-Negative Matrix Factorization model factorizes a given audio
spectrogram using frequency templates with a temporal dimension. In this paper,
we present a convolutional auto-encoder model that acts as a neural network
alternative to convolutive NMF. Using the modeling flexibility granted by
neural networks, we also explore the idea of using a Recurrent Neural Network
in the encoder. Experimental results on speech mixtures from TIMIT dataset
indicate that the convolutive architecture provides a significant improvement
in separation performance in terms of BSSeval metrics.
</summary>
    <author>
      <name>Shrikant Venkataramani</name>
    </author>
    <author>
      <name>Y. Cem Subakan</name>
    </author>
    <author>
      <name>Paris Smaragdis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in MLSP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.07908v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.07908v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.04724v2</id>
    <updated>2017-11-14T23:27:16Z</updated>
    <published>2017-10-12T21:27:34Z</published>
    <title>Training deep neural networks for the inverse design of nanophotonic
  structures</title>
    <summary>  Data inconsistency leads to a slow training process when deep neural networks
are used for the inverse design of photonic devices, an issue that arises from
the fundamental property of non-uniqueness in all inverse scattering problems.
Here we show that by combining forward modeling and inverse design in a tandem
architecture, one can overcome this fundamental issue, allowing deep neural
networks to be effectively trained by data sets that contain non-unique
electromagnetic scattering instances. This paves the way for using deep neural
networks to design complex photonic structures that requires large training
sets.
</summary>
    <author>
      <name>Dianjing Liu</name>
    </author>
    <author>
      <name>Yixuan Tan</name>
    </author>
    <author>
      <name>Zongfu Yu</name>
    </author>
    <link href="http://arxiv.org/abs/1710.04724v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.04724v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.app-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.04252v1</id>
    <updated>2017-11-12T09:01:35Z</updated>
    <published>2017-11-12T09:01:35Z</published>
    <title>Extracting Critical Exponent by Finite-Size Scaling with Convolutional
  Neural Networks</title>
    <summary>  Machine learning has been successfully applied to identify phases and phase
transitions in condensed matter systems. However, quantitative characterization
of the critical fluctuations near phase transitions is lacking. In this study
we extract the critical behavior of a quantum Hall plateau transition with a
convolutional neural network. We introduce a finite-size scaling approach and
show that the localization length critical exponent learned by the neural
network is consistent with the value obtained by conventional approaches. We
illustrate the physics behind the approach by a cross-examination of the
inverse participation ratios.
</summary>
    <author>
      <name>Zhenyu Li</name>
    </author>
    <author>
      <name>Mingxing Luo</name>
    </author>
    <author>
      <name>Xin Wan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.04252v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.04252v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.04606v1</id>
    <updated>2017-11-13T14:55:32Z</updated>
    <published>2017-11-13T14:55:32Z</published>
    <title>Provably efficient neural network representation for image
  classification</title>
    <summary>  The state-of-the-art approaches for image classification are based on neural
networks. Mathematically, the task of classifying images is equivalent to
finding the function that maps an image to the label it is associated with. To
rigorously establish the success of neural network methods, we should first
prove that the function has an efficient neural network representation, and
then design provably efficient training algorithms to find such a
representation. Here, we achieve the first goal based on a set of assumptions
about the patterns in the images. The validity of these assumptions is very
intuitive in many image classification problems, including but not limited to,
recognizing handwritten digits.
</summary>
    <author>
      <name>Yichen Huang</name>
    </author>
    <link href="http://arxiv.org/abs/1711.04606v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.04606v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.00720v1</id>
    <updated>2017-12-03T06:37:15Z</updated>
    <published>2017-12-03T06:37:15Z</published>
    <title>Automatic Recognition of Coal and Gangue based on Convolution Neural
  Network</title>
    <summary>  We designed a gangue sorting system,and built a convolutional neural network
model based on AlexNet. Data enhancement and transfer learning are used to
solve the problem which the convolution neural network has insufficient
training data in the training stage. An object detection and region clipping
algorithm is proposed to adjust the training image data to the optimum size.
Compared with traditional neural network and SVM algorithm, this algorithm has
higher recognition rate for coal and coal gangue, and provides important
reference for identification and separation of coal and gangue.
</summary>
    <author>
      <name>Huichao Hong</name>
    </author>
    <author>
      <name>Lixin Zheng</name>
    </author>
    <author>
      <name>Jianqing Zhu</name>
    </author>
    <author>
      <name>Shuwan Pan</name>
    </author>
    <author>
      <name>Kaiting Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/1712.00720v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.00720v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.05417v1</id>
    <updated>2018-01-16T22:00:40Z</updated>
    <published>2018-01-16T22:00:40Z</published>
    <title>Quantum Walk Inspired Neural Networks for Graph-Structured Data</title>
    <summary>  In recent years, along with the overwhelming advances in the field of neural
information processing, quantum information processing (QIP) has shown
significant progress in solving problems that are intractable on classical
computers. Quantum machine learning (QML) explores the ways in which these
fields can learn from one another. We propose quantum walk neural networks
(QWNN), a new graph neural network architecture based on quantum random walks,
the quantum parallel to classical random walks. A QWNN learns a quantum walk on
a graph to construct a diffusion operator which can be applied to a signal on a
graph. We demonstrate the use of the network for prediction tasks for graph
structured signals.
</summary>
    <author>
      <name>Stefan Dernbach</name>
    </author>
    <author>
      <name>Arman Mohseni-Kabir</name>
    </author>
    <author>
      <name>Don Towsley</name>
    </author>
    <author>
      <name>Siddharth Pal</name>
    </author>
    <link href="http://arxiv.org/abs/1801.05417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.05417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.02091v1</id>
    <updated>2018-02-06T17:46:32Z</updated>
    <published>2018-02-06T17:46:32Z</published>
    <title>Structural Recurrent Neural Network (SRNN) for Group Activity Analysis</title>
    <summary>  A group of persons can be analyzed at various semantic levels such as
individual actions, their interactions, and the activity of the entire group.
In this paper, we propose a structural recurrent neural network (SRNN) that
uses a series of interconnected RNNs to jointly capture the actions of
individuals, their interactions, as well as the group activity. While previous
structural recurrent neural networks assumed that the number of nodes and edges
is constant, we use a grid pooling layer to address the fact that the number of
individuals in a group can vary. We evaluate two variants of the structural
recurrent neural network on the Volleyball Dataset.
</summary>
    <author>
      <name>Sovan Biswas</name>
    </author>
    <author>
      <name>Juergen Gall</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in WACV 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.02091v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.02091v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.07384v1</id>
    <updated>2018-02-21T00:47:32Z</updated>
    <published>2018-02-21T00:47:32Z</published>
    <title>Interpreting Neural Network Judgments via Minimal, Stable, and Symbolic
  Corrections</title>
    <summary>  The paper describes a new algorithm to generate minimal, stable, and symbolic
corrections to an input that will cause a neural network with ReLU neurons to
change its output. We argue that such a correction is a useful way to provide
feedback to a user when the neural network produces an output that is different
from a desired output. Our algorithm generates such a correction by solving a
series of linear constraint satisfaction problems. The technique is evaluated
on a neural network that has been trained to predict whether an applicant will
pay a mortgage.
</summary>
    <author>
      <name>Xin Zhang</name>
    </author>
    <author>
      <name>Armando Solar-Lezama</name>
    </author>
    <author>
      <name>Rishabh Singh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.07384v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.07384v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.10275v1</id>
    <updated>2018-02-28T06:16:33Z</updated>
    <published>2018-02-28T06:16:33Z</published>
    <title>Solving for high dimensional committor functions using artificial neural
  networks</title>
    <summary>  In this note we propose a method based on artificial neural network to study
the transition between states governed by stochastic processes. In particular,
we aim for numerical schemes for the committor function, the central object of
transition path theory, which satisfies a high-dimensional Fokker-Planck
equation. By working with the variational formulation of such partial
differential equation and parameterizing the committor function in terms of a
neural network, approximations can be obtained via optimizing the neural
network weights using stochastic algorithms. The numerical examples show that
moderate accuracy can be achieved for high-dimensional problems.
</summary>
    <author>
      <name>Yuehaw Khoo</name>
    </author>
    <author>
      <name>Jianfeng Lu</name>
    </author>
    <author>
      <name>Lexing Ying</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.10275v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.10275v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65Nxx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.00885v1</id>
    <updated>2018-03-02T15:22:10Z</updated>
    <published>2018-03-02T15:22:10Z</published>
    <title>Essentially No Barriers in Neural Network Energy Landscape</title>
    <summary>  Training neural networks involves finding minima of a high-dimensional
non-convex loss function. Knowledge of the structure of this energy landscape
is sparse. Relaxing from linear interpolations, we construct continuous paths
between minima of recent neural network architectures on CIFAR10 and CIFAR100.
Surprisingly, the paths are essentially flat in both the training and test
landscapes. This implies that neural networks have enough capacity for
structural changes, or that these changes are small between minima. Also, each
minimum has at least one vanishing Hessian eigenvalue in addition to those
resulting from trivial invariance.
</summary>
    <author>
      <name>Felix Draxler</name>
    </author>
    <author>
      <name>Kambis Veschgini</name>
    </author>
    <author>
      <name>Manfred Salmhofer</name>
    </author>
    <author>
      <name>Fred A. Hamprecht</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to 35th International Conference on Machine Learning (ICML
  2018) on February 9th, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.00885v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.00885v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0808.2341v3</id>
    <updated>2009-04-04T10:38:29Z</updated>
    <published>2008-08-18T06:27:12Z</published>
    <title>A Moving Bump in a Continuous Manifold: A Comprehensive Study of the
  Tracking Dynamics of Continuous Attractor Neural Networks</title>
    <summary>  Understanding how the dynamics of a neural network is shaped by the network
structure, and consequently how the network structure facilitates the functions
implemented by the neural system, is at the core of using mathematical models
to elucidate brain functions. This study investigates the tracking dynamics of
continuous attractor neural networks (CANNs). Due to the translational
invariance of neuronal recurrent interactions, CANNs can hold a continuous
family of stationary states. They form a continuous manifold in which the
neural system is neutrally stable. We systematically explore how this property
facilitates the tracking performance of a CANN, which is believed to have clear
correspondence with brain functions. By using the wave functions of the quantum
harmonic oscillator as the basis, we demonstrate how the dynamics of a CANN is
decomposed into different motion modes, corresponding to distortions in the
amplitude, position, width or skewness of the network state. We then develop a
perturbative approach that utilizes the dominating movement of the network's
stationary states in the state space. This method allows us to approximate the
network dynamics up to an arbitrary accuracy depending on the order of
perturbation used. We quantify the distortions of a Gaussian bump during
tracking, and study their effects on the tracking performance. Results are
obtained on the maximum speed for a moving stimulus to be trackable and the
reaction time for the network to catch up with an abrupt change in the
stimulus.
</summary>
    <author>
      <name>C. C. Alan Fung</name>
    </author>
    <author>
      <name>K. Y. Michael Wong</name>
    </author>
    <author>
      <name>Si Wu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/neco.2009.07-08-824</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/neco.2009.07-08-824" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">43 pages, 10 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Comput. 2010 22(3): 752-92</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0808.2341v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0808.2341v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.05504v1</id>
    <updated>2018-01-16T23:43:34Z</updated>
    <published>2018-01-16T23:43:34Z</published>
    <title>Automatic Classification of Music Genre using Masked Conditional Neural
  Networks</title>
    <summary>  Neural network based architectures used for sound recognition are usually
adapted from other application domains such as image recognition, which may not
harness the time-frequency representation of a signal. The ConditionaL Neural
Networks (CLNN) and its extension the Masked ConditionaL Neural Networks
(MCLNN) are designed for multidimensional temporal signal recognition. The CLNN
is trained over a window of frames to preserve the inter-frame relation, and
the MCLNN enforces a systematic sparseness over the network's links that mimics
a filterbank-like behavior. The masking operation induces the network to learn
in frequency bands, which decreases the network susceptibility to
frequency-shifts in time-frequency representations. Additionally, the mask
allows an exploration of a range of feature combinations concurrently analogous
to the manual handcrafting of the optimum collection of features for a
recognition task. MCLNN have achieved competitive performance on the Ballroom
music dataset compared to several hand-crafted attempts and outperformed models
based on state-of-the-art Convolutional Neural Networks.
</summary>
    <author>
      <name>Fady Medhat</name>
    </author>
    <author>
      <name>David Chesmore</name>
    </author>
    <author>
      <name>John Robinson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICDM.2017.125</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICDM.2017.125" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Restricted Boltzmann Machine; RBM; Conditional RBM; CRBM; Deep Belief
  Net; DBN; Conditional Neural Network; CLNN; Masked Conditional Neural
  Network; MCLNN; Music Information Retrieval; MIR. IEEE International
  Conference on Data Mining (ICDM), 2017</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Data Mining (ICDM) Year: 2017
  Pages: 979 - 984</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1801.05504v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.05504v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.06488v3</id>
    <updated>2016-01-07T13:50:22Z</updated>
    <published>2015-11-20T04:55:46Z</published>
    <title>Resiliency of Deep Neural Networks under Quantization</title>
    <summary>  The complexity of deep neural network algorithms for hardware implementation
can be much lowered by optimizing the word-length of weights and signals.
Direct quantization of floating-point weights, however, does not show good
performance when the number of bits assigned is small. Retraining of quantized
networks has been developed to relieve this problem. In this work, the effects
of retraining are analyzed for a feedforward deep neural network (FFDNN) and a
convolutional neural network (CNN). The network complexity is controlled to
know their effects on the resiliency of quantized networks by retraining. The
complexity of the FFDNN is controlled by varying the unit size in each hidden
layer and the number of layers, while that of the CNN is done by modifying the
feature map configuration. We find that the performance gap between the
floating-point and the retrain-based ternary (+1, 0, -1) weight neural networks
exists with a fair amount in 'complexity limited' networks, but the discrepancy
almost vanishes in fully complex networks whose capability is limited by the
training data, rather than by the number of connections. This research shows
that highly complex DNNs have the capability of absorbing the effects of severe
weight quantization through retraining, but connection limited networks are
less resilient. This paper also presents the effective compression ratio to
guide the trade-off between the network size and the precision when the
hardware resource is limited.
</summary>
    <author>
      <name>Wonyong Sung</name>
    </author>
    <author>
      <name>Sungho Shin</name>
    </author>
    <author>
      <name>Kyuyeon Hwang</name>
    </author>
    <link href="http://arxiv.org/abs/1511.06488v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.06488v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.04199v2</id>
    <updated>2017-04-19T00:53:16Z</updated>
    <published>2017-04-13T16:23:19Z</published>
    <title>Evolution and Analysis of Embodied Spiking Neural Networks Reveals
  Task-Specific Clusters of Effective Networks</title>
    <summary>  Elucidating principles that underlie computation in neural networks is
currently a major research topic of interest in neuroscience. Transfer Entropy
(TE) is increasingly used as a tool to bridge the gap between network
structure, function, and behavior in fMRI studies. Computational models allow
us to bridge the gap even further by directly associating individual neuron
activity with behavior. However, most computational models that have analyzed
embodied behaviors have employed non-spiking neurons. On the other hand,
computational models that employ spiking neural networks tend to be restricted
to disembodied tasks. We show for the first time the artificial evolution and
TE-analysis of embodied spiking neural networks to perform a
cognitively-interesting behavior. Specifically, we evolved an agent controlled
by an Izhikevich neural network to perform a visual categorization task. The
smallest networks capable of performing the task were found by repeating
evolutionary runs with different network sizes. Informational analysis of the
best solution revealed task-specific TE-network clusters, suggesting that
within-task homogeneity and across-task heterogeneity were key to behavioral
success. Moreover, analysis of the ensemble of solutions revealed that
task-specificity of TE-network clusters correlated with fitness. This provides
an empirically testable hypothesis that links network structure to behavior.
</summary>
    <author>
      <name>Madhavun Candadai Vasu</name>
    </author>
    <author>
      <name>Eduardo J. Izquierdo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3071178.3071336</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3071178.3071336" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Camera ready version of accepted for GECCO'17</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.04199v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.04199v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.10203v1</id>
    <updated>2017-11-28T09:41:34Z</updated>
    <published>2017-11-28T09:41:34Z</published>
    <title>Visualisation and 'diagnostic classifiers' reveal how recurrent and
  recursive neural networks process hierarchical structure</title>
    <summary>  We investigate how neural networks can learn and process languages with
hierarchical, compositional semantics. To this end, we define the artificial
task of processing nested arithmetic expressions, and study whether different
types of neural networks can learn to compute their meaning. We find that
recursive neural networks can find a generalising solution to this problem, and
we visualise this solution by breaking it up in three steps: project, sum and
squash. As a next step, we investigate recurrent neural networks, and show that
a gated recurrent unit, that processes its input incrementally, also performs
very well on this task. To develop an understanding of what the recurrent
network encodes, visualisation techniques alone do not suffice. Therefore, we
develop an approach where we formulate and test multiple hypotheses on the
information encoded and processed by the network. For each hypothesis, we
derive predictions about features of the hidden state representations at each
time step, and train 'diagnostic classifiers' to test those predictions. Our
results indicate that the networks follow a strategy similar to our
hypothesised 'cumulative strategy', which explains the high accuracy of the
network on novel expressions, the generalisation to longer expressions than
seen in training, and the mild deterioration with increasing length. This is
turn shows that diagnostic classifiers can be a useful technique for opening up
the black box of neural networks. We argue that diagnostic classification,
unlike most visualisation techniques, does scale up from small networks in a
toy domain, to larger and deeper recurrent networks dealing with real-life
data, and may therefore contribute to a better understanding of the internal
dynamics of current state-of-the-art models in natural language processing.
</summary>
    <author>
      <name>Dieuwke Hupkes</name>
    </author>
    <author>
      <name>Sara Veldhoen</name>
    </author>
    <author>
      <name>Willem Zuidema</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.10203v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.10203v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.7005v1</id>
    <updated>2014-08-29T13:00:17Z</updated>
    <published>2014-08-29T13:00:17Z</published>
    <title>The quest for a Quantum Neural Network</title>
    <summary>  With the overwhelming success in the field of quantum information in the last
decades, the "quest" for a Quantum Neural Network (QNN) model began in order to
combine quantum computing with the striking properties of neural computing.
This article presents a systematic approach to QNN research, which so far
consists of a conglomeration of ideas and proposals. It outlines the challenge
of combining the nonlinear, dissipative dynamics of neural computing and the
linear, unitary dynamics of quantum computing. It establishes requirements for
a meaningful QNN and reviews existing literature against these requirements. It
is found that none of the proposals for a potential QNN model fully exploits
both the advantages of quantum physics and computing in neural networks. An
outlook on possible ways forward is given, emphasizing the idea of Open Quantum
Neural Networks based on dissipative quantum computing.
</summary>
    <author>
      <name>M. Schuld</name>
    </author>
    <author>
      <name>I. Sinayskiy</name>
    </author>
    <author>
      <name>F. Petruccione</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11128-014-0809-8</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11128-014-0809-8" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Review of Quantum Neural Networks research; 21 pages, 5 figs, 71 Refs</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.7005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.7005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04466v1</id>
    <updated>2016-06-14T17:26:27Z</updated>
    <published>2016-06-14T17:26:27Z</published>
    <title>Neural Networks and Continuous Time</title>
    <summary>  The fields of neural computation and artificial neural networks have
developed much in the last decades. Most of the works in these fields focus on
implementing and/or learning discrete functions or behavior. However,
technical, physical, and also cognitive processes evolve continuously in time.
This cannot be described directly with standard architectures of artificial
neural networks such as multi-layer feed-forward perceptrons. Therefore, in
this paper, we will argue that neural networks modeling continuous time are
needed explicitly for this purpose, because with them the synthesis and
analysis of continuous and possibly periodic processes in time are possible
(e.g. for robot behavior) besides computing discrete classification functions
(e.g. for logical reasoning). We will relate possible neural network
architectures with (hybrid) automata models that allow to express continuous
processes.
</summary>
    <author>
      <name>Frieder Stolzenburg</name>
    </author>
    <author>
      <name>Florian Ruh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 10 figures. This paper is an extended version of a
  contribution presented at KI 2009 Workshop Complex Cognition</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.04466v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04466v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.05672v1</id>
    <updated>2017-09-17T14:44:07Z</updated>
    <published>2017-09-17T14:44:07Z</published>
    <title>Neural Affine Grayscale Image Denoising</title>
    <summary>  We propose a new grayscale image denoiser, dubbed as Neural Affine Image
Denoiser (Neural AIDE), which utilizes neural network in a novel way. Unlike
other neural network based image denoising methods, which typically apply
simple supervised learning to learn a mapping from a noisy patch to a clean
patch, we formulate to train a neural network to learn an \emph{affine} mapping
that gets applied to a noisy pixel, based on its context. Our formulation
enables both supervised training of the network from the labeled training
dataset and adaptive fine-tuning of the network parameters using the given
noisy image subject to denoising. The key tool for devising Neural AIDE is to
devise an estimated loss function of the MSE of the affine mapping, solely
based on the noisy data. As a result, our algorithm can outperform most of the
recent state-of-the-art methods in the standard benchmark datasets. Moreover,
our fine-tuning method can nicely overcome one of the drawbacks of the
patch-level supervised learning methods in image denoising; namely, a
supervised trained model with a mismatched noise variance can be mostly
corrected as long as we have the matched noise variance during the fine-tuning
step.
</summary>
    <author>
      <name>Sungmin Cha</name>
    </author>
    <author>
      <name>Taesup Moon</name>
    </author>
    <link href="http://arxiv.org/abs/1709.05672v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.05672v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.08314v2</id>
    <updated>2018-01-06T23:16:33Z</updated>
    <published>2017-12-22T06:28:28Z</published>
    <title>Benchmarking Decoupled Neural Interfaces with Synthetic Gradients</title>
    <summary>  Artifical Neural Networks are a particular class of learning systems modeled
after biological neural functions with an interesting penchant for Hebbian
learning, that is "neurons that wire together, fire together". However, unlike
their natural counterparts, artificial neural networks have a close and
stringent coupling between the modules of neurons in the network. This coupling
or locking imposes upon the network a strict and inflexible structure that
prevent layers in the network from updating their weights until a full
feed-forward and backward pass has occurred. Such a constraint though may have
sufficed for a while, is now no longer feasible in the era of very-large-scale
machine learning, coupled with the increased desire for parallelization of the
learning process across multiple computing infrastructures. To solve this
problem, synthetic gradients (SG) with decoupled neural interfaces (DNI) are
introduced as a viable alternative to the backpropagation algorithm. This paper
performs a speed benchmark to compare the speed and accuracy capabilities of
SG-DNI as opposed to a standard neural interface using multilayer perceptron
MLP. SG-DNI shows good promise, in that it not only captures the learning
problem, it is also over 3-fold faster due to it asynchronous learning
capabilities.
</summary>
    <author>
      <name>Ekaba Bisong</name>
    </author>
    <link href="http://arxiv.org/abs/1712.08314v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.08314v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.05508v1</id>
    <updated>2015-08-22T13:15:09Z</updated>
    <published>2015-08-22T13:15:09Z</published>
    <title>Towards Neural Network-based Reasoning</title>
    <summary>  We propose Neural Reasoner, a framework for neural network-based reasoning
over natural language sentences. Given a question, Neural Reasoner can infer
over multiple supporting facts and find an answer to the question in specific
forms. Neural Reasoner has 1) a specific interaction-pooling mechanism,
allowing it to examine multiple facts, and 2) a deep architecture, allowing it
to model the complicated logical relations in reasoning tasks. Assuming no
particular structure exists in the question and facts, Neural Reasoner is able
to accommodate different types of reasoning and different forms of language
expressions. Despite the model complexity, Neural Reasoner can still be trained
effectively in an end-to-end manner. Our empirical studies show that Neural
Reasoner can outperform existing neural reasoning systems with remarkable
margins on two difficult artificial tasks (Positional Reasoning and Path
Finding) proposed in [8]. For example, it improves the accuracy on Path
Finding(10K) from 33.4% [6] to over 98%.
</summary>
    <author>
      <name>Baolin Peng</name>
    </author>
    <author>
      <name>Zhengdong Lu</name>
    </author>
    <author>
      <name>Hang Li</name>
    </author>
    <author>
      <name>Kam-Fai Wong</name>
    </author>
    <link href="http://arxiv.org/abs/1508.05508v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.05508v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0111151v3</id>
    <updated>2011-02-13T06:35:04Z</updated>
    <published>2001-11-09T05:07:18Z</published>
    <title>Neural Networks with Finite Width Action Potentials</title>
    <summary>  The paper was done as an assigned Princeton university project. It is being
withdrawn since it needs to be changed and updated substantially.
</summary>
    <author>
      <name>Fariel Shafee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0111151v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0111151v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.1390v1</id>
    <updated>2007-05-10T05:52:22Z</updated>
    <published>2007-05-10T05:52:22Z</published>
    <title>Machine and Component Residual Life Estimation through the Application
  of Neural Networks</title>
    <summary>  This paper concerns the use of neural networks for predicting the residual
life of machines and components. In addition, the advantage of using
condition-monitoring data to enhance the predictive capability of these neural
networks was also investigated. A number of neural network variations were
trained and tested with the data of two different reliability-related datasets.
The first dataset represents the renewal case where the failed unit is repaired
and restored to a good-as-new condition. Data was collected in the laboratory
by subjecting a series of similar test pieces to fatigue loading with a
hydraulic actuator. The average prediction error of the various neural networks
being compared varied from 431 to 841 seconds on this dataset, where test
pieces had a characteristic life of 8,971 seconds. The second dataset was
collected from a group of pumps used to circulate a water and magnetite
solution within a plant. The data therefore originated from a repaired system
affected by reliability degradation. When optimized, the multi-layer perceptron
neural networks trained with the Levenberg-Marquardt algorithm and the general
regression neural network produced a sum-of-squares error within 11.1% of each
other. The potential for using neural networks for residual life prediction and
the advantage of incorporating condition-based data into the model were proven
for both examples.
</summary>
    <author>
      <name>M. A. Herzog</name>
    </author>
    <author>
      <name>T. Marwala</name>
    </author>
    <author>
      <name>P. S. Heyns</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0705.1390v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.1390v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7948v2</id>
    <updated>2013-06-02T20:32:05Z</updated>
    <published>2013-04-30T10:41:26Z</published>
    <title>Convolutional Neural Networks learn compact local image descriptors</title>
    <summary>  A standard deep convolutional neural network paired with a suitable loss
function learns compact local image descriptors that perform comparably to
state-of-the art approaches.
</summary>
    <author>
      <name>Christian Osendorfer</name>
    </author>
    <author>
      <name>Justin Bayer</name>
    </author>
    <author>
      <name>Patrick van der Smagt</name>
    </author>
    <link href="http://arxiv.org/abs/1304.7948v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7948v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.5598v1</id>
    <updated>2014-04-22T19:24:23Z</updated>
    <published>2014-04-22T19:24:23Z</published>
    <title>Descriptive examples of the limitations of Artificial Neural Networks
  applied to the analysis of independent stochastic data</title>
    <summary>  We show with a few descriptive examples the limitations of Artificial Neural
Networks when they are applied to the analysis of independent stochastic data.
</summary>
    <author>
      <name>Henry Navarro</name>
    </author>
    <author>
      <name>Leonardo Bennun</name>
    </author>
    <link href="http://arxiv.org/abs/1404.5598v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.5598v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.0261v1</id>
    <updated>2013-09-01T20:35:17Z</updated>
    <published>2013-09-01T20:35:17Z</published>
    <title>Multi-Column Deep Neural Networks for Offline Handwritten Chinese
  Character Classification</title>
    <summary>  Our Multi-Column Deep Neural Networks achieve best known recognition rates on
Chinese characters from the ICDAR 2011 and 2013 offline handwriting
competitions, approaching human performance.
</summary>
    <author>
      <name>Dan Cire≈üan</name>
    </author>
    <author>
      <name>J√ºrgen Schmidhuber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure, IDSIA tech report</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.0261v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.0261v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.5549v2</id>
    <updated>2016-05-02T16:52:36Z</updated>
    <published>2014-08-24T04:10:33Z</published>
    <title>On the Neuron Response Features of Convolutional Neural Networks for
  Remote Sensing Image</title>
    <summary>  In this paper, some patterns of the Neuron Response of deep Convolutional
Neural Networks were observed.
</summary>
    <author>
      <name>Jie Chen</name>
    </author>
    <author>
      <name>Min Deng</name>
    </author>
    <author>
      <name>Haifeng Li</name>
    </author>
    <link href="http://arxiv.org/abs/1408.5549v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.5549v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.00036v2</id>
    <updated>2015-04-14T22:55:08Z</updated>
    <published>2015-02-27T23:50:22Z</published>
    <title>Norm-Based Capacity Control in Neural Networks</title>
    <summary>  We investigate the capacity, convexity and characterization of a general
family of norm-constrained feed-forward networks.
</summary>
    <author>
      <name>Behnam Neyshabur</name>
    </author>
    <author>
      <name>Ryota Tomioka</name>
    </author>
    <author>
      <name>Nathan Srebro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.00036v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.00036v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04644v2</id>
    <updated>2017-03-22T17:46:16Z</updated>
    <published>2016-08-16T15:59:35Z</published>
    <title>Towards Evaluating the Robustness of Neural Networks</title>
    <summary>  Neural networks provide state-of-the-art results for most machine learning
tasks. Unfortunately, neural networks are vulnerable to adversarial examples:
given an input $x$ and any target classification $t$, it is possible to find a
new input $x'$ that is similar to $x$ but classified as $t$. This makes it
difficult to apply neural networks in security-critical areas. Defensive
distillation is a recently proposed approach that can take an arbitrary neural
network, and increase its robustness, reducing the success rate of current
attacks' ability to find adversarial examples from $95\%$ to $0.5\%$.
  In this paper, we demonstrate that defensive distillation does not
significantly increase the robustness of neural networks by introducing three
new attack algorithms that are successful on both distilled and undistilled
neural networks with $100\%$ probability. Our attacks are tailored to three
distance metrics used previously in the literature, and when compared to
previous adversarial example generation algorithms, our attacks are often much
more effective (and never worse). Furthermore, we propose using high-confidence
adversarial examples in a simple transferability test we show can also be used
to break defensive distillation. We hope our attacks will be used as a
benchmark in future defense attempts to create neural networks that resist
adversarial examples.
</summary>
    <author>
      <name>Nicholas Carlini</name>
    </author>
    <author>
      <name>David Wagner</name>
    </author>
    <link href="http://arxiv.org/abs/1608.04644v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04644v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03147v1</id>
    <updated>2016-09-11T10:19:36Z</updated>
    <published>2016-09-11T10:19:36Z</published>
    <title>SDSS-DR12 Bulk Stellar Spectral Classification: Artificial Neural
  Networks Approach</title>
    <summary>  This paper explores the application of Probabilistic Neural Network (PNN),
Support Vector Machine (SVM) and Kmeans clustering as tools for automated
classification of massive stellar spectra.
</summary>
    <author>
      <name>S. Kheirdastan</name>
    </author>
    <author>
      <name>M. Bazarghan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10509-016-2880-3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10509-016-2880-3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 6 figures in Astrophysics and Space Science,2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.03147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.09060v2</id>
    <updated>2017-11-22T16:06:06Z</updated>
    <published>2016-09-28T19:59:56Z</published>
    <title>Machine Learning Topological States</title>
    <summary>  Artificial neural networks and machine learning have now reached a new era
after several decades of improvement where applications are to explode in many
fields of science, industry, and technology. Here, we use artificial neural
networks to study an intriguing phenomenon in quantum physics--- the
topological phases of matter. We find that certain topological states, either
symmetry-protected or with intrinsic topological order, can be represented with
classical artificial neural networks. This is demonstrated by using three
concrete spin systems, the one-dimensional (1D) symmetry-protected topological
cluster state and the 2D and 3D toric code states with intrinsic topological
orders. For all three cases we show rigorously that the topological ground
states can be represented by short-range neural networks in an \textit{exact}
and \textit{efficient} fashion---the required number of hidden neurons is as
small as the number of physical spins and the number of parameters scales only
\textit{linearly} with the system size. For the 2D toric-code model, we find
that the proposed short-range neural networks can describe the excited states
with abelain anyons and their nontrivial mutual statistics as well. In
addition, by using reinforcement learning we show that neural networks are
capable of finding the topological ground states of non-integrable Hamiltonians
with strong interactions and studying their topological phase transitions. Our
results demonstrate explicitly the exceptional power of neural networks in
describing topological quantum states, and at the same time provide valuable
guidance to machine learning of topological phases in generic lattice models.
</summary>
    <author>
      <name>Dong-Ling Deng</name>
    </author>
    <author>
      <name>Xiaopeng Li</name>
    </author>
    <author>
      <name>S. Das Sarma</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevB.96.195145</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevB.96.195145" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 7 figures, accepted for publication in Phys. Rev. B</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. B 96, 195145 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1609.09060v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09060v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.str-el" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.05189v1</id>
    <updated>2017-11-14T16:53:39Z</updated>
    <published>2017-11-14T16:53:39Z</published>
    <title>CryptoDL: Deep Neural Networks over Encrypted Data</title>
    <summary>  Machine learning algorithms based on deep neural networks have achieved
remarkable results and are being extensively used in different domains.
However, the machine learning algorithms requires access to raw data which is
often privacy sensitive. To address this issue, we develop new techniques to
provide solutions for running deep neural networks over encrypted data. In this
paper, we develop new techniques to adopt deep neural networks within the
practical limitation of current homomorphic encryption schemes. More
specifically, we focus on classification of the well-known convolutional neural
networks (CNN). First, we design methods for approximation of the activation
functions commonly used in CNNs (i.e. ReLU, Sigmoid, and Tanh) with low degree
polynomials which is essential for efficient homomorphic encryption schemes.
Then, we train convolutional neural networks with the approximation polynomials
instead of original activation functions and analyze the performance of the
models. Finally, we implement convolutional neural networks over encrypted data
and measure performance of the models. Our experimental results validate the
soundness of our approach with several convolutional neural networks with
varying number of layers and structures. When applied to the MNIST optical
character recognition tasks, our approach achieves 99.52\% accuracy which
significantly outperforms the state-of-the-art solutions and is very close to
the accuracy of the best non-private version, 99.77\%. Also, it can make close
to 164000 predictions per hour. We also applied our approach to CIFAR-10, which
is much more complex compared to MNIST, and were able to achieve 91.5\%
accuracy with approximation polynomials used as activation functions. These
results show that CryptoDL provides efficient, accurate and scalable
privacy-preserving predictions.
</summary>
    <author>
      <name>Ehsan Hesamifard</name>
    </author>
    <author>
      <name>Hassan Takabi</name>
    </author>
    <author>
      <name>Mehdi Ghasemi</name>
    </author>
    <link href="http://arxiv.org/abs/1711.05189v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.05189v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.00171v1</id>
    <updated>2017-12-30T18:11:59Z</updated>
    <published>2017-12-30T18:11:59Z</published>
    <title>PAC-Bayesian Margin Bounds for Convolutional Neural Networks - Technical
  Report</title>
    <summary>  Recently the generalisation error of deep neural networks has been analysed
through the PAC-Bayesian framework, for the case of fully connected layers. We
adapt this approach to the convolutional setting.
</summary>
    <author>
      <name>Pitas Konstantinos</name>
    </author>
    <author>
      <name>Mike Davies</name>
    </author>
    <author>
      <name>Pierre Vandergheynst</name>
    </author>
    <link href="http://arxiv.org/abs/1801.00171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.00171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.04928v1</id>
    <updated>2018-01-15T00:51:29Z</updated>
    <published>2018-01-15T00:51:29Z</published>
    <title>Leapfrogging for parallelism in deep neural networks</title>
    <summary>  We present a technique, which we term leapfrogging, to parallelize back-
propagation in deep neural networks. We show that this technique yields a
savings of $1-1/k$ of a dominant term in backpropagation, where k is the number
of threads (or gpus).
</summary>
    <author>
      <name>Yatin Saraiya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.04928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.04928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.05387v1</id>
    <updated>2018-01-16T17:47:13Z</updated>
    <published>2018-01-16T17:47:13Z</published>
    <title>StressedNets: Efficient Feature Representations via Stress-induced
  Evolutionary Synthesis of Deep Neural Networks</title>
    <summary>  The computational complexity of leveraging deep neural networks for
extracting deep feature representations is a significant barrier to its
widespread adoption, particularly for use in embedded devices. One particularly
promising strategy to addressing the complexity issue is the notion of
evolutionary synthesis of deep neural networks, which was demonstrated to
successfully produce highly efficient deep neural networks while retaining
modeling performance. Here, we further extend upon the evolutionary synthesis
strategy for achieving efficient feature extraction via the introduction of a
stress-induced evolutionary synthesis framework, where stress signals are
imposed upon the synapses of a deep neural network during training to induce
stress and steer the synthesis process towards the production of more efficient
deep neural networks over successive generations and improved model fidelity at
a greater efficiency. The proposed stress-induced evolutionary synthesis
approach is evaluated on a variety of different deep neural network
architectures (LeNet5, AlexNet, and YOLOv2) on different tasks (object
classification and object detection) to synthesize efficient StressedNets over
multiple generations. Experimental results demonstrate the efficacy of the
proposed framework to synthesize StressedNets with significant improvement in
network architecture efficiency (e.g., 40x for AlexNet and 33x for YOLOv2) and
speed improvements (e.g., 5.5x inference speed-up for YOLOv2 on an Nvidia Tegra
X1 mobile processor).
</summary>
    <author>
      <name>Mohammad Javad Shafiee</name>
    </author>
    <author>
      <name>Brendan Chwyl</name>
    </author>
    <author>
      <name>Francis Li</name>
    </author>
    <author>
      <name>Rongyan Chen</name>
    </author>
    <author>
      <name>Michelle Karg</name>
    </author>
    <author>
      <name>Christian Scharfenberger</name>
    </author>
    <author>
      <name>Alexander Wong</name>
    </author>
    <link href="http://arxiv.org/abs/1801.05387v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.05387v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.5721v2</id>
    <updated>2012-02-17T20:18:44Z</updated>
    <published>2012-01-27T08:37:14Z</published>
    <title>Short-term synaptic facilitation improves information retrieval in noisy
  neural networks</title>
    <summary>  Short-term synaptic depression and facilitation have been found to greatly
influence the performance of autoassociative neural networks. However, only
partial results, focused for instance on the computation of the maximum storage
capacity at zero temperature, have been obtained to date. In this work, we
extended the study of the effect of these synaptic mechanisms on
autoassociative neural networks to more realistic and general conditions,
including the presence of noise in the system. In particular, we characterized
the behavior of the system by means of its phase diagrams, and we concluded
that synaptic facilitation significantly enlarges the region of good retrieval
performance of the network. We also found that networks with facilitating
synapses may have critical temperatures substantially higher than those of
standard autoassociative networks, thus allowing neural networks to perform
better under high-noise conditions.
</summary>
    <author>
      <name>J. F. Mejias</name>
    </author>
    <author>
      <name>B. Hernandez-Gomez</name>
    </author>
    <author>
      <name>J. J. Torres</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1209/0295-5075/97/48008</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1209/0295-5075/97/48008" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, to appear in EPL</arxiv:comment>
    <link href="http://arxiv.org/abs/1201.5721v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.5721v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.01008v1</id>
    <updated>2015-08-05T09:11:06Z</updated>
    <published>2015-08-05T09:11:06Z</published>
    <title>INsight: A Neuromorphic Computing System for Evaluation of Large Neural
  Networks</title>
    <summary>  Deep neural networks have been demonstrated impressive results in various
cognitive tasks such as object detection and image classification. In order to
execute large networks, Von Neumann computers store the large number of weight
parameters in external memories, and processing elements are timed-shared,
which leads to power-hungry I/O operations and processing bottlenecks. This
paper describes a neuromorphic computing system that is designed from the
ground up for the energy-efficient evaluation of large-scale neural networks.
The computing system consists of a non-conventional compiler, a neuromorphic
architecture, and a space-efficient microarchitecture that leverages existing
integrated circuit design methodologies. The compiler factorizes a trained,
feedforward network into a sparsely connected network, compresses the weights
linearly, and generates a time delay neural network reducing the number of
connections. The connections and units in the simplified network are mapped to
silicon synapses and neurons. We demonstrate an implementation of the
neuromorphic computing system based on a field-programmable gate array that
performs the MNIST hand-written digit classification with 97.64% accuracy.
</summary>
    <author>
      <name>Jaeyong Chung</name>
    </author>
    <author>
      <name>Taehwan Shin</name>
    </author>
    <author>
      <name>Yongshin Kang</name>
    </author>
    <link href="http://arxiv.org/abs/1508.01008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.01008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.06071v1</id>
    <updated>2016-01-22T16:59:01Z</updated>
    <published>2016-01-22T16:59:01Z</published>
    <title>Bitwise Neural Networks</title>
    <summary>  Based on the assumption that there exists a neural network that efficiently
represents a set of Boolean functions between all binary inputs and outputs, we
propose a process for developing and deploying neural networks whose weight
parameters, bias terms, input, and intermediate hidden layer output signals,
are all binary-valued, and require only basic bit logic for the feedforward
pass. The proposed Bitwise Neural Network (BNN) is especially suitable for
resource-constrained environments, since it replaces either floating or
fixed-point arithmetic with significantly more efficient bitwise operations.
Hence, the BNN requires for less spatial complexity, less memory bandwidth, and
less power consumption in hardware. In order to design such networks, we
propose to add a few training schemes, such as weight compression and noisy
backpropagation, which result in a bitwise network that performs almost as well
as its corresponding real-valued network. We test the proposed network on the
MNIST dataset, represented using binary features, and show that BNNs result in
competitive performance while offering dramatic computational savings.
</summary>
    <author>
      <name>Minje Kim</name>
    </author>
    <author>
      <name>Paris Smaragdis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper was presented at the International Conference on Machine
  Learning (ICML) Workshop on Resource-Efficient Machine Learning, Lille,
  France, Jul. 6-11, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.06071v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.06071v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.01664v1</id>
    <updated>2017-04-05T23:04:43Z</updated>
    <published>2017-04-05T23:04:43Z</published>
    <title>The Relative Performance of Ensemble Methods with Deep Convolutional
  Neural Networks for Image Classification</title>
    <summary>  Artificial neural networks have been successfully applied to a variety of
machine learning tasks, including image recognition, semantic segmentation, and
machine translation. However, few studies fully investigated ensembles of
artificial neural networks. In this work, we investigated multiple widely used
ensemble methods, including unweighted averaging, majority voting, the Bayes
Optimal Classifier, and the (discrete) Super Learner, for image recognition
tasks, with deep neural networks as candidate algorithms. We designed several
experiments, with the candidate algorithms being the same network structure
with different model checkpoints within a single training process, networks
with same structure but trained multiple times stochastically, and networks
with different structure. In addition, we further studied the over-confidence
phenomenon of the neural networks, as well as its impact on the ensemble
methods. Across all of our experiments, the Super Learner achieved best
performance among all the ensemble methods in this study.
</summary>
    <author>
      <name>Cheng Ju</name>
    </author>
    <author>
      <name>Aur√©lien Bibaut</name>
    </author>
    <author>
      <name>Mark J. van der Laan</name>
    </author>
    <link href="http://arxiv.org/abs/1704.01664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.01664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.09774v1</id>
    <updated>2018-01-29T21:45:05Z</updated>
    <published>2018-01-29T21:45:05Z</published>
    <title>On Psychoacoustically Weighted Cost Functions Towards Resource-Efficient
  Deep Neural Networks for Speech Denoising</title>
    <summary>  We present a psychoacoustically enhanced cost function to balance network
complexity and perceptual performance of deep neural networks for speech
denoising. While training the network, we utilize perceptual weights added to
the ordinary mean-squared error to emphasize contribution from frequency bins
which are most audible while ignoring error from inaudible bins. To generate
the weights, we employ psychoacoustic models to compute the global masking
threshold from the clean speech spectra. We then evaluate the speech denoising
performance of our perceptually guided neural network by using both objective
and perceptual sound quality metrics, testing on various network structures
ranging from shallow and narrow ones to deep and wide ones. The experimental
results showcase our method as a valid approach for infusing perceptual
significance to deep neural network operations. In particular, the more
perceptually sensible enhancement in performance seen by simple neural network
topologies proves that the proposed method can lead to resource-efficient
speech denoising implementations in small devices without degrading the
perceived signal fidelity.
</summary>
    <author>
      <name>Kai Zhen</name>
    </author>
    <author>
      <name>Aswin Sivaraman</name>
    </author>
    <author>
      <name>Jongmo Sung</name>
    </author>
    <author>
      <name>Minje Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.09774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.09774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.00406v1</id>
    <updated>2018-02-23T06:11:37Z</updated>
    <published>2018-02-23T06:11:37Z</published>
    <title>Left ventricle segmentation By modelling uncertainty in prediction of
  deep convolutional neural networks and adaptive thresholding inference</title>
    <summary>  Deep neural networks have shown great achievements in solving complex
problems. However, there are fundamental problems that limit their real world
applications. Lack of measurable criteria for estimating uncertainty in the
network outputs is one of these problems. In this paper, we address this
limitation by introducing deformation to the network input and measuring the
level of stability in the network's output. We calculate simple random
transformations to estimate the prediction uncertainty of deep convolutional
neural networks. For a real use-case, we apply this method to left ventricle
segmentation in MRI cardiac images. We also propose an adaptive thresholding
method to consider the deep neural network uncertainty. Experimental results
demonstrate state-of-the-art performance and highlight the capabilities of
simple methods in conjunction with deep neural networks.
</summary>
    <author>
      <name>Alireza Norouzi</name>
    </author>
    <author>
      <name>Ali Emami</name>
    </author>
    <author>
      <name>S. M. Reza Soroushmehr</name>
    </author>
    <author>
      <name>Nader Karimi</name>
    </author>
    <author>
      <name>Shadrokh Samavi</name>
    </author>
    <author>
      <name>Kayvan Najarian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.00406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.00406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.02444v1</id>
    <updated>2015-02-09T11:45:02Z</updated>
    <published>2015-02-09T11:45:02Z</published>
    <title>On the Dynamics of a Recurrent Hopfield Network</title>
    <summary>  In this research paper novel real/complex valued recurrent Hopfield Neural
Network (RHNN) is proposed. The method of synthesizing the energy landscape of
such a network and the experimental investigation of dynamics of Recurrent
Hopfield Network is discussed. Parallel modes of operation (other than fully
parallel mode) in layered RHNN is proposed. Also, certain potential
applications are proposed.
</summary>
    <author>
      <name>Rama Garimella</name>
    </author>
    <author>
      <name>Berkay Kicanaoglu</name>
    </author>
    <author>
      <name>Moncef Gabbouj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures, 1 table, submitted to IJCNN-2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.02444v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.02444v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.03130v1</id>
    <updated>2017-11-08T19:30:15Z</updated>
    <published>2017-11-08T19:30:15Z</published>
    <title>EnergyNet: Energy-based Adaptive Structural Learning of Artificial
  Neural Network Architectures</title>
    <summary>  We present E NERGY N ET , a new framework for analyzing and building
artificial neural network architectures. Our approach adaptively learns the
structure of the networks in an unsupervised manner. The methodology is based
upon the theoretical guarantees of the energy function of restricted Boltzmann
machines (RBM) of infinite number of nodes. We present experimental results to
show that the final network adapts to the complexity of a given problem.
</summary>
    <author>
      <name>Gus Kristiansen</name>
    </author>
    <author>
      <name>Xavi Gonzalvo</name>
    </author>
    <link href="http://arxiv.org/abs/1711.03130v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.03130v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0003215v2</id>
    <updated>2000-06-21T14:37:18Z</updated>
    <published>2000-03-13T17:12:50Z</published>
    <title>Topological Evolution of Dynamical Networks: Global Criticality from
  Local Dynamics</title>
    <summary>  We evolve network topology of an asymmetrically connected threshold network
by a simple local rewiring rule: quiet nodes grow links, active nodes lose
links. This leads to convergence of the average connectivity of the network
towards the critical value $K_c =2$ in the limit of large system size $N$. How
this principle could generate self-organization in natural complex systems is
discussed for two examples: neural networks and regulatory networks in the
genome.
</summary>
    <author>
      <name>Stefan Bornholdt</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Kiel University</arxiv:affiliation>
    </author>
    <author>
      <name>Thimo Rohlf</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Kiel University</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevLett.84.6114</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevLett.84.6114" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages RevTeX, 4 figures PostScript, revised version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. Lett. 84 (2000) 6114-6117</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0003215v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0003215v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0608702v1</id>
    <updated>2006-08-31T06:30:16Z</updated>
    <published>2006-08-31T06:30:16Z</published>
    <title>Strong Effects of Network Architecture in the Entrainment of Coupled
  Oscillator Systems</title>
    <summary>  Entrainment of randomly coupled oscillator networks by periodic external
forcing applied to a subset of elements is numerically and analytically
investigated. For a large class of interaction functions, we find that the
entrainment window with a tongue shape becomes exponentially narrow for
networks with higher hierarchical organization. However, the entrainment is
significantly facilitated if the networks are directionally biased, i.e.,
closer to the feedforward networks. Furthermore, we show that the networks with
high entrainment ability can be constructed by evolutionary optimization
processes. The neural network structure of the master clock of the circadian
rhythm in mammals is discussed from the viewpoint of our results.
</summary>
    <author>
      <name>Hiroshi Kori</name>
    </author>
    <author>
      <name>Alexander S. Mikhailov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.74.066115</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.74.066115" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 11 figures, RevTeX</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0608702v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0608702v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.4682v1</id>
    <updated>2010-11-21T17:39:27Z</updated>
    <published>2010-11-21T17:39:27Z</published>
    <title>Analysis of attractor distances in Random Boolean Networks</title>
    <summary>  We study the properties of the distance between attractors in Random Boolean
Networks, a prominent model of genetic regulatory networks. We define three
distance measures, upon which attractor distance matrices are constructed and
their main statistic parameters are computed. The experimental analysis shows
that ordered networks have a very clustered set of attractors, while chaotic
networks' attractors are scattered; critical networks show, instead, a pattern
with characteristics of both ordered and chaotic networks.
</summary>
    <author>
      <name>Andrea Roli</name>
    </author>
    <author>
      <name>Stefano Benedettini</name>
    </author>
    <author>
      <name>Roberto Serra</name>
    </author>
    <author>
      <name>Marco Villani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures. Presented at WIRN 2010 - Italian workshop on
  neural networks, May 2010. To appear in a volume published by IOS Press</arxiv:comment>
    <link href="http://arxiv.org/abs/1011.4682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.4682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.07279v1</id>
    <updated>2017-08-24T05:24:26Z</updated>
    <published>2017-08-24T05:24:26Z</published>
    <title>Combining Discrete and Neural Features for Sequence Labeling</title>
    <summary>  Neural network models have recently received heated research attention in the
natural language processing community. Compared with traditional models with
discrete features, neural models have two main advantages. First, they take
low-dimensional, real-valued embedding vectors as inputs, which can be trained
over large raw data, thereby addressing the issue of feature sparsity in
discrete models. Second, deep neural networks can be used to automatically
combine input features, and including non-local features that capture semantic
patterns that cannot be expressed using discrete indicator features. As a
result, neural network models have achieved competitive accuracies compared
with the best discrete models for a range of NLP tasks.
  On the other hand, manual feature templates have been carefully investigated
for most NLP tasks over decades and typically cover the most useful indicator
pattern for solving the problems. Such information can be complementary the
features automatically induced from neural networks, and therefore combining
discrete and neural features can potentially lead to better accuracy compared
with models that leverage discrete or neural features only.
  In this paper, we systematically investigate the effect of discrete and
neural feature combination for a range of fundamental NLP tasks based on
sequence labeling, including word segmentation, POS tagging and named entity
recognition for Chinese and English, respectively. Our results on standard
benchmarks show that state-of-the-art neural models can give accuracies
comparable to the best discrete models in the literature for most tasks and
combing discrete and neural features unanimously yield better results.
</summary>
    <author>
      <name>Jie Yang</name>
    </author>
    <author>
      <name>Zhiyang Teng</name>
    </author>
    <author>
      <name>Meishan Zhang</name>
    </author>
    <author>
      <name>Yue Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by International Conference on Computational Linguistics and
  Intelligent Text Processing (CICLing) 2016, April</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.07279v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.07279v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.07620v1</id>
    <updated>2016-01-28T01:48:42Z</updated>
    <published>2016-01-28T01:48:42Z</published>
    <title>Using Firing-Rate Dynamics to Train Recurrent Networks of Spiking Model
  Neurons</title>
    <summary>  Recurrent neural networks are powerful tools for understanding and modeling
computation and representation by populations of neurons. Continuous-variable
or "rate" model networks have been analyzed and applied extensively for these
purposes. However, neurons fire action potentials, and the discrete nature of
spiking is an important feature of neural circuit dynamics. Despite significant
advances, training recurrently connected spiking neural networks remains a
challenge. We present a procedure for training recurrently connected spiking
networks to generate dynamical patterns autonomously, to produce complex
temporal outputs based on integrating network input, and to model physiological
data. Our procedure makes use of a continuous-variable network to identify
targets for training the inputs to the spiking model neurons. Surprisingly, we
are able to construct spiking networks that duplicate tasks performed by
continuous-variable networks with only a relatively minor expansion in the
number of neurons. Our approach provides a novel view of the significance and
appropriate use of "firing rate" models, and it is a useful approach for
building model spiking networks that can be used to address important questions
about representation and computation in neural systems.
</summary>
    <author>
      <name>Brian DePasquale</name>
    </author>
    <author>
      <name>Mark M. Churchland</name>
    </author>
    <author>
      <name>L. F. Abbott</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.07620v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.07620v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05571v1</id>
    <updated>2016-12-16T17:57:15Z</updated>
    <published>2016-12-16T17:57:15Z</published>
    <title>Delta Networks for Optimized Recurrent Network Computation</title>
    <summary>  Many neural networks exhibit stability in their activation patterns over time
in response to inputs from sensors operating under real-world conditions. By
capitalizing on this property of natural signals, we propose a Recurrent Neural
Network (RNN) architecture called a delta network in which each neuron
transmits its value only when the change in its activation exceeds a threshold.
The execution of RNNs as delta networks is attractive because their states must
be stored and fetched at every timestep, unlike in convolutional neural
networks (CNNs). We show that a naive run-time delta network implementation
offers modest improvements on the number of memory accesses and computes, but
optimized training techniques confer higher accuracy at higher speedup. With
these optimizations, we demonstrate a 9X reduction in cost with negligible loss
of accuracy for the TIDIGITS audio digit recognition benchmark. Similarly, on
the large Wall Street Journal speech recognition benchmark even existing
networks can be greatly accelerated as delta networks, and a 5.7x improvement
with negligible loss of accuracy can be obtained through training. Finally, on
an end-to-end CNN trained for steering angle prediction in a driving dataset,
the RNN cost can be reduced by a substantial 100X.
</summary>
    <author>
      <name>Daniel Neil</name>
    </author>
    <author>
      <name>Jun Haeng Lee</name>
    </author>
    <author>
      <name>Tobi Delbruck</name>
    </author>
    <author>
      <name>Shih-Chii Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1612.05571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.02615v2</id>
    <updated>2017-11-19T17:34:35Z</updated>
    <published>2017-10-07T01:22:24Z</published>
    <title>A Transfer-Learning Approach for Accelerated MRI using Deep Neural
  Networks</title>
    <summary>  Neural network based architectures have recently been proposed for
reconstruction of undersampled MR acquisitions. A deep network containing many
free parameters is typically trained using a relatively large set of
fully-sampled MRI data, and later used for on-line reconstruction of
undersampled data. Ideally network performance should be optimized by drawing
the training and testing data from the same domain. In practice, however, large
datasets comprising hundreds of subjects scanned under a common protocol are
rare. Here, we propose a transfer-learning approach to address the problem of
data scarcity in training deep networks for accelerated MRI. The proposed
approach trains neural networks using thousands of samples from a public
dataset of natural images (ImageNet). The network is then fine-tuned using only
few tens of MR images acquired in the testing domain (T1- or T2-weighted MRI).
The ImageNet-trained network yields nearly identical reconstructions to
networks trained directly in the testing domain using thousands of MR images,
and it outperforms conventional compressed sensing reconstructions in terms of
image quality. The proposed approach might facilitate the use of neural
networks for MRI reconstruction without the need for collection of extensive
imaging datasets.
</summary>
    <author>
      <name>Salman Ul Hassan Dar</name>
    </author>
    <author>
      <name>Tolga √áukur</name>
    </author>
    <link href="http://arxiv.org/abs/1710.02615v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.02615v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.00282v2</id>
    <updated>2018-01-11T05:36:36Z</updated>
    <published>2017-12-31T13:26:20Z</published>
    <title>Using Deep Neural Network Approximate Bayesian Network</title>
    <summary>  We present a new method to approximate posterior probabilities of Bayesian
Network using Deep Neural Network. Experiment results on several public
Bayesian Network datasets shows that Deep Neural Network is capable of learning
joint probability distri- bution of Bayesian Network by learning from a few
observation and posterior probability distribution pairs with high accuracy.
Compared with traditional approximate method likelihood weighting sampling
algorithm, our method is much faster and gains higher accuracy in medium sized
Bayesian Network. Another advantage of our method is that our method can be
parallelled much easier in GPU without extra effort. We also ex- plored the
connection between the accuracy of our model and the number of training
examples. The result shows that our model saturate as the number of training
examples grow and we don't need many training examples to get reasonably good
result. Another contribution of our work is that we have shown discriminative
model like Deep Neural Network can approximate generative model like Bayesian
Network.
</summary>
    <author>
      <name>Jie Jia</name>
    </author>
    <author>
      <name>Honggang Zhou</name>
    </author>
    <author>
      <name>Yunchun Li</name>
    </author>
    <link href="http://arxiv.org/abs/1801.00282v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.00282v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.3081v1</id>
    <updated>2010-03-16T05:13:41Z</updated>
    <published>2010-03-16T05:13:41Z</published>
    <title>Optimal hierarchical modular topologies for producing limited sustained
  activation of neural networks</title>
    <summary>  An essential requirement for the representation of functional patterns in
complex neural networks, such as the mammalian cerebral cortex, is the
existence of stable regimes of network activation, typically arising from a
limited parameter range. In this range of limited sustained activity (LSA), the
activity of neural populations in the network persists between the extremes of
either quickly dying out or activating the whole network. Hierarchical modular
networks were previously found to show a wider parameter range for LSA than
random or small-world networks not possessing hierarchical organization or
multiple modules. Here we explored how variation in the number of hierarchical
levels and modules per level influenced network dynamics and occurrence of LSA.
We tested hierarchical configurations of different network sizes, approximating
the large-scale networks linking cortical columns in one hemisphere of the rat,
cat, or macaque monkey brain. Scaling of the network size affected the number
of hierarchical levels and modules in the optimal networks, also depending on
whether global edge density or the numbers of connections per node were kept
constant. For constant edge density, only few network configurations,
possessing an intermediate number of levels and a large number of modules, led
to a large range of LSA independent of brain size. For a constant number of
node connections, there was a trend for optimal configurations in larger-size
networks to possess a larger number of hierarchical levels or more modules.
These results may help to explain the trend to greater network complexity
apparent in larger brains and may indicate that this complexity is required for
maintaining stable levels of neural activation.
</summary>
    <author>
      <name>Marcus Kaiser</name>
    </author>
    <author>
      <name>Claus C. Hilgetag</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3389/fninf.2010.00008</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3389/fninf.2010.00008" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Contribution to Frontiers in Neuroinformatics special issue on
  'Hierarchy and dynamics in neural networks'
  (http://frontiersin.org/neuroscience/neuroinformatics/specialtopics/29/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Kaiser M and Hilgetag CC (2010) Optimal hierarchical modular
  topologies for producing limited sustained activation of neural networks.
  Front. Neuroinform. 4:8</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.3081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.3081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0310050v4</id>
    <updated>2005-03-31T12:45:11Z</updated>
    <published>2003-10-27T14:27:14Z</published>
    <title>Feedforward Neural Networks with Diffused Nonlinear Weight Functions</title>
    <summary>  In this paper, feedforward neural networks are presented that have nonlinear
weight functions based on look--up tables, that are specially smoothed in a
regularization called the diffusion. The idea of such a type of networks is
based on the hypothesis that the greater number of adaptive parameters per a
weight function might reduce the total number of the weight functions needed to
solve a given problem. Then, if the computational complexity of a propagation
through a single such a weight function would be kept low, then the introduced
neural networks might possibly be relatively fast.
  A number of tests is performed, showing that the presented neural networks
may indeed perform better in some cases than the classic neural networks and a
number of other learning machines.
</summary>
    <author>
      <name>Artur Rataj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 7 figures. Corrected, some parts rewritten</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0310050v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0310050v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/q-bio/0310025v3</id>
    <updated>2003-10-21T22:07:29Z</updated>
    <published>2003-10-20T19:36:04Z</published>
    <title>Pattern Excitation-Based Processing: The Music of The Brain</title>
    <summary>  An approach to information processing based on the excitation of patterns of
activity by non-linear active resonators in response to their input patterns is
proposed. Arguments are presented to show that any computation performed by a
conventional Turing machine-based computer, called T-machine in this paper,
could also be performed by the pattern excitation-based machine, which will be
called P-machine. A realization of this processing scheme by neural networks is
discussed. In this realization, the role of the resonators is played by neural
pattern excitation networks, which are the neural circuits capable of exciting
different spatio-temporal patterns of activity in response to different inputs.
Learning in the neural pattern excitation networks is also considered. It is
shown that there is a duality between pattern excitation and pattern
recognition neural networks, which allows to create new pattern excitation
modes corresponding to recognizable input patterns, based on Hebbian learning
rules. Hierarchically organized, such networks can produce complex behavior.
Animal behavior, human language and thought are treated as examples produced by
such networks.
</summary>
    <author>
      <name>Lev Koyrakh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 7 figures, a reference corrected</arxiv:comment>
    <link href="http://arxiv.org/abs/q-bio/0310025v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/q-bio/0310025v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.2987v2</id>
    <updated>2010-05-26T08:55:11Z</updated>
    <published>2009-03-17T15:51:05Z</published>
    <title>Adaptive self-organization in a realistic neural network model</title>
    <summary>  Information processing in complex systems is often found to be maximally
efficient close to critical states associated with phase transitions. It is
therefore conceivable that also neural information processing operates close to
criticality. This is further supported by the observation of power-law
distributions, which are a hallmark of phase transitions. An important open
question is how neural networks could remain close to a critical point while
undergoing a continual change in the course of development, adaptation,
learning, and more. An influential contribution was made by Bornholdt and
Rohlf, introducing a generic mechanism of robust self-organized criticality in
adaptive networks. Here, we address the question whether this mechanism is
relevant for real neural networks. We show in a realistic model that
spike-time-dependent synaptic plasticity can self-organize neural networks
robustly toward criticality. Our model reproduces several empirical
observations and makes testable predictions on the distribution of synaptic
strength, relating them to the critical state of the network. These results
suggest that the interplay between dynamics and topology may be essential for
neural information processing.
</summary>
    <author>
      <name>Christian Meisel</name>
    </author>
    <author>
      <name>Thilo Gross</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.80.061917</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.80.061917" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 80, 061917 (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0903.2987v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.2987v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.1815v1</id>
    <updated>2009-12-09T17:47:49Z</updated>
    <published>2009-12-09T17:47:49Z</published>
    <title>Detection of Denial of Service Attacks against Domain Name System Using
  Neural Networks</title>
    <summary>  In this paper we introduce an intrusion detection system for Denial of
Service (DoS) attacks against Domain Name System (DNS). Our system architecture
consists of two most important parts: a statistical preprocessor and a neural
network classifier. The preprocessor extracts required statistical features in
a shorttime frame from traffic received by the target name server. We compared
three different neural networks for detecting and classifying different types
of DoS attacks. The proposed system is evaluated in a simulated network and
showed that the best performed neural network is a feed-forward backpropagation
with an accuracy of 99%.
</summary>
    <author>
      <name>Samaneh Rastegari</name>
    </author>
    <author>
      <name>M. Iqbal Saripan</name>
    </author>
    <author>
      <name>Mohd Fadlee A. Rasid</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues, IJCSI Volume 6,
  Issue 1, pp23-27, November 2009</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">S. Rastegari, M. I. Saripan and M. F. A. Rasid, "Detection of
  Denial of Service Attacks against Domain Name System Using Neural Networks",
  IJCSI, Volume 6, Issue 1, pp23-27, November 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0912.1815v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.1815v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.2770v4</id>
    <updated>2012-06-21T15:42:41Z</updated>
    <published>2012-02-13T15:48:04Z</published>
    <title>Multi-Level Error-Resilient Neural Networks with Learning</title>
    <summary>  The problem of neural network association is to retrieve a previously
memorized pattern from its noisy version using a network of neurons. An ideal
neural network should include three components simultaneously: a learning
algorithm, a large pattern retrieval capacity and resilience against noise.
Prior works in this area usually improve one or two aspects at the cost of the
third.
  Our work takes a step forward in closing this gap. More specifically, we show
that by forcing natural constraints on the set of learning patterns, we can
drastically improve the retrieval capacity of our neural network. Moreover, we
devise a learning algorithm whose role is to learn those patterns satisfying
the above mentioned constraints. Finally we show that our neural network can
cope with a fair amount of noise.
</summary>
    <author>
      <name>Amir Hesam Salavati</name>
    </author>
    <author>
      <name>Amin Karbasi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Part of this draft has been submitted to International Symposium on
  Information Theory (ISIT) 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1202.2770v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.2770v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.3134v1</id>
    <updated>2013-07-11T15:03:22Z</updated>
    <published>2013-07-11T15:03:22Z</published>
    <title>Air quality prediction using optimal neural networks with stochastic
  variables</title>
    <summary>  We apply recent methods in stochastic data analysis for discovering a set of
few stochastic variables that represent the relevant information on a
multivariate stochastic system, used as input for artificial neural networks
models for air quality forecast. We show that using these derived variables as
input variables for training the neural networks it is possible to
significantly reduce the amount of input variables necessary for the neural
network model, without considerably changing the predictive power of the model.
The reduced set of variables including these derived variables is therefore
proposed as optimal variable set for training neural networks models in
forecasting geophysical and weather properties. Finally, we briefly discuss
other possible applications of such optimized neural network models.
</summary>
    <author>
      <name>Ana Russo</name>
    </author>
    <author>
      <name>Frank Raischel</name>
    </author>
    <author>
      <name>Pedro G. Lind</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.atmosenv.2013.07.072</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.atmosenv.2013.07.072" rel="related"/>
    <link href="http://arxiv.org/abs/1307.3134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.3134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.ao-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ao-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.5417v1</id>
    <updated>2014-04-22T08:25:23Z</updated>
    <published>2014-04-22T08:25:23Z</published>
    <title>Attractor Metadynamics in Adapting Neural Networks</title>
    <summary>  Slow adaption processes, like synaptic and intrinsic plasticity, abound in
the brain and shape the landscape for the neural dynamics occurring on
substantially faster timescales. At any given time the network is characterized
by a set of internal parameters, which are adapting continuously, albeit
slowly. This set of parameters defines the number and the location of the
respective adiabatic attractors. The slow evolution of network parameters hence
induces an evolving attractor landscape, a process which we term attractor
metadynamics. We study the nature of the metadynamics of the attractor
landscape for several continuous-time autonomous model networks. We find both
first- and second-order changes in the location of adiabatic attractors and
argue that the study of the continuously evolving attractor landscape
constitutes a powerful tool for understanding the overall development of the
neural dynamics.
</summary>
    <author>
      <name>Claudius Gros</name>
    </author>
    <author>
      <name>Mathias Linkerhand</name>
    </author>
    <author>
      <name>Valentin Walther</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Artificial Neural Networks and Machine Learning-ICANN 2014 , S.
  Wermter et al. (Eds), pp. 65-72. Springer (2014)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.5417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.5417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.0595v1</id>
    <updated>2014-12-01T19:12:54Z</updated>
    <published>2014-12-01T19:12:54Z</published>
    <title>Scalability and Optimization Strategies for GPU Enhanced Neural Networks
  (GeNN)</title>
    <summary>  Simulation of spiking neural networks has been traditionally done on
high-performance supercomputers or large-scale clusters. Utilizing the parallel
nature of neural network computation algorithms, GeNN (GPU Enhanced Neural
Network) provides a simulation environment that performs on General Purpose
NVIDIA GPUs with a code generation based approach. GeNN allows the users to
design and simulate neural networks by specifying the populations of neurons at
different stages, their synapse connection densities and the model of
individual neurons. In this report we describe work on how to scale synaptic
weights based on the configuration of the user-defined network to ensure
sufficient spiking and subsequent effective learning. We also discuss
optimization strategies particular to GPU computing: sparse representation of
synapse connections and occupancy based block-size determination.
</summary>
    <author>
      <name>Naresh Balaji</name>
    </author>
    <author>
      <name>Esin Yavuz</name>
    </author>
    <author>
      <name>Thomas Nowotny</name>
    </author>
    <link href="http://arxiv.org/abs/1412.0595v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.0595v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.09308v2</id>
    <updated>2015-11-10T20:08:41Z</updated>
    <published>2015-09-30T19:39:20Z</published>
    <title>Fast Algorithms for Convolutional Neural Networks</title>
    <summary>  Deep convolutional neural networks take GPU days of compute time to train on
large data sets. Pedestrian detection for self driving cars requires very low
latency. Image recognition for mobile phones is constrained by limited
processing resources. The success of convolutional neural networks in these
situations is limited by how fast we can compute them. Conventional FFT based
convolution is fast for large filters, but state of the art convolutional
neural networks use small, 3x3 filters. We introduce a new class of fast
algorithms for convolutional neural networks using Winograd's minimal filtering
algorithms. The algorithms compute minimal complexity convolution over small
tiles, which makes them fast with small filters and small batch sizes. We
benchmark a GPU implementation of our algorithm with the VGG network and show
state of the art throughput at batch sizes from 1 to 64.
</summary>
    <author>
      <name>Andrew Lavin</name>
    </author>
    <author>
      <name>Scott Gray</name>
    </author>
    <link href="http://arxiv.org/abs/1509.09308v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.09308v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; F.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.03651v2</id>
    <updated>2016-10-13T07:11:46Z</updated>
    <published>2016-01-14T16:30:41Z</published>
    <title>Improved Relation Classification by Deep Recurrent Neural Networks with
  Data Augmentation</title>
    <summary>  Nowadays, neural networks play an important role in the task of relation
classification. By designing different neural architectures, researchers have
improved the performance to a large extent in comparison with traditional
methods. However, existing neural networks for relation classification are
usually of shallow architectures (e.g., one-layer convolutional neural networks
or recurrent networks). They may fail to explore the potential representation
space in different abstraction levels. In this paper, we propose deep recurrent
neural networks (DRNNs) for relation classification to tackle this challenge.
Further, we propose a data augmentation method by leveraging the directionality
of relations. We evaluated our DRNNs on the SemEval-2010 Task~8, and achieve an
F1-score of 86.1%, outperforming previous state-of-the-art recorded results.
</summary>
    <author>
      <name>Yan Xu</name>
    </author>
    <author>
      <name>Ran Jia</name>
    </author>
    <author>
      <name>Lili Mou</name>
    </author>
    <author>
      <name>Ge Li</name>
    </author>
    <author>
      <name>Yunchuan Chen</name>
    </author>
    <author>
      <name>Yangyang Lu</name>
    </author>
    <author>
      <name>Zhi Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by COLING-16</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.03651v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.03651v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.04010v2</id>
    <updated>2017-05-18T09:15:26Z</updated>
    <published>2016-11-12T15:59:22Z</published>
    <title>Multi-Language Identification Using Convolutional Recurrent Neural
  Network</title>
    <summary>  Language Identification, being an important aspect of Automatic Speaker
Recognition has had many changes and new approaches to ameliorate performance
over the last decade. We compare the performance of using audio spectrum in the
log scale and using Polyphonic sound sequences from raw audio samples to train
the neural network and to classify speech as either English or Spanish. To
achieve this, we use the novel approach of using a Convolutional Recurrent
Neural Network using Long Short Term Memory (LSTM) or a Gated Recurrent Unit
(GRU) for forward propagation of the neural network. Our hypothesis is that the
performance of using polyphonic sound sequence as features and both LSTM and
GRU as the gating mechanisms for the neural network outperform the traditional
MFCC features using a unidirectional Deep Neural Network.
</summary>
    <author>
      <name>Vrishabh Ajay Lakhani</name>
    </author>
    <author>
      <name>Rohan Mahadev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Further experiments were performed on the model using LibriVox speech
  dataset and it was found that a Time Distributed CRNN model performed better
  and represented our initial ideas about the speaker recognition task better.
  The dataset contains speech in three languages - English, Spanish and Czech.
  A report on our findings along with experimental results will be published
  soon</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.04010v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.04010v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.08835v2</id>
    <updated>2017-05-31T17:15:34Z</updated>
    <published>2017-02-28T16:10:31Z</published>
    <title>Deep Forest: Towards An Alternative to Deep Neural Networks</title>
    <summary>  In this paper, we propose gcForest, a decision tree ensemble approach with
performance highly competitive to deep neural networks. In contrast to deep
neural networks which require great effort in hyper-parameter tuning, gcForest
is much easier to train. Actually, even when gcForest is applied to different
data from different domains, excellent performance can be achieved by almost
same settings of hyper-parameters. The training process of gcForest is
efficient and scalable. In our experiments its training time running on a PC is
comparable to that of deep neural networks running with GPU facilities, and the
efficiency advantage may be more apparent because gcForest is naturally apt to
parallel implementation. Furthermore, in contrast to deep neural networks which
require large-scale training data, gcForest can work well even when there are
only small-scale training data. Moreover, as a tree-based approach, gcForest
should be easier for theoretical analysis than deep neural networks.
</summary>
    <author>
      <name>Zhi-Hua Zhou</name>
    </author>
    <author>
      <name>Ji Feng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IJCAI 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.08835v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.08835v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.10371v2</id>
    <updated>2017-12-06T19:10:46Z</updated>
    <published>2017-03-30T09:10:09Z</published>
    <title>Born to Learn: the Inspiration, Progress, and Future of Evolved Plastic
  Artificial Neural Networks</title>
    <summary>  Biological plastic neural networks are systems of extraordinary computational
capabilities shaped by evolution, development, and lifetime learning. The
interplay of these elements leads to the emergence of adaptive behavior and
intelligence. Inspired by such intricate natural phenomena, Evolved Plastic
Artificial Neural Networks (EPANNs) use simulated evolution in-silico to breed
plastic neural networks with a large variety of dynamics, architectures, and
plasticity rules: these artificial systems are composed of inputs, outputs, and
plastic components that change in response to experiences in an environment.
These systems may autonomously discover novel adaptive algorithms, and lead to
hypotheses on the emergence of biological adaptation. EPANNs have seen
considerable progress over the last two decades. Current scientific and
technological advances in artificial neural networks are now setting the
conditions for radically new approaches and results. In particular, the
limitations of hand-designed networks could be overcome by more flexible and
innovative solutions. This paper brings together a variety of inspiring ideas
that define the field of EPANNs. The main methods and results are reviewed.
Finally, new opportunities and developments are presented.
</summary>
    <author>
      <name>Andrea Soltoggio</name>
    </author>
    <author>
      <name>Kenneth O. Stanley</name>
    </author>
    <author>
      <name>Sebastian Risi</name>
    </author>
    <link href="http://arxiv.org/abs/1703.10371v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.10371v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.05892v1</id>
    <updated>2017-06-19T11:45:50Z</updated>
    <published>2017-06-19T11:45:50Z</published>
    <title>Identification of an Open-loop Plasma Vertical Position Using Fractional
  Order Dynamic Neural Network</title>
    <summary>  In order to identify complicated systems, more prominent and promising
methods are needed among which we may refer to fractional order differential
equations. The aim of this paper is to propose a fractional order nonlinear
model to predict the vertical position of a plasma column system in a Tokamak
by using real data from Damavand Tokamak. The system is identified based on a
newly introduced fractional order dynamic neural network. The proposed
fractional order dynamic neural network (FODNN) is an extension of the integer
order dynamic neural network that employs the so called fractional-order
operators. FODNN is implemented and comparison of the numerical simulation
results with experimental results shows that performance of the proposed method
by using fractional order neural network is preferred to the integer neural
network.
</summary>
    <author>
      <name>Z. Aslipour</name>
    </author>
    <author>
      <name>A. Yazdizadeh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages,8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.05892v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.05892v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.plasm-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.plasm-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09453v2</id>
    <updated>2017-07-04T17:03:20Z</updated>
    <published>2017-06-28T19:41:25Z</published>
    <title>Toward Computation and Memory Efficient Neural Network Acoustic Models
  with Binary Weights and Activations</title>
    <summary>  Neural network acoustic models have significantly advanced state of the art
speech recognition over the past few years. However, they are usually
computationally expensive due to the large number of matrix-vector
multiplications and nonlinearity operations. Neural network models also require
significant amounts of memory for inference because of the large model size.
For these two reasons, it is challenging to deploy neural network based speech
recognizers on resource-constrained platforms such as embedded devices. This
paper investigates the use of binary weights and activations for computation
and memory efficient neural network acoustic models. Compared to real-valued
weight matrices, binary weights require much fewer bits for storage, thereby
cutting down the memory footprint. Furthermore, with binary weights or
activations, the matrix-vector multiplications are turned into addition and
subtraction operations, which are computationally much faster and more energy
efficient for hardware platforms. In this paper, we study the applications of
binary weights and activations for neural network acoustic modeling, reporting
encouraging results on the WSJ and AMI corpora.
</summary>
    <author>
      <name>Liang Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.09453v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09453v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.05922v1</id>
    <updated>2017-07-19T02:29:49Z</updated>
    <published>2017-07-19T02:29:49Z</published>
    <title>Improving Output Uncertainty Estimation and Generalization in Deep
  Learning via Neural Network Gaussian Processes</title>
    <summary>  We propose a simple method that combines neural networks and Gaussian
processes. The proposed method can estimate the uncertainty of outputs and
flexibly adjust target functions where training data exist, which are
advantages of Gaussian processes. The proposed method can also achieve high
generalization performance for unseen input configurations, which is an
advantage of neural networks. With the proposed method, neural networks are
used for the mean functions of Gaussian processes. We present a scalable
stochastic inference procedure, where sparse Gaussian processes are inferred by
stochastic variational inference, and the parameters of neural networks and
kernels are estimated by stochastic gradient descent methods, simultaneously.
We use two real-world spatio-temporal data sets to demonstrate experimentally
that the proposed method achieves better uncertainty estimation and
generalization performance than neural networks and Gaussian processes.
</summary>
    <author>
      <name>Tomoharu Iwata</name>
    </author>
    <author>
      <name>Zoubin Ghahramani</name>
    </author>
    <link href="http://arxiv.org/abs/1707.05922v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.05922v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.03344v1</id>
    <updated>2017-10-09T22:51:28Z</updated>
    <published>2017-10-09T22:51:28Z</published>
    <title>Iterative PET Image Reconstruction Using Convolutional Neural Network
  Representation</title>
    <summary>  PET image reconstruction is challenging due to the ill-poseness of the
inverse problem and limited number of detected photons. Recently deep neural
networks have been widely and successfully used in computer vision tasks and
attracted growing interests in medical imaging. In this work, we trained a deep
residual convolutional neural network to improve PET image quality by using the
existing inter-patient information. An innovative feature of the proposed
method is that we embed the neural network in the iterative reconstruction
framework for image representation, rather than using it as a post-processing
tool. We formulate the objective function as a constraint optimization problem
and solve it using the alternating direction method of multipliers (ADMM)
algorithm. Both simulation data and hybrid real data are used to evaluate the
proposed method. Quantification results show that our proposed iterative neural
network method can outperform the neural network denoising and conventional
penalized maximum likelihood methods.
</summary>
    <author>
      <name>Kuang Gong</name>
    </author>
    <author>
      <name>Jiahui Guan</name>
    </author>
    <author>
      <name>Kyungsang Kim</name>
    </author>
    <author>
      <name>Xuezhu Zhang</name>
    </author>
    <author>
      <name>Georges El Fakhri</name>
    </author>
    <author>
      <name>Jinyi Qi</name>
    </author>
    <author>
      <name>Quanzheng Li</name>
    </author>
    <link href="http://arxiv.org/abs/1710.03344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.03344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.03425v1</id>
    <updated>2017-10-10T07:16:54Z</updated>
    <published>2017-10-10T07:16:54Z</published>
    <title>AdaDNNs: Adaptive Ensemble of Deep Neural Networks for Scene Text
  Recognition</title>
    <summary>  Recognizing text in the wild is a really challenging task because of complex
backgrounds, various illuminations and diverse distortions, even with deep
neural networks (convolutional neural networks and recurrent neural networks).
In the end-to-end training procedure for scene text recognition, the outputs of
deep neural networks at different iterations are always demonstrated with
diversity and complementarity for the target object (text). Here, a simple but
effective deep learning method, an adaptive ensemble of deep neural networks
(AdaDNNs), is proposed to simply select and adaptively combine classifier
components at different iterations from the whole learning system. Furthermore,
the ensemble is formulated as a Bayesian framework for classifier weighting and
combination. A variety of experiments on several typical acknowledged
benchmarks, i.e., ICDAR Robust Reading Competition (Challenge 1, 2 and 4)
datasets, verify the surprised improvement from the baseline DNNs, and the
effectiveness of AdaDNNs compared with the recent state-of-the-art methods.
</summary>
    <author>
      <name>Chun Yang</name>
    </author>
    <author>
      <name>Xu-Cheng Yin</name>
    </author>
    <author>
      <name>Zejun Li</name>
    </author>
    <author>
      <name>Jianwei Wu</name>
    </author>
    <author>
      <name>Chunchao Guo</name>
    </author>
    <author>
      <name>Hongfa Wang</name>
    </author>
    <author>
      <name>Lei Xiao</name>
    </author>
    <link href="http://arxiv.org/abs/1710.03425v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.03425v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.11431v2</id>
    <updated>2018-02-20T17:33:48Z</updated>
    <published>2017-10-31T12:24:26Z</published>
    <title>Physics-guided Neural Networks (PGNN): An Application in Lake
  Temperature Modeling</title>
    <summary>  This paper introduces a novel framework for combining scientific knowledge of
physics-based models with neural networks to advance scientific discovery. This
framework, termed as physics-guided neural network (PGNN), leverages the output
of physics-based model simulations along with observational features to
generate predictions using a neural network architecture. Further, this paper
presents a novel framework for using physics-based loss functions in the
learning objective of neural networks, to ensure that the model predictions not
only show lower errors on the training set but are also scientifically
consistent with the known physics on the unlabeled set. We illustrate the
effectiveness of PGNN for the problem of lake temperature modeling, where
physical relationships between the temperature, density, and depth of water are
used to design a physics-based loss function. By using scientific knowledge to
guide the construction and learning of neural networks, we are able to show
that the proposed framework ensures better generalizability as well as
scientific consistency of results.
</summary>
    <author>
      <name>Anuj Karpatne</name>
    </author>
    <author>
      <name>William Watkins</name>
    </author>
    <author>
      <name>Jordan Read</name>
    </author>
    <author>
      <name>Vipin Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to ACM SIGKDD 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.11431v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.11431v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.05133v2</id>
    <updated>2017-11-15T07:47:34Z</updated>
    <published>2017-11-14T14:54:23Z</published>
    <title>Reinforcement Learning in a large scale photonic Recurrent Neural
  Network</title>
    <summary>  Photonic Neural Network implementations have been gaining considerable
attention as a potentially disruptive future technology. Demonstrating learning
in large scale neural networks is essential to establish photonic machine
learning substrates as viable information processing systems. Realizing
photonic Neural Networks with numerous nonlinear nodes in a fully parallel and
efficient learning hardware was lacking so far. We demonstrate a network of up
to 2500 diffractively coupled photonic nodes, forming a large scale Recurrent
Neural Network. Using a Digital Micro Mirror Device, we realize reinforcement
learning. Our scheme is fully parallel, and the passive weights maximize energy
efficiency and bandwidth. The computational output efficiently converges and we
achieve very good performance.
</summary>
    <author>
      <name>Julian Bueno</name>
    </author>
    <author>
      <name>Sheler Maktoobi</name>
    </author>
    <author>
      <name>Luc Froehly</name>
    </author>
    <author>
      <name>Ingo Fischer</name>
    </author>
    <author>
      <name>Maxime Jacquot</name>
    </author>
    <author>
      <name>Laurent Larger</name>
    </author>
    <author>
      <name>Daniel Brunner</name>
    </author>
    <link href="http://arxiv.org/abs/1711.05133v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.05133v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.02643v1</id>
    <updated>2018-02-07T21:29:52Z</updated>
    <published>2018-02-07T21:29:52Z</published>
    <title>Gradient conjugate priors and deep neural networks</title>
    <summary>  The paper deals with learning the probability distribution of the observed
data by artificial neural networks. We suggest a so-called gradient conjugate
prior (GCP) update appropriate for neural networks, which is a modification of
the classical Bayesian update for conjugate priors. We establish a connection
between the gradient conjugate prior update and the maximization of the
log-likelihood of the predictive distribution. Unlike for the Bayesian neural
networks, we do not impose a prior on the weights of the neural networks, but
rather assume that the ground truth distribution is normal with unknown mean
and variance and learn by neural networks the parameters of a prior
(normal-gamma distribution) for these unknown mean and variance. The update of
the parameters is done, using the gradient that, at each step, directs towards
minimizing the Kullback--Leibler divergence from the prior to the posterior
distribution (both being normal-gamma). We obtain a corresponding dynamical
system for the prior's parameters and analyze its properties. In particular, we
study the limiting behavior of all the prior's parameters and show how it
differs from the case of the classical full Bayesian update. The results are
validated on synthetic and real world data sets.
</summary>
    <author>
      <name>Pavel Gurevich</name>
    </author>
    <author>
      <name>Hannes Stuke</name>
    </author>
    <link href="http://arxiv.org/abs/1802.02643v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.02643v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.06981v1</id>
    <updated>2018-02-20T06:58:32Z</updated>
    <published>2018-02-20T06:58:32Z</published>
    <title>Reachable Set Estimation and Safety Verification for Piecewise Linear
  Systems with Neural Network Controllers</title>
    <summary>  In this work, the reachable set estimation and safety verification problems
for a class of piecewise linear systems equipped with neural network
controllers are addressed. The neural network is considered to consist of
Rectified Linear Unit (ReLU) activation functions. A layer-by-layer approach is
developed for the output reachable set computation of ReLU neural networks. The
computation is formulated in the form of a set of manipulations for a union of
polytopes. Based on the output reachable set for neural network controllers,
the output reachable set for a piecewise linear feedback control system can be
estimated iteratively for a given finite-time interval. With the estimated
output reachable set, the safety verification for piecewise linear systems with
neural network controllers can be performed by checking the existence of
intersections of unsafe regions and output reach set. A numerical example is
presented to illustrate the effectiveness of our approach.
</summary>
    <author>
      <name>Weiming Xiang</name>
    </author>
    <author>
      <name>Hoang-Dung Tran</name>
    </author>
    <author>
      <name>Joel A. Rosenfeld</name>
    </author>
    <author>
      <name>Taylor T. Johnson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures, ACC 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.06981v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06981v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.2336v1</id>
    <updated>2010-11-10T10:40:46Z</updated>
    <published>2010-11-10T10:40:46Z</published>
    <title>A network model with structured nodes</title>
    <summary>  We present a network model in which words over a specific alphabet, called
{\it structures}, are associated to each node and undirected edges are added
depending on some distance between different structures. It is shown that this
model can generate, without the use of preferential attachment or any other
heuristic, networks with topological features similar to biological networks:
power law degree distribution, clustering coefficient independent from the
network size, etc. Specific biological networks ({\it C. Elegans} neural
network and {\it E. Coli} protein-protein interaction network) are replicated
using this model.
</summary>
    <author>
      <name>P. Frisco</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.84.021931</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.84.021931" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">match MRSA gene network not present because MRSA gene network still
  unpublished</arxiv:comment>
    <link href="http://arxiv.org/abs/1011.2336v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.2336v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07706v1</id>
    <updated>2016-09-25T06:44:42Z</updated>
    <published>2016-09-25T06:44:42Z</published>
    <title>Learning by Stimulation Avoidance: A Principle to Control Spiking Neural
  Networks Dynamics</title>
    <summary>  Learning based on networks of real neurons, and by extension biologically
inspired models of neural networks, has yet to find general learning rules
leading to widespread applications. In this paper, we argue for the existence
of a principle allowing to steer the dynamics of a biologically inspired neural
network. Using carefully timed external stimulation, the network can be driven
towards a desired dynamical state. We term this principle "Learning by
Stimulation Avoidance" (LSA). We demonstrate through simulation that the
minimal sufficient conditions leading to LSA in artificial networks are also
sufficient to reproduce learning results similar to those obtained in
biological neurons by Shahaf and Marom [1]. We examine the mechanism's basic
dynamics in a reduced network, and demonstrate how it scales up to a network of
100 neurons. We show that LSA has a higher explanatory power than existing
hypotheses about the response of biological neural networks to external
simulation, and can be used as a learning rule for an embodied application:
learning of wall avoidance by a simulated robot. The surge in popularity of
artificial neural networks is mostly directed to disembodied models of neurons
with biologically irrelevant dynamics: to the authors' knowledge, this is the
first work demonstrating sensory-motor learning with random spiking networks
through pure Hebbian learning.
</summary>
    <author>
      <name>Lana Sinapayen</name>
    </author>
    <author>
      <name>Atsushi Masumori</name>
    </author>
    <author>
      <name>Takashi Ikegami</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pone.0170388</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pone.0170388" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.07706v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07706v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/q-bio/0505021v1</id>
    <updated>2005-05-10T19:51:16Z</updated>
    <published>2005-05-10T19:51:16Z</published>
    <title>Characterizing Self-Developing Biological Neural Networks: A First Step
  Towards their Application To Computing Systems</title>
    <summary>  Carbon nanotubes are often seen as the only alternative technology to silicon
transistors. While they are the most likely short-term one, other longer-term
alternatives should be studied as well. While contemplating biological neurons
as an alternative component may seem preposterous at first sight, significant
recent progress in CMOS-neuron interface suggests this direction may not be
unrealistic; moreover, biological neurons are known to self-assemble into very
large networks capable of complex information processing tasks, something that
has yet to be achieved with other emerging technologies. The first step to
designing computing systems on top of biological neurons is to build an
abstract model of self-assembled biological neural networks, much like computer
architects manipulate abstract models of transistors and circuits. In this
article, we propose a first model of the structure of biological neural
networks. We provide empirical evidence that this model matches the biological
neural networks found in living organisms, and exhibits the small-world graph
structure properties commonly found in many large and self-organized systems,
including biological neural networks. More importantly, we extract the simple
local rules and characteristics governing the growth of such networks, enabling
the development of potentially large but realistic biological neural networks,
as would be needed for complex information processing/computing tasks. Based on
this model, future work will be targeted to understanding the evolution and
learning properties of such networks, and how they can be used to build
computing systems.
</summary>
    <author>
      <name>Hugues Berry</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs</arxiv:affiliation>
    </author>
    <author>
      <name>Olivier Temam</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/q-bio/0505021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/q-bio/0505021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.07174v2</id>
    <updated>2016-12-27T04:53:56Z</updated>
    <published>2016-11-22T07:36:21Z</published>
    <title>Deep Recurrent Convolutional Neural Network: Improving Performance For
  Speech Recognition</title>
    <summary>  A deep learning approach has been widely applied in sequence modeling
problems. In terms of automatic speech recognition (ASR), its performance has
significantly been improved by increasing large speech corpus and deeper neural
network. Especially, recurrent neural network and deep convolutional neural
network have been applied in ASR successfully. Given the arising problem of
training speed, we build a novel deep recurrent convolutional network for
acoustic modeling and then apply deep residual learning to it. Our experiments
show that it has not only faster convergence speed but better recognition
accuracy over traditional deep convolutional recurrent network. In the
experiments, we compare the convergence speed of our novel deep recurrent
convolutional networks and traditional deep convolutional recurrent networks.
With faster convergence speed, our novel deep recurrent convolutional networks
can reach the comparable performance. We further show that applying deep
residual learning can boost the convergence speed of our novel deep recurret
convolutional networks. Finally, we evaluate all our experimental networks by
phoneme error rate (PER) with our proposed bidirectional statistical n-gram
language model. Our evaluation results show that our newly proposed deep
recurrent convolutional network applied with deep residual learning can reach
the best PER of 17.33\% with the fastest convergence speed on TIMIT database.
The outstanding performance of our novel deep recurrent convolutional neural
network with deep residual learning indicates that it can be potentially
adopted in other sequential problems.
</summary>
    <author>
      <name>Zewang Zhang</name>
    </author>
    <author>
      <name>Zheng Sun</name>
    </author>
    <author>
      <name>Jiaqi Liu</name>
    </author>
    <author>
      <name>Jingwen Chen</name>
    </author>
    <author>
      <name>Zhao Huo</name>
    </author>
    <author>
      <name>Xiao Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.07174v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.07174v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00539v1</id>
    <updated>2018-02-02T02:12:33Z</updated>
    <published>2018-02-02T02:12:33Z</published>
    <title>Complex Network Classification with Convolutional Neural Network</title>
    <summary>  Classifying large scale networks into several categories and distinguishing
them according to their fine structures is of great importance with several
applications in real life. However, most studies of complex networks focus on
properties of a single network but seldom on classification, clustering, and
comparison between different networks, in which the network is treated as a
whole. Due to the non-Euclidean properties of the data, conventional methods
can hardly be applied on networks directly. In this paper, we propose a novel
framework of complex network classifier (CNC) by integrating network embedding
and convolutional neural network to tackle the problem of network
classification. By training the classifiers on synthetic complex network data
and real international trade network data, we show CNC can not only classify
networks in a high accuracy and robustness, it can also extract the features of
the networks automatically.
</summary>
    <author>
      <name>Ruyue Xin</name>
    </author>
    <author>
      <name>Jiang Zhang</name>
    </author>
    <author>
      <name>Yitong Shao</name>
    </author>
    <link href="http://arxiv.org/abs/1802.00539v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00539v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505019v1</id>
    <updated>2005-05-10T06:37:31Z</updated>
    <published>2005-05-10T06:37:31Z</published>
    <title>Artificial Neural Networks and their Applications</title>
    <summary>  The Artificial Neural network is a functional imitation of simplified model
of the biological neurons and their goal is to construct useful computers for
real world problems. The ANN applications have increased dramatically in the
last few years fired by both theoretical and practical applications in a wide
variety of applications. A brief theory of ANN is presented and potential areas
are identified and future trends are discussed.
</summary>
    <author>
      <name>Nitin Malik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0607019v1</id>
    <updated>2006-07-06T18:49:20Z</updated>
    <published>2006-07-06T18:49:20Z</published>
    <title>Modelling the Probability Density of Markov Sources</title>
    <summary>  This paper introduces an objective function that seeks to minimise the
average total number of bits required to encode the joint state of all of the
layers of a Markov source. This type of encoder may be applied to the problem
of optimising the bottom-up (recognition model) and top-down (generative model)
connections in a multilayer neural network, and it unifies several previous
results on the optimisation of multilayer neural networks.
</summary>
    <author>
      <name>Stephen Luttrell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0607019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0607019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.0197v1</id>
    <updated>2008-05-02T09:20:11Z</updated>
    <published>2008-05-02T09:20:11Z</published>
    <title>Flatness of the Energy Landscape for Horn Clauses</title>
    <summary>  The Little-Hopfield neural network programmed with Horn clauses is studied.
We argue that the energy landscape of the system, corresponding to the
inconsistency function for logical interpretations of the sets of Horn clauses,
has minimal ruggedness. This is supported by computer simulations.
</summary>
    <author>
      <name>Saratha Sathasivam</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">USM</arxiv:affiliation>
    </author>
    <author>
      <name>Wan Ahmad Tajuddin Wan Abdullah</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Univ. Malaya</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Matematika 23 (2007) 147-156</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0805.0197v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.0197v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.0968v1</id>
    <updated>2014-06-04T08:25:56Z</updated>
    <published>2014-06-04T08:25:56Z</published>
    <title>Integration of a Predictive, Continuous Time Neural Network into
  Securities Market Trading Operations</title>
    <summary>  This paper describes recent development and test implementation of a
continuous time recurrent neural network that has been configured to predict
rates of change in securities. It presents outcomes in the context of popular
technical analysis indicators and highlights the potential impact of continuous
predictive capability on securities market trading operations.
</summary>
    <author>
      <name>Christopher S Kirk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.0968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.0968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.1434v1</id>
    <updated>2010-06-08T01:17:00Z</updated>
    <published>2010-06-08T01:17:00Z</published>
    <title>Computing by Means of Physics-Based Optical Neural Networks</title>
    <summary>  We report recent research on computing with biology-based neural network
models by means of physics-based opto-electronic hardware. New technology
provides opportunities for very-high-speed computation and uncovers problems
obstructing the wide-spread use of this new capability. The Computation
Modeling community may be able to offer solutions to these cross-boundary
research problems.
</summary>
    <author>
      <name>A. Steven Younger</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Missouri State University</arxiv:affiliation>
    </author>
    <author>
      <name>Emmett Redd</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Missouri State University</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.26.15</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.26.15" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 26, 2010, pp. 159-167</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1006.1434v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.1434v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.00299v1</id>
    <updated>2015-01-01T18:37:36Z</updated>
    <published>2015-01-01T18:37:36Z</published>
    <title>Sequence Modeling using Gated Recurrent Neural Networks</title>
    <summary>  In this paper, we have used Recurrent Neural Networks to capture and model
human motion data and generate motions by prediction of the next immediate data
point at each time-step. Our RNN is armed with recently proposed Gated
Recurrent Units which has shown promising results in some sequence modeling
problems such as Machine Translation and Speech Synthesis. We demonstrate that
this model is able to capture long-term dependencies in data and generate
realistic motions.
</summary>
    <author>
      <name>Mohammad Pezeshki</name>
    </author>
    <link href="http://arxiv.org/abs/1501.00299v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.00299v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.02788v1</id>
    <updated>2015-08-12T01:01:11Z</updated>
    <published>2015-08-12T01:01:11Z</published>
    <title>The Effects of Hyperparameters on SGD Training of Neural Networks</title>
    <summary>  The performance of neural network classifiers is determined by a number of
hyperparameters, including learning rate, batch size, and depth. A number of
attempts have been made to explore these parameters in the literature, and at
times, to develop methods for optimizing them. However, exploration of
parameter spaces has often been limited. In this note, I report the results of
large scale experiments exploring these different parameters and their
interactions.
</summary>
    <author>
      <name>Thomas M. Breuel</name>
    </author>
    <link href="http://arxiv.org/abs/1508.02788v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.02788v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.04568v1</id>
    <updated>2016-01-18T15:22:48Z</updated>
    <published>2016-01-18T15:22:48Z</published>
    <title>Content Aware Neural Style Transfer</title>
    <summary>  This paper presents a content-aware style transfer algorithm for paintings
and photos of similar content using pre-trained neural network, obtaining
better results than the previous work. In addition, the numerical experiments
show that the style pattern and the content information is not completely
separated by neural network.
</summary>
    <author>
      <name>Rujie Yin</name>
    </author>
    <link href="http://arxiv.org/abs/1601.04568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.04568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.10; I.5.2; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09002v1</id>
    <updated>2016-03-29T23:48:27Z</updated>
    <published>2016-03-29T23:48:27Z</published>
    <title>Dataflow Matrix Machines as a Generalization of Recurrent Neural
  Networks</title>
    <summary>  Dataflow matrix machines are a powerful generalization of recurrent neural
networks. They work with multiple types of arbitrary linear streams, multiple
types of powerful neurons, and allow to incorporate higher-order constructions.
We expect them to be useful in machine learning and probabilistic programming,
and in the synthesis of dynamic systems and of deterministic and probabilistic
programs.
</summary>
    <author>
      <name>Michael Bukatin</name>
    </author>
    <author>
      <name>Steve Matthews</name>
    </author>
    <author>
      <name>Andrey Radul</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages position paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.09002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.06212v1</id>
    <updated>2016-12-19T14:59:14Z</updated>
    <published>2016-12-19T14:59:14Z</published>
    <title>A recurrent neural network without chaos</title>
    <summary>  We introduce an exceptionally simple gated recurrent neural network (RNN)
that achieves performance comparable to well-known gated architectures, such as
LSTMs and GRUs, on the word-level language modeling task. We prove that our
model has simple, predicable and non-chaotic dynamics. This stands in stark
contrast to more standard gated architectures, whose underlying dynamical
systems exhibit chaotic behavior.
</summary>
    <author>
      <name>Thomas Laurent</name>
    </author>
    <author>
      <name>James von Brecht</name>
    </author>
    <link href="http://arxiv.org/abs/1612.06212v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.06212v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05923v1</id>
    <updated>2017-01-20T20:53:51Z</updated>
    <published>2017-01-20T20:53:51Z</published>
    <title>Gate-Variants of Gated Recurrent Unit (GRU) Neural Networks</title>
    <summary>  The paper evaluates three variants of the Gated Recurrent Unit (GRU) in
recurrent neural networks (RNN) by reducing parameters in the update and reset
gates. We evaluate the three variant GRU models on MNIST and IMDB datasets and
show that these GRU-RNN variant models perform as well as the original GRU RNN
model while reducing the computational expense.
</summary>
    <author>
      <name>Rahul Dey</name>
    </author>
    <author>
      <name>Fathi M. Salem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 8 Figures, 4 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.05923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.03523v1</id>
    <updated>2018-01-09T03:35:20Z</updated>
    <published>2018-01-09T03:35:20Z</published>
    <title>Generative Models for Stochastic Processes Using Convolutional Neural
  Networks</title>
    <summary>  The present paper aims to demonstrate the usage of Convolutional Neural
Networks as a generative model for stochastic processes, enabling researchers
from a wide range of fields (such as quantitative finance and physics) to
develop a general tool for forecasts and simulations without the need to
identify/assume a specific system structure or estimate its parameters.
</summary>
    <author>
      <name>Fernando Fernandes Neto</name>
    </author>
    <link href="http://arxiv.org/abs/1801.03523v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.03523v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04846v1</id>
    <updated>2016-09-15T20:21:30Z</updated>
    <published>2016-09-15T20:21:30Z</published>
    <title>A Tutorial about Random Neural Networks in Supervised Learning</title>
    <summary>  Random Neural Networks (RNNs) are a class of Neural Networks (NNs) that can
also be seen as a specific type of queuing network. They have been successfully
used in several domains during the last 25 years, as queuing networks to
analyze the performance of resource sharing in many engineering areas, as
learning tools and in combinatorial optimization, where they are seen as neural
systems, and also as models of neurological aspects of living beings. In this
article we focus on their learning capabilities, and more specifically, we
present a practical guide for using the RNN to solve supervised learning
problems. We give a general description of these models using almost
indistinctly the terminology of Queuing Theory and the neural one. We present
the standard learning procedures used by RNNs, adapted from similar
well-established improvements in the standard NN field. We describe in
particular a set of learning algorithms covering techniques based on the use of
first order and, then, of second order derivatives. We also discuss some issues
related to these objects and present new perspectives about their use in
supervised learning problems. The tutorial describes their most relevant
applications, and also provides a large bibliography.
</summary>
    <author>
      <name>Sebasti√°n Basterrech</name>
    </author>
    <author>
      <name>Gerardo Rubino</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14311/NNW.2015.25.024</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14311/NNW.2015.25.024" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is a draft of an article to be published in Neural Network
  World</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Network World, Volume 5, Number 15, pp.:457-499, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1609.04846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09934v2</id>
    <updated>2017-08-22T09:19:11Z</updated>
    <published>2016-11-29T23:06:21Z</published>
    <title>Neural Network Models for Software Development Effort Estimation: A
  Comparative Study</title>
    <summary>  Software development effort estimation (SDEE) is one of the main tasks in
software project management. It is crucial for a project manager to efficiently
predict the effort or cost of a software project in a bidding process, since
overestimation will lead to bidding loss and underestimation will cause the
company to lose money. Several SDEE models exist; machine learning models,
especially neural network models, are among the most prominent in the field. In
this study, four different neural network models: Multilayer Perceptron,
General Regression Neural Network, Radial Basis Function Neural Network, and
Cascade Correlation Neural Network are compared with each other based on: (1)
predictive accuracy centered on the Mean Absolute Error criterion, (2) whether
such a model tends to overestimate or underestimate, and (3) how each model
classifies the importance of its inputs. Industrial datasets from the
International Software Benchmarking Standards Group (ISBSG) are used to train
and validate the four models. The main ISBSG dataset was filtered and then
divided into five datasets based on the productivity value of each project.
Results show that the four models tend to overestimate in 80percent of the
datasets, and the significance of the model inputs varies based on the selected
model. Furthermore, the Cascade Correlation Neural Network outperforms the
other three models in the majority of the datasets constructed on the Mean
Absolute Residual criterion.
</summary>
    <author>
      <name>Ali Bou Nassif</name>
    </author>
    <author>
      <name>Mohammad Azzeh</name>
    </author>
    <author>
      <name>Luiz Fernando Capretz</name>
    </author>
    <author>
      <name>Danny Ho</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00521-015-2127-1,</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00521-015-2127-1," rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Computing &amp; Applications, Volume 27, Issue 8, pp.
  2369-2381, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1611.09934v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09934v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.00580v1</id>
    <updated>2017-08-02T02:35:20Z</updated>
    <published>2017-08-02T02:35:20Z</published>
    <title>A Novel Neural Network Model Specified for Representing Logical
  Relations</title>
    <summary>  With computers to handle more and more complicated things in variable
environments, it becomes an urgent requirement that the artificial intelligence
has the ability of automatic judging and deciding according to numerous
specific conditions so as to deal with the complicated and variable cases. ANNs
inspired by brain is a good candidate. However, most of current numeric ANNs
are not good at representing logical relations because these models still try
to represent logical relations in the form of ratio based on functional
approximation. On the other hand, researchers have been trying to design novel
neural network models to make neural network model represent logical relations.
In this work, a novel neural network model specified for representing logical
relations is proposed and applied. New neurons and multiple kinds of links are
defined. Inhibitory links are introduced besides exciting links. Different from
current numeric ANNs, one end of an inhibitory link connects an exciting link
rather than a neuron. Inhibitory links inhibit the connected exciting links
conditionally to make this neural network model represent logical relations
correctly. This model can simulate the operations of Boolean logic gates, and
construct complex logical relations with the advantages of simpler neural
network structures than recent works in this area. This work provides some
ideas to make neural networks represent logical relations more directly and
efficiently, and the model could be used as the complement to current numeric
ANN to deal with logical issues and expand the application areas of ANN.
</summary>
    <author>
      <name>Gang Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1708.00580v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00580v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.07128v3</id>
    <updated>2018-02-14T19:24:55Z</updated>
    <published>2017-11-20T03:19:03Z</published>
    <title>Hello Edge: Keyword Spotting on Microcontrollers</title>
    <summary>  Keyword spotting (KWS) is a critical component for enabling speech based user
interactions on smart devices. It requires real-time response and high accuracy
for good user experience. Recently, neural networks have become an attractive
choice for KWS architecture because of their superior accuracy compared to
traditional speech processing algorithms. Due to its always-on nature, KWS
application has highly constrained power budget and typically runs on tiny
microcontrollers with limited memory and compute capability. The design of
neural network architecture for KWS must consider these constraints. In this
work, we perform neural network architecture evaluation and exploration for
running KWS on resource-constrained microcontrollers. We train various neural
network architectures for keyword spotting published in literature to compare
their accuracy and memory/compute requirements. We show that it is possible to
optimize these neural network architectures to fit within the memory and
compute constraints of microcontrollers without sacrificing accuracy. We
further explore the depthwise separable convolutional neural network (DS-CNN)
and compare it against other neural network architectures. DS-CNN achieves an
accuracy of 95.4%, which is ~10% higher than the DNN model with similar number
of parameters.
</summary>
    <author>
      <name>Yundong Zhang</name>
    </author>
    <author>
      <name>Naveen Suda</name>
    </author>
    <author>
      <name>Liangzhen Lai</name>
    </author>
    <author>
      <name>Vikas Chandra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code available in github at
  https://github.com/ARM-software/ML-KWS-for-MCU</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.07128v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.07128v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9402096v1</id>
    <updated>1994-02-22T14:02:08Z</updated>
    <published>1994-02-22T14:02:08Z</published>
    <title>Response Functions Improving Performance in Analog Attractor Neural
  Networks</title>
    <summary>  In the context of attractor neural networks, we study how the equilibrium
analog neural activities, reached by the network dynamics during memory
retrieval, may improve storage performance by reducing the interferences
between the recalled pattern and the other stored ones. We determine a simple
dynamics that stabilizes network states which are highly correlated with the
retrieved pattern, for a number of stored memories that does not exceed
$\alpha_{\star} N$, where $\alpha_{\star}\in[0,0.41]$ depends on the global
activity level in the network and $N$ is the number of neurons.
</summary>
    <author>
      <name>Nicolas Brunel</name>
    </author>
    <author>
      <name>Riccardo Zecchina</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.49.R1823</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.49.R1823" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages (with figures), LaTex (RevTex), to appear on Phys.Rev.E (RC)</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9402096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9402096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9901251v1</id>
    <updated>1999-01-23T11:42:56Z</updated>
    <published>1999-01-23T11:42:56Z</published>
    <title>Fixed points of Hopfield type neural networks</title>
    <summary>  The set of the fixed points of the Hopfield type network is under
investigation. The connection matrix of the network is constructed according
the Hebb rule from the set of memorized patterns which are treated as distorted
copies of the standard-vector. It is found that the dependence of the set of
the fixed points on the value of the distortion parameter can be described
analytically. The obtained results are interpreted in the terms of neural
networks and the Ising model at T=0.
</summary>
    <author>
      <name>Leonid B. Litinskii</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Institute for High Pressure Physics Russian Academy of Sciences</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, Revtex, 1 Postscript-file</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9901251v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9901251v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/9906197v1</id>
    <updated>1999-06-14T14:17:55Z</updated>
    <published>1999-06-14T14:17:55Z</published>
    <title>Fixed Points of Hopfield Type Neural Networks</title>
    <summary>  The set of the fixed points of the Hopfield type network is under
investigation. The connection matrix of the network is constructed according to
the Hebb rule from the set of memorized patterns which are treated as distorted
copies of the standard-vector. It is found that the dependence of the set of
the fixed points on the value of the distortion parameter can be described
analytically. The obtained results are interpreted in the terms of neural
networks and the Ising model.
</summary>
    <author>
      <name>Leonid B. Litinskii</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">High Pressure Physics Institute of Russian Academy of Sciences</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/BF02557200</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/BF02557200" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">RevTEX, 19 pages, 2 Postscript figures, the full version of the
  earler brief report (cond-mat/9901251)</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/9906197v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/9906197v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0203011v1</id>
    <updated>2002-03-01T08:47:31Z</updated>
    <published>2002-03-01T08:47:31Z</published>
    <title>Interacting neural networks and cryptography</title>
    <summary>  Two neural networks which are trained on their mutual output bits are
analysed using methods of statistical physics. The exact solution of the
dynamics of the two weight vectors shows a novel phenomenon: The networks
synchronize to a state with identical time dependent weights. Extending the
models to multilayer networks with discrete weights, it is shown how
synchronization by mutual learning can be applied to secret key exchange over a
public channel.
</summary>
    <author>
      <name>Wolfgang Kinzel</name>
    </author>
    <author>
      <name>Ido Kanter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Invited talk for the meeting of the German Physical Society</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0203011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0203011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0208281v2</id>
    <updated>2002-08-14T22:30:50Z</updated>
    <published>2002-08-14T14:57:08Z</published>
    <title>Time evolution of the extremely diluted Blume-Emery-Griffiths neural
  network</title>
    <summary>  The time evolution of the extremely diluted Blume-Emery-Griffiths neural
network model is studied, and a detailed equilibrium phase diagram is obtained
exhibiting pattern retrieval, fluctuation retrieval and self-sustained activity
phases. It is shown that saddle-point solutions associated with fluctuation
overlaps slow down considerably the flow of the network states towards the
retrieval fixed points. A comparison of the performance with other three-state
networks is also presented.
</summary>
    <author>
      <name>D. Bolle'</name>
    </author>
    <author>
      <name>D. R. C. Dominguez</name>
    </author>
    <author>
      <name>R. Erichsen Jr.</name>
    </author>
    <author>
      <name>E. Korutcheva</name>
    </author>
    <author>
      <name>W. K. Theumann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.68.062901</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.68.062901" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cond-mat/0208281v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0208281v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0209293v1</id>
    <updated>2002-09-12T14:59:46Z</updated>
    <published>2002-09-12T14:59:46Z</published>
    <title>Coupling parameter in synchronization of diluted neural networks</title>
    <summary>  We study the critical features of coupling parameter in the synchronization
of neural networks with diluted synapses. Based on simulations, the exponential
decay form is observed in the extreme case of global coupling among subsystems
and fully linking in each network: there exists maximum and minimum of the
critical coupling intensity for synchronization in this spatially extended
system. For the partial coupling, we present the primary result about the
critical coupling fraction for various linking degrees of networks.
</summary>
    <author>
      <name>Qi Li</name>
    </author>
    <author>
      <name>Yong Chen</name>
    </author>
    <author>
      <name>Ying Hai Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.65.041916</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.65.041916" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 8 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Phys. Rev. E65, 041916(2002)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0209293v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0209293v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0303425v1</id>
    <updated>2003-03-20T18:12:40Z</updated>
    <published>2003-03-20T18:12:40Z</published>
    <title>Mean-field dynamics of sequence processing neural networks with finite
  connectivity</title>
    <summary>  A recent dynamic mean-field theory for sequence processing in fully connected
neural networks of Hopfield-type (During, Coolen and Sherrington, 1998) is
extended and analized here for a symmetrically diluted network with finite
connectivity near saturation. Equations for the dynamics and the stationary
states are obtained for the macroscopic observables and the precise equivalence
is established with the single-pattern retrieval problem in a layered
feed-forward network with finite connectivity.
</summary>
    <author>
      <name>W. K. Theumann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/S0378-4371(03)00569-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/S0378-4371(03)00569-7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages Latex</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physica A 328, 1-12 (2003)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0303425v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0303425v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0304021v1</id>
    <updated>2003-04-01T19:14:26Z</updated>
    <published>2003-04-01T19:14:26Z</published>
    <title>Topology and Computational Performance of Attractor Neural Networks</title>
    <summary>  To explore the relation between network structure and function, we studied
the computational performance of Hopfield-type attractor neural nets with
regular lattice, random, small-world and scale-free topologies. The random net
is the most efficient for storage and retrieval of patterns by the entire
network. However, in the scale-free case retrieval errors are not distributed
uniformly: the portion of a pattern encoded by the subset of highly connected
nodes is more robust and efficiently recognized than the rest of the pattern.
The scale-free network thus achieves a very strong partial recognition.
Implications for brain function and social dynamics are suggestive.
</summary>
    <author>
      <name>Patrick N. Mcgraw</name>
    </author>
    <author>
      <name>Michael Menzinger</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.68.047102</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.68.047102" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 figures included. Submitted to Phys. Rev. Letters</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 68, 047102 (2003)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0304021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0304021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0503626v2</id>
    <updated>2005-06-29T21:47:34Z</updated>
    <published>2005-03-27T18:32:27Z</published>
    <title>Conditions for the emergence of spatial asymmetric states in attractor
  neural network</title>
    <summary>  In this paper we show that during the retrieval process in a binary symmetric
Hebb neural network, spatial localized states can be observed when the
connectivity of the network is distance-dependent and when a constraint on the
activity of the network is imposed, which forces different levels of activity
in the retrieval and learning states. This asymmetry in the activity during the
retrieval and learning is found to be sufficient condition in order to observe
spatial localized states. The result is confirmed analytically and by
simulation.
</summary>
    <author>
      <name>Kostadin Koroutchev</name>
    </author>
    <author>
      <name>Elka Korutcheva</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2478/BF02475647</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2478/BF02475647" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 p</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CEJP 3(3) 2005 409-419</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cond-mat/0503626v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0503626v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0404042v2</id>
    <updated>2004-04-22T09:00:52Z</updated>
    <published>2004-04-21T16:05:27Z</published>
    <title>Extraction of topological features from communication network
  topological patterns using self-organizing feature maps</title>
    <summary>  Different classes of communication network topologies and their
representation in the form of adjacency matrix and its eigenvalues are
presented. A self-organizing feature map neural network is used to map
different classes of communication network topological patterns. The neural
network simulation results are reported.
</summary>
    <author>
      <name>W. Ali</name>
    </author>
    <author>
      <name>R. J. Mondragon</name>
    </author>
    <author>
      <name>F. Alavi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 Pages, 5 figures, To be appeared in IEE Electronics Letter Journal</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0404042v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0404042v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2; I.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/nlin/0601005v2</id>
    <updated>2007-03-01T08:28:51Z</updated>
    <published>2006-01-04T07:32:35Z</published>
    <title>Reinforcement learning of recurrent neural network for temporal coding</title>
    <summary>  We study a reinforcement learning for temporal coding with neural network
consisting of stochastic spiking neurons. In neural networks, information can
be coded by characteristics of the timing of each neuronal firing, including
the order of firing or the relative phase differences of firing. We derive the
learning rule for this network and show that the network consisting of
Hodgkin-Huxley neurons with the dynamical synaptic kinetics can learn the
appropriate timing of each neuronal firing. We also investigate the system size
dependence of learning efficiency.
</summary>
    <author>
      <name>Daichi Kimura</name>
    </author>
    <author>
      <name>Yoshinori Hayakawa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/nlin/0601005v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/nlin/0601005v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1102.4240v1</id>
    <updated>2011-02-21T14:48:20Z</updated>
    <published>2011-02-21T14:48:20Z</published>
    <title>Sparse neural networks with large learning diversity</title>
    <summary>  Coded recurrent neural networks with three levels of sparsity are introduced.
The first level is related to the size of messages, much smaller than the
number of available neurons. The second one is provided by a particular coding
rule, acting as a local constraint in the neural activity. The third one is a
characteristic of the low final connection density of the network after the
learning phase. Though the proposed network is very simple since it is based on
binary neurons and binary connections, it is able to learn a large number of
messages and recall them, even in presence of strong erasures. The performance
of the network is assessed as a classifier and as an associative memory.
</summary>
    <author>
      <name>Vincent Gripon</name>
    </author>
    <author>
      <name>Claude Berrou</name>
    </author>
    <link href="http://arxiv.org/abs/1102.4240v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.4240v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.3474v1</id>
    <updated>2014-06-13T10:11:18Z</updated>
    <published>2014-06-13T10:11:18Z</published>
    <title>Heterogeneous Multi-task Learning for Human Pose Estimation with Deep
  Convolutional Neural Network</title>
    <summary>  We propose an heterogeneous multi-task learning framework for human pose
estimation from monocular image with deep convolutional neural network. In
particular, we simultaneously learn a pose-joint regressor and a sliding-window
body-part detector in a deep network architecture. We show that including the
body-part detection task helps to regularize the network, directing it to
converge to a good solution. We report competitive and state-of-art results on
several data sets. We also empirically show that the learned neurons in the
middle layer of our network are tuned to localized body parts.
</summary>
    <author>
      <name>Sijin Li</name>
    </author>
    <author>
      <name>Zhi-Qiang Liu</name>
    </author>
    <author>
      <name>Antoni B. Chan</name>
    </author>
    <link href="http://arxiv.org/abs/1406.3474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.3474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.5917v1</id>
    <updated>2013-02-24T15:53:19Z</updated>
    <published>2013-02-24T15:53:19Z</published>
    <title>Modeling of interstitial branching of axonal networks</title>
    <summary>  A single axon can generate branches connecting with plenty synaptic targets.
Process of branching is very important for making connections in central
nervous system. The interstitial branching along primary axon shaft occurs
during nervous system development. Growing axon makes pause in its movement and
leaves active points behind its terminal. The new branches appear from these
points. We suggest mathematical model to describe and investigate neural
network branching process. The model under consideration describes neural
network growth in which the concentration of axon guidance molecules manages
axon's growth. We model the interstitial branching from axon shaft. Numerical
simulations show that in the model framework axonal networks are similar to
neural network.
</summary>
    <author>
      <name>Y. Suleymanov</name>
    </author>
    <author>
      <name>F. Gafarov</name>
    </author>
    <author>
      <name>N. Khusnutdinov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1142/S0219635213500064</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1142/S0219635213500064" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">1. J. Integr. Neurosci. 12, 1-14, (2013)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1302.5917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.5917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06829v1</id>
    <updated>2016-03-22T15:26:26Z</updated>
    <published>2016-03-22T15:26:26Z</published>
    <title>Multi-velocity neural networks for gesture recognition in videos</title>
    <summary>  We present a new action recognition deep neural network which adaptively
learns the best action velocities in addition to the classification. While deep
neural networks have reached maturity for image understanding tasks, we are
still exploring network topologies and features to handle the richer
environment of video clips. Here, we tackle the problem of multiple velocities
in action recognition, and provide state-of-the-art results for gesture
recognition, on known and new collected datasets. We further provide the
training steps for our semi-supervised network, suited to learn from huge
unlabeled datasets with only a fraction of labeled examples.
</summary>
    <author>
      <name>Otkrist Gupta</name>
    </author>
    <author>
      <name>Dan Raviv</name>
    </author>
    <author>
      <name>Ramesh Raskar</name>
    </author>
    <link href="http://arxiv.org/abs/1603.06829v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06829v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.01097v3</id>
    <updated>2017-02-28T02:58:11Z</updated>
    <published>2016-07-05T02:51:33Z</published>
    <title>AdaNet: Adaptive Structural Learning of Artificial Neural Networks</title>
    <summary>  We present new algorithms for adaptively learning artificial neural networks.
Our algorithms (AdaNet) adaptively learn both the structure of the network and
its weights. They are based on a solid theoretical analysis, including
data-dependent generalization guarantees that we prove and discuss in detail.
We report the results of large-scale experiments with one of our algorithms on
several binary classification tasks extracted from the CIFAR-10 dataset. The
results demonstrate that our algorithm can automatically learn network
structures with very competitive performance accuracies when compared with
those achieved for neural networks found by standard approaches.
</summary>
    <author>
      <name>Corinna Cortes</name>
    </author>
    <author>
      <name>Xavi Gonzalvo</name>
    </author>
    <author>
      <name>Vitaly Kuznetsov</name>
    </author>
    <author>
      <name>Mehryar Mohri</name>
    </author>
    <author>
      <name>Scott Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1607.01097v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.01097v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.04153v1</id>
    <updated>2017-05-11T13:11:23Z</updated>
    <published>2017-05-11T13:11:23Z</published>
    <title>Dynamic Compositional Neural Networks over Tree Structure</title>
    <summary>  Tree-structured neural networks have proven to be effective in learning
semantic representations by exploiting syntactic information. In spite of their
success, most existing models suffer from the underfitting problem: they
recursively use the same shared compositional function throughout the whole
compositional process and lack expressive power due to inability to capture the
richness of compositionality. In this paper, we address this issue by
introducing the dynamic compositional neural networks over tree structure
(DC-TreeNN), in which the compositional function is dynamically generated by a
meta network. The role of meta-network is to capture the metaknowledge across
the different compositional rules and formulate them. Experimental results on
two typical tasks show the effectiveness of the proposed models.
</summary>
    <author>
      <name>Pengfei Liu</name>
    </author>
    <author>
      <name>Xipeng Qiu</name>
    </author>
    <author>
      <name>Xuanjing Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by IJCAI 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.04153v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04153v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.03301v1</id>
    <updated>2017-06-11T03:07:42Z</updated>
    <published>2017-06-11T03:07:42Z</published>
    <title>Neural networks and rational functions</title>
    <summary>  Neural networks and rational functions efficiently approximate each other. In
more detail, it is shown here that for any ReLU network, there exists a
rational function of degree $O(\text{polylog}(1/\epsilon))$ which is
$\epsilon$-close, and similarly for any rational function there exists a ReLU
network of size $O(\text{polylog}(1/\epsilon))$ which is $\epsilon$-close. By
contrast, polynomials need degree $\Omega(\text{poly}(1/\epsilon))$ to
approximate even a single ReLU. When converting a ReLU network to a rational
function as above, the hidden constants depend exponentially on the number of
layers, which is shown to be tight; in other words, a compositional
representation can be beneficial even for rational functions.
</summary>
    <author>
      <name>Matus Telgarsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear, ICML 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.03301v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.03301v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.05853v2</id>
    <updated>2017-08-09T09:58:43Z</updated>
    <published>2017-07-18T20:47:06Z</published>
    <title>Encoding Word Confusion Networks with Recurrent Neural Networks for
  Dialog State Tracking</title>
    <summary>  This paper presents our novel method to encode word confusion networks, which
can represent a rich hypothesis space of automatic speech recognition systems,
via recurrent neural networks. We demonstrate the utility of our approach for
the task of dialog state tracking in spoken dialog systems that relies on
automatic speech recognition output. Encoding confusion networks outperforms
encoding the best hypothesis of the automatic speech recognition in a neural
system for dialog state tracking on the well-known second Dialog State Tracking
Challenge dataset.
</summary>
    <author>
      <name>Glorianna Jagfeld</name>
    </author>
    <author>
      <name>Ngoc Thang Vu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Speech-Centric Natural Language Processing Workshop @EMNLP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.05853v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.05853v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.01255v3</id>
    <updated>2017-10-26T15:36:53Z</updated>
    <published>2017-09-30T11:50:02Z</published>
    <title>Variational Grid Setting Network</title>
    <summary>  We propose a new neural network architecture for automatic generation of
missing characters in a Chinese font set. We call the neural network
architecture the Variational Grid Setting Network which is based on the
variational autoencoder (VAE) with some tweaks. The neural network model is
able to generate missing characters relatively large in size ($256 \times 256$
pixels). Moreover, we show that one can use very few samples for training data
set, and get a satisfied result.
</summary>
    <author>
      <name>Yu-Neng Chuang</name>
    </author>
    <author>
      <name>Zi-Yu Huang</name>
    </author>
    <author>
      <name>Yen-Lung Tsai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.01255v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.01255v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.04323v1</id>
    <updated>2017-12-12T14:50:51Z</updated>
    <published>2017-12-12T14:50:51Z</published>
    <title>Deep Echo State Network (DeepESN): A Brief Survey</title>
    <summary>  The study of deep recurrent neural networks (RNNs) and, in particular, of
deep Reservoir Computing (RC) is gaining an increasing research attention in
the neural networks community. The recently introduced deep Echo State Network
(deepESN) model opened the way to an extremely efficient approach for designing
deep neural networks for temporal data. At the same time, the study of deepESNs
allowed to shed light on the intrinsic properties of state dynamics developed
by hierarchical compositions of recurrent layers, i.e. on the bias of depth in
RNNs architectural design. In this paper, we summarize the advancements in the
development, analysis and applications of deepESNs.
</summary>
    <author>
      <name>Claudio Gallicchio</name>
    </author>
    <author>
      <name>Alessio Micheli</name>
    </author>
    <link href="http://arxiv.org/abs/1712.04323v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.04323v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.05245v1</id>
    <updated>2017-12-14T14:25:52Z</updated>
    <published>2017-12-14T14:25:52Z</published>
    <title>Point-wise Convolutional Neural Network</title>
    <summary>  Deep learning with 3D data such as reconstructed point clouds and CAD models
has received great research interests recently. However, the capability of
using point clouds with convolutional neural network has been so far not fully
explored. In this technical report, we present a convolutional neural network
for semantic segmentation and object recognition with 3D point clouds. At the
core of our network is point-wise convolution, a convolution operator that can
be applied at each point of a point cloud. Our fully convolutional network
design, while being simple to implement, can yield competitive accuracy in both
semantic segmentation and object recognition task.
</summary>
    <author>
      <name>Binh-Son Hua</name>
    </author>
    <author>
      <name>Minh-Khoi Tran</name>
    </author>
    <author>
      <name>Sai-Kit Yeung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures, 10 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.05245v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.05245v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.09097v1</id>
    <updated>2018-01-27T14:41:59Z</updated>
    <published>2018-01-27T14:41:59Z</published>
    <title>Towards an Understanding of Neural Networks in Natural-Image Spaces</title>
    <summary>  Two major uncertainties, dataset bias and perturbation, prevail in
state-of-the-art AI algorithms with deep neural networks. In this paper, we
present an intuitive explanation for these issues as well as an interpretation
of the performance of deep networks in a natural-image space. The explanation
consists of two parts: the philosophy of neural networks and a hypothetic model
of natural-image spaces. Following the explanation, we slightly improve the
accuracy of a CIFAR-10 classifier by introducing an additional "random-noise"
category during training. We hope this paper will stimulate discussion in the
community regarding the topological and geometric properties of natural-image
spaces to which deep networks are applied.
</summary>
    <author>
      <name>Yifei Fan</name>
    </author>
    <author>
      <name>Anthony Yezzi</name>
    </author>
    <link href="http://arxiv.org/abs/1801.09097v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.09097v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00541v1</id>
    <updated>2018-02-02T02:24:24Z</updated>
    <published>2018-02-02T02:24:24Z</published>
    <title>Causal Learning and Explanation of Deep Neural Networks via Autoencoded
  Activations</title>
    <summary>  Deep neural networks are complex and opaque. As they enter application in a
variety of important and safety critical domains, users seek methods to explain
their output predictions. We develop an approach to explaining deep neural
networks by constructing causal models on salient concepts contained in a CNN.
We develop methods to extract salient concepts throughout a target network by
using autoencoders trained to extract human-understandable representations of
network activations. We then build a bayesian causal model using these
extracted concepts as variables in order to explain image classification.
Finally, we use this causal model to identify and visualize features with
significant causal influence on final classification.
</summary>
    <author>
      <name>Michael Harradon</name>
    </author>
    <author>
      <name>Jeff Druce</name>
    </author>
    <author>
      <name>Brian Ruttenberg</name>
    </author>
    <link href="http://arxiv.org/abs/1802.00541v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00541v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7118v1</id>
    <updated>2013-04-26T10:42:23Z</updated>
    <published>2013-04-26T10:42:23Z</published>
    <title>Synthesis of neural networks for spatio-temporal spike pattern
  recognition and processing</title>
    <summary>  The advent of large scale neural computational platforms has highlighted the
lack of algorithms for synthesis of neural structures to perform predefined
cognitive tasks. The Neural Engineering Framework offers one such synthesis,
but it is most effective for a spike rate representation of neural information,
and it requires a large number of neurons to implement simple functions. We
describe a neural network synthesis method that generates synaptic connectivity
for neurons which process time-encoded neural signals, and which makes very
sparse use of neurons. The method allows the user to specify, arbitrarily,
neuronal characteristics such as axonal and dendritic delays, and synaptic
transfer functions, and then solves for the optimal input-output relationship
using computed dendritic weights. The method may be used for batch or online
learning and has an extremely fast optimization process. We demonstrate its use
in generating a network to recognize speech which is sparsely encoded as spike
times.
</summary>
    <author>
      <name>J. Tapson</name>
    </author>
    <author>
      <name>G. Cohen</name>
    </author>
    <author>
      <name>S. Afshar</name>
    </author>
    <author>
      <name>K. Stiefel</name>
    </author>
    <author>
      <name>Y. Buskila</name>
    </author>
    <author>
      <name>R. Wang</name>
    </author>
    <author>
      <name>T. J. Hamilton</name>
    </author>
    <author>
      <name>A. van Schaik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In submission to Frontiers in Neuromorphic Engineering</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.7118v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7118v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.00531v2</id>
    <updated>2017-08-15T16:29:05Z</updated>
    <published>2017-08-01T21:53:56Z</published>
    <title>End-to-End Neural Segmental Models for Speech Recognition</title>
    <summary>  Segmental models are an alternative to frame-based models for sequence
prediction, where hypothesized path weights are based on entire segment scores
rather than a single frame at a time. Neural segmental models are segmental
models that use neural network-based weight functions. Neural segmental models
have achieved competitive results for speech recognition, and their end-to-end
training has been explored in several studies. In this work, we review neural
segmental models, which can be viewed as consisting of a neural network-based
acoustic encoder and a finite-state transducer decoder. We study end-to-end
segmental models with different weight functions, including ones based on
frame-level neural classifiers and on segmental recurrent neural networks. We
study how reducing the search space size impacts performance under different
weight functions. We also compare several loss functions for end-to-end
training. Finally, we explore training approaches, including multi-stage vs.
end-to-end training and multitask training that combines segmental and
frame-level losses.
</summary>
    <author>
      <name>Hao Tang</name>
    </author>
    <author>
      <name>Liang Lu</name>
    </author>
    <author>
      <name>Lingpeng Kong</name>
    </author>
    <author>
      <name>Kevin Gimpel</name>
    </author>
    <author>
      <name>Karen Livescu</name>
    </author>
    <author>
      <name>Chris Dyer</name>
    </author>
    <author>
      <name>Noah A. Smith</name>
    </author>
    <author>
      <name>Steve Renals</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/JSTSP.2017.2752462</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/JSTSP.2017.2752462" rel="related"/>
    <link href="http://arxiv.org/abs/1708.00531v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00531v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.00942v1</id>
    <updated>2017-10-03T00:01:55Z</updated>
    <published>2017-10-03T00:01:55Z</published>
    <title>Neural Trojans</title>
    <summary>  While neural networks demonstrate stronger capabilities in pattern
recognition nowadays, they are also becoming larger and deeper. As a result,
the effort needed to train a network also increases dramatically. In many
cases, it is more practical to use a neural network intellectual property (IP)
that an IP vendor has already trained. As we do not know about the training
process, there can be security threats in the neural IP: the IP vendor
(attacker) may embed hidden malicious functionality, i.e. neural Trojans, into
the neural IP. We show that this is an effective attack and provide three
mitigation techniques: input anomaly detection, re-training, and input
preprocessing. All the techniques are proven effective. The input anomaly
detection approach is able to detect 99.8% of Trojan triggers although with
12.2% false positive. The re-training approach is able to prevent 94.1% of
Trojan triggers from triggering the Trojan although it requires that the neural
IP be reconfigurable. In the input preprocessing approach, 90.2% of Trojan
triggers are rendered ineffective and no assumption about the neural IP is
needed.
</summary>
    <author>
      <name>Yuntao Liu</name>
    </author>
    <author>
      <name>Yang Xie</name>
    </author>
    <author>
      <name>Ankur Srivastava</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The shorth-length version of this paper is to appear in the 2017 IEEE
  International Conference on Computer Design (ICCD)</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.00942v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.00942v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0312068v1</id>
    <updated>2003-12-02T13:36:06Z</updated>
    <published>2003-12-02T13:36:06Z</published>
    <title>Cooperating Attackers in Neural Cryptography</title>
    <summary>  A new and successful attack strategy in neural cryptography is presented. The
neural cryptosystem, based on synchronization of neural networks by mutual
learning, has been recently shown to be secure under different attack
strategies. The advanced attacker presented here, named the ``Majority-Flipping
Attacker'', is the first whose success does not decay with the parameters of
the model. This new attacker's outstanding success is due to its using a group
of attackers which cooperate throughout the synchronization process, unlike any
other attack strategy known. An analytical description of this attack is also
presented, and fits the results of simulations.
</summary>
    <author>
      <name>L. N. Shacham</name>
    </author>
    <author>
      <name>E. Klein</name>
    </author>
    <author>
      <name>R. Mislovaty</name>
    </author>
    <author>
      <name>I. Kanter</name>
    </author>
    <author>
      <name>W. Kinzel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.69.066137</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.69.066137" rel="related"/>
    <link href="http://arxiv.org/abs/cond-mat/0312068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cond-mat/0312068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
