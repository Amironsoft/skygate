id	link	date	title	authors	summary
1509.06333v1	http://arxiv.org/pdf/1509.06333v1	2015	Network Capability in Localizing Node Failures via End-to-end Path   Measurements	Liang Ma|Ting He|Ananthram Swami|Don Towsley|Kin K. Leung	  We investigate the capability of localizing node failures in communication networks from binary states (normal/failed) of end-to-end paths. Given a set of nodes of interest, uniquely localizing failures within this set requires that different observable path states associate with different node failure events. However, this condition is difficult to test on large networks due to the need to enumerate all possible node failures. Our first contribution is a set of sufficient/necessary conditions for identifying a bounded number of failures within an arbitrary node set that can be tested in polynomial time. In addition to network topology and locations of monitors, our conditions also incorporate constraints imposed by the probing mechanism used. We consider three probing mechanisms that differ according to whether measurement paths are (i) arbitrarily controllable, (ii) controllable but cycle-free, or (iii) uncontrollable (determined by the default routing protocol). Our second contribution is to quantify the capability of failure localization through (1) the maximum number of failures (anywhere in the network) such that failures within a given node set can be uniquely localized, and (2) the largest node set within which failures can be uniquely localized under a given bound on the total number of failures. Both measures in (1-2) can be converted into functions of a per-node property, which can be computed efficiently based on the above sufficient/necessary conditions. We demonstrate how measures (1-2) proposed for quantifying failure localization capability can be used to evaluate the impact of various parameters, including topology, number of monitors, and probing mechanisms. 	
1311.6700v2	http://arxiv.org/pdf/1311.6700v2	2014	Failure mechanisms of load sharing complex systems	Shahnewaz Siddique|Vitali Volovoi	  We investigate the failure mechanisms of load sharing complex systems. The system is composed of multiple nodes or components whose failures are determined based on the interaction of their respective strengths and loads (or capacity and demand respectively) as well as the ability of a component to share its load with its neighbors when needed. We focus on two distinct mechanisms to model the interaction between components' strengths and loads. The failure mechanisms of these two models demonstrate temporal scaling phenomena, phase transitions and multiple distinct failure modes excited by extremal dynamics. For critical ranges of parameters the models demonstrate power law and exponential failure patterns. We identify the similarities and differences between the two mechanisms and the implications of our results to the failure mechanisms of complex systems in the real world. 	
1310.5948v1	http://arxiv.org/pdf/1310.5948v1	2013	Failure modes of complex materials with spatially-correlated mechanical   properties -- the critical role of internal damage	Jerome Faillettaz|Dani Or	  The study reports a systematic evaluation of the role of spatially correlated mechanical elements on failure behavior of heterogeneous materials represented by fiber bundle models (FBM) with different load redistribution rules. The increase of spatial correlation FBM for a local load sharing, results in a transition from ductile-like failure characteristics into brittle-like failure. The study identified a global failure criterion based on macroscopic properties (external load and cumulative damage) which is independent of spatial correlation or load redistribution rules. This invariant metric could be applied for early warning of a class of geophysical ruptures. 	
1607.07502v1	http://arxiv.org/pdf/1607.07502v1	2016	Cascading Node Failure with Continuous States in Random Geometric   Networks	Khashayar Kamran|Edmund Yeh	  The increasing complexity and interdependency of today's networks highlight the importance of studying network robustness to failure and attacks. Many large-scale networks are prone to cascading effects where a limited number of initial failures (due to attacks, natural hazards or resource depletion) propagate through a dependent mechanism, ultimately leading to a global failure scenario where a substantial fraction of the network loses its functionality. These cascading failure scenarios often take place in networks which are embedded in space and constrained by geometry. Building on previous results on cascading failure in random geometric networks, we introduce and analyze a continuous cascading failure model where a node has an initial continuously-valued state, and fails if the aggregate state of its neighbors fall below a threshold. Within this model, we derive analytical conditions for the occurrence and non-occurrence of cascading node failure, respectively. 	
1110.2270v1	http://arxiv.org/pdf/1110.2270v1	2011	Noise Analysis and Detection Based on RF Energy Duration in wireless LAN	R. Seshadri|N. Penchalaiah	  Noise is the major problem while working with wireless LAN. In this paper we analyze the noise by using active receiving antenna and also propose the detection mechanism based on RF energy duration. The standard back off mechanism of 802.11 wireless LAN (WLAN) increases the contention window when a transmission failure occurs in order to alleviate contentions in a WLAN. In addition, many proposed schemes for 802.11 WLAN behave adaptively to transmission failures. Transmission failures in WLANs occur mostly by two causes: collision and channel noise. However, in 802.11 WLAN, a station cannot know the cause of a transmission failure, thus the adaptive schemes assume the ideal situation in which all transmission failures occur by only one of two causes. For this reason, they may behave erroneously in a real world where transmission failures occur by both causes. In this paper, we propose a novel scheme to detect collision, which utilizes transmission time information and RF energy duration on the channel. By detecting collisions, a station can differentiate the causes of transmission failures and the adaptive schemes can operate correctly by using the detection information. 	
1706.04579v1	http://arxiv.org/pdf/1706.04579v1	2017	Limits of Predictability of Cascading Overload Failures in   Spatially-Embedded Networks with Distributed Flows	Alaa Moussawi|Noemi Derzsy|Xin Lin|Boleslaw K. Szymanski|Gyorgy Korniss	  Cascading failures are a critical vulnerability of complex information or infrastructure networks. Here we investigate the properties of load-based cascading failures in real and synthetic spatially-embedded network structures, and propose mitigation strategies to reduce the severity of damages caused by such failures. We introduce a stochastic method for optimal heterogeneous distribution of resources (node capacities) subject to a fixed total cost. Additionally, we design and compare the performance of networks with N-stable and (N-1)-stable network-capacity allocations by triggering cascades using various real-world node-attack and node-failure scenarios. We show that failure mitigation through increased node protection can be effectively achieved against single node failures. However, mitigating against multiple node failures is much more difficult due to the combinatorial increase in possible failures. We analyze the robustness of the system with increasing protection, and find that a critical tolerance exists at which the system undergoes a phase transition, and above which the network almost completely survives an attack. Moreover, we show that cascade-size distributions measured in this region exhibit a power-law decay. Finally, we find a strong correlation between cascade sizes induced by individual nodes and sets of nodes. We also show that network topology alone is a weak factor in determining the progression of cascading failures. 	
1101.5330v1	http://arxiv.org/pdf/1101.5330v1	2011	A contact model for the yielding of caked granular materials	L. Brendel|J. Török|R. Kirsch|U. Bröckel	  We present a visco-elastic coupling model between caked spheres, suitable for DEM simulations, which incorporates the different loading mechanisms (tension, shear, bending, torsion) in a combined manner and allows for a derivation of elastic and failure properties on a common basis. In pull, shear, and torsion failure tests with agglomerates of up to 10000 particles, we compare the failure criterion to different approximative variants of it, with respect to accuracy and computational cost. The failure of the agglomerates, which behave according to elastic parameters derived from the contact elasticity, gives also insight into the relative relevance of the different load modes. 	
0706.1134v1	http://arxiv.org/pdf/0706.1134v1	2007	Comment on: Failure of the work-Hamiltonian connection for free energy   calculations	A. Imparato|L. Peliti	  We argue that the apparent failure of the work-Hamiltonian connection for free energy calculations reported by Vilar and Rubi' (cond-mat arXiv:0704.0761v2) stems from their incorrect expression for the work. 	
0208359v2	http://arxiv.org/pdf/cond-mat/0208359v2	2003	Failure due to fatigue in fiber bundles and solids	Srutarshi Pradhan|Bikas K. Chakrabarti	  We consider first a homogeneous fiber bundle model where all the fibers have got the same stress threshold beyond which all fail simultaneously in absence of noise. At finite noise, the bundle acquires a fatigue behavior due to the noise-induced failure probability at any stress. We solve this dynamics of failure analytically and show that the average failure time of the bundle decreases exponentially as the stress increases. We also determine the avalanche size distribution during such failure and find a power law decay. We compare this fatigue behavior with that obtained phenomenologically for the nucleation of Griffith cracks. Next we study numerically the fatigue behavior of random fiber bundles having simple distributions of individual fiber strengths, at stress less than the bundle's strength (beyond which it fails instantly). The average failure time is again seen to decrease exponentially as the stress increases and the avalanche size distribution shows similar power law decay. These results are also in broad agreement with experimental observations on fatigue in solids. We believe, these observations regarding the failure time are useful for quantum breakdown phenomena in disordered systems. 	
1505.04628v1	http://arxiv.org/pdf/1505.04628v1	2015	Building a fault tolerant application using the GASPI communication   layer	Faisal Shahzad|Moritz Kreutzer|Thomas Zeiser|Rui Machado|Andreas Pieper|Georg Hager|Gerhard Wellein	  It is commonly agreed that highly parallel software on Exascale computers will suffer from many more runtime failures due to the decreasing trend in the mean time to failures (MTTF). Therefore, it is not surprising that a lot of research is going on in the area of fault tolerance and fault mitigation. Applications should survive a failure and/or be able to recover with minimal cost. MPI is not yet very mature in handling failures, the User-Level Failure Mitigation (ULFM) proposal being currently the most promising approach is still in its prototype phase. In our work we use GASPI, which is a relatively new communication library based on the PGAS model. It provides the missing features to allow the design of fault-tolerant applications. Instead of introducing algorithm-based fault tolerance in its true sense, we demonstrate how we can build on (existing) clever checkpointing and extend applications to allow integrate a low cost fault detection mechanism and, if necessary, recover the application on the fly. The aspects of process management, the restoration of groups and the recovery mechanism is presented in detail. We use a sparse matrix vector multiplication based application to perform the analysis of the overhead introduced by such modifications. Our fault detection mechanism causes no overhead in failure-free cases, whereas in case of failure(s), the failure detection and recovery cost is of reasonably acceptable order and shows good scalability. 	
1511.01446v2	http://arxiv.org/pdf/1511.01446v2	2015	ATLAS: An Adaptive Failure-aware Scheduler for Hadoop	Mbarka Soualhia|Foutse Khomh|Sofiene Tahar	  Hadoop has become the de facto standard for processing large data in today's cloud environment. The performance of Hadoop in the cloud has a direct impact on many important applications ranging from web analytic, web indexing, image and document processing to high-performance scientific computing. However, because of the scale, complexity and dynamic nature of the cloud, failures are common and these failures often impact the performance of jobs running in Hadoop. Although Hadoop possesses built-in failure detection and recovery mechanisms, several scheduled jobs still fail because of unforeseen events in the cloud environment. A single task failure can cause the failure of the whole job and unpredictable job running times. In this report, we propose ATLAS (AdapTive faiLure-Aware Scheduler), a new scheduler for Hadoop that can adapt its scheduling decisions to events occurring in the cloud environment. Using statistical models, ATLAS predicts task failures and adjusts its scheduling decisions on the fly to reduce task failure occurrences. We implement ATLAS in the Hadoop framework of Amazon Elastic MapReduce (EMR) and perform a case study to compare its performance with those of the FIFO, Fair and Capacity schedulers. Results show that ATLAS can reduce the percentage of failed jobs by up to 28% and the percentage of failed tasks by up to 39%, and the total execution time of jobs by 10 minutes on average. ATLAS also reduces CPU and memory usages. 	
0306509v5	http://arxiv.org/pdf/cond-mat/0306509v5	2003	Bug propagation and debugging in asymmetric software structures	Damien Challet|Andrea Lombardoni	  Software dependence networks are shown to be scale-free and asymmetric. We then study how software components are affected by the failure of one of them, and the inverse problem of locating the faulty component. Software at all levels is fragile with respect to the failure of a random single component. Locating a faulty component is easy if the failures only affect their nearest neighbors, while it is hard if the failures propagate further. 	
0909.4185v2	http://arxiv.org/pdf/0909.4185v2	2010	Stochastic Load-Redistribution Model for Cascading Failure Propagation	Jörg Lehmann|Jakob Bernasconi	  A new class of probabilistic models for cascading failure propagation in interconnected systems is proposed. The models take into account important characteristics of real systems that are not considered in existing generic approaches. Specifically, it is assumed that the load increments after a failure are proportional to the failed load and that the load redistribution among the remaining elements is stochastic. The models are solved analytically in terms of generalized branching processes, and the failure propagation properties of a prototype example are analyzed in detail. 	
1203.0850v2	http://arxiv.org/pdf/1203.0850v2	2012	Damage-cluster distributions and size effect on strength in compressive   failure	Lucas Girard|Jerome Weiss|David Amitrano	  We investigate compressive failure of heterogeneous materials on the basis of a continuous progressive damage model. The model explicitely accounts for tensile and shear local damage and reproduces the main features of compressive failure of brittle materials like rocks or ice. We show that the size distribution of damage-clusters, as well as the evolution of an order parameter, the size of the largest damage-cluster, argue for a critical interpretation of fracture. The compressive failure strength follows a normal distribution with a very small size effect on the mean strength, in good agreement with experiments. 	
1606.04895v3	http://arxiv.org/pdf/1606.04895v3	2016	Magnetar Outbursts from Avalanches of Hall Waves and Crustal Failures	Xinyu Li|Yuri Levin|Andrei M. Beloborodov	  We explore the interaction between Hall waves and mechanical failures inside a magnetar crust, using detailed one-dimentional models that consider temperature-sensitive plastic flow, heat transport and cooling by neutrino emission, as well as the coupling of the crustal motion to the magnetosphere. We find that the dynamics is enriched and accelerated by the fast, short-wavelength Hall waves that are emitted by each failure. The waves propagate and cause failures elsewhere, triggering avalanches. We argue that these avalanches are the likely sources of outbursts in transient magnetars. 	
1711.05802v1	http://arxiv.org/pdf/1711.05802v1	2017	Avalanche precursors of failure in hierarchical fuse networks	Paolo Moretti|Bastien Dietemann|Michael Zaiser	  We study precursors of failure in hierarchical random fuse network models which can be considered as idealizations of hierarchical (bio)materials where fibrous assemblies are held together by multi-level (hierarchical) cross-links. When such structures are loaded towards failure, the patterns of precursory avalanche activity exhibit generic scale invariance: Irrespective of load, precursor activity is characterized by power-law avalanche size distributions without apparent cut-off, with power-law exponents that decrease continuously with increasing load. This failure behavior and the ensuing super-rough crack morphology differ significantly from the findings in non-hierarchical structures. 	
0309026v1	http://arxiv.org/pdf/cs/0309026v1	2003	A thought experiment on Quantum Mechanics and Distributed Failure   Detection	Mark C. Little	  One of the biggest problems in current distributed systems is that presented by one machine attempting to determine the liveness of another in a timely manner. Unfortunately, the symptoms exhibited by a failed machine can also be the result of other causes, e.g., an overloaded machine or network which drops messages, making it impossible to detect a machine failure with cetainty until that machine recovers. This is a well understood problem and one which has led to a large body of research into failure suspectors: since it is not possible to detect a failure, the best one can do is suspect a failure and program accordingly. However, one machine's suspicions may not be the same as another's; therefore, these algorithms spend a considerable effort in ensuring a consistent view among all available machines of who is suspects of being failed. This paper describes a thought experiment on how quantum mechanics may be used to provide a failure detector that is guaranteed to give both accurate and instantaneous information about the liveness of machines, no matter the distances involved. 	
1705.10831v3	http://arxiv.org/pdf/1705.10831v3	2017	Self-Organization of Dragon Kings	Yuansheng Lin|Keith Burghardt|Martin Rohden|Pierre-André Noël|Raissa M. D'Souza	  The mechanisms underlying cascading failures are often modeled via the paradigm of self-organized criticality. Here we introduce a simple model where nodes self-organize to be either weak or strong to failure which captures the trade-off between degradation and reinforcement of nodes inherent in many network systems. If strong nodes cannot fail, this leads to power law distributions of failure sizes with so-called "Black Swan" rare events. In contrast, if strong nodes fail once a sufficient fraction of their neighbors fail, this leads to "Dragon Kings", which are massive failures caused by mechanisms distinct from smaller failures. In our model, we find that once an initial failure size is above a critical value, the Dragon King mechanism kicks in, leading to piggybacking system-wide failures. We demonstrate that the size of the initial failed weak cluster predicts the likelihood of a Dragon King event with high accuracy and we develop a simple control strategy which also reveals that a random upgrade can inadvertently make the system more vulnerable. The Dragon Kings observed are self-organized, existing throughout the parameter regime. 	
9509004v1	http://arxiv.org/pdf/quant-ph/9509004v1	1995	Quantum Mechanics as an Exotic Probability Theory	Saul Youssef	  Recent results suggest that quantum mechanical phenomena may be interpreted as a failure of standard probability theory and may be described by a Bayesian complex probability theory. 	
1608.00308v3	http://arxiv.org/pdf/1608.00308v3	2017	Experimental evidence that electrical fatigue failure obeys a   generalized Coffin-Manson law	Xiangtong He|John Y. Fu	  The empirical Coffin-Manson law has been used to characterize the low-cycle mechanical fatigue failure of metallic materials for decades. Our experimental studies reported in this letter have shown that the electrical fatigue failure in dielectrics can be well described by a fitting function having the same mathematical expression as that of the Coffin-Manson law. This observation indicates that the physical mechanism beneath the formation and evolution of atomic disordered structures, the key factor influencing both mechanical and electrical fatigue, might be the same. 	
1712.09116v1	http://arxiv.org/pdf/1712.09116v1	2017	A Mechanism of Failure in Shear Bands	Mohammad E. Torki|A. Amine Benzerga	  We have carried out dilatant plasticity simulations to investigate the process of failure inside a shear band. The constitutive model accounts for possibly inhomogeneous flow within the band, void rotation and void elongation. We found that the material in the band may soften with no increase in the void volume fraction. For a given matrix hardening capacity, the rate of softening was found to depend strongly on the ratio of shear band width to in-plane void spacing. The emergent softening led to complete loss of load bearing capacity thereby providing a physical mechanism of failure in shear bands. The mechanism is consistent with essential features of shear-fractured specimens in terms of surface roughness, porosity and dimple shape. 	
0701015v2	http://arxiv.org/pdf/cs/0701015v2	2007	Asynchronous Implementation of Failure Detectors with partial   connectivity and unknown participants	Pierre Sens|Luciana Arantes|Mathieu Bouillaguet|Véronique Martin|Fabiola Greve	  We consider the problem of failure detection in dynamic networks such as MANETs. Unreliable failure detectors are classical mechanisms which provide information about process failures. However, most of current implementations consider that the network is fully connected and that the initial number of nodes of the system is known. This assumption is not applicable to dynamic environments. Furthermore, such implementations are usually timer-based while in dynamic networks there is no upper bound for communication delays since nodes can move. This paper presents an asynchronous implementation of a failure detector for unknown and mobile networks. Our approach does not rely on timers and neither the composition nor the number of nodes in the system are known. We prove that our algorithm can implement failure detectors of class <>S when behavioral properties and connectivity conditions are satisfied by the underlying system. 	
0205090v1	http://arxiv.org/pdf/physics/0205090v1	2002	Effects of Defects on the Strength of Nanotubes:   Experimental-Computational Comparisons	T. Belytschko|S. P. Xiao|R. Ruoff	  The failure stresses and strains of nanotubes given by theoretical or numerical predictions are much higher than observed in experiments. We show that defects can explain part of this discrepancy: for an n-atom defect with 2<=n<=8, the range of failure stresses for a molecular mechanics calculation is found to be 36GPa to 64GPa. This compares quite well with upper end of the experimental failure stresses, 11GPa to 63GPa. The computed failure strains are 4% to 8%, whereas the experimental values are 2% to 13%. The underprediction of failure strains can be explained by the slippage that occurred in the experiments. The failure processes of nanotubes are clearly brittle in both the experiments and our calculations. 	
0910.0708v1	http://arxiv.org/pdf/0910.0708v1	2009	Robust Failure Detection Architecture for Large Scale Distributed   Systems	Ciprian Mihai Dobre|Florin Pop|Alexandru Costan|Mugurel Ionut Andreica|Valentin Cristea	  Failure detection is a fundamental building block for ensuring fault tolerance in large scale distributed systems. There are lots of approaches and implementations in failure detectors. Providing flexible failure detection in off-the-shelf distributed systems is difficult. In this paper we present an innovative solution to this problem. Our approach is based on adaptive, decentralized failure detectors, capable of working asynchronous and independent on the application flow. The proposed solution considers an architecture for the failure detectors, based on clustering, the use of a gossip-based algorithm for detection at local level and the use of a hierarchical structure among clusters of detectors along which traffic is channeled. The solution can scale to a large number of nodes, considers the QoS requirements of both applications and resources, and includes fault tolerance and system orchestration mechanisms, added in order to asses the reliability and availability of distributed systems. 	
1506.08352v1	http://arxiv.org/pdf/1506.08352v1	2015	Threshold for the Outbreak of Cascading Failures in Degree-degree   Uncorrelated Networks	Junbiao Liu|Xinyu Jin|Lurong Jiang|Yongxiang Xia|Bo Ouyang|Fang Dong|Yicong Lang|Wenping Zhang	  In complex networks, the failure of one or very few nodes may cause cascading failures. When this dynamical process stops in steady state, the size of the giant component formed by remaining un-failed nodes can be used to measure the severity of cascading failures, which is critically important for estimating the robustness of networks. In this paper, we provide a cascade of overload failure model with local load sharing mechanism, and then explore the threshold of node capacity when the large-scale cascading failures happen and un-failed nodes in steady state cannot connect to each other to form a large connected sub-network. We get the theoretical derivation of this threshold in degree-degree uncorrelated networks, and validate the effectiveness of this method in simulation. This threshold provide us a guidance to improve the network robustness under the premise of limited capacity resource when creating a network and assigning load. Therefore, this threshold is useful and important to analyze the robustness of networks. 	
1610.07997v1	http://arxiv.org/pdf/1610.07997v1	2016	Artificial Intelligence Safety and Cybersecurity: a Timeline of AI   Failures	Roman V. Yampolskiy|M. S. Spellchecker	  In this work, we present and analyze reported failures of artificially intelligent systems and extrapolate our analysis to future AIs. We suggest that both the frequency and the seriousness of future AI failures will steadily increase. AI Safety can be improved based on ideas developed by cybersecurity experts. For narrow AIs safety failures are at the same, moderate, level of criticality as in cybersecurity, however for general AI, failures have a fundamentally different impact. A single failure of a superintelligent system may cause a catastrophic event without a chance for recovery. The goal of cybersecurity is to reduce the number of successful attacks on the system; the goal of AI Safety is to make sure zero attacks succeed in bypassing the safety mechanisms. Unfortunately, such a level of performance is unachievable. Every security system will eventually fail; there is no such thing as a 100% secure system. 	
1703.03106v1	http://arxiv.org/pdf/1703.03106v1	2017	Oxygen migration during resistance switching and failure of hafnium   oxide memristors	Suhas Kumar|Ziwen Wang|Xiaopeng Huang|Niru Kumari|Noraica Davila|John Paul Strachan|David Vine|A. L. David Kilcoyne|Yoshio Nishi|R. Stanley Williams	  While the recent establishment of the role of thermophoresis/diffusion-driven oxygen migration during resistance switching in metal oxide memristors provided critical insights required for memristor modeling, extended investigations of the role of oxygen migration during ageing and failure remain to be detailed. Such detailing will enable failure-tolerant design, which can lead to enhanced performance of memristor-based next-generation storage-class memory. Here we directly observed lateral oxygen migration using in-situ synchrotron x-ray absorption spectromicroscopy of HfOx memristors during initial resistance switching, wear over millions of switching cycles, and eventual failure, through which we determined potential physical causes of failure. Using this information, we reengineered devices to mitigate three failure mechanisms, and demonstrated an improvement in endurance of about three orders of magnitude. 	
1703.00626v1	http://arxiv.org/pdf/1703.00626v1	2017	The RowHammer Problem and Other Issues We May Face as Memory Becomes   Denser	Onur Mutlu	  As memory scales down to smaller technology nodes, new failure mechanisms emerge that threaten its correct operation. If such failure mechanisms are not anticipated and corrected, they can not only degrade system reliability and availability but also, perhaps even more importantly, open up security vulnerabilities: a malicious attacker can exploit the exposed failure mechanism to take over the entire system. As such, new failure mechanisms in memory can become practical and significant threats to system security.   In this work, we discuss the RowHammer problem in DRAM, which is a prime (and perhaps the first) example of how a circuit-level failure mechanism in DRAM can cause a practical and widespread system security vulnerability. RowHammer, as it is popularly referred to, is the phenomenon that repeatedly accessing a row in a modern DRAM chip causes bit flips in physically-adjacent rows at consistently predictable bit locations. It is caused by a hardware failure mechanism called DRAM disturbance errors, which is a manifestation of circuit-level cell-to-cell interference in a scaled memory technology. We analyze the root causes of the RowHammer problem and examine various solutions. We also discuss what other vulnerabilities may be lurking in DRAM and other types of memories, e.g., NAND flash memory or Phase Change Memory, that can potentially threaten the foundations of secure systems, as the memory technologies scale to higher densities. We conclude by describing and advocating a principled approach to memory reliability and security research that can enable us to better anticipate and prevent such vulnerabilities. 	
0010023v1	http://arxiv.org/pdf/quant-ph/0010023v1	2000	Macroscopic Local Realism Incompatible with Quantum Mechanics: Failure   of Local Realism where Measurements give Macroscopic Uncertainties	M. D. Reid	  We show that quantum mechanics predicts a contradiction with local hidden variable theories for photon number measurements which have limited resolving power, to the point of imposing an uncertainty in the photon number result which is macroscopic in absolute terms. We show how this can be interpreted as a failure of a new premise, macroscopic local realism. 	
0808.2855v2	http://arxiv.org/pdf/0808.2855v2	2008	Comment on "Failure of the work-Hamiltonian connection for free-energy   calculations" by Jose M. G. Vilar and J. Miguel Rubi	Luca Peliti	  I point out that the arguments raised by Vilar and Rubi against the work-Hamiltonian connection in free-energy calculations imply, if correct, the failure of the statistical mechanical expression of the thermodynamical free-energy via the logarithm of the partition function. 	
0508513v1	http://arxiv.org/pdf/cond-mat/0508513v1	2005	Mechanical Failure of a Small and Confined Solid	Debasish Chaudhuri|Surajit Sengupta	  Starting from a commensurate triangular thin solid strip, confined within two hard structureless walls, a stretch along its length introduces a rectangular distortion. Beyond a critical strain the solid fails through nucleation of "smectic"-like bands. We show using computer simulations and simple density functional based arguments, how a solid-smectic transition mediates the failure. Further, we show that the critical strain introducing failure is {\em inversely} proportional to the channel width i.e. thinner strips are stronger! 	
1509.06949v1	http://arxiv.org/pdf/1509.06949v1	2015	Co-detection of acoustic emissions during failure of heterogeneous   media: new perspectives for natural hazard early warning	J. Faillettaz|D. Or|I. Reiweger	  A promising method for real time early warning of gravity driven rupture that considers both the heterogeneity of natural media and characteristics of acoustic emissions attenuation is proposed. The method capitalizes on co-detection of elastic waves emanating from micro-cracks by multiple and spatially separated sensors. Event co-detection is considered as surrogate for large event size with more frequent co-detected events marking imminence of catastrophic failure. Using a spatially explicit fiber bundle numerical model with spatially correlated mechanical strength and two load redistribution rules, we constructed a range of mechanical failure scenarios and associated failure events (mapped into AE) in space and time. Analysis considering hypothetical arrays of sensors and consideration of signal attenuation demonstrate the potential of the co-detection principles even for insensitive sensors to provide early warning for imminent global failure. 	
1309.2175v2	http://arxiv.org/pdf/1309.2175v2	2014	Cascading failures in spatially-embedded random networks	Andrea Asztalos|Sameet Sreenivasan|Boleslaw K. Szymanski|Gyorgy Korniss	  Cascading failures constitute an important vulnerability of interconnected systems. Here we focus on the study of such failures on networks in which the connectivity of nodes is constrained by geographical distance. Specifically, we use random geometric graphs as representative examples of such spatial networks, and study the properties of cascading failures on them in the presence of distributed flow. The key finding of this study is that the process of cascading failures is non-self-averaging on spatial networks, and thus, aggregate inferences made from analyzing an ensemble of such networks lead to incorrect conclusions when applied to a single network, no matter how large the network is. We demonstrate that this lack of self-averaging disappears with the introduction of a small fraction of long-range links into the network. We simulate the well studied preemptive node removal strategy for cascade mitigation and show that it is largely ineffective in the case of spatial networks. We introduce an altruistic strategy designed to limit the loss of network nodes in the event of a cascade triggering failure and show that it performs better than the preemptive strategy. Finally, we consider a real-world spatial network viz. a European power transmission network and validate that our findings from the study of random geometric graphs are also borne out by simulations of cascading failures on the empirical network. 	
0701003v1	http://arxiv.org/pdf/cond-mat/0701003v1	2006	Comment on "Failure of the Jarzynski identity for a simple quantum   system"	Shaul Mukamel	  The distribution of work done on a quantum system by instantaneously changing the Hamiltonian is shown to satisfy the Jarzynski identity. 	
0905.3860v3	http://arxiv.org/pdf/0905.3860v3	2009	A Cellular Automaton Model of Damage	C. A. Serino|W. Klein|J. B. Rundle	  We investigate the role of equilibrium methods and stress transfer range in describing the process of damage. We find that equilibrium approaches are not applicable to the description of damage and the catastrophic failure mechanism if the stress transfer is short ranged. In the long range limit, equilibrium methods apply only if the healing mechanism associated with ruptured elements is instantaneous. Furthermore we find that the nature of the catastrophic failure depends strongly on the stress transfer range. Long range transfer systems have a failure mechanism that resembles nucleation. In short range stress transfer systems, the catastrophic failure is a continuous process that, in some respects, resembles a critical point. 	
1509.03437v1	http://arxiv.org/pdf/1509.03437v1	2015	Generic failure mechanisms in adhesive bonds	Philipp Hass|Falk K. Wittel|Peter Niemz	  The failure of adhesive bondlines has been studied at the microscopic level via tensile tests. Stable crack propagation could be generated by means of samples with improved geometry, which made in-situ observations possible. The interaction of cracks with adhesive bondlines under various angles to the crack propagation was the focus of this study as well as the respective loading situations for the adhesives UF, PUR, and PVAc, which have distinctly different mechanical behaviors. It is shown how adhesive properties influence the occurrence of certain failure mechanisms and determine their appearance and order of magnitude. With the observed failure mechanisms, it becomes possible to predict the propagation path of a crack through the specimen. 	
0307734v1	http://arxiv.org/pdf/cond-mat/0307734v1	2003	Failure properties of fiber bundle models	Srutarshi Pradhan|Bikas K. Chakrabarti	  We study the failure properties of fiber bundles when continuous rupture goes on due to the application of external load on the bundles. We take the two extreme models: equal load sharing model (democratic fiber bundles) and local load sharing model. The strength of the fibers are assumed to be distributed randomly within a finite interval. The democratic fiber bundles show a solvable phase transition at a critical stress (load per fiber). The dynamic critical behavior is obtained analytically near the critical point and the critical exponents are found to be universal. This model also shows elastic-plastic like nonlinear deformation behavior when the fiber strength distribution has a lower cut-off. We solve analytically the fatigue-failure in a democratic bundle, and the behavior qualitatively agrees with the experimental observations. The strength of the local load sharing bundles is obtained numerically and compared with the existing results. Finally we map the failure phenomena of fiber bundles in terms of magnetic model (Ising model) which may resolve the ambiguity of studying the failure properties of fiber bundles in higher dimensions. 	
0310723v1	http://arxiv.org/pdf/cond-mat/0310723v1	2003	Precursors of catastrophic failures	Srutarshi Pradhan|Bikas K. Chakrabarti	  We review here briefly the nature of precursors of global failures in three different kinds of many-body dynamical systems. First, we consider the lattice models of self-organised criticality in sandpiles and investigate numerically the effect of pulsed perturbations to the systems prior to reaching their respective critical points. We consider next, the random strength fiber bundle models, under global load sharing approximation, and derive analytically the partial failure response behavior at loading level less than its global failure or critical point. Finally, we consider the two-fractal overlap model of earthquake and analyse numerically the overlap time series data as one fractal moves over the other with uniform velocity. The precursors of global or major failure in all three cases are shown to be very well characterized and prominent. 	
0404035v1	http://arxiv.org/pdf/cond-mat/0404035v1	2004	Andrade and Critical Time-to-Failure Laws in Fiber-Matrix Composites:   Experiments and Model	H. Nechad|A. Helmstetter|R. El Guerjouma|D. Sornette	  We present creep experiments on fiber composite materials. Recorded strain rates and acoustic emission (AE) rates exhibit both a power law relaxation in the primary creep regime and a power-law acceleration before global failure. In particular, we observe time-to-failure power laws in the tertiary regime for acoustic emissions over four decades in time. We also discover correlations between some characteristics of the primary creep (exponent of the power-law and duration) and the time to failure of the samples. This result indicates that the tertiary regime is dependent on the relaxation and damage processes that occur in the primary regime and suggests a method of prediction of the time to failure based on the early time recording of the strain rate or AE rate. We consider a simple model of representative elements, interacting via democratic load sharing, with a large heterogeneity of strengths. Each element consists of a non-linear dashpot in parallel with a spring. This model recovers the experimental observations of the strain rate as a function of time. 	
1001.2202v1	http://arxiv.org/pdf/1001.2202v1	2010	Local Coulomb versus Global Failure Criterion for Granular Packings	Silke Henkes|Carolina Brito|Olivier Dauchot|Wim Van Saarloos	  Contacts at the Coulomb threshold are unstable to tangential perturbations and thus contribute to failure at the microscopic level. How is such a local property related to global failure, beyond the effective picture given by a Mohr-Coulomb type failure criterion? Here, we use a simulated bed of frictional disks slowly tilted under the action of gravity to investigate the link between the avalanche process and a global generalized isostaticity criterion. The avalanche starts when the packing as a whole is still stable according to this criterion, underlining the role of large heterogeneities in the destabilizing process: the clusters of particles with fully mobilized contacts concentrate local failure. We demonstrate that these clusters, at odds with the pile as a whole, are also globally marginal with respect to generalized isostaticity. More precisely, we observe how the condition of their stability from a local mechanical proprety progressively builds up to the generalized isostaticity criterion as they grow in size and eventually span the whole system when approaching the avalanche. 	
1211.2330v3	http://arxiv.org/pdf/1211.2330v3	2014	Simultaneous first and second order percolation transitions in   interdependent networks	Dong Zhou|Amir Bashan|Reuven Cohen|Yehiel Berezin|Nadav Shnerb|Shlomo Havlin	  In a system of interdependent networks, an initial failure of nodes invokes a cascade of iterative failures that may lead to a total collapse of the whole system in a form of an abrupt first order transition. When the fraction of initial failed nodes $1-p$ reaches criticality, $p=p_c$, the abrupt collapse occurs by spontaneous cascading failures. At this stage, the giant component decreases slowly in a plateau form and the number of iterations in the cascade, $\tau$, diverges. The origin of this plateau and its increasing with the size of the system remained unclear. Here we find that simultaneously with the abrupt first order transition a spontaneous second order percolation occurs during the cascade of iterative failures. This sheds light on the origin of the plateau and on how its length scales with the size of the system. Understanding the critical nature of the dynamical process of cascading failures may be useful for designing strategies for preventing and mitigating catastrophic collapses. 	
1502.02538v2	http://arxiv.org/pdf/1502.02538v2	2015	Consensus using Asynchronous Failure Detectors	Nancy Lynch|Srikanth Sastry	  The FLP result shows that crash-tolerant consensus is impossible to solve in asynchronous systems, and several solutions have been proposed for crash-tolerant consensus under alternative (stronger) models. One popular approach is to augment the asynchronous system with appropriate failure detectors, which provide (potentially unreliable) information about process crashes in the system, to circumvent the FLP impossibility.   In this paper, we demonstrate the exact mechanism by which (sufficiently powerful) asynchronous failure detectors enable solving crash-tolerant consensus. Our approach, which borrows arguments from the FLP impossibility proof and the famous result from CHT, which shows that $\Omega$ is a weakest failure detector to solve consensus, also yields a natural proof to $\Omega$ as a weakest asynchronous failure detector to solve consensus. The use of I/O automata theory in our approach enables us to model execution in a more detailed fashion than CHT and also addresses the latent assumptions and assertions in the original result in CHT. 	
1507.04933v1	http://arxiv.org/pdf/1507.04933v1	2015	Distributed Monitoring for Prevention of Cascading Failures in   Operational Power Grids	Martijn Warnier|Stefan Dulman|Yakup Koç|Eric Pauwels	  Electrical power grids are vulnerable to cascading failures that can lead to large blackouts. Detection and prevention of cascading failures in power grids is impor- tant. Currently, grid operators mainly monitor the state (loading level) of individual components in power grids. The complex architecture of power grids, with many interdependencies, makes it difficult to aggregate data provided by local compo- nents in a timely manner and meaningful way: monitoring the resilience with re- spect to cascading failures of an operational power grid is a challenge. This paper addresses this challenge. The main ideas behind the paper are that (i) a robustness metric based on both the topology and the operative state of the power grid can be used to quantify power grid robustness and (ii) a new proposed a distributed computation method with self-stabilizing properties can be used to achieving near real-time monitoring of the robustness of the power grid. Our con- tributions thus provide insight into the resilience with respect to cascading failures of a dynamic operational power grid at runtime, in a scalable and robust way. Com- putations are pushed into the network, making the results available at each node, allowing automated distributed control mechanisms to be implemented on top. 	
1610.06942v3	http://arxiv.org/pdf/1610.06942v3	2018	Modes of failures in disordered solids	Subhadeep Roy|Soumyajyoti Biswas|Purusattam Ray	  The two principal ingredients determining the failure modes of disordered solids are the level of heterogeneity and the length scale of the region affected in the solid following a local failure. While the latter facilitates damage nucleation, the former leads to diffused damage, the two extreme failure modes. In this study, using the random fiber bundle model as a prototype for disorder solids, we classify every failure modes that are the results of interplay between these two effects. We obtain scaling criteria for the different modes and propose a general phase diagram that provides a framework for understanding previous theoretical and experimental attempts of interpolation between these modes. 	
1711.07620v2	http://arxiv.org/pdf/1711.07620v2	2017	Stability in fiber bundle model : Existence of strong links and the   effect of disorder	Subhadeep Roy	  In this paper I have studied the fiber bundle model with a fraction {\alpha} of infinitely strong fibers. Inclusion of such unbreakable fraction has been proven to affect the failure process in early studies, especially around a critical value {\alpha}_c . The present work has a twofold purpose: (i) study of failure abruptness, mainly the brittle to quasi-brittle transition point ({\delta}_c ) with varying {\alpha} and (ii) variation of {\alpha}_c as we change the disorder introduced in the model. The brittle to quasi-brittle transition is confirmed from the failure abruptness. On the other hand, the {\alpha}_c is obtained from the knowledge of failure abruptness and statistics of avalanches. It is observed that {\delta}_c scales to lower values, suggesting more quasi-brittle like continuous failure even at low strength of disorder, when {\alpha} is increased. Also, the critical fraction {\alpha}_c, required to make the model deviate from the conventional results, increases with decreasing {\delta} values. The analytical expression for {\alpha}_c shows good agreement with the numerical result. Finally, the findings in the paper are compared with previous results as well as with the real life application of composite materials. 	
9712313v1	http://arxiv.org/pdf/cond-mat/9712313v1	1997	Conditions for abrupt failure in the democratic fiber bundle model	D. Sornette|K. -T. Leung|J. V. Andersen	  We argue that the existence of abrupt failure in the democratic fiber bundle model is more general than concluded by da Silveira in his comment (cond-mat/9709327). We refute his claim that the nature of the rupture process in the DFBM depends on the ``disorder distribution only via its large failure strength behavior''. 	
9906031v1	http://arxiv.org/pdf/cond-mat/9906031v1	1999	Time to failure of hierarchical load-transfer models of fracture	M. Vazquez-Prada|J. B. Gomez|Y. Moreno|A. F. Pacheco	  The time to failure, $T$, of dynamical models of fracture for a hierarchical load-transfer geometry is studied. Using a probabilistic strategy and juxtaposing hierarchical structures of height $n$, we devise an exact method to compute $T$, for structures of height $n+1$. Bounding $T$, for large $n$, we are able to deduce that the time to failure tends to a non-zero value when $n$ tends to infinity. This numerical conclusion is deduced for both power law and exponential breakdown rules. 	
0108514v1	http://arxiv.org/pdf/cond-mat/0108514v1	2001	The effect of disorder on the fracture nucleation process	S. Ciliberto|A. Guarino|R. Scorretti	  The statistical properties of failure are studied in a fiber bundle model with thermal noise. We show that the macroscopic failure is produced by a thermal activation of microcracks. Most importantly the effective temperature of the system is amplified by the spatial disorder (heterogeneity) of the fiber bundle. The case of a time dependent force and the validity of the Kaiser effects are also discussed. These results can give more insight to the recent experimental observations on thermally activated crack and can be useful to study the failure of electrical networks. 	
0204368v1	http://arxiv.org/pdf/cond-mat/0204368v1	2002	Functional Correlation Approach to Operational Risk in Banking   Organizations	Reimer Kuehn|Peter Neu	  A Value-at-Risk based model is proposed to compute the adequate equity capital necessary to cover potential losses due to operational risks, such as human and system process failures, in banking organizations. Exploring the analogy to a lattice gas model from physics, correlations between sequential failures are modeled by as functionally defined, heterogeneous couplings between mutually supportive processes. In contrast to traditional risk models for market and credit risk, where correlations are described by the covariance of Gaussian processes, the dynamics of the model shows collective phenomena such as bursts and avalanches of process failures. 	
0509290v1	http://arxiv.org/pdf/cond-mat/0509290v1	2005	Optimization of scale-free network for random failures	Jian-Guo Liu|Zhong-Tuo Wang|Yan-Zhong Dang	  It has been found that the networks with scale-free distribution are very resilient to random failures. The purpose of this work is to determine the network design guideline which maximize the network robustness to random failures with the average number of links per node of the network is constant. The optimal value of the distribution exponent and the minimum connectivity to different network size are given in this paper. Finally, the optimization strategy how to improve the evolving network robustness is given. 	
1109.6679v1	http://arxiv.org/pdf/1109.6679v1	2011	Studies of VCSEL Failures in the Optical Readout Systems of the ATLAS   Silicon Trackers and Liquid Argon Calorimeters	Mark S. Cooke	  The readout systems for the ATLAS silicon trackers and liquid argon calorimeters utilize vertical-cavity surface-emitting laser diodes to communicate between on and off detector readout components. A number of these VCSEL devices have failed well before their expected lifetime. We summarize the failure history and present what has been learned thus far about failure mechanisms and the dependence of the lifetime on environmental conditions. 	
1607.01858v2	http://arxiv.org/pdf/1607.01858v2	2016	Mechanical Properties of Phosphorene Nanotubes: A Density Functional   Tight-Binding Study	V. Sorkin|Y. W. Zhang	  Using density functional tight-binding method, we studied the elastic properties, deformation and failure of armchair (AC) and zigzag (ZZ) phosphorene nano tubes (PNTs) under uniaxial tensile strain. We found that the deformation and failure of PNTs are very anisotropic. For ZZ PNTs, three deformation phases are recognized: The primary linear elastic phase, which is associated with the interactions between the neighboring puckers, succeeded by the bond rotation phase, where the puckered configuration of phosphorene is smoothed via bond rotation, and lastly the bond elongation phase, where the P-P bonds are directly stretched up to the maximally allowed limit and the failure is initiated by the rupture of the most stretched bonds. 	
1210.6647v2	http://arxiv.org/pdf/1210.6647v2	2013	Relative acceleration approach for conduction failure of cardiac   excitation propagation on anisotropic curved surfaces	Sehun Chun	  In cardiac electrophysiology, it is important to predict the necessary conditions for conduction failure, the failure of the cardiac excitation propagation even in the presence of normal excitable tissue, in high-dimensional anisotropic space because these conditions may provide feasible mechanisms for abnormal excitation propagations such as atrial re-entry and, subsequently, atrial fibrillation even without taking into account the time-dependent refractory region. Some conditions of conduction failure have been studied for anisotropy or simple curved surfaces, but the general conditions on anisotropic curved surfaces (anisotropic and curved surface) remain unknown. To predict and analyze conduction failure on anisotropic curved surfaces, a new analytic approach is proposed, called the relative acceleration approach borrowed from spacetime physics. Motivated by a discrete model of cardiac excitation propagation, this approach is based on the hypothesis that a large relative acceleration can translate to a dramatic increase in the curvature of the wavefront and, subsequently, to conduction failure. For simple anisotropic surfaces, the relative acceleration approach is validated by computational simulations or the previously known results from the kinematics approach. As a practical application, this approach is proposed to provide theoretical explanations of the mechanism of cardiac excitation propagation around the pulmonary vein with anatomically observed anisotropy. 	
1307.3182v1	http://arxiv.org/pdf/1307.3182v1	2013	Flaw-driven Failure in Nanostructures	X. Wendy Gu|Zhaoxuan Wu|Yong-Wei Zhang|David J. Srolovitz|Julia R. Greer	  Understanding failure in nanomaterials is critical for the design of reliable structural materials and small-scale devices that have components or microstructural elements at the nanometer length scale. No consensus exists on the effect of flaws on fracture in bulk nanostructured materials or in nanostructures. Proposed theories include nanoscale flaw tolerance and maintaining macroscopic fracture relationships at the nanoscale with virtually no experimental support. We explore fracture mechanisms in nanomaterials via nanomechanical experiments on nanostructures with pre-fabricated surface flaws in combination with molecular dynamics simulations. Nanocrystalline Pt cylinders with diameters of 120 nm with intentionally introduced surface notches were created using a template-assisted electroplating method and tested in uniaxial tension in in-situ SEM. Experiments demonstrate that 8 out of 12 samples failed at the notches and that tensile failure strengths were 1.8 GPa regardless of whether failure occurred at or away from the flaw. These findings suggest that failure location was sensitive to the presence of flaws, while strength was flaw-insensitive. Molecular dynamics simulations support these observations and show that incipient plastic deformation commences via nucleation and motion of dislocations in concert with grain boundary sliding. We postulate that such local plasticity reduces stress concentration ahead of the flaw to levels comparable with the strengths of intrinsic microstructural features like grain boundary triple junctions, a phenomenon unique to nano-scale solids that contain an internal microstructural energy landscape. This mechanism causes failure to occur at the weakest link, be it an internal inhomogeneity or a surface feature with a high local stress. 	
1706.04345v1	http://arxiv.org/pdf/1706.04345v1	2017	Towards Adaptive Resilience in High Performance Computing	Siavash Ghiasvand|Florina M. Ciorba	  Failure rates in high performance computers rapidly increase due to the growth in system size and complexity. Hence, failures became the norm rather than the exception. Different approaches on high performance computing (HPC) systems have been introduced, to prevent failures (e. g., redundancy) or at least minimize their impacts (e. g., checkpoint and restart). In most cases, when these approaches are employed to increase the resilience of certain parts of a system, energy consumption rapidly increases, or performance significantly degrades. To address this challenge, we propose on-demand resilience as an approach to achieve adaptive resilience in HPC systems. In this work, the HPC system is considered in its entirety and resilience mechanisms such as checkpointing, isolation, and migration, are activated on-demand. Using the proposed approach, the unavoidable increase in total energy consumption and system performance degradation is decreased compared to the typical checkpoint/restart and redundant resilience mechanisms. Our work aims to mitigate a large number of failures occurring at various layers in the system, to prevent their propagation, and to minimize their impact, all of this in an energy-saving manner. In the case of failures that are estimated to occur but cannot be mitigated using the proposed on-demand resilience approach, the system administrators will be notified in view of performing further investigations into the causes of these failures and their impacts. 	
1610.07340v1	http://arxiv.org/pdf/1610.07340v1	2016	Mechanical Properties and Failure Behavior of Phosphorene with Grain   Boundaries	V. Sorkin|Y. W. Zhang	  Using density functional tight-binding method, we studied the effect of grain boundaries on the mechanical properties and failure behavior of phosphorene. We found that the large angle tilt boundaries with a higher density of (5|7) defect pairs (oriented along the AC direction) are stronger than the low-angle tilt boundaries with a lower defect density, and similarly the large angle boundaries with a higher density of (4|8) defect pairs (oriented along the ZZ direction) are stronger than the low-angle boundaries with a lower defect density. The failure is due to the rupture of the most pre-strained bonds in the heptagons of the (5|7) defect pair or octagons of the (4|8) pairs. The large-angle grain boundaries are better off in accommodating the pre-strained bonds in heptagons and octagons defects, leading to a higher failure stress and strain. The results cannot be described by Griffith-type fracture mechanics criterion since it does not take into account the bond pre-stretching. Interestingly, these anomalous mechanical and failure characteristics of tilt grain boundaries in phosphorene are also shared by graphene and hexagonal born-nitride, signifying that they may be universal for 2D materials. The findings revealed here may be useful in tuning the mechanical properties of phosphorene via defect engineering for specific applications. 	
0701268v1	http://arxiv.org/pdf/cond-mat/0701268v1	2007	Burst statistics as a criterion for imminent failure	Srutarshi Pradhan|Alex Hansen|Per C. Hemmer	  The distribution of the magnitudes of damage avalanches during a failure process typically follows a power law. When these avalanches are recorded close to the point at which the system fails catastrophically, we find that the power law has an exponent which differs from the one characterizing the size distribution of all avalanches. We demonstrate this analytically for bundles of many fibers with statistically distributed breakdown thresholds for the individual fibers. In this case the magnitude distribution $D(\Delta)$ for the avalanche size $\Delta$ follows a power law $\Delta^{-\xi}$ with $\xi=3/2$ near complete failure, and $\xi=5/2$ elsewhere. We also study a network of electric fuses, and find numerically an exponent 2.0 near breakdown, and 3.0 elsewhere. We propose that this crossover in the size distribution may be used as a signal for imminent system failure. 	
0910.5179v1	http://arxiv.org/pdf/0910.5179v1	2009	Damage nucleation phenomena: Statistics of times to failure	S. G. Abaimov|A. Roy|J. P. Cusumano	  In this paper we investigate the statistical behavior of an annealed continuous damage model. For different model variations we study distributions of times to failure and compare these results with the classical case of metastable nucleation in statistical mechanics. We show that our model has a tuning parameter which significantly determines the model behavior. Depending on the values of this tuning parameter, our model exhibits statistical behavior either similar to nucleation of systems in statistical mechanics or an absolutely different type of behavior intrinsic only for systems with damage. This lets us investigate the possible similarities and differences between damage phenomena and classical phenomena of nucleation in statistical mechanics. 	
9706054v1	http://arxiv.org/pdf/quant-ph/9706054v1	1997	On the failure of Bell's theorem	Gyula Bene	  Using a new approach to quantum mechanics we revisit Hardy's proof for Bell's theorem and point out a loophole in it. We also demonstrate on this example that quantum mechanics is a local realistic theory. 	
0107036v2	http://arxiv.org/pdf/cond-mat/0107036v2	2001	Precursors of catastrophe in the BTW, Manna and random fiber bundle   models of failure	Srutarshi Pradhan|Bikas K. Chakrabarti	  We have studied precursors of the global failure in some self-organised critical models of sand-pile (in BTW and Manna models) and in the random fiber bundle model (RFB). In both BTW and Manna model, as one adds a small but fixed number of sand grains (heights) to any central site of the stable pile, the local dynamics starts and continues for an average relaxation time (\tau) and an average number of topplings (\Delta) spread over a radial distance (\xi). We find that these quantities all depend on the average height (h_{av}) of the pile and they all diverge as (h_{av}) approaches the critical height (h_{c}) from below: (\Delta) (\sim (h_{c}-h_{av}))(^{-\delta}), (\tau \sim (h_{c}-h_{av})^{-\gamma}) and (\xi) (\sim) ((h_{c}-h_{av})^{-\nu}). Numerically we find (\delta \simeq 2.0), (\gamma \simeq 1.2) and (\nu \simeq 1.0) for both BTW and Manna model in two dimensions. In the strained RFB model we find that the breakdown susceptibility (\chi) (giving the differential increment of the number of broken fibers due to increase in external load) and the relaxation time (\tau), both diverge as the applied load or stress (\sigma) approaches the network failure threshold (\sigma_{c}) from below: (\chi) (\sim) ((\sigma_{c}) (-)(\sigma)^{-1/2}) and (\tau) (\sim) ((\sigma_{c}) (-)(\sigma)^{-1/2}). These self-organised dynamical models of failure therefore show some definite precursors with robust power laws long before the failure point. Such well-characterised precursors should help predicting the global failure point of the systems in advance. 	
0601290v2	http://arxiv.org/pdf/cond-mat/0601290v2	2006	Failure process of a bundle of plastic fibers	F. Raischel|F. Kun|H. J. Herrmann	  We present an extension of fiber bundle models considering that failed fibers still carry a fraction $0 \leq \alpha \leq 1$ of their failure load. The value of $\alpha$ interpolates between the perfectly brittle failure $(\alpha = 0)$ and perfectly plastic behavior $(\alpha=1)$ of fibers. We show that the finite load bearing capacity of broken fibers has a substantial effect on the failure process of the bundle. In the case of global load sharing it is found that for $\alpha \to 1$ the macroscopic response of the bundle becomes perfectly plastic with a yield stress equal to the average fiber strength. On the microlevel, the size distribution of avalanches has a crossover from a power law of exponent $\approx 2.5$ to a faster exponential decay. For localized load sharing, computer simulations revealed a sharp transition at a well defined value $\alpha_c$ from a phase where macroscopic failure occurs due to localization as a consequence of local stress enhancements, to another one where the disordered fiber strength dominates the damage process. Analysing the microstructure of damage, the transition proved to be analogous to percolation. At the critical point $\alpha_c$, the spanning cluster of damage is found to be compact with a fractal boundary. The distribution of bursts of fiber breakings shows a power law behaviour with a universal exponent $\approx 1.5$ equal to the mean field exponent of fiber bundles of critical strength distributions. The model can be relevant to understand the shear failure of glued interfaces where failed regions can still transmit load by remaining in contact. 	
0509114v1	http://arxiv.org/pdf/physics/0509114v1	2005	Comparison of compact bone failure under two different loadings rates:   experimental and modelling approaches	Martine Pithioux|D. Subit|P. Chabrand	  Understanding the mechanical behaviour of bones up to failure is necesary for diagnosis and prevention of accident and trauma. As far as we know, no authors have yet studied the tensile behaviour of compact bone including failure under dynamic loadings (1m/s). The originality of this study comes from not only the analysis of compact bone failure under dynamic loadings, the results of which are compared to those obtained under quasi static loadings but also the development of a statistical model. We developed a protocol using three different devices. Firstly, an X-ray scanner to analyse bone density, secondly, a common tensile device to perform quasi static experiments and thirdly, a special device based upon a hydraulic cylinder to perform dynamic tests. For all the tests, we used the same sample shape which took into account the brittleness of the compact bone. We first performed relaxation and hysteresis tests followed by tensile tests up to failure. Viscous and plastic effects were not relevant to the compact bone behaviour so its behaviour was considered elastic and brittle. The bovine compact bone was three to four times more brittle under a dynamic load than under a quasi static one. Numerically, a statistical model, based upon the Weibull theory is used to predict the failure stress in compact bone. 	
1405.2038v1	http://arxiv.org/pdf/1405.2038v1	2014	Predicting Failure: Acoustic Emission of Berlinite under Compression	Guillaume F Nataf|Pedro O Castillo-Villa|Pathikumar Sellappan|Waltraud M Kriven|Eduard Vives|Antoni Planes|Ekhard K H Salje	  Acoustic emission has been measured and statistical characteristics have been analyzed during the stress-induced collapse of porous berlinite, AlPO4, containing up to 50 vol% porosity. Stress collapse occurs in a series of individual events (avalanches), and each avalanche leads to a jerk in sample compression with corresponding acoustic emission (AE) signals. The distribution of AE avalanche energies can be approximately described by a power law over a large stress interval. We observed several collapse mechanisms whereby less porous minerals show the superposition of independent jerks, which were not related to the major collapse at the failure stress. In highly porous berlinite (40% and 50%) an increase of the energy emission occurred near the failure point. In contrast, the less porous samples did not show such an increase in energy emission. Instead, in the near vicinity of the main failure point they showed a reduction in the energy exponent to ~ 1.4, which is consistent with the value reported for compressed porous systems displaying critical behavior. This indicated that a critical avalanche regime with a lack of precursor events occurs. In this case, all preceding large events were false alarms and unrelated to the main failure event. Our results identify a method to use pico-seismicity detection of foreshocks to warn of mine collapse before the main failure collapse occurs, which can be applied for highly porous materials only. 	
0609135v2	http://arxiv.org/pdf/cond-mat/0609135v2	2008	Spontaneous thermal runaway as an ultimate failure mechanism of   materials	S. Braeck|Y. Y. Podladchikov	  The first theoretical estimate of the shear strength of a perfect crystal was given by Frenkel [Z. Phys. 37, 572 (1926)]. He assumed that as slip occurred, two rigid atomic rows in the crystal would move over each other along a slip plane. Based on this simple model, Frenkel derived the ultimate shear strength to be about one tenth of the shear modulus. Here we present a theoretical study showing that catastrophic material failure may occur below Frenkel's ultimate limit as a result of thermal runaway. We demonstrate that the condition for thermal runaway to occur is controlled by only two dimensionless variables and, based on the thermal runaway failure mechanism, we calculate the maximum shear strength $\sigma_c$ of viscoelastic materials. Moreover, during the thermal runaway process, the magnitude of strain and temperature progressively localize in space producing a narrow region of highly deformed material, i.e. a shear band. We then demonstrate the relevance of this new concept for material failure known to occur at scales ranging from nanometers to kilometers. 	
0808.1375v3	http://arxiv.org/pdf/0808.1375v3	2009	Failure Processes in Elastic Fiber Bundles	Srutarshi Pradhan|Alex Hansen|Bikas K. Chakrabarti	  The fiber bundle model describes a collection of elastic fibers under load. the fibers fail successively and for each failure, the load distribution among the surviving fibers change. Even though very simple, the model captures the essentials of failure processes in a large number of materials and settings. We present here a review of fiber bundle model with different load redistribution mechanism from the point of view of statistics and statistical physics rather than materials science, with a focus on concepts such as criticality, universality and fluctuations. We discuss the fiber bundle model as a tool for understanding phenomena such as creep, and fatigue, how it is used to describe the behavior of fiber reinforced composites as well as modelling e.g. network failure, traffic jams and earthquake dynamics. 	
1707.06539v1	http://arxiv.org/pdf/1707.06539v1	2017	Vitality of Neural Networks under Reoccurring Catastrophic Failures	Shira Sardi|Amir Goldental|Hamutal Amir|Roni Vardi|Ido Kanter	  Catastrophic failures are complete and sudden collapses in the activity of large networks such as economics, electrical power grids and computer networks, which typically require a manual recovery process. Here we experimentally show that excitatory neural networks are governed by a non-Poissonian reoccurrence of catastrophic failures, where their repetition time follows a multimodal distribution characterized by a few tenths of a second and tens of seconds timescales. The mechanism underlying the termination and reappearance of network activity is quantitatively shown here to be associated with nodal time-dependent features, neuronal plasticity, where hyperactive nodes damage the response capability of their neighbors. It presents a complementary mechanism for the emergence of Poissonian catastrophic failures from damage conductivity. The effect that hyperactive nodes degenerate their neighbors represents a type of local competition which is a common feature in the dynamics of real-world complex networks, whereas their spontaneous recoveries represent a vitality which enhances reliable functionality. 	
1710.04182v1	http://arxiv.org/pdf/1710.04182v1	2017	Modeling Dynamic Helium Release as a Tracer of Rock Deformation	W. Payton Gardner|Stephen J. Bauer|Kristopher L. Kuhlman|Jason E. Heath	  We use helium released during mechanical deformation of shales as a signal to explore the effects of deformation and failure on material transport properties. A dynamic dual-permeability model with evolving pore and fracture networks is used to simulate gases released from shale during deformation and failure. Changes in material properties required to reproduce experimentally observed gas signals are explored. We model two different experiments of $^4$He flow rate measured from shale undergoing mechanical deformation, a core parallel to bedding and a core perpendicular to bedding. We find that the helium signal is sensitive to fracture development and evolution as well as changes in the matrix transport properties. We constrain the timing and effective fracture aperture, as well as the increase in matrix porosity and permeability. Increases in matrix permeability are required to explain gas flow prior to macroscopic failure, and the short-term gas flow post failure. Increased matrix porosity, is required to match the long-term, post-failure gas flow. Our model provides the first quantitative interpretation of helium release as a result of mechanical deformation. The sensitivity of this model to changes in the fracture network, as well as to matrix properties during deformation, indicates that helium release can be used as a quantitative tool to evaluate the state of stress and strain in earth materials. 	
1303.2294v1	http://arxiv.org/pdf/1303.2294v1	2013	Comprehensive Analysis on the Vulnerability and Efficiency of P2P   Networks under Static Failures and Targeted Attacks	Farshad Safaei|Hamidreza Sotoodeh	  Peer to peer systems are the networks consisting of a group of nodes possible to be as wide as the Internet. These networks are required of evaluation mechanisms and distributed control and configurations, so each peer will be able to communicate with other peers. Resilience to faults, failures and attacks, are the main requirements of most communication systems and networks today. Thus, since P2P networks can be individually used as an infrastructure and an alternative for many other communication networks, they have to be more reliable, and resilient to the faults, failures and attacks compared to the client and server approach. In this work, we present a detailed study on the behavior of various P2P networks toward faults and failures, and focus on fault-tolerance subject. We consider two different static failure scenarios: a)a random strategy in which nodes or edges of the network will be removed with an equal probability and without any knowledge of the networks infrastructure, b)a targeted strategy that uses some information about the nodes, and in which the nodes with the highest degree have the most priority to be attacked. By static faults, we mean a situation where the nodes or components encounter some faults before the network starts to work or through its operation, and will remain faulty to the end of the work session. Our goal is to introduce various measures to analyzing P2P networks evaluating their vulnerability rate. The presented criteria can be used for evaluating the reliability and vulnerability of P2P networks toward both random and targeted failures. There is no limit to the number and types of failures, the presented measures are able to be used for different types of failures and even a wide range of networks. 	
1602.04032v1	http://arxiv.org/pdf/1602.04032v1	2016	A Truthful Mechanism with Biparameter Learning for Online Crowdsourcing	Satyanath Bhat|Divya Padmanabhan|Shweta Jain|Y Narahari	  We study a problem of allocating divisible jobs, arriving online, to workers in a crowdsourcing setting which involves learning two parameters of strategically behaving workers. Each job is split into a certain number of tasks that are then allocated to workers. Each arriving job has to be completed within a deadline and each task has to be completed satisfying an upper bound on probability of failure. The job population is homogeneous while the workers are heterogeneous in terms of costs, completion times, and times to failure. The job completion time and time to failure of each worker are stochastic with fixed but unknown means. The requester is faced with the challenge of learning two separate parameters of each (strategically behaving) worker simultaneously, namely, the mean job completion time and the mean time to failure. The time to failure of a worker depends on the duration of the task handled by the worker. Assuming non-strategic workers to start with, we solve this biparameter learning problem by applying the Robust UCB algorithm. Then, we non-trivially extend this algorithm to the setting where the workers are strategic about their costs. Our proposed mechanism is dominant strategy incentive compatible and ex-post individually rational with asymptotically optimal regret performance. 	
1607.08300v2	http://arxiv.org/pdf/1607.08300v2	2017	Nonlinear viscoelasticity and generalized failure criterion for polymer   gels	Bavand Keshavarz|Thibaut Divoux|Sébastien Manneville|Gareth H. McKinley	  Polymer gels behave as soft viscoelastic solids and exhibit a generic nonlinear mechanical response characterized by pronounced stiffening prior to irreversible failure, most often through macroscopic fractures. Here, we aim at capturing the latter scenario for a protein gel using a nonlinear integral constitutive equation built upon ($i$) the linear viscoelastic response of the gel, here well described by a power-law relaxation modulus, and ($ii$) the nonlinear viscoelastic properties of the gel, encoded into a "damping function". Such formalism predicts quantitatively the gel mechanical response to a shear start-up experiment, up to the onset of macroscopic failure. Moreover, as the gel failure involves the irreversible growth of macroscopic cracks, we couple the latter stress response with Bailey's durability criterion for brittle solids in order to predict the critical values of the stress $\sigma_c$ and strain $\gamma_c$ at the failure point, and how they scale with the applied shear rate. The excellent agreement between theory and experiments suggests that the crack growth in this soft viscoelastic gel is a Markovian process, and that Baileys' criterion extends well beyond hard materials such as metals, glasses, or minerals. 	
0401074v2	http://arxiv.org/pdf/cond-mat/0401074v2	2004	Cascade control and defense in complex networks	Adilson E. Motter	  Complex networks with heterogeneous distribution of loads may undergo a global cascade of overload failures when highly loaded nodes or edges are removed due to attacks or failures. Since a small attack or failure has the potential to trigger a global cascade, a fundamental question regards the possible strategies of defense to prevent the cascade from propagating through the entire network. Here we introduce and investigate a costless strategy of defense based on a selective further removal of nodes and edges, right after the initial attack or failure. This intentional removal of network elements is shown to drastically reduce the size of the cascade. 	
0403679v1	http://arxiv.org/pdf/cond-mat/0403679v1	2004	Two-Peak and Three-Peak Optimal Complex Networks	Andre X. C. N. Valente|Abhijit Sarkar|Howard A. Stone	  A central issue in complex networks is tolerance to random failures and intentional attacks. Current literature emphasizes the dichotomy between networks with a power-law node connectivity distribution, which are robust to random failures but fragile to targeted attacks, versus networks with an exponentially decaying connectivity distribution, which are less tolerant to failures but more resilient to attacks. We prove analytically that the optimal network configuration under a classic measure of robustness is altogether different from both of the above: in all cases, failure and/or attack, there are no more than three distinct node connectivities in the optimal network. 	
0607606v1	http://arxiv.org/pdf/cond-mat/0607606v1	2006	Damage and Healing in Fatigue Fracture	F. Kun|M. H. A. S. Costa|R. N. Costa Filho|J. S. Andrade Jr|J. B. Soares|S. Zapperi|H. J. Herrmann	  We present an experimental and theoretical study of the fatigue failure of asphalt under cyclic compression. Varying the load amplitude, experiments reveal a finite fatigue limit below which the specimen does not break, while approaching the tensile strength of the material a rapid failure occurs. In the intermediate load range, the lifetime decreases with the load as a power law. We introduce two novel theoretical approaches, namely, a fiber bundle model and a fuse model, and show that both capture the major microscopic mechanisms of the fatigue failure of asphalt, providing an excellent agreement with the experimental findings. Model calculations show that the competition of damage accumulation and healing of microcracks gives rise to novel scaling laws for fatigue failure. 	
0911.1911v2	http://arxiv.org/pdf/0911.1911v2	2010	Random neighbour model for yielding	Fergal Dalton|Alberto Petri|Giorgio Pontuale	  We introduce a model for yielding, inspired by fracture models and the failure of a sheared granular medium in which the applied shear is resisted by self-organized force chains. The force chains in the granular medium (GM) are considered as a bundle of fibres of finite strength amongst which stress is randomly redistributed after any other fibre breaks under excessive load. The model provides an exponential distribution of the internal stress and a log-normal shaped distribution of failure stress, in agreement with experimental observations. The model displays critical behaviour which approaches mean field as the number of random neighbours $k$ becomes large and also displays a failure strength which remains finite in the limit of infinite size. From comparison with different models it is argued that this is an effect of uncorrelation. All these macroscopic properties appear statistically stable with respect to the choice of the chains' initial strength distribution. The investigated model is relevant for all systems in which some generic external load or pressure is borne by a number of units, independent of one another except when failure of a unit causes load transfer to some random choice of neighbouring units. 	
1009.1264v1	http://arxiv.org/pdf/1009.1264v1	2010	Can we predict the failure point of a loaded composite material?	Srutarshi Pradhan	  As a model of composite material, the fiber bundle model has been chosen -where a bundle of fibers is subjected to external load and fibers have distributed thresholds. For different loading conditions, such a system shows few precursors which indicate that the complete failure is imminent. When external load is increased quasi-statically - \textit{bursts} (number of failing fibers) of different sizes are produced. The burst statistics shows a robust crossover behavior near the failure point, around which the average burst size seems to diverge. If the load is increased by discrete steps, susceptibility and relaxation time diverge as failure point is approached. When the bundle is overloaded (external load is more than critical load) the rate of breaking shows a minimum at half way to the collapse point. The pattern and statistics of energy emission bursts show characteristic difference for below-critical and over-critical load levels. 	
1010.5198v1	http://arxiv.org/pdf/1010.5198v1	2010	Creep rupture of materials: insights from a fiber bundle model with   relaxation	E. A. Jagla	  I adapted a model recently introduced in the context of seismic phenomena, to study creep rupture of materials. It consists of linear elastic fibers that interact in an equal load sharing scheme, complemented with a local viscoelastic relaxation mechanism. The model correctly describes the three stages of the creep process, namely an initial Andrade regime of creep relaxation, an intermediate regime of rather constant creep rate, and a tertiary regime of accelerated creep towards final failure of the sample. In the tertiary regime creep rate follows the experimentally observed one over time-to-failure dependence. The time of minimum strain rate is systematically observed to be about 60-65 % of the time to failure, in accordance with experimental observations. In addition, burst size statistics of breaking events display a -3/2 power law for events close to the time of failure, and a steeper decay for the all-time distribution. Statistics of interevent times shows a tendency of the events to cluster temporarily. This behavior should be observable in acoustic emission experiments. 	
1206.2062v1	http://arxiv.org/pdf/1206.2062v1	2012	The extreme vulnerability of interdependent spatially embedded networks	Amir Bashan|Yehiel Berezin|Sergey V. Buldyrev|Shlomo Havlin	  Recent studies show that in interdependent networks a very small failure in one network may lead to catastrophic consequences. Above a critical fraction of interdependent nodes, even a single node failure can invoke cascading failures that may abruptly fragment the system, while below this "critical dependency" (CD) a failure of few nodes leads only to small damage to the system. So far, the research has been focused on interdependent random networks without space limitations. However, many real systems, such as power grids and the Internet, are not random but are spatially embedded. Here we analytically and numerically analyze the stability of systems consisting of interdependent spatially embedded networks modeled as lattice networks. Surprisingly, we find that in lattice systems, in contrast to non-embedded systems, there is no CD and \textit{any} small fraction of interdependent nodes leads to an abrupt collapse. We show that this extreme vulnerability of very weakly coupled lattices is a consequence of the critical exponent describing the percolation transition of a single lattice. Our results are important for understanding the vulnerabilities and for designing robust interdependent spatial embedded networks. 	
1307.6998v1	http://arxiv.org/pdf/1307.6998v1	2013	CDPM2: A damage-plasticity approach to modelling the failure of concrete	Peter Grassl|Dimitrios Xenos|Ulrika Nystrom|Rasmus Rempling|Kent Gylltoft	  A constitutive model based on the combination of damage mechanics and plasticity is developed to analyse the failure of concrete structures. The aim is to obtain a model, which describes the important characteristics of the failure process of concrete subjected to multiaxial loading. This is achieved by combining an effective stress based plasticity model with a damage model based on plastic and elastic strain measures. The model response in tension, uni-, bi- and triaxial compression is compared to experimental results. The model describes well the increase in strength and displacement capacity for increasing confinement levels. Furthermore, the model is applied to the structural analyses of tensile and compressive failure. 	
1310.4999v1	http://arxiv.org/pdf/1310.4999v1	2013	Creep rupture as a non-homogeneous Poissonian process	Zsuzsa Danku|Ferenc Kun	  Creep rupture of heterogeneous materials occurring under constant sub-critical external loads is responsible for the collapse of engineering constructions and for natural catastrophes. Acoustic monitoring of crackling bursts provides microscopic insight into the failure process. Based on a fiber bundle model, we show that the accelerating bursting activity when approaching failure can be described by the Omori law. For long range load redistribution the time series of bursts proved to be a non-homogeneous Poissonian process with power law distributed burst sizes and waiting times. We demonstrate that limitations of experiments such as finite detection threshold and time resolution have striking effects on the characteristic exponents, which have to be taken into account when comparing model calculations with experiments. Recording events solely within the Omori time to failure the size distribution of bursts has a crossover to a lower exponent which is promising for forecasting the imminent catastrophic failure. 	
1408.5303v1	http://arxiv.org/pdf/1408.5303v1	2014	Fracture in Disordered Heterogeneous Materials as a Stochastic Process	Yon Visell|Guillaume Millet	  Fracture processes in heterogeneous materials comprise a large number of disordered spatial degrees of freedom, representing the dynamical state of a sample over the entire domain of interest. This complexity is usually modeled directly, obscuring the underlying physics, which can often be characterized by a small number of physical parameters. In this paper, we derive a closed-form expression for a low dimensional model that reproduces the stochastic dynamical evolution of time-dependent failure in heterogeneous materials, and efficiently captures the spatial fluctuations and critical behavior near failure. Our construction is based on a novel time domain formulation of Fiber Bundle Models, which represent spatial variations in material strength via lattices of brittle, viscoelastic fiber elements. We apply the inverse transform method of random number sampling in order to construct an exact stochastic jump process for the failure sequence in a material with arbitrary strength distributions. We also complement this with a mean field approximation that captures the coupled constitutive dynamics, and validate both with numerical simulations. Our method provides a compact representation of random fiber lattices with arbitrary failure distributions, even in the presence of rapid loading and nontrivial fiber dynamics. 	
1410.8525v4	http://arxiv.org/pdf/1410.8525v4	2015	Information Theory Perspective on Network Robustness	Tiago A. Schieber|Laura Carpi|Alejandro C. Frery|Osvaldo A Rosso|Panos M. Pardalos|Martin G. Ravetti	  A crucial challenge in network theory is the study of the robustness of a network after facing a sequence of failures. In this work, we propose a dynamical definition of network's robustness based on Information Theory, that considers measurements of the structural changes caused by failures of the network's components. Failures are defined here, as a temporal process defined in a sequence. The robustness of the network is then evaluated by measuring dissimilarities between topologies after each time step of the sequence, providing a dynamical information about the topological damage. We thoroughly analyze the efficiency of the method in capturing small perturbations by considering both, the degree and distance distributions. We found the network's distance distribution more consistent in capturing network structural deviations, as better reflects the consequences of the failures. Theoretical examples and real networks are used to study the performance of this methodology. 	
1411.7827v2	http://arxiv.org/pdf/1411.7827v2	2015	Nucleation versus percolation: Scaling criterion for failure in   disordered solids	Soumyajyoti Biswas|Subhadeep Roy|Purusattam Ray	  One of the major factors governing the mode of failure in disordered solids is the effective range $R$, over which the stress field is modified following a local rupture event. In random fiber bundle model, considered as a prototype of disordered solids, we show that the failure mode is nucleation dominated in the large system size limit, as long as $R$ scales slower than $L^{\zeta}$, with $\zeta=2/3$. For a faster increase in $R$, the failure properties are dominated by the mean-field critical point, where the damages are uncorrelated in space. In that limit, the precursory avalanches of all sizes are obtained even in the large system size limit. We expect these results to be valid for systems with finite (normalizable) disorder. 	
1507.00121v1	http://arxiv.org/pdf/1507.00121v1	2015	Robustness of scale-free networks to cascading failures induced by   fluctuating loads	Shogo Mizutaka|Kousuke Yakubo	  Taking into account the fact that overload failures in real-world functional networks are usually caused by extreme values of temporally fluctuating loads that exceed the allowable range, we study the robustness of scale-free networks against cascading overload failures induced by fluctuating loads. In our model, loads are described by random walkers moving on a network and a node fails when the number of walkers on the node is beyond the node capacity. Our results obtained by using the generating function method shows that scale-free networks are more robust against cascading overload failures than Erd\H{o}s-R\'enyi random graphs with homogeneous degree distributions. This conclusion is contrary to that predicted by previous works which neglect the effect of fluctuations of loads. 	
1609.01784v1	http://arxiv.org/pdf/1609.01784v1	2016	Optimal Cloning of Quantum States with a Fixed Failure Rate	E. Bagan|V. Yerokhin|A. Shehu|E. Feldman|J. A. Bergou	  Perfect cloning of a known set of states with arbitrary prior probabilities is possible if we allow the cloner to sometimes fail completely. In the optimal case the probability of failure is at its minimum allowed by the laws of quantum mechanics. Here we show that it is possible to lower the failure rate below that of the perfect probabilistic cloner but the price to pay is that the clones are not perfect; the global fidelity is less than one. We determine the optimal fidelity of a cloner with a Fixed Failure Rate (FFR cloner) in the case of a pair of known states. Optimality is shown to be attainable by a measure-and-prepare protocol in the limit of infinitely many clones. The optimal protocol consists of discrimination with a fixed rate of inconclusive outcome followed by preparation of the appropriate clones. The convergence shows a symmetry-breaking second-order phase transition in the fidelity of the approximate infinite clones. 	
1610.09705v2	http://arxiv.org/pdf/1610.09705v2	2017	The sounds of failure: forecasting granular slip events with passive   acoustic measurements	Theodore A. Brzinski|Karen E. Daniels	  Granular materials can fail through spontaneous events like earthquakes or brittle fracture. However, measurements and analytic models which forecast failure in this class of materials, while of both fundamental and practical interest, remain elusive. Materials including numerical packings of spheres, colloidal glasses, and granular materials have been known to develop an excess of low-frequency vibrational modes as the confining pressure is reduced. Here, we report experiments on sheared granular materials in which we monitor the evolving density of excited modes via passive monitoring of acoustic emissions. We observe a broadening of the distribution of excited modes coincident with both bulk and local plasticity, and clear evolution in the shape of the distribution before and after bulk failure. These results provide a new interpretation of the changing state of the material on its approach to stick-slip failure. 	
1801.01930v1	http://arxiv.org/pdf/1801.01930v1	2018	Universal avalanche statistics and triggering close to failure in a mean   field model of rheological fracture	Jordi Baró|Jörn Davidsen	  The hypothesis of critical failure relates the presence of an ultimate stability point in the structural constitutive equation of materials to a divergence of characteristic scales in the microscopic dynamics responsible for deformation. Avalanche models involving critical failure have determined common universality classes for stick-slip processes and fracture. However, not all empirical failure processes exhibit the trademarks of criticality. The rheological properties of materials introduce dissipation, usually reproduced in conceptual models as a hardening of the coarse grained elements of the system. Here, we investigate the effects of transient hardening on (i) the activity rate and (ii) the statistical properties of avalanches. We find the explicit representation of transient hardening in the presence of generalized viscoelasticity and solve the corresponding mean field model of fracture. In the quasistatic limit, the accelerated energy release is invariant with respect to rheology and the avalanche propagation can be reinterpreted in terms of a stochastic counting process. A single universality class can be defined from such analogy, and all statistical properties depend only on the distance to criticality. We also prove that inter-event correlations emerge due to the hardening --- even in the quasistatic limit --- that can be interpreted as "aftershocks" and "foreshocks". 	
9207260v1	http://arxiv.org/pdf/hep-ph/9207260v1	1992	Color Confinement, Abelian Gauge and Renormalization Group Flow	H. Hata|I. Niigata	  Under the assumption that the color charge can be written in a BRST exact form, the color confinement mechanism proposed by Kugo and Ojima (KO) explains the confinement of any colored particles including dynamical quarks and gluons. This mechanism, however, is known to break down in the Abelian gauge which treats the maximal Abelian subgroup of the gauge group in a special manner. In order to study whether the failure of the KO mechanism is particular only to the Abelian gauge or whether this failure occurs in a wide class of gauges including the ordinary Lorentz type gauge, we carry out a renormalization group study of the $SU(2)$ gauge theory in the gauge fixing space. Our gauge fixing space consists of four distinct regions that are not connected with each other by renormalization group flows, and we find that the Abelian gauge is {\it infrared unstable} in three regions which include the Lorentz type gauge. This suggests that the failure of the KO mechanism is a phenomenon which occurs only in the Abelian gauge. We also find that the Lorentz gauge is infrared stable. 	
1004.1849v1	http://arxiv.org/pdf/1004.1849v1	2010	Failure mechanisms of graphene under tension	Chris A. Marianetti|Hannah G. Yevick	  Recent experiments established pure graphene as the strongest material known to mankind, further invigorating the question of how graphene fails. Using density functional theory, we reveal the mechanisms of mechanical failure of pure graphene under a generic state of tension. One failure mechanism is a novel soft-mode phonon instability of the $K_1$-mode, whereby the graphene sheet undergoes a phase transition and is driven towards isolated benzene rings resulting in a reduction of strength. The other is the usual elastic instability corresponding to a maximum in the stress-strain curve. Our results indicate that finite wave vector soft modes can be the key factor in limiting the strength of monolayer materials. 	
1204.2605v1	http://arxiv.org/pdf/1204.2605v1	2012	On the dynamics of mechanical failures in magnetized neutron-star crusts	Yuri Levin|Maxim Lyutikov	  We consider the dynamics of a mechanical failure induced by a shear stress in a strongly magnetized neutron-star crust. We show that even if the elastic properties of the crust allow the creation of a shear crack, the strongly sheared magnetic field around the crack leads to a back-reaction from the Lorentz force which does not allow large relative displacement of the crack surfaces. Instead, the global evolution of the crack proceeds on a slow resistive time scale, and is unable to release any substantial mechanical energy. Our calculations demostrate that for {\it some} magnetic-field configurations, the magnetic forces cause, effectively, a plastic deformation of the crust when the resulting elastic shear stress exceeds the critical value for mechanical failure. 	
1606.06283v1	http://arxiv.org/pdf/1606.06283v1	2016	Failure mechanisms of single-crystal silicon electrodes in lithium-ion   batteries	Feifei Shi|Zhichao Song|Philip N. Ross|Gabor A. Somorjai|Robert O. Ritchie|Kyriakos Komvopoulos	  Long-term durability is a major obstacle limiting the widespread use of lithium ion batteries (LIBs) in heavy-duty applications and others demanding extended lifetime. As one of the root causes of degradation and failure of battery performance, the electrode failure mechanisms are still unknown. Here, we reveal the fundamental fracture mechanisms of single-crystal silicon electrodes over extended lithiation/delithiation cycles, using electrochemical testing, microstructure characterization, fracture mechanics, and finite element analysis. Anisotropic lithium invasion causes crack initiation perpendicular to the electrode surface, followed by growth through the electrode thickness. The low fracture energy of the lithiated/unlithiated silicon interface provides a weak microstructural path for crack deflection, accounting for the crack patterns and delamination observed after repeated cycling. Based on this physical understanding, we demonstrate how electrolyte additives can heal electrode cracks and provide strategies to enhance the fracture resistance in future LIBs from surface chemical, electrochemical, and material science perspectives. 	
1506.00391v1	http://arxiv.org/pdf/1506.00391v1	2015	CCNCheck: Enabling Checkpointed Distributed Applications in Content   Centric Networks	Nitinder Mohan|Pushpendra Singh	  We consider the problem of checkpointing a distributed application efficiently in Content Centric Networks so that it can withstand transient failures. We present CCNCheck, a system which enables a sender optimized way of checkpointing distributed applications in CCN's and provides an efficient mechanism for failure recovery in such applications. CCNCheck's checkpointing mechanism is a fork of DMTCP repository CCNCheck is capable of running any distributed application written in C/C++ language. 	
1612.08911v1	http://arxiv.org/pdf/1612.08911v1	2016	Fiber networks below the isostatic point: fracture without stress   concentration	Leyou Zhang|D. Zeb Rocklin|Leonard M. Sander|Xiaoming Mao	  Crack nucleation is a ubiquitous phenomena during materials failure, because stress focuses on crack tips. It is known that exceptions to this general rule arise in the limit of strong disorder or vanishing mechanical stability, where stress distributes over a divergent length scale and the material displays diffusive damage. Here we show, using simulations, that a class of diluted lattices displays a new critical phase when they are below isostaticity, where stress never concentrates, damage always occurs over a divergent length scale, and catastrophic failure is avoided. 	
0808.1224v1	http://arxiv.org/pdf/0808.1224v1	2008	Comment on: Failure of the Work-Hamiltonian Connection for Free-Energy   Calculations [Phys Rev Lett 100, 020601 (2008), arXiv:0704.0761]	Jordan Horowitz|Christopher Jarzynski	  We comment on a Letter by Vilar and Rubi [arXiv:0704.0761]. 	
0201067v3	http://arxiv.org/pdf/cond-mat/0201067v3	2002	Dynamic critical behavior of failure and plastic deformation in the   random fiber bundle model	S. Pradhan|P. Bhattacharyya|B. K. Chakrabarti	  The random fiber bundle (RFB) model, with the strength of the fibers distributed uniformly within a finite interval, is studied under the assumption of global load sharing among all unbroken fibers of the bundle. At any fixed value of the applied stress (load per fiber initially present in the bundle), the fraction of fibers that remain unbroken at successive time steps is shown to follow simple recurrence relations. The model is found to have stable fixed point for applied stress in the range 0 and 1; beyond which total failure of the bundle takes place discontinuously. The dynamic critical behavior near this failure point has been studied for this model analysing the recurrence relations. We also investigated the finite size scaling behavior. At the critical point one finds strict power law decay (with time t) of the fraction of unbroken fibers. The avalanche size distribution for this mean-field dynamics of failure has been studied. The elastic response of the RFB model has also been studied analytically for a specific probability distribution of fiber strengths, where the bundle shows plastic behavior before complete failure, following an initial linear response. 	
0709.2642v1	http://arxiv.org/pdf/0709.2642v1	2007	Techniques, advances, problems and issues in numerical modelling of   landslide hazard	Theo Van Asch|Jean-Philippe Malet|Ludovicus Van Beek|David Amitrano	  Slope movements (e.g. landslides) are dynamic systems that are complex in time and space and closely linked to both inherited and current preparatory and triggering controls. It is not yet possible to assess in all cases conditions for failure, reactivation and rapid surges and successfully simulate their transient and multi-dimensional behaviour and development, although considerable progress has been made in isolating many of the key variables and elementary mechanisms and to include them in physically-based models for landslide hazard assessments. Therefore, the objective of this paper is to review the state-of-the-art in the understanding of landslide processes and to identify some pressing challenges for the development of our modelling capabilities in the forthcoming years for hazard assessment. This paper focuses on the special nature of slope movements and the difficulties related to simulating their complex time-dependent behaviour in mathematical, physically-based models. It analyses successively the research frontiers in the recognition of first-time failures (pre-failure and failure stages), reactivation and the catastrophic transition to rapid gravitational processes (post-failure stage). Subsequently, the paper discusses avenues to transfer local knowledge on landslide activity to landslide hazard forecasts on regional scales and ends with an outline how geomorphological investigations and supporting monitoring techniques could be applied to improve the theoretical concepts and the modelling performance of physically-based landslide models at different spatial and temporal scales. 	
1312.7126v1	http://arxiv.org/pdf/1312.7126v1	2013	Link Quality and MAC-Overhead aware Predictive Preemptive Routing   Protocol for Mobile Ad hoc Network	Ali Cherif Moussa|Faraoun Mohamed Kamel	  In Ad Hoc networks, route failure may occur due to less received power, mobility, congestion and node failures. Many approaches have been proposed in literature to solve this problem, where a node predicts pre-emptively the route failure that occurs with the less received power. However, this approach encounters some difficulties, especially in scenario without mobility where route failures may arise. In this paper, we propose an improvement of AODV protocol called LO-PPAODV (Link Quality and MAC-Overhead aware Predictive Preemptive AODV). This protocol is based on new metric combine more routing metrics (Link Quality, MAC Overhead) between each node and one hop neighbor. Also we propose a cross-layer networking mechanism to distinguish between both situations, failures due to congestion or mobility, and consequently avoiding unnecessary route repair process. The LO-PPAODV was implemented using NS-2. The simulation results show that our approach improves the overall performance of the network. It reduces the average end to end delay, the routing overhead, MAC errors and route errors, and increases the packet delivery fraction of the network. 	
1407.6177v1	http://arxiv.org/pdf/1407.6177v1	2014	Power laws statistics of cliff failures, scaling and percolation	Andrea Baldassarri|Bernard Sapoval	  The size of large cliff failures may be described in several ways, for instance considering the horizontal eroded area at the cliff top and the maximum local retreat of the coastline. Field studies suggest that, for large failures, the frequencies of these two quantities decrease as power laws of the respective magnitudes, defining two different decay exponents. Moreover, the horizontal area increases as a power law of the maximum local retreat, identifying a third exponent. Such observation suggests that the geometry of cliff failures are statistically similar for different magnitudes. Power laws are familiar in the physics of critical systems. The corresponding exponents satisfy precise relations and are proven to be universal features, common to very different systems. Following the approach typical of statistical physics, we propose a "scaling hypothesis" resulting in a relation between the three above exponents: there is a precise, mathematical relation between the distributions of magnitudes of erosion events and their geometry. Beyond its theoretical value, such relation could be useful for the validation of field catalogs analysis. Pushing the statistical physics approach further, we develop a numerical model of marine erosion that reproduces the observed failure statistics. Despite the minimality of the model, the exponents resulting from extensive numerical simulations fairly agree with those measured on the field. These results suggest that the mathematical theory of percolation, which lies behind our simple model, can possibly be used as a guide to decipher the physics of rocky coast erosion and could provide precise predictions to the statistics of cliff collapses. 	
1602.03153v1	http://arxiv.org/pdf/1602.03153v1	2016	Limiting Self-Propagating Malware Based on Connection Failure Behavior   through Hyper-Compact Estimators	You Zhou|Yian Zhou|Shigang Chen|O. Patrick Kreidl	  Self-propagating malware (e.g., an Internet worm) exploits security loopholes in software to infect servers and then use them to scan the Internet for more vulnerable servers. While the mechanisms of worm infection and their propagation models are well understood, defense against worms remains an open problem. One branch of defense research investigates the behavioral difference between worm-infected hosts and normal hosts to set them apart. One particular observation is that a worm-infected host, which scans the Internet with randomly selected addresses, has a much higher connection-failure rate than a normal host. Rate-limit algorithms have been proposed to control the spread of worms by traffic shaping based on connection failure rate. However, these rate-limit algorithms can work properly only if it is possible to measure failure rates of individual hosts efficiently and accurately. This paper points out a serious problem in the prior method. To address this problem, we first propose a solution based on a highly efficient double-bitmap data structure, which places only a small memory footprint on the routers, while providing good measurement of connection failure rates whose accuracy can be tuned by system parameters. Furthermore, we propose another solution based on shared register array data structure, achieving better memory efficiency and much larger estimation range than our double-bitmap solution. 	
0412478v1	http://arxiv.org/pdf/cond-mat/0412478v1	2004	Characterization of Failure Mechanism in Composite Materials Through   Fractal Analysis of Acoustic Emission Signals	F. E. Silva|L. L. Goncalves|D. B. B. Fereira|J. M. A. Rebello	  In this paper it is presented a detailed numerical investigation of acoustic emission signals obtained from test samples of fibreglass reinforced polymeric matrix composites, when subjected to tensile and flexural tests. Various fractal indices, characteristic of the signals emitted at the different structural failures of the test samples and which satisfy non-stationary distributions, have been determined. From the results obtained for these indices, related to the Hurst analysis, detrended fluctuation analysis, minimal cover analysis and to the boxcounting dimension analysis, it has been shown they can discriminate the different failure mechanisms and, therefore, they constitute their signature. 	
9710002v1	http://arxiv.org/pdf/patt-sol/9710002v1	1997	Propagation Failure in Excitable Media	Aric Hagberg|Ehud Meron	  We study a mechanism of pulse propagation failure in excitable media where stable traveling pulse solutions appear via a subcritical pitchfork bifurcation. The bifurcation plays a key role in that mechanism. Small perturbations, externally applied or from internal instabilities, may cause pulse propagation failure (wave breakup) provided the system is close enough to the bifurcation point. We derive relations showing how the pitchfork bifurcation is unfolded by weak curvature or advective field perturbations and use them to demonstrate wave breakup. We suggest that the recent observations of wave breakup in the Belousov-Zhabotinsky reaction induced either by an electric field or a transverse instability are manifestations of this mechanism. 	
1310.8486v1	http://arxiv.org/pdf/1310.8486v1	2013	On the Combination of Silent Error Detection and Checkpointing	Guillaume Aupy|Anne Benoit|Thomas Hérault|Yves Robert|Frédéric Vivien|Dounia Zaidouni	  In this paper, we revisit traditional checkpointing and rollback recovery strategies, with a focus on silent data corruption errors. Contrarily to fail-stop failures, such latent errors cannot be detected immediately, and a mechanism to detect them must be provided. We consider two models: (i) errors are detected after some delays following a probability distribution (typically, an Exponential distribution); (ii) errors are detected through some verification mechanism. In both cases, we compute the optimal period in order to minimize the waste, i.e., the fraction of time where nodes do not perform useful computations. In practice, only a fixed number of checkpoints can be kept in memory, and the first model may lead to an irrecoverable failure. In this case, we compute the minimum period required for an acceptable risk. For the second model, there is no risk of irrecoverable failure, owing to the verification mechanism, but the corresponding overhead is included in the waste. Finally, both models are instantiated using realistic scenarios and application/architecture parameters. 	
1601.03237v1	http://arxiv.org/pdf/1601.03237v1	2016	Dynamic weakening by acoustic fluidization during stick-slip motion	Ferdinando Giacco|Luigi Saggese|Lucilla de Arcangelis|Eugenio Lippiello|Massimo Pica Ciamarra	  The unexpected weakness of some faults has been attributed to the emergence of acoustic waves that promote failure by reducing the confining pressure through a mechanism known as acoustic fluidization, also proposed to explain earthquake remote triggering. Here we validate this mechanism via the numerical investigation of a granular fault model system. We find that the stick-slip dynamics is affected only by perturbations applied at a characteristic frequency corresponding to oscillations normal to the fault, leading to gradual dynamical weakening as failure is approaching. Acoustic waves at the same frequency spontaneously emerge at the onset of failure in absence of perturbations, supporting the relevance of acoustic fluidization in earthquake triggering. 	
0502303v1	http://arxiv.org/pdf/cond-mat/0502303v1	2005	Intrinsic vulnerabilities to mechanical failure in nanoscale films	Pooja Shah|Thomas M. Truskett	  We use molecular simulations to explore how sample dimensions and interfacial properties impact some generic aspects of the mechanical and structural behavior of nanoconfined materials. Specifically, we calculate the strain-dependent properties of minimum-energy thin-film particle configurations (i.e., inherent structures) confined between attractive, parallel substrates. We examine how the relationship between the transverse strain and the stress tensor (the equation of state of the energy landscape) depends on the properties of the film and substrate. We find that both film thickness and film-substrate attractions influence not only the mechanical properties of thin films, but also the shape and location of the "weak spots" where voids preferentially form in a film as it is strained beyond its point of maximum tensile stress. The sensitivity of weak spots to film properties suggests that nanoscale materials may be intrinsically vulnerabile to specific mechanisms of mechanical failure. 	
1710.08334v1	http://arxiv.org/pdf/1710.08334v1	2017	Micromechanical Analysis of Strength of Polymer Networks with   Polydisperse Structures	Mohammad Tehrani	  The effect of network chain distribution on the mechanical behavior of elastomers is one of the long-standing problems in rubber mechanics. The classical theory of rubber elasticity is built upon the assumption of entropic elasticity of networks whose constitutive strands are of uniform length. The kinetic theories for vulcanization, computer simulations, and indirect experimental measurements all indicate that the microstructure of vulcanizates is made of polymer strands with a random distribution of length. The polydispersity in strand length is expected to control the mechanical strength of rubber as the overloaded short strands break at small deformations and transfer the load to the longer strands. The purpose of the contributions presented in this thesis is to present simple theories of rubber mechanics that take into account the length distribution of strands and its effect on elasticity and the onset of bulk failure in unfilled and filled elastomers. In unfilled system, the population of short chains is identified as the culprits for damage initiation. Upon deformation of a polydisperse network, shorter strands break at considerably smaller stretches compared to the longer ones. The network alteration continues concurrent with increasing deformation and controls the onset of mechanical failure. In the filled networks, the degradation in network mechanical behavior is assumed to be controlled by the adhesive failure of the short strands adsorbed onto the filler surface. The finite extensibility of the short adsorbed strands is a key determinant of mechanical strength. 	
1011.5072v1	http://arxiv.org/pdf/1011.5072v1	2010	A self-managing fault management mechanism for wireless sensor networks	Muhammad Asim|Hala Mokhtar|Madjid Merabti	  A sensor network can be described as a collection of sensor nodes which co-ordinate with each other to perform some specific function. These sensor nodes are mainly in large numbers and are densely deployed either inside the phenomenon or very close to it. They can be used for various application areas (e.g. health, military, home). Failures are inevitable in wireless sensor networks due to inhospitable environment and unattended deployment. Therefore, it is necessary that network failures are detected in advance and appropriate measures are taken to sustain network operation. We previously proposed a cellular approach for fault detection and recovery. In this paper we extend the cellular approach and propose a new fault management mechanism to deal with fault detection and recovery. We propose a hierarchical structure to properly distribute fault management tasks among sensor nodes by introducing more 'self-managing' functions. The proposed failure detection and recovery algorithm has been compared with some existing related work and proven to be more energy efficient. 	
1310.2466v1	http://arxiv.org/pdf/1310.2466v1	2013	Geometric analysis on the unidirectionality of the pulmonary veins for   atrial reentry	Sehun Chun	  It is widely believed that the pulmonary veins (PVs) of the atrium play the central role in the generation of atrial reentry leading to atrial fibrillation, but its mechanism has not been analytically explained. In order to improve the current clinical procedures for atrial reentry by understanding its mechanism, geometrical analysis is proposed on the conditions of conduction failure at the PVs and is validated by various computational modeling. To achieve this, a new analytic approach is proposed by adapting the geometric relative acceleration analysis from spacetime physics on the hypothesis that a large relative acceleration can translate to a dramatic increase in the curvature of the wavefront and subsequently to conduction failure. This analytic method is applied to a simplified model of the PV to reveal the strong dependency of the propagational direction and the magnitude of anisotropy for conduction failure. The unidirectionality of the PVs follows directly and is validated by computational tests in a T-shaped domain, computational simulations for three-dimensional atrial reentry and previous in-silico reports for atrial reentry. 	
1411.7711v2	http://arxiv.org/pdf/1411.7711v2	2015	Detour Planning for Fast and Reliable Failure Recovery in SDN with   OpenState	Antonio Capone|Carmelo Cascone|Alessandro Q. T. Nguyen|Brunilde Sansò	  A reliable and scalable mechanism to provide protection against a link or node failure has additional requirements in the context of SDN and OpenFlow. Not only it has to minimize the load on the controller, but it must be able to react even when the controller is unreachable. In this paper we present a protection scheme based on precomputed backup paths and inspired by MPLS crankback routing, that guarantees instantaneous recovery times and aims at zero packet-loss after failure detection, regardless of controller reachability, even when OpenFlow's "fast-failover" feature cannot be used. The proposed mechanism is based on OpenState, an OpenFlow extension that allows a programmer to specify how forwarding rules should autonomously adapt in a stateful fashion, reducing the need to rely on remote controllers. We present the scheme as well as two different formulations for the computation of backup paths. 	
1502.05179v1	http://arxiv.org/pdf/1502.05179v1	2015	Dependability Tests Selection Based on the Concept of Layered Networks	Andrey A. Shchurov|Radek Marik	  Nowadays, the consequences of failure and downtime of distributed systems have become more and more severe. As an obvious solution, these systems incorporate protection mechanisms to tolerate faults that could cause systems failures and system dependability must be validated to ensure that protection mechanisms have been implemented correctly and the system will provide the desired level of reliable service. This paper presents a systematic approach for identifying (1) characteristic sets of critical system elements for dependability testing (single points of failure and recovery groups) based on the concept of layered networks; and (2) the most important combinations of components from each recovery group based on a combinatorial technique. Based on these combinations, we determine a set of test templates to be performed to demonstrate system dependability. 	
1502.07433v4	http://arxiv.org/pdf/1502.07433v4	2015	Maximizing the strength of fiber bundles under uniform loading	Soumyajyoti Biswas|Parongama Sen	  The collective strength of a system of fibers, each having a failure threshold drawn randomly from a distribution, indicates the maximum load carrying capacity of different disordered systems ranging from disordered solids, power-grid networks, to traffic in a parallel system of roads. In many of the cases where the redistribution of load following a local failure can be controlled, it is a natural requirement to find the most efficient redistribution scheme, i.e., following which system can carry the maximum load. We address the question here and find that the answer depends on the mode of loading. We analytically find the maximum strength and corresponding redistribution schemes for sudden and quasi static loading. The associated phase transition from partial to total failure by increasing the load has been studied. The universality class is found to be dependent on the redistribution mechanism. 	
1510.01133v3	http://arxiv.org/pdf/1510.01133v3	2016	A New Method for Compensation and Rematch of Cavity Failure in C-ADS   Linac	Zhou Xue|JianPing Dai|Cai Meng	  For proton linear accelerators used in applications such as accelerator-driven systems, due to the nature of the operation, it is essential for the beam failure rate to be several orders of magnitude lower than usual performance of similar accelerators. A fault-tolerant mechanism should be mandatorily imposed in order to maintain short recovery time, high uptime and extremely low frequency of beam loss. This paper proposes an innovative and challenging way for compensation and rematch of cavity failure using fast electronic devices and Field Programmable Gate Arrays (FPGAs) instead of embedded computers to complete the computation of beam dynamics. A method of building an equivalent model for the FPGA, with optimization using a genetic algorithm, is shown. Results based on the model and algorithm are compared with TRACEWIN simulation to show the precision and correctness of the mechanism. 	
1511.00235v1	http://arxiv.org/pdf/1511.00235v1	2015	Broadband Macroscopic Cortical Oscillations Emerge from Intrinsic   Neuronal Response Failures	Amir Goldental|Roni Vardi|Shira Sardi|Pinhas Sabo|Ido Kanter	  Broadband spontaneous macroscopic neural oscillations are rhythmic cortical firing which were extensively examined during the last century, however, their possible origination is still controversial. In this work we show how macroscopic oscillations emerge in solely excitatory random networks and without topological constraints. We experimentally and theoretically show that these oscillations stem from the counterintuitive underlying mechanism - the intrinsic stochastic neuronal response failures. These neuronal response failures, which are characterized by short-term memory, lead to cooperation among neurons, resulting in sub- or several- Hertz macroscopic oscillations which coexist with high frequency gamma oscillations. A quantitative interplay between the statistical network properties and the emerging oscillations is supported by simulations of large networks based on single-neuron in-vitro experiments and a Langevin equation describing the network dynamics. Results call for the examination of these oscillations in the presence of inhibition and external drives. 	
1702.04138v1	http://arxiv.org/pdf/1702.04138v1	2017	Agent Failures in All-Pay Auctions	Yoad Lewenberg|Omer Lev|Yoram Bachrach|Jeffrey S. Rosenschein	  All-pay auctions, a common mechanism for various human and agent interactions, suffers, like many other mechanisms, from the possibility of players' failure to participate in the auction. We model such failures, and fully characterize equilibrium for this class of games, we present a symmetric equilibrium and show that under some conditions the equilibrium is unique. We reveal various properties of the equilibrium, such as the lack of influence of the most-likely-to-participate player on the behavior of the other players. We perform this analysis with two scenarios: the sum-profit model, where the auctioneer obtains the sum of all submitted bids, and the max-profit model of crowdsourcing contests, where the auctioneer can only use the best submissions and thus obtains only the winning bid.   Furthermore, we examine various methods of influencing the probability of participation such as the effects of misreporting one's own probability of participating, and how influencing another player's participation chances changes the player's strategy. 	
1704.00943v3	http://arxiv.org/pdf/1704.00943v3	2017	Mechanics of disordered auxetic metamaterials	Maryam Hanifpour|Charlotte F. Petersen|Mikko J. Alava|Stefano Zapperi	  Auxetic materials are of great engineering interest not only because of their fascinating negative Poisson's ratio, but also due to their increased toughness and indentation resistance. These materials are typically synthesized polyester foams with a very heterogeneous structure, but the role of disorder in auxetic behavior is not fully understood. Here, we provide a systematic theoretical and experimental investigation in to the effect of disorder on the mechanical properties of a paradigmatic auxetic lattice with a re-entrant hexagonal geometry. We show that disorder has a marginal effect on the Poisson's ratio unless the lattice topology is altered, and in all cases examined the disorder preserves the auxetic characteristics. Depending on the direction of loading applied to these disordered auxetic lattices, either brittle or ductile failure is observed. It is found that brittle failure is associated with a disorder-dependent tensile strength, whereas in ductile failure disorder does not affect strength. Our work thus provides general guidelines to optimize elasticity and strength of disordered auxetic metamaterials. 	
9811297v1	http://arxiv.org/pdf/cond-mat/9811297v1	1998	Scaling in the time-dependent failure of a fiber bundle with local load   sharing	Shu-dong Zhang	  We study the scaling behaviors of a time-dependent fiber-bundle model with local load sharing. Upon approaching the complete failure of the bundle, the breaking rate of fibers diverges according to $r(t)\propto (T_f-t)^{-\xi}$, where $T_f$ is the lifetime of the bundle, and $\xi \approx 1.0$ is a quite universal scaling exponent. The average lifetime of the bundle $<T_f>$ scales with the system size as $N^{-\delta}$, where $\delta$ depends on the distribution of individual fiber as well as the breakdown rule. 	
9909273v1	http://arxiv.org/pdf/cond-mat/9909273v1	1999	Universal Scaling of Wave Propagation Failure in Arrays of Coupled   Nonlinear Cells	Konstantin Kladko|Igor Mitkov|A. R. Bishop	  We study the onset of the propagation failure of wave fronts in systems of coupled cells. We introduce a new method to analyze the scaling of the critical external field at which fronts cease to propagate, as a function of intercellular coupling. We find the universal scaling of the field throughout the range of couplings, and show that the field becomes exponentially small for large couplings. Our method is generic and applicable to a wide class of cellular dynamics in chemical, biological, and engineering systems. We confirm our results by direct numerical simulations. 	
0012256v1	http://arxiv.org/pdf/cond-mat/0012256v1	2000	What do emulsification failure and Bose-Einstein condensation have in   common?	R. P. Sear|J. A. Cuesta	  Ideal bosons and classical ring polymers formed via self-assembly, are known to have the same partition function, and so analogous phase transitions. In ring polymers, the analogue of Bose-Einstein condensation occurs when a ring polymer of macroscopic size appears. We show that a transition of the same general form occurs within a whole class of systems with self-assembly, and illustrate it with the emulsification failure of a microemulsion phase of water, oil and surfactant. As with Bose-Einstein condensation, the transition occurs even in the absence of interactions. 	
0206201v1	http://arxiv.org/pdf/cond-mat/0206201v1	2002	Failure time in the fiber-bundle model with thermal noise and disorder	Antonio Politi|Segio Ciliberto|Riccardo Scorretti	  The average time for the onset of macroscopic fractures is analytically and numerically investigated in the fiber-bundle model with quenched disorder and thermal noise under a constant load. We find an implicit exact expression for the failure time in the low-temperature limit that is accurately confirmed by direct simulations. The effect of the disorder is to lower the energy barrier. 	
0209474v1	http://arxiv.org/pdf/cond-mat/0209474v1	2002	Critical load and congestion instabilities in scale-free networks	Y. Moreno|R. Pastor-Satorras|A. Vazquez|A. Vespignani	  We study the tolerance to congestion failures in communication networks with scale-free topology. The traffic load carried by each damaged element in the network must be partly or totally redistributed among the remaining elements. Overloaded elements might fail on their turn, triggering the occurrence of failure cascades able to isolate large parts of the network. We find a critical traffic load above which the probability of massive traffic congestions destroying the network communication capabilities is finite. 	
0511457v1	http://arxiv.org/pdf/cond-mat/0511457v1	2005	Electrical transport in deformed nanostrips: electrical signature of   reversible mechanical failure	Soumendu Datta|Debasish Chaudhuri|Tanusri Saha-Dasgupta|Surajit Sengupta	  We calculate the electrical conductivity of a thin crystalline strip of atoms confined within a quasi one dimensional channel of fixed width. The conductivity shows anomalous behavior as the strip is deformed under tensile loading. Beyond a critical strain, the solid fails by the nucleation of alternating bands of solid and {\em smectic} like phases accompanied by a jump in the conductivity. Since the failure of the strip in this system is known to be reversible, the onductivity anomaly may have practical use as a sensitive strain transducer. 	
0603839v1	http://arxiv.org/pdf/cond-mat/0603839v1	2006	A Fiber Bundle Model of Traffic Jams	Bikas K. Chakrabarti	  We apply the equal load-sharing fiber bundle model of fracture failure in composite materials to model the traffic failure in a system of parallel road network in a city. For some special distributions of traffic handling capacities (thresholds) of the roads, the critical behavior of the jamming transition can be studied analytically. This has been compared with that for the asymmetric simple exclusion process in a single channel or road. 	
0507101v1	http://arxiv.org/pdf/physics/0507101v1	2005	Production networks and failure avalanches	Gerard Weisbuch|Stefano Battiston	  Although standard economics textbooks are seldom interested in production networks, modern economies are more and more based upon suppliers/customers interactions. One can consider entire sectors of the economy as generalised supply chains. We will take this view in the present paper and study under which conditions local failures to produce or simply to deliver can result in avalanches of shortage and bankruptcies across the network. We will show that a large class of models exhibit scale free distributions of production and wealth among firms and that metastable regions of high production are highly localised. 	
0803.0190v2	http://arxiv.org/pdf/0803.0190v2	2008	Crackling dynamics in material failure as the signature of a   self-organized dynamic phase transition	Daniel Bonamy|Stéphane Santucci|Laurent Ponson	  We derive here a linear elastic stochastic description for slow crack growth in heterogeneous materials. This approach succeeds in reproducing quantitatively the intermittent crackling dynamics observed recently during the slow propagation of a crack along a weak heterogeneous plane of a transparent Plexiglas block [M{\aa}l{\o}y {\it et al.}, PRL {\bf 96} 045501]. In this description, the quasi-static failure of heterogeneous media appears as a self-organized critical phase transition. As such, it exhibits universal and to some extent predictable scaling laws, analogue to that of other systems like for example magnetization noise in ferromagnets. 	
0811.3407v3	http://arxiv.org/pdf/0811.3407v3	2008	Lambda-prophage induction modeled as a cooperative failure mode of lytic   repression	Nicholas Chia|Ido Golding|Nigel Goldenfeld	  We analyze a system-level model for lytic repression of lambda-phage in E. coli using reliability theory, showing that the repressor circuit comprises 4 redundant components whose failure mode is prophage induction. Our model reflects the specific biochemical mechanisms involved in regulation, including long-range cooperative binding, and its detailed predictions for prophage induction in E. coli under ultra-violet radiation are in good agreement with experimental data. 	
1004.5095v1	http://arxiv.org/pdf/1004.5095v1	2010	Failure mechanisms in thin electroactive polymer actuators	D. De Tommasi|G. Puglisi|G. Saccomandi|G. Zurlo	  We propose a model to analyze the insurgence of pull-in and wrinkling failures in electroactive thin films. We take in consideration both cases of voltage and charge control, and study the role of prestretch and size of activated regions, which are essential in the analysis of realistic applications of EAPs. Based on simple geometrical and material assumptions we deduce an explicit analytical description of these phenomena, allowing a clear physical interpretation. Despite our simplifying assumptions, the comparison with experiments shows a satisfying qualitative and, interestingly, quantitative agreement. In particular our model shows, in accordance with experiments, the existence of different optimal prestretch values, depending on the choice of the actuating parameter of the EAP. 	
1312.4136v1	http://arxiv.org/pdf/1312.4136v1	2013	Percolating Plastic Failure as a Mechanism for Shear Softening in   Amorphous Solids	Vijayakumar Chikkadi|Oleg Gendelman|Valery Ilyin|J Ashwin|Itamar Procaccia	  ``Shear softening" refers to the observed reduction in shear modulus when the stress on an amorphous solid is increased beyond the initial linear region. Careful numerical quasi-static simulations reveal an intimate relation between plastic failure and shear softening. The attaintment of the steady-state value of the shear modulus associated with plastic flow is identified with a percolation of the regions that underwent a plastic event. We present an elementary ``two-state" model that interpolates between failed and virgin regions and provides a simple and effective characterization of the shear softening. 	
1408.5731v2	http://arxiv.org/pdf/1408.5731v2	2014	Effect of small-world topology on wave propagation on networks of   excitable elements	Thomas Isele|Eckehard Schöll	  We study excitation waves on a Newman-Watts small-world network model of coupled excitable elements. Depending on the global coupling strength, we find differing resilience to the added long-range links and different mechanisms of propagation failure. For high coupling strengths, we show agreement between the network and a reaction-diffusion model with additional mean-field term. Employing this approximation, we are able to estimate the critical density of long-range links for propagation failure. 	
1504.04618v1	http://arxiv.org/pdf/1504.04618v1	2015	Critical analysis and remedy of switching failures in straintronic logic   using Bennett clocking in the presence of thermal fluctuations	Kuntal Roy	  Straintronic logic is a promising platform for beyond Moore's law computing. Using Bennett clocking mechanism, information can propagate through an array of strain-mediated multiferroic nanomagnets exploiting the dipolar coupling between the magnets without having to physically interconnect them. Here we perform a critical analysis of switching failures, i.e., error in information propagation due to thermal fluctuations through a chain of such straintronic devices. We solved stochastic Landau-Lifshitz-Gilbert equation considering room-temperature thermal perturbations and show that magnetization switching may fail due to inherent magnetization dynamics accompanied by thermally broadened switching delay distribution. Avenues available to circumvent such issue are proposed. 	
1505.03116v1	http://arxiv.org/pdf/1505.03116v1	2015	Distributed Cohesive Control for Robot Swarms: Maintaining Good   Connectivity in the Presence of Exterior Forces	Dominik Krupke|Maximilian Ernestus|Michael Hemmer|Sandor P. Fekete	  We present a number of powerful local mechanisms for maintaining a dynamic swarm of robots with limited capabilities and information, in the presence of external forces and permanent node failures. We propose a set of local continuous algorithms that together produce a generalization of a Euclidean Steiner tree. At any stage, the resulting overall shape achieves a good compromise between local thickness, global connectivity, and flexibility to further continuous motion of the terminals. The resulting swarm behavior scales well, is robust against node failures, and performs close to the best known approximation bound for a corresponding centralized static optimization problem. 	
1511.05490v2	http://arxiv.org/pdf/1511.05490v2	2015	SPIDER: Fault Resilient SDN Pipeline with Recovery Delay Guarantees	Carmelo Cascone|Luca Pollini|Davide Sanvito|Antonio Capone|Brunilde Sansò	  When dealing with node or link failures in Software Defined Networking (SDN), the network capability to establish an alternative path depends on controller reachability and on the round trip times (RTTs) between controller and involved switches. Moreover, current SDN data plane abstractions for failure detection (e.g. OpenFlow "Fast-failover") do not allow programmers to tweak switches' detection mechanism, thus leaving SDN operators still relying on proprietary management interfaces (when available) to achieve guaranteed detection and recovery delays. We propose SPIDER, an OpenFlow-like pipeline design that provides i) a detection mechanism based on switches' periodic link probing and ii) fast reroute of traffic flows even in case of distant failures, regardless of controller availability. SPIDER can be implemented using stateful data plane abstractions such as OpenState or Open vSwitch, and it offers guaranteed short (i.e. ms) failure detection and recovery delays, with a configurable trade off between overhead and failover responsiveness. We present here the SPIDER pipeline design, behavioral model, and analysis on flow tables' memory impact. We also implemented and experimentally validated SPIDER using OpenState (an OpenFlow 1.3 extension for stateful packet processing), showing numerical results on its performance in terms of recovery latency and packet losses. 	
1702.07688v5	http://arxiv.org/pdf/1702.07688v5	2018	What determines the ultimate precision of a quantum computer?	Xavier Waintal	  A quantum error correction (QEC) code uses $N_{\rm c}$ quantum bits to construct one "logical" quantum bits of better quality than the original "physical" ones. QEC theory predicts that the failure probability $p_L$ of logical qubits decreases exponentially with $N_{\rm c}$ provided the failure probability $p$ of the physical qubit is below a certain threshold $p<p_{\rm th}$. In particular QEC theorems imply that the logical qubits can be made arbitrarily precise by simply increasing $N_{\rm c}$. In this letter, we search for physical mechanisms that lie outside of the hypothesis of QEC theorems and set a limit $\eta_{\rm L}$ to the precision of the logical qubits (irrespectively of $N_{\rm c}$). $\eta_{\rm L}$ directly controls the maximum number of operations $\propto 1/\eta_{\rm L}^2$ that can be performed before the logical quantum state gets randomized, hence the depth of the quantum circuits that can be considered. We identify a type of error - silent stabilizer failure - as a mechanism responsible for finite $\eta_{\rm L}$ and discuss its possible causes. Using the example of the topological surface code, we show that a single local event can provoke the failure of the logical qubit, irrespectively of $N_c$. 	
1706.01600v2	http://arxiv.org/pdf/1706.01600v2	2017	Atomistic Representation of Anomalies in the Failure Behaviour of   Nanocrystalline Silicene	Tawfiqur Rakib|Sourav Saha|Mohammad Motalab|Satyajit Mojumder|Md Mahbubul Islam	  Silicene, a 2D analogue of graphene, has spurred a tremendous research interest in the scientific community for its unique properties essential for next generation electronic devices. In this work, for the first time, we present a molecular dynamics (MD) investigation to determine the fracture strength and toughness of nanocrystalline silicene (nc silicene) sheet of varied grain size and pre existing crack length at room temperature. Our results suggest that the transition from an inverse pseudo Hall Petch to a pseudo Hall Petch behavior in nc silicene occurs at a critical grain size of 17.32 nm. This phenomenon is also prevalent in nanocrystalline graphene. However, nc silicene with pre existing cracks exhibits anomalous crack propagation and fracture toughness behaviour. We have observed two distinct types of failure mechanisms (crack sensitive and insensitive failure) and devised the mechanophysical conditions under which they occur. Fracture toughness calculated from both Griffiths theory and MD simulations indicate that the former overpredicts the fracture toughness of nc silicene. The most striking outcome, however, is that despite the presence of a pre existing crack, the crack sensitivity of nc silicene is found to be dependent on the grain size and their orientations. This study is the first direct comparison of atomistic simulations to the continuum theories to predict the anomalous behaviour in deformation and failure mechanisms of nc silicene. 	
1708.02030v1	http://arxiv.org/pdf/1708.02030v1	2017	CRAFT: A library for easier application-level Checkpoint/Restart and   Automatic Fault Tolerance	Faisal Shahzad|Jonas Thies|Moritz Kreutzer|Thomas Zeiser|Georg Hager|Gerhard Wellein	  In order to efficiently use the future generations of supercomputers, fault tolerance and power consumption are two of the prime challenges anticipated by the High Performance Computing (HPC) community. Checkpoint/Restart (CR) has been and still is the most widely used technique to deal with hard failures. Application-level CR is the most effective CR technique in terms of overhead efficiency but it takes a lot of implementation effort. This work presents the implementation of our C++ based library CRAFT (Checkpoint-Restart and Automatic Fault Tolerance), which serves two purposes. First, it provides an extendable library that significantly eases the implementation of application-level checkpointing. The most basic and frequently used checkpoint data types are already part of CRAFT and can be directly used out of the box. The library can be easily extended to add more data types. As means of overhead reduction, the library offers a build-in asynchronous checkpointing mechanism and also supports the Scalable Checkpoint/Restart (SCR) library for node level checkpointing. Second, CRAFT provides an easier interface for User-Level Failure Mitigation (ULFM) based dynamic process recovery, which significantly reduces the complexity and effort of failure detection and communication recovery mechanism. By utilizing both functionalities together, applications can write application-level checkpoints and recover dynamically from process failures with very limited programming effort. This work presents the design and use of our library in detail. The associated overheads are thoroughly analyzed using several benchmarks. 	
1411.3990v2	http://arxiv.org/pdf/1411.3990v2	2015	Dynamic Modeling of Cascading Failure in Power Systems	Jiajia Song|Eduardo Cotilla-Sanchez|Goodarz Ghanavati|Paul D. H. Hines	  The modeling of cascading failure in power systems is difficult because of the many different mechanisms involved; no single model captures all of these mechanisms. Understanding the relative importance of these different mechanisms is an important step in choosing which mechanisms need to be modeled for particular types of cascading failure analysis. This work presents a dynamic simulation model of both power networks and protection systems, which can simulate a wider variety of cascading outage mechanisms, relative to existing quasi-steady state (QSS) models. The model allows one to test the impact of different load models and protections on cascading outage sizes. This paper describes each module of the developed dynamic model and demonstrates how different mechanisms interact. In order to test the model we simulated a batch of randomly selected $N-2$ contingencies for several different static load configurations, and found that the distribution of blackout sizes and event lengths from the proposed dynamic simulator correlates well with historical trends. The results also show that load models have significant impacts on the cascading risks. This dynamic model was also compared against a QSS model based on the dc power flow approximations; we find that the two models largely agree, but produce substantially different results for later stages of cascading. 	
1707.08403v2	http://arxiv.org/pdf/1707.08403v2	2017	Atomistic study of hardening mechanism in Al-Cu nanostructure	Satyajit Mojumder|Tawfiqur Rakib|Mohammad Motalab|Dibakar Datta	  Nanostructures have the immense potential to supplant the traditional metallic structure as they show enhanced mechanical properties through strain hardening. In this paper, the effect of grain size on the hardening mechanism of Al-Cu nanostructure is elucidated by molecular dynamics simulation. Al-Cu (50-54% Cu by weight) nanostructure having an average grain size of 4.57 to 7.26 nm are investigated for tensile simulation at different strain rate using embedded atom method (EAM) potential at a temperature of 50~500K. It is found that the failure mechanism of the nanostructure is governed by the temperature, grain size as well as strain rate effect. At the high temperature of 300-500K, the failure strength of Al-Cu nanostructure increases with the decrease of average grain size following Hall-Petch relation. Dislocation motions are hindered significantly when the grain size is decreased which play a vital role on the hardening of the nanostructure. The failure is always found to initiate at a particular Al grain due to its weak link and propagates through grain boundary (GB) sliding, diffusion, dislocation nucleation and propagation. We also visualize the dislocation density at different grain size to show how the dislocation affects the material properties at the nanoscale. These results will further aid investigation on the deformation mechanism of nanostructure. 	
9712018v1	http://arxiv.org/pdf/chao-dyn/9712018v1	1997	Failure of linear control in noisy coupled map lattices	David A. Egolf|Joshua E. S. Socolar	  We study a 1D ring of diffusively coupled logistic maps in the vicinity of an unstable, spatially homogeneous fixed point. The failure of linear controllers due to additive noise is discussed with the aim of clarifying the failure mechanism. A criterion is suggested for estimating the noise level that can be tolerated by the given controller. The criterion implies the loss of control for surprisingly low noise levels in certain cases of interest, and accurately accounts for the results of numerical experiments over a broad range of parameter values. Previous results of Grigoriev, et al (Phys. Rev. Lett., 79, 2795) are reviewed and compared with our numerical and analytical results. 	
9612096v1	http://arxiv.org/pdf/cond-mat/9612096v1	1996	Modeling Acoustic Emission in Microfracturing Phenomena	Stefano Zapperi|Alessandro Vespignani|H. Eugene Stanley	  It has been recently observed that synthetic materials subjected to an external elastic stress give rise to scaling phenomena in the acoustic emission signal. Motivated by this experimental finding we develop a mesoscopic model in order to clarify the nature of this phenomenon. We model the synthetic material by an array of resistors with random failure thresholds. The failure of a resistor produces an decrease in the conductivity and a redistribution of the disorder. By increasing the applied voltage the system organizes itself in a stationary state. The acoustic emission signal is associated with the failure events. We find scaling behavior in the amplitude of these events and in the times between different events. The model allows us to study the geometrical and topological properties of the micro-fracturing process that drives the system to the self-organized stationary state. 	
9811153v3	http://arxiv.org/pdf/cond-mat/9811153v3	1998	Bounds for the time to failure of hierarchical systems of fracture	J. B. Gomez|M. Vazquez-Prada|Y. Moreno|A. F. Pacheco	  For years limited Monte Carlo simulations have led to the suspicion that the time to failure of hierarchically organized load-transfer models of fracture is non-zero for sets of infinite size. This fact could have a profound significance in engineering practice and also in geophysics. Here, we develop an exact algebraic iterative method to compute the successive time intervals for individual breaking in systems of height $n$ in terms of the information calculated in the previous height $n-1$. As a byproduct of this method, rigorous lower and higher bounds for the time to failure of very large systems are easily obtained. The asymptotic behavior of the resulting lower bound leads to the evidence that the above mentioned suspicion is actually true. 	
0201257v1	http://arxiv.org/pdf/cond-mat/0201257v1	2002	Failure time and critical behaviour of fracture precursors in   heterogeneous materials	A. Guarino|S. Ciliberto|A. Garcimartin|M. Zei|R. Scorretti	  The acoustic emission of fracture precursors, and the failure time of samples of heterogeneous materials (wood, fiberglass) are studied as a function of the load features and geometry. It is shown that in these materials the failure time is predicted with a good accuracy by a model of microcrack nucleation proposed by Pomeau. We find that the time interval $% \delta t$ between events (precursors) and the energy $\varepsilon$ are power law distributed and that the exponents of these power laws depend on the load history and on the material. In contrast, the cumulated acoustic energy $E$ presents a critical divergency near the breaking time $\tau $ which is $% E\sim \left( \frac{\tau -t}\tau \right) ^{-\gamma }$. The positive exponent $% \gamma $ is independent, within error bars, on all the experimental parameters. 	
0410132v4	http://arxiv.org/pdf/cond-mat/0410132v4	2009	Quantum phase transitions in the sub-ohmic spin-boson model: Failure of   the quantum-classical mapping	Matthias Vojta|Ning-Hua Tong|Ralf Bulla	  The effective theories for many quantum phase transitions can be mapped onto those of classical transitions. Here we show that such a mapping fails for the sub-ohmic spin-boson model which describes a two-level system coupled to a bosonic bath with power-law spectral density, J(omega) ~ omega^s. Using an epsilon expansion we prove that this model has a quantum transition controlled by an interacting fixed point at small s, and support this by numerical calculations. In contrast, the corresponding classical long-range Ising model is known to have an upper-critical dimension at s = 1/2, with mean-field transition behavior controlled by a non-interacting fixed point for 0 < s < 1/2. The failure of the quantum-classical mapping is argued to arise from the long-ranged interaction in imaginary time in the quantum model. 	
0502122v2	http://arxiv.org/pdf/cond-mat/0502122v2	2005	A simple beam model for the shear failure of interfaces	F. Raischel|F. Kun|H. J. Herrmann	  We propose a novel model for the shear failure of a glued interface between two solid blocks. We model the interface as an array of elastic beams which experience stretching and bending under shear load and break if the two deformation modes exceed randomly distributed breaking thresholds. The two breaking modes can be independent or combined in the form of a von Mises type breaking criterion. Assuming global load sharing following the beam breaking, we obtain analytically the macroscopic constitutive behavior of the system and describe the microscopic process of the progressive failure of the interface. We work out an efficient simulation technique which allows for the study of large systems. The limiting case of very localized interaction of surface elements is explored by computer simulations. 	
0503231v1	http://arxiv.org/pdf/cond-mat/0503231v1	2005	Failure of a granular step	Saloome Siavoshi|Arshad Kudrolli	  We investigate the gravity driven rapid failure of a granular step composed of non-cohesive steel beads. The step is initially held together with electromagnets, and released when the current is switched off. We visualize the surface and the motion of the grains during the entire relaxation. The initial failure occurs at the surface and the subsequent flow is also confined to the surface as the step relaxes to its final state. The final shape of the surface is almost linear, depends on the initial angle of the step, and is not sensitive to the size of the grains. The average final slope of the pile is only slightly lower than the angle of repose of a pile formed by slowly pouring particles on to a flat surface. The evolution of the step is compared with a proposed convective-diffusion model of our system. The qualitative features of the relaxation are captured by the model after a flow-dependent dissipation parameter is introduced. 	
0506699v1	http://arxiv.org/pdf/cond-mat/0506699v1	2005	Optimization of robustness of scale-free network to random and targeted   attacks	Jian-Guo Liu|Zhong-Tuo Wang|Yan-Zhong Dang	  The scale-fee networks, having connectivity distribution $P(k)\sim k^{-\alpha}$ (where $k$ is the site connectivity), is very resilient to random failures but fragile to intentional attack. The purpose of this paper is to find the network design guideline which can make the robustness of the network to both random failures and intentional attack maximum while keeping the average connectivity $<k>$ per node constant. We find that when $<k>=3$ the robustness of the scale-free networks reach its maximum value if the minimal connectivity $m=1$, but when $<k>$ is larger than four, the networks will become more robust to random failures and targeted attacks as the minimal connectivity $m$ gets larger. 	
0601402v2	http://arxiv.org/pdf/cond-mat/0601402v2	2006	Mechanism for the failure of the Edwards hypothesis in the SK spin glass	P. R. Eastham|R. A. Blythe|A. J. Bray|M . A. Moore	  The dynamics of the SK model at T=0 starting from random spin configurations is considered. The metastable states reached by such dynamics are atypical of such states as a whole, in that the probability density of site energies, $p(\lambda)$, is small at $\lambda=0$. Since virtually all metastable states have a much larger $p(0)$, this behavior demonstrates a qualitative failure of the Edwards hypothesis. We look for its origins by modelling the changes in the site energies during the dynamics as a Markov process. We show how the small $p(0)$ arises from features of the Markov process that have a clear physical basis in the spin-glass, and hence explain the failure of the Edwards hypothesis. 	
0006008v1	http://arxiv.org/pdf/cs/0006008v1	2000	Performing work efficiently in the presence of faults	Cynthia Dwork|Joseph Y. Halpern|O. Waarts	  We consider a system of t synchronous processes that communicate only by sending messages to one another, and that together must perform $n$ independent units of work. Processes may fail by crashing; we want to guarantee that in every execution of the protocol in which at least one process survives, all n units of work will be performed. We consider three parameters: the number of messages sent, the total number of units of work performed (including multiplicities), and time. We present three protocols for solving the problem. All three are work-optimal, doing O(n+t) work. The first has moderate costs in the remaining two parameters, sending O(t\sqrt{t}) messages, and taking O(n+t) time. This protocol can be easily modified to run in any completely asynchronous system equipped with a failure detection mechanism. The second sends only O(t log{t}) messages, but its running time is large (exponential in n and t). The third is essentially time-optimal in the (usual) case in which there are no failures, and its time complexity degrades gracefully as the number of failures increases. 	
0209060v2	http://arxiv.org/pdf/nlin/0209060v2	2002	Spectral theory for the failure of linear control in a nonlinear   stochastic system	Roman O. Grigoriev|Andreas Handel	  We consider the failure of localized control in a nonlinear spatially extended system caused by extremely small amounts of noise. It is shown that this failure occurs as a result of a nonlinear instability. Nonlinear instabilities can occur in systems described by linearly stable but strongly nonnormal evolution operators. In spatially extended systems the nonnormality manifests itself in two different but complementary ways: transient amplification and spectral focusing of disturbances. We show that temporal and spatial aspects of the nonnormality and the type of nonlinearity are all crucially important to understanding and describing the mechanism of nonlinear instability. Presented results are expected to apply equally to other physical systems where strong nonnormality is due to the presence of mean flow rather than the action of control. 	
0701002v2	http://arxiv.org/pdf/nlin/0701002v2	2007	Failure of Parameter Identification Based on Adaptive Synchronization   Techniques	Wei Lin|Huan-fei Ma	  In the paper, several concrete examples, as well as their numerical simulations, are given to show that parameter identification based on the so-called adaptive synchronization techniques might be failed if those functions with parameters pending for identification in coupled systems are designed to be mutually linearly dependent or approximately linearly dependent on the orbit in the synchronization manifold. This failure might be emergent not only when the synchronized orbit is selected to be some sort of equilibrium or some sort of periodic oscillation, but also when it is taken as some type of chaotic attractor produced by driving system. This result implies that chaotic property of driving signal is not necessary to achievement of parameter identification. The mechanism inducing such a failure, as well as the bounded property of all trajectories generated by coupled systems, is theoretically expatiated. New synchronization techniques are proposed to rigorously realize the complete synchronization and parameter identification in a class of systems where the nonlinearity is not globally Lipschitz. In addition, parameter identification are discussed for systems with time delay. 	
0706.1554v1	http://arxiv.org/pdf/0706.1554v1	2007	Continuous Damage Fiber Bundle Model for Strongly Disordered Materials	F. Raischel|F. Kun|H. J. Herrmann	  We present an extension of the continuous damage fiber bundle model to describe the gradual degradation of highly heterogeneous materials under an increasing external load. Breaking of a fiber in the model is preceded by a sequence of partial failure events occurring at random threshold values. In order to capture the subsequent propagation and arrest of cracks, furthermore, the disorder of the number of degradation steps of material constituents, the failure thresholds of single fibers are sorted into ascending order and their total number is a Poissonian distributed random variable over the fibers. Analytical and numerical calculations showed that the failure process of the system is governed by extreme value statistics, which has a substantial effect on the macroscopic constitutive behaviour and on the microscopic bursting activity as well. 	
0708.2709v1	http://arxiv.org/pdf/0708.2709v1	2007	Bicomponents and the robustness of networks to failure	M. E. J. Newman|Gourab Ghoshal	  A common definition of a robust connection between two nodes in a network such as a communication network is that there should be at least two independent paths connecting them, so that the failure of no single node in the network causes them to become disconnected. This definition leads us naturally to consider bicomponents, subnetworks in which every node has a robust connection of this kind to every other. Here we study bicomponents in both real and model networks using a combination of exact analytic techniques and numerical methods. We show that standard network models predict there to be essentially no small bicomponents in most networks, but there may be a giant bicomponent, whose presence coincides with the presence of the ordinary giant component, and we find that real networks seem by and large to follow this pattern, although there are some interesting exceptions. We study the size of the giant bicomponent as nodes in the network fail, using a specially developed computer algorithm based on data trees, and find in some cases that our networks are quite robust to failure, with large bicomponents persisting until almost all vertices have been removed. 	
0709.2647v2	http://arxiv.org/pdf/0709.2647v2	2007	Rupture by damage accumulation in rocks	David Amitrano	  The deformation of rocks is associated with microcracks nucleation and propagation, i.e. damage. The accumulation of damage and its spatial localization lead to the creation of a macroscale discontinuity, so-called "fault" in geological terms, and to the failure of the material, i.e. a dramatic decrease of the mechanical properties as strength and modulus. The damage process can be studied both statically by direct observation of thin sections and dynamically by recording acoustic waves emitted by crack propagation (acoustic emission). Here we first review such observations concerning geological objects over scales ranging from the laboratory sample scale (dm) to seismically active faults (km), including cliffs and rock masses (Dm, hm). These observations reveal complex patterns in both space (fractal properties of damage structures as roughness and gouge), time (clustering, particular trends when the failure approaches) and energy domains (power-law distributions of energy release bursts). We use a numerical model based on progressive damage within an elastic interaction framework which allows us to simulate these observations. This study shows that the failure in rocks can be the result of damage accumulation. 	
0711.1602v2	http://arxiv.org/pdf/0711.1602v2	2008	Preservation of network Degree Distributions from non-uniform failures	Brian Karrer|Gourab Ghoshal	  There has been a considerable amount of interest in recent years on the robustness of networks to failures. Many previous studies have concentrated on the effects of node and edge removals on the connectivity structure of a static network; the networks are considered to be static in the sense that no compensatory measures are allowed for recovery of the original structure. Real world networks such as the world wide web, however, are not static and experience a considerable amount of turnover, where nodes and edges are both added and deleted. Considering degree-based node removals, we examine the possibility of preserving networks from these types of disruptions. We recover the original degree distribution by allowing the network to react to the attack by introducing new nodes and attaching their edges via specially tailored schemes. We focus particularly on the case of non-uniform failures, a subject that has received little attention in the context of evolving networks. Using a combination of analytical techniques and numerical simulations, we demonstrate how to preserve the exact degree distribution of the studied networks from various forms of attack. 	
0805.3235v1	http://arxiv.org/pdf/0805.3235v1	2008	Classical and quantum breakdown in disordered materials	Debashis Samanta|Bikas K. Chakrabarti|Purusattam Ray	  We have discussed the classical failure of the fuse system, the dielectric breakdown and the quantum breakdown in the Anderson insulators. We have discussed how the extreme value statistics and the resulting Gumbel distribution arises in breakdown and failure processes, especially when the disorder concentration is low. At high concentration of disorder near the percolation threshold, we have discussed how the cross-over might take place from extreme value to percolation statistics. We have discussed the system size dependence that arises at the distribution of the failure current at low disordered regime. Finally, the extension of Zener breakdown phenomenon for band insulators to the disordered-induced Anderson insulators has been discussed. 	
0808.4081v1	http://arxiv.org/pdf/0808.4081v1	2008	Numerical modelling of the effect of weathering on the progressive   failure of underground limestone mines	Siavash Ghabezloo|Ahmad Pouya	  The observations show that the collapse of underground limestone mines results from a progressive failure due to gradual weathering of the rockmass. The following stages can be considered for the limestone weathering and degradation process in underground mines: condensation of the water on the roof of the gallery, infiltration of water in the porous rock, migration of the air CO2 molecules in the rock pore water by convection and molecular diffusion, dissolution of limestone by CO2 rich water and consequently, reduction of the strength properties of rock. Considering this process, a set of equations governing different hydrochemo-mechanical aspects of the weathering phenomenon and progressive failure occurring in these mines is presented. Then the feasibility of numerical modelling of this process is studied and a simple example of application is presented. 	
0812.3591v1	http://arxiv.org/pdf/0812.3591v1	2008	How to make a fragile network robust and vice versa	Andre A. Moreira|Jose S. Andrade|Hans J. Herrmann|Joseph O. Indekeu	  We investigate topologically biased failure in scale-free networks with degree distribution $P(k) \propto k^{-\gamma}$. The probability $p$ that an edge remains intact is assumed to depend on the degree $k$ of adjacent nodes $i$ and $j$ through $p_{ij}\propto(k_{i}k_{j})^{-\alpha}$. By varying the exponent $\alpha$, we interpolate between random ($\alpha=0$) and systematic failure. For $\alpha >0 $ ($<0$) the most (least) connected nodes are depreciated first. This topological bias introduces a characteristic scale in $P(k)$ of the depreciated network, marking a crossover between two distinct power laws. The critical percolation threshold, at which global connectivity is lost, depends both on $\gamma$ and on $\alpha$. As a consequence, network robustness or fragility can be controlled through fine tuning of the topological bias in the failure process. 	
1001.4350v1	http://arxiv.org/pdf/1001.4350v1	2010	Phase field modeling of crack propagation	R. Spatschek|E. Brener|A. Karma	  Fracture is a fundamental mechanism of materials failure. Propagating cracks can exhibit a rich dynamical behavior controlled by a subtle interplay between microscopic failure processes in the crack tip region and macroscopic elasticity. We review recent approaches to understand crack dynamics using the phase field method. This method, developed originally for phase transformations, has the well-known advantage of avoiding explicit front tracking by making material interfaces spatially diffuse. In a fracture context, this method is able to capture both the short-scale physics of failure and macroscopic linear elasticity within a self-consistent set of equations that can be simulated on experimentally relevant length and time scales. We discuss the relevance of different models, which stem from continuum field descriptions of brittle materials and crystals, to address questions concerning crack path selection and branching instabilities, as well as models that are based on mesoscale concepts for crack tip scale selection. Open questions which may be addressed using phase field models of fracture are summarized. 	
1012.0206v1	http://arxiv.org/pdf/1012.0206v1	2010	Catastrophic Cascade of Failures in Interdependent Networks	S. Havlin|N. A. M. Araujo|S. V. Buldyrev|C. S. Dias|R. Parshani|G. Paul|H. E. Stanley	  Modern network-like systems are usually coupled in such a way that failures in one network can affect the entire system. In infrastructures, biology, sociology, and economy, systems are interconnected and events taking place in one system can propagate to any other coupled system. Recent studies on such coupled systems show that the coupling increases their vulnerability to random failure. Properties for interdependent networks differ significantly from those of single-network systems. In this article, these results are reviewed and the main properties discussed. 	
1012.0815v1	http://arxiv.org/pdf/1012.0815v1	2010	Statistical Classification of Cascading Failures in Power Grids	René Pfitzner|Konstantin Turitsyn|Michael Chertkov	  We introduce a new microscopic model of the outages in transmission power grids. This model accounts for the automatic response of the grid to load fluctuations that take place on the scale of minutes, when the optimum power flow adjustments and load shedding controls are unavailable. We describe extreme events, initiated by load fluctuations, which cause cascading failures of loads, generators and lines. Our model is quasi-static in the causal, discrete time and sequential resolution of individual failures. The model, in its simplest realization based on the Directed Current description of the power flow problem, is tested on three standard IEEE systems consisting of 30, 39 and 118 buses. Our statistical analysis suggests a straightforward classification of cascading and islanding phases in terms of the ratios between average number of removed loads, generators and links. The analysis also demonstrates sensitivity to variations in line capacities. Future research challenges in modeling and control of cascading outages over real-world power networks are discussed. 	
1101.3520v1	http://arxiv.org/pdf/1101.3520v1	2011	Error-Free Multi-Valued Consensus with Byzantine Failures	Guanfeng Liang|Nitin Vaidya	  In this paper, we present an efficient deterministic algorithm for consensus in presence of Byzantine failures. Our algorithm achieves consensus on an $L$-bit value with communication complexity $O(nL + n^4 L^{0.5} + n^6)$ bits, in a network consisting of $n$ processors with up to $t$ Byzantine failures, such that $t<n/3$. For large enough $L$, communication complexity of the proposed algorithm approaches $O(nL)$ bits. In other words, for large $L$, the communication complexity is linear in the number of processors in the network. This is an improvement over the work of Fitzi and Hirt (from PODC 2006), who proposed a probabilistically correct multi-valued Byzantine consensus algorithm with a similar complexity for large $L$. In contrast to the algorithm by Fitzi and Hirt, our algorithm is guaranteed to be always error-free. Our algorithm require no cryptographic technique, such as authentication, nor any secret sharing mechanism. To the best of our knowledge, we are the first to show that, for large $L$, error-free multi-valued Byzantine consensus on an $L$-bit value is achievable with $O(nL)$ bits of communication. 	
1102.5085v2	http://arxiv.org/pdf/1102.5085v2	2016	Robustness and modular structure in networks	James P. Bagrow|Sune Lehmann|Yong-Yeol Ahn	  Complex networks have recently attracted much interest due to their prevalence in nature and our daily lives [1, 2]. A critical property of a network is its resilience to random breakdown and failure [3-6], typically studied as a percolation problem [7-9] or by modeling cascading failures [10-12]. Many complex systems, from power grids and the Internet to the brain and society [13-15], can be modeled using modular networks comprised of small, densely connected groups of nodes [16, 17]. These modules often overlap, with network elements belonging to multiple modules [18, 19]. Yet existing work on robustness has not considered the role of overlapping, modular structure. Here we study the robustness of these systems to the failure of elements. We show analytically and empirically that it is possible for the modules themselves to become uncoupled or non-overlapping well before the network disintegrates. If overlapping modular organization plays a role in overall functionality, networks may be far more vulnerable than predicted by conventional percolation theory. 	
1105.3574v3	http://arxiv.org/pdf/1105.3574v3	2012	Robustness and Assortativity for Diffusion-like Processes in Scale-free   Networks	Gregorio D'Agostino|Antonio Scala|Vinko Zlatić|Guido Caldarelli	  By analysing the diffusive dynamics of epidemics and of distress in complex networks, we study the effect of the assortativity on the robustness of the networks. We first determine by spectral analysis the thresholds above which epidemics/failures can spread; we then calculate the slowest diffusional times. Our results shows that disassortative networks exhibit a higher epidemiological threshold and are therefore easier to immunize, while in assortative networks there is a longer time for intervention before epidemic/failure spreads. Moreover, we study by computer simulations the sandpile cascade model, a diffusive model of distress propagation (financial contagion). We show that, while assortative networks are more prone to the propagation of epidemic/failures, degree-targeted immunization policies increases their resilience to systemic risk. 	
1108.3883v1	http://arxiv.org/pdf/1108.3883v1	2011	Exact Regenerating Codes for Byzantine Fault Tolerance in Distributed   Storage	Yunghsiang S. Han|Rong Zheng|Wai Ho Mow	  Due to the use of commodity software and hardware, crash-stop and Byzantine failures are likely to be more prevalent in today's large-scale distributed storage systems. Regenerating codes have been shown to be a more efficient way to disperse information across multiple nodes and recover crash-stop failures in the literature. In this paper, we present the design of regeneration codes in conjunction with integrity check that allows exact regeneration of failed nodes and data reconstruction in presence of Byzantine failures. A progressive decoding mechanism is incorporated in both procedures to leverage computation performed thus far. The fault-tolerance and security properties of the schemes are also analyzed. 	
1204.1529v1	http://arxiv.org/pdf/1204.1529v1	2012	Ensuring QOS Guarantees in a Hybrid OCS/OBS Network	Sunish Kumar O S	  The bursting aggregation assembly in edge nodes is one of the key technologies in OBS (Optical Burst Switching) network, which has a direct impact on flow characteristics and packet loss rate. An optical burst assembly technique supporting QoS is presented through this paper, which can automatically adjust the threshold along with the increasing and decreasing volume of business, reduce the operational burst, and generate corresponding BDP (Burst Data Packet) and BCP (Burst Control Packet). In addition to the burst aggregation technique a packet recovery technique by restoration method is also described. The data packet loss due to the physical optical link failure is not currently included in the QoS descriptions. This link failure is also a severe problem which reduces the data throughput of the transmitter node. A mechanism for data recovery from this link failure is vital for guaranteeing the QoS demanded by each user. So this paper will also discusses a specific protocol for reducing the packet loss by utilizing the features of both optical circuit switching (OCS) and Optical Burst switching (OBS) techniques. 	
1205.2909v3	http://arxiv.org/pdf/1205.2909v3	2012	Evolution of robust network topologies: Emergence of central backbones	Tiago P. Peixoto|Stefan Bornholdt	  We model the robustness against random failure or intentional attack of networks with arbitrary large-scale structure. We construct a block-based model which incorporates --- in a general fashion --- both connectivity and interdependence links, as well as arbitrary degree distributions and block correlations. By optimizing the percolation properties of this general class of networks, we identify a simple core-periphery structure as the topology most robust against random failure. In such networks, a distinct and small "core" of nodes with higher degree is responsible for most of the connectivity, functioning as a central "backbone" of the system. This centralized topology remains the optimal structure when other constraints are imposed, such as a given fraction of interdependence links and fixed degree distributions. This distinguishes simple centralized topologies as the most likely to emerge, when robustness against failure is the dominant evolutionary force. 	
1210.8421v2	http://arxiv.org/pdf/1210.8421v2	2014	Distribution of the Number of Retransmissions of Bounded Documents	Predrag R. Jelenković|Evangelia D. Skiani	  Retransmission-based failure recovery represents a primary approach in existing communication networks that guarantees data delivery in the presence of channel failures. Recent work has shown that, when data sizes have infinite support, retransmissions can cause long (-tailed) delays even if all traffic and network characteristics are light-tailed. In this paper we investigate the practically important case of bounded data units 0 <= L_b <= b under the condition that the hazard functions of the distributions of data sizes and channel statistics are proportional. To this end, we provide an explicit and uniform characterization of the entire body of the retransmission distribution Pr[N_b > n] in both n and b. Our main discovery is that this distribution can be represented as the product of a power law and Gamma distribution. This rigorous approximation clearly demonstrates the coupling of a power law distribution, dominating the main body, and the Gamma distribution, determining the exponential tail. Our results are validated via simulation experiments and can be useful for designing retransmission-based systems with the required performance characteristics. From a broader perspective, this study applies to any other system, e.g., computing, where restart mechanisms are employed after a job processing failure. 	
1301.2851v1	http://arxiv.org/pdf/1301.2851v1	2013	Efficient algorithm to study interconnected networks	Christian M. Schneider|Nuno A. M. Araújo|Hans J. Herrmann	  Interconnected networks have been shown to be much more vulnerable to random and targeted failures than isolated ones, raising several interesting questions regarding the identification and mitigation of their risk. The paradigm to address these questions is the percolation model, where the resilience of the system is quantified by the dependence of the size of the largest cluster on the number of failures. Numerically, the major challenge is the identification of this cluster and the calculation of its size. Here, we propose an efficient algorithm to tackle this problem. We show that the algorithm scales as O(N log N), where N is the number of nodes in the network, a significant improvement compared to O(N^2) for a greedy algorithm, what permits studying much larger networks. Our new strategy can be applied to any network topology and distribution of interdependencies, as well as any sequence of failures. 	
1305.2060v1	http://arxiv.org/pdf/1305.2060v1	2013	Nonlocal failures in complex supply networks by single link additions	Dirk Witthaut|Marc Timme	  How do local topological changes affect the global operation and stability of complex supply networks? Studying supply networks on various levels of abstraction, we demonstrate that and how adding new links may not only promote but also degrade stable operation of a network. Intriguingly, the resulting overloads may emerge remotely from where such a link is added, thus resulting in nonlocal failure. We link this counter-intuitive phenomenon to Braess' paradox originally discovered in traffic networks. We use elementary network topologies to explain its underlying mechanism for different types of supply networks and find that it generically occurs across these systems. As an important consequence, upgrading supply networks such as communication networks, biological supply networks or power grids requires particular care because even adding only single connections may destabilize normal network operation and induce disturbances remotely from the location of structural change and even global cascades of failures. 	
1306.1861v1	http://arxiv.org/pdf/1306.1861v1	2013	Online Parallel Scheduling of Non-uniform Tasks: Trading Failures for   Energy	Antonio Fernández Anta|Chryssis Georgiou|Dariusz R. Kowalski|Elli Zavou	  Consider a system in which tasks of different execution times arrive continuously and have to be executed by a set of processors that are prone to crashes and restarts. In this paper we model and study the impact of parallelism and failures on the competitiveness of such an online system. In a fault-free environment, a simple Longest-in-System scheduling policy, enhanced by a redundancy-avoidance mechanism, guarantees optimality in a long-term execution. In the presence of failures though, scheduling becomes a much more challenging task. In particular, no parallel deterministic algorithm can be competitive against an offline optimal solution, even with one single processor and tasks of only two different execution times. We find that when additional energy is provided to the system in the form of processor speedup, the situation changes. Specifically, we identify thresholds on the speedup under which such competitiveness cannot be achieved by any deterministic algorithm, and above which competitive algorithms exist. Finally, we propose algorithms that achieve small bounded competitive ratios when the speedup is over the threshold. 	
1401.0280v2	http://arxiv.org/pdf/1401.0280v2	2016	Failure Processes in Embedded Monolayer Graphene under Axial Compression	Charalampos Androulidakis|Emmanuel N. Koukaras|Otakar Frank|Georgia Tsoukleri|Dimitris Sfyris|John Parthenios|Nicola Pugno|Konstantinos Papagelis|Kostya S. Novoselov|Costas Galiotis	  Exfoliated monolayer graphene flakes were embedded in a polymer matrix and loaded under axial compression. By monitoring the shifts of the 2D Raman phonons of rectangular flakes of various sizes under load, the critical strain to failure was determined. Prior to loading care was taken for the examined area of the flake to be free of residual stresses. The critical strain values for first failure were found to be independent of flake size at a mean value of -0.60 % corresponding to a yield stress of -6 GPa. By combining Euler mechanics with a Winkler approach, we show that unlike buckling in air, the presence of the polymer constraint results in graphene buckling at a fixed value of strain with an estimated wrinkle wavelength of the order of 1-2 nm. These results were compared with DFT computations performed on analogue coronene/ PMMA oligomers and a reasonable agreement was obtained. 	
1501.05976v1	http://arxiv.org/pdf/1501.05976v1	2015	Robustness of Spatial Micronetworks	Thomas C. McAndrew|Christopher M. Danforth|James P. Bagrow	  Power lines, roadways, pipelines and other physical infrastructure are critical to modern society. These structures may be viewed as spatial networks where geographic distances play a role in the functionality and construction cost of links. Traditionally, studies of network robustness have primarily considered the connectedness of large, random networks. Yet for spatial infrastructure physical distances must also play a role in network robustness. Understanding the robustness of small spatial networks is particularly important with the increasing interest in microgrids, small-area distributed power grids that are well suited to using renewable energy resources. We study the random failures of links in small networks where functionality depends on both spatial distance and topological connectedness. By introducing a percolation model where the failure of each link is proportional to its spatial length, we find that, when failures depend on spatial distances, networks are more fragile than expected. Accounting for spatial effects in both construction and robustness is important for designing efficient microgrids and other network infrastructure. 	
1502.05817v1	http://arxiv.org/pdf/1502.05817v1	2015	A Hybrid Model to Extend Vehicular Intercommunication V2V through D2D   Architecture	Emad Abd-Elrahman|Adel Mounir Said|Thouraya Toukabri|Hossam Afifi|Michel Marot	  In the recent years, many solutions for Vehicle to Vehicle (V2V) communication were proposed to overcome failure problems (also known as dead ends). This paper proposes a novel framework for V2V failure recovery using Device-to-Device (D2D) communications. Based on the unified Intelligent Transportation Systems (ITS) architecture, LTE-based D2D mechanisms can improve V2V dead ends failure recovery delays. This new paradigm of hybrid V2V-D2D communications overcomes the limitations of traditional V2V routing techniques. According to NS2 simulation results, the proposed hybrid model decreases the end to end delay (E2E) of messages delivery. A complete comparison of different D2D use cases (best & worst scenarios) is presented to show the enhancements brought by our solution compared to traditional V2V techniques. 	
1505.02571v4	http://arxiv.org/pdf/1505.02571v4	2015	Criticality in the approach to failure in amorphous solids	Jie Lin|Thomas Gueudré|Alberto Rosso|Matthieu Wyart	  Failure of amorphous solids is fundamental to various phenomena, including landslides and earthquakes. Recent experiments indicate that highly plastic regions form elongated structures that are especially apparent near the maximal shear stress $\Sigma_{\max}$ where failure occurs. This observation suggested that $\Sigma_{\max}$ acts as a critical point where the length scale of those structures diverges, possibly causing macroscopic transient shear bands. Here we argue instead that the entire solid phase ($\Sigma<\Sigma_{\max}$) is critical, that plasticity always involves system-spanning events, and that their magnitude diverges at $\Sigma_{\max}$ independently of the presence of shear bands. We relate the statistics and fractal properties of these rearrangements to an exponent $\theta$ that captures the stability of the material, which is observed to vary continuously with stress, and we confirm our predictions in elastoplastic models. 	
1507.05716v1	http://arxiv.org/pdf/1507.05716v1	2015	Fractal and Small-World Networks Formed by Self-Organized Critical   Dynamics	Akitomo Watanabe|Shogo Mizutaka|Kousuke Yakubo	  We propose a dynamical model in which a network structure evolves in a self-organized critical (SOC) manner and explain a possible origin of the emergence of fractal and small-world networks. Our model combines a network growth and its decay by failures of nodes. The decay mechanism reflects the instability of large functional networks against cascading overload failures. It is demonstrated that the dynamical system surely exhibits SOC characteristics, such as power-law forms of the avalanche size distribution, the cluster size distribution, and the distribution of the time interval between intermittent avalanches. During the network evolution, fractal networks are spontaneously generated when networks experience critical cascades of failures that lead to a percolation transition. In contrast, networks far from criticality have small-world structures. We also observe the crossover behavior from fractal to small-world structure in the network evolution. 	
1705.07998v1	http://arxiv.org/pdf/1705.07998v1	2017	Synaptic Noise Facilitates the Emergence of Self-Organized Criticality   in the Caenorhabditis elegans Neuronal Network	Koray Çiftçi	  Avalanches with power-law distributed size parameters have been observed in neuronal networks. This observation might be a manifestation of the self-organized criticality (SOC). Yet, the physiological mechanicsm of this behavior is currently unknown. Describing synaptic noise as transmission failures mainly originating from the probabilistic nature of neurotransmitter release, this study investigates the potential of this noise as a mechanism for driving the functional architecture of the neuronal networks towards SOC. To this end, a simple finite state neuron model, with activity dependent and synapse specific failure probabilities, was built based on the known anatomical connectivity data of the nematode Ceanorhabditis elegans. Beginning from random values, it was observed that synaptic noise levels picked out a set of synapses and consequently an active subnetwork which generates power-law distributed neuronal avalanches. The findings of this study brings up the possibility that synaptic failures might be a component of physiological processes underlying SOC in neuronal networks. 	
1707.01974v1	http://arxiv.org/pdf/1707.01974v1	2017	A Tissue Engineered Model of Aging: Interdependence and Cooperative   Effects in Failing Tissues	Aylin Acun|Dervis Can Vural|Pinar Zorlutuna	  Aging remains a fundamental open problem in modern biology. Although there exist a number of theories on aging on the cellular scale, nearly nothing is known about how microscopic failures cascade to macroscopic failures of tissues, organs and ultimately the organism. The goal of this work is to bridge microscopic cell failure to macroscopic manifestations of aging. We use tissue engineered constructs to control the cellular-level damage and cell-cell distance in individual tissues to establish the role of complex interdependence and interactions between cells in aging tissues. We found that while microscopic mechanisms drive aging, the interdependency between cells plays a major role in tissue death, providing evidence on how cellular aging is connected to its higher systemic consequences. 	
1707.08136v1	http://arxiv.org/pdf/1707.08136v1	2017	Monte-Carlo acceleration: importance sampling and hybrid dynamic systems	H. Chraibi|A. Dutfoy|T. Galtier|J. Garnier	  The reliability of a complex industrial system can rarely be assessed analytically. As system failure is often a rare event, crude Monte-Carlo methods are prohibitively expensive from a computational point of view. In order to reduce computation times, variance reduction methods such as importance sampling can be used. We propose an adaptation of this method for a class of multi-component dynamical systems. We address a system whose failure corresponds to a physical variable of the system (temperature, pressure, water level) entering a critical region. Such systems are common in hydraulic and nuclear industry. In these systems, the statuses of the components (on, off, or out-of-order) determine the dynamics of the physical variables, and is altered both by deterministic feedback mechanisms and random failures or repairs. In order to deal with this interplay between components status and physical variables we model trajectory using piecewise deterministic Markovian processes (PDMP). We show how to adapt the importance sampling method to PDMP, by introducing a reference measure on the trajectory space, and we present a biasing strategy for importance sampling. A simulation study compares our importance sampling method to the crude Monte-Carlo method for a three-component-system. 	
1711.00964v1	http://arxiv.org/pdf/1711.00964v1	2017	Ergodicity breaking dynamics of arch collapse	Carl Merrigan|Sumit Kumar Birwa|Shubha Tewari|Bulbul Chakraborty	  Gravity driven flows such as in hoppers and silos are susceptible to clogging due to the formation of arches at the exit whose failure is the key to re-initiation of flow. In vibrated hoppers, clog durations exhibit a broad distribution, which poses a challenge for devising efficient unclogging protocols. Using numerical simulations, we demonstrate that the dynamics of arch shapes preceding failure can be modeled as a continuous time random walk (CTRW) with a broad distribution of waiting times, which breaks ergodicity. Treating arch failure as a first passage process of this random walk, we argue that the distribution of unclogging times is determined by this waiting time distribution. We hypothesize that this is a generic feature of unclogging, and that specific characteristics, such as hopper geometry, and mechanical properties of the grains modify the waiting time distribution. 	
1712.06934v1	http://arxiv.org/pdf/1712.06934v1	2017	Effect of NBTI/PBTI Aging and Process Variations on Write Failures in   MOSFET and FinFET Flip-Flops	Usman Khalid|Antonio Mastrandrea|Mauro Olivieri	  The assessment of noise margins and the related probability of failure in digital cells has growingly become essential, as nano-scale CMOS and FinFET technologies are confronting reliability issues caused by aging mechanisms, such as NBTI, and variability in process parameters. The influence of such phenomena is particularly associated to the Write Noise Margins (WNM) in memory elements, since a wrong stored logic value can result in an upset of the system state. In this work, we calculated and compared the effect of process variations and NBTI aging over the years on the actual WNM of various CMOS and FinFET based flip-flop cells. The massive transistor-level Monte Carlo simulations produced both nominal (i.e. mean) values and associated standard deviations of the WNM of the chosen flip-flops. This allowed calculating the consequent write failure probability as a function of an input voltage shift on the flip-flop cells, and assessing a comparison for robustness among different circuit topologies and technologies. 	
1008.3967v1	http://arxiv.org/pdf/1008.3967v1	2010	Statistical Physics of the Yielding Transition in Amorphous Solids	Smarajit Karmakar|Edan Lerner|Itamar Procaccia	  The art of making structural, polymeric and metallic glasses is rapidly developing with many applications. A limitation to their use is their mechanical stability: under increasing external strain all amorphous solids respond elastically to small strains but have a finite yield stress which cannot be exceeded without effecting a plastic response which typically leads to mechanical failure. Understanding this is crucial for assessing the risk of failure of glassy materials under mechanical loads. Here we show that the statistics of the energy barriers \Delta E that need to be surmounted changes from a probability distribution function (pdf) that goes smoothly to zero to a pdf which is finite at \Delta E=0. This fundamental change implies a dramatic transition in the mechanical stability properties with respect to external strain. We derive exact results for the scaling exponents that characterize the magnitudes of average energy and stress drops in plastic events as a function of system size. 	
1801.08557v1	http://arxiv.org/pdf/1801.08557v1	2018	Fracturing of topological Maxwell lattices	Leyou Zhang|Xiaoming Mao	  We present fracturing analysis of topological Maxwell lattices when they are stretched by applied stress. Maxwell lattices are mechanical structures containing equal numbers of degrees of freedom and constraints in the bulk and are thus on the verge of mechanical instability. Recent progress in topological mechanics led to the discovery of topologically protected floppy modes and states of self stress at edges and domain walls of Maxwell lattices. When normal brittle materials are being stretched, stress focuses on crack tips, leading to catastrophic failure. In contrast, we find that when topological Maxwell lattices are being stretched, stress focuses on states of self stress domain walls instead, and bond-breaking events start at these domain walls, even in presence of cracks. Remarkably, we find that the stress-focusing feature of the self-stress domain walls persists deep into the the failure process, when a lot of damages already occurred at these domain walls. We explain the results using topological mechanics theory and discuss the potential use of these topological Maxwell lattice structures as mechanical metamaterials that exhibit high strength against fracturing and well controlled fracturing process. 	
1410.1917v1	http://arxiv.org/pdf/1410.1917v1	2014	Tensile Fracture of Welded Polymer Interfaces: Miscibility,   Entanglements and Crazing	Ting Ge|Gary S. Grest|Mark O. Robbins	  Large-scale molecular simulations are performed to investigate tensile failure of polymer interfaces as a function of welding time $t$. Changes in the tensile stress, mode of failure and interfacial fracture energy $G_I$ are correlated to changes in the interfacial entanglements as determined from Primitive Path Analysis. Bulk polymers fail through craze formation, followed by craze breakdown through chain scission. At small $t$ welded interfaces are not strong enough to support craze formation and fail at small strains through chain pullout at the interface. Once chains have formed an average of about one entanglement across the interface, a stable craze is formed throughout the sample. The failure stress of the craze rises with welding time and the mode of craze breakdown changes from chain pullout to chain scission as the interface approaches bulk strength. The interfacial fracture energy $G_I$ is calculated by coupling the simulation results to a continuum fracture mechanics model. As in experiment, $G_I$ increases as $t^{1/2}$ before saturating at the average bulk fracture energy $G_b$. As in previous simulations of shear strength, saturation coincides with the recovery of the bulk entanglement density. Before saturation, $G_I$ is proportional to the areal density of interfacial entanglements. Immiscibiltiy limits interdiffusion and thus suppresses entanglements at the interface. Even small degrees of immisciblity reduce interfacial entanglements enough that failure occurs by chain pullout and $G_I \ll G_b$. 	
1509.04012v1	http://arxiv.org/pdf/1509.04012v1	2015	Mechanical Properties of Norway Spruce: Intra-Ring Variation and Generic   Behavior of Earlywood and Latewood until Failure	Christian Lanvermann|Philipp Hass|Falk K. Wittel|Peter Niemz	  The alternating earlywood and latewood growth ring structure has a strong influence on the mechanical performance of Norway spruce. In the current study, tensile tests in the longitudinal and tangential directions were performed on a series of specimens representing one growth ring at varying relative humidities. All tested mechanical parameters, namely modulus of elasticity and ultimate tensile stress, followed the density distribution in the growth ring, with the minimum values in earlywood and the maximum values in latewood. The samples were conditioned at three the relative humidities 50%, 65% and 95%. With increasing relative humidity, the values of the mechanical parameters were found to decrease. However, due to the high local variability, this decrease was not statistically significant. The test in the tangential direction on a set of earlywood and latewood specimens at 65% relative humidity revealed a similar limit of linear elasticity for both early- and latewood. Where the strength of both tissues was equal, the strain at failure was significantly greater for earlywood. Furthermore, the portion of the non-linear stress/strain behavior for earlywood was significantly greater. A Weibull analysis on the ultimate tensile strength revealed a tissue-independent Weibull modulus, which indicates similar defect distributions. For both, the failure occurred in the middle lamella. 	
1007.2180v1	http://arxiv.org/pdf/1007.2180v1	2010	A damage-mechanics model for fracture nucleation and propagation	G. Yakovlev|J. D. Gran|D. L. Turcotte|J. B. Rundle|W. Klein	  In this paper a composite model for earthquake rupture initiation and propagation is proposed. The model includes aspects of damage mechanics, fiber-bundle models, and slider-block models. An array of elements is introduced in analogy to the fibers of a fiber bundle. Time to failure for each element is specified from a Poisson distribution. The hazard rate is assumed to have a power-law dependence on stress. When an element fails it is removed, the stress on a failed element is redistributed uniformly to a specified number of neighboring elements in a given range of interaction. Damage is defined to be the fraction of elements that have failed. Time to failure and modes of rupture propagation are determined as a function of the hazard-rate exponent and the range of interaction. 	
1210.3024v1	http://arxiv.org/pdf/1210.3024v1	2012	Sensitivity analysis of GSI based mechanical characterization of rock   mass	P. Ván|B. Vásárhelyi	  Recently, the rock mechanical and rock engineering designs and calculations are frequently based on Geological Strength Index (GSI) method, because it is the only system that provides a complete set of mechanical properties for design purpose. Both the failure criteria and the deformation moduli of the rock mass can be calculated with GSI based equations, which consists of the disturbance factor, as well. The aim of this paper is the sensitivity analysis of GSI and disturbance factor dependent equations that characterize the mechanical properties of rock masses. The survey of the GSI system is not our purpose. The results show that the rock mass strength calculated by the Hoek-Brown failure criteria and both the Hoek-Diederichs and modified Hoek-Diederichs deformation moduli are highly sensitive to changes of both the GSI and the D factor, hence their exact determination is important for the rock engineering design. 	
1409.2040v2	http://arxiv.org/pdf/1409.2040v2	2014	Strain shielding from mechanically-activated covalent bond formation in   nanoindentation	Sandeep Kumar|David M. Parks	  Mechanical failure of an ideal crystal is dictated either by an elastic instability or a soft-mode instability. We show that the ideal strength measurement of graphene based on nano-indentation experiments \cite{lee2008measurement, lee2013high}, however, indicates an anomaly: the inferred strain beneath the diamond indenter at the failure load is anomalously large compared to the fracture strain predicted by soft-mode analysis or acoustic analysis. Here we present a systematic investigation - based on multi-scale modeling combining the results of continuum, atomistic, and quantum calculations; and analysis of experiments - that identifies the operative mechanism responsible for the anomalous difference between the fracture strains. We suggest that a strain-shielding effect due to mechanically-activated covalent bond formation at graphene-indenter interface is responsible for this anomaly. Using Finite Elements Analysis (FEA) and MD simulations of the nanoindentation experiments, we explicitly show that the bonded interaction at the graphene-indenter interface substantially disperses (shields) the strain beneath the indenter, preventing the intensification of strain. Our calculations indicate that the extent of strain shielding depends upon the hydrogen coverage at the indenter surface; and at optimal hydrogen coverage, the strain-shielding effect can delay the onset of fracture to the experimentally-observed indentation load and depth. 	
1703.09984v1	http://arxiv.org/pdf/1703.09984v1	2017	Mechanics of a granular skin	Somnath Karmakar|Anit Sane|S. Bhattacharya|Shankar Ghosh	  Magic Sand, a hydrophobic toy granular material, is widely used in popular science instructions because of its non-intuitive mechanical properties. A detailed study of the failure of an underwater column of magic sand shows that these properties can be traced to a single phenomenon: the system self-generates a cohesive skin that encapsulates the material inside. The skin, consists of pinned air-water-grain interfaces, shows multi-scale mechanical properties: they range from contact-line dynamics in the intra-grain roughness scale, plastic flow at the grain scale, all the way to the sample-scale mechanical responses. With decreasing rigidity of the skin, the failure mode transforms from brittle to ductile (both of which are collective in nature) to a complete disintegration at the single grain scale. 	
1712.03378v1	http://arxiv.org/pdf/1712.03378v1	2017	Mechanical Twinning in Phosphorene	V. Sorkin|Y. Q. Cai|D. J. Srolovitz|Y. W. Zhang	  We investigate the deformation and failure mechanisms of phosphorene sheet and nanoribbons under uniaxial tensile strain along the zigzag direction using the density functional tight-binding method. Surprisingly, twin-like deformation occurs homogenously across the phosphorene sheet, which significantly increases its failure strain. Vacancies within the sheet lead to the heterogeneous nucleation of twins at a lower critical strain which, subsequently, propagate across the entire sheet. Twin-like deformation always occurs heterogeneously in phosphorene nanoribbons (with or without vacancies). Propagation of the twins is interrupted by fracture which initiates along the ribbon edge. The underlying mechanism is bond breaking between the atoms within phosphorene puckers and simultaneous bond formation between the atoms in neighboring puckers. This unusual deformation behavior in phosphorene may be exploited in novel nano-electronic-mechanical applications. 	
0609650v1	http://arxiv.org/pdf/cond-mat/0609650v1	2006	Statistical Models of Fracture	Mikko J. Alava|Phani K. V. V. Nukala|Stefano Zapperi	  Disorder and long-range interactions are two of the key components that make material failure an interesting playfield for the application of statistical mechanics. The cornerstone in this respect has been lattice models of the fracture in which a network of elastic beams, bonds or electrical fuses with random failure thresholds are subject to an increasing external load. These models describe on a qualitative level the failure processes of real, brittle or quasi-brittle materials. This has been particularly important in solving the classical engineering problems of material strength: the size dependence of maximum stress and its sample to sample statistical fluctuations. At the same time, lattice models pose many new fundamental questions in statistical physics, such as the relation between fracture and phase transitions. Experimental results point out to the existence of an intriguing crackling noise in the acoustic emission and of self-affine fractals in the crack surface morphology. Recent advances in computer power have enabled considerable progress in the understanding of such models. Among these partly still controversial issues, are the scaling and size effects in material strength and accumulated damage, the statistics of avalanches or bursts of microfailures, and the morphology of the crack surface. Here we present an overview of the results obtained with lattice models for fracture, highlighting the relations with statistical physics theories and more conventional fracture mechanics approaches. 	
0704.2925v1	http://arxiv.org/pdf/0704.2925v1	2007	Failure mechanisms and surface roughness statistics of fractured   Fontainebleau sandstone	Laurent Ponson|Harold Auradou|Marc Pessel|Véronique Lazarus|Jean-Pierre Hulin	  In an effort to investigate the link between failure mechanisms and the geometry of fractures of compacted grains materials, a detailed statistical analysis of the surfaces of fractured Fontainebleau sandstones has been achieved. The roughness of samples of different widths W is shown to be self affine with an exponent zeta=0.46 +- 0.05 over a range of length scales ranging from the grain size d up to an upper cut-off length \xi = 0.15 W. This low zeta value is in agreement with measurements on other sandstones and on sintered materials. The probability distributions P(delta z,delta h) of the variations of height over different distances delta z > d can be collapsed onto a single Gaussian distribution with a suitable normalisation and do not display multifractal features. The roughness amplitude, as characterized by the height-height correlation over fixed distances delta z, does not depend on the sample width, implying that no anomalous scaling of the type reported for other materials is present. It is suggested, in agreement with recent theoretical work, to explain these results by the occurence of brittle fracture (instead of damage failure in materials displaying a higher value of zeta = 0.8). 	
1202.2613v1	http://arxiv.org/pdf/1202.2613v1	2012	Molecular Simulation of Fracture Dynamics of Symmetric Tilt Grain   Boundaries in Graphene	Young In Jhon|Pil Seung Chung|Robert Smith|Myung S. Jhon	  Atomistic simulations were utilized to obtain microscopic information of the elongation process in graphene sheets consisting of various embedded symmetric tilt grain boundaries (GBs). In contrast to pristine graphene, these GBs fractured in an extraordinary pattern under transverse uniaxial elongation in all but the largest misorientation angle case, which exhibited intermittent crack propagation and formed many stringy residual connections after quasi mechanical failure. The strings known as monoatomic carbon chains (MACCs), whose importance was recently highlighted, gradually extended to a maximum of a few nanometers as the elongation proceeded. These features, which critically affect the tensile stress and the shape of stress-strain curve, were observed in both armchair and zigzag-oriented symmetric tilt GBs. However, there exist remarkable differences in the population density and the achievable length of MACCs appearing after quasi mechanical failure which were higher in the zigzag-oriented GBs. In addition, the maximum stress and ultimate strain for armchair-oriented GBs were significantly greater than those of zigzag-oriented GBs in case of the largest misorientation angle while they were slightly smaller in other cases. The maximum stress was larger as the misorientation angle increased for both armchair and zigzag-oriented GBs ranging between 32~80 GPa, and the ultimate strains were between 0.06~0.11, the lower limit of which agrees very well with the experimental value of threshold strain beyond which mechanical failure often occurred in polycrystalline graphene. 	
1604.03226v2	http://arxiv.org/pdf/1604.03226v2	2017	Fast Failure Recovery for Main-Memory DBMSs on Multicores	Yingjun Wu|Wentian Guo|Chee-Yong Chan|Kian-Lee Tan	  Main-memory database management systems (DBMS) can achieve excellent performance when processing massive volume of on-line transactions on modern multi-core machines. But existing durability schemes, namely, tuple-level and transaction-level logging-and-recovery mechanisms, either degrade the performance of transaction processing or slow down the process of failure recovery. In this paper, we show that, by exploiting application semantics, it is possible to achieve speedy failure recovery without introducing any costly logging overhead to the execution of concurrent transactions. We propose PACMAN, a parallel database recovery mechanism that is specifically designed for lightweight, coarse-grained transaction-level logging. PACMAN leverages a combination of static and dynamic analyses to parallelize the log recovery: at compile time, PACMAN decomposes stored procedures by carefully analyzing dependencies within and across programs; at recovery time, PACMAN exploits the availability of the runtime parameter values to attain an execution schedule with a high degree of parallelism. As such, recovery performance is remarkably increased. We evaluated PACMAN in a fully-fledged main-memory DBMS running on a 40-core machine. Compared to several state-of-the-art database recovery mechanisms, PACMAN can significantly reduce recovery time without compromising the efficiency of transaction processing. 	
1608.01193v1	http://arxiv.org/pdf/1608.01193v1	2016	Crazing of Nanocomposites with Polymer-Tethered Nanoparticles	Dong Meng|Sanat K. Kumar|Ting Ge|Mark O. Robbins|Gary S. Grest	  The crazing behavior of polymer nanocomposites formed by blending polymer grafted nanoparticles with an entangled polymer melt is studied by molecular dynamics simulations. We focus on the three key differences in the crazing behavior of a composite relative to the pure homopolymer matrix, namely, a lower yield stress, a smaller extension ratio and a grafted chain length dependent failure stress. The yield behavior is found to be mostly controlled by the local nanoparticle-grafted polymer interfacial energy, with the grafted polymer-polymer matrix interfacial structure being of little to no relevance. Increasing the attraction between nanoparticle core and the grafted polymer inhibits void nucleation and leads to a higher yield stress. In the craze growth regime, the presence of grafted chain sections of 100 monomers alters the mechanical response of composite samples, giving rise to smaller extension ratios and higher drawing stresses than for the homopolymer matrix. The dominant failure mechanism of composite samples depends strongly on the length of the grafted chains, with disentanglement being the dominant mechanism for short chains, while bond breaking is the failure mode for chain lengths greater than 10Ne, where Ne is the entanglement length. 	
1301.0595v1	http://arxiv.org/pdf/1301.0595v1	2012	Mechanism Design with Execution Uncertainty	Ryan Porter|Amir Ronen|Yoav Shoham|Moshe Tennenholtz	  We introduce the notion of fault tolerant mechanism design, which extends the standard game theoretic framework of mechanism design to allow for uncertainty about execution. Specifically, we define the problem of task allocation in which the private information of the agents is not only their costs to attempt the tasks, but also their probabilities of failure. For several different instances of this setting we present technical results, including positive ones in the form of mechanisms that are incentive compatible, individually rational and efficient, and negative ones in the form of impossibility theorems. 	
0201016v1	http://arxiv.org/pdf/cs/0201016v1	2002	A computer scientist looks at game theory	Joseph Y. Halpern	  I consider issues in distributed computation that should be of relevance to game theory. In particular, I focus on (a) representing knowledge and uncertainty, (b) dealing with failures, and (c) specification of mechanisms. 	
0108113v2	http://arxiv.org/pdf/quant-ph/0108113v2	2002	The N-box paradox in orthodox quantum mechanics	Conall Boyle|Roger Schafir	  The prediction of the N-box paradox, that whichever box is opened will contain the record of the particle having passed through it, is traced to a failure to specify whether the other boxes are distinguishable or indistinguishable. These correspond to different ways of lifting the degeneracy of a certain measurement, and have incompatible consequences. 	
0709.0992v1	http://arxiv.org/pdf/0709.0992v1	2007	Graphene nano-ribbon under tension	Zhiping Xu	  The mechanical response of graphene nano-ribbon under tensile loading has been investigated using atomistic simulation. Lattice symmetry dependence of elastic properties are found, which fits prediction from Cauchy-Born rule well. Concurrent brittle and ductile behaviors are observed in the failure process at elastic limit, which dominates at low and high temperature respectively. In addition, the free edges of finite width ribbon help to activate bond-flip events and initialize ductile behavior. 	
0808.3272v1	http://arxiv.org/pdf/0808.3272v1	2008	Reply to Comment on "Failure of the work-Hamiltonian connection for   free-energy calculations" by Luca Peliti	J. M. G. Vilar|J. M. Rubi	  We show that Peliti's Comment [arXiv:0808.2855] fails to appreciate basic physical principles and consequently misrepresents both our work as well as the classical work of Gibbs and Tolman. 	
1210.2232v1	http://arxiv.org/pdf/1210.2232v1	2012	Application of hyperbolic scaling for calculation of   reaction-subdiffusion front propagation	A. Iomin|I. M. Sokolov	  A technique of hyperbolic scaling is applied to calculate a reaction front velocity in an irreversible autocatalytic conversion reaction $A+B\,\rightarrow\, 2A$ under subdiffusion. The method, based on the geometric optics approach is a technically elegant observation of the propagation front failure obtained in Phys. Rev. E {\bf 78}, 011128 (2008). 	
1502.06710v1	http://arxiv.org/pdf/1502.06710v1	2015	Actin polymerization front propagation in a comb-reaction system	A. Iomin|V. Zaburdaev|T. Pfohl	  Anomalous transport and reaction dynamics are considered by providing the theoretical grounds for the possible experimental realization of actin polymerization in comb-like geometry. Two limiting regimes are recovered, depending on the concentration of reagents (magnesium and actin). These are both the failure of the reaction front propagation and a finite speed corresponding to the Fisher-KPP long time asymptotic regime. 	
1504.06460v1	http://arxiv.org/pdf/1504.06460v1	2015	Epistemic nature of quantum reasoning	Alfredo B. Henriques|Amílcar Sernadas	  Doubts are raised concerning the usual interpretation of the alleged failure, by quantum mechanics, of the distributive law of classical logic. The difficulty raised by incompatible sets of observables is overcome within an epistemic enrichment of classical logic that provides the means for distinguishing between the value of a variable and its observation while retaining the classical connectives. 	
0907.5325v2	http://arxiv.org/pdf/0907.5325v2	2010	Systemic Risk in a Unifying Framework for Cascading Processes on   Networks	Jan Lorenz|Stefano Battiston|Frank Schweitzer	  We introduce a general framework for models of cascade and contagion processes on networks, to identify their commonalities and differences. In particular, models of social and financial cascades, as well as the fiber bundle model, the voter model, and models of epidemic spreading are recovered as special cases. To unify their description, we define the net fragility of a node, which is the difference between its fragility and the threshold that determines its failure. Nodes fail if their net fragility grows above zero and their failure increases the fragility of neighbouring nodes, thus possibly triggering a cascade. In this framework, we identify three classes depending on the way the fragility of a node is increased by the failure of a neighbour. At the microscopic level, we illustrate with specific examples how the failure spreading pattern varies with the node triggering the cascade, depending on its position in the network and its degree. At the macroscopic level, systemic risk is measured as the final fraction of failed nodes, $X^\ast$, and for each of the three classes we derive a recursive equation to compute its value. The phase diagram of $X^\ast$ as a function of the initial conditions, thus allows for a prediction of the systemic risk as well as a comparison of the three different model classes. We could identify which model class lead to a first-order phase transition in systemic risk, i.e. situations where small changes in the initial conditions may lead to a global failure. Eventually, we generalize our framework to encompass stochastic contagion models. This indicates the potential for further generalizations. 	
1705.03377v1	http://arxiv.org/pdf/1705.03377v1	2017	Real time observation of granular rock analogue material deformation and   failure using nonlinear laser interferometry	Pierre Walczak|Francesco Mezzapesa|Abderrahmane Bouakline|Julien Ambre|Stéphane Bouissou|Stéphane Barland	  A better understanding and anticipation of natural processes such as landsliding or seismic fault activity requires detailed theoretical and experimental analysis of rock mechanics and geomaterial dynamics. These last decades, considerable progress has been made towards understanding deformation and fracture process in laboratory experiment on granular rock materials, as the well-known shear banding experiment. One of the reasons for this progress is the continuous improvement in the instrumental techniques of observation. But the lack of real time methods does not allow the detection of indicators of the upcoming fracture process and thus to anticipate the phenomenon. Here, we have performed uniaxial compression experiments to analyse the response of a granular rock material sample to different shocks. We use a novel interferometric laser sensor based on the nonlinear self-mixing interferometry technique to observe in real time the deformations of the sample and assess its usefulness as a diagnostic tool for the analysis of geomaterial dynamics. Due to the high spatial and temporal resolution of this approach, we observe both vibrations processes in response to a dynamic loading and the onset of failure. The latter is preceded by a continuous variation of vibration period of the material. After several shocks, the material response is no longer reversible and we detect a progressive accumulation of irreversible deformation leading to the fracture process. We demonstrate that material failure is anticipated by the critical slowing down of the surface vibrational motion, which may therefore be envisioned as an early warning signal or predictor to the macroscopic failure of the sample. The nonlinear self-mixing interferometry technique is readily extensible to fault propagation measurements. As such, it opens a new window of observation for the study of geomaterial deformation and failure. 	
0905.4851v1	http://arxiv.org/pdf/0905.4851v1	2009	Craters Formed in Granular Beds by Impinging Jets of Gas	Philip T. Metzger|Robert C. Latta III|Jason M. Schuler|Christopher D. Immer	  When a jet of gas impinges vertically on a granular bed and forms a crater, the grains may be moved by several different mechanisms: viscous erosion, diffused gas eruption, bearing capacity failure, and/or diffusion-driven shearing. The relative importance of these mechanisms depends upon the flow regime of the gas, the mechanical state of the granular material, and other physical parameters. Here we report research in two specific regimes: viscous erosion forming scour holes as a function of particle size and gravity; and bearing capacity failure forming deep transient craters as a function of soil compaction. 	
0909.3174v1	http://arxiv.org/pdf/0909.3174v1	2009	Fiber bundle model with stick-slip dynamics	Zoltan Halasz|Ferenc Kun	  We propose a generic model to describe the mechanical response and failure of systems which undergo a series of stick-slip events when subjected to an external load. We model the system as a bundle of fibers, where single fibers can gradually increase their relaxed length with a stick-slip mechanism activated by the increasing load. We determine the constitutive equation of the system and show by analytical calculations that on the macro-scale a plastic response emerges followed by a hardening or softening regime. Releasing the load, an irreversible permanent deformation occurs which depends on the properties of sliding events. For quenched and annealed disorder of the failure thresholds the same qualitative behavior is found, however, in the annealed case the plastic regime is more pronounced. 	
1006.3770v1	http://arxiv.org/pdf/1006.3770v1	2010	Modeling Vacuum Arcs	Z. Insepov|J. Norem|T. Proslier|D. Huang|S. Mahalingam|S. Veitzer	  We are developing a model of vacuum arcs. This model assumes that arcs develop as a result of mechanical failure of the surface due to Coulomb explosions, followed by ionization of fragments by field emission and the development of a small, dense plasma that interacts with the surface primarily through self sputtering and terminates as a unipolar arc capable of producing breakdown sites with high enhancement factors. We have attempted to produce a self consistent picture of triggering, arc evolution and surface damage. We are modeling these mechanisms using Molecular Dynamics (mechanical failure, Coulomb explosions, self sputtering), Particle-In-Cell (PIC) codes (plasma evolution), mesoscale surface thermodynamics (surface evolution), and finite element electrostatic modeling (field enhancements). We can present a variety of numerical results. We identify where our model differs from other descriptions of this phenomenon. 	
1007.4985v1	http://arxiv.org/pdf/1007.4985v1	2010	Anomalous Strength Characteristics of Tilt Grain Boundaries in Graphene	Rassin Grantab|Vivek B. Shenoy|Rodney S. Ruoff	  Using molecular dynamics simulations and first principles calculations, we have studied the structure and mechanical strength of tilt grain boundaries in graphene sheets that arise during CVD growth of graphene on metal substrates. Surprisingly, we find that for tilt boundaries in the vicinity of both the zig-zag and arm-chair orientations, large angle boundaries with a higher density of 5-7 defect pairs are stronger than the low-angle boundaries which are comprised of fewer defects per unit length. Interestingly, the trends in our results cannot be explained by a continuum Griffith-type fracture mechanics criterion, which predicts the opposite trend due to that fact that it does not account for the critical bonds that are responsible for the failure mechanism. We have identified the highly-strained bonds in the 7-member rings that lead to the failure of the sheets, and we have found that large angle boundaries are able to better accommodate the strained 7-rings. Our results provide guidelines for designing growth methods to obtain grain boundary structures that can have strengths close to that of pristine graphene. 	
1106.0837v2	http://arxiv.org/pdf/1106.0837v2	2011	Mechanochemical reaction in graphane under uniaxial tension	N. A. Popova|E. F. Sheka	  The quantum-mechanochemical-reaction-coordinate simulations have been performed to investigate the mechanical properties of hydrogen functionalized graphene. The simulations disclosed atomically matched peculiarities that accompany the deformation-failure-rupture process occurred in the body. A comparative study of the deformation peculiarities related to equi-carbon-core (5,5) nanographene and nanographane sheets exhibited a high stiffness of both bodies that is provided by the related hexagon units, namely benzenoid and cyclohexanoid, respectively. The two units are characterized by anisotropy in the microscopic behavior under elongation along mechanochemical internal coordinates when the later are oriented either along (zg) or normally (ach) to the C-C bonds chain. The unit feature in combination with different configuration of their packing with respect to the body C-C bond chains forms the ground for the structure-sensitive mechanical behavior that is different for zg and ach deformation modes. Hydrogenation of graphene drastically influences behavior and numerical characteristics of the body making tricotage-like pattern of the graphene failure less pronounced and inverting it from the zg to ach mode as well as providing less mechanical resistance of graphane it total. 	
1107.1027v1	http://arxiv.org/pdf/1107.1027v1	2011	Failure of Mineralized Collagen Microfibrils Using Finite Element   Simulation Coupled to Mechanical Quasi-brittle Damage	Abdelwahed Barkaoui|Awad Bettamer|Ridha Hambli	  Bone is a multiscale heterogeneous materiel of which principal function is to support the body structure and to resist mechanical loading and fractures. Bone strength does not depend only on the quantity and quality of bone which is characterized by the geometry and the shape of bones but also on the mechanical proprieties of its compounds, which have a significant influence on its deformation and failure. This work aim to use a 3D nano-scale finite element model coupled to the concept of quasi-brittle damage with the behaviour law isotropic elasticity to investigate the fracture behaviour of composite materiel collagen-mineral (mineralized collagen microfibril). Fracture stress-number of cross-links and damping capacity-number of cross-links curves were obtained under tensile loading conditions at different densities of the mineral phase. The obtained results show that number of cross-links as well as the density of mineral has an important influence on the strength of microfibrils which in turn clarify the bone fracture at macro-scale. 	
1405.7924v2	http://arxiv.org/pdf/1405.7924v2	2014	Combining mechanical and chemical effects in the deformation and failure   of a cylindrical electrode particle in a Li-ion battery	Jeevanjyoti Chakraborty|Colin P. Please|Alain Goriely|S. Jonathan Chapman	  A general framework to study the mechanical behaviour of a cylindrical silicon anode particle in a lithium ion battery as it undergoes lithiation is presented. The two-way coupling between stress and concentration of lithium in silicon, including the possibility of plastic deformation, is taken into account and two particular cases are considered. First, the cylindrical particle is assumed to be free of surface traction and second, the axial deformation of the cylinder is prevented. In both cases plastic stretches develop through the entire cylinder and not just near the surface as is commonly found in spherical anode particles. It is shown that the stress evolution depends both on the lithiation rate and the external constraints. Furthermore, as the cylinder expands during lithiation it can develop a compressive axial stress large enough to induce buckling, which in turn may lead to mechanical failure. An explicit criterion for swelling-induced buckling obtained as a modification of the classical Euler buckling criterion shows the competition between the stabilising effect of radius increase and the destabilising effect of axial stress. 	
1602.08473v1	http://arxiv.org/pdf/1602.08473v1	2016	Modeling, Minimizing and Managing the Risk of Fatigue for Mechanical   Components	L. Bittner|H. Gottschalk|M. Gröger|N. Moch|M. Saadi|S. Schmitz	  Mechanical components that are exposed to cyclic mechanical loading fail at loads that are well below the ultimate tensile strength. This process is known as fatigue. The failure time, that is the time when a first crack forms, is highly random. In this work we review some recent developments in the modelling of probabilistic failure times, understood as the time to the formation of a fatigue crack.   We also discuss the how probabilistic models can be used in shape design with the design intent of optimizing the component's reliability. We give review a recent existence result for optimal shapes and we discuss continuous and discrete shape derivatives. Another application is optimal service scheduling. The mathematical fields involved range from reliability statistics over stochastic point processes, multiscale modeling, PDEs on variable geometries, shape optimization and numerical analysis to operations research. 	
1611.00345v2	http://arxiv.org/pdf/1611.00345v2	2016	Fracture of a model cohesive granular material	Alexander Schmeink|Lucas Goehring|Arnaud Hemmerle	  We study experimentally the fracture mechanisms of a model cohesive granular medium consisting of glass beads held together by solidified polymer bridges. The elastic response of this material can be controlled by changing the cross-linking of the polymer phase, for example. Here we show that its fracture toughness can be tuned over an order of magnitude by adjusting the stiffness and size of the polymer bridges. We extract a well-defined fracture energy from fracture testing under a range of material preparations. This energy is found to scale linearly with the cross-sectional area of the bridges. Finally, X-ray microcomputed tomography shows that crack propagation is driven by adhesive failure of about one polymer bridge per bead located at the interface, along with microcracks in the vicinity of the failure plane. Our findings provide insight to the fracture mechanisms of this model material, and the mechanical properties of disordered cohesive granular media in general. 	
1304.5402v1	http://arxiv.org/pdf/1304.5402v1	2013	Context-Independent Centrality Measures Underestimate the Vulnerability   of Power Grids	Trivik Verma|Wendy Ellens|Robert E. Kooij	  Power grids vulnerability is a key issue in society. A component failure may trigger cascades of failures across the grid and lead to a large blackout. Complex network approaches have shown a direction to study some of the problems faced by power grids. Within Complex Network Analysis structural vulnerabilities of power grids have been studied mostly using purely topological approaches, which assumes that flow of power is dictated by shortest paths. However, this fails to capture the real flow characteristics of power grids. We have proposed a flow redistribution mechanism that closely mimics the flow in power grids using the PTDF. With this mechanism we enhance existing cascading failure models to study the vulnerability of power grids.   We apply the model to the European high-voltage grid to carry out a comparative study for a number of centrality measures. `Centrality' gives an indication of the criticality of network components. Our model offers a way to find those centrality measures that give the best indication of node vulnerability in the context of power grids, by considering not only the network topology but also the power flowing through the network. In addition, we use the model to determine the spare capacity that is needed to make the grid robust to targeted attacks. We also show a brief comparison of the end results with other power grid systems to generalise the result. 	
0209308v1	http://arxiv.org/pdf/cond-mat/0209308v1	2002	Scaling laws of creep rupture of fiber bundles	Ferenc Kun|Raul Cruz Hidalgo|Hans J. Herrmann|Karoly F. Pal	  We study the creep rupture of fiber composites in the framework of fiber bundle models. Two novel fiber bundle models are introduced based on different microscopic mechanisms responsible for the macroscopic creep behavior. Analytical and numerical calculations show that above a critical load the deformation of the creeping system monotonically increases in time resulting in global failure at a finite time $t_f$, while below the critical load the system suffers only partial failure and the deformation tends to a constant value giving rise to an infinite lifetime. It is found that approaching the critical load from below and above the creeping system is characterized by universal power laws when the fibers have long range interaction. The lifetime of the composite above the critical point has a universal dependence on the system size. 	
0305319v3	http://arxiv.org/pdf/cond-mat/0305319v3	2004	Entropy of Pseudo Random Number Generators	Stephan Mertens|Heiko Bauke	  Since the work of Ferrenberg et al.[PRL 69, (1992)] some pseudo random number generators are known to yield wrong results in cluster Monte Carlo simulations. In this contribution the fundamental mechanism behind this failure is discussed. Almost all random number generators calculate a new pseudo random number $x_i$ from preceding values, $x_i = f(x_{i-1}, x_{i-2},..., x_{i-q})$. Failure of these generators in cluster Monte Carlo simulations and related experiments can be attributed to the low entropy of the production rule $f()$ conditioned on the statistics of the input values $x_{i-1},...,x_{i-q}$. Being a measure only of the arithmetic operations in the generator rule, the conditional entropy is independent of the lag in the recurrence or the period of the sequence. In that sense it measures a more profound quality of a random number generator than empirical tests with their limited horizon. 	
0411019v1	http://arxiv.org/pdf/cs/0411019v1	2004	Programmable Ethernet Switches and Their Applications	Srikant Sharma|Tzi-cker Chiueh	  Modern Ethernet switches support many advanced features beyond route learning and packet forwarding such as VLAN tagging, IGMP snooping, rate limiting, and status monitoring, which can be controlled through a programmatic interface. Traditionally, these features are mostly used to statically configure a network. This paper proposes to apply them as dynamic control mechanisms to maximize physical network link resources, to minimize failure recovery time, to enforce QoS requirements, and to support link-layer multicast without broadcasting. With these advanced programmable control mechanisms, standard Ethernet switches can be used as effective building blocks for metropolitan-area Ethernet networks (MEN), storage-area networks (SAN), and computation cluster interconnects. We demonstrate the usefulness of this new level of control over Ethernet switches with a MEN architecture that features multi-fold throughput gains and sub-second failure recovery time. 	
0607201v1	http://arxiv.org/pdf/physics/0607201v1	2006	A two phase harmonic model for left ventricular function	S. Dubi|C. Dubi|Y. Dubi	  A minimal model for mechanical motion of the left ventricle is proposed. The model assumes the left ventricle to be a harmonic oscillator with two distinct phases, simulating the systolic and diastolic phases, at which both the amplitude and the elastic constant of the oscillator are different. Taking into account the pressure within the left ventricle, the model shows qualitative agreement with functional parameters of the left ventricle. The model allows for a natural explanation of heart failure with preserved systolic left ventricular function, also termed diastolic heart failure. Specifically, the rise in left ventricular filling pressures following increased left-ventricular wall stiffness is attributed to a mechanism aimed at preserving heart rate and cardiac output. 	
0106072v6	http://arxiv.org/pdf/quant-ph/0106072v6	2003	"Quantal" behavior in classical probability	K. A. Kirkpatrick	  A number of phenomena generally believed characteristic of quantum mechanics and seen as interpretively problematic--the incompatibility and value-indeterminacy of variables, the non-existence of dispersion-free states, the failure of the standard marginal-probability formula, the failure of the distributive law of disjunction and interference--are exemplified in an emphatically non-quantal system: a deck of playing cards. Thus the appearance, in quantum mechanics, of incompatibility and these associated phenomena requires neither explanation nor interpretation. 	
0705.3694v1	http://arxiv.org/pdf/0705.3694v1	2007	Calculation of The Lifetimes of Thin Stripper Targets Under Bombardment   of Intense Pulsed Ions	S. G. Lebedev|A. S. Lebedev	  The problems of stripper target behavior in the nonstationary intense particle beams are considered. The historical sketch of studying of radiation damage failure of carbon targets under ion bombardment is presented. The simple model of evaporation of a target by an intensive pulsing beam is supposed. Stripper foils lifetimes in the nonstationary intense particle can be described by two failure mechanisms: radiation damage accumulation and evaporation of target. At the maximal temperatures less than 2500K the radiation damage are dominated; at temperatures above 2500K the mechanism of evaporation of a foil prevails. The proposed approach has been applied to the discription of behaviour of stripper foils in the BNL linac and SNS conditions. 	
0705.4321v2	http://arxiv.org/pdf/0705.4321v2	2007	Coulomb oscillations as a remedy for the helium atom	Manfred Bucher	  The largest failure of the old, Bohr-Sommerfeld quantum theory was with the helium atom. It brought about the theory's demise. I show that this failure does not originate, as commonly believed, with the orbit concept per se. Instead, it was caused by the wrong choice of orbits, compounded by ignorance of the exclusion principle. Choosing semiclassical electron oscillations through the He nucleus, I calculate a singlet ground-state energy that rivals in accuracy with quantum-mechanical results. The same method reveals Bohr's historic energy value as the forbidden triplet ground state--a result beyond the reach of quantum mechanics. At the qualitative level, the concept of Coulomb oscillations visually explains the major features in the He double spectrum in terms of crossed or parallel orbit orientation. 	
0805.1802v1	http://arxiv.org/pdf/0805.1802v1	2008	Depinning transition in failure of inhomogeneous brittle materials	Laurent Ponson	  The dynamics of a crack propagating in an elastic inhomogeneous material is investigated. The variations of the average crack velocity with the external loading are measured for a brittle rock and are shown to display two distinct regimes: Below a given threshold Gc, the crack velocity is well described by an exponential law v ~ exp^{-(C/(G-(Gamma))} characteristic of subcritical propagation, while for larger values of the driving force G > Gc, the velocity evolves as a power law v ~ (G - G_c)^theta with theta = 0.80 $\pm$ 0.15. These results can be explained extending the continuum theory of Fracture Mechanics to disordered systems. In this description, the motion of a crack is analogue to the one of an elastic line driven in a random medium and critical failure occurs when the loading is sufficiently large to depinne the crack front from the heterogeneities of the material. 	
0903.4534v1	http://arxiv.org/pdf/0903.4534v1	2009	Geometrical and transport properties of single fractures: influence of   the roughness of the fracture walls	Harold Auradou	  This article reviews the main features of the transport properties of single fractures. A particular attention paid to fractures in geological materials which often display a roughness covering a broad range of length scales. Because of the small distance separating the fracture walls, the surface roughness is a key parameter influencing the structure of the void space. Studies devoted to the characterization of the surface roughness are presented as well as works aimed at characterizing the void space geometry. The correlation of the free space is found to be crucially function of the failure mechanism (brittle, quasi brittle or plastic...) but also of possible shear displacements during the failure. The influence of the surface roughness on the mechanical behavior of fractures under a normal load and a shear stress is also described. Finally, experimental, numerical and theoretical works devoted to the study of the influence of the fracture void geometry on the permeability and on the hydrodynamic dispersion of a dissolved species are discussed. 	
0910.0940v3	http://arxiv.org/pdf/0910.0940v3	2010	A homoclinic route to asymptotic full cooperation in adaptive networks   and its failure	Gerd Zschaler|Arne Traulsen|Thilo Gross	  We consider the evolutionary dynamics of a cooperative game on an adaptive network, where the strategies of agents (cooperation or defection) feed back on their local interaction topology. While mutual cooperation is the social optimum, unilateral defection yields a higher payoff and undermines the evolution of cooperation. Although no a priori advantage is given to cooperators, an intrinsic dynamical mechanism can lead asymptotically to a state of full cooperation. In finite systems, this state is characterized by long periods of strong cooperation interrupted by sudden episodes of predominant defection, suggesting a possible mechanism for the systemic failure of cooperation in real-world systems. 	
1008.1361v1	http://arxiv.org/pdf/1008.1361v1	2010	Pressure-Induced Critical Influences on Workpiece-Tool Thermal   Interaction in High Speed Dry Machining of Titanium	H. A. Abdel-Aal|M. El Mansori	  Cutting tools are subject to extreme thermal and mechanical loads during operation. The state of loading is intensified in dry cutting environment especially when cutting the so called hard-to-cut-materials. Although, the effect of mechanical loads on tool failure have been extensively studied, detailed studies on the effect of thermal loads on the deterioration of the cutting tool are rather scarce. In this paper we study failure of coated carbide tools due to thermal loading. The study emphasizes the role assumed by the thermo-physical properties of the tool material in enhancing or preventing mass attrition of the cutting elements within the tool. It is shown that within a comprehensive view of the nature of conduction in the tool zone, thermal conduction is not solely affected by temperature. Rather it is a function of the so called thermodynamic forces. These are the stress, the strain, strain rate, rate of temperature rise, and the temperature gradient. Although that within such consideration description of thermal conduction is non-linear, it is beneficial to employ such a form because it facilitates a full mechanistic understanding of thermal activation of tool wear. 	
1203.4196v3	http://arxiv.org/pdf/1203.4196v3	2012	Mechanical properties of polycrystalline graphene based on a realistic   atomistic model	Jani Kotakoski|Jannik C. Meyer	  Graphene can at present be grown at large quantities only by the chemical vapor deposition method, which produces polycrystalline samples. Here, we describe a method for constructing realistic polycrystalline graphene samples for atomistic simulations, and apply it for studying their mechanical properties. We show that cracks initiate at points where grain boundaries meet and then propagate through grains predominantly in zigzag or armchair directions, in agreement with recent experimental work. Contrary to earlier theoretical predictions, we observe normally distributed intrinsic strength (~ 50% of that of the mono-crystalline graphene) and failure strain which do not depend on the misorientation angles between the grains. Extrapolating for grain sizes above 15 nm results in a failure strain of ~ 0.09 and a Young's modulus of ~ 600 GPa. The decreased strength can be adequately explained with a conventional continuum model when the grain boundary meeting points are identified as Griffith cracks. 	
1307.3370v1	http://arxiv.org/pdf/1307.3370v1	2013	Crackling vs. continuum-like dynamics in brittle failure	Jonathan Barés|Luc Barbier|Daniel Bonamy	  We study how the loading rate, specimen geometry and microstructural texture select the dynamics of a crack moving through an heterogeneous elastic material in the quasi-static approximation. We find a transition, fully controlled by two dimensionless variables, between dynamics ruled by continuum fracture mechanics and crackling dynamics. Selection of the latter by the loading, microstructure and specimen parameters is formulated in terms of scaling laws on the power spectrum of crack velocity. This analysis defines the experimental conditions required to observe crackling in fracture. Beyond failure problems, the results extend to a variety of situations described by models of the same universality class, e.g. the dynamics in wetting or of domain walls in amorphous ferromagnets. 	
1308.1210v2	http://arxiv.org/pdf/1308.1210v2	2014	Cavity-based robustness analysis of interdependent networks: Influences   of intranetwork and internetwork degree-degree correlations	Shunsuke Watanabe|Yoshiyuki Kabashima	  We develop a methodology for analyzing the percolation phenomena of two mutually coupled (interdependent) networks based on the cavity method of statistical mechanics. In particular, we take into account the influence of degree-degree correlations inside and between the networks on the network robustness against targeted attacks and random failures. We show that the developed methodology is reduced to the well-known generating function formalism in the absence of degree-degree correlations. The validity of the developed methodology is confirmed by a comparison with the results of numerical experiments. Our analytical results imply that the robustness of the interdependent networks depends considerably on both the intra- and internetwork degree-degree correlations in the case of targeted attacks, whereas the significance of the degree-degree correlations is relatively low for random failures. 	
1309.3150v2	http://arxiv.org/pdf/1309.3150v2	2013	How (Not) to Shoot in Your Foot with SDN Local Fast Failover: A   Load-Connectivity Tradeoff	Michael Borokhovich|Stefan Schmid	  This paper studies the resilient routing and (in-band) fast failover mechanisms supported in Software-Defined Networks (SDN). We analyze the potential benefits and limitations of such failover mechanisms, and focus on two main metrics: (1) correctness (in terms of connectivity and loop-freeness) and (2) load-balancing. We make the following contributions. First, we show that in the worst-case (i.e., under adversarial link failures), the usefulness of local failover is rather limited: already a small number of failures will violate connectivity properties under any fast failover policy, even though the underlying substrate network remains highly connected. We then present randomized and deterministic algorithms to compute resilient forwarding sets; these algorithms achieve an almost optimal tradeoff. Our worst-case analysis is complemented with a simulation study. 	
1310.8040v1	http://arxiv.org/pdf/1310.8040v1	2013	Homophyly and Randomness Resist Cascading Failure in Networks	Angsheng Li|Wei Zhang|Yicheng Pan|Xuechen Li	  The universal properties of power law and small world phenomenon of networks seem unavoidably obstacles for security of networking systems. Existing models never give secure networks. We found that the essence of security is the security against cascading failures of attacks and that nature solves the security by mechanisms. We proposed a model of networks by the natural mechanisms of homophyly, randomness and preferential attachment. It was shown that homophyly creates a community structure, that homophyly and randomness introduce ordering in the networks, and that homophyly creates inclusiveness and introduces rules of infections. These principles allow us to provably guarantee the security of the networks against any attacks. Our results show that security can be achieved provably by structures, that there is a tradeoff between the roles of structures and of thresholds in security engineering, and that power law and small world property are never obstacles for security of networks. 	
1405.5504v1	http://arxiv.org/pdf/1405.5504v1	2014	Mechanical Properties of Graphene Nanowiggles	R. A. Bizao|T. Botari|D. S. Galvao	  In this work we have investigated the mechanical properties and fracture patterns of some graphene nanowiggles (GNWs). Graphene nanoribbons are finite graphene segments with a large aspect ratio, while GNWs are nonaligned periodic repetitions of graphene nanoribbons. We have carried out fully atomistic molecular dynamics simulations using a reactive force field (ReaxFF), as implemented in the LAMPPS (Large-scale Atomic/Molecular Massively Parallel Simulator) code. Our results showed that the GNW fracture patterns are strongly dependent on the nanoribbon topology and present an interesting behavior, since some narrow sheets have larger ultimate failure strain values. This can be explained by the fact that narrow nanoribbons have more angular freedom when compared to wider ones, which can create a more efficient way to accumulate and to dissipate strain/stress. We have also observed the formation of linear atomic chains (LACs) and some structural defect reconstructions during the material rupture. The reported graphene failure patterns, where zigzag/armchair edge terminated graphene structures are fractured along armchair/zigzag lines, were not observed in the GNW analyzed cases. 	
1511.06472v1	http://arxiv.org/pdf/1511.06472v1	2015	Enhancing the Performance of the T-Peel Test for Thin and Flexible   Adhered Laminates	Nikhil Padhye|David M. Parks|Alexander H. Slocum|Bernhardt L. Trout	  Symmetrically bonded thin and flexible T-peel specimens, when tested on vertical travel machines, can be subject to significant gravitational loading; with the associated asymmetry and mixed-mode failure during peeling. This can cause erroneously high experimental peel forces to be recorded which leads to uncertainty in estimating interfacial fracture toughness and failure mode. To overcome these issues, a mechanical test fixture has been designed for use with vertical test machines, that supports the unpeeled portion of the test specimen and suppresses parasitic loads due to gravity from affecting the peel test. The mechanism, driven by the test machine cross-head, moves at one-half of the velocity of the cross-head such that the unpeeled portion always lies in the plane of the instantaneous center of motion. Several specimens such as bonded polymeric films, laminates, and commercial tapes were tested with and without the fixture, and the importance of the proposed T-peel procedure has been demonstrated. 	
1601.03162v1	http://arxiv.org/pdf/1601.03162v1	2016	Jump-starting coordination in a stag hunt: Motivation, mechanisms, and   their analysis	Ioannis Avramopoulos	  The stag hunt (or assurance game) is a simple game that has been used as a prototype of a variety of social coordination problems (ranging from the social contract to the adoption of technical standards). Players have the option to either use a superior cooperative strategy whose payoff depends on the other players' choices or use an inferior strategy whose payoff is independent of what other players do; the cooperative strategy may incur a loss if sufficiently many other players do not cooperate. Stag hunts have two (strict) pure Nash equilibria, namely, universal cooperation and universal defection (as well as a mixed equilibrium of low predictive value). Selection of the inferior (pure) equilibrium is called a coordination failure. In this paper, we present and analyze using game-theoretic techniques mechanisms aiming to avert coordination failures and incite instead selection of the superior equilibrium. Our analysis is based on the solution concepts of Nash equilibrium, dominance solvability, as well as a formalization of the notion of "incremental deployability," which is shown to be keenly relevant to the sink equilibrium. 	
1607.07360v2	http://arxiv.org/pdf/1607.07360v2	2016	A cohesive granular material with tunable elasticity	Arnaud Hemmerle|Matthias Schröter|Lucas Goehring	  By mixing glass beads with a curable polymer we create a well-defined cohesive granular medium, held together by solidified, and hence elastic, capillary bridges. This material has a geometry similar to a wet packing of beads, but with an additional control over the elasticity of the bonds holding the particles together. We show that its mechanical response can be varied over several orders of magnitude by adjusting the size and stiffness of the bridges, and the size of the particles. We also investigate its mechanism of failure under unconfined uniaxial compression in combination with in situ x-ray microtomography. We show that a broad linear-elastic regime ends at a limiting strain of about 8%, whatever the stiffness of the agglomerate, which corresponds to the beginning of shear failure. The possibility to finely tune the stiffness, size and shape of this simple material makes it an ideal model system for investigations on, for example, fracturing of porous rocks, seismology, or root growth in cohesive porous media. 	
1609.09411v1	http://arxiv.org/pdf/1609.09411v1	2016	Seismic collapse prediction of frame structures by means of genetic   algorithms	A. Greco|F. Cannizzaro|A. Pluchino	  This paper presents an automatic approach for the evaluation of the plastic load and failure modes of planar frames. The method is based on the generation of elementary collapse mechanisms and on their linear combination aimed at minimizing the collapse load factor. The minimization procedure is efficiently performed by means of genetic algorithms which allow to compute an approximate collapse load factor, and the correspondent failure mode, with sufficient accuracy in a very short computing time. A user-friendly original software in the agent-based programming language Netlogo, here employed for the first time with structural engineering purposes, has been developed showing its great versatility and advantages. Many applications have been performed both with reference to the classical plastic analysis approach, in which all the loads increase proportionally, and with a seismic point of view considering a system of horizontal forces whose magnitude increases while the vertical loads are assumed to be constant. In this latter case a parametric study has been performed aiming at evaluating the influence of some geometric, mechanical and load distribution parameters on the ultimate collapse load of planar frames. 	
1302.0792v3	http://arxiv.org/pdf/1302.0792v3	2014	Probe Scheduling for Efficient Detection of Silent Failures	Edith Cohen|Avinatan Hassidim|Haim Kaplan|Yishay Mansour|Danny Raz|Yoav Tzur	  Most discovery systems for silent failures work in two phases: a continuous monitoring phase that detects presence of failures through probe packets and a localization phase that pinpoints the faulty element(s). This separation is important because localization requires significantly more resources than detection and should be initiated only when a fault is present.   We focus on improving the efficiency of the detection phase, where the goal is to balance the overhead with the cost associated with longer failure detection times. We formulate a general model which unifies the treatment of probe scheduling mechanisms, stochastic or deterministic, and different cost objectives - minimizing average detection time (SUM) or worst-case detection time (MAX).   We then focus on two classes of schedules. {\em Memoryless schedules} -- a subclass of stochastic schedules which is simple and suitable for distributed deployment. We show that the optimal memorlyess schedulers can be efficiently computed by convex programs (for SUM objectives) or linear programs (for MAX objectives), and surprisingly perhaps, are guaranteed to have expected detection times that are not too far off the (NP hard) stochastic optima. {\em Deterministic schedules} allow us to bound the maximum (rather than expected) cost of undetected faults, but like stochastic schedules, are NP hard to optimize. We develop novel efficient deterministic schedulers with provable approximation ratios.   An extensive simulation study on real networks, demonstrates significant performance gains of our memoryless and deterministic schedulers over previous approaches. Our unified treatment also facilitates a clear comparison between different objectives and scheduling mechanisms. 	
0807.1943v1	http://arxiv.org/pdf/0807.1943v1	2008	Failure of antibiotic treatment in microbial populations	Patrick De Leenheer|Nick Cogan	  The tolerance of bacterial populations to biocidal or antibiotic treatment has been well documented in both biofilm and planktonic settings. However, there is still very little known about the mechanisms that produce this tolerance. Evidence that small, non-mutant subpopulations of bacteria are not affected by antibiotic challenge has been accumulating and provides an attractive explanation for the failure of typical dosing protocols. Although a dosing challenge can kill all the susceptible bacteria, the remaining persister cells can serve as a source of population regrowth. We give a robust condition for the failure of a periodic dosing protocol for a general chemostat model, which supports the mathematical conclusions and simulations of an earlier, more specialized batch model. Our condition implies that the treatment protocol fails globally, in the sense that a mixed bacterial population will ultimately persist above a level that is independent of the initial composition of the population. We also give a sufficient condition for treatment success, at least for initial population compositions near the steady state of interest, corresponding to bacterial washout. Finally, we investigate how the speed at which the bacteria are wiped out depends on the duration of administration of the antibiotic. We find that this dependence is not necessarily monotone, implying that optimal dosing does not necessarily correspond to continuous administration of the antibiotic. Thus, genuine periodic protocols can be more advantageous in treating a wide variety of bacterial infections. 	
0908.3154v1	http://arxiv.org/pdf/0908.3154v1	2009	Impact of Random Failures and Attacks on Poisson and Power-Law Random   Networks	Clemence Magnien|Matthieu Latapy|Jean-Loup Guillaume	  It appeared recently that the underlying degree distribution of networks may play a crucial role concerning their robustness. Empiric and analytic results have been obtained, based on asymptotic and mean-field approximations. Previous work insisted on the fact that power-law degree distributions induce high resilience to random failure but high sensitivity to attack strategies, while Poisson degree distributions are quite sensitive in both cases. Then, much work has been done to extend these results.   We aim here at studying in depth these results, their origin, and limitations. We review in detail previous contributions and give full proofs in a unified framework, and identify the approximations on which these results rely. We then present new results aimed at enlightening some important aspects. We also provide extensive rigorous experiments which help evaluate the relevance of the analytic results.   We reach the conclusion that, even if the basic results of the field are clearly true and important, they are in practice much less striking than generally thought. The differences between random failures and attacks are not so huge and can be explained with simple facts. Likewise, the differences in the behaviors induced by power-law and Poisson distributions are not as striking as often claimed. 	
1104.3667v1	http://arxiv.org/pdf/1104.3667v1	2011	Reliability-based design optimization using kriging surrogates and   subset simulation	V. Dubourg|B. Sudret|J. -M. Bourinet	  The aim of the present paper is to develop a strategy for solving reliability-based design optimization (RBDO) problems that remains applicable when the performance models are expensive to evaluate. Starting with the premise that simulation-based approaches are not affordable for such problems, and that the most-probable-failure-point-based approaches do not permit to quantify the error on the estimation of the failure probability, an approach based on both metamodels and advanced simulation techniques is explored. The kriging metamodeling technique is chosen in order to surrogate the performance functions because it allows one to genuinely quantify the surrogate error. The surrogate error onto the limit-state surfaces is propagated to the failure probabilities estimates in order to provide an empirical error measure. This error is then sequentially reduced by means of a population-based adaptive refinement technique until the kriging surrogates are accurate enough for reliability analysis. This original refinement strategy makes it possible to add several observations in the design of experiments at the same time. Reliability and reliability sensitivity analyses are performed by means of the subset simulation technique for the sake of numerical efficiency. The adaptive surrogate-based strategy for reliability estimation is finally involved into a classical gradient-based optimization algorithm in order to solve the RBDO problem. The kriging surrogates are built in a so-called augmented reliability space thus making them reusable from one nested RBDO iteration to the other. The strategy is compared to other approaches available in the literature on three academic examples in the field of structural mechanics. 	
1105.5329v1	http://arxiv.org/pdf/1105.5329v1	2011	Ising model for distribution networks	H. Hooyberghs|S. Van Lombeek|C. Giuraniuc|B. Van Schaeybroeck|J. O. Indekeu	  An elementary Ising spin model is proposed for demonstrating cascading failures (break-downs, blackouts, collapses, avalanches, ...) that can occur in realistic networks for distribution and delivery by suppliers to consumers. A ferromagnetic Hamiltonian with quenched random fields results from policies that maximize the gap between demand and delivery. Such policies can arise in a competitive market where firms artificially create new demand, or in a solidary environment where too high a demand cannot reasonably be met. Network failure in the context of a policy of solidarity is possible when an initially active state becomes metastable and decays to a stable inactive state. We explore the characteristics of the demand and delivery, as well as the topological properties, which make the distribution network susceptible of failure. An effective temperature is defined, which governs the strength of the activity fluctuations which can induce a collapse. Numerical results, obtained by Monte Carlo simulations of the model on (mainly) scale-free networks, are supplemented with analytic mean-field approximations to the geometrical random field fluctuations and the thermal spin fluctuations. The role of hubs versus poorly connected nodes in initiating the breakdown of network activity is illustrated and related to model parameters. 	
1202.5931v1	http://arxiv.org/pdf/1202.5931v1	2012	Predicting effects of structural stress in a genome-reduced model   bacterial metabolism	Oriol Güell|Francesc Sagués|M. Ángeles Serrano	  We studied in silico effects of structural stress in Mycoplasma pneumoniae, a genome-reduced model bacterial organism, by tracking the damage propagating on its metabolic network after a deleterious perturbation. First, we analyzed failure cascades spreading from individual reactions and pairs of reactions and compared the results to those in Staphylococcus aureus and Escherichia coli. To alert to the potential damage caused by the failure of individual reactions, we propose a generic predictor based on local information that identifies target reactions for structural vulnerability. With respect to the simultaneous failure of pairs of reactions, we detected strong non-linear amplification effects that can be predicted by the presence of specific motifs in the intersection of single cascades. We further connected the metabolic and gene co-expression networks of M. pneumoniae through enzyme activities, and studied the consequences of knocking out individual genes and clusters of genes. Damage caused by single gene knockouts reveals a strong correlation between genome-scale cascades of large impact and gene essentiality. At the same time, we found that genes controlling high-damage reactions tend to operate in functional isolation, as a metabolic protection mechanism. We conclude that the architecture of M. pneumoniae, both at the level of metabolism and genome, seems to have evolved towards increased structural robustness, similarly to other more complex model bacterial organisms, despite its reduced genome size and its greater metabolic network linearity. Our approach, although motivated biochemically, is generic enough to be of potential use toward analyzing and predicting spreading of structural stress in any bipartite complex network. 	
1206.3838v1	http://arxiv.org/pdf/1206.3838v1	2012	On The Recovery Performance of Single- and Multipath OLSR in Wireless   Multi-Hop Networks	Inès Doghri|Laurent Reynaud|Isabelle Guérin-Lassous	  In this paper, we study and improve the recovery properties of single and multipath routing strategies when facing network failure situations. In particular, we focus our study on two MANET routing protocols: OLSR and its multipath extension MP-OLSR. In various wireless multi-hop network environments, especially in multiple chain topologies, we define and seek to evaluate the latency introduced by these protocols to find a new path after a link failure. Theoretical estimations and simulation results show that, under dual chain-topologies, this latency can be too long and incompatible with the needs of loss and delay constrained applications. As the source nodes cannot detect link failures immediately because of the delay incurred by the well-known nature of link state protocols in general, and of OLSR Topology Control (TC) messages in particular, these nodes keep sending packets along broken paths. We thus study the inconsistencies between the actual network topology and the nodes' own representation. After analyzing the consequences of this long latency, we seek to alleviate these problems with the introduction of adapted mechanisms. We propose three new different schemes and accordingly extend the original OLSR and MP-OLSR protocols in order to decrease the expected latency and improve the protocol performance. Simulation results show a steep decrease of the latency when using these new schemes in dual chain-topologies. We also discuss these results in terms of packet loss, end-to-end delay and overhead. 	
1210.4954v2	http://arxiv.org/pdf/1210.4954v2	2013	Optimal Reliability in Design for Fatigue Life	Hanno Gottschalk|Sebastian Schmitz	  The failure of a component often is the result of a degradation process that originates with the formation of a crack. Fatigue describes the crack formation in the material under cyclic loading. Activation and deactivation operations of technical units are important examples in engineering where fatigue and especially low-cycle fatigue (LCF) play an essential role. A significant scatter in fatigue life for many materials results in the necessity of advanced probabilistic models for fatigue. Moreover, optimization of reliability is of vital interest in engineering, where with respect to fatigue the cost functionals are motivated by the predicted probability for the integrity of the component after a certain number of load cycles. The natural mathematical language to model failure, here understood as crack initiation, is the language of spatio-temporal point processes and their first failure times. This translates the problem of optimal reliability in the framework of shape optimization. The cost functionals derived in this way for realistic optimal reliability problems are too singular to be $H^1$-lower semi-continuous as many damage mechanisms, like LCF, lead to crack initiation as a function of the stress at the component's surface. Realistic crack formation models therefore impose a new challenge to the theory of shape optimization. In this work, we have to modify the existence proof of optimal shapes, for the case of sufficiently smooth shapes using elliptic regularity, uniform Schauder estimates and compactness of strong solutions via the Arzela-Ascoli theorem. This result applies to a variety of crack initiation models and in particular applies to a recent probabilistic model for LCF. 	
1306.3416v1	http://arxiv.org/pdf/1306.3416v1	2013	Percolation of a general network of networks	Jianxi Gao|Sergey V. Buldyrev|H. Eugene Stanley|Xiaoming Xu|Shlomo Havlin	  Percolation theory is an approach to study vulnerability of a system. We develop analytical framework and analyze percolation properties of a network composed of interdependent networks (NetONet). Typically, percolation of a single network shows that the damage in the network due to a failure is a continuous function of the fraction of failed nodes. In sharp contrast, in NetONet, due to the cascading failures, the percolation transition may be discontinuous and even a single node failure may lead to abrupt collapse of the system. We demonstrate our general framework for a NetONet composed of $n$ classic Erd\H{o}s-R\'{e}nyi (ER) networks, where each network depends on the same number $m$ of other networks, i.e., a random regular network of interdependent ER networks. In contrast to a \emph{treelike} NetONet in which the size of the largest connected cluster (mutual component) depends on $n$, the loops in the RR NetONet cause the largest connected cluster to depend only on $m$. We also analyzed the extremely vulnerable feedback condition of coupling. In the case of ER networks, the NetONet only exhibits two phases, a second order phase transition and collapse, and there is no first phase transition regime unlike the no feedback condition. In the case of NetONet composed of RR networks, there exists a first order phase transition when $q$ is large and second order phase transition when $q$ is small. Our results can help in designing robust interdependent systems. 	
1310.8388v1	http://arxiv.org/pdf/1310.8388v1	2013	Provable Security of Networks	Angsheng Li|Yicheng Pan|Wei Zhang	  We propose a definition of {\it security} and a definition of {\it robustness} of networks against the cascading failure models of deliberate attacks and random errors respectively, and investigate the principles of the security and robustness of networks. We propose a {\it security model} such that networks constructed by the model are provably secure against any attacks of small sizes under the cascading failure models, and simultaneously follow a power law, and have the small world property with a navigating algorithm of time complex $O(\log n)$. It is shown that for any network $G$ constructed from the security model, $G$ satisfies some remarkable topological properties, including: (i) the {\it small community phenomenon}, that is, $G$ is rich in communities of the form $X$ of size poly logarithmic in $\log n$ with conductance bounded by $O(\frac{1}{|X|^{\beta}})$ for some constant $\beta$, (ii) small diameter property, with diameter $O(\log n)$ allowing a navigation by a $O(\log n)$ time algorithm to find a path for arbitrarily given two nodes, and (iii) power law distribution, and satisfies some probabilistic and combinatorial principles, including the {\it degree priority theorem}, and {\it infection-inclusion theorem}. By using these principles, we show that a network $G$ constructed from the security model is secure for any attacks of small scales under both the uniform threshold and random threshold cascading failure models. Our security theorems show that networks constructed from the security model are provably secure against any attacks of small sizes, for which natural selections of {\it homophyly, randomness} and {\it preferential attachment} are the underlying mechanisms. 	
1503.03774v1	http://arxiv.org/pdf/1503.03774v1	2015	Record breaking bursts in a fiber bundle model of creep rupture	Zsuzsa Danku|Ferenc Kun	  We investigate the statistics of record breaking events in the time series of crackling bursts in a fiber bundle model of the creep rupture of heterogeneous materials. In the model fibers break due to two mechanisms: slowly accumulating damage triggers bursts of immediate breakings analogous to acoustic emissions in experiments. The rupture process accelerates such that the size of breaking avalanches increases while the waiting time between consecutive events decreases towards failure. Record events are defined as bursts which have a larger size than all previous events in the time series. We analyze the statistics of records focusing on the limit of equal load sharing (ELS) of the model and compare the results to the record statistics of sequences of independent identically distributed random variables. Computer simulations revealed that the number of records grows with the logarithm of the event number except for the close vicinity of macroscopic failure where an exponential dependence is evidenced. The two regimes can be attributed to the dominance of disorder with small burst sizes and to stress enhancements giving rise efficient triggering of extended bursts, respectively. Both the size of records and the increments between consecutive record events are characterized by power law distributions with a common exponent 1.33 significantly different from the usual ELS burst size exponents of fiber bundles. The distribution of waiting times follows the same behavior, however, with two distinct exponents for low and high loads. Studying the evolution of records we identify a load dependent characteristic scale of the system which separates slow down and acceleration of record breaking as failure is approached. 	
1601.06230v1	http://arxiv.org/pdf/1601.06230v1	2016	Coping with Prospective Memory Failures: An Optimal Reminder System   Design	Jinghua Hou	  Forgetting is in common in daily life, and 50-80% everyday's forgetting is due to prospective memory failures, which have significant impacts on our life. More seriously, some of these memory lapses can bring fatal consequences such as forgetting a sleeping infant in the back seat of a car. People tend to use various techniques to improve their prospective memory performance. Setting up a reminder is one of the most important techniques. The existing studies provide evidences in support of using reminders to cope with prospective memory failures. However, people are not satisfied with existing reminders because of their limitations in different aspects including reliability, optimization, and adaption.   Through analysing the functions and features of existing reminder systems, this book draft summarizes their advantages and limitations. We are motivated to improve the performance of reminder systems. For the improvements, the relevant theories and mechanisms of prospective memory from psychology must be complied with, incorporated, and applied in this new study.   Based on the literature review, a new reminder model is proposed, which includes a novel reminder planer, a prospective memory based agent, and a personalized user model. The reminder planer is responsible for determining the optimal reminder plan (including the optimal number of reminders, the optimal reminding schedule and the optimal reminding way). The prospective memory agent is responsible for executing the reminding processes. The personalized user model is proposed to learn from users' behaviors and preferences based on human-system interactions and is responsible for adapting the reminder plan to meet users' preferences as much as possible. 	
1601.06496v1	http://arxiv.org/pdf/1601.06496v1	2016	Lightweight Fault Tolerance in Large-Scale Distributed Graph Processing	Da Yan|James Cheng|Fan Yang	  The success of Google's Pregel framework in distributed graph processing has inspired a surging interest in developing Pregel-like platforms featuring a user-friendly "think like a vertex" programming model. Existing Pregel-like systems support a fault tolerance mechanism called checkpointing, which periodically saves computation states as checkpoints to HDFS, so that when a failure happens, computation rolls back to the latest checkpoint. However, a checkpoint in existing systems stores a huge amount of data, including vertex states, edges, and messages sent by vertices, which significantly degrades the failure-free performance. Moreover, the high checkpointing cost prevents frequent checkpointing, and thus recovery has to replay all the computations from a state checkpointed some time ago.   In this paper, we propose a novel checkpointing approach which only stores vertex states and incremental edge updates to HDFS as a lightweight checkpoint (LWCP), so that writing an LWCP is typically tens of times faster than writing a conventional checkpoint. To recover from the latest LWCP, messages are generated from the vertex states, and graph topology is recovered by replaying incremental edge updates. We show how to realize lightweight checkpointing with minor modifications of the vertex-centric programming interface. We also apply the same idea to a recently-proposed log-based approach for fast recovery, to make it work efficiently in practice by significantly reducing the cost of garbage collection of logs. Extensive experiments on large real graphs verified the effectiveness of LWCP in improving both failure-free performance and the performance of recovery. 	
1605.01994v2	http://arxiv.org/pdf/1605.01994v2	2016	Rolex: Resilience-Oriented Language Extensions for Extreme-Scale Systems	Saurabh Hukerikar|Robert F. Lucas	  Future exascale high-performance computing (HPC) systems will be constructed from VLSI devices that will be less reliable than those used today, and faults will become the norm, not the exception. This will pose significant problems for system designers and programmers, who for half-a-century have enjoyed an execution model that assumed correct behavior by the underlying computing system. The mean time to failure (MTTF) of the system scales inversely to the number of components in the system and therefore faults and resultant system level failures will increase, as systems scale in terms of the number of processor cores and memory modules used. However every error detected need not cause catastrophic failure. Many HPC applications are inherently fault resilient. Yet it is the application programmers who have this knowledge but lack mechanisms to convey it to the system.   In this paper, we present new Resilience Oriented Language Extensions (Rolex) which facilitate the incorporation of fault resilience as an intrinsic property of the application code. We describe the syntax and semantics of the language extensions as well as the implementation of the supporting compiler infrastructure and runtime system. Our experiments show that an approach that leverages the programmer's insight to reason about the context and significance of faults to the application outcome significantly improves the probability that an application runs to a successful conclusion. 	
1705.09829v4	http://arxiv.org/pdf/1705.09829v4	2017	Universality and scaling laws in the cascading failure model with   healing	Marcell Stippinger|János Kertész	  Cascading failures may lead to dramatic collapse in interdependent networks, where the breakdown takes place as a discontinuity of the order parameter. In the cascading failure (CF) model with healing there is a control parameter which at some value suppresses the discontinuity of the order parameter. However, up to this value of the healing parameter the breakdown is a hybrid transition, meaning that, besides this first order character, the transition shows scaling too. In this paper we investigate the question of universality related to the scaling behavior. Recently we showed that the hybrid phase transition in the original CF model has two sets of exponents describing respectively the order parameter and the cascade statistics, which are connected by a scaling law. In the CF model with healing we measure these exponents as a function of the healing parameter. We find two universality classes: In the wide range below the critical healing value the exponents agree with those of the original model, while above this value the model displays trivial scaling meaning that fluctuations follow the central limit theorem. 	
1710.07845v1	http://arxiv.org/pdf/1710.07845v1	2017	Seamless Paxos Coordinators	Gustavo M. D. Vieira|Islene C. Garcia|Luiz E. Buzato	  The Paxos algorithm requires a single correct coordinator process to operate. After a failure, the replacement of the coordinator may lead to a temporary unavailability of the application implemented atop Paxos. So far, this unavailability has been addressed by reducing the coordinator replacement rate through the use of stable coordinator selection algorithms. We have observed that the cost of recovery of the newly elected coordinator's state is at the core of this unavailability problem. In this paper we present a new technique to manage coordinator replacement that allows the recovery to occur concurrently with new consensus rounds. Experimental results show that our seamless approach effectively solves the temporary unavailability problem, its adoption entails uninterrupted execution of the application. Our solution removes the restriction that the occurrence of coordinator replacements is something to be avoided, allowing the decoupling of the application execution from the accuracy of the mechanism used to choose a coordinator. This result increases the performance of the application even in the presence of failures, it is of special importance to the autonomous operation of replicated applications that have to adapt to varying network conditions and partial failures. 	
1801.04668v1	http://arxiv.org/pdf/1801.04668v1	2018	The decoding failure probability of MDPC codes	Jean-Pierre Tillich	  Moderate Density Parity Check (MDPC) codes are defined here as codes which have a parity-check matrix whose row weight is $O(\sqrt{n})$ where $n$ is the length $n$ of the code. They can be decoded like LDPC codes but they decode much less errors than LDPC codes: the number of errors they can decode in this case is of order $\Theta(\sqrt{n})$. Despite this fact they have been proved very useful in cryptography for devising key exchange mechanisms. They have also been proposed in McEliece type cryptosystems. However in this case, the parameters that have been proposed in \cite{MTSB13} were broken in \cite{GJS16}. This attack exploits the fact that the decoding failure probability is non-negligible. We show here that this attack can be thwarted by choosing the parameters in a more conservative way. We first show that such codes can decode with a simple bit-flipping decoder any pattern of $O\left(\frac{\sqrt{n} \log \log n}{\log n}\right)$ errors. This avoids the previous attack at the cost of significantly increasing the key size of the scheme. We then show that under a very reasonable assumption the decoding failure probability decays almost exponentially with the codelength with just two iterations of bit-flipping. With an additional assumption it has even been proved that it decays exponentially with an unbounded number of iterations and we show that in this case the increase of the key size which is required for resisting to the attack of \cite{GJS16} is only moderate. 	
1406.1052v2	http://arxiv.org/pdf/1406.1052v2	2014	A Conceptual Approach to Two-Scale Constitutive Modelling For   Hydro-Mechanical Coupling	Giang D. Nguyen|Abbas El-Zein|Terry Bennett	  Large scale modelling of fluid flow coupled with solid failure in geothermal reservoirs or hydrocarbon extraction from reservoir rocks usually involves behaviours at two scales: lower scale of the inelastic localization zone, and larger scale of the bulk continuum where elastic behaviour can be reasonably assumed. The hydraulic conductivities corresponding to the mechanical properties at these two scales are different. In the bulk elastic host rock, the hydraulic conductivity does not vary much with the deformation, while it significantly changes in the lower scale of the localization zone due to inelastic deformation. Increase of permeability due to fracture and/or dilation, or reduction of permeability due to material compaction can take place inside this zone. The challenge is to predict the evolution of hydraulic conductivities coupled with the mechanical behaviour of the material in all stages of the deformation process. In the early stage of diffuse deformation, the permeability of the material can be reasonably assumed to be homogenous over the whole Representative Volume Element (RVE) However, localized failure results in distinctly different conductivities in different parts of the RVE. This paper establishes a general framework and corresponding field equations to describe the hydro-mechanical coupling in both diffuse and localized stages of deformation in rocks. In particular, embedding the lower scale hydro-mechanical behaviour of the localization zone inside an elastic bulk, together with their corresponding effective sizes, helps effectively deal with scaling issues in large-scale modelling. Preliminary results are presented which demonstrate the promising features of this new approach. 	
1406.1701v1	http://arxiv.org/pdf/1406.1701v1	2014	A computational study of the effects of remodelled electrophysiology and   mechanics on initiation of ventricular fibrillation in human heart failure	Nathan Kirk|Alan Benson|Christopher Goodyer|Matthew Hubbard	  The study of pathological cardiac conditions such as arrhythmias, a major cause of mortality in heart failure, is becoming increasingly informed by computational simulation, numerically modelling the governing equations. This can provide insight where experimental work is constrained by technical limitations and/or ethical issues.   As the models become more realistic, the construction of efficient and accurate computational models becomes increasingly challenging. In particular, recent developments have started to couple the electrophysiology models with mechanical models in order to investigate the effect of tissue deformation on arrhythmogenesis, thus introducing an element of nonlinearity into the mathematical representation. This paper outlines a biophysically-detailed computational model of coupled electromechanical cardiac activity which uses the finite element method to approximate both electrical and mechanical systems on unstructured, deforming, meshes. An ILU preconditioner is applied to improve performance of the solver.   This software is used to examine the role of electrophysiology, fibrosis and mechanical deformation on the stability of spiral wave dynamics in human ventricular tissue by applying it to models of both healthy and failing tissue. The latter was simulated by modifying (i) cellular electrophysiological properties, to generate an increased action potential duration and altered intracellular calcium handling, and (ii) tissue-level properties, to simulate the gap junction remodelling, fibrosis and increased tissue stiffness seen in heart failure. The resulting numerical experiments suggest that, for the chosen mathematical models of electrophysiology and mechanical response, introducing tissue level fibrosis can have a destabilising effect on the dynamics, while the net effect of the electrophysiological remodelling stabilises the system. 	
0205130v1	http://arxiv.org/pdf/cond-mat/0205130v1	2002	Chemical fracture and distribution of extreme values	A. Baldassarri|A. Gabrielli|B. Sapoval	  When a corrosive solution reaches the limits of a solid sample, a chemical fracture occurs. An analytical theory for the probability of this chemical fracture is proposed and confirmed by extensive numerical experiments on a two dimensional model. This theory follows from the general probability theory of extreme events given by Gumbel. The analytic law differs from the Weibull law commonly used to describe mechanical failures for brittle materials. However a three parameters fit with the Weibull law gives good results, confirming the empirical value of this kind of analysis. 	
0412195v1	http://arxiv.org/pdf/quant-ph/0412195v1	2004	Complementarity and Scientific Rationality	Simon Saunders	  Bohr's interpretation of quantum mechanics has been criticized as incoherent and opportunistic, and based on doubtful philosophical premises. If so Bohr's influence, in the pre-war period of 1927-1939, is the harder to explain, and the acceptance of his approach to quantum mechanics over de Broglie's had no reasonable foundation. But Bohr's interpretation changed little from the time of its first appearance, and stood independent of any philosophical presuppositions. The principle of complementarity is itself best read as a conjecture of unusually wide scope, on the nature and future course of explanations in the sciences (and not only the physical sciences). If it must be judged a failure today, it is not because of any internal inconsistency. 	
0708.3298v2	http://arxiv.org/pdf/0708.3298v2	2007	Collapse times for attractive Bose-Einstein condensates	Esteban Calzetta	  We argue that the main mechanism for condensate collapse in an attractive Bose-Einstein condensate is the loss of coherence between atoms a finite distance apart, rather than the growth of the occupation number in noncondensate modes. Since the former mechanism is faster than the latter by a factor of approximately 3/2, this helps to dispel the apparent failure of field theoretical models in predicting the collapse time of the condensate. 	
0902.1424v1	http://arxiv.org/pdf/0902.1424v1	2009	Non-monotonic dependence of the rupture force in polymer chains on their   lengths	S. Fugmann|I. M. Sokolov	  We consider the rupture dynamics of a homopolymer chain pulled at one end at a constant loading rate. Our model of the breakable polymer is related to the Rouse chain, with the only difference that the interaction between the monomers is described by the Morse potential instead of the harmonic one, and thus allows for mechanical failure. We show that in the experimentally relevant domain of parameters the dependence of the most probable rupture force on the chain length may be non-monotonic, so that the medium-length chains break easier than the short and the long ones. The qualitative theory of the effect is presented. 	
0904.3746v2	http://arxiv.org/pdf/0904.3746v2	2009	Breakdown of thermalization in finite one-dimensional systems	Marcos Rigol	  We use quantum quenches to study the dynamics and thermalization of hardcore bosons in finite one-dimensional lattices. We perform exact diagonalizations and find that, far away from integrability, few-body observables thermalize. We then study the breakdown of thermalization as one approaches an integrable point. This is found to be a smooth process in which the predictions of standard statistical mechanics continuously worsen as the system moves toward integrability. We establish a direct connection between the presence or absence of thermalization and the validity or failure of the eigenstate thermalization hypothesis, respectively. 	
0906.3244v1	http://arxiv.org/pdf/0906.3244v1	2009	Mechanical, Electrical, and Magnetic Properties of Ni Nanocontacts	M. R. Calvo|M. J. Caturla|D. Jacob|C. Untiedt|J. J. Palacios	  The dynamic deformation upon stretching of Ni nanowires as those formed with mechanically controllable break junctions or with a scanning tunneling microscope is studied both experimentally and theoretically. Molecular dynamics simulations of the breaking process are performed. In addition, and in order to compare with experiments, we also compute the transport properties in the last stages before failure using the first-principles implementation of Landauer's formalism included in our transport package ALACANT. 	
1106.0689v3	http://arxiv.org/pdf/1106.0689v3	2012	Mechanical properties of carbynes investigated by ab initio total-energy   calculations	Ivano E. Castelli|Paolo Salvestrini|Nicola Manini	  As sp carbon chains (carbynes) are relatively rigid molecular objects, can we exploit them as construction elements in nanomechanics? To answer this question, we investigate their remarkable mechanical properties by ab-initio total-energy simulations. In particular, we evaluate their linear response to small longitudinal and bending deformations and their failure limits for longitudinal compression and elongation. 	
1302.5418v1	http://arxiv.org/pdf/1302.5418v1	2013	Failure of the Bell Locality Condition over a Space of Ideal Particles   and their Paths	Warren Leffler	  We construct a space of ideal elements (particles and their paths) to analyze certain aspects of quantum physics. The particles are taken from a model of particle interaction first described by David Deutsch (based on a different but related framework, that of MWI), and the paths are based on Richard Feynman's path-integral formulation of quantum mechanics. By combining the two systems we develop a new approach to quantum mechanics that eliminates various quantum paradoxes. 	
1310.3762v1	http://arxiv.org/pdf/1310.3762v1	2013	Vortex distribution in a confining potential	Yan Levin|Matheus Girotto|Alexandre Pereira dos Santos	  We study a model of interacting vortices in a type II superconductor. In the weak coupling limit, we constructed a mean-field theory which allows us to accurately calculate the vortex density distribution inside a confining potential. In the strong coupling limit, the correlations between the particles become important and the mean-field theory fails. Contrary to recent suggestions, this does not imply failure of the Boltzmann-Gibbs statistical mechanics, as we clearly demonstrate by comparing the results of Molecular Dynamics and Monte Carlo simulations. 	
1311.7190v2	http://arxiv.org/pdf/1311.7190v2	2014	Disintegration of graphene nanoribbons in large electrostatic fields	Haiming Huang|Zhibing Li|H. J. Kreuzer|Weiliang Wang	  The deformation and disintegration of a graphene nanoribbon under external electrostatic fields are investigated by first principle quantum mechanical calculations to establish its stability range. The zigzag edges terminated by various functional groups are considered. By analyzing the phonon spectrum, the critical fracture field for each edge structure is obtained. It is found that different terminal groups on the zigzag graphene nanoribbons lead to different fracture patterns at different fracture fields. The failure mechanism is demonstrated to rely on both the carbon bond alternation feature across the ribbon and terminal group electronegativity. 	
1404.0073v1	http://arxiv.org/pdf/1404.0073v1	2014	General dynamic recovery for compensating CSP	Abeer S. Al-Humaimeedy|Maribel Fernández	  Compensation is a technique to roll-back a system to a consistent state in case of failure. Recovery mechanisms for compensating calculi specify the order of execution of compensation sequences. Dynamic recovery means that the order of execution is determined at runtime. In this paper, we define an extension of Compensating CSP, called DEcCSP, with general dynamic recovery. We provide a formal, operational semantics for the calculus, and illustrate its expressive power with a case study. In contrast with previous versions of Compensating CSP, DEcCSP provides mechanisms to replace or discard compensations at runtime. Additionally, we bring back to DEcCSP standard CSP operators that are not available in other compensating CSP calculi, and introduce channel communication. 	
1409.3682v1	http://arxiv.org/pdf/1409.3682v1	2014	A novel recovery mechanism enabling fine-granularity locking and fast,   REDO-only recovery	Caetano Sauer|Theo Härder	  We present a series of novel techniques and algorithms for transaction commit, logging, recovery, and propagation control. In combination, they provide a recovery component that maintains the persistent state of the database (both log and data pages) always in a committed state. Recovery from system and media failures only requires only REDO operations, which can happen concurrently with the processing of new transactions. The mechanism supports fine-granularity locking, partial rollbacks, and snapshot isolation for reader transactions. Our design does not assume a specific hardware configuration such as non-volatile RAM or flash---it is designed for traditional disk environments. Nevertheless, it can exploit modern I/O devices for higher transaction throughput and reduced recovery time with a high degree of flexibility. 	
1604.02526v1	http://arxiv.org/pdf/1604.02526v1	2016	Practical Recovery Solution for Information Loss in Real-Time Network   Environment	Hengky Susanto|ByungGuk Kim	  Feedback mechanism based algorithms are frequently used to solve network optimization problems. These schemes involve users and network exchanging information (e.g. requests for bandwidth allocation and pricing) to achieve convergence towards an optimal solution. However, in the implementation, these algorithms do not guarantee that messages will be delivered to the destination when network congestion occurs. This in turn often results in packet drops, which may cause information loss, and this condition may lead to algorithm failing to converge. To prevent this failure, we propose least square (LS) estimation algorithm to recover the missing information when packets are dropped from the network. The simulation results involving several scenarios demonstrate that LS estimation can provide the convergence for feedback mechanism based algorithm. 	
1604.03811v1	http://arxiv.org/pdf/1604.03811v1	2016	Microstructural modeling of ductile fracture initiation in multi-phase   materials	T. W. J. de Geus|R. H. J. Peerlings|M. G. D. Geers	  The precise mechanisms underlying the failure of multi-phase materials may be strongly dependent on the material's microstructural morphology. Micromechanical modeling has provided much insight into this dependence, but uncertainties remain about crucial modeling assumptions. This paper assesses the influence of different grain shapes, damage indicators, and stress states using a structured numerical model. A distinct spatial arrangement of phases around fracture incidents is found, consisting of hard regions in the tensile direction interrupted by soft regions in the directions of shear. These key features are only mildly sensitive to the studied variations. 	
1802.03460v1	http://arxiv.org/pdf/1802.03460v1	2018	Electrochemical and mechanical behaviors of dissimilar friction stir   welding between 5086 and 6061 aluminum alloy	Zhitong Chen|Shengxi Li|Lloyd H. Hihara	  The electrochemical behavior and mechanical properties of friction stir welded AA5086 and AA6061 Al alloys were investigated. Micro-hardness measurements and tensile tests showed that the heat-affected zone (HAZ) in AA6061 had minimum hardness value (i.e., 88 HV) and served as failure site in the dissimilar weld. Corrosion testing revealed that the minimum value of Icorr appeared in the HAZ 5086 (0.54 uA/cm2) and HAZ 5086 was most resistant to corrosion. The AA 5086 side of the weld showed better corrosion resistance than the AA 6061 side. 	
1312.1993v4	http://arxiv.org/pdf/1312.1993v4	2014	Enhancing resilience of interdependent networks by healing	Marcell Stippinger|János Kertész	  Interdependent networks are characterized by two kinds of interactions: The usual connectivity links within each network and the dependency links coupling nodes of different networks. Due to the latter links such networks are known to suffer from cascading failures and catastrophic breakdowns. When modeling these phenomena, usually one assumes that a fraction of nodes gets damaged in one of the networks, which is followed possibly by a cascade of failures. In real life the initiating failures do not occur at once and effort is made replace the ties eliminated due to the failing nodes. Here we study a dynamic extension of the model of interdependent networks and introduce the possibility of link formation with a probability w, called healing, to bridge non-functioning nodes and enhance network resilience. A single random node is removed, which may initiate an avalanche. After each removal step healing sets in resulting in a new topology. Then a new node fails and the process continues until the giant component disappears either in a catastrophic breakdown or in a smooth transition. Simulation results are presented for square lattices as starting networks under random attacks of constant intensity. We find that the shift in the position of the breakdown has a power-law scaling as a function of the healing probability with an exponent close to 1. Below a critical healing probability, catastrophic cascades form and the average degree of surviving nodes decreases monotonically, while above this value there are no macroscopic cascades and the average degree has first an increasing character and decreases only at the very late stage of the process. These findings facilitate to plan intervention in case of crisis situation by describing the efficiency of healing efforts needed to suppress cascading failures. 	
1605.01784v1	http://arxiv.org/pdf/1605.01784v1	2016	Neural mechanisms underlying catastrophic failure in human-machine   interaction during aerial navigation	Sameer Saproo|Victor Shih|David C. Jangraw|Paul Sajda	  Objective. We investigated the neural correlates of workload buildup in a fine visuomotor task called the boundary avoidance task (BAT). The BAT has been known to induce naturally occurring failures of human-machine coupling in high performance aircraft that can potentially lead to a crash; these failures are termed pilot induced oscillations (PIOs). Approach. We recorded EEG and pupillometry data from human subjects engaged in a flight BAT simulated within a virtual 3D environment. Main results. We find that workload buildup in a BAT can be successfully decoded from oscillatory features in the electroencephalogram (EEG). Information in delta, theta, alpha, beta, and gamma spectral bands of the EEG all contribute to successful decoding, however gamma band activity with a lateralized somatosensory topography has the highest contribution, while theta band activity with a frontocentral topography has the most robust contribution in terms of real world usability. We show that the output of the spectral decoder can be used to predict PIO susceptibility. We also find that workload buildup in the task induces pupil dilation, the magnitude of which is significantly correlated with the magnitude of the decoded EEG signals. These results suggest that PIOs may result from the dysregulation of cortical networks such as the locus coeruleus (LC) anterior cingulate cortex (ACC) circuit. Significance. Our findings may generalize to similar control failures in other cases of tight man machine coupling where gains and latencies in the control system must be inferred and compensated for by the human operators. A closed-loop intervention using neurophysiological decoding of workload buildup that targets the LC ACC circuit may positively impact operator performance in such situations. 	
0704.0879v1	http://arxiv.org/pdf/0704.0879v1	2007	A Hierarchical Approach for Dependability Analysis of a Commercial   Cache-Based RAID Storage Architecture	Mohamed Kaaniche|Luigi Romano|Zbigniew Kalbarczyk|Ravishankar Iyer|Rick Karcich	  We present a hierarchical simulation approach for the dependability analysis and evaluation of a highly available commercial cache-based RAID storage system. The archi-tecture is complex and includes several layers of overlap-ping error detection and recovery mechanisms. Three ab-straction levels have been developed to model the cache architecture, cache operations, and error detection and recovery mechanism. The impact of faults and errors oc-curring in the cache and in the disks is analyzed at each level of the hierarchy. A simulation submodel is associated with each abstraction level. The models have been devel-oped using DEPEND, a simulation-based environment for system-level dependability analysis, which provides facili-ties to inject faults into a functional behavior model, to simulate error detection and recovery mechanisms, and to evaluate quantitative measures. Several fault models are defined for each submodel to simulate cache component failures, disk failures, transmission errors, and data errors in the cache memory and in the disks. Some of the parame-ters characterizing fault injection in a given submodel cor-respond to probabilities evaluated from the simulation of the lower-level submodel. Based on the proposed method-ology, we evaluate and analyze 1) the system behavior un-der a real workload and high error rate (focusing on error bursts), 2) the coverage of the error detection mechanisms implemented in the system and the error latency distribu-tions, and 3) the accumulation of errors in the cache and in the disks. 	
1111.4924v1	http://arxiv.org/pdf/1111.4924v1	2011	The precursory electric signals, observed before the Izmit Turkey EQ (Mw   = 7.6, August 17th, 1999), analyzed in terms of a hypothetically   pre-activated, in the focal area, large scale piezoelectric mechanism	C. Thanassoulas|V. Klentos	  The generated, prior to the Izmit Turkey large EQ, preseismic electric signals were recorded in Greece by the VOL Earth's electric field monitoring site. In order to explain their peculiar character and their generating mechanism, a large scale piezoelectric mechanism was assumed that was initiated in the Izmit seismogenic region long before the EQ occurrence time. The theoretical analysis of the adopted physical model justifies the generation of a number of specific electric signals that can be emitted from the focal area before the rock formation failure. The processing of the registered by the VOL monitoring site raw data revealed the presence of similar signals as the expected theoretical ones. Therefore, it is concluded that long before the Izmit EQ occurrence a large scale piezoelectric mechanism was initiated that was modulated too by the tidally triggered lithospheric oscillation and therefore generated the observed preseismic electric signals. The adopted piezoelectric model provides critical information about the time of occurrence of the seismogenic area rock formation failure and therefore the possibility for a real short-term time prediction of a large EQ. The other two predictive EQ parameters, location and magnitude, are discussed in the frame of electric field triangulation and the Lithospheric Seismic Energy Flow Model (LSEFM). 	
1704.05285v1	http://arxiv.org/pdf/1704.05285v1	2017	Mechanical Failure in Amorphous Solids: Scale Free Spinodal Criticality	Itamar Procaccia|Corrado Rainone|Murari Singh	  The mechanical failure of amorphous media is a ubiquitous phenomenon from material engineering to geology. It has been noticed for a long time that the phenomenon is "scale-free", indicating some type of criticality. In spite of attempts to invoke "Self-Organized Criticality", the physical origin of this criticality, and also its universal nature, being quite insensitive to the nature of microscopic interactions, remained elusive. Recently we proposed that the precise nature of this critical behavior is manifested by a spinodal point of a thermodynamic phase transition. Moreover, at the spinodal point there exists a divergent correlation length which is associated with the system-spanning instabilities (known also as shear bands) which are typical to the mechanical yield. Demonstrating this requires the introduction of an "order parameter" that is suitable for distinguishing between disordered amorphous systems, and an associated correlation function, suitable for picking up the growing correlation length. The theory, the order parameter, and the correlation functions used are universal in nature and can be applied to any amorphous solid that undergoes mechanical yield. Critical exponents for the correlation length divergence and the system size dependence are estimated. The phenomenon is seen at its sharpest in athermal systems, as is explained below; in this paper we extend the discussion also to thermal systems, showing that at sufficiently high temperatures the spinodal phenomenon is destroyed by thermal fluctuations. 	
0407096v1	http://arxiv.org/pdf/quant-ph/0407096v1	2004	The Emergence of Classical Dynamics in a Quantum World	Tanmoy Bhattacharya|Salman Habib|Kurt Jacobs	  Ever since the advent of quantum mechanics, it has been clear that the atoms composing matter do not obey Newton's laws. Instead, their behavior is described by the Schroedinger equation. Surprisingly though, until recently, no clear explanation was given for why everyday objects, which are merely collections of atoms, are observed to obey Newton's laws. It would seem that, if quantum mechanics explains all the properties of atoms accurately, they, too, should obey quantum mechanics. This reasoning led some scientists to believe in a distinct macroscopic, or ``big and complicated,'' world in which quantum mechanics fails and classical mechanics takes over, although there has never been experimental evidence for such a failure. Even those who insisted that Newtonian mechanics would somehow emerge from the underlying quantum mechanics as the system became increasingly macroscopic were hindered by the lack of adequate experimental and theoretical tools. In the last decade, however, this quantum-to-classical transition has become accessible to experimental study and quantitative description, and the resulting insights are the subject of this article. 	
0501176v1	http://arxiv.org/pdf/astro-ph/0501176v1	2005	Supernova Ia without Accelerated Expansion The First Global Failure of   Relativity Theory	Charles B. Leffert	  A new cosmological model has been developed that shows great promise for solving many of the present problems of physics. A new concept of space and its production, spatial condensation (SC) is the cause of the expansion. Dark mass (not matter) scales with the expansion differently than matter. Many other non-relativistic concepts predict a simple beginning, absence of singularities, a definition of energy and the cause of space curvature and gravity. Predicted cosmological parameters agree with recent measurements including t0=13.5 Gy, H0=68.6 km/(s Mpc), Omega (mass)=0.28 and no dark energy. Other predictions include: Hubble flow at the Planck level, vacuum energy (no mass), Evac/Emass=10^123 in agreement with quantum mechanics, and the pattern in the CMB is the distribution of very early dark mass black holes of average mass 10^8 Msun. Excellent agreement with supernova Ia data is obtained with no acceleration of the expansion rate. It is concluded that the SC-model announces the first global failure of relativity theory. 	
0509514v1	http://arxiv.org/pdf/astro-ph/0509514v1	2005	Low Mass X-ray Binaries and Metallicity Dependence: Story of Failures	Natalia Ivanova	  Observations of galactic and extra-galactic globular clusters have shown that on average metal-rich clusters are ~3 times as likely to contain a bright X-ray source than their metal-poor counterparts. We propose that this can be explained by taking into account the difference in the stellar structure of main sequence donors with masses between ~0.85 Msun and ~1.25 Msun at different metallicities. Metal-poor main sequence stars in this mass range do not have an outer convective zone while metal-rich stars do. The absence of this zone turns off magnetic braking, a powerful mechanism of orbital shrinkage, leading to the failure of dynamically formed main sequence - neutron star binaries to start mass transfer or appear as bright low-mass X-ray binaries. 	
0008064v1	http://arxiv.org/pdf/cond-mat/0008064v1	2000	Error and attack tolerance of complex networks	Reka Albert|Hawoong Jeong|Albert-Laszlo Barabasi	  Many complex systems, such as communication networks, display a surprising degree of robustness: while key components regularly malfunction, local failures rarely lead to the loss of the global information-carrying ability of the network. The stability of these complex systems is often attributed to the redundant wiring of the functional web defined by the systems' components. In this paper we demonstrate that error tolerance is not shared by all redundant systems, but it is displayed only by a class of inhomogeneously wired networks, called scale-free networks. We find that scale-free networks, describing a number of systems, such as the World Wide Web, Internet, social networks or a cell, display an unexpected degree of robustness, the ability of their nodes to communicate being unaffected by even unrealistically high failure rates. However, error tolerance comes at a high price: these networks are extremely vulnerable to attacks, i.e. to the selection and removal of a few nodes that play the most important role in assuring the network's connectivity. 	
0008171v2	http://arxiv.org/pdf/cond-mat/0008171v2	2000	Scaling behaviour in the fracture of fibrous materials	I. L. Menezes-Sobrinho|J. G. Moreira|A. T. Bernardes	  We study the existence of distinct failure regimes in a model for fracture in fibrous materials. We simulate a bundle of parallel fibers under uniaxial static load and observe two different failure regimes: a catastrophic and a slowly shredding. In the catastrophic regime the initial deformation produces a crack which percolates through the bundle. In the slowly shredding regime the initial deformations will produce small cracks which gradually weaken the bundle. The boundary between the catastrophic and the shredding regimes is studied by means of percolation theory and of finite-size scaling theory. In this boundary, the percolation density $\rho$ scales with the system size $L$, which implies the existence of a second-order phase transition with the same critical exponents as those of usual percolation. 	
0104080v1	http://arxiv.org/pdf/cond-mat/0104080v1	2001	Criticality in a model of banking crises	Giulia Iori|Saqib Jafarey	  An interbank market lets participants pool the risk arising from the combination of illiquid investments and random withdrawals by depositors. But it also creates the potential for one bank's failure to trigger off avalanches of further failures. We simulate a model of interbank lending to study the interplay of these two effects. We show that when banks are similar in size and exposure to risk, avalanche effects are small so that widening the interbank market leads to more stability. But as heterogeneity increases, avalanche effects become more important. By varying the heterogeneity and connectivity across banks, the system enters a critical regime with a power law distribution of avalanche sizes. 	
0106012v2	http://arxiv.org/pdf/cond-mat/0106012v2	2002	The Random Fuse Network as a Dipolar Magnet	Marc Barthelemy|Rava da Silveira|Henri Orland	  We introduce an approximate mapping between the random fuse network (RFN) and a random field dipolar Ising model (RFDIM). The state of the network damage is associated with a metastable spin configuration. A mean-field treatment, numerical solutions, and heuristic arguments support the broad validity of the approximation and yield a generic phase diagram. At low disorder, the growth of a single unstable `crack' leads to an abrupt global failure. Beyond a critical disorder, the conducting network sustains significant damage before the coalescence of cracks results in global failure. 	
0312138v2	http://arxiv.org/pdf/cond-mat/0312138v2	2005	Power Laws, Precursors and Predictability During Failure	Rumi De|G. Ananthakrishna	  We investigate the dynamics of a modified Burridge-Knopoff model by introducing a dissipative term to mimic the bursts of acoustic emission (AE) from rock samples. The model explains many features of the statistics of AE signals observed in experiments such as the crossover in the exponent value from relatively small amplitude AE signals to larger regime, and their dependence on the pulling speed. Significantly, we find that the cumulative energy dissipated identified with acoustic emission can be used to predict a major slip event. We also find a data collapse of the acoustic activity for several major slip events describable by a universal stretched exponential with corrections in terms of time-to-failure. 	
0404331v1	http://arxiv.org/pdf/cond-mat/0404331v1	2004	Optimization of Robustness of Complex Networks	G. Paul|T. Tanizawa|S. Havlin|H. E. Stanley	  Networks with a given degree distribution may be very resilient to one type of failure or attack but not to another. The goal of this work is to determine network design guidelines which maximize the robustness of networks to both random failure and intentional attack while keeping the cost of the network (which we take to be the average number of links per node) constant. We find optimal parameters for: (i) scale free networks having degree distributions with a single power-law regime, (ii) networks having degree distributions with two power-law regimes, and (iii) networks described by degree distributions containing two peaks. Of these various kinds of distributions we find that the optimal network design is one in which all but one of the nodes have the same degree, $k_1$ (close to the average number of links per node), and one node is of very large degree, $k_2 \sim N^{2/3}$, where $N$ is the number of nodes in the network. 	
0406450v3	http://arxiv.org/pdf/cond-mat/0406450v3	2005	Failure properties of loaded fiber bundles having a lower cutoff in   fiber threshold distribution	Srutarshi Pradhan|Alex Hansen	  Presence of lower cutoff in fiber threshold distribution may affect the failure properties of a bundle of fibers subjected to external load. We investigate this possibility both in a equal load sharing (ELS) fiber bundle model and in local load sharing (LLS) one. We show analytically that in ELS model, the critical strength gets modified due to the presence of lower cutoff and it becomes bounded by an upper limit. Although the dynamic exponents for the susceptibility and relaxation time remain unchanged, the avalanche size distribution shows a permanent deviation from the mean-fiels power law. In the LLS model, we analytically estimate the upper limit of the lower cutoff above which the bundle fails at one instant. Also the system size variation of bundle's strength and the avalanche statistics show strong dependence on the lower cutoff level. 	
0408580v2	http://arxiv.org/pdf/cond-mat/0408580v2	2005	Universality Class of Fiber Bundle Model on Complex Networks	Dong-Hee Kim|Beom Jun Kim|Hawoong Jeong	  We investigate the failure characteristics of complex networks within the framework of the fiber bundle model subject to the local load sharing rule in which the load of the broken fiber is transferred only to its neighbor fibers. Although the load sharing is strictly local, it is found that the critical behavior belongs to the universality class of the global load sharing where the load is transferred equally to all fibers in the system. From the numerical simulations and the analytical approach applied to the microscopic behavior, it is revealed that the emergence of a single dominant hub cluster of broken fibers causes the global load sharing effect in the failure process. 	
0411529v2	http://arxiv.org/pdf/cond-mat/0411529v2	2005	Regeneration of Stochastic Processes: An Inverse Method	J. Peinke|M. Reza Rahimi Tabar|Muhammad Sahimi|F. Ghasemi	  We propose a novel inverse method that utilizes a set of data to construct a simple equation that governs the stochastic process for which the data have been measured, hence enabling us to reconstruct the stochastic process. As an example, we analyze the stochasticity in the beat-to-beat fluctuations in the heart rates of healthy subjects as well as those with congestive heart failure. The inverse method provides a novel technique for distinguishing the two classes of subjects in terms of a drift and a diffusion coefficients which behave completely differently for the two classes of subjects, hence potentially providing a novel diagnostic tool for distinguishing healthy subjects from those with congestive heart failure, even at the early stages of the disease development. 	
0503473v1	http://arxiv.org/pdf/cond-mat/0503473v1	2005	Breakdown of Heterogeneous Materials	Purusattam Ray	  We discuss the threshold activated extremal dynamics that is prevalent in the breakdown processes in heterogeneous materials. We model such systems by an elastic spring network with random breaking thresholds assigned to the springs. Results are obtained from molecular dynamics simulation of the system under constant stress and constant strain conditions. We find that the distribution $P(m)$ of the avalanches of size $m$, caused by the rupturing of the springs till the failure of the network, decays as a power-law: $P(m) \sim m^{-\alpha}$, where $\alpha$ can be closely approximated to 5/2. The average avalanche size $<m>$ diverges as $<m> \sim (F_c - F)^{-1/2}$ close to the stress $F_c$ at which the total failure of the network occurs. We study the time evolution of the breakdown process: we find that the bonds rupture randomly over the network at initial times but the rupturing becomes highly correlated at late times to give rise to a well-defined macroscopic crack. 	
0508682v2	http://arxiv.org/pdf/cond-mat/0508682v2	2005	Avalanche dynamics driven by adaptive rewirings in complex networks	K. Rho|S. R. Hong|B. Kahng	  We introduce a toy model displaying the avalanche dynamics of failure in scale-free networks. In the model, the network growth is based on the Barab\'asi and Albert model and each node is assigned a capacity or tolerance, which is constant irrespective of node index. The degree of each node increases over time. When the degree of a node exceeds its capacity, it fails and each link connected to it is is rewired to other unconnected nodes by following the preferential attachment rule. Such a rewiring edge may trigger another failure. This dynamic process can occur successively, and it exhibits a self-organized critical behavior in which the avalanche size distribution follows a power law. The associated exponent is $\tau \approx 2.6(1)$. The entire system breaks down when any rewired edges cannot locate target nodes: the time at which this occurs is referred to as the breaking time. We obtain the breaking time as a function of the capacity. Moreover, using extreme value statistics, we determine the distribution function of the breaking time. 	
0602371v2	http://arxiv.org/pdf/cond-mat/0602371v2	2006	Rupture processes in fiber bundle models	Per C. Hemmer|Alex Hansen|Srutarshi Pradhan	  Fiber bundles with statistically distributed thresholds for breakdown of individual fibers are interesting models of the static and dynamics of failures in materials under stress. They can be analyzed to an extent that is not possible for more complex materials. During the rupture process in a fiber bundle avalanches, in which several fibers fail simultaneously, occur. We study by analytic and numerical methods the statistics of such avalanches, and the breakdown process for several models of fiber bundles. The models differ primarily in the way the extra stress caused by a fiber failure is redistributed among the surviving fibers. 	
0607305v1	http://arxiv.org/pdf/cond-mat/0607305v1	2006	Local load sharing fiber bundles with a lower cutoff of strength   disorder	Frank Raischel|Ferenc Kun|Hans J. Herrmann	  We study the failure properties of fiber bundles with a finite lower cutoff of the strength disorder varying the range of interaction between the limiting cases of completely global and completely local load sharing. Computer simulations revealed that at any range of load redistribution there exists a critical cutoff strength where the macroscopic response of the bundle becomes perfectly brittle, i.e. linearly elastic behavior is obtained up to global failure, which occurs catastrophically after the breaking of a small number of fibers. As an extension of recent mean field studies [Phys. Rev. Lett. 95, 125501 (2005)], we demonstrate that approaching the critical cutoff, the size distribution of bursts of breaking fibers shows a crossover to a universal power law form with an exponent 3/2 independent of the range of interaction. 	
0701237v1	http://arxiv.org/pdf/cond-mat/0701237v1	2007	Failure avalanches in fiber bundles for discrete load increase	Per C. Hemmer|Srutarshi Pradhan	  The statistics of burst avalanche sizes $n$ during failure processes in a fiber bundle follows a power law, $D(n)\sim n^{-\xi}$, for large avalanches. The exponent $\xi$ depends upon how the avalanches are provoked. While it is known that when the load on the bundle is increased in a continuous manner, the exponent takes the value $\xi=5/2$, we show that when the external load is increased in discrete and not too small steps, the exponent value $\xi=3$ is relevant. Our analytic treatment applies to bundles with a general probability distribution of the breakdown thresholds for the individual fibers. The pre-asymptotic size distribution of avalanches is also considered. 	
0009049v1	http://arxiv.org/pdf/gr-qc/0009049v1	2000	Does the third law of black hole thermodynamics really have a serious   failure?	Istvan Racz	  The almost perfect correspondence between certain laws of classical black hole mechanics and the ordinary laws of thermodynamics is spoiled by the failure of the conventional back hole analogue of the third law. Our aim here is to contribute to the associated discussion by flashing light on some simple facts of black hole physics. However, no attempt is made to lay to rest the corresponding long lasting debate. Instead, merely some evidence is provided to make it clear that although the borderline between extremal and non-extremal black holes is very thin they are essentially different. Hopefully, a careful investigation of the related issues will end up with an appropriate form of the third law and hence with an unblemished setting of black hole thermodynamics. 	
9407243v1	http://arxiv.org/pdf/hep-ph/9407243v1	1994	CP Violation in Beauty Decays -- the Standard Model Paradigm of Large   Effects	I. I. Bigi	  The Standard Model contains a natural source for CP asymmetries in weak decays, which is described by the KM mechanism. Beyond $\epsilon _K$ it generates only elusive manifestations of CP violation in {\em light-}quark systems. On the other hand it naturally leads to large asymmetries in certain non-leptonic beauty decays. In particular when $B^0-\bar B^0$ oscillations are involved, theoretical uncertainties in the hadronic matrix elements either drop out or can be controlled, and one predicts asymmetries well in excess of 10\% with high parametric reliability. It is briefly described how the KM triangle can be determined experimentally and then subjected to sensitive consistency tests. Any failure would constitute indirect, but unequivocal evidence for the intervention of New Physics; some examples are sketched. Any outcome of a comprehensive program of CP studies in $B$ decays -- short of technical failure -- will provide us with fundamental and unique insights into nature's design. 	
0411206v1	http://arxiv.org/pdf/physics/0411206v1	2004	Leave-one-out prediction error of systolic arterial pressure time series   under paced breathing	N. Ancona|R. Maestri|D. Marinazzo|L. Nitti|M. Pellicoro|G. D. Pinna|S. Stramaglia	  In this paper we show that different physiological states and pathological conditions may be characterized in terms of predictability of time series signals from the underlying biological system. In particular we consider systolic arterial pressure time series from healthy subjects and Chronic Heart Failure patients, undergoing paced respiration. We model time series by the regularized least squares approach and quantify predictability by the leave-one-out error. We find that the entrainment mechanism connected to paced breath, that renders the arterial blood pressure signal more regular, thus more predictable, is less effective in patients, and this effect correlates with the seriousness of the heart failure. The leave-one-out error separates controls from patients and, when all orders of nonlinearity are taken into account, alive patients from patients for which cardiac death occurred. 	
0501014v1	http://arxiv.org/pdf/physics/0501014v1	2005	Numerical study of the temperature and porosity effects on the fracture   propagation in a 2D network of elastic bonds	Harold Auradou|Maria Zei|Elisabeth Bouchaud	  This article reports results concerning the fracture of a 2d triangular lattice of atoms linked by springs. The lattice is submitted to controlled strain tests and the influence of both porosity and temperature on failure is investigated. The porosity is found on one hand to decrease the stiffness of the material but on the other hand it increases the deformation sustained prior to failure. Temperature is shown to control the ductility due to the presence of cavities that grow and merge. The rough surfaces resulting from the propagation of the crack exhibit self-affine properties with a roughness exponent $\zeta = 0.59 \pm 0.07$ over a range of length scales which increases with temperature. Large cavities also have rough walls which are found to be fractal with a dimension, $D$, which evolves with the distance from the crack tip. For large distances, $D$ is found to be close to 1.5, and close to 1.0 for cavities just before their coalescence with the main crack. 	
0101050v1	http://arxiv.org/pdf/quant-ph/0101050v1	2001	New Tests of Macroscopic Local Realism using Continuous Variable   Measurements	M. D. Reid	  We show that quantum mechanics predicts an Einstein-Podolsky-Rosen paradox (EPR), and also a contradiction with local hidden variable theories, for photon number measurements which have limited resolving power, to the point of imposing an uncertainty in the photon number result which is macroscopic in absolute terms. We show how this can be interpreted as a failure of a new, very strong premise, called macroscopic local realism. We link this premise to the Schrodinger-cat paradox. Our proposed experiments ensure all fields incident on each measurement apparatus are macroscopic. We show that an alternative measurement scheme corresponds to balanced homodyne detection of quadrature phase amplitudes. The implication is that where either EPR correlations or failure of local realism is predicted for continuous variable (quadrature phase amplitude) measurements, one can perform a modified experiment which would lead to conclusions about the much stronger premise of macroscopic local realism. 	
0603017v2	http://arxiv.org/pdf/quant-ph/0603017v2	2006	Feats, Features and Failures of the PR-box	Valerio Scarani	  One of the most intriguing features of quantum physics is the non-locality of correlations that can be obtained by measuring entangled particles. Recently, it has been noticed that non-locality can be studied without reference to the Hilbert space formalism. I review here the properties of the basic mathematical tool used for such studies, the so called Popescu-Rohrlich-box, in short PR-box. Among its feats, are the simulation of the correlations of the singlet and of other non-local probability distributions. Among its features, the "anomaly of non-locality" and a great power for information-theoretical tasks. Among its failures, the impossibility of reproducing all multi-partite distributions and the triviality of the allowed dynamics. 	
0801.4701v1	http://arxiv.org/pdf/0801.4701v1	2008	Energy bursts in fiber bundle models of composite materials	Srutarshi Pradhan|Per C. Hemmer	  As a model of composite materials, a bundle of many fibers with stochastically distributed breaking thresholds for the individual fibers is considered. The bundle is loaded until complete failure to capture the failure scenario of composite materials under external load. The fibers are assumed to share the load equally, and to obey Hookean elasticity right up to the breaking point. We determine the distribution of bursts in which an amount of energy $E$ is released. The energy distribution follows asymptotically a universal power law $E^{-5/2}$, for any statistical distribution of fiber strengths. A similar power law dependence is found in some experimental acoustic emission studies of loaded composite materials. 	
0806.3121v1	http://arxiv.org/pdf/0806.3121v1	2008	Algorithmic Based Fault Tolerance Applied to High Performance Computing	George Bosilca|Remi Delmas|Jack Dongarra|Julien Langou	  We present a new approach to fault tolerance for High Performance Computing system. Our approach is based on a careful adaptation of the Algorithmic Based Fault Tolerance technique (Huang and Abraham, 1984) to the need of parallel distributed computation. We obtain a strongly scalable mechanism for fault tolerance. We can also detect and correct errors (bit-flip) on the fly of a computation. To assess the viability of our approach, we have developed a fault tolerant matrix-matrix multiplication subroutine and we propose some models to predict its running time. Our parallel fault-tolerant matrix-matrix multiplication scores 1.4 TFLOPS on 484 processors (cluster jacquard.nersc.gov) and returns a correct result while one process failure has happened. This represents 65% of the machine peak efficiency and less than 12% overhead with respect to the fastest failure-free implementation. We predict (and have observed) that, as we increase the processor count, the overhead of the fault tolerance drops significantly. 	
0812.3303v1	http://arxiv.org/pdf/0812.3303v1	2008	Wetting failure and contact line dynamics in a Couette flow	M. Sbragaglia|K. Sugiyama|L. Biferale	  Liquid-liquid wetting failure is investigated in a two-dimensional Couette system with two immiscible fluids of arbitrary viscosity. The problem is solved exactly using a sharp interface treatment of hydrodynamics (lubrication theory) as a function of the capillary number, viscous ratio and separation of scale, i.e. slip length versus macroscopic scale of the system. The existence of critical velocities, above which no stationary solutions are found, is analyzed in detail in terms of the relevant parameters of the system. Comparisons with existing analysis for other geometries are also carried out. A numerical method of analysis is also presented, based on diffuse interface models obtained from multiphase extensions of the lattice Boltzmann equation (LBE). Sharp interface and diffuse interface models are quantitatively compared face to face indicating the correct limit of applicability of the diffuse interface models. 	
0907.3353v2	http://arxiv.org/pdf/0907.3353v2	2010	Intermittency and roughening in the failure of brittle heterogeneous   materials	D. Bonamy	  Stress enhancement in the vicinity of brittle cracks makes the macro-scale failure properties extremely sensitive to the micro-scale material disorder. Therefore: (i) Fracturing systems often display a jerky dynamics, so-called crackling noise, with seemingly random sudden energy release spanning over a broad range of scales, reminiscent of earthquakes; (ii) Fracture surfaces exhibit roughness at scales much larger than that of material micro-structure. Here, I provide a critical review of experiments and simulations performed in this context, highlighting the existence of universal scaling features, independent of both the material and the loading conditions, reminiscent of critical phenomena. I finally discuss recent stochastic descriptions of crack growth in brittle disordered media that seem to capture qualitatively - and sometimes quantitatively - these scaling features. 	
0910.3972v1	http://arxiv.org/pdf/0910.3972v1	2009	Thermally Induced Local Failures in Quasi-One-Dimensional Systems:   Collapse in Carbon Nanotubes, Necking in Nanowires and Opening of Bubbles in   DNA	Cristiano Nisoli|Douglas Abraham|Turab Lookman|Avadh Saxena	  We present a general framework to explore thermally activated failures in quasi one dimensional systems. We apply it to the collapse of carbon nanotubes, the formation of bottlenecks in nanowires, both of which limit conductance, and the opening of local regions or "bubbles" of base pairs in strands of DNA that are relevant for transcription and danaturation. We predict an exponential behavior for the probability of the opening of bubbles in DNA, the average distance between flattened regions of a nanotube or necking in a nanowire as a monotonically decreasing function of temperature, and compute a temperature below which these events become extremely rare. These findings are difficult to obtain numerically, however, they could be accessible experimentally. 	
1001.1225v2	http://arxiv.org/pdf/1001.1225v2	2010	Percolation on bipartite scale-free networks	Hans Hooyberghs|Bert Van Schaeybroeck|Joseph O. Indekeu	  Recent studies introduced biased (degree-dependent) edge percolation as a model for failures in real-life systems. In this work, such process is applied to networks consisting of two types of nodes with edges running only between nodes of unlike type. Such bipartite graphs appear in many social networks, for instance in affiliation networks and in sexual contact networks in which both types of nodes show the scale-free characteristic for the degree distribution. During the depreciation process, an edge between nodes with degrees k and q is retained with probability proportional to (kq)^(-alpha), where alpha is positive so that links between hubs are more prone to failure. The removal process is studied analytically by introducing a generating functions theory. We deduce exact self-consistent equations describing the system at a macroscopic level and discuss the percolation transition. Critical exponents are obtained by exploiting the Fortuin-Kasteleyn construction which provides a link between our model and a limit of the Potts model. 	
1002.4938v2	http://arxiv.org/pdf/1002.4938v2	2010	Cavity analysis on the robustness of random networks against targeted   attacks: Influences of degree-degree correlations	Yoshifumi Shiraki|Yoshiyuki Kabashima	  We developed a scheme for evaluating the size of the largest connected subnetwork (giant component) in random networks and the percolation threshold when sites (nodes) and/or bonds (edges) are removed from the networks based on the cavity method of statistical mechanics of disordered systems. An advantage of our scheme is the capability of handling targeted attacks on sites/bonds in the presence of degree correlations beyond naive analyses on random failures (crashes) in networks of no degree correlations. We apply our scheme particularly to random networks of bimodal degree distribution (two-peak networks), which have been proposed in earlier studies as robust networks against random failures of site and/or targeted attacks on sites, and show that the correlations among degrees affect a network's robustness against targeted attacks on sites or bonds non-trivially depending on details of network configurations. 	
1004.2322v1	http://arxiv.org/pdf/1004.2322v1	2010	Dynamical Jumping Real-Time Fault-Tolerant Routing Protocol for Wireless   Sensor Networks	Guowei Wu|Chi Lin|Feng Xia|Lin Yao|He Zhang|Bing Liu	  In time-critical wireless sensor network (WSN) applications, a high degree of reliability is commonly required. A dynamical jumping real-time fault-tolerant routing protocol (DMRF) is proposed in this paper. Each node utilizes the remaining transmission time of the data packets and the state of the forwarding candidate node set to dynamically choose the next hop. Once node failure, network congestion or void region occurs, the transmission mode will switch to jumping transmission mode, which can reduce the transmission time delay, guaranteeing the data packets to be sent to the destination node within the specified time limit. By using feedback mechanism, each node dynamically adjusts the jumping probabilities to increase the ratio of successful transmission. Simulation results show that DMRF can not only efficiently reduce the effects of failure nodes, congestion and void region, but also yield higher ratio of successful transmission, smaller transmission delay and reduced number of control packets. 	
1004.4684v2	http://arxiv.org/pdf/1004.4684v2	2010	Deformation and Failure of Amorphous Solidlike Materials	Michael L. Falk|James S. Langer	  Since the 1970's, theories of deformation and failure of amorphous, solidlike materials have started with models in which stress-driven, molecular rearrangements occur at localized flow defects via "shear transformations". This picture is the basis for the modern theory of "shear transformation zones" (STZ's), which is the focus of this review. We begin by describing the structure of the theory in general terms and by showing several applications, specifically: interpretation of stress-strain measurements for a bulk metallic glass, analysis of numerical simulations of shear banding, and the use of the STZ equations of motion in free-boundary calculations. In the second half of this article, we focus for simplicity on what we call an "athermal" model of amorphous plasticity, and use that model to illustrate how the STZ theory emerges within a systematic formulation of nonequilibrium thermodynamics. 	
1011.4135v1	http://arxiv.org/pdf/1011.4135v1	2010	Progressive Decoding for Data Availability and Reliability in   Distributed Networked Storage	Yunghsiang Han|Soji Omiwade|Rong Zheng	  To harness the ever growing capacity and decreasing cost of storage, providing an abstraction of dependable storage in the presence of crash-stop and Byzantine failures is compulsory. We propose a decentralized Reed Solomon coding mechanism with minimum communication overhead. Using a progressive data retrieval scheme, a data collector contacts only the necessary number of storage nodes needed to guarantee data integrity. The scheme gracefully adapts the cost of successful data retrieval to the number of storage node failures. Moreover, by leveraging the Welch-Berlekamp algorithm, it avoids unnecessary computations. Compared to the state-of-the-art decoding scheme, the implementation and evaluation results show that our progressive data retrieval scheme has up to 35 times better computation performance for low Byzantine node rates. Additionally, the communication cost in data retrieval is derived analytically and corroborated by Monte-Carlo simulation results. Our implementation is flexible in that the level of redundancy it provides is independent of the number of data generating nodes, a requirement for distributed storage systems 	
1102.1609v3	http://arxiv.org/pdf/1102.1609v3	2011	Exact Minimum-Repair-Bandwidth Cooperative Regenerating Codes for   Distributed Storage Systems	Kenneth W. Shum|Yuchong Hu	  In order to provide high data reliability, distributed storage systems disperse data with redundancy to multiple storage nodes. Regenerating codes is a new class of erasure codes to introduce redundancy for the purpose of improving the data repair performance in distributed storage. Most of the studies on regenerating codes focus on the single-failure recovery, but it is not uncommon to see two or more node failures at the same time in large storage networks. To exploit the opportunity of repairing multiple failed nodes simultaneously, a cooperative repair mechanism, in the sense that the nodes to be repaired can exchange data among themselves, is investigated. A lower bound on the repair-bandwidth for cooperative repair is derived and a construction of a family of exact cooperative regenerating codes matching this lower bound is presented. 	
1106.3234v3	http://arxiv.org/pdf/1106.3234v3	2013	Towards designing robust coupled networks	Christian M. Schneider|Nuri Yazdani|Nuno A. M. Araujo|Shlomo Havlin|Hans J. Herrmann	  Natural and technological interdependent systems have been shown to be highly vulnerable due to cascading failures and an abrupt collapse of global connectivity under initial failure. Mitigating the risk by partial disconnection endangers their functionality. Here we propose a systematic strategy of selecting a minimum number of autonomous nodes that guarantee a smooth transition in robustness. Our method which is based on betweenness is tested on various examples including the famous 2003 electrical blackout of Italy. We show that, with this strategy, the necessary number of autonomous nodes can be reduced by a factor of five compared to a random choice. We also find that the transition to abrupt collapse follows tricritical scaling characterized by a set of exponents which is independent on the protection strategy. 	
1108.3167v2	http://arxiv.org/pdf/1108.3167v2	2011	Local/global model order reduction strategy for the simulation of   quasi-brittle fracture	Pierre Kerfriden|Jean-Charles Passieux|Stephane Pierre-Alain Bordas	  This paper proposes a novel technique to reduce the computational burden associated with the simulation of localised failure. The proposed methodology affords the simulation of damage initiation and propagation whilst concentrating the computational effort where it is most needed, i.e. in the localisation zones. To do so, a local/global technique is devised where the global (slave) problem (far from the zones undergoing severe damage and cracking) is solved for in a reduced space computed by the classical Proper Orthogonal Decomposition, while the local (master) degrees of freedom (associated with the part of the structure where most of the damage is taking place) are fully resolved. Both domains are coupled through a local/global technique. This method circumvents the difficulties associated with model order reduction for the simulation of highly non-linear mechanical failure and offers an alternative or complementary approach to the development of multiscale fracture simulators. 	
1112.2046v1	http://arxiv.org/pdf/1112.2046v1	2011	Improving TCP Performance over Wireless Network with Frequent   Disconnections	Purvang Dalal|Nikhil Kothari|K. S. Dasgupta	  Presented in this paper is the solution to the problem that arises when the TCP/IP protocol suite is used to provide Internet connectivity through mobile terminals over emerging 802.11 wireless links. Taking into consideration the strong drive towards wireless Internet access through mobile terminals, the problem of frequent disconnections causing serial timeouts is examined and analyzed, with the help of extensive simulations. After a detailed review of wireless link loss recovery mechanism and identification of related problems, a new scheme with modifications at link layer and transport layer is proposed. The proposed modifications which depend on interaction between two layers (i) reduce the idle time before transmission at TCP by preventing timeout occurrences and (ii) decouple the congestion control from recovery of the losses due to link failure. Results of simulation based experiments demonstrate considerable performance improvement with the proposed modifications over the conventional TCP, when a wireless sender is experiencing frequent link failures. 	
1203.6778v1	http://arxiv.org/pdf/1203.6778v1	2012	Systemic losses in banking networks: indirect interaction of nodes via   asset prices	Igor Tsatskis	  A simple banking network model is proposed which features multiple waves of bank defaults and is analytically solvable in the limiting case of an infinitely large homogeneous network. The model is a collection of nodes representing individual banks; associated with each node is a balance sheet consisting of assets and liabilities. Initial node failures are triggered by external correlated shocks applied to the asset sides of the balance sheets. These defaults lead to further reductions in asset values of all nodes which in turn produce additional failures, and so on. This mechanism induces indirect interactions between the nodes and leads to a cascade of defaults. There are no interbank links, and therefore no direct interactions, between the nodes. The resulting probability distribution for the total (direct plus systemic) network loss can be viewed as a modification of the well-known Vasicek distribution. 	
1212.6967v1	http://arxiv.org/pdf/1212.6967v1	2012	Entropic Inference: some pitfalls and paradoxes we can avoid	Ariel Caticha	  The method of maximum entropy has been very successful but there are cases where it has either failed or led to paradoxes that have cast doubt on its general legitimacy. My more optimistic assessment is that such failures and paradoxes provide us with valuable learning opportunities to sharpen our skills in the proper way to deploy entropic methods. The central theme of this paper revolves around the different ways in which constraints are used to capture the information that is relevant to a problem. This leads us to focus on four epistemically different types of constraints. I propose that the failure to recognize the distinctions between them is a prime source of errors. I explicitly discuss two examples. One concerns the dangers involved in replacing expected values with sample averages. The other revolves around misunderstanding ignorance. I discuss the Friedman-Shimony paradox as it is manifested in the three-sided die problem and also in its original thermodynamic formulation. 	
1302.0744v2	http://arxiv.org/pdf/1302.0744v2	2013	Explicit MBR All-Symbol Locality Codes	Govinda M. Kamath|Natalia Silberstein|N. Prakash|Ankit S. Rawat|V. Lalitha|O. Ozan Koyluoglu|P. Vijay Kumar|Sriram Vishwanath	  Node failures are inevitable in distributed storage systems (DSS). To enable efficient repair when faced with such failures, two main techniques are known: Regenerating codes, i.e., codes that minimize the total repair bandwidth; and codes with locality, which minimize the number of nodes participating in the repair process. This paper focuses on regenerating codes with locality, using pre-coding based on Gabidulin codes, and presents constructions that utilize minimum bandwidth regenerating (MBR) local codes. The constructions achieve maximum resilience (i.e., optimal minimum distance) and have maximum capacity (i.e., maximum rate). Finally, the same pre-coding mechanism can be combined with a subclass of fractional-repetition codes to enable maximum resilience and repair-by-transfer simultaneously. 	
1303.4918v1	http://arxiv.org/pdf/1303.4918v1	2013	Non-Markovian Models of Blocking in Concurrent and Countercurrent Flows	Andrea Gabrielli|Julian Talbot|Pascal Viot	  We investigate models in which blocking can interrupt a particulate flow process at any time. Filtration, and flow in micro/nano-channels and traffic flow are examples of such processes. We first consider concurrent flow models where particles enter a channel randomly. If at any time two particles are simultaneously present in the channel, failure occurs. The key quantities are the survival probability and the distribution of the number of particles that pass before failure. We then consider a counterflow model with two opposing Poisson streams. There is no restriction on the number of particles passing in the same direction, but blockage occurs if, at any time, two opposing particles are simultaneously present in the passage. 	
1307.0433v2	http://arxiv.org/pdf/1307.0433v2	2013	'Mutual Watch-dog Networking': Distributed Awareness of Faults and   Critical Events in Petascale/Exascale systems	Roberto Ammendola|Andrea Biagioni|Ottorino Frezza|Francesca Lo Cicero|Alessandro Lonardo|Pier Stanislao Paolucci|Davide Rossetti|Francesco Simula|Laura Tosoratto|Piero Vicini	  Many tile systems require techniques to be applied to increase components resilience and control the FIT (Failures In Time) rate. When scaling to peta- exa-scale systems the FIT rate may become unacceptable due to component numerosity, requiring more systemic countermeasures. Thus, the ability to be fault aware, i.e. to detect and collect information about fault and critical events, is a necessary feature that large scale distributed architectures must provide in order to apply systemic fault tolerance techniques. In this context, the LO|FA|MO approach is a way to obtain systemic fault awareness, by implementing a mutual watchdog mechanism and guaranteeing fault detection in a no-single-point-of-failure fashion. This document contains specification and implementation details about this approach, in the shape of a technical report. 	
1307.1253v2	http://arxiv.org/pdf/1307.1253v2	2014	Network robustness of multiplex networks with interlayer degree   correlations	Byungjoon Min|Su Do Yi|Kyu-Min Lee|K. -I. Goh	  We study the robustness properties of multiplex networks consisting of multiple layers of distinct types of links, focusing on the role of correlations between degrees of a node in different layers. We use generating function formalism to address various notions of the network robustness relevant to multiplex networks such as the resilience of ordinary- and mutual connectivity under random or targeted node removals as well as the biconnectivity. We found that correlated coupling can affect the structural robustness of multiplex networks in diverse fashion. For example, for maximally-correlated duplex networks, all pairs of nodes in the giant component are connected via at least two independent paths and network structure is highly resilient to random failure. In contrast, anti-correlated duplex networks are on one hand robust against targeted attack on high-degree nodes, but on the other hand they can be vulnerable to random failure. 	
1307.1354v3	http://arxiv.org/pdf/1307.1354v3	2014	Modeling and Predicting the Growth and Death of Membership-based   Websites	Bruno Ribeiro	  Driven by outstanding success stories of Internet startups such as Facebook and The Huffington Post, recent studies have thoroughly described their growth. These highly visible online success stories, however, overshadow an untold number of similar ventures that fail. The study of website popularity is ultimately incomplete without general mechanisms that can describe both successes and failures. In this work we present six years of the daily number of users (DAU) of twenty-two membership-based websites - encompassing online social networks, grassroots movements, online forums, and membership-only Internet stores - well balanced between successes and failures. We then propose a combination of reaction-diffusion-decay processes whose resulting equations seem not only to describe well the observed DAU time series but also provide means to roughly predict their evolution. This model allows an approximate automatic DAU-based classification of websites into self-sustainable v.s. unsustainable and whether the startup growth is mostly driven by marketing & media campaigns or word-of-mouth adoptions. 	
1307.6682v1	http://arxiv.org/pdf/1307.6682v1	2013	Phase Model with Feedback Control for Power Grids	Tatsuma Matsuo|Hidetsugu Sakaguchi	  A phase model with feedback control is studied as a dynamical model of power grids. As an example, we study a model network corresponding to the power grid in the Kyushu region. The standard frequency is maintained by the mutual synchronization and the feedback control. Electric failures are induced by an overload. We propose a local feedback method in which the strength of feedback control is proportional to the magnitude of generators. We find that the electric failures do not occur until the utilization ratio is close to 1 under this feedback control. We also find that the temporal response for the time-varying input power is suppressed under this feedback control. We explain the mechanisms using the corresponding global feedback method. 	
1308.4301v1	http://arxiv.org/pdf/1308.4301v1	2013	Crossover Behaviour In Driven Cascades	James Burridge	  We propose a model which explains how power-law crossover behaviour can arise in a system which is capable of experiencing cascading failure. In our model the susceptibility of the system to cascades is described by a single number, the propagation power, which measures the ease with which cascades propagate. Physically, such a number could represent the density of unstable material in a system, its internal connectivity, or the mean susceptibility of its component parts to failure. We assume that the propagation power follows an upward drifting Brownian motion between cascades, and drops discontinuously each time a cascade occurs. Cascades are described by a continuous state branching process with distributional properties determined by the value of the propagation power when they occur. In common with many cascading models, pure power law behaviour is exhibited at a critical level of propagation power, and the mean cascade size diverges. This divergence constrains large systems to the subcritical region. We show that as a result, crossover behaviour appears in the cascade distribution when an average is performed over the distribution of propagation power. We are able to analytically determine the exponents before and after the crossover. 	
1312.3739v1	http://arxiv.org/pdf/1312.3739v1	2013	Semantics of (Resilient) X10	Silvia Crafa|David Cunningham|Vijay Saraswat|Avraham Shinnar|Olivier Tardieu	  We present a formal small-step structural operational semantics for a large fragment of X10, unifying past work. The fragment covers multiple places, mutable objects on the heap, sequencing, \code{try/catch}, \code{async}, \code{finish}, and \code{at} constructs. This model accurately captures the behavior of a large class of concurrent, multi-place X10 programs. Further, we introduce a formal model of resilience in X10. During execution of an X10 program, a place may fail for many reasons. Resilient X10 permits the program to continue executing, losing the data at the failed place, and most of the control state, and repairing the global control state in such a way that key semantic principles hold, the Invariant Happens Before Principle, and the Failure Masking Principle. These principles permit an X10 programmer to write clean code that continues to work in the presence of place failure. The given semantics have additionally been mechanized in Coq. 	
1402.6841v2	http://arxiv.org/pdf/1402.6841v2	2014	On the failure of mean-field theories near a critical point	Navinder Singh	  It is well known that mean-field theories fail to reproduce the experimentally known critical exponents. The traditional argument which explain this failure of mean-field theories near a critical point is the Ginsburg criterion in which diverging fluctuations of the order parameter is the root cause. We argue, contrary to the above mentioned traditional view, that diverging fluctuations in real physical systems near a critical point are genuine consequence of the breakdown of the property of statistical independence, and are faithfully reproduced by the mean-field theory. By looking at the problem from the point of view of "statistical independence" the divergence of fluctuations in real physical systems near criticality becomes immediately apparent as a connection can be established between diverging correlation length and diverging fluctuations. To address the question of why mean-field theories, much successful qualitatively, fail to reproduce the known values of critical indices we argue, using the essential ideas of the Wilsonian renormalization group, that mean-field theories fail to capture the long length scale averages of an order parameter near a critical point. 	
1404.4584v2	http://arxiv.org/pdf/1404.4584v2	2014	Fracture strength: Stress concentration, extreme value statistics and   the fate of the Weibull distribution	Zsolt Bertalan|Ashivni Shekhawat|James P. Sethna|Stefano Zapperi	  The fracture strength distribution of materials is often described in terms of the Weibull law which can be derived by using extreme value statistics if elastic interactions are ignored. Here, we consider explicitly the interplay between elasticity and disorder and test the asymptotic validity of the Weibull distribution through numerical simulations of the two-dimensional random fuse model. Even when the local fracture strength follows the Weibull distribution, the global failure distribution is dictated by stress enhancement at the tip of the cracks and sometimes deviates from the Weibull law. Only in the case of a pre-existing power law distribution of crack widths do we find that the failure strength is Weibull distributed. Contrary to conventional assumptions, even in this case, the Weibull exponent can not be simply inferred from the exponent of the initial crack width distribution. Our results thus raise some concerns on the applicability of the Weibull distribution in most practical cases. 	
1404.7287v2	http://arxiv.org/pdf/1404.7287v2	2014	Disjoint-Path Selection in Internet: What traceroutes tell us?	Sameer Qazi|Tim Moors	  Routing policies used in the Internet can be restrictive, limiting communication between source-destination pairs to one path, when often better alternatives exist. To avoid route flapping, recovery mechanisms may be dampened, making adaptation slow. Unstructured overlays have been proposed to mitigate the issues of path and performance failures in the Internet by routing through an indirect-path via overlay peer(s). Choosing alternate-paths in overlay networks is a challenging issue. Ensuring both availability and performance guarantees on alternate paths requires aggressive monitoring of all overlay paths using active probing; this limits scalability. An alternate technique to select an overlay-path is to bias its selection based on physical disjointness criteria to bypass the failure on the primary-path. Recently, several techniques have emerged which can optimize the selection of a disjoint-path without incurring the high costs associated with probing paths. In this paper, we show that using only commodity approaches, i.e. running infrequent traceroutes between overlay hosts, a lot of information can be revealed about the underlying physical path diversity in the overlay network which can be used to make informed-guesses for alternate-path selection. We test our approach using datasets between real-world hosts in AMP and RIPE networks. 	
1405.2992v1	http://arxiv.org/pdf/1405.2992v1	2014	Correlating power consumption and network traffic for improving data   centers resiliency	Roberto Baldoni|Mario Caruso|Adriano Cerocchi|Claudio Ciccotelli|Luca Montanari|Luca Nicoletti	  The deployment of business critical applications and information infrastructures are moving to the cloud. This means they are hosted in large scale data centers with other business applications and infrastructures with less (or none) mission critical constraints. This mixed and complex environment makes very challenging the process of monitoring critical applications and handling (detecting and recovering) possible failures of servers' data center that could affect responsiveness and/or reliability of mission critical applications. Monitoring mechanisms used in data center are usually intrusive in the sense that they need to install agents on each single server. This has considerable drawbacks: huge usage of human resources to install and patch the system and interference with the critical application because agents share application resources. In order to detect (and possibly predict) failures in data centers the paper does a first attempt in showing the correlation between network traffic and servers' power consumption. This is an important step in deriving non-intrusive monitoring systems, as both network traffic and power consumption can be captured without installing any software at the servers. This will improve in its turn the overall resiliency of the data center and its self-managing capacity. 	
1406.4613v2	http://arxiv.org/pdf/1406.4613v2	2014	Failure of the Generalized Eigenstate Thermalization Hypothesis in   integrable models with multiple particle species	B. Pozsgay	  It has been recently observed for a particular quantum quench in the XXZ spin chain that local observables do not equilibrate to the predictions of the Generalized Gibbs Ensemble (GGE). In this work we argue that the breakdown of the GGE can be attributed to the failure of the Generalized Eigenstate Thermalization Hypothesis (GETH), which has been the main candidate to explain the validity of the GGE. We provide explicit counterexamples to the GETH and argue that generally it does not hold in models with multiple particle species. Therefore there is no reason to assume that the GGE should describe the long time limit of observables in these integrable models. 	
1406.5903v3	http://arxiv.org/pdf/1406.5903v3	2016	Blind Sensor Calibration using Approximate Message Passing	Christophe Schülke|Francesco Caltagirone|Lenka Zdeborová	  The ubiquity of approximately sparse data has led a variety of com- munities to great interest in compressed sensing algorithms. Although these are very successful and well understood for linear measurements with additive noise, applying them on real data can be problematic if imperfect sensing devices introduce deviations from this ideal signal ac- quisition process, caused by sensor decalibration or failure. We propose a message passing algorithm called calibration approximate message passing (Cal-AMP) that can treat a variety of such sensor-induced imperfections. In addition to deriving the general form of the algorithm, we numerically investigate two particular settings. In the first, a fraction of the sensors is faulty, giving readings unrelated to the signal. In the second, sensors are decalibrated and each one introduces a different multiplicative gain to the measures. Cal-AMP shares the scalability of approximate message passing, allowing to treat big sized instances of these problems, and ex- perimentally exhibits a phase transition between domains of success and failure. 	
1407.6910v1	http://arxiv.org/pdf/1407.6910v1	2014	Probabilistic metrology defeats ultimate deterministic bound	J. Calsamiglia|B. Gendra|R. Munoz-Tapia|E. Bagan	  Quantum-enhanced measurements exploit quantum mechanical effects to provide ultra-precise estimates of physical variables for use in advanced technologies, such as frequency calibration of atomic clocks, gravitational waves detection, and biosensing. Quantum metrology studies the fundamental limits in the estimation precision given a certain amount of resources (e.g. the number of probe systems) and restrictions (e.g. limited interaction time, or coping with unavoidable presence of noise). Here we show that, even in the presence of noise, probabilistic measurement strategies (which have a certain probability of failure or abstention) can provide, upon a heralded successful outcome, estimates with a precision that violates the deterministic bounds. This establishes a new ultimate quantum metrology limit. For probe systems subject to local dephasing, we quantify such precision limit as a function of the probability of failure that can be tolerated. We show that the possibility of abstaining can substantially set back the detrimental effects of noise. 	
1408.0485v1	http://arxiv.org/pdf/1408.0485v1	2014	Size dependent crush analysis of lithium orthosilicate pebbles	Ratna Kumar Annabattula|Matthias Kolb|Yixiang Gan|Rolf Rolli|Marc Kamlah	  Crushing strength of the breeder materials (lithium orthosilicate, $\rm{Li_4SiO_4}$ or OSi) in the form of pebbles to be used for EU solid breeder concept is investigated. The pebbles are fabricated using a melt-spray method and hence a size variation in the pebbles produced is expected. The knowledge of the mechanical integrity (crush strength) of the pebbles is important for a successful design of breeder blanket. In this paper, we present the experimental results of the crush (failure) loads for spherical OSi pebbles of different diameters ranging from $250~\mu$m to $800~\mu$m. The ultimate failure load for each size shows a Weibull distribution. Furthermore, the mean crush load increases with increase in pebble diameter. It is also observed that the level of opacity of the pebble influences the crush load significantly. The experimental data presented in this paper and the associated analysis could possibly help us to develop a framework for simulating a crushable polydisperse pebble assembly using discrete element method. 	
1409.5622v1	http://arxiv.org/pdf/1409.5622v1	2014	Instability of Sharing Systems in the Presence of Retransmissions	Predrag R. Jelenković|Evangelia D. Skiani	  Retransmissions represent a primary failure recovery mechanism on all layers of communication network architecture. Similarly, fair sharing, e.g. processor sharing (PS), is a widely accepted approach to resource allocation among multiple users. Recent work has shown that retransmissions in failure-prone, e.g. wireless ad hoc, networks can cause heavy tails and long delays. In this paper, we discover a new phenomenon showing that PS-based scheduling induces complete instability with zero throughput in the presence of retransmissions, regardless of how low the traffic load may be. This phenomenon occurs even when the job sizes are bounded/fragmented, e.g. deterministic. Our analytical results are further validated via simulation experiments. Moreover, our work demonstrates that scheduling one job at a time, such as first-come-first-serve, achieves stability and should be preferred in these systems. 	
1412.1331v1	http://arxiv.org/pdf/1412.1331v1	2014	On analysis of incomplete field failure data	Zhisheng Ye|Hon Keung Tony Ng	  Many commercial products are sold with warranties and indirectly through dealers. The manufacturer-retailer distribution mechanism results in serious missing data problems in field return data, as the sales date for an unreturned unit is generally unknown to the manufacturer. This study considers a general setting for field failure data with unknown sales dates and a warranty limit. A stochastic expectation-maximization (SEM) algorithm is developed to estimate the distributions of the sales lag (time between shipment to a retailer and sale to a customer) and the lifetime of the product under study. Extensive simulations are used to evaluate the performance of the SEM algorithm and to compare with the imputation method proposed by Ghosh [Ann. Appl. Stat. 4 (2010) 1976-1999]. Three real examples illustrate the methodology proposed in this paper. 	
1412.2313v1	http://arxiv.org/pdf/1412.2313v1	2014	Microstructural Evolution of Charged Defects in the Fatigue Process of   Polycrystalline BiFeO3 Thin Films	Qingqing Ke|Amit Kumar|Xiaojie Lou|Yuan Ping Feng|Kaiyang Zeng|Yongqing Cai|John Wang	  Fatigue failure in ferroelectrics has been intensively investigated in the past few decades. Most of the mechanisms discussed for ferroelectric fatigue have been built on the "hypothesis of variation in charged defects", which however are rarely evidenced by experimental observation. Here, using a combination of complex impedance spectra techniques, piezoresponse force microscopy and first-principles theory, we examine the microscopic evolution and redistribution of charged defects during the electrical cycling in BiFeO3 thin films. The dynamic formation and melting behaviors of oxygen vacancy (VO) order are identified during the fatigue process. It reveals that the isolated VO tend to self-order along grain boundaries to form a planar-aligned structure, which blocks the domain reversals. Upon further electrical cycling, migration of VO within vacancy clusters is accommodated with a lower energy barrier (~0.2 eV) and facilitates the formation of nearby-electrode layer incorporated with highly concentrated VO. The interplay between the macroscopic fatigue and microscopic evolution of charged defects clearly demonstrates the role of ordered VO cluster in the fatigue failure of BiFeO3 thin films. 	
1501.07400v1	http://arxiv.org/pdf/1501.07400v1	2015	Resilience for Exascale Enabled Multigrid Methods	Markus Huber|Björn Gmeiner|Ulrich Rüde|Barbara Wohlmuth	  With the increasing number of components and further miniaturization the mean time between faults in supercomputers will decrease. System level fault tolerance techniques are expensive and cost energy, since they are often based on redundancy. Also classical check-point-restart techniques reach their limits when the time for storing the system state to backup memory becomes excessive. Therefore, algorithm-based fault tolerance mechanisms can become an attractive alternative. This article investigates the solution process for elliptic partial differential equations that are discretized by finite elements. Faults that occur in the parallel geometric multigrid solver are studied in various model scenarios. In a standard domain partitioning approach, the impact of a failure of a core or a node will affect one or several subdomains. Different strategies are developed to compensate the effect of such a failure algorithmically. The recovery is achieved by solving a local subproblem with Dirichlet boundary conditions using local multigrid cycling algorithms. Additionally, we propose a superman strategy where extra compute power is employed to minimize the time of the recovery process. 	
1504.08359v2	http://arxiv.org/pdf/1504.08359v2	2015	New insights into the problem with a singular drift term in the complex   Langevin method	Jun Nishimura|Shinji Shimasaki	  The complex Langevin method aims at performing path integral with a complex action numerically based on complexification of the original real dynamical variables. One of the poorly understood issues concerns occasional failure in the presence of logarithmic singularities in the action, which appear, for instance, from the fermion determinant in finite density QCD. We point out that the failure should be attributed to the breakdown of the relation between the complex weight that satisfies the Fokker-Planck equation and the probability distribution associated with the stochastic process. In fact, this problem can occur in general when the stochastic process involves a singular drift term. We show, however, in a simple example that there exists a parameter region in which the method works although the standard reweighting method is hardly applicable. 	
1509.04357v1	http://arxiv.org/pdf/1509.04357v1	2015	Fragmentation properties of two-dimensional Proximity Graphs considering   random failures and targeted attacks	Christoph Norrenbrock|Oliver Melchert|Alexander K. Hartmann	  The pivotal quality of proximity graphs is connectivity, i.e. all nodes in the graph are connected to one another either directly or via intermediate nodes. These types of graphs are robust, i.e., they are able to function well even if they are subject to limited removal of elementary building blocks, as it may occur for random failures or targeted attacks. Here, we study how the structure of these graphs is affected when nodes get removed successively until an extensive fraction is removed such that the graphs fragment. We study different types of proximity graphs for various node removal strategies. We use different types of observables to monitor the fragmentation process, simple ones like number and sizes of connected components, and more complex ones like the hop diameter and the backup capacity, which is needed to make a network N-1 resilient. The actual fragmentation turns out to be described by a second order phase transition. Using finite-size scaling analyses we numerically assess the threshold fraction of removed nodes, which is characteristic for the particular graph type and node deletion scheme, that suffices to decompose the underlying graphs. 	
1602.05370v1	http://arxiv.org/pdf/1602.05370v1	2016	Super-stretchable borophene and its stability under straining	Zhenqian Pang|Xin Qian|Ronggui Yang|Yujie Wei	  Recent success in synthesizing two-dimensional borophene on silver substrate attracts strong interest in exploring its possible extraordinary physical properties. By using the density functional theory calculations, we show that borophene is highly stretchable along the transverse direction. The strain-to-failure in the transverse direction is nearly twice as that along the longitudinal direction. The straining induced flattening and subsequent stretch of the flat borophene are accounted for the large strain-to-failure for tension in the transverse direction. The mechanical properties in the other two directions exhibit strong anisotropy. Phonon dispersions of the strained borophene monolayers suggest that negative frequencies are presented, which indicates the instability of free-standing borophene even under high tensile stress. 	
1602.07657v2	http://arxiv.org/pdf/1602.07657v2	2016	Substrate effect and nanoindentation fracture toughness based on pile up   and failure	Arnab S. Bhattacharyya|R. Praveen Kumar|Rohit Mandal|Nikhil Kumar|N. Rajak|Abhishek Sharma|Shashi Kant	  The effect of substrate was studied using nanoindentation on thin films. Soft films on hard substrate showed more pile up than usual which was attributed to the dislocation pile up at the film substrate interface. The effect of tip blunting on the load depth and hardness plots of nanoindentation was shown. The experimental date of variation of Vickers hardness with film thickness and loads were fitted and new parameters were analyzed. The delaminated area was analyzed using geometrical shapes using optical view of the failure region along with the load displacement Indentation fracture using Nanoindentation using Berkovich indenter has been studied. Indentation fracture toughness (KR) was analyzed based on computational programs. The contact mechanics during nanoindentation was studied with parameters related to indenter shape and tip sharpness. Elastic, plastic and total energies were computationally determined. The energy difference was related to shear stress being generated with elastic to plastic transition. Change in the nature of residual stress was related to film thickness. 	
1603.00154v1	http://arxiv.org/pdf/1603.00154v1	2016	Broadcast Repair for Wireless Distributed Storage Systems	Ping Hu|Chi Wan Sung|Terence H. Chan	  In wireless distributed storage systems, storage nodes are connected by wireless channels, which are broadcast in nature. This paper exploits this unique feature to design an efficient repair mechanism, called broadcast repair, for wireless distributed storage systems with multiple-node failures. Since wireless channels are typically bandwidth limited, we advocate a new measure on repair performance called repair-transmission bandwidth, which measures the average number of packets transmitted by helper nodes per failed node. The fundamental tradeoff between storage amount and repair-transmission bandwidth is obtained. It is shown that broadcast repair outperforms cooperative repair, which is the basic repair method for wired distributed storage systems with multiple-node failures, in terms of storage efficiency and repair-transmission bandwidth, thus yielding a better tradeoff curve. 	
1604.03258v1	http://arxiv.org/pdf/1604.03258v1	2016	Topology and morphology influences on the onset of ductile failure in a   two-phase microstructure	T. W. J. de Geus|R. H. J. Peerlings|M. G. D. Geers	  Multi-phase material are frequently applied in a wide variety of products, as they posses a unique set of properties by combining two or more distinct phases at the level of the microstructure. Although the macroscopic stiffness and hardening are reasonably well understood, questions remain about the dominant failure mechanism(s). We identify the role of the microstructural topology (the distribution of phases) on damage "hot-spot" in the microstructure, by performing a numerical study on a large set of randomly generated topologies. The result identifies a distinct probability distribution of phases around a typical damage "hot-spot". This work is focused on assessing the sensitivity of the result to the assumptions made on the microstructural geometry. 	
1604.04591v1	http://arxiv.org/pdf/1604.04591v1	2016	An Interference-Free Programming Model for Network Objects	Mischael Schill|Christopher M. Poskitt|Bertrand Meyer	  Network objects are a simple and natural abstraction for distributed object-oriented programming. Languages that support network objects, however, often leave synchronization to the user, along with its associated pitfalls, such as data races and the possibility of failure. In this paper, we present D-SCOOP, a distributed programming model that allows for interference-free and transaction-like reasoning on (potentially multiple) network objects, with synchronization handled automatically, and network failures managed by a compensation mechanism. We achieve this by leveraging the runtime semantics of a multi-threaded object-oriented concurrency model, directly generalizing it with a message-based protocol for efficiently coordinating remote objects. We present our pathway to fusing these contrasting but complementary ideas, and evaluate the performance overhead of the automatic synchronization in D-SCOOP, finding that it comes close to---or outperforms---explicit locking-based synchronization in Java RMI. 	
1604.06173v1	http://arxiv.org/pdf/1604.06173v1	2016	Onset of Plasticity in Thin Polystyrene Films	Bekele J. Gurmessa|Andrew B. Croll	  Polymer glasses have numerous advantageous mechanical properties in comparison to other materials. One of the most useful is the high degree of toughness that can be achieved due to significant yield occurring in the material. Remarkably, the onset of plasticity in polymeric materials is very poorly quantified, despite its importance as the ultimate limit of purely elastic behavior. Here we report the results of a novel experiment which is extremely sensitive to the onset of yield and discuss its impact on measurement and elastic theory. In particular, we use an elastic instability to locally bend and impart a \textit{local} tensile stress in a thin, glassy polystyrene film, and directly measure the resulting residual stress caused by the bending. We show that plastic failure is initiated at extremely low strains, of order $10^{-3}$ for polystyrene. Not only is this critical strain found to be small in comparison to bulk measurement, we show that it is influenced by thin film confinement - leading to an increase in the critical strain for plastic failure as film thickness approaches zero. 	
1609.02293v1	http://arxiv.org/pdf/1609.02293v1	2016	Evolution of Strength and Failure of SCC during Early Hydration	Linus K. Mettler|Falk K. Wittel|Robert J. Flatt|Hans J. Herrmann	  The early strength evolution of self-consolidating concrete (SCC) is studied by a set of non-standard mechanical tests for compressive, tensile, shear and bending failure. The results are applicable in an industrial environment for process control, e.g. of slip casting with adaptive molds in robotic fabrication. A procedure for collapsing data to a master evolution curve is presented that allows to distinguish two regimes in the evolution. In the first, the material is capable of undergoing large localized plastic deformation, as expected from thixotropic yield stress fluids. This is followed by a transition to cohesive frictional material behavior dominated by crack growth. The typical differences in tensile and compressive strength of hardened concrete are observed to originate at the transition. Finally, the evolution of a limit surface in principal stress space is constructed and discussed. 	
1611.02717v2	http://arxiv.org/pdf/1611.02717v2	2016	Resilience Design Patterns - A Structured Approach to Resilience at   Extreme Scale (version 1.0)	Saurabh Hukerikar|Christian Engelmann	  In this document, we develop a structured approach to the management of HPC resilience based on the concept of resilience-based design patterns. A design pattern is a general repeatable solution to a commonly occurring problem. We identify the commonly occurring problems and solutions used to deal with faults, errors and failures in HPC systems. The catalog of resilience design patterns provides designers with reusable design elements. We define a design framework that enhances our understanding of the important constraints and opportunities for solutions deployed at various layers of the system stack. The framework may be used to establish mechanisms and interfaces to coordinate flexible fault management across hardware and software components. The framework also enables optimization of the cost-benefit trade-offs among performance, resilience, and power consumption. The overall goal of this work is to enable a systematic methodology for the design and evaluation of resilience technologies in extreme-scale HPC systems that keep scientific applications running to a correct solution in a timely and cost-efficient manner in spite of frequent faults, errors, and failures of various types. 	
1612.01284v1	http://arxiv.org/pdf/1612.01284v1	2016	Modeling Structure and Resilience of the Dark Network	M. De Domenico|A. Arenas	  While the statistical and resilience properties of the Internet are no more changing significantly across time, the Darknet, a network devoted to keep anonymous its traffic, still experiences rapid changes to improve the security of its users. Here, we study the structure of the Darknet and we find that its topology is rather peculiar, being characterized by non-homogenous distribution of connections -- typical of scale-free networks --, very short path lengths and high clustering -- typical of small-world networks -- and lack of a core of highly connected nodes.   We propose a model to reproduce such features, demonstrating that the mechanisms used to improve cyber-security are responsible for the observed topology. Unexpectedly, we reveal that its peculiar structure makes the Darknet much more resilient than the Internet -- used as a benchmark for comparison at a descriptive level -- to random failures, targeted attacks and cascade failures, as a result of adaptive changes in response to the attempts of dismantling the network across time. 	
1705.02245v1	http://arxiv.org/pdf/1705.02245v1	2017	Data Readiness Levels	Neil D. Lawrence	  Application of models to data is fraught. Data-generating collaborators often only have a very basic understanding of the complications of collating, processing and curating data. Challenges include: poor data collection practices, missing values, inconvenient storage mechanisms, intellectual property, security and privacy. All these aspects obstruct the sharing and interconnection of data, and the eventual interpretation of data through machine learning or other approaches. In project reporting, a major challenge is in encapsulating these problems and enabling goals to be built around the processing of data. Project overruns can occur due to failure to account for the amount of time required to curate and collate. But to understand these failures we need to have a common language for assessing the readiness of a particular data set. This position paper proposes the use of data readiness levels: it gives a rough outline of three stages of data preparedness and speculates on how formalisation of these levels into a common language for data readiness could facilitate project management. 	
1707.02422v2	http://arxiv.org/pdf/1707.02422v2	2017	Predictability and Strength of a Heterogeneous System : The Role of   System Size and Disorder	Subhadeep Roy	  In this work I have studied the effect of disorder and system size in fiber bundle model with a certain range of stress redistribution. The strength of the bundle as well as the failure abruptness is observed with varying disorder, stress release range and system sizes. With a local stress concentration, the strength of the bundle is observed to decrease with system size. The behavior of such decrement changes drastically as disorder strength is tuned. At moderate disorder, the critical stress scales with system size in an inverse logarithmic manner. In low disorder, where the brittle response is highly expected, the strength decreases in a scale free manner. With increasing system size and stress release range the model approaches thermodynamic limit and the mean field limit respectively. A detail study expresses different limit in the model and the corresponding modes of failure on the plane of above mentioned parameters. 	
1709.08329v1	http://arxiv.org/pdf/1709.08329v1	2017	Graphene helicoid as novel nanospring	Haifei Zhan|Yingyan Zhang|Chunhui Yang|Gang Zhang|Yuantong Gu	  Advancement of nanotechnology has greatly accelerated the miniaturization of mechanical or electronic devices. This work proposes a new nanoscale spring - a graphene nanoribbon-based helicoid (GH) structure by using large-scale molecular dynamics simulation. It is found that the GH structure not only possesses an extraordinary high tensile deformation capability, but also exhibits unique features not accessible from traditional springs. Specifically, its yield strain increases when its inner radius is enlarged, which can exceed 1000%, and it has three elastic deformation stages including the initial delamination, stable delamination and elastic deformation. Moreover, the failure of the GH is found to be governed by the failure of graphene nanoribbon and the inner edge atoms absorb most of the tensile strain energy. Such fact leads to a constant elastic limit force (corresponding to the yield point) for all GHs. This study has provided a comprehensive understanding of the tensile behaviors of GH, which opens the avenue to design novel nanoscale springs based on 2D nanomaterials. 	
1711.05317v1	http://arxiv.org/pdf/1711.05317v1	2017	Correlated network of networks enhances robustness against catastrophic   failures	Byungjoon Min|Muhua Zheng|Hernán A. Makse	  Networks in nature rarely function in isolation but instead interact with one another with a form of network of networks (NoN). Network of networks with interdependency between distinct networks contains instabilities of abrupt collapse related to the global rule of activation. As a remedy of the collapse instability, here we investigate a model of correlated NoN and find that the collapse instabilities can be removed with a specific pattern of correlated connectivity. We find that when hubs provide majority of interconnections and interconnections are convergent, a system of networks becomes stable systematically. Our study identifies a stable structure of correlated NoN against catastrophic failures. Our result further suggests a plausible way to enhance network robustness by manipulating connection patterns, along with other methods such as controlling the state of node based on local rule. 	
1802.01340v1	http://arxiv.org/pdf/1802.01340v1	2018	Experimental Constraints On The Fatigue of Icy Satellite Lithospheres by   Tidal Forces	Noah P. Hammond|Amy C. Barr|Reid F. Cooper|Tess E. Caswell|Greg Hirth	  Fatigue can cause materials that undergo cyclic loading to experience brittle failure at much lower stresses than under monotonic loading. We propose that the lithospheres of icy satellites could become fatigued and thus weakened by cyclical tidal stresses. To test this hypothesis, we performed a series of laboratory experiments to measure the fatigue of water ice at temperatures of $198$ K and $233$ K and at a loading frequency of $1$ Hz. We find that ice is \textit{not} susceptible to fatigue at our experimental conditions and that the brittle failure stress does not decrease with increasing number of loading cycles. Even though fatigue was not observed at our experimental conditions, colder temperatures, lower loading frequencies, and impurities in the ice shells of icy satellites may increase the likelihood of fatigue crack growth. We also explore other mechanisms that may explain the weak behavior of the lithospheres of some icy satellites. 	
1802.09490v1	http://arxiv.org/pdf/1802.09490v1	2018	Controlling Human Utilization of Failure-Prone Systems via Taxes	Ashish R. Hota|Shreyas Sundaram	  We consider a game-theoretic model where individuals compete over a shared failure-prone system or resource. We investigate the effectiveness of a taxation mechanism in controlling the utilization of the resource at the Nash equilibrium when the decision-makers have behavioral risk preferences, captured by prospect theory. We first observe that heterogeneous prospect-theoretic risk preferences can lead to counter-intuitive outcomes. In particular, for resources that exhibit network effects, utilization can increase under taxation and there may not exist a tax rate that achieves the socially optimal level of utilization. We identify conditions under which utilization is monotone and continuous, and then characterize the range of utilizations that can be achieved by a suitable choice of tax rate. We further show that resource utilization is higher when players are charged differentiated tax rates compared to the case when all players are charged an identical tax rate. 	
0303067v1	http://arxiv.org/pdf/physics/0303067v1	2003	The power law character of off-site power failures	A. John Arul|C. Senthil Kumar|S. Marimuthu|Om Pal Singh	  A study on the behavior of off-site AC power failure recovery times at three nuclear plant sites is presented. It is shown, that power law is appropriate for the representation of failure frequency-duration correlation function of off-site power failure events, based on simple assumptions about component failure and repair rates. It is also found that the annual maxima of power failure duration follow Frechet distribution, which is a type II asymptotic distribution, strengthening our assumption of power law for the parent distribution. The extreme value distributions obtained are used to extrapolate for failure durations beyond the observed range. 	
1502.00821v1	http://arxiv.org/pdf/1502.00821v1	2015	Software that Learns from its Own Failures	Martin Monperrus	  All non-trivial software systems suffer from unanticipated production failures. However, those systems are passive with respect to failures and do not take advantage of them in order to improve their future behavior: they simply wait for them to happen and trigger hard-coded failure recovery strategies. Instead, I propose a new paradigm in which software systems learn from their own failures. By using an advanced monitoring system they have a constant awareness of their own state and health. They are designed in order to automatically explore alternative recovery strategies inferred from past successful and failed executions. Their recovery capabilities are assessed by self-injection of controlled failures; this process produces knowledge in prevision of future unanticipated failures. 	
1008.0471v1	http://arxiv.org/pdf/1008.0471v1	2010	Stress Orientation Confidence Intervals from Focal Mechanism Inversion	Stefan A. Revets	  The determination of confidence intervals of stress orientation is a crucial element in the discussion of homogeneity or heterogeneity of the stress field under study. The error estimates provided by the grid search method Focal Mechanism Stress Inversion of Gephart and Forsyth (1984) have been shown to be too wide but the reasons for this failure have escaped elucidation. Through the use of directional statistics and synthetic focal mechanisms, I show that the grid search methodology does yield appropriate uncertainty estimates. The direct perturbation of the synthetic focal mechanisms introduces bias which leads to confidence intervals which become increasingly too wide as the amount of perturbation increases. The synthetic data also show at what point the method fails to overcome this bias and when confidence intervals will be too wide. The indirect perturbation of the focal mechanisms by perturbing the generating deviatoric stress tensor generates synthetic data devoid of bias. Inversion of these data sets yields correct confidence intervals. The Focal Mechanism Stress Inversion method is vindicated as a highly effective method, and with the use of appropriate directional statistics, its results can be assessed and homogeneity or heterogeneity of the stress field can be discussed with confidence. 	
1209.5291v1	http://arxiv.org/pdf/1209.5291v1	2012	Stress relaxation through crosslink unbinding in cytoskeletal networks	Claus Heussinger	  The mechanical properties of cells are dominated by the cytoskeleton, an interconnected network of long elastic filaments. The connections between the filaments are provided by crosslinking proteins, which constitute, next to the filaments, the second important mechanical element of the network. An important aspect of cytoskeletal assemblies is their dynamic nature, which allows remodeling in response to external cues. The reversible nature of crosslink binding is an important mechanism that underlies these dynamical processes. Here, we develop a theoretical model that provides insight into how the mechanical properties of cytoskeletal networks may depend on their underlying constituting elements. We incorporate three important ingredients: nonaffine filament deformations in response to network strain; interplay between filament and crosslink mechanical properties; reversible crosslink (un)binding in response to imposed stress. With this we are able to self-consistently calculate the nonlinear modulus of the network as a function of deformation amplitude and crosslink as well as filament stiffnesses. During loading crosslink unbinding processes lead to a relaxation of stress and therefore to a reduction of the network modulus and eventually to network failure, when all crosslink are unbound. This softening due to crosslink unbinding generically competes with an inherent stiffening response, which may either be due to filament or crosslink nonlinear elasticity. 	
1401.4287v1	http://arxiv.org/pdf/1401.4287v1	2014	The model of stress distribution in polymer electrolyte membrane	Vadim V. Atrazhev|Tatiana Yu. Astakhova|Dmitry V. Dmitriev|Nikolay S. Erikhman|Vadim I. Sultanov|Timothy Patterson|Sergei F. Burlatsky	  An analytical model of mechanical stress in a polymer electrolyte membrane (PEM) of a hydrogen/air fuel cell with porous Water Transfer Plates (WTP) is developed in this work. The model considers a mechanical stress in the membrane is a result of the cell load cycling under constant oxygen utilization. The load cycling causes the cycling of the inlet gas flow rate, which results in the membrane hydration/dehydration close to the gas inlet. Hydration/dehydration of the membrane leads to membrane swelling/shrinking, which causes mechanical stress in the constrained membrane. Mechanical stress results in through-plane crack formation. Thereby, the mechanical stress in the membrane causes mechanical failure of the membrane, limiting fuel cell lifetime. The model predicts the stress in the membrane as a function of the cell geometry, membrane material properties and operation conditions. The model was applied for stress calculation in GORE-SELECT. 	
1711.04505v1	http://arxiv.org/pdf/1711.04505v1	2017	Tailoring mechanically-tunable strain fields in graphene	M. Goldsche|J. Sonntag|T. Khodkov|G. Verbiest|S. Reichardt|C. Neumann|T. Ouaj|N. von den Driesch|D. Buca|C. Stampfer	  There are a number of theoretical proposals based on strain engineering of graphene and other two-dimensional materials, however purely mechanical control of strain fields in these systems has remained a major challenge. The two approaches mostly used so far either couple the electrical and mechanical properties of the system simultaneously or introduce some unwanted disturbances due to the substrate. Here, we report on silicon micro-machined comb-drive actuators to controllably and reproducibly induce strain in a suspended graphene sheet, in an entirely mechanical way. We use spatially resolved confocal Raman spectroscopy to quantify the induced strain, and we show that different strain fields can be obtained by engineering the clamping geometry, including tunable strain gradients of up to 1.4 %/$\mu$m. Our approach also allows for multiple axis straining and is equally applicable to other two-dimensional materials, opening the door to an investigating their mechanical and electromechanical properties. Our measurements also clearly identify defects at the edges of a graphene sheet as being weak spots responsible for its mechanical failure. 	
1801.04292v1	http://arxiv.org/pdf/1801.04292v1	2018	Mechanical Properties of Phagraphene Membranes: A Fully Atomistic   Molecular Dynamics Investigation	Jose M. de Sousa|Acrisio L. Aguiar|Eduardo C. Girao|Alexandre F. Fonseca|Antonio G. Sousa Filho|Douglas S. Galvao	  Recently, a new 2D carbon allotrope structure, named phagraphene (PG), was proposed. PG has a densely array of penta-hexa-hepta-graphene carbon rings. PG was shown to present low and anisotropic thermal conductivity and it is believed that this anisotropy should be also reflected in its mechanical properties. Although PG mechanical properties have been investigated, a detailed and comprehensive study is still lacking. In the present work we have carried out fully atomistic reactive molecular dynamics simulations using the ReaxFF force field, to investigate the mechanical properties and fracture patterns of PG membranes. The Young's modulus values of the PG membranes were estimated from the stress-strain curves. Our results show that these curves present three distinct regimes: one regime where ripples dominate the structure and mechanical properties of the PG membranes; an elastic regime where the membranes exhibit fully planar configurations; and finally a plastic regime where permanent deformations happened to the PG membrane up to the mechanical failure or fracture. 	
0504073v1	http://arxiv.org/pdf/cs/0504073v1	2005	Rendezvous Regions: A Scalable Architecture for Resource Discovery and   Service Location in Large-Scale Mobile Networks	Karim Seada|Ahmed Helmy	  In large-scale wireless networks such as mobile ad hoc and sensor networks, efficient and robust service discovery and data-access mechanisms are both essential and challenging. Rendezvous-based mechanisms provide a valuable solution for provisioning a wide range of services. In this paper, we describe Rendezvous Regions (RRs) - a novel scalable rendezvous-based architecture for wireless networks. RR is a general architecture proposed for service location and bootstrapping in ad hoc networks, in addition to data-centric storage, configuration, and task assignment in sensor networks. In RR the network topology is divided into geographical regions, where each region is responsible for a set of keys representing the services or data of interest. Each key is mapped to a region based on a hash-table-like mapping scheme. A few elected nodes inside each region are responsible for maintaining the mapped information. The service or data provider stores the information in the corresponding region and the seekers retrieve it from there. We run extensive detailed simulations, and high-level simulations and analysis, to investigate the design space, and study the architecture in various environments including node mobility and failures. We evaluate it against other approaches to identify its merits and limitations. The results show high success rate and low overhead even with dynamics. RR scales to large number of nodes and is highly robust and efficient to node failures. It is also robust to node mobility and location inaccuracy with a significant advantage over point-based rendezvous mechanisms. 	
0803.1625v1	http://arxiv.org/pdf/0803.1625v1	2008	Physicalism versus quantum mechanics	Henry P. Stapp	  In the context of theories of the connection between mind and brain, physicalism is the demand that all is basically purely physical. But the concept of "physical" embodied in this demand is characterized essentially by the properties of the physical that hold in classical physical theories. Certain of these properties contradict the character of the physical in quantum mechanics, which provides a better, more comprehensive, and more fundamental account of phenomena. It is argued that the difficulties that have plaged physicalists for half a century, and that continue to do so, dissolve when the classical idea of the physical is replaced by its quantum successor. The argument is concretized in a way that makes it accessible to non-physicists by exploiting the recent evidence connecting our conscious experiences to macroscopic measurable synchronous oscillations occurring in well-separated parts of the brain. A specific new model of the mind-brain connection that is fundamentally quantum mechanical but that ties conscious experiences to these macroscopic synchronous oscillations is used to illustrate the essential disparities between the classical and quantum notions of the physical, and in particular to demonstrate the failure in the quantum world of the principle of the causal closure of the physical, a failure that goes beyond what is entailed by the randomness in the outcomes of observations, and that accommodates the efficacy in the brain of conscious intent. 	
0808.3228v1	http://arxiv.org/pdf/0808.3228v1	2008	Hydro-Gravitational-Dynamics of Planets and Dark Energy	Carl H. Gibson|Rudolph E. Schild	  Self-gravitational fluid mechanical methods termed hydro-gravitational-dynamics (HGD) predict plasma fragmentation 0.03 Myr after the turbulent big bang to form protosuperclustervoids, turbulent protosuperclusters, and protogalaxies at the 0.3 Myr transition from plasma to gas. Linear protogalaxyclusters fragment at 0.003 Mpc viscous-inertial scales along turbulent vortex lines or in spirals, as observed. The plasma protogalaxies fragment on transition into white-hot planet-mass gas clouds (PFPs) in million-solar-mass clumps (PGCs) that become globular-star-clusters (GCs) from tidal forces or dark matter (PGCs) by freezing and diffusion into 0.3 Mpc halos with 97% of the galaxy mass. The weakly collisional non-baryonic dark matter diffuses to > Mpc scales and frag-ments to form galaxy cluster halos. Stars and larger planets form by binary mergers of the trillion PFPs per PGC on 0.03 Mpc galaxy accretion disks. Star deaths depend on rates of planet accretion and internal star mixing. Moderate accretion rates produce white dwarfs that evaporate surrounding gas planets by spin-radiation to form planetary nebulae before Supernova Ia events, dimming some events to give systematic distance errors misinterpreted as the dark energy hypothesis and overestimates of the universe age. Failures of standard LCDM cosmological models reflect not only obsolete Jeans 1902 fluid mechanical assumptions, but also failures of standard turbulence models that claim the cascade of turbulent kinetic energy is from large scales to small. Because turbulence is always driven at all scales by inertial-vortex forces the turbulence cascade is always from small scales to large. 	
1304.6283v1	http://arxiv.org/pdf/1304.6283v1	2013	Damage mechanisms in the dynamic fracture of nominally brittle polymers	Davy Dalmas|Claudia Guerra|Julien Scheibert|Daniel Bonamy	  Linear Elastic Fracture Mechanics (LEFM) provides a consistent framework to evaluate quantitatively the energy flux released to the tip of a growing crack. Still, the way in which the crack selects its velocity in response to this energy flux remains far from completely understood. To uncover the underlying mechanisms, we experimentally studied damage and dissipation processes that develop during the dynamic failure of polymethylmethacrylate (PMMA), classically considered as the archetype of brittle amorphous materials. We evidenced a well-defined critical velocity along which failure switches from nominally-brittle to quasi-brittle, where crack propagation goes hand in hand with the nucleation and growth of microcracks. Via post-mortem analysis of the fracture surfaces, we were able to reconstruct the complete spatiotemporal microcracking dynamics with micrometer/nanosecond resolution. We demonstrated that the true local propagation speed of individual crack fronts is limited to a fairly low value, which can be much smaller than the apparent speed measured at the continuum-level scale. By coalescing with the main front, microcracks boost the macroscale velocity through an acceleration factor of geometrical origin. We discuss the key role of damage-related internal variables in the selection of macroscale fracture dynamics. 	
1511.02050v1	http://arxiv.org/pdf/1511.02050v1	2015	Finite size effects on crack front pinning at heterogeneous planar   interfaces: Experimental, finite elements and perturbation approaches	Sylvain Patinet|L Alzate|E Barthel|D Dalmas|D Vandembroucq|V Lazarus	  Understanding the role played by the microstructure of materials on their macroscopic failure properties is an important challenge in solid mechanics. Indeed, when a crack propagates at a heterogeneous brittle interface, the front is trapped by tougher regions and deforms. This pinning induces non-linearities in the crack propagation problem, even within Linear Elastic Fracture Mechanics theory, and modifies the overall failure properties of the material. For example crack front pinning by tougher places could increase the fracture resistance of multilayer structures, with interesting technological applications. Analytical perturbation approaches, based on Bueckner-Rice elastic line models, focus on the crack front perturbations, hence allow for a description of these phenomena. Here, they are applied to experiments investigating the propagation of a purely interfacial crack in a simple toughness pattern: a single defect strip surrounded by homogeneous interface. We show that by taking into account the finite size of the body, quantitative agreement with experimental and finite elements results is achieved. In particular this method allows to predict the toughness contrast, i.e. the toughness difference between the single defect strip and its homogeneous surrounding medium. This opens the way to a more accurate use of the perturbation method to study more disordered heterogeneous materials, where the finite elements method is less adequate. From our results, we also propose a simple method to determine the adhesion energy of tough interfaces by measuring the crack front deformation induced by known interface patterns. 	
1701.01193v2	http://arxiv.org/pdf/1701.01193v2	2017	Graphene and its elemental analogue: A molecular dynamics view of   fracture phenomenon	Tawfiqur Rakib|Satyajit Mojumder|Sourav Das|Sourav Saha|Mohammad Motalab	  Graphene and some graphene like two dimensional materials; hexagonal boron nitride (hBN) and silicene have unique mechanical properties which severely limit the suitability of conventional theories used for common brittle and ductile materials to predict the fracture response of these materials. This study revealed the fracture response of graphene, hBN and silicene nanosheets under different tiny crack lengths by molecular dynamics (MD) simulations using LAMMPS. The useful strength of these large area two dimensional materials are determined by their fracture toughness. Our study shows a comparative analysis of mechanical properties among the elemental analogues of graphene and suggested that hBN can be a good substitute for graphene in terms of mechanical properties. We have also found that the pre-cracked sheets fail in brittle manner and their failure is governed by the strength of the atomic bonds at the crack tip. The MD prediction of fracture toughness shows significant difference with the fracture toughness determined by Griffth's theory of brittle failure which restricts the applicability of Griffith's criterion for these materials in case of nano-cracks. Moreover, the strengths measured in armchair and zigzag directions of nanosheets of these materials implied that the bonds in armchair direction has the stronger capability to resist crack propagation compared to zigzag direction. 	
1507.01716v1	http://arxiv.org/pdf/1507.01716v1	2015	Temporal-varying failures of nodes in networks	Georgie Knight|Giampaolo Cristadoro|Eduardo G. Altmann	  We consider networks in which random walkers are removed because of the failure of specific nodes. We interpret the rate of loss as a measure of the importance of nodes, a notion we denote as failure-centrality. We show that the degree of the node is not sufficient to determine this measure and that, in a first approximation, the shortest loops through the node have to be taken into account. We propose approximations of the failure-centrality which are valid for temporal-varying failures and we dwell on the possibility of externally changing the relative importance of nodes in a given network, by exploiting the interference between the loops of a node and the cycles of the temporal pattern of failures. In the limit of long failure cycles we show analytically that the escape in a node is larger than the one estimated from a stochastic failure with the same failure probability. We test our general formalism in two real-world networks (air-transportation and e-mail users) and show how communities lead to deviations from predictions for failures in hubs. 	
1301.1433v1	http://arxiv.org/pdf/1301.1433v1	2013	Deformation mechanisms in a TiNi shape memory alloy during cyclic   loading	Anne-Lise Gloanec|Giovambattista Billota|Michel Gerland	  The deformation mechanisms governing the cyclic stress-strain behaviour of a TiNi shape memory alloy were investigated in this work. To understand the development of these mechanisms during cyclic loading, three low-cycle fatigue tests were performed and stopped at different stages. The first test was stopped after the first cycle; the second one was stopped after 40 cycles, corresponding to the beginning of the stabilisation of the cyclic strain-stress behaviour; and the last one was carried out to failure (3324 cycles). Submitted to fatigue loading, the response of the TiNi shape memory alloy presents a classical pseudoelastic response. Two deformation mechanisms, identified by TEM observations, are highlighted, the first one by twins and the second by dislocation slip and its interaction with precipitates. These two mechanisms evolve without competition during cyclic loading. The nanomechanical properties of the alloy were also examined, and the evolution of the microhardness or indentation modulus was monitored. 	
1505.07524v1	http://arxiv.org/pdf/1505.07524v1	2015	Hydrogen Segregation in Palladium and the Combined Effects of   Temperature and Defects on Mechanical Properties	Hieu H. Pham|A. Amine Benzerga|Tahir Cagin	  Atomistic calculations were carried out to investigate the mechanical properties of Pd crystals as a combined function of structural defects, hydrogen concentration and high temperature. These factors are found to individually induce degradation in the mechanical strength of Pd in a monotonous manner. In addition, defects such as vacancies and grain boundaries could provide a driving force for hydrogen segregation, thus enhance the tendency for their trapping. The simulations show that hydrogen maintains the highest localization at grain boundaries at ambient temperatures. This finding correlates well with the experimental observation that hydrogen embrittlement is more frequently observed around room temperature. The strength-limiting mechanism of mechanical failures induced by hydrogen is also discussed, which supports the hydrogen-enhanced localized plasticity theorem. 	
1509.02879v1	http://arxiv.org/pdf/1509.02879v1	2015	Progressive Collapse Mechanisms of Brittle and Ductile Framed Structures	Enrico Masoero|Falk K. Wittel|Hans J. Herrmann|B. M. Chiaia	  In this paper, we study the progressive collapse of 3D framed structures made of reinforced concrete after the sudden loss of a column. The structures are represented by elasto-plastic Euler Bernoulli beams with elongation-rotation failure threshold. We performed simulations using the Discrete Element Method considering inelastic collisions between the structural elements. The results show what collapse initiation and impact-driven propagation mechanisms are activated in structures with different geometric and mechanical features. Namely, we investigate the influence of the cross sectional size and reinforcement $\alpha$ and of the plastic capacity $\beta$ of the structural elements. We also study the final collapse extent and the fragment size distribution and their relation to $\alpha$, $\beta$ and to the observed collapse mechanisms. Finally, we compare the damage response of structures with symmetric and asymmetric reinforcement in the beams. 	
1512.00665v1	http://arxiv.org/pdf/1512.00665v1	2015	HBTM: A Heartbeat-based Behavior Detection Mechanism for POSIX Threads   and OpenMP Applications	Weidong Wang|Chunhua Liao|Liqiang Wang|Daniel J. Quinlan|Wei Lu	  Extreme-scale computing involves hundreds of millions of threads with multi-level parallelism running on large-scale hierarchical and heterogeneous hardware. In POSIX threads and OpenMP applications, some key behaviors occurring in runtime such as thread failure, busy waiting, and exit need to be accurately and timely detected. However, for the most of these applications, there are lack of unified and efficient detection mechanisms to do this. In this paper, a heartbeat-based behavior detection mechanism for POSIX threads (Pthreads) and OpenMP applications (HBTM) is proposed. In the design, two types of implementations are conducted, centralized and decentralized respectively. In both implementations, unified API has been designed to guarantee the generality of the mechanism. Meanwhile, a ring-based detection algorithm is designed to ease the burden of the centra thread at runtime. To evaluate the mechanism, the NAS Parallel Benchmarks (NPB) are used to test the performance of the HBTM. The experimental results show that the HBTM supports detection of behaviors of POSIX threads and OpenMP applications while acquiring a short latency and near 1% overhead. 	
1602.00295v2	http://arxiv.org/pdf/1602.00295v2	2016	Understanding the Planck Blackbody Spectrum and Landau Diamagnetism   within Classical Electromagnetism	Timothy H. Boyer	  Electromagnetism is a \textit{relativistic} theory and one must exercise care in coupling this theory with \textit{nonrelativistic} classical mechanics and with \textit{nonrelativistic} classical statistical mechanics. Indeed historically, both the blackbody radiation spectrum and diamagnetism within classical theory have been misunderstood because of two crucial failures: 1)the neglect of classical electromagnetic zero-point radiation, and 2) the use of erroneous combinations of nonrelativistic mechanics with relativistic electrodynamics. Here we review the treatment of classical blackbody radiation, and show that use of Lorentz-invariant classical electromagnetic zero-point radiation can be used to explain both the Planck blackbody spectrum and Landau diamagnetism at thermal equilibrium within classical electromagnetic theory. The analysis requires that relativistic electromagnetism is joined appropriately with simple nonrelativistic mechanical systems which can be regarded as the zero-velocity limits of relativistic systems, and that nonrelativistic classical statistical mechanics is applied only in the low-frequency limit when zero-point energy makes no contribution. 	
1710.01124v1	http://arxiv.org/pdf/1710.01124v1	2017	Effect of chain length distribution on mechanical behavior of polymeric   networks	Mohammad Tehrani|Alireza Sarvestani	  The effect of network chain distribution on mechanical behavior of elastomers is one of the long standing problems in rubber mechanics. The classical theory of rubber elasticity is built upon the assumption of entropic elasticity of networks whose constitutive strands are of uniform length. The kinetic theories for vulcanization, computer simulations, and indirect experimental measurements all indicate that the microstructure of vulcanizates is made of polymer strands with a random distribution of length. The polydispersity in strand length is expected to control the mechanical strength of rubber as the overloaded short strands break at small deformations and transfer the load to the longer strands. The purpose of this contribution is to present a simple theory of rubber mechanics which takes into account the length distribution of strands and its effect on the onset of bulk failure. 	
1802.00598v1	http://arxiv.org/pdf/1802.00598v1	2018	Mechanical responses of two-dimensional MoTe2; pristine 2H, 1T and 1T'   and 1T'/2H heterostructure	Bohayra Mortazavi|Golibjon R Berdiyorov|Meysam Makaremi|Timon Rabczuk	  Transition metal dichalcogenides (TMD) are currently among the most interesting two-dimensional (2D) materials due to their outstanding properties. MoTe2 involves attractive polymorphic TMD crystals which can exist in three different 2D atomic lattices of 2H, 1T and 1T', with diverse properties, like semiconducting and metallic electronic characters. Using the polymorphic heteroepitaxy, most recently coplanar semiconductor/metal (2H/1T') few-layer MoTe2 heterostructures were experimentally synthesized, highly promising to build circuit components for next generation nanoelectronics. Motivated by the recent experimental advances, we conducted first-principles calculations to explore the mechanical properties of single-layer MoTe2 structures. We first studied the mechanical responses of pristine and single-layer 2H-, 1T- and 1T'-MoTe2. In these cases we particularly analyzed the possibility of engineering of the electronic properties of these attractive 2D structures using the biaxial or uniaxial tensile loadings. Finally, the mechanical-failure responses of 1T'/2H-MoTe2 heterostructure were explored, which confirms the remarkable strength of this novel 2D system. 	
1703.01595v1	http://arxiv.org/pdf/1703.01595v1	2017	Creep stability of the proposed AIDA mission target 65803 Didymos: I.   Discrete cohesionless granular physics model	Yun Zhang|Derek C. Richardson|Olivier S. Barnouin|Clara Maurel|Patrick Michel|Stephen R. Schwartz|Ronald-Louis Ballouz|Lance A. M. Benner|Shantanu P. Naidu|Junfeng Li	  As the target of the proposed Asteroid Impact & Deflection Assessment (AIDA) mission, the near-Earth binary asteroid 65803 Didymos represents a special class of binary asteroids, those whose primaries are at risk of rotational disruption. To gain a better understanding of these binary systems and to support the AIDA mission, this paper investigates the creep stability of the Didymos primary by representing it as a cohesionless self-gravitating granular aggregate subject to rotational acceleration. To achieve this goal, a soft-sphere discrete element model (SSDEM) capable of simulating granular systems in quasi-static states is implemented and a quasi-static spin-up procedure is carried out. We devise three critical spin limits for the simulated aggregates to indicate their critical states triggered by reshaping and surface shedding, internal structural deformation, and shear failure, respectively. The failure condition and mode, and shear strength of an aggregate can all be inferred from the three critical spin limits. The effects of arrangement and size distribution of constituent particles, bulk density, spin-up path, and interparticle friction are numerically explored. The results show that the shear strength of a spinning self-gravitating aggregate depends strongly on both its internal configuration and material parameters, while its failure mode and mechanism are mainly affected by its internal configuration. Additionally, this study provides some constraints on the possible physical properties of the Didymos primary based on observational data and proposes a plausible formation mechanism for this binary system. With a bulk density consistent with observational uncertainty and close to the maximum density allowed for the asteroid, the Didymos primary in certain configurations can remain geo-statically stable without including cohesion. 	
1707.00788v1	http://arxiv.org/pdf/1707.00788v1	2017	Lifeguard : SWIM-ing with Situational Awareness	Armon Dadgar|James Phillips|Jon Currey	  SWIM is a peer-to-peer group membership protocol that uses randomized probing and gossip to obtain attractive scaling and robustness properties. Sensitivity to slow message processing, due to factors such as CPU exhaustion, network delay or loss, can lead SWIM to declare healthy members faulty. To counter this, SWIM adds a Suspicion mechanism, that trades increased failure detection latency for a lower false positive failure detection rate. However, relatively short lived periods of slow message processing commonly experienced in data centers can still lead to healthy members being marked as failed.   We observe that the Suspicion mechanism still assumes timely processing of some messages. In particular, refutation of a suspicion can only succeed if it is processed by the suspecting member in a timely manner. However, missing expected responses could indicate a member is experiencing slow message processing, and an episode of slow message processing at a given group member is likely to impact multiple of its interactions with other members in a short period of time. Based on these insights, we define a set of extensions to SWIM that allow a member to dynamically adjust its timeouts to mitigate timeliness issues. We call these extensions Lifeguard.   We analyze the effect of Lifeguard using synthetic benchmarks that vary message processing delays in a controlled manner. Across the wide range of cases tested, Lifeguard is able to reduce the false positive rate by a factor of more than 50x, while modestly increasing failure detection latency and message load.   Furthermore, by modifying tuning parameters, Life- guard allows users to reduce median detection latency by 45% while still reducing false positives at healthy members by 3x compared to without Lifeguard. The tuning parameters allow users to choose a suitable trade-off between lower false positives and lower detection latency. 	
9612095v1	http://arxiv.org/pdf/cond-mat/9612095v1	1996	First-Order Transition in the Breakdown of Disordered Media	Stefano Zapperi|Purusattam Ray|H. Eugene Stanley|Alessandro Vespignani	  We study the approach to global breakdown in disordered media driven by increasing external forces. We first analyze the problem by mean-field theory, showing that the failure process can be described as a first-order phase transition, similarly to the case of thermally activated fracture in homogeneous media. Then we quantitatively confirm the predictions of the mean-field theory using numerical simulations of discrete models. Widely distributed avalanches and the corresponding mean-field scaling are explained by the long-range nature of elastic interactions. We discuss the analogy of our results to driven disordered first-order transitions and spinodal nucleation in magnetic systems. 	
9711296v1	http://arxiv.org/pdf/cond-mat/9711296v1	1997	Analytical approaches to CA for traffic flow: Approximations and exact   solutions	Andreas Schadschneider	  Cellular automata have turned out to be important tools for the simulation of traffic flow. They are designed for an efficient impletmentation on the computer, but hard to treat analytically. Here we discuss several approaches for an analytical description of the Nagel-Schreckenberg (NaSch) model and its variants. These methods yield the exact solution for the special case $\vm=1$ of the NaSch model and are good approximations for higher values of the velocity ($\vm > 1$). We discuss the validity of these approximations and the conclusions for the underlying physics that can be drawn from the success or failure of the approximation. 	
9906057v1	http://arxiv.org/pdf/cond-mat/9906057v1	1999	Rate-and-State Theory of Plastic Deformation Near a Circular Hole	J. S. Langer|Alexander E. Lobkovsky	  We show that a simple rate-and-state theory accounts for most features of both time-independent and time-dependent plasticity in a spatially inhomogeneous situation, specifically, a circular hole in a large stressed plate. Those features include linear viscoelastic flow at small applied stresses, strain hardening at larger stresses, and a dynamic transition to viscoplasticity at a yield stress. In the static limit, this theory predicts the existence of a plastic zone near the hole for some but not all ranges of parameters. The rate-and-state theory also predicts dynamic failure modes that we believe may be relevant to fracture mechanics. 	
9908226v2	http://arxiv.org/pdf/cond-mat/9908226v2	2000	Damage in Fiber Bundle Models	Ferenc Kun|Stefano Zapperi|Hans J. Herrmann	  We introduce a continuous damage fiber bundle model that gives rise to macroscopic plasticity and compare its behavior with that of dry fiber bundles. Several interesting constitutive behaviors are found in this model depending on the value of the damage parameter and on the form of the disorder distribution. In addition, we compare the behavior of global load transfer models with local load transfer models and study in detail the damage evolution before failure. We emphasize the analogies between our results and spinodal nucleation in first-order phase transitions. 	
0004398v1	http://arxiv.org/pdf/cond-mat/0004398v1	2000	Fast fluctuating fields as the source of low-frequency conductance   fluctuations in many-electron systems and failure of quantum kinetics	Yu. E. Kuzovlev	  It is shown that in many-electron systems quantum transfer amplitudes and thus transfer probabilities may be strongly influenced by fast fluctuating fields, in particular, caused by simultaneous electron transfers. Corresponding mutual interplay of many electron jumps, arising at the fundamental level of quantum phases, results in long-correlated (1/f type) conductance fluctuations. However, thats could not be theoretically catched if neglect the real discreteness of quantum energy spectra and use the continuous spectrum approximation when building kinetic theory. Basing on first principles, the estimates of low-frequency fluctuations of tunneling conductance are presented. 	
0008208v1	http://arxiv.org/pdf/cond-mat/0008208v1	2000	Effective attraction between like-charged colloids in a 2D plasma	Ning Ma|S. M. Girvin|R. Rajaraman	  The existence of attractions between like-charged colloids immersed in ionic solution have been discovered in recent experiments. This phenomenon contradicts the predictions of DLVO theory and indicates a failure of mean field theory. We study a toy model based on a two dimensional one-component plasma, which is exactly soluble at one particular coupling constant. We show that colloidal interaction results from a competition between ion-ion repulsion and longer ranged ion-void attraction. 	
0011113v2	http://arxiv.org/pdf/cond-mat/0011113v2	2001	Failure regime in (1+1) dimensions in fibrous materials	I. L. Menezes-Sobrinho|A. T. Bernardes|J. G. Moreira	  In this paper, we introduce a model for fracture in fibrous materials that takes into account the rupture height of the fibers, in contrast with previous models. Thus, we obtain the profile of the fracture and calculate its roughness, defined as the variance around the mean height. We investigate the relationship between the fracture roughness and the fracture toughness. 	
0101071v1	http://arxiv.org/pdf/cond-mat/0101071v1	2001	Conditional statistics of temperature fluctuations in turbulent   convection	Emily S. C. Ching|K. L. Chau	  We find that the conditional statistics of temperature difference at fixed values of the locally averaged temperature dissipation rate in turbulent convection become Gaussian in the regime where the mixing dynamics is expected to be driven by buoyancy. Hence, intermittency of the temperature fluctuations in this buoyancy-driven regime can be solely attributed to the variation of the locally averaged temperature dissipation rate. We further obtain the functional behavior of these conditional temperature structure functions. This functional form demonstrates explicitly the failure of dimensional agruments and enhances the understanding of the temperature structure functions. 	
0105274v1	http://arxiv.org/pdf/cond-mat/0105274v1	2001	Failure of random matrix theory to correctly describe quantum dynamics	Tsampikos Kottos|Doron Cohen	  Consider a classically chaotic system which is described by a Hamiltonian H_0. At t=0 the Hamiltonian undergoes a sudden-change H_0 -> H. We consider the quantum-mechanical spreading of the evolving energy distribution, and argue that it cannot be analyzed using a random-matrix theory (RMT) approach. RMT can be trusted only to the extend that it gives trivial results that are implied by first-order perturbation theory. Non-perturbative effects are sensitive to the underlying classical dynamics, and therefore the hbar-->0 behavior for effective RMT models is strikingly different from the correct semiclassical limit. 	
0107597v1	http://arxiv.org/pdf/cond-mat/0107597v1	2001	Memory beyond memory in heart beating: an efficient way to detect   pathological conditions	P. Allegrini|P. Grigolini|P. Hamilton|L. Palatella|G. Raffaelli	  We study the long-range correlations of heartbeat fluctuations with the method of diffusion entropy. We show that this method of analysis yields a scaling parameter $\delta$ that apparently conflicts with the direct evaluation of the distribution of times of sojourn in states with a given heartbeat frequency. The strength of the memory responsible for this discrepancy is given by a parameter $\epsilon^{2}$, which is derived from real data. The distribution of patients in the ($\delta$, $\epsilon^{2}$)-plane yields a neat separation of the healthy from the congestive heart failure subjects. 	
0202330v1	http://arxiv.org/pdf/cond-mat/0202330v1	2002	Optimal design, robustness, and risk aversion	M. E. J. Newman|Michelle Girvan|J. Doyne Farmer	  Highly optimized tolerance is a model of optimization in engineered systems, which gives rise to power-law distributions of failure events in such systems. The archetypal example is the highly optimized forest fire model. Here we give an analytic solution for this model which explains the origin of the power laws. We also generalize the model to incorporate risk aversion, which results in truncation of the tails of the power law so that the probability of disastrously large events is dramatically lowered, giving the system more robustness. 	
0208405v1	http://arxiv.org/pdf/cond-mat/0208405v1	2002	Depinning transitions in discrete reaction-diffusion equations	A. Carpio|L. L. Bonilla	  We consider spatially discrete bistable reaction-diffusion equations that admit wave front solutions. Depending on the parameters involved, such wave fronts appear to be pinned or to glide at a certain speed. We study the transition of traveling waves to steady solutions near threshold and give conditions for front pinning (propagation failure). The critical parameter values are characterized at the depinning transition and an approximation for the front speed just beyond threshold is given. 	
0302307v1	http://arxiv.org/pdf/cond-mat/0302307v1	2003	Lonely adatoms in space	Joachim Krug	  There is a close relation between the problems of second layer nucleation in epitaxial crystal growth and chemical surface reactions, such as hydrogen recombination, on interstellar dust grains. In both cases standard rate equation analysis has been found to fail because the process takes place in a confined geometry. Using scaling arguments developed in the context of second layer nucleation, I present a simple derivation of the hydrogen recombination rate for small and large grains. I clarify the reasons for the failure of rate equations for small grains, and point out a logarithmic correction to the reaction rate when the reaction is limited by the desorption of hydrogen atoms (the second order reaction regime). 	
0307138v2	http://arxiv.org/pdf/cond-mat/0307138v2	2004	Pseudo Random Coins Show More Heads Than Tails	Heiko Bauke|Stephan Mertens	  Tossing a coin is the most elementary Monte Carlo experiment. In a computer the coin is replaced by a pseudo random number generator. It can be shown analytically and by exact enumerations that popular random number generators are not capable of imitating a fair coin: pseudo random coins show more heads than tails. This bias explains the empirically observed failure of some random number generators in random walk experiments. It can be traced down to the special role of the value zero in the algebra of finite fields. 	
0506757v1	http://arxiv.org/pdf/cond-mat/0506757v1	2005	Quantum Non-Locality in Systems with Open Boundaries: Failure of the   Wigner-Function Formalism	Luigi Genovese|David Taj|Fausto Rossi	  We shall revisit the conventional treatment of open quantum devices based on the Wigner-Function formalism. Our analysis will show that the artificial spatial separation between device active region and external reservoirs -properly defined within a semiclassical simulation scheme- is intrinsically incompatible with the non-local character of quantum mechanics. More specifically, by means of an exactly-solvable semiconductor model, we shall show that the application of the conventional boundary-condition scheme to the Wigner transport equation may produce highly non-physical results, like thermal injection of coherent state superpositions and boundary-driven negative probability distributions. 	
0509409v1	http://arxiv.org/pdf/cond-mat/0509409v1	2005	Planar Voronoi cells and the failure of Aboav's law	Hendrik-Jan Hilhorst	  Aboav's law is a quantitative expression of the empirical fact that in planar cellular structures many-sided cells tend to have few-sided neighbors. This law is nonetheless violated in the most widely used model system, {\it viz.} the Poisson-Voronoi tessellation. We obtain the correct law for this model: Given an $n$-sided cell, any of its neighbors has on average $m\_n$ sides where $m\_n=4+3(\pi/n)^{-{1/2}}+...$ in the limit of large $n$. This expression is quite accurate also for nonasymptotic $n$ and we discuss its implications for the analysis of experimental data. 	
0511407v1	http://arxiv.org/pdf/cond-mat/0511407v1	2005	Mode-Coupling Theory (MCT) Lecture Notes	David R. Reichman|Patrick Charbonneau	  In this set of lecture notes we review the mode-coupling theory of the glass transition from several perspectives. First, we derive mode-coupling equations for the description of density fluctuations from microscopic considerations with the use the Mori-Zwanzig projection operator technique. We also derive schematic mode-coupling equations of a similar form from a field-theoretic perspective. We review the successes and failures of mode-coupling theory, and discuss recent advances in the applications of the theory. 	
0601125v1	http://arxiv.org/pdf/cond-mat/0601125v1	2006	On the free energy within the mean-field approximation	R. Agra|F. van Wijland|E. Trizac	  We compare two widespread formulations of the mean-field approximation, based on minimizing an appropriately built mean-field free energy. We use the example of the antiferromagnetic Ising model to show that one of these formulations does not guarantee the existence of an underlying variational principle. This results in a severe failure where straightforward minimization of the corresponding mean-field free energy leads to incorrect results. 	
0209014v1	http://arxiv.org/pdf/cs/0209014v1	2002	Randomized protocols for asynchronous consensus	James Aspnes	  The famous Fischer, Lynch, and Paterson impossibility proof shows that it is impossible to solve the consensus problem in a natural model of an asynchronous distributed system if even a single process can fail. Since its publication, two decades of work on fault-tolerant asynchronous consensus algorithms have evaded this impossibility result by using extended models that provide (a) randomization, (b) additional timing assumptions, (c) failure detectors, or (d) stronger synchronization mechanisms than are available in the basic model. Concentrating on the first of these approaches, we illustrate the history and structure of randomized asynchronous consensus protocols by giving detailed descriptions of several such protocols. 	
0005092v2	http://arxiv.org/pdf/hep-ph/0005092v2	2000	Semi-inclusive hadron-hadron transverse spin asymmetries and their   implication for polarized DIS	M. Boglione|E. Leader	  We discuss a possible explanation of the 25 year old mystery of the large transverse spin asymmetries found in many semi-inclusive hadron-hadron reactions. We obtain the first reliable information about the transverse polarized quark densities Delta_T q(x) and we find surprising implications for the usual, longitudinal, polarized DIS. The plan of the presentation is as follows: 1) A brief reminder about the internal structure of the nucleon. 2) The transverse asymmetries. 3) Why it is so difficult to explain the asymmetries. 4) Failure and then success using a new soft mechanism. 5) implications for polarized DIS. 	
0111351v3	http://arxiv.org/pdf/hep-ph/0111351v3	2002	Brane Cosmology Solutions with Bulk Scalar Fields	Stephen C. Davis	  Brane cosmologies with static, five-dimensional and Z_2 symmetric bulks are analysed. A general solution generating mechanism is outlined. The qualatitive cosmological behaviour of all such solutions is determined. Conditions for avoiding naked bulk singularities are also discussed. The restrictions placed on the solutions by the assumption of such a static bulk are investigated. In particular the requirement of a non-standard energy-momentum conservation law. The failure of such solutions to provide viable quintessence terms in the Friedmann equations is also discussed. 	
9310176v1	http://arxiv.org/pdf/hep-th/9310176v1	1993	Elements of Reality and the Failure of the Product Rule Measurability of   Commuting Observables	Lev Vaidman	  The concept of ``elements of reality" is analyzed within the framework of quantum theory. It is shown that elements of reality fail to fulfill the product rule. This is the core of recent proofs of the impossibility of a Lorentz-invariant interpretation of quantum mechanics. A generalization and extension of the concept of elements of reality is presented. Lorentz-invariance is restored by giving up the product rule. The consequences of giving up the ``and" rule, which must be abandoned together with the product rule, are discussed. 	
9906156v2	http://arxiv.org/pdf/hep-th/9906156v2	1999	Reparametrization Invariance of Path Integrals	H. Kleinert|A. Chervyakov	  We demonstrate the reparametrization invariance of perturbatively defined one-dimensional functional integrals up to the three-loop level for a path integral of a quantum-mechanical point particle in a box. We exhibit the origin of the failure of earlier authors to establish reparametrization invariance which led them to introduce, superfluously, a compensating potential depending on the connection of the coordinate system. We show that problems with invariance are absent by defining path integrals as the epsilon-> 0 -limit of 1+ epsilon -dimensional functional integrals. 	
0011255v2	http://arxiv.org/pdf/hep-th/0011255v2	2000	On Isolated Vacua and Background Independence	T. Banks	  I argue that isolated vacua of M-theory, cannot in any conventional way be said to live in the same theory as other disconnected parts of the moduli space. The usual field theoretic mechanisms, which allow an observer in one disconnected component of a moduli space to verify the existence of other components, fail. The failure is a consequence of robust properties of black holes. When barriers between components are much smaller than the Planck scale, the usual field theoretic picture is approximately valid. 	
0512034v1	http://arxiv.org/pdf/physics/0512034v1	2005	"Long live effrontery!" Albert Einstein and the birth of Quantum Theory	Domenico Giulini	  From its very beginning, Quantum Theory developed contrary to the intentions of its creators. For Max Planck it marks the failure of a long-term research program, in which he tried to understand the 2nd law of thermodynamics deterministically in terms of mechanics and electrodynamics. For Albert Einstein it meant a refutation of his scientific credo. I describe parts of the early stages of this most remarkable development, up to Einstein's light-quantum hypotheis and its unfavourable reception by most other physicists. 	
0312035v2	http://arxiv.org/pdf/quant-ph/0312035v2	2004	Bell's inequality and the coincidence-time loophole	Jan-Ake Larsson|Richard Gill	  This paper analyzes effects of time-dependence in the Bell inequality. A generalized inequality is derived for the case when coincidence and non-coincidence [and hence whether or not a pair contributes to the actual data] is controlled by timing that depends on the detector settings. Needless to say, this inequality is violated by quantum mechanics and could be violated by experimental data provided that the loss of measurement pairs through failure of coincidence is small enough, but the quantitative bound is more restrictive in this case than in the previously analyzed "efficiency loophole." 	
0501081v2	http://arxiv.org/pdf/quant-ph/0501081v2	2005	Quantum perfect correlations	Masanao Ozawa	  The notion of perfect correlations between arbitrary observables, or more generally arbitrary POVMs, is introduced in the standard formulation of quantum mechanics, and characterized by several well-established statistical conditions. The transitivity of perfect correlations is proved to generally hold, and applied to a simple articulation for the failure of Hardy's nonlocality proof for maximally entangled states. The notion of perfect correlations between observables and POVMs is used for defining the notion of a precise measurement of a given observable in a given state. A longstanding misconception on the correlation made by the measuring interaction is resolved in the light of the new theory of quantum perfect correlations. 	
0702233v1	http://arxiv.org/pdf/quant-ph/0702233v1	2007	Quantum Fermi's Golden Rule	Fausto Rossi	  We shall revisit the conventional adiabatic or Markov approximation, showing its intrinsic failure in describing the proper quantum-mechanical evolution of a generic subsystem interacting with its environment. In particular, we shall show that -contrary to the semiclassical case- the Markov limit does not preserve the positive-definite character of the corresponding density matrix, thus leading to highly non-physical results. To overcome this problem, we shall propose an alternative adiabatic procedure which (i) in the semiclassical limit reduces to the standard Fermi's golden rule, and (ii) describes a genuine Limblad evolution, thus providing a reliable/robust treatment of energy-dissipation and dephasing processes. 	
0709.1585v1	http://arxiv.org/pdf/0709.1585v1	2007	Ab initio Study of Misfit Dislocations at the SiC/Si(001) Interface	Giancarlo Cicero|Laurent Pizzagalli|Alessandra Catellani	  The high lattice mismatched SiC/Si(001) interface was investigated by means of combined classical and ab initio molecular dynamics. Among the several configurations analyzed, a dislocation network pinned at the interface was found to be the most efficient mechanism for strain relief. A detailed description of the dislocation core is given, and the related electronic properties are discussed for the most stable geometry: we found interface states localized in the gap that may be a source of failure of electronic devices. 	
0709.4571v1	http://arxiv.org/pdf/0709.4571v1	2007	Einstein's Lost Battles and Neglected Achievements	Petar Grujic	  We analyze some of Einstein's failures to accomplish tasks which he posed to himself, notably deterministic interpretation of Quantum mechanics and formulation of the Unified theory of physical interactions, putting them into broader ontological and epistemological context. We highlight, on the other hand, a number of his contributions that have been unjustly neglected by the physical community, like the semiclassical quantization principle and the concept of antiparticle. Finally, we mention briefly three issues, which are related to Einstein, but are subject to controversy. 	
0711.3935v1	http://arxiv.org/pdf/0711.3935v1	2007	Coding for Network Coding	Andrea Montanari|Ruediger Urbanke	  We consider communication over a noisy network under randomized linear network coding. Possible error mechanism include node- or link- failures, Byzantine behavior of nodes, or an over-estimate of the network min-cut. Building on the work of Koetter and Kschischang, we introduce a probabilistic model for errors. We compute the capacity of this channel and we define an error-correction scheme based on random sparse graphs and a low-complexity decoding algorithm. By optimizing over the code degree profile, we show that this construction achieves the channel capacity in complexity which is jointly quadratic in the number of coded information bits and sublogarithmic in the error probability. 	
0801.0382v2	http://arxiv.org/pdf/0801.0382v2	2008	Some conjectures about the mechanism of poltergeist phenomenon	P. Brovetto|V. Maxia	  Poltergeist accounts concern at least four kinds of strange spontaneous manifestations, such as burning of materials, failures of electric equipments, rapping noises and movements of objects. A simple analysis of phenomenology of these disturbances shows that they might have a common origin, that is, a reduction in strength of molecular bonds due to an enhancement in polarization of vacuum which decreases the actual electron charge. Arguments based on Prigogine' nonequilibrium thermodynamics are proposed, which show how transformations in brain of some pubescent childs or young womans might be the cause of these effects. 	
0802.4010v1	http://arxiv.org/pdf/0802.4010v1	2008	Brain architecture: A design for natural computation	Marcus Kaiser	  Fifty years ago, John von Neumann compared the architecture of the brain with that of computers that he invented and which is still in use today. In those days, the organisation of computers was based on concepts of brain organisation. Here, we give an update on current results on the global organisation of neural systems. For neural systems, we outline how the spatial and topological architecture of neuronal and cortical networks facilitates robustness against failures, fast processing, and balanced network activation. Finally, we discuss mechanisms of self-organization for such architectures. After all, the organization of the brain might again inspire computer architecture. 	
0804.2394v1	http://arxiv.org/pdf/0804.2394v1	2008	Front propagation in A+B -> 2A reaction under subdiffusion	Daniela Froemberg|Hauke Schmidt-Martens|Igor M. Sokolov|Francesc Sagues	  We consider an irreversible autocatalytic conversion reaction A+B -> 2A under subdiffusion described by continuous time random walks. The reactants' transformations take place independently on their motion and are described by constant rates. The analog of this reaction in the case of normal diffusion is described by the Fisher-Kolmogorov-Petrovskii-Piskunov (FKPP) equation leading to the existence of a nonzero minimal front propagation velocity which is really attained by the front in its stable motion. We show that for subdiffusion this minimal propagation velocity is zero, which suggests propagation failure. 	
0808.3271v1	http://arxiv.org/pdf/0808.3271v1	2008	Reply to Comment on "Failure of the work-Hamiltonian connection for   free-energy calculations" by Horowitz and Jarzynski	J. M. G. Vilar|J. M. Rubi	  We show that the Comment [arXiv:0808.1224] by Horowitz and Jarzynski obtains as a main result a general free energy change for a harmonic system that in the macroscopic limit does not recover the textbook expression for the energy change of a Hookean spring. The reason is that Horowitz and Jarzynski improperly identify work with parametric changes of the Hamiltonian instead of with the standard quantity, force times displacement. 	
0811.2776v2	http://arxiv.org/pdf/0811.2776v2	2009	On a damage-plasticity approach to model concrete failure	Peter Grassl	  A damage-plasticity constitutive model for the description of fracture in plain concrete is presented. Two approaches, the local model comprising the adjustment of the softening modulus and the nonlocal model based on spatial averaging of history variables, are applied to the analysis of a concrete bar subjected to uniaxial tension and to a three-point bending test. The influence of mesh size and the decomposition into damage and plasticity components are discussed. It is shown that for the two examples studied, both approaches result in mesh-independent results. However, the nonlocal model, which relies on spatial averaging of history variables, exhibits sensitivity with respect to boundary conditions, which requires further studies. 	
0901.3277v1	http://arxiv.org/pdf/0901.3277v1	2009	Size effects in statistical fracture	Mikko J. Alava|Phani K. V. V. Nukala|Stefano Zapperi	  We review statistical theories and numerical methods employed to consider the sample size dependence of the failure strength distribution of disordered materials. We first overview the analytical predictions of extreme value statistics and fiber bundle models and discuss their limitations. Next, we review energetic and geometric approaches to fracture size effects for specimens with a flaw. Finally, we overview the numerical simulations of lattice models and compare with theoretical models. 	
0906.3832v1	http://arxiv.org/pdf/0906.3832v1	2009	Hardware Trojan by Hot Carrier Injection	Y. Shiyanovskii|F. Wolff|C. Papachristou|D. Weyer|W. Clay	  This paper discusses how hot carrier injection (HCI) can be exploited to create a trojan that will cause hardware failures. The trojan is produced not via additional logic circuitry but by controlled scenarios that maximize and accelerate the HCI effect in transistors. These scenarios range from manipulating the manufacturing process to varying the internal voltage distribution. This new type of trojan is difficult to test due to its gradual hardware degradation mechanism. This paper describes the HCI effect, detection techniques and discusses the possibility for maliciously induced HCI trojans. 	
1001.4103v1	http://arxiv.org/pdf/1001.4103v1	2010	The Failure of the Ergodic Assumption	M. Ignaccolo|M. Latka|B. J. West	  The well established procedure of constructing phenomenological ensemble from a single long time series is investigated. It is determined that a time series generated by a simple Uhlenbeck-Ornstein Langevin equation is mean ergodic. However the probability ensemble average yields a variance that is different from that determined using the phenomenological ensemble (time average). We conclude that the latter ensemble is often neither stationary nor ergodic and consequently the probability ensemble averages can misrepresent the underlying dynamic process. 	
1003.2191v1	http://arxiv.org/pdf/1003.2191v1	2010	Spectral matrix methods for partitioning power grids: Applications to   the Italian and Floridian high-voltage networks	Ibrahim Abou Hamad|Brett Israels|Per Arne Rikvold|Svetlana V. Poroseva	  Intentional islanding is used to limit cascading power failures by isolating highly connected "islands" with local generating capacity. To efficiently isolate an island, one should break as few power lines as possible. This is a graph partitioning problem, and here we give preliminary results on islanding of the Italian and Floridian high-voltage grids by spectral matrix methods. 	
1005.5448v1	http://arxiv.org/pdf/1005.5448v1	2010	Failover in cellular automata	Shailesh Kumar|Shrisha Rao	  A cellular automata (CA) configuration is constructed that exhibits emergent failover. The configuration is based on standard Game of Life rules. Gliders and glider-guns form the core messaging structure in the configuration. The blinker is represented as the basic computational unit, and it is shown how it can be recreated in case of a failure. Stateless failover using primary-backup mechanism is demonstrated. The details of the CA components used in the configuration and its working are described, and a simulation of the complete configuration is also presented. 	
1009.1268v1	http://arxiv.org/pdf/1009.1268v1	2010	Prediction of the collapse point of overloaded materials by monitoring   energy emissions	Srutarshi Pradhan|Per C. Hemmer	  A bundle of many fibers with stochastically distributed breaking thresholds is considered as a model of composite materials. The fibers are assumed to share the load equally, and to obey Hookean elasticity up to the breaking point. The bundle is slightly overloaded, which leads to complete failure. We study the properties of emission bursts in which an amount of energy $E$ is released. The analysis shows that the size of the energy bursts has a minimum when the system is half-way from the collapse point. 	
1009.3429v3	http://arxiv.org/pdf/1009.3429v3	2011	Semantics of Typed Lambda-Calculus with Constructors	Barbara Petit	  We present a Curry-style second-order type system with union and intersection types for the lambda-calculus with constructors of Arbiser, Miquel and Rios, an extension of lambda-calculus with a pattern matching mechanism for variadic constructors. We then prove the strong normalisation and the absence of match failure for a restriction of this system, by adapting the standard reducibility method. 	
1009.4127v1	http://arxiv.org/pdf/1009.4127v1	2010	Piezonuclear Reactions	Fabio Cardone|Roberto Mignani|Andrea Petrucci	  In this paper, we deal with the subject of piezonuclear reactions, namely nuclear reactions (of new type) triggered by pressure waves. We discuss the experimental evidences obtained in the last two decades, which can be summarized essentially as follows: experiments in cavitation of liquids, where transmutation of elements, creation of elements and emission of neutrons have been observed; emission of neutrons in brittle failure of solids subjected to mechanical pressure; alteration of the lifetime of un unstable element (thorium) subjected to cavitation. A theoretical model to explain these facts is proposed. Future perspectives of these experimental and theoretical investigations are also underlined. 	
1103.1288v2	http://arxiv.org/pdf/1103.1288v2	2011	A damage-plasticity model for the dynamic failure of concrete	Peter Grassl|Ulrika Nystrom|Rasmus Rempling|Kent Gylltoft	  A constitutive model based on the combination of damage mechanics and plasticity is developed to analyse concrete structures subjected to dynamic loading. The aim is to obtain a model, which requires input parameters with clear physical meanings. The model should describe the important characteristics of concrete subjected to multiaxial and rate-depending loading. This is achieved by combining an effective stress based plasticity model with an isotropic damage model based on plastic and elastic strain measures. The model response in tension, uni-, bi- and tri-axial compression is compared to experimental results in the literature. 	
1105.0379v1	http://arxiv.org/pdf/1105.0379v1	2011	Self-Repairing Codes for Distributed Storage - A Projective Geometric   Construction	Frederique Oggier|Anwitaman Datta	  Self-Repairing Codes (SRC) are codes designed to suit the need of coding for distributed networked storage: they not only allow stored data to be recovered even in the presence of node failures, they also provide a repair mechanism where as little as two live nodes can be contacted to regenerate the data of a failed node. In this paper, we propose a new instance of self-repairing codes, based on constructions of spreads coming from projective geometry. We study some of their properties to demonstrate the suitability of these codes for distributed networked storage. 	
1106.0386v1	http://arxiv.org/pdf/1106.0386v1	2011	Cubic to hexagonal iron phase transition promoted by interstitial   hydrogen	A. Castedo|J. Sanchez|J. Fullea|M. C. Andrade|P. L. de Andres	  Using ab-initio density functional theory we study the role of interstitial hydrogen on the energetics of the phase transformation of iron from bcc to hcp along Bain's pathway. The impurity creates an internal stress field that can be released through a tetragonal distortion of the lattice, promoting the bcc (ferromagnetic) $\rightarrow$ fcc (frustrated antiferromagnetic) $\rightarrow$ hcp (ferromagnetic) transition. The transformation between crystal systems is accompanied by a drastic magnetic reorganization and sudden variations of the unit cell volume, that can be one of the reasons for embrittlement and mechanical failure of iron upon hydrogen adsorption. 	
1111.1091v1	http://arxiv.org/pdf/1111.1091v1	2011	On the merit of a Central Limit Theorem-based approximation in   statistical physics	Bruno Leggio|Oleg Lychkovskiy|Antonino Messina	  The applicability conditions of a recently reported Central Limit Theorem-based approximation method in statistical physics are investigated and rigorously determined. The failure of this method at low and intermediate temperature is proved as well as its inadequacy to disclose quantum criticalities at fixed temperatures. Its high temperature predictions are in addition shown to coincide with those stemming from straightforward appropriate expansions up to (k_B T)^(-2). Our results are clearly illustrated by comparing the exact and approximate temperature dependence of the free energy of some exemplary physical systems. 	
1111.1826v1	http://arxiv.org/pdf/1111.1826v1	2011	Monitoring Software Reliability using Statistical Process control: An   MMLE approach	R. Satya Prasad|Bandla Sreenivasa Rao|R. R. L. Kantam	  This paper consider an MMLE (Modified Maximum Likelihood Estimation) based scheme to estimate software reliability using exponential distribution. The MMLE is one of the generalized frameworks of software reliability models of Non Homogeneous Poisson Processes (NHPPs). The MMLE gives analytical estimators rather than an iterative approximation to estimate the parameters. In this paper we proposed SPC (Statistical Process Control) Charts mechanism to determine the software quality using inter failure times data. The Control charts can be used to measure whether the software process is statistically under control or not. 	
1204.2183v2	http://arxiv.org/pdf/1204.2183v2	2012	Atomic Mechanism of Flow in Simple Liquids under Shear	Takuya Iwashita|Takeshi Egami	  Atomic correlations in a simple liquid in steady-state flow under shear stress were studied by molecular dynamics simulation. The local atomic level strain was determined through the anisotropic pair-density function (PDF). The atomic level strain has a limited spatial extension whose range is dependent on the strain rate and extrapolates to zero at the critical strain rate. A failure event is identified with altering the local topology of atomic connectivity by exchanging bonds among neighboring atoms. 	
1204.6098v1	http://arxiv.org/pdf/1204.6098v1	2012	On Locality in Distributed Storage Systems	Ankit Singh Rawat|Sriram Vishwanath	  This paper studies the design of codes for distributed storage systems (DSS) that enable local repair in the event of node failure. This paper presents locally repairable codes based on low degree multivariate polynomials. Its code construction mechanism extends work on Noisy Interpolating Set by Dvir et al. \cite{dvir2011}. The paper presents two classes of codes that allow node repair to be performed by contacting 2 and 3 surviving nodes respectively. It further shows that both classes are good in terms of their rate and minimum distance, and allow their rate to be bartered for greater flexibility in the repair process. 	
1205.4922v2	http://arxiv.org/pdf/1205.4922v2	2013	On "Novel attractive forces" between ions in quantum plasmas -- failure   of linearized quantum hydrodynamics	M. Bonitz|E. Pehlke|T. Schoof	  In a recent letter [P.K. Shukla and B. Eliasson, Phys. Rev. Lett. 108, 165007 (2012)] the discovery of a new attractive force between protons in a hydrogen plasma was reported that would be responsible for the formation of molecules and of a proton lattice. Here we show, based on ab initio density functional theory calculations, that these predictions are wrong and caused by using linearized quantum hydrodynamics beyond the limits of its applicability. 	
1210.7982v1	http://arxiv.org/pdf/1210.7982v1	2012	Derivation of the Johnson-Samwer $T^{(2/3)}$ Temperature Dependence of   the Yield Strain in Metallic Glasses	Ratul Dasgupta|Ashwin Joy|H. G. E. Hentschel|Itamar Procaccia	  Metallic Glasses are prone to fail mechanically via a shear-banding instability. In a remarkable paper Johnson and Samwer demonstrated that this failure enjoys a high degree of universality in the sense that a large group of metallic glasses appears to possess a yield-strain that decreases with temperature following a $-T^{2/3}$ law up to logarithmic corrections. In this Letter we offer a theoretical derivation of this law. We show that our formula fits very well simulational data on typical amorphous solids. 	
1212.5292v2	http://arxiv.org/pdf/1212.5292v2	2015	Low-Temperature Magnetization Dynamics of Magnetic Molecular Solids in a   Swept Field	Erik Lenferink|Avinash Vijayaraghavan|Anupam Garg	  The swept-field experiments on magnetic molecular solids such as \Fe8 are studied using Monte Carlo simulations. A kinetic equation is developed to understand the phenomenon. It is found that the simulations provide a quantitatively accurate account of the experiments. The kinetic equation provides a similarly accurate account except at very low sweep velocities, where it fails modestly. This failure is due to the neglect of short-range correlations between the dipolar magnetic fields seen by the molecular spins. Both the simulations and the kinetic equation provide a good understanding of the distribution of these dipolar fields. 	
1301.0605v1	http://arxiv.org/pdf/1301.0605v1	2012	Loopy Belief Propogation and Gibbs Measures	Sekhar Tatikonda|Michael I. Jordan	  We address the question of convergence in the loopy belief propagation (LBP) algorithm. Specifically, we relate convergence of LBP to the existence of a weak limit for a sequence of Gibbs measures defined on the LBP s associated computation tree.Using tools FROM the theory OF Gibbs measures we develop easily testable sufficient conditions FOR convergence.The failure OF convergence OF LBP implies the existence OF multiple phases FOR the associated Gibbs specification.These results give new insight INTO the mechanics OF the algorithm. 	
1301.6331v1	http://arxiv.org/pdf/1301.6331v1	2013	Optimal Locally Repairable Codes via Rank-Metric Codes	Natalia Silberstein|Ankit Singh Rawat|O. Ozan Koyluoglu|Sriram Vishwanath	  This paper presents a new explicit construction for locally repairable codes (LRCs) for distributed storage systems which possess all-symbols locality and maximal possible minimum distance, or equivalently, can tolerate the maximal number of node failures. This construction, based on maximum rank distance (MRD) Gabidulin codes, provides new optimal vector and scalar LRCs. In addition, the paper also discusses mechanisms by which codes obtained using this construction can be used to construct LRCs with efficient repair of failed nodes by combination of LRC with regenerating codes. 	
1302.0345v1	http://arxiv.org/pdf/1302.0345v1	2013	On the single-electron theory of quantum spin Hall effect in two   dimensional topological insulators	Yi-Dong Wu	  Recently we wrote a paper on the theory of the quantum spin Hall effect(QSHE) in two dimensional(2D) topological insulators(TIs)1 which have been considered as do not add much new insight to the exhaustively studied topic of TI within a single-electron picture by the referees. In this paper we review the papers on the mechanism of the QSHE which have significant influence on understanding of the subject. By illustrating the failures of the previous works we show our paper do contribute a different point of view to this topic, which we believe is not only a new but also the correct way to approach the problem at the single-electron level. 	
1306.6587v1	http://arxiv.org/pdf/1306.6587v1	2013	SPH-based simulation of multi-material asteroid collisions	Thomas I. Maindl|Christoph Schäfer|Roland Speith|Áron Süli|Emese Forgács-Dajka|Rudolf Dvorak	  We give a brief introduction to smoothed particle hydrodynamics methods for continuum mechanics. Specifically, we present our 3D SPH code to simulate and analyze collisions of asteroids consisting of two types of material: basaltic rock and ice. We consider effects like brittle failure, fragmentation, and merging in different impact scenarios. After validating our code against previously published results we present first collision results based on measured values for the Weibull flaw distribution parameters of basalt. 	
1308.2519v1	http://arxiv.org/pdf/1308.2519v1	2013	Damage accumulation in quasi-brittle fracture	Claudio Manzato|Mikko J. Alava|Stefano Zapperi	  The strength of quasi-brittle materials depends on the ensemble of defects inside the sample and on the way damage accumulates before failure. Using large scale numerical simulations of the random fuse model, we investigate the evolution of the microcrack distribution that is directly related to the strength distribution and its size effects. We show that the broadening of the distribution tail originates from the dominating microcracks in each sample and is related to a tendency of crack coalescence that increases with system size. We study how the observed behavior depends on the disorder present in the sample. 	
1404.0944v1	http://arxiv.org/pdf/1404.0944v1	2014	Negative Ion Sources: Magnetron and Penning	D. C. Faircloth	  The history of the magnetron and Penning electrode geometry is briefly outlined. Plasma generation by electrical discharge-driven electron impact ionization is described and the basic physics of plasma and electrodes relevant to magnetron and Penning discharges are explained. Negative ions and their applications are introduced, along with their production mechanisms. Caesium and surface production of negative ions are detailed. Technical details of how to build magnetron and Penning surface plasma sources are given, along with examples of specific sources from around the world. Failure modes are listed and lifetimes compared. 	
1408.4052v1	http://arxiv.org/pdf/1408.4052v1	2014	QCD. What else is needed for the Proton Structure Function?	Y. S. Kim	  While QCD can provide corrections to the parton distribution function, it cannot produce the distribution. Where is then the starting point for the proton structure function? The only known source is the quark-model wave function for the proton at rest. The harmonic oscillator is used for the trial wave function. When Lorentz-boosted, this wave function exhibits all the peculiarities of Feynman's parton picture. The time-separation between the quarks plays the key role in the boosting process. This variable is hidden in the present form of quantum mechanics, and the failure to measure it leads to an increase in entropy. This leads to a picture of boiling quarks which become partons in their plasma state. 	
1502.01237v1	http://arxiv.org/pdf/1502.01237v1	2015	Running Identical Threads in C-Slow Retiming based Designs for   Functional Failure Detection	Tobias Strauch	  This paper shows the usage of C-Slow Retiming (CSR) in safety critical and low power applications. CSR generates C copies of a design by reusing the given logic resources in a time sliced fashion. When all C design copies are stimulated with the same input values, then all C design copies should behave the same way and will therefore create a redundant system. The paper shows that this special method of using CSR offers great benefits when used in safety critical and low power applications. Additional optimization techniques towards reducing register count are shown and an on-the-fly recovery mechanism is discussed. 	
1504.06274v1	http://arxiv.org/pdf/1504.06274v1	2015	A new approach for physiological time series	Dong Mao|Yang Wang|Qiang Wu	  We developed a new approach for the analysis of physiological time series. An iterative convolution filter is used to decompose the time series into various components. Statistics of these components are extracted as features to characterize the mechanisms underlying the time series. Motivated by the studies that show many normal physiological systems involve irregularity while the decrease of irregularity usually implies the abnormality, the statistics for "outliers" in the components are used as features measuring irregularity. Support vector machines are used to select the most relevant features that are able to differentiate the time series from normal and abnormal systems. This new approach is successfully used in the study of congestive heart failure by heart beat interval time series. 	
1506.01508v1	http://arxiv.org/pdf/1506.01508v1	2015	Escaping the Tragedy of the Commons through Targeted Punishment	Samuel Johnson	  Failures of cooperation cause many of society's gravest problems. It is well known that cooperation among many players faced with a social dilemma can be maintained thanks to the possibility of punishment, but achieving the initial state of widespread cooperation is often much more difficult. We show here that there exist strategies of `targeted punishment' whereby a small number of punishers can shift a population of defectors into a state of global cooperation. The heterogeneity of players, often regarded as an obstacle, can in fact boost the mechanism's effectivity. We conclude by outlining how the international community could use a strategy of this kind to combat climate change. 	
1509.01670v1	http://arxiv.org/pdf/1509.01670v1	2015	Mesoscopic description of random walks on combs	Vicenc Mendez|Alexander Iomin|Daniel Campos|Werner Horsthemke	  Combs are a simple caricature of various types of natural branched structures, which belong to the category of loopless graphs and consist of a backbone and branches. We study continuous time random walks on combs and present a generic method to obtain their transport properties. The random walk along the branches may be biased, and we account for the effect of the branches by renormalizing the waiting time probability distribution function for the motion along the backbone. We analyze the overall diffusion properties along the backbone and find normal diffusion, anomalous diffusion, and stochastic localization (diffusion failure), respectively, depending on the characteristics of the continuous time random walk along the branches. 	
1510.03718v1	http://arxiv.org/pdf/1510.03718v1	2015	Elastic interactions between 2D geometric defects	Michael Moshe|Eran Sharon|Raz Kupferman	  In this paper, we introduce a methodology applicable to a wide range of localized two-dimensional sources of stress. This methodology is based on a geometric formulation of elasticity. Localized sources of stress are viewed as singular defects---point charges of the curvature associated with a reference metric. The stress field in the presence of defects can be solved using a scalar stress function that generalizes the classical Airy stress function to the case of materials with nontrivial geometry. This approach allows the calculation of interaction energies between various types of defects. We apply our methodology to two physical systems: shear-induced failure of amorphous materials and the mechanical interaction between contracting cells. 	
1510.07148v1	http://arxiv.org/pdf/1510.07148v1	2015	Mobility and Energy Conscious Clustering Protocol for Wireless Networks	Abhinav Singh|Awadhesh Kumar Singh	  In this paper we present a distributed clustering protocol for mobile wireless sensor networks. A large majority of research in clustering and routing algorithms for WSNs assume a static network and hence are rendered inefficient in cases of highly mobile sensor networks, which is an aspect addressed here. MECP is an energy efficient, mobility aware protocol and utilizes information about movement of sensor nodes and residual energy as attributes in network formation. It also provides a mechanism for fault tolerance to decrease packet data loss in case of cluster head failures. 	
1511.05705v1	http://arxiv.org/pdf/1511.05705v1	2015	Continuous Wire Reinforcement for Jammed Granular Architecture	Matthias Fauconneau|Falk K. Wittel|Hans J. Herrmann	  The mechanical behavior of continuous fiber reinforced granular columns is simulated by means of a Discrete Element Model. Spherical particles are randomly deposited simultaneously with a wire, that is deployed following different patterns inside of a flexible cylinder for triaxial compression testing. We quantify the effect of three different fiber deployment patterns on the failure envelope, represented by Mohr-Coulomb cones, and derive suggestions for improved deployment strategies. 	
1601.05167v1	http://arxiv.org/pdf/1601.05167v1	2016	Breakdown of the Isobaric Multiplet Mass Equation as An Effect of the   Isospin-Symmetry Breaking	J. M. Dong|W. Zuo|J. Z. Gu	  The breakdown of the quadratic form of isobaric multiplet mass equation (IMME), presents a long-standing challenge to the existing theoretical models. In particular, recent high-precision nuclear mass measurements have indicated a dramatic failure of the IMME for several isobaric multiplets. We propose a new mechanism that the isospin-projection $T_z$ dependence of the 1st-order symmetry energy coefficient (SEC) drives a significant breakdown of the IMME, where the 1st-order SEC is primarily induced by the isospin-symmetry breaking (ISB) of strong nuclear force. Completely different from the existing knowledge, the deviation from the IMME cannot be measured simply by the high-order terms such as cubic term $dT_{z}^3$. 	
1602.01240v1	http://arxiv.org/pdf/1602.01240v1	2016	On the apparent failure of the topological theory of phase transitions	Matteo Gori|Roberto Franzosi|Marco Pettini	  The topological theory of phase transitions has its strong point in two theorems proving that, for a wide class of physical systems, phase transitions necessarily stem from topological changes of some submanifolds of configuration space. It has been recently argued that the $2D$ lattice $\phi^4$-model provides a counterexample that falsifies this theory. It is here shown that this is not the case: the phase transition of this model stems from an asymptotic ($N\to\infty$) change of topology of the energy level sets, in spite of the absence of critical points of the potential in correspondence of the transition energy. 	
1603.08898v2	http://arxiv.org/pdf/1603.08898v2	2016	Fracture initiation in multi-phase materials: a statistical   characterization of microstructural damage sites	T. W. J. de Geus|J. E. P. van Duuren|R. H. J. Peerlings|M. G. D. Geers	  Understanding the microstructural influence on the failure mechanisms in multi-phase materials calls for the identification of the worst-case scenario. This necessitates a statistical approach. By performing simulations directly based on micrographs, such an approach becomes feasible. This is applied here to extract the average microstructure around damage sites. 	
1605.07734v1	http://arxiv.org/pdf/1605.07734v1	2016	Recursive SDN for Carrier Networks	James McCauley|Zhi Liu|Aurojit Panda|Teemu Koponen|Barath Raghavan|Jennifer Rexford|Scott Shenker	  Control planes for global carrier networks should be programmable (so that new functionality can be easily introduced) and scalable (so they can handle the numerical scale and geographic scope of these networks). Neither traditional control planes nor new SDN-based control planes meet both of these goals. In this paper, we propose a framework for recursive routing computations that combines the best of SDN (programmability) and traditional networks (scalability through hierarchy) to achieve these two desired properties. Through simulation on graphs of up to 10,000 nodes, we evaluate our design's ability to support a variety of routing and traffic engineering solutions, while incorporating a fast failure recovery mechanism. 	
1606.00111v2	http://arxiv.org/pdf/1606.00111v2	2016	It's Time: OS Mechanisms for Enforcing Asymmetric Temporal Integrity	Anna Lyons|Gernot Heiser	  Mixed-criticality systems combine real-time components of different levels of criticality, i.e. severity of failure, on the same processor, in order to obtain good resource utilisation. They must guarantee deadlines of highly-critical tasks at the expense of lower-criticality ones in the case of overload. Present operating systems provide inadequate support for this kind of system, which is of growing importance in avionics and other verticals. We present an approach that provides the required asymmetric integrity and its implementation in the high-assurance seL4 microkernel. 	
1612.08554v1	http://arxiv.org/pdf/1612.08554v1	2016	A Fidelity Susceptibility Approach to Quantum Annealing of NP-hard   problems	Jun Takahashi|Koji Hukushima	  The computational complexity conjecture of NP $\nsubseteq$ BQP implies that there should be an exponentially small energy gap for Quantum Annealing (QA) of NP-hard problems. We aim to verify how this computation originated gapless point could be understood based on physics, using the quantum Monte Carlo method. As a result, we found a phase transition detectable only by the divergence of fidelity susceptibility. The exponentially small gapless points of each instance are all located in the phase found in this study, which suggests that this phase transition is the physical cause of the failure of QA for NP-hard problems. 	
1701.01234v4	http://arxiv.org/pdf/1701.01234v4	2017	Failure of deterministic and stochastic thermostats to control   temperature of molecular systems	Hiroshi Watanabe	  We investigate the ergodicity and "hot solvent/cold solute" problems in molecular dynamics simulations. While the kinetic moments and the stimulated Nos\'e--Hoover methods improve the ergodicity of a harmonic-oscillator system, both methods exhibit the "hot solvent/cold solute" problem in a binary liquid system. These results show that the devices to improve the ergodicity do not resolve the "hot solvent/cold solute" problem. 	
1712.07662v1	http://arxiv.org/pdf/1712.07662v1	2017	Bosonization in Non-Relativistic CFTs	Carl Turner	  We demonstrate explicitly the correspondence between all protected operators in a 2+1 dimensional non-supersymmetric bosonization duality in the non-relativistic limit. Roughly speaking we consider $SU(N)$ Chern-Simons field theory at level $k$ with $N_f$ flavours of fundamental boson, and match its chiral sector to that of a $SU(k)$ theory at level $N$ with $N_f$ fundamental fermions. We present the matching at the level of indices and individual operators, seeing the mechanism of failure for $N_f > N$, and point out that the non-relativistic setting is a particularly friendly setting for studying interesting questions about such dualities. 	
1802.01083v2	http://arxiv.org/pdf/1802.01083v2	2018	Mesoscopic Description of the Equal Load Sharing Fiber Bundle Model	Martin Hendrick|Srutarshi Pradhan|Alex Hansen	  One aim of the equal load sharing fiber bundle model is to describe the critical behavior of failure events. One way of accomplishing this, is through a discrete recursive dynamics. We introduce a continuous mesoscopic equation catching the critical behavior found through recursive dynamics. It allows us to formulate the model using the unifying framework of absorbing phase transitions traditionally used in the study of non-equilibrium phase transitions. Consequently, this work is a first step towards a field theory for fiber bundle models. 	
1803.01110v1	http://arxiv.org/pdf/1803.01110v1	2018	Nickel Titanium Alloy failure analysis under thermal cycling and   mechanical Loading: A Preliminary Study	Mahdi Mohajeri|Behrouz Haghgouyan|Homero Castaneda|Dimitris C. Lagoudas	  The electrochemical frequency modulation (EFM) technique can consider as a new tool for electrochemical corrosion monitoring. The calculation of corrosion rate with a non-destructive and rapid technique is a necessity to study corrosion behavior of metals under loading and thermal cycling. NiTi shape memory alloy (SMA) is characterized by differential scanning calorimetry (DSC) and uniaxial tensile testing. The corrosion behavior and reliability of technique have been examined for NiTi sample in artificial physiological solution. The results show the sensitivity of EFM technique to temperature and base frequencies. 	
0001425v1	http://arxiv.org/pdf/cond-mat/0001425v1	2000	The critical earthquake concept applied to mine rockbursts with   time-to-failure analysis	G. Ouillon|D. Sornette	  We report new tests of the critical earthquake concepts performed on rockbursts in deep South African mines. We extend the concept of an optimal time and space correlation region and test it on the eight main shocks of our catalog provided by ISSI. In a first test, we use the simplest signature of criticality in terms of a power law time-to-failure formula. Notwithstanding the fact that the search for the optimal correlation size is performed with this simple power law, we find evidence both for accelerated seismicity and for the presence of logperiodic behavior with a prefered scaling factor close to 2. We then propose a new algorithm based on a space and time smoothing procedure, which is also intended to account for the finite range and time mechanical interactions between events. This new algorithm provides a much more robust and efficient construction of the optimal correlation region, which allows us the use of the logperiodic formula directly in the search process. In this preliminary work, we have only tested the new algorithm on the largest event on the catalog. The result is of remarkable good quality with a dramatic improvement in accuracy and robustness. This confirms the potential importance of logperiodic signals. Our study opens the road for an efficient implemention of a systematic testing procedure of real-time predictions. 	
0402557v2	http://arxiv.org/pdf/cond-mat/0402557v2	2004	A Biased Resistor Network Model for Electromigration Failure and Related   Phenomena in Metallic Lines	C. Pennetta|E. Alfinito|L. Reggiani|F. Fantini|I. DeMunari|A. Scorzoni	  Electromigration phenomena in metallic lines are studied by using a biased resistor network model. The void formation induced by the electron wind is simulated by a stochastic process of resistor breaking, while the growth of mechanical stress inside the line is described by an antagonist process of recovery of the broken resistors. The model accounts for the existence of temperature gradients due to current crowding and Joule heating. Alloying effects are also accounted for. Monte Carlo simulations allow the study within a unified theoretical framework of a variety of relevant features related to the electromigration. The predictions of the model are in excellent agreement with the experiments and in particular with the degradation towards electrical breakdown of stressed Al-Cu thin metallic lines. Detailed investigations refer to the damage pattern, the distribution of the times to failure (TTFs), the generalized Black's law, the time evolution of the resistance, including the early-stage change due to alloying effects and the electromigration saturation appearing at low current densities or for short line lengths. The dependence of the TTFs on the length and width of the metallic line is also well reproduced. Finally, the model successfully describes the resistance noise properties under steady state conditions. 	
1006.3724v1	http://arxiv.org/pdf/1006.3724v1	2010	A Peer-to-Peer Middleware Framework for Resilient Persistent Programming	Alan Dearle|Graham Kirby|Stuart Norcross|Andrew McCarthy	  The persistent programming systems of the 1980s offered a programming model that integrated computation and long-term storage. In these systems, reliable applications could be engineered without requiring the programmer to write translation code to manage the transfer of data to and from non-volatile storage. More importantly, it simplified the programmer's conceptual model of an application, and avoided the many coherency problems that result from multiple cached copies of the same information. Although technically innovative, persistent languages were not widely adopted, perhaps due in part to their closed-world model. Each persistent store was located on a single host, and there were no flexible mechanisms for communication or transfer of data between separate stores. Here we re-open the work on persistence and combine it with modern peer-to-peer techniques in order to provide support for orthogonal persistence in resilient and potentially long-running distributed applications. Our vision is of an infrastructure within which an application can be developed and distributed with minimal modification, whereupon the application becomes resilient to certain failure modes. If a node, or the connection to it, fails during execution of the application, the objects are re-instantiated from distributed replicas, without their reference holders being aware of the failure. Furthermore, we believe that this can be achieved within a spectrum of application programmer intervention, ranging from minimal to totally prescriptive, as desired. The same mechanisms encompass an orthogonally persistent programming model. We outline our approach to implementing this vision, and describe current progress. 	
1109.3561v1	http://arxiv.org/pdf/1109.3561v1	2011	Universal adaptive self-stabilizing traversal scheme: random walk and   reloading wave	Thibault Bernard|Alain Bui|Devan Sohier	  In this paper, we investigate random walk based token circulation in dynamic environments subject to failures. We describe hypotheses on the dynamic environment that allow random walks to meet the important property that the token visits any node infinitely often. The randomness of this scheme allows it to work on any topology, and require no adaptation after a topological change, which is a desirable property for applications to dynamic systems. For random walks to be a traversal scheme and to answer the concurrence problem, one needs to guarantee that exactly one token circulates in the system. In the presence of transient failures, configurations with multiple tokens or with no token can occur. The meeting property of random walks solves the cases with multiple tokens. The reloading wave mechanism we propose, together with timeouts, allows to detect and solve cases with no token. This traversal scheme is self-stabilizing, and universal, meaning that it needs no assumption on the system topology. We describe conditions on the dynamicity (with a local detection criterion) under which the algorithm is tolerant to dynamic reconfigurations. We conclude by a study on the time between two visits of the token to a node, which we use to tune the parameters of the reloading wave mechanism according to some system characteristics. 	
1305.0989v1	http://arxiv.org/pdf/1305.0989v1	2013	The Dynamics of Rapid Fracture: Instabilities, Nonlinearities and Length   Scales	Eran Bouchbinder|Tamar Goldman|Jay Fineberg	  The failure of materials and interfaces is mediated by cracks, nearly singular dissipative structures that propagate at velocities approaching the speed of sound. Crack initiation and subsequent propagation -- the dynamic process of fracture -- couples a wide range of time and length scales. Crack dynamics challenge our understanding of the fundamental physics processes that take place in the extreme conditions within the nearly singular region where material failure occurs. Here, we first briefly review the classic approach to dynamic fracture, "Linear Elastic Fracture Mechanics" (LEFM), and discuss its successes and limitations. We show how, on the one hand, recent experiments performed on straight cracks propagating in soft brittle materials have quantitatively confirmed the predictions of this theory to an unprecedented degree. On the other hand, these experiments show how LEFM breaks down as the singular region at the tip of a crack is approached. This breakdown naturally leads to a new theoretical framework coined "Weakly Nonlinear Fracture Mechanics", where weak elastic nonlinearities are incorporated. The stronger singularity predicted by this theory gives rise to a new and intrinsic length scale, $\ell_{nl}$. These predictions are verified in detail through direct measurements. We then theoretically and experimentally review how the emergence of $\ell_{nl}$ is linked to a new equation for crack motion, which predicts the existence of a high-speed oscillatory crack instability whose wave-length is determined by $\ell_{nl}$. We conclude by delineating outstanding challenges in the field. 	
1406.2087v1	http://arxiv.org/pdf/1406.2087v1	2014	Mechanical Properties and Plasticity of a Model Glass Loaded Under   Stress Control	Vladimir Dailidonis|Valery Ilyin|Pankaj Mishra|Itamar Procaccia	  Much of the progress achieved in understanding plasticity and failure in amorphous solids had been achieved using experiments and simulations in which the materials were loaded using strain control. There is paucity of results under stress control. Here we present a new method that was carefully geared to allow loading under stress control either at $T=0$ or at any other temperature, using Monte-Carlo techniques. The method is applied to a model perfect crystalline solid, to a crystalline solid contaminated with topological defects, and to a generic glass. The highest yield stress belongs to the crystal, the lowest to the crystal with a few defects, with the glass in between. Although the glass is more disordered than the crystal with a few defects, it yields stress is much higher than that of the latter. We explain this fact   by considering the actual microscopic interactions that are typical to glass forming materials, pointing out the reasons for the higher cohesive nature of the glass. The main conclusion of this paper is that the instabilities encountered in stress-control condition are the identical saddle-node bifurcation seen in strain-control. Accordingly one can use the latter condition to infer about the former. Finally we discuss temperature effects and comment on the time needed to see a stress controlled material failure. 	
1603.01168v1	http://arxiv.org/pdf/1603.01168v1	2016	Numerical simulation of wave propagation and snow failure from explosive   loading	Rolf Sidler|Stephan Simioni|Jürg Dual|Jürg Schweizer	  Avalanche control by explosion is a widely applied method to minimize the avalanche risk to infrastructure in snow-covered mountain areas. However, the mechanisms involved leading from an explosion to the release of an avalanche are not well understood. Here we test the hypothesis that weak layers fail due to the stress caused by propagating acoustic waves. The underlying mechanism is that the stress induced by the acoustic waves exceeds the strength of the snow layers. We compare field measurements to a numerical simulation of acoustic wave propagation in a porous material. The simulation consists of an acoustic domain for the air above the snowpack and a poroelastic domain for the dry snowpack. The two domains are connected by a wave field decomposition and open pore boundary conditions. Empirical relations are used to derive a porous model of the snowpack from density profiles of the field experiment. Biot's equations are solved in the poroelastic domain to obtain simulated accelerations in the snowpack and a time dependent stress field. Locations of snow failure were identified by comparing the principal normal and shear stress fields to snow strength which is assumed to be a function of snow porosity. One air pressure measurement above the snowpack was used to calibrate the pressure amplitude of the source in the simulation. Additional field measurements of air pressure and acceleration measurements inside the snowpack were compared to individual field variables of the simulation. The acceleration of the air flowing inside the pore space of the snowpack was identified to have the highest correlation to the acceleration measurements in the snowpack. 	
1603.08910v2	http://arxiv.org/pdf/1603.08910v2	2016	Fracture in multi-phase materials: why some microstructures are more   critical than others	T. W. J. de Geus|R. H. J. Peerlings|M. G. D. Geers	  Our goal is to unravel the mechanisms that lead to failure of a ductile two-phase material - that consists of a ductile soft phase and a relatively brittle hard phase. An idealized microstructural model is used to study damage propagation systematically and transparently. The analysis uncovers distinct microstructural features around early voids, whereby regions of the hard phase are aligned with the tensile axis and regions of the soft phase are aligned with the shear directions. These features are consistently found in regions exhibiting damage propagation, whereby the damage remains initiation driven, i.e. voids nucleate independently of each other. Upon localization, damage is controlled on a longer length-scale relying on a critical relative position of 'initiation hot-spots'. The damage rapidly increases in bands of the soft phase wherein several voids are aligned with the shear directions. The relative arrangement of the voids determines whether the microstructure fails early, or at a substantially higher strain. Although much research is needed to refine these findings for real or more realistic microstructures, in particular in three-dimensions, this paper opens a route to a deeper understanding of the ultimate failure of multi-phase materials. 	
1605.06178v1	http://arxiv.org/pdf/1605.06178v1	2016	Lattice Discrete Particle Model (LDPM) for pressure-dependent   inelasticity in granular rocks	Shiva Esna Ashari|Giuseppe Buscarnera|Gianluca Cusatis	  This paper deals with the formulation, calibration, and validation of a Lattice Discrete Particle Model (LDPM) for the simulation of the pressure-dependent inelastic response of granular rocks. LDPM is formulated in the framework of discrete mechanics and it simulates the heterogeneous deformation of cemented granular systems by means of discrete compatibility/equilibrium equations defined at the grain scale. A numerical strategy is proposed to generate a realistic microstructure based on the actual grain size distribution of a sandstone and the capabilities of the method are illustrated with reference to the particular case of Bleurswiller sandstone, i.e. a granular rock that has been extensively studied at the laboratory scale. LDPM micromechanical parameters are calibrated based on evidences from triaxial experiments, such as hydrostatic compression, brittle failure at low confinement and plastic behavior at high confinement. Results show that LDPM allows exploring the effect of fine-scale heterogeneity on the inelastic response of rock cores, achieving excellent quantitative performance across a wide range of stress conditions. In addition, LDPM simulations demonstrate its capability of capturing different modes of strain localization within a unified mechanical framework, which makes this approach applicable for a wide variety of geomechanical settings. Such promising performance suggests that LDPM may constitute a viable alternative to existing discrete numerical methods for granular rocks, as well as a versatile tool for the interpretation of their complex deformation/failure patterns and for the development of continuum models capturing the effect of micro-scale heterogeneity. 	
1702.01963v1	http://arxiv.org/pdf/1702.01963v1	2017	Seamless Handover in IP over ICN Networks: a Coding Approach	Mohammed Al-Khalidi|Nikolaos Thomos|Martin J. Reed|Mays F. AL-Naday|Dirk Trossen	  Seamless connectivity plays a key role in realising QoS-based delivery in mobile networks. However, current handover mechanisms hinder the ability to meet this target, due to the high ratio of handover failures, packet loss and service interruption. These challenges are further magnified in Heterogeneous Cellular Networks (HCN) such as Advanced Long Term Evolution (LTE-Advanced) and LTE in unlicensed spectrum (LTE-LAA), due to the variation in handover requirements. Although mechanisms, such as Fast Handover for Proxy Mobile IPv6 (PFMIPv6), attempt to tackle these issues; they come at a high cost with sub-optimal outcomes. This primarily stems from various limitations of existing IP core networks. In this paper we propose a novel handover solution for mobile networks, exploiting the advantages of a revolutionary IP over Information-Centric Networking (IP-over-ICN) architecture in supporting flexible service provisioning through anycast and multicast, combined with the advantages of random linear coding techniques in eliminating the need for retransmissions. Our solution allows coded traffic to be disseminated in a multicast fashion during handover phase from source directly to the destination(s), without the need for an intermediate anchor as in exiting solutions; thereby, overcoming packet loss and handover failures, while reducing overall delivery cost. We evaluate our approach with an analytical and simulation model showing significant cost reduction compared to PFMIPv6. 	
1707.08928v2	http://arxiv.org/pdf/1707.08928v2	2017	Localizing softness and stress along loops in three-dimensional   topological metamaterials	Guido Baardink|Anton Souslov|Jayson Paulose|Vincenzo Vitelli	  Topological states can be used to control the mechanical properties of a material along an edge or around a localized defect. The surface rigidity of elastic networks is characterized by a bulk topological invariant called the polarization; materials with a well-defined uniform polarization display a dramatic range of edge softnesses depending on the orientation of the polarization relative to the terminating surface. However, in all three-dimensional mechanical metamaterials proposed to date, the topological edge modes are mixed with bulk soft modes and so-called Weyl loops. Here, we report the design of a gapped 3D topological metamaterial with a uniform polarization that displays a corresponding asymmetry between the number of soft modes on opposing surfaces and, in addition, no bulk soft modes. We then use this construction to localize topological soft modes in interior regions of the material by including defect structures---dislocation loops---that are unique to three dimensions. We derive a general formula that relates the difference in the number of soft modes and states of self-stress localized along the dislocation loop to the handedness of the vector triad formed by the lattice polarization, Burgers vector, and dislocation-line direction. Our findings suggest a novel strategy for pre-programming failure and softness localized along lines in 3D, while avoiding extended periodic failure modes associated with Weyl loops. 	
1708.07233v1	http://arxiv.org/pdf/1708.07233v1	2017	Reliability and Fault-Tolerance by Choreographic Design	Ian Cassar|Adrian Francalanza|Claudio Antares Mezzina|Emilio Tuosto	  Distributed programs are hard to get right because they are required to be open, scalable, long-running, and tolerant to faults. In particular, the recent approaches to distributed software based on (micro-)services where different services are developed independently by disparate teams exacerbate the problem. In fact, services are meant to be composed together and run in open context where unpredictable behaviours can emerge. This makes it necessary to adopt suitable strategies for monitoring the execution and incorporate recovery and adaptation mechanisms so to make distributed programs more flexible and robust. The typical approach that is currently adopted is to embed such mechanisms in the program logic, which makes it hard to extract, compare and debug. We propose an approach that employs formal abstractions for specifying failure recovery and adaptation strategies. Although implementation agnostic, these abstractions would be amenable to algorithmic synthesis of code, monitoring and tests. We consider message-passing programs (a la Erlang, Go, or MPI) that are gaining momentum both in academia and industry. Our research agenda consists of (1) the definition of formal behavioural models encompassing failures, (2) the specification of the relevant properties of adaptation and recovery strategy, (3) the automatic generation of monitoring, recovery, and adaptation logic in target languages of interest. 	
1802.02258v1	http://arxiv.org/pdf/1802.02258v1	2018	A computational framework for microstructural modelling of   polycrystalline materials with damage and failure	Vincenzo Gulizzi	  In the present thesis, a computational framework for the analysis of the deformation and damage phenomena occurring at the micro-scale of polycrystalline materials is presented.   Micro-mechanics studies are commonly performed using the Finite Element Method (FEM) for its versatility and robustness. However, finite element formulations usually lead to an extremely high number of degrees of freedom of the considered micro-structures, thus making alternative formulations of great engineering interest. Among the others, the Boundary Element Method (BEM) represents a viable alternative to FEM approaches as it allows to express the problem in terms of boundary values only, thus reducing the total number of degrees of freedom.   The computational framework developed in this thesis is based on a non-linear multi-domain BEM approach for generally anisotropic materials and is devoted to the analysis of three-dimensional polycrystalline microstructures. Different theoretical and numerical aspects of the polycrystalline problem using the boundary element method are investigated: first, being the formulation based on a integral representation of the governing equations, a novel and more compact expression of the integration kernels capable of representing the multi-field behaviour of generally anisotropic materials is presented; second, the sources of the high computational cost of polycrystalline analyses are identified and suitably treated by means of different strategies including an ad-hoc grain boundary meshing technique developed to tackle the large statistical variability of polycrystalline micro-morphologies; third, non-linear deformation and failure mechanisms such as inter-granular and trans-granular cracking and generally anisotropic crystal plasticity are studied and the numerical results presented throughout the thesis demonstrate the potential of the developed framework. 	
1508.03000v1	http://arxiv.org/pdf/1508.03000v1	2015	Few common failure cases in mobile robots	Ramviyas Parasuraman	  A mobile robot deployed for remote inspection, surveying or rescue missions can fail due to various possibilities and can be hardware or software related. These failure scenarios necessitate manual recovery (self-rescue) of the robot from the environment. It would bring unforeseen challenges to recover the mobile robot if the environment where it was deployed had hazardous or harmful conditions (e.g. ionizing radiations). While it is not fully possible to predict all the failures in the robot, failures can be reduced by employing certain design/usage considerations. Few example failure cases based on real experiences are presented in this short article along with generic suggestions on overcoming the illustrated failure situations. 	
0105277v1	http://arxiv.org/pdf/cond-mat/0105277v1	2001	Mechanical properties and formation mechanisms of a wire of single gold   atoms	G. Rubio-Bollinger|S. R. Bahn|N. Agrait|K. W. Jacobsen|S. Vieira	  A scanning tunneling microscope (STM) supplemented with a force sensor is used to study the mechanical properties of a novel metallic nanostructure: a freely suspended chain of single gold atoms. We find that the bond strength of the nanowire is about twice that of a bulk metallic bond. We perform ab initio calculations of the force at chain fracture and compare quantitatively with experimental measurements. The observed mechanical failure and nanoelastic processes involved during atomic wire fabrication are investigated using molecular dynamics (MD) simulations, and we find that the total effective stiffness of the nanostructure is strongly affected by the detailed local atomic arrangement at the chain bases. 	
0510009v1	http://arxiv.org/pdf/cond-mat/0510009v1	2005	Mechanical Properties of Viral Capsids	Roya Zandi|David Reguera	  Viruses are known to tolerate wide ranges of pH and salt conditions and to withstand internal pressures as high as 100 atmospheres. In this paper we investigate the mechanical properties of viral capsids, calling explicit attention to the inhomogeneity of the shells that is inherent to their discrete and polyhedral nature. We calculate the distribution of stress in these capsids and analyze their response to isotropic internal pressure (arising, for instance, from genome confinement and/or osmotic activity). We compare our results with appropriate generalizations of classical (i.e., continuum) elasticity theory. We also examine competing mechanisms for viral shell failure, e.g., in-plane crack formation versus radial bursting. The biological consequences of the special stabilities and stress distributions of viral capsids are also discussed. 	
0805.3184v1	http://arxiv.org/pdf/0805.3184v1	2008	The clouds of physics and Einstein's last query: Can quantum mechanics   be derived from general relativity?	Friedwardt Winterberg	  Towards the end of the 19th century, Kelvin pronounced as the "clouds of physics" 1) the failure of the Michelson-Morely experiment to detect an ether wind, 2) the violation of the classical mechanical equipartition theorem in statistical thermodynamics. And he believed that the removal of these clouds would bring physics to an end. But as we know, the removal of these clouds led to the two great breakthoughts of modern physics: 1) The theory of relativity, and 2) to quantum mechanics. Towards the end of the 20th century more clouds of physics became apparent. They are 1) the riddle of quantum gravity, 2) the superluminal quantum correlations, 3) the small cosmological constant. Furthermore, there is the riddle of dark energy making up 70% of the physical universe, the non-baryonic cold dark matter making up 26% and the very small initial entropy of the universe. An attempt is made to explain the importance of these clouds for the future of physics. Conjectures for a possible solution are presented. they have to do with Einstein's last query: "Can quantum mechanics be derived general relativity", and with the question is there an ether? 	
1012.2516v1	http://arxiv.org/pdf/1012.2516v1	2010	An Efficient Security Mechanism for High-Integrity Wireless Sensor   Networks	Jaydip Sen	  Wireless sensor networks (WSNs) have recently attracted a lot of interest in the research community due their wide range of applications. Unfortunately, these networks are vulnerable to numerous security threats that can adversely affect their proper functioning. This problem is more critical if the network is deployed for some mission-critical applications such as in a tactical battlefield. Random failure of nodes and intentional compromise of nodes by an insider attack in a WSN pose particularly difficult challenges to security engineers as these attacks cannot be defended by traditional cryptography-based mechanisms. In this paper, a security solution is proposed for detecting compromised and faulty nodes in a WSN. The mechanism also isolates a compromised node from the network so that it cannot participate in any network activity. The proposed mechanism is based on misbehavior classification, behaviour monitoring and trust management. It involves minimum computation and communication overhead and is ideally suited for a resource-constrained, high-integrity WSN. 	
1111.0380v2	http://arxiv.org/pdf/1111.0380v2	2012	An Efficient Security Mechanism for High-Integrity Wireless Sensor   Networks	Jaydip Sen|Sripad Krishna	  Wireless sensor networks (WSNs) have recently attracted a lot of interest in the research community due their wide range of applications. Unfortunately, these networks are vulnerable to numerous security threats that can adversely affect their proper functioning. This problem is more critical if the network is deployed for some mission-critical applications such as in a tactical battlefield. Random failure of nodes and intentional compromise of nodes by an insider attack in a WSN pose particularly difficult challenges to security engineers as these attacks cannot be defended by traditional cryptography-based mechanisms. In this paper, a security solution is proposed for detecting compromised and faulty nodes in a WSN. The mechanism also isolates a compromised node from the network so that it cannot participate in any network activity. The proposed mechanism is based on misbehavior classification, behaviour monitoring and trust management. It involves minimum computation and communication overhead and is ideally suited for a resource-constrained, high-integrity WSN. 	
1211.0355v3	http://arxiv.org/pdf/1211.0355v3	2013	Ideal Strength of Doped Graphene	S. J. Woo|Young-Woo Son	  While the mechanical distortions change the electronic properties of graphene significantly, the effects of electronic manipulation on its mechanical properties have not been known. Using first-principles calculation methods, we show that, when graphene expands isotropically under equibiaxial strain, both the electron and hole doping can maintain or improve its ideal strength slightly and enhance the critical breaking strain dramatically. Contrary to the isotropic expansions, the electron doping decreases the ideal strength as well as critical strain of uniaxially strained graphene while the hole doping increases the both. Distinct failure mechanisms depending on type of strains are shown to be origins of the different doping induced mechanical stabilities. Our findings may resolve a contradiction between recent experimental and theoretical results on the strength of graphene. 	
1402.1006v1	http://arxiv.org/pdf/1402.1006v1	2014	Role of defects and geometry in the strength of polycrystalline graphene	Zhigong Song|Jian Wu|Zhiping Xu	  Defects in solid commonly limit mechanical performance of the material. However, recent measurements reported that the extraordinarily high strength of graphene is almost retained with the presence of grain boundaries. We clarify in this work that lattice defects in the grain boundaries and distorted geometry thus induced define the mechanical properties characterized under specific loading conditions. Atomistic simulations and theoretical analysis show that tensile tests measure in-plane strength that is governed by defect-induced stress buildup, while nanoindentation probes local strength under the indenter tip and bears additional geometrical effects from warping. These findings elucidate the failure mechanisms of graphene under realistic loading conditions and assess the feasibility of abovementioned techniques in quantifying the strength of graphene, and suggest that mechanical properties of low-dimensional materials could be tuned by implanting defects and geometrical distortion they leads to. 	
1407.0382v2	http://arxiv.org/pdf/1407.0382v2	2014	Reversible mechanical and electrical properties of ripped graphene	J. Henry Hinnefeld|Stephen T. Gill|Shuze Zhu|William J. Swanson|Teng Li|Nadya Mason	  We examine the mechanical properties of graphene devices stretched on flexible elastomer substrates. Using atomic force microscopy, transport measurements, and mechanics simulations, we show that micro-rips form in the graphene during the initial application of tensile strain; however subsequent applications of the same tensile strain elastically open and close the existing rips. Correspondingly, while the initial tensile strain degrades the devices' transport properties, subsequent strain-relaxation cycles affect transport only moderately, and in a largely reversible fashion, yielding robust electrical transport even after partial mechanical failure. 	
1410.0454v1	http://arxiv.org/pdf/1410.0454v1	2014	Strain-enhanced tunneling magnetoresistance in MgO magnetic tunnel   junctions	Li Ming Loong|Xuepeng Qiu|Zhi Peng Neo|Praveen Deorani|Yang Wu|Charanjit S. Bhatia|Mark Saeys|Hyunsoo Yang	  While the effects of lattice mismatch-induced strain, mechanical strain, as well as the intrinsic strain of thin films are sometimes detrimental, resulting in mechanical deformation and failure, strain can also be usefully harnessed for applications such as data storage, transistors, solar cells, and strain gauges, among other things. Here, we demonstrate that quantum transport across magnetic tunnel junctions (MTJs) can be significantly affected by the introduction of controllable mechanical strain, achieving an enhancement factor of ~2 in the experimental tunneling magnetoresistance (TMR) ratio. We further correlate this strain-enhanced TMR with coherent spin tunneling through the MgO barrier. Moreover, the strain-enhanced TMR is analyzed using non-equilibrium Green's function (NEGF) quantum transport calculations. Our results help elucidate the TMR mechanism at the atomic level and can provide a new way to enhance, as well as tune, the quantum properties in nanoscale materials and devices. 	
1506.07503v1	http://arxiv.org/pdf/1506.07503v1	2015	Attention-Based Models for Speech Recognition	Jan Chorowski|Dzmitry Bahdanau|Dmitriy Serdyuk|Kyunghyun Cho|Yoshua Bengio	  Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks in- cluding machine translation, handwriting synthesis and image caption gen- eration. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in reaches a competitive 18.7% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the at- tention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6% level. 	
1601.08043v1	http://arxiv.org/pdf/1601.08043v1	2016	Mechanism for the stabilization of protein clusters above the solubility   curve: the role of non-ideal chemical reactions	James F. Lutsko	  Dense protein clusters are known to play an important role in nucleation of protein crystals from dilute solutions. While these have generally been thought to be formed from a metastable phase, the observation of similar, if not identical, clusters above the critical point for the dilute-solution/strong-solution phase transition has thrown this into doubt. Furthermore, the observed clusters are stable for relatively long times. Because protein aggregation plays an important role in some pathologies, understanding the nature of such clusters is an important problem. One mechanism for the stabilization of such structures was proposed by Pan, Vekilov and Lubchenko and was investigated using a DDFT model which confirmed the viability of the model. Here, we revisit that model and incorporate additional physics in the form of state-dependent reaction rates. We show by a combination of numerical results and general arguments that the state-dependent rates disrupt the stability mechanism. Finally, we argue that the state-depedent reactions correct unphysical aspects of the model with ideal (state-independent) reactions and that this necessarily leads to the failure of the proposed mechanism. 	
1602.00456v2	http://arxiv.org/pdf/1602.00456v2	2016	The Ideal Tensile Strength and Phonon Instability of Borophene	Haifeng Wang|Qingfang Li|Yan Gao|F. Miao|Xiang-Feng Zhou|X. G. Wan	  Very recently, two-dimensional(2D) boron sheets (borophene) with rectangular structure has been grown successfully on single crystal Ag(111) substrates.The fabricated boroprene is predicted to have unusual mechanical properties. We performed first-principle calculations to investigate the mechanical properties of the monolayer borophene, including ideal tensile strength and critical strain. It was found that monolayer borophene can withstand stress up to 20.26 N/m and 12.98 N/m in a and b directions, respectively.However, its critical strain was found to be small. In a direction, the critical value is only 8%, which, to the best of our knowledge, is the lowest among all studied 2D materials.Our numerical results show that the tensile strain applied in b direction enhances the bucking height of borophene resulting in an out-of-plane negative Poisson's ratio, which makes the boron sheet show superior mechanical flexibility along b direction.The failure mechanism and phonon instability of monolayer borophene were also explored. 	
1607.03141v1	http://arxiv.org/pdf/1607.03141v1	2016	Competing Mechanisms between Dislocation and Phase Transformation in   Plastic Deformation of Single Crystalline Yttria-Stabilized Tetragonal   Zirconia Nanopillars	Ning Zhang|Mohsen Asle Zaeem	  Molecular dynamics (MD) is employed to investigate the plastic deformation mechanisms of single crystalline yttria-stabilized tetragonal zirconia (YSTZ) nanopillars under uniaxial compression. Simulation results show that the nanoscale plastic deformation of YSTZ is strongly dependent on the crystallographic orientation of zirconia nanopillars. For the first time, the experimental explored tetragonal to monoclinic phase transformation is reproduced by MD simulations in some particular loading directions. Three distinct mechanisms of dislocation, phase transformation, and a combination of dislocation and phase transformation are identified when applying compressive loading along different directions. The strength of zirconia nanopillars exhibits a sensitive behavior depending on the failure mechanisms, such that the dislocation-mediated deformation leads to the lowest strength, while the phase transformation-dominated deformation results in the highest strength. 	
1609.08123v1	http://arxiv.org/pdf/1609.08123v1	2016	Strong Equivalence Principle in Polymer Quantum Mechanics and deformed   Heisenberg Algebra	Nirmalya Kajuri	  The Strong equivalence Principle (SEP) states that the description of a physical system in a gravitational field is indistinguishable from the description of the same system at rest in an accelerating frame. While this statement holds true in both General Relativity and ordinary Quantum Mechanics, one expects it to fail when quantum gravity corrections are taken into account. In this paper we investigate the possible failure of the SEP in two Quantum Gravity inspired modifications of Quantum Mechanics - Polymer Quantum Mechanics and deformed Heisenberg Algebra. We find that the SEP fails to hold in both these theories. We estimate the deviation from SEP and find in both cases that it is too small to be measured in present day experiments. 	
1609.08338v1	http://arxiv.org/pdf/1609.08338v1	2016	Mechanical stability of particle-stabilized droplets under micropipette   aspiration	Niveditha Samudrala|Jin Nam|Raphael Sarfati|Robert W. Style|Eric R. Dufresne	  We investigate the mechanical behavior of particle-stabilized droplets using micropipette aspiration. We observe that droplets stabilized with amphiphilic dumbbell-shaped particles exhibit a two-stage response to increasing suction pressure. Droplets first drip, then wrinkle and buckle like an elastic shell. While particles have a dramatic impact on the mechanism of failure, the mechanical strength of the droplets is only modestly increased. On the other hand, droplets coated with the molecular surfactant Sodium Dodecyl Sulfate are even weaker than bare droplets. In all cases, the magnitude of the critical pressure for the onset of instabilities is set by the fluid surface tension. 	
1706.01428v3	http://arxiv.org/pdf/1706.01428v3	2017	A correspondence between thermodynamics and inference	Colin H. LaMont|Paul A. Wiggins	  We systematically explore a natural analogy between Bayesian statistics and thermal physics in which sample size corresponds to inverse temperature. We discover that some canonical thermodynamic quantities already correspond to well-established statistical quantities. Motivated by physical insight into thermal physics, we define two novel statistical quantities: a learning capacity and Gibbs entropy. The definition of the learning capacity leads to a critical insight: The well-known mechanism of failure of the equipartition theorem in statistical mechanics is the mechanism for anomalously-predictive or sloppy models in statistics. This correspondence between the learning and heat capacities provides new insight into the mechanism of machine learning. The correspondence also suggests a solution to a long-standing difficulty in Bayesian statistics: the definition of an objective prior. We propose that the Gibbs entropy provides a natural generalization of the principle of indifference that defines objectivity. This approach unifies the disparate Bayesian, frequentist and information-based paradigms of statistics by achieving coherent inference between these competing formulations. 	
1709.08412v1	http://arxiv.org/pdf/1709.08412v1	2017	Non-local plasticity effects on notch fracture mechanics	Emilio Martínez-Pañeda|Susana del Busto|Covadonga Betegón	  We investigate the influence of gradient-enhanced dislocation hardening on the mechanics of notch-induced failure. The role of geometrically necessary dislocations (GNDs) in enhancing cracking is assessed by means of a mechanism-based strain gradient plasticity theory. Both stationary and propagating cracks from notch-like defects are investigated through the finite element method. A cohesive zone formulation incorporating monotonic and cyclic damage contributions is employed to address both loading conditions. Computations are performed for a very wide range of length scale parameters and numerous geometries are addressed, covering the main types of notches. Results reveal a strong influence of the plastic strain gradients in all the scenarios considered. Transitional combinations of notch angle, radius and length scale parameter are identified that establish the regimes of GNDs-relevance, laying the foundations for the rational application of gradient plasticity models in damage assessment of notched components. 	
1710.01117v2	http://arxiv.org/pdf/1710.01117v2	2017	A control mechanism for intramural periarterial drainage via astrocytes:   How neuronal activity could improve waste clearance from the brain	Alexandra K. Diem|Roxana O. Carare|Neil. W. Bressloff	  The mechanisms behind waste clearance from deep within the parenchyma of the brain remain unclear to this date. Experimental evidence has shown that one pathway for waste clearance, termed intramural periarterial drainage (IPAD), is the rapid drainage of interstitial fluid (ISF) via basement membranes (BM) of the smooth muscle cells (SMC) of cerebral arteries and its failure is closely associated with the pathology of Alzheimer's disease (AD). We have previously shown that arterial pulsations from the heart beat are not strong enough to drive waste clearance. Here we demonstrate computational evidence for a mechanism for cerebral waste clearance that is driven by functional hyperaemia, that is, the dilation of cerebral arteries as a consequence of increased neuronal demand. This mechanism is based on our model for fluid flow through the vascular basement membrane. It accounts for waste clearance rates observed in mouse experiments and aligns with pathological observations as well as recommendations to lower the individual risk of AD, such as keeping mentally and physically active. 	
1801.05639v1	http://arxiv.org/pdf/1801.05639v1	2018	Mechanical Properties of Schwarzites - A Fully Atomistic Reactive   Molecular Dynamics Investigation	Cristiano F. Woellner|Tiago Botari|Eric Perim|Douglas S. Galvao	  Schwarzites are crystalline, 3D porous structures with stable negative curvature formed of sp2-hybridized carbon atoms. These structures present topologies with tunable porous size and shape and unusual mechanical properties. In this work, we have investigated the mechanical behavior under compressive strains and energy absorption of four different Schwarzites, through reactive molecular dynamics simulations, using the ReaxFF force field as available in the LAMMPS code. We considered two Schwarzites families, the so-called Gyroid and Primitive and two structures from each family. Our results also show they exhibit remarkable resilience under mechanical compression. They can be reduced to half of their original size before structural failure (fracture) occurs. 	
1801.04015v1	http://arxiv.org/pdf/1801.04015v1	2018	Spatio-Temporal Pricing for Ridesharing Platforms	Hongyao Ma|Fei Fang|David C. Parkes	  Ridesharing platforms match drivers and riders to trips, using dynamic prices to balance supply and demand. A challenge is to set prices that are appropriately smooth in space and time, in the sense that drivers will choose to accept their dispatched trips, rather than drive to another area or wait for higher prices or a better trip. We introduce the Spatio-Temporal Pricing (STP) mechanism. The mechanism is incentive-aligned, in that it is a subgame-perfect equilibrium for drivers to accept their dispatches, and the mechanism is welfare-optimal, envy-free, individually rational and budget balanced from any history onward. We work in a complete information, discrete time, multi-period, multi-location model, and prove the existence of anonymous, origin-destination, competitive equilibrium (CE) prices. The STP mechanism employs driver-pessimal CE prices, and the proof of incentive alignment makes use of the $M^\natural$ concavity of min-cost flow objectives. The same connection to min-cost flow problems provides an efficient algorithm to compute an optimal matching and prices. We also give an impossibility result, that there can be no dominant-strategy mechanism with the same economic properties. An empirical analysis conducted in simulation suggests that the STP mechanism can achieve significantly higher social welfare than a myopic pricing mechanism, and highlights the failure of incentive alignment due to non-smooth prices in myopic mechanisms. 	
9905161v1	http://arxiv.org/pdf/cond-mat/9905161v1	1999	Failure time and microcrack nucleation	A. Guarino|S. Ciliberto|A. Garcimartin	  The failure time of samples of heterogeneous materials (wood, fiberglass) is studied as a function of the applied stress. It is shown that in these materials the failure time is predicted with a good accuracy by a model of microcrack nucleation proposed by Pomeau. It is also shown that the crack growth process presents critical features when the failure time is approached. 	
0012009v1	http://arxiv.org/pdf/cs/0012009v1	2000	Finding Failure Causes through Automated Testing	Holger Cleve|Andreas Zeller	  A program fails. Under which circumstances does this failure occur? One single algorithm, the delta debugging algorithm, suffices to determine these failure-inducing circumstances. Delta debugging tests a program systematically and automatically to isolate failure-inducing circumstances such as the program input, changes to the program code, or executed statements. 	
0410051v1	http://arxiv.org/pdf/cs/0410051v1	2004	Turing Machine with Faults, Failures and Recovery	Alex Vinokur	  A Turing machine with faults, failures and recovery (TMF) is described. TMF is (weakly) non-deterministic Turing machine consisting of five semi-infinite tapes (Master Tape, Synchro Tape, Backup Tape, Backup Synchro Tape, User Tape) and four controlling components (Program, Daemon, Apparatus, User). Computational process consists of three phases (Program Phase, Failure Phase, Repair Phase). C++ Simulator of a Turing machine with faults, failures and recovery has been developed. 	
0709.1875v1	http://arxiv.org/pdf/0709.1875v1	2007	Failure Analysis and Field Failures: a Real Shortcut to Reliability   Improvement	G. Mura|G. Cassanelli	  Starting from two case histories, where only after thorough Failure Analysis the suddenly appearance of a failure was linked to much earlier events, the possibility of improving the reliability and of adjusting the reliability prediction tools are discussed. 	
1203.6404v1	http://arxiv.org/pdf/1203.6404v1	2012	Definition, Detection, and Recovery of Single-Page Failures, a Fourth   Class of Database Failures	Goetz Graefe|Harumi Kuno	  The three traditional failure classes are system, media, and transaction failures. Sometimes, however, modern storage exhibits failures that differ from all of those. In order to capture and describe such cases, single-page failures are introduced as a fourth failure class. This class encompasses all failures to read a data page correctly and with plausible contents despite all correction attempts in lower system levels. Efficient recovery seems to require a new data structure called the page recovery index. Its transactional maintenance can be accomplished writing the same number of log records as today's efficient implementations of logging and recovery. Detection and recovery of a single-page failure can be sufficiently fast that the affected data access is merely delayed, without the need to abort the transaction. 	
1509.04557v1	http://arxiv.org/pdf/1509.04557v1	2015	Spatio-temporal propagation of cascading overload failures	Jichang Zhao|Daqing Li|Hillel Sanhedrai|Reuven Cohen|Shlomo Havlin	  Different from the direct contact in epidemics spread, overload failures propagate through hidden functional dependencies. Many studies focused on the critical conditions and catastrophic consequences of cascading failures. However, to understand the network vulnerability and mitigate the cascading overload failures, the knowledge of how the failures propagate in time and space is essential but still missing. Here we study the spatio-temporal propagation behavior of cascading overload failures analytically and numerically. The cascading overload failures are found to spread radially from the center of the initial failure with an approximately constant velocity. The propagation velocity decreases with increasing tolerance, and can be well predicted by our theoretical framework with one single correction for all the tolerance values. This propagation velocity is found similar in various model networks and real network structures. Our findings may help to predict and mitigate the dynamics of cascading overload failures in realistic systems. 	
1707.05428v1	http://arxiv.org/pdf/1707.05428v1	2017	Coordination and Control of Distributed Discrete-event Systems subject   to Sensor and Actuator Failures	Jin Dai|Hai Lin	  We study the coordination and control problem of distributed discrete-event systems with synchronous communication, in the presence of subsystems whose sensors and/or actuators may be affected by unexpected failures. We model sensor failures as permanent loss of observability of certain sensor events that belong to a subsystem, while characterize actuator failures as loss of controllability of the subsystems' actuator events. The failure tolerance property requires that the distributed discrete-event systems satisfy a global specification prior to as well as after occurrences of potential failures. To prevent the failure-pruned subsystems from jeopardizing the fulfillment of the specification, we propose automaton-theoretic frameworks corresponding to the enforcement of sensor and actuator failure tolerance by incorporating learning-based supervisor synthesis and coordination approaches with appropriate post-failure control reconfiguration schemes. The effectiveness of the proposed frameworks is demonstrated by an illustrative example. 	
1710.09722v2	http://arxiv.org/pdf/1710.09722v2	2018	Exhaustive Exploration of the Failure-oblivious Computing Search Space	Thomas Durieux|Youssef Hamadi|Zhongxing Yu|Martin Monperrus	  High-availability of software systems requires automated handling of crashes in presence of errors. Failure-oblivious computing is one technique that aims to achieve high availability. We note that failure-obliviousness has not been studied in depth yet, and there is very few study that helps understand why failure-oblivious techniques work. In order to make failure-oblivious computing to have an impact in practice, we need to deeply understand failure-oblivious behaviors in software. In this paper, we study, design and perform an experiment that analyzes the size and the diversity of the failure-oblivious behaviors. Our experiment consists of exhaustively computing the search space of 16 field failures of large-scale open-source Java software. The outcome of this experiment is a much better understanding of what really happens when failure-oblivious computing is used, and this opens new promising research directions. 	
1711.04491v1	http://arxiv.org/pdf/1711.04491v1	2017	The impact of a network split on cascading failure processes	Fiona Sloothaak|Sem C. Borst|Bert Zwart	  Cascading failure models are typically used to capture the phenomenon where failures possibly trigger further failures in succession, causing knock-on effects. In many networks this ultimately leads to a disintegrated network where the failure propagation continues independently across the various components. In order to gain insight in the impact of network splitting on cascading failure processes, we extend a well-established cascading failure model for which the number of failures obeys a power-law distribution. We assume that a single line failure immediately splits the network in two components, and examine its effect on the power-law exponent. The results provide valuable qualitative insights that are crucial first steps towards understanding more complex network splitting scenarios. 	
1802.07455v1	http://arxiv.org/pdf/1802.07455v1	2018	Asymptotic efficiency of restart and checkpointing	Antonio Sodre	  Many tasks are subject to failure before completion. Two of the most common failure recovery strategies are restart and checkpointing. Under restart, once a failure occurs, it is restarted from the beginning. Under checkpointing, the task is resumed from the preceding checkpoint after the failure. We study asymptotic efficiency of restart for an infinite sequence of tasks, whose sizes form a stationary sequence. We define asymptotic efficiency as the limit of the ratio of the total time to completion in the absence of failures over the total time to completion when failures take place. Whether the asymptotic efficiency is positive or not depends on the comparison of the tail of the distributions of the task size and the random variables governing failures. Our framework allows for variations in the failure rates and dependencies between task sizes. We also study a similar notion of asymptotic efficiency for checkpointing when the task is infinite a.s. and the inter-checkpoint times are i.i.d.. Moreover, in checkpointing, when the failures are exponentially distributed, we prove the existence of an infinite sequence of universal checkpoints, which are always used whenever the system starts from any checkpoint that precedes them. 	
0506078v2	http://arxiv.org/pdf/quant-ph/0506078v2	2005	Generalization of Classical Statistical Mechanics to Quantum Mechanics   and Stable Property of Condensed Matter	Y. C. Huang|F. C. Ma|N. Zhang	  Classical statistical average values are generally generalized to average values of quantum mechanics, it is discovered that quantum mechanics is direct generalization of classical statistical mechanics, and we generally deduce both a new general continuous eigenvalue equation and a general discrete eigenvalue equation in quantum mechanics, and discover that a eigenvalue of quantum mechanics is just an extreme value of an operator in possibility distribution, the eigenvalue f is just classical observable quantity. A general classical statistical uncertain relation is further given, the general classical statistical uncertain relation is generally generalized to quantum uncertainty principle, the two lost conditions in classical uncertain relation and quantum uncertainty principle, respectively, are found. We generally expound the relations among uncertainty principle, singularity and condensed matter stability, discover that quantum uncertainty principle prevents from the appearance of singularity of the electromagnetic potential between nucleus and electrons, and give the failure conditions of quantum uncertainty principle. Finally, we discover that the classical limit of quantum mechanics is classical statistical mechanics, the classical statistical mechanics may further be degenerated to classical mechanics, and we discover that only saying that the classical limit of quantum mechanics is classical mechanics is mistake. As application examples, we deduce both Shrodinger equation and state superposition principle, deduce that there exist decoherent factor from a general mathematical representation of state superposition principle, and the consistent difficulty between statistical interpretation of quantum mechanics and determinant property of classical mechanics is overcome. 	
0711.0952v1	http://arxiv.org/pdf/0711.0952v1	2007	On the Luttinger theorem concerning number of particles in the ground   states of systems of interacting fermions	Behnam Farid	  We analyze the original proof by Luttinger and Ward of the Luttinger theorem, according to which for uniform ground states of systems of (interacting) fermions, which may be metallic or insulating, the number of k points corresponding to non-negative values of G_s(k;mu) is equal to the total number of particles with spin index s in these ground states. Here G_s(k;mu) is the single-particle Green function of particles with spin index s at the chemical potential mu. For the cases where the two-body interaction potential is short-range, and in particular for lattice models, we explicitly demonstrate that this theorem is unconditionally valid, irrespective of the strength of the bare interaction potential. We arrive at this conclusion by amongst other things demonstrating that the perturbation series expansion for self-energy in terms of skeleton diagrams, as encountered in the proof of the Luttinger-Ward identity, is uniformly convergent for almost all momenta and energies. We further investigate the mechanisms underlying some reported instances of failure of the Luttinger theorem. With one exception, for all the cases considered in this paper, we show that the apparent failures of the Luttinger theorem can be attributed either to shortcomings of the employed single-particle Green functions or to misapplication of this theorem. The one exceptional case brings to light the possibility of a genuine failure of the Luttinger theorem for insulating ground states, which we show to be brought about by a false limit that in principle can be reached on taking the zero-temperature limit without the value of mu coinciding with the zero-temperature limit of the chemical potential satisfying the equation of state at finite temperatures; no such ambiguity can arise for metallic states. 	
0901.3806v1	http://arxiv.org/pdf/0901.3806v1	2009	Modeling long-term longitudinal HIV dynamics with application to an AIDS   clinical study	Yangxin Huang|Tao Lu	  A virologic marker, the number of HIV RNA copies or viral load, is currently used to evaluate antiretroviral (ARV) therapies in AIDS clinical trials. This marker can be used to assess the ARV potency of therapies, but is easily affected by drug exposures, drug resistance and other factors during the long-term treatment evaluation process. HIV dynamic studies have significantly contributed to the understanding of HIV pathogenesis and ARV treatment strategies. However, the models of these studies are used to quantify short-term HIV dynamics ($<$ 1 month), and are not applicable to describe long-term virological response to ARV treatment due to the difficulty of establishing a relationship of antiviral response with multiple treatment factors such as drug exposure and drug susceptibility during long-term treatment. Long-term therapy with ARV agents in HIV-infected patients often results in failure to suppress the viral load. Pharmacokinetics (PK), drug resistance and imperfect adherence to prescribed antiviral drugs are important factors explaining the resurgence of virus. To better understand the factors responsible for the virological failure, this paper develops the mechanism-based nonlinear differential equation models for characterizing long-term viral dynamics with ARV therapy. The models directly incorporate drug concentration, adherence and drug susceptibility into a function of treatment efficacy and, hence, fully integrate virologic, PK, drug adherence and resistance from an AIDS clinical trial into the analysis. A Bayesian nonlinear mixed-effects modeling approach in conjunction with the rescaled version of dynamic differential equations is investigated to estimate dynamic parameters and make inference. In addition, the correlations of baseline factors with estimated dynamic parameters are explored and some biologically meaningful correlation results are presented. Further, the estimated dynamic parameters in patients with virologic success were compared to those in patients with virologic failure and significantly important findings were summarized. These results suggest that viral dynamic parameters may play an important role in understanding HIV pathogenesis, designing new treatment strategies for long-term care of AIDS patients. 	
1410.6836v3	http://arxiv.org/pdf/1410.6836v3	2015	Reducing Cascading Failure Risk by Increasing Infrastructure Network   Interdependency	Mert Korkali|Jason G. Veneman|Brian F. Tivnan|Paul D. H. Hines	  Increased coupling between critical infrastructure networks, such as power and communication systems, will have important implications for the reliability and security of these systems. To understand the effects of power-communication coupling, several have studied interdependent network models and reported that increased coupling can increase system vulnerability. However, these results come from models that have substantially different mechanisms of cascading, relative to those found in actual power and communication networks. This paper reports on two sets of experiments that compare the network vulnerability implications resulting from simple topological models and models that more accurately capture the dynamics of cascading in power systems. First, we compare a simple model of topological contagion to a model of cascading in power systems and find that the power grid shows a much higher level of vulnerability, relative to the contagion model. Second, we compare a model of topological cascades in coupled networks to three different physics-based models of power grids coupled to communication networks. Again, the more accurate models suggest very different conclusions. In all but the most extreme case, the physics-based power grid models indicate that increased power-communication coupling decreases vulnerability. This is opposite from what one would conclude from the coupled topological model, in which zero coupling is optimal. Finally, an extreme case in which communication failures immediately cause grid failures, suggests that if systems are poorly designed, increased coupling can be harmful. Together these results suggest design strategies for reducing the risk of cascades in interdependent infrastructure systems. 	
0207393v3	http://arxiv.org/pdf/cond-mat/0207393v3	2003	Phase transition in fiber bundle models with recursive dynamics	Pratip Bhattacharyya|Srutarshi Pradhan|Bikas K. Chakrabarti	  We study the phase transition in a class of fiber bundle models in which the fiber strengths are distributed randomly within a finite interval and global load sharing is assumed. The dynamics is expressed as recursion relations for the redistribution of the applied stress and the evolution of the surviving fraction of fibers. We show that an irreversible phase transition of second-order occurs, from a phase of partial failure to a phase of total failure, when the initial applied stress just exceeds a critical value. The phase transition is characterised by static and dynamic critical properties. We calculate exactly the critical value of the initial stress for three models of this kind, each with a different distribution of fiber strengths. We derive the exact expressions for the order parameter, the susceptibility to changes in the initial applied sress and the critical relaxation of the surviving fraction of fibers for all the three models. The static and dynamic critical exponents obtained from these expressions are found to be universal. 	
0301076v1	http://arxiv.org/pdf/cond-mat/0301076v1	2003	Fracture of Notched Single Crystal Silicon	Nicholas P. Bailey|James P. Sethna	  We study atomistically the fracture of single crystal silicon at atomically sharp notches with opening angles of 0 degrees (a crack), 70.53 degrees, 90 degrees and 125.3 degrees. Such notches occur in silicon that has been formed by etching into microelectromechanical structures and tend to be the initiation sites for failure by fracture of these structures. Analogous to the stress intensity factor of traditional linear elastic fracture mechanics which characterizes the stress state in the limiting case of a crack, there exists a similar parameter K for the case of the notch. In the case of silicon, a brittle material, this characterization appears to be particularly valid. We use three interatomic potentials: a modified Stillinger-Weber potential, the Environment-Dependent Interatomic Potential (EDIP), and the modified embedded atom method (MEAM). Of these, MEAM gives critical K-values closest to experiment. In particular the EDIP potential leads to unphysical ductile failure in most geometries. Because the units of K depend on the notch angle, the shape of the K versus angle plot depends on the units used. In particular when an atomic length unit is used the plot is almost flat, showing--in principle from macroscopic observations alone--the association of an atomic length scale to the fracture process. 	
0409524v2	http://arxiv.org/pdf/cond-mat/0409524v2	2005	Statistical Physics of Rupture in Heterogeneous Media	D. Sornette	  The damage and fracture of materials are technologically of enormous interest due to their economic and human cost. They cover a wide range of phenomena like e.g. cracking of glass, aging of concrete, the failure of fiber networks in the formation of paper and the breaking of a metal bar subject to an external load. Failure of composite systems is of utmost importance in naval, aeronautics and space industry. By the term composite, we refer to materials with heterogeneous microscopic structures and also to assemblages of macroscopic elements forming a super-structure. Chemical and nuclear plants suffer from cracking due to corrosion either of chemical or radioactive origin, aided by thermal and/or mechanical stress. Despite the large amount of experimental data and the considerable effort that has been undertaken by material scientists, many questions about fracture have not been answered yet. There is no comprehensive understanding of rupture phenomena but only a partial classification in restricted and relatively simple situations. This lack of fundamental understanding is indeed reflected in the absence of reliable prediction methods for rupture, based on a suitable monitoring of the stressed system. Not only is there a lack of non-empirical understanding of the reliability of a system, but also the empirical laws themselves have often limited value. The difficulties stem from the complex interplay between heterogeneities and modes of damage and the possible existence of a hierarchy of characteristic scales (static and dynamic).   The paper presents a review of recent efforts from the statistical physics community to address these points. 	
0508424v1	http://arxiv.org/pdf/cond-mat/0508424v1	2005	Predicting Failure using Conditioning on Damage History: Demonstration   on Percolation and Hierarchical Fiber Bundles	J. Andersen|D. Sornette	  We formulate the problem of probabilistic predictions of global failure in the simplest possible model based on site percolation and on one of the simplest model of time-dependent rupture, a hierarchical fiber bundle model. We show that conditioning the predictions on the knowledge of the current degree of damage (occupancy density $p$ or number and size of cracks) and on some information on the largest cluster improves significantly the prediction accuracy, in particular by allowing to identify those realizations which have anomalously low or large clusters (cracks). We quantify the prediction gains using two measures, the relative specific information gain (which is the variation of entropy obtained by adding new information) and the root-mean-square of the prediction errors over a large ensemble of realizations. The bulk of our simulations have been obtained with the two-dimensional site percolation model on a lattice of size $L \times L=20 \times 20$ and hold true for other lattice sizes. For the hierarchical fiber bundle model, conditioning the measures of damage on the information of the location and size of the largest crack extends significantly the critical region and the prediction skills. These examples illustrate how on-going damage can be used as a revelation of both the realization-dependent pre-existing heterogeneity and the damage scenario undertaken by each specific sample. 	
0610399v1	http://arxiv.org/pdf/cond-mat/0610399v1	2006	Equilibrium and transport properties of constrained systems	Debasish Chaudhuri	  Systems under external confinement and constraints often show interesting properties. In this thesis, we study some systems under external confinement. We begin by finding out the probability distribution of end-to-end separation of a Worm Like Chain (WLC) polymer whose ends are positionally (and orientationally) constrained. We use Monte-Carlo simulations (MC) and a theoretical mapping of the WLC to a quantum particle moving on the surface of an unit sphere to find multimodality in Helmholtz ensemble as a generic signature of semi-flexibility. Secondly, we study Laser Induced Freezing using a Kosterlitz-Thouless type renormalization group calculation and a restricted MC simulation to obtain phase diagrams for Hard Disk, Soft Disk and DLVO potentials. They show very good agreement with phase diagrams simulated by other groups. Lastly, we study the strain response and failure mechanism of a two-dimensional solid confined within a hard wall channel using MC and molecular dynamics simulations. We find a reversible plastic failure through solid-smectic coexistence and observe layering transitions. Mean field calculations can capture some of these features. We study the heat transport in this system thorugh nonequilibrium molecular dynamics simulations and find strong signatures of the transitions. We propose a simple free volume calculation that reproduces some qualitative features of the strain response of heat current for small strains. 	
0411006v2	http://arxiv.org/pdf/q-bio/0411006v2	2006	Lethality and synthetic lethality in the genome-wide metabolic network   of Escherichia coli	C. -M. Ghim|K. -I. Goh|B. Kahng	  Recent genomic analyses on the cellular metabolic network show that reaction flux across enzymes are diverse and exhibit power-law behavior in its distribution. While one may guess that the reactions with larger fluxes are more likely to be lethal under the blockade of its catalyzing gene products or gene knockouts, we find, by in silico flux analysis, that the lethality rarely has correlations with the flux level owing to the widespread backup pathways innate in the genome-wide metabolism of \textit{Escherichia coli}. Lethal reactions, of which the deletion generates cascading failure of following reactions up to the biomass reaction, are identified in terms of the Boolean network scheme as well as the flux balance analysis. The avalanche size of a reaction, defined as the number of subsequently blocked reactions after its removal, turns out to be a useful measure of lethality. As a means to elucidate phenotypic robustness to a single deletion, we investigate synthetic lethality in reaction level, where simultaneous deletion of a pair of nonlethal reactions leads to the failure of the biomass reaction. Synthetic lethals identified via flux balance and Boolean scheme are consistently shown to act in parallel pathways, working in such a way that the backup machinery is compromised. 	
0711.0399v1	http://arxiv.org/pdf/0711.0399v1	2007	Slow failure of quasi-brittle solids	Leonid S. Metlov	  A new mesoscopic non-equilibrium thermodynamic approach is developed. The approach is based on the thermodynamic identity associated the first and second law of thermodynamics. In the framework of the approach different internal dissipative channels of energy are taken in account in an explicit form, namely, the thermal channel and channels of defect subsystems. The identity has a perfect differential form what permits to introduce an extended non-equilibrium state and use the good developed mathematical formalism of equilibrium and non-equilibrium thermodynamics. The evolution of non-equilibrium variables of a physical system are described by a Landau-based equation set expressed through internal or different kinds of free energy connected by means of the Legendre transforms. The accordance between the different kinds of energy is possible owing to introduction of some trends into the equation subset described the defect subsystems and having a nature of structural viscosity. The possibilities of the approach are illustrated on the example of quasibrittle solid damage and failure. Taking into account only one type of defects (viz., microcracks) and mechanical parameters in an expansion of free energy down to third powers in relative to average energy per microcrack, the description of destruction of quasi-brittle solids during long-term loading is considered. The consideration allows to find equilibrium and non-equilibrium values of the free energy. A qualitative behavior of the system on parameters of the theory is analyzed. The destruction of material is described from the uniform positions, both at uniform tension and uniaxial compression. Origins of the high stability of mine workings at small depths and their instability at large depths are explained. 	
0806.1775v1	http://arxiv.org/pdf/0806.1775v1	2008	Particle size effect on strength, failure and shock behavior in   Polytetrafluoroethylene-Al-W granular composites	E. B. Herbold|V. F. Nesterenko|D. J. Benson|J. Cai|K. S. Vecchio|F. Jiang|J. W. Addiss|S. M. Walley|W. G. Proud	  The variation of metallic particle size and sample porosity significantly alters the dynamic mechanical properties of high density granular composites processed using a cold isostatically pressed mixture of polytetrafluoroethylene (PTFE), aluminum (Al) and tungsten (W) powders. Quasi-static and dynamic experiments are performed with identical constituent mass fractions with variations in the size of the W particles and pressing conditions. The relatively weak polymer matrix allows the strength and fracture modes of this material to be governed by the granular type behavior of agglomerated metal particles. A higher ultimate compressive strength was observed in relatively high porosity samples with small W particles compared to those with coarse W particles in all experiments. Mesoscale granular force chains comprised of the metallic particles explain this unusual phenomenon as observed in a hydrocode simulation of a drop-weight test. Macrocracks forming below the critical failure strain for the matrix and unusual behavior due to a competition between densification and fracture in dynamic tests of porous samples were also observed. Shock loading of this granular composite resulted in higher fraction of total internal energy deposition in the soft PTFE matrix, specifically thermal energy, which can be tailored by the W particle size distribution. 	
0907.4290v1	http://arxiv.org/pdf/0907.4290v1	2009	Dragon-Kings, Black Swans and the Prediction of Crises	Didier Sornette	  We develop the concept of ``dragon-kings'' corresponding to meaningful outliers, which are found to coexist with power laws in the distributions of event sizes under a broad range of conditions in a large variety of systems. These dragon-kings reveal the existence of mechanisms of self-organization that are not apparent otherwise from the distribution of their smaller siblings. We present a generic phase diagram to explain the generation of dragon-kings and document their presence in six different examples (distribution of city sizes, distribution of acoustic emissions associated with material failure, distribution of velocity increments in hydrodynamic turbulence, distribution of financial drawdowns, distribution of the energies of epileptic seizures in humans and in model animals, distribution of the earthquake energies). We emphasize the importance of understanding dragon-kings as being often associated with a neighborhood of what can be called equivalently a phase transition, a bifurcation, a catastrophe (in the sense of Rene Thom), or a tipping point. The presence of a phase transition is crucial to learn how to diagnose in advance the symptoms associated with a coming dragon-king. Several examples of predictions using the derived log-periodic power law method are discussed, including material failure predictions and the forecasts of the end of financial bubbles. 	
1005.3011v1	http://arxiv.org/pdf/1005.3011v1	2010	On the relevance of avoided crossings away from quantum critical point   to the complexity of quantum adiabatic algorithm	S. Knysh|V. Smelyanskiy	  Two recent preprints [B. Altshuler, H. Krovi, and J. Roland, "Quantum adiabatic optimization fails for random instances of NP-complete problems", arXiv:0908.2782 and "Anderson localization casts clouds over adiabatic quantum optimization", arXiv:0912.0746] argue that random 4th order perturbative corrections to the energies of local minima of random instances of NP-complete problem lead to avoided crossings that cause the failure of quantum adiabatic algorithm (due to exponentially small gap) close to the end, for very small transverse field that scales as an inverse power of instance size N. The theoretical portion of this work does not to take into account the exponential degeneracy of the ground and excited states at zero field. A corrected analysis shows that unlike those in the middle of the spectrum, avoided crossings at the edge would require high [O(1)] transverse fields, at which point the perturbation theory may become divergent due to quantum phase transition. This effect manifests itself only in large instances [exp(0.02 N) >> 1], which might be the reason it had not been observed in the authors' numerical work. While we dispute the proposed mechanism of failure of quantum adiabatic algorithm, we cannot draw any conclusions on its ultimate complexity. 	
1007.0656v5	http://arxiv.org/pdf/1007.0656v5	2010	Failure of the fluctuation-dissipation relation to ensure equilibrium	A Bhattacharyay	  Fluctuation-dissipation relation ensures thermodynamic equilibrium of a particle immersed in a heat bath. We will show that, under certain circumstances, the fluctuation-dissipation relation fails to ensure equilibrium between the immersed system and the heat bath. We consider a symmetry broken dimer, constrained to move in one dimension, is in compliance with the requirements of fluctuation-dissipation relation. An exact analytic result shows a nonzero average velocity of the center of mass of the dimer indicating that the state of the system is a nonequilibrium one. Based on this new physical observation, we propose an alternative paradigm for a Brownian motor which would extract useful energy directly from the heat bath unlike the ones based on Brownian Ratchet principle. 	
1104.3479v2	http://arxiv.org/pdf/1104.3479v2	2017	Reliability-based design optimization of shells with uncertain geometry   using adaptive Kriging metamodels	V. Dubourg|J. -M. Bourinet|B. Sudret	  Optimal design under uncertainty has gained much attention in the past ten years due to the ever increasing need for manufacturers to build robust systems at the lowest cost. Reliability-based design optimization (RBDO) allows the analyst to minimize some cost function while ensuring some minimal performances cast as admissible failure probabilities for a set of performance functions. In order to address real-world engineering problems in which the performance is assessed through computational models (e.g., finite element models in structural mechanics) metamodeling techniques have been developed in the past decade. This paper introduces adaptive Kriging surrogate models to solve the RBDO problem. The latter is cast in an augmented space that "sums up" the range of the design space and the aleatory uncertainty in the design parameters and the environmental conditions. The surrogate model is used (i) for evaluating robust estimates of the failure probabilities (and for enhancing the computational experimental design by adaptive sampling) in order to achieve the requested accuracy and (ii) for applying a gradient-based optimization algorithm to get optimal values of the design parameters. The approach is applied to the optimal design of ring-stiffened cylindrical shells used in submarine engineering under uncertain geometric imperfections. For this application the performance of the structure is related to buckling which is addressed here by means of a finite element solution based on the asymptotic numerical method. 	
1105.0562v2	http://arxiv.org/pdf/1105.0562v2	2011	Metamodel-based importance sampling for structural reliability analysis	V. Dubourg|F. Deheeger|B. Sudret	  Structural reliability methods aim at computing the probability of failure of systems with respect to some prescribed performance functions. In modern engineering such functions usually resort to running an expensive-to-evaluate computational model (e.g. a finite element model). In this respect simulation methods, which may require $10^{3-6}$ runs cannot be used directly. Surrogate models such as quadratic response surfaces, polynomial chaos expansions or kriging (which are built from a limited number of runs of the original model) are then introduced as a substitute of the original model to cope with the computational cost. In practice it is almost impossible to quantify the error made by this substitution though. In this paper we propose to use a kriging surrogate of the performance function as a means to build a quasi-optimal importance sampling density. The probability of failure is eventually obtained as the product of an augmented probability computed by substituting the meta-model for the original performance function and a correction term which ensures that there is no bias in the estimation even if the meta-model is not fully accurate. The approach is applied to analytical and finite element reliability problems and proves efficient up to 100 random variables. 	
1107.4785v1	http://arxiv.org/pdf/1107.4785v1	2011	A Novel Cyber-Insurance for Internet Security	Ranjan Pal|Leana Golubchik|Konstantinos Psounis	  Internet users such as individuals and organizations are subject to different types of epidemic risks such as worms, viruses, and botnets. To reduce the probability of risk, an Internet user generally invests in self-defense mechanisms like antivirus and antispam software. However, such software does not completely eliminate risk. Recent works have considered the problem of residual risk elimination by proposing the idea of cyber-insurance. In reality, an Internet user faces risks due to security attacks as well as risks due to non-security related failures (e.g., reliability faults in the form of hardware crash, buffer overflow, etc.) . These risk types are often indistinguishable by a naive user. However, a cyber-insurance agency would most likely insure risks only due to security attacks. In this case, it becomes a challenge for an Internet user to choose the right type of cyber-insurance contract as standard optimal contracts, i.e., contracts under security attacks only, might prove to be sub-optimal for himself. In this paper, we address the problem of analyzing cyber-insurance solutions when a user faces risks due to both, security as well as non-security related failures. We propose \emph{Aegis}, a novel cyber-insurance model in which the user accepts a fraction \emph{(strictly positive)} of loss recovery on himself and transfers rest of the loss recovery on the cyber-insurance agency. We mathematically show that given an option, Internet users would prefer Aegis contracts to traditional cyber-insurance contracts, under all premium types. This result firmly establishes the non-existence of traditional cyber-insurance markets when Aegis contracts are offered to users. 	
1201.0916v1	http://arxiv.org/pdf/1201.0916v1	2012	Device and method for investigation of mechanical properties of the   materials under high-strain rate tensile load	Sergey Lopatnikov|Nikolas Shevchenko|John W. Gillespie Jr	  A new apparatus and method is proposed for the investigation of material behavior under high-strain-rate tensile loads. We refer this apparatus as a Split Flying Bar. The method is based on using the inertia of a working mass attached to a specimen. The specimen is placed between the working mass (front part of the bar) and backing part of the bar, which is captured in flight by special brake. When the back part of the flying split bar is stopped, the working mass continues the flight by inertia, creating specimen tension with strain rate depending on the length of specimen and velocity of flying bar. Properly choosing the working mass and the speed of the flying bar, one can tune maximal stress and strain rate over a wide range. The method is highly scalable and can be used for investigation of specimens from single filament up to reasonable macroscopic size. Contrary to tensile SHPB, the method provides a way to investigate materials with practically arbitrary strain to failure. For example, polyuria, whose strain to failure reaches hundreds of percent. Also, the method can be used for high-strain rate pull-out tests or to measure the quality of adhesion layers under high-stress rates, e.t.c. In this paper I introduce the basics of the method and interpretation of the data. 	
1201.2439v1	http://arxiv.org/pdf/1201.2439v1	2012	Material point method simulations of fragmenting cylinders	Biswajit Banerjee	  Most research on the simulation of deformation and failure of metals has been and continues to be performed using the finite element method. However, the issues of mesh entanglement under large deformation, considerable complexity in handling contact, and difficulties encountered while solving large deformation fluid-structure interaction problems have led to the exploration of alternative approaches. The material point method uses Lagrangian solid particles embedded in an Eulerian grid. Particles interact via the grid with other particles in the same body, with other solid bodies, and with fluids. Thus, the three issues mentioned in the context of finite element analysis are circumvented.   In this paper, we present simulations of cylinders which fragment due to explosively expanding gases generated by reactions in a high energy material contained inside. The material point method is the numerical method chosen for these simulations discussed in this paper. The plastic deformation of metals is simulated using a hypoelastic-plastic stress update with radial return that assumes an additive decomposition of the rate of deformation tensor. Various plastic strain, plastic strain rate, and temperature dependent flow rules and yield conditions are investigated. Failure at individual material points is determined using porosity, damage and bifurcation conditions. Our models are validated using data from high strain rate impact experiments. It is concluded that the material point method possesses great potential for simulating high strain-rate, large deformation fluid-structure interaction problems. 	
1208.6116v1	http://arxiv.org/pdf/1208.6116v1	2012	A computational toy model for shallow landslides: Molecular Dynamics   approach	Gianluca Martelloni|Franco Bagnoli|Emanuele Massaro	  The aim of this paper is to propose a 2D computational algorithm for modeling of the trigger and the propagation of shallow landslides caused by rainfall. We used a Molecular Dynamics (MD) inspired model, similar to discrete element method (DEM), that is suitable to model granular material and to observe the trajectory of single particle, so to identify its dynamical properties. We consider that the triggering of shallow landslides is caused by the decrease of the static friction along the sliding surface due to water infiltration by rainfall. Thence the triggering is caused by two following conditions: (a) a threshold speed of the particles and (b) a condition on the static friction, between particles and slope surface, based on the Mohr-Coulomb failure criterion. The latter static condition is used in the geotechnical model to estimate the possibility of landslide triggering. Finally the interaction force between particles is defined trough a potential that, in the absence of experimental data, we have modeled as the Lennard-Jones 2-1 potential. In the model the viscosity is also introduced and for a large range of values of the model's parameters, we observe a characteristic velocity pattern, with acceleration increments, typical of real landslides. The results of simulations are quite promising: the energy and the time triggering distributions of local avalanches shows a power law distribution, analogous to the observed Gutenberg-Richter and Omori power law distributions for earthquakes. Finally it is possible to apply the method of the inverse surface displacement velocity [Fukuzono 1985] for predicting the failure time. 	
1402.4700v2	http://arxiv.org/pdf/1402.4700v2	2014	Slow slip and the transition from fast to slow fronts in the rupture of   frictional interfaces	Jørgen Kjoshagen Trømborg|Henrik Andersen Sveinsson|Julien Scheibert|Kjetil Thøgersen|David Skålid Amundsen|Anders Malthe-Sørenssen	  The failure of the population of micro-junctions forming the frictional interface between two solids is central to fields ranging from biomechanics to seismology. This failure is mediated by the propagation along the interface of various types of rupture fronts, covering a wide range of velocities. Among them are so-called slow fronts, which are recently discovered fronts much slower than the materials' sound speeds. Despite intense modelling activity, the mechanisms underlying slow fronts remain elusive. Here, we introduce a multi-scale model capable of reproducing both the transition from fast to slow fronts in a single rupture event and the short-time slip dynamics observed in recent experiments. We identify slow slip immediately following the arrest of a fast front as a phenomenon sufficient for the front to propagate further at a much slower pace. Whether slow fronts are actually observed is controlled both by the interfacial stresses and by the width of the local distribution of forces among micro-junctions. Our results show that slow fronts are qualitatively different from faster fronts. Since the transition from fast to slow fronts is potentially as generic as slow slip, we anticipate that it might occur in the wide range of systems in which slow slip has been reported, including seismic faults. 	
1404.2815v2	http://arxiv.org/pdf/1404.2815v2	2014	Usage leading to an abrupt collapse of connectivity	D. V. Stäger|N. A. M. Araújo|H. J. Herrmann	  Network infrastructures are essential for the distribution of resources such as electricity and water. Typical strategies to assess their resilience focus on the impact of a sequence of random or targeted failures of network nodes or links. Here we consider a more realistic scenario, where elements fail based on their usage. We propose a dynamic model of transport based on the Bak-Tang-Wiesenfeld sandpile model where links fail after they have transported more than an amount $\mu$ (threshold) of the resource and we investigate it on the square lattice. As we deal with a new model, we provide insight on its fundamental behavior and dependence on parameters. We observe that for low values of the threshold due to a positive feedback of link failure, an avalanche develops that leads to an abrupt collapse of the lattice. By contrast, for high thresholds the lattice breaks down in an uncorrelated fashion. We determine the critical threshold $\mu^*$ separating these two regimes and show how it depends on the toppling threshold of the nodes and the mass increment added stepwise to the system. We find that the time of major disconnection is well described with a linear dependence on $\mu$. Furthermore, we propose a lower bound for $\mu^*$ by measuring the strength of the dynamics leading to abrupt collapses. 	
1406.3053v2	http://arxiv.org/pdf/1406.3053v2	2014	Successes and failures of Hubbard-corrected density functional theory:   The case of Mg doped LiCoO$_2$	Juan A. Santana|Jeongnim Kim|P. R. C. Kent|Fernando A. Reboredo	  We have evaluated the successes and failures of the Hubbard-corrected density functional theory (DFT+U) approach to study Mg doping of LiCoO$_2$. We computed the effect of the U parameter on the energetic, geometric and electronic properties of two possible doping mechanisms: (1) substitution of Mg onto a Co (or Li) site with an associated impurity state and, (2) formation of impurity-state-free complexes of substitutional Mg and point defects in LiCoO$_2$. We find that formation of impurity states results in changes on the valency of Co in LiCoO$_2$. Variation of the Co U shifts the energy of the impurity state, resulting in energetic, geometric and electronic properties that depend significantly on the specific value of U. In contrast, the properties of the impurity-state-free complexes are insensitive to U. These results identify reasons for the strong dependence on the doping properties on the chosen value of U and for the overall difficulty of achieving agreement with the experimentally known energetic and electronic properties of doped transition metal oxides such as LiCoO$_2$. 	
1502.04848v2	http://arxiv.org/pdf/1502.04848v2	2015	Failing softly: A fracture theory of highly-deformable materials	Tamar Goldman Boué|Roi Harpaz|Jay Fineberg|Eran Bouchbinder	  Highly-deformable materials, from synthetic hydrogels to biological tissues, are becoming increasingly important from both fundamental and practical perspectives. Their mechanical behaviors, in particular the dynamics of crack propagation during failure, are not yet fully understood. Here we propose a theoretical framework for the dynamic fracture of highly-deformable materials, in which the effects of a dynamic crack are treated with respect to the nonlinearly deformed (pre-stressed/strained), non-cracked, state of the material. Within this framework, we derive analytic and semi-analytic solutions for the near-tip deformation fields and energy release rates of dynamic cracks propagating in incompressible neo-Hookean solids under biaxial and uniaxial loading. We show that moderately large pre-stressing has a marked effect on the stress fields surrounding a crack's tip. We verify these predictions by performing extensive experiments on the fracture of soft brittle elastomers over a range of loading levels and propagation velocities, showing that the newly developed framework offers significantly better approximations to the measurements than standard approaches at moderately large levels of external loadings and high propagation velocities. This framework should be relevant to the failure analysis of soft and tough, yet brittle, materials. 	
1505.05259v4	http://arxiv.org/pdf/1505.05259v4	2016	SAF: Stochastic Adaptive Forwarding in Named Data Networking	Daniel Posch|Benjamin Rainer|Hermann Hellwagner	  Forwarding decisions in classical IP-based networks are predetermined by routing. This is necessary to avoid loops, inhibiting opportunities to implement an adaptive and intelligent forwarding plane. Consequently, content distribution efficiency is reduced due to a lack of inherent multi-path transmission. In Named Data Networking (NDN) instead, routing shall hold a supporting role to forwarding, providing sufficient potential to enhance content dissemination at the forwarding plane. In this paper we design, implement, and evaluate a novel probability-based forwarding strategy, called Stochastic Adaptive Forwarding (SAF) for NDN. SAF imitates a self-adjusting water pipe system, intelligently guiding and distributing Interests through network crossings circumventing link failures and bottlenecks. Just as real pipe systems, SAF employs overpressure valves enabling congested nodes to lower pressure autonomously. Through an implicit feedback mechanism it is ensured that the fraction of the traffic forwarded via congested nodes decreases. By conducting simulations we show that our approach outperforms existing forwarding strategies in terms of the Interest satisfaction ratio in the majority of the evaluated scenarios. This is achieved by extensive utilization of NDN's multipath and content-lookup capabilities without relying on the routing plane. SAF explores the local environment by redirecting requests that are likely to be dropped anyway. This enables SAF to identify new paths to the content origin or to cached replicas, circumventing link failures and resource shortages without relying on routing updates. 	
1508.06854v1	http://arxiv.org/pdf/1508.06854v1	2015	How Evolution Learns to Generalise: Principles of under-fitting,   over-fitting and induction in the evolution of developmental organisation	Kostas Kouvaris|Jeff Clune|Louis Kounios|Markus Brede|Richard A. Watson	  One of the most intriguing questions in evolution is how organisms exhibit suitable phenotypic variation to rapidly adapt in novel selective environments which is crucial for evolvability. Recent work showed that when selective environments vary in a systematic manner, it is possible that development can constrain the phenotypic space in regions that are evolutionarily more advantageous. Yet, the underlying mechanism that enables the spontaneous emergence of such adaptive developmental constraints is poorly understood. How can natural selection, given its myopic and conservative nature, favour developmental organisations that facilitate adaptive evolution in future previously unseen environments? Such capacity suggests a form of \textit{foresight} facilitated by the ability of evolution to accumulate and exploit information not only about the particular phenotypes selected in the past, but regularities in the environment that are also relevant to future environments. Here we argue that the ability of evolution to discover such regularities is analogous to the ability of learning systems to generalise from past experience. Conversely, the canalisation of evolved developmental processes to past selective environments and failure of natural selection to enhance evolvability in future selective environments is directly analogous to the problem of over-fitting and failure to generalise in machine learning. We show that this analogy arises from an underlying mechanistic equivalence by showing that conditions corresponding to those that alleviate over-fitting in machine learning enhance the evolution of generalised developmental organisations under natural selection. This equivalence provides access to a well-developed theoretical framework that enables us to characterise the conditions where natural selection will find general rather than particular solutions to environmental conditions. 	
1601.02709v1	http://arxiv.org/pdf/1601.02709v1	2016	Dynamic signal tracking in a simple V1 spiking model	Guillaume Lajoie|Lai-Sang Young	  This work is part of an effort to understand the neural basis for our visual system's ability, or failure, to accurately track moving visual signals. We consider here a ring model of spiking neurons, intended as a simplified computational model of a single hypercolumn of the primary visual cortex. Signals that consist of edges with time-varying orientations localized in space are considered. Our model is calibrated to produce spontaneous and driven firing rates roughly consistent with experiments, and our two main findings, for which we offer dynamical explanation on the level of neuronal interactions, are the following: (1) We have documented consistent transient overshoots in signal perception following signal switches due to emergent interactions of the E- and I-populations, and (2) for continuously moving signals, we have found that accuracy is considerably lower at reversals of orientation than when continuing in the same direction (as when the signal is a rotating bar). To measure performance, we use two metrics, called fidelity and reliability, to compare signals reconstructed by the system to the ones presented, and to assess trial-to-trial variability. We propose that the same population mechanisms responsible for orientation selectivity also impose constraints on dynamic signal tracking that manifest in perception failures consistent with psychophysical observations. 	
1603.08635v1	http://arxiv.org/pdf/1603.08635v1	2016	Development and Validation of Functional Model of a Cruise Control   System	Avinash Visagan Varadarajan|Marcel Romijn|Bart Oosthoek|Joanna van de Mortel-Fronczak|Jos Beijer	  Modern automobiles can be considered as a collection of many subsystems working with each other to realize safe transportation of the occupants. Innovative technologies that make transportation easier are increasingly incorporated into the automobile in the form of functionalities. These new functionalities in turn increase the complexity of the system framework present and traceability is lost or becomes very tricky in the process. This hugely impacts the development phase of an automobile, in which, the safety and reliability of the automobile design should be ensured. Hence, there is a need to ensure operational safety of the vehicles while adding new functionalities to the vehicle. To address this issue, functional models of such systems are created and analysed. The main purpose of developing a functional model is to improve the traceability and reusability of a system which reduces development time and cost. Operational safety of the system is ensured by analysing the system with respect to random and systematic failures and including safety mechanism to prevent such failures. This paper discusses the development and validation of a functional model of a conventional cruise control system in a passenger vehicle based on the ISO 26262 Road Vehicles - Functional Safety standard. A methodology for creating functional architectures and an architecture of a cruise control system developed using the methodology are presented. 	
1604.02858v1	http://arxiv.org/pdf/1604.02858v1	2016	Microstructural topology effects on the onset of ductile failure in   multi-phase materials - a systematic computational approach	T. W. J. de Geus|R. H. J. Peerlings|M. G. D. Geers	  Multi-phase materials are key for modern engineering applications. They are generally characterized by a high strength and ductility. Many of these materials fail by ductile fracture of the, generally softer, matrix phase. In this work we systematically study the influence of the arrangement of the phases by correlating the microstructure of a two-phase material to the onset of ductile failure. A single topological feature is identified in which critical levels of damage are consistently indicated. It consists of a small region of the matrix phase with particles of the hard phase on both sides in a direction that depends on the applied deformation. Due to this configuration, a large tensile hydrostatic stress and plastic strain is observed inside the matrix, indicating high damage. This topological feature has, to some extent, been recognized before for certain multi-phase materials. This study however provides insight in the mechanics involved, including the influence of the loading conditions and the arrangement of the phases in the material surrounding the feature. Furthermore, a parameter study is performed to explore the influence of volume fraction and hardness of the inclusion phase. For the same macroscopic hardening response, the ductility is predicted to increase if the volume fraction of the hard phase increases while at the same time its hardness decreases. 	
1611.07012v3	http://arxiv.org/pdf/1611.07012v3	2017	GRAM: Graph-based Attention Model for Healthcare Representation Learning	Edward Choi|Mohammad Taha Bahadori|Le Song|Walter F. Stewart|Jimeng Sun	  Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two important challenges remain: -Data insufficiency:Often in healthcare predictive modeling, the sample size is insufficient for deep learning methods to achieve satisfactory results. -Interpretation:The representations learned by deep learning methods should align with medical knowledge. To address these challenges, we propose a GRaph-based Attention Model, GRAM that supplements electronic health records (EHR) with hierarchical information inherent to medical ontologies. Based on the data volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in the ontology via an attention mechanism. We compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart failure prediction task. Compared to the basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely observed in the training data and 3% improved area under the ROC curve for predicting heart failure using an order of magnitude less training data. Additionally, unlike other methods, the medical concept representations learned by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts when facing data insufficiency at the lower level concepts. 	
1707.01549v2	http://arxiv.org/pdf/1707.01549v2	2017	Topology determines force distributions in one-dimensional random spring   networks	Knut M. Heidemann|Andrew O. Sageman-Furnas|Abhinav Sharma|Florian Rehfeldt|Christoph F. Schmidt|Max Wardetzky	  Networks of elastic fibers are ubiquitous in biological systems and often provide mechanical stability to cells and tissues. Fiber reinforced materials are also common in technology. An important characteristic of such materials is their resistance to failure under load. Rupture occurs when fibers break under excessive force and when that failure propagates. Therefore it is crucial to understand force distributions. Force distributions within such networks are typically highly inhomogeneous and are not well understood. Here we construct a simple one-dimensional model system with periodic boundary conditions by randomly placing linear springs on a circle. We consider ensembles of such networks that consist of $N$ nodes and have an average degree of connectivity $z$, but vary in topology. Using a graph-theoretical approach that accounts for the full topology of each network in the ensemble, we show that, surprisingly, the force distributions can be fully characterized in terms of the parameters $(N,z)$. Despite the universal properties of such $(N,z)$-ensembles, our analysis further reveals that a classical mean-field approach fails to capture force distributions correctly. We demonstrate that network topology is a crucial determinant of force distributions in elastic spring networks. 	
1707.01996v1	http://arxiv.org/pdf/1707.01996v1	2017	Capacity of Wireless Distributed Storage Systems with Broadcast Repair	Ping Hu|Chi Wan Sung|Terence H. Chan	  In wireless distributed storage systems, storage nodes are connected by wireless channels, which are broadcast in nature. This paper exploits this unique feature to design an efficient repair mechanism, called broadcast repair, for wireless distributed storage systems in the presence of multiple-node failures. Due to the broadcast nature of wireless transmission, we advocate a new measure on repair performance called repair-transmission bandwidth. In contrast to repair bandwidth, which measures the average number of packets downloaded by a newcomer to replace a failed node, repair-transmission bandwidth measures the average number of packets transmitted by helper nodes per failed node. A fundamental study on the storage capacity of wireless distributed storage systems with broadcast repair is conducted by modeling the storage system as a multicast network and analyzing the minimum cut of the corresponding information flow graph. The fundamental tradeoff between storage efficiency and repair-transmission bandwidth is also obtained for functional repair. The performance of broadcast repair is compared both analytically and numerically with that of cooperative repair, the basic repair method for wired distributed storage systems with multiple-node failures. While cooperative repair is based on the idea of allowing newcomers to exchange packets, broadcast repair is based on the idea of allowing a helper to broadcast packets to all newcomers simultaneously. We show that broadcast repair outperforms cooperative repair, offering a better tradeoff between storage efficiency and repair-transmission bandwidth. 	
1708.08286v2	http://arxiv.org/pdf/1708.08286v2	2018	A Scalable and Extensible Checkpointing Scheme for Massively Parallel   Simulations	Nils Kohl|Johannes Hötzer|Florian Schornbaum|Martin Bauer|Christian Godenschwager|Harald Köstler|Britta Nestler|Ulrich Rüde	  Realistic simulations in engineering or in the materials sciences can consume enormous computing resources and thus require the use of massively parallel supercomputers. The probability of a failure increases both with the runtime and with the number of system components. For future exascale systems it is therefore considered critical that strategies are developed to make software resilient against failures. In this article, we present a scalable, distributed, diskless, and resilient checkpointing scheme that can create and recover snapshots of a partitioned simulation domain. We demonstrate the efficiency and scalability of the checkpoint strategy for simulations with up to $40$ billion computational cells executing on more than $400$ billion floating point values. A checkpoint creation is shown to require only a few seconds and the new checkpointing scheme scales almost perfectly up to more than $260\,000$ ($2^{18}$) processes. To recover from a diskless checkpoint during runtime, we realize the recovery algorithms using ULFM MPI. The checkpointing mechanism is fully integrated in a state-of-the-art high-performance multi-physics simulation framework. We demonstrate the efficiency and robustness of the method with a realistic phase-field simulation originating in the material sciences and with a lattice Boltzmann method implementation. 	
1710.00940v1	http://arxiv.org/pdf/1710.00940v1	2017	The "weak" interdependence of infrastructure systems produces mixed   percolation transitions in multilayer networks	Run-Ran Liu|Daniel A. Eisenberg|Thomas P. Seager|Ying-Cheng Lai	  In this work, we propose an interdependent, multilayer network model and percolation process that matches infrastructures better than previous models by allowing some nodes to survive when their interdependent neighbors fail. We consider a node-to-link failure propagation mechanism and establish "weak" interdependence across layers via a tolerance parameter $\alpha$ which quantifies the likelihood that a node survives when one of its interdependent neighbors fails. We measure the robustness of any individual layer by the final size of its giant component. Analytical and numerical results show that weak interdependence produces a striking phenomenon: layers at different positions within the multilayer system experience distinct percolation transitions. Especially, layers with high super degree values percolate in an abrupt manner, while those with low super degree values exhibit both continuous and abrupt transitions. This novel phenomenon we call \emph{mixed percolation transitions} has significant implications for network robustness. Previous results that do not consider cascade tolerance and layer super degree may be under- or over-estimating the vulnerability of real systems. Moreover, since $\alpha$ represents a generic measure of various risk management strategies used to buffer infrastructure assets from cascades, our model reveals how nodal protection activities influence failure dynamics in interdependent, multilayer systems. 	
1710.02345v1	http://arxiv.org/pdf/1710.02345v1	2017	Atomistic simulations on ductile-brittle transition in <111> BCC Fe   nanowires	G. Sainath|B. K. Choudhary	  Molecular dynamics simulations have been performed to understand the influence of temperature on the tensile deformation and fracture behavior of $<$111$>$ BCC Fe nanowires. The simulations have been carried out at different temperatures in the range 10-1000 K employing a constant strain rate of $1\times$ $10^8$ $s^{-1}$. The results indicate that at low temperatures (10-375 K), the nanowires yield through the nucleation of a sharp crack and fails in brittle manner. On the other hand, nucleation of multiple 1/2$<$111$>$ dislocations at yielding followed by significant plastic deformation leading to ductile failure has been observed at high temperatures in the range 450-1000 K. At the intermediate temperature of 400 K, the nanowire yields through nucleation of crack associated with many mobile 1/2$<$111$>$ and immobile $<$100$>$ dislocations at the crack tip and fails in ductile manner. The ductile-brittle transition observed in $<$111$>$ BCC Fe nanowires is appropriately reflected in the stress-strain behavior and plastic strain at failure. The ductile-brittle transition increases with increasing nanowire size. The change in fracture behavior has been discussed in terms of the relative variations in yield and fracture stresses and change in slip behavior with respect to temperature. Further, the dislocation multiplication mechanism assisted by the kink nucleation from the nanowire surface observed at high temperatures has been presented. 	
1712.05532v1	http://arxiv.org/pdf/1712.05532v1	2017	Soft modes and strain redistribution in continuous models of amorphous   plasticity: the Eshelby paradigm, and beyond?	Xiangyu Cao|Alexandre Nicolas|Denny Trimcev|Alberto Rosso	  The deformation of disordered solids relies on swift and localised rearrangements of particles. The inspection of soft vibrational modes can help predict the locations of these rearrangements, while the strain that they actually redistribute mediates collective effects. Here, we study soft modes and strain redistribution in a two-dimensional continuous mesoscopic model based on a Ginzburg-Landau free energy for perfect solids, supplemented with a plastic disorder potential that accounts for shear softening and rearrangements. Regardless of the disorder strength, our numerical simulations show soft modes that are always sharply peaked at the softest point of the material (unlike what happens for the depinning of an elastic interface). Contrary to widespread views, the deformation halo around this peak does not always have a quadrupolar (Eshelby-like) shape. Instead, for finite and narrowly-distributed disorder, it looks like a fracture, with a strain field that concentrates along some easy directions. These findings are rationalised with analytical calculations in the case where the plastic disorder is confined to a point-like `impurity'. In this case, we unveil a continuous family of elastic propagators, which are identical for the soft modes and for the equilibrium configurations. This family interpolates between the standard quadrupolar propagator and the fracture-like one as the anisotropy of the elastic medium is increased. Therefore, we expect to see a fracture-like propagator when extended regions on the brink of failure have already softened along the shear direction and thus rendered the material anisotropic, but not failed yet. We speculate that this might be the case in carefully aged glasses just before macroscopic failure. 	
1802.00245v3	http://arxiv.org/pdf/1802.00245v3	2018	Towards Reliable (and Efficient) Job Executions in a Practical   Geo-distributed Data Analytics System	Xiaoda Zhang|Zhuzhong Qian|Sheng Zhang|Yize Li|Xiangbo Li|Xiaoliang Wang|Sanglu Lu	  Geo-distributed data analytics are increasingly common to derive useful information in large organisations. Naive extension of existing cluster-scale data analytics systems to the scale of geo-distributed data centers faces unique challenges including WAN bandwidth limits, regulatory constraints, changeable/unreliable runtime environment, and monetary costs. Our goal in this work is to develop a practical geo-distribued data analytics system that (1) employs an intelligent mechanism for jobs to efficiently utilize (adjust to) the resources (changeable environment) across data centers; (2) guarantees the reliability of jobs due to the possible failures; and (3) is generic and flexible enough to run a wide range of data analytics jobs without requiring any changes.   To this end, we present a new, general geo-distributed data analytics system, HOUTU, that is composed of multiple autonomous systems, each operating in a sovereign data center. HOUTU maintains a job manager (JM) for a geo-distributed job in each data center, so that these replicated JMs could individually and cooperatively manage resources and assign tasks. Our experiments on the prototype of HOUTU running across four Alibaba Cloud regions show that HOUTU provides nearly efficient job performance as in the existing centralized architecture, and guarantees reliable job executions when facing failures. 	
1803.02695v1	http://arxiv.org/pdf/1803.02695v1	2018	The Altes Family of Log-Periodic Chirplets and the Hyperbolic Chirplet   Transform	Donnacha Daly|Didier Sornette	  This work revisits a class of biomimetically inspired log-periodic waveforms first introduced by R.A. Altes in the 1970s for generalized target description. It was later observed that there is a close connection between such sonar techniques and wavelet decomposition for multiresolution analysis. Motivated by this, we formalize the original Altes waveforms as a family of hyperbolic chirplets suitable for the detection of accelerating time-series oscillations. The formalism results in a remarkably flexible set of wavelets with desirable properties of admissibility, regularity, vanishing moments, and time-frequency localization. These "Altes wavelets" also facilitate efficient implementation of the scale invariant hyperbolic chirplet transform (HCT).   From a practical perspective, log-periodic oscillations with an acceleration towards criticality can serve as indicators of an incipient bifurcation. Such signals abound in nature, often as precursors to phase transitions in the non-linear dynamics of complex systems. For example, the authors' interest lies in automatic detection of the well documented phenomenon of log-periodic price dynamics during financial bubbles and preceding market crashes. However, the methodology presented here is more widely applicable in such diverse domains as prediction of critical failures in mechanical systems, and fault detection in electrical networks. Examples beyond failure diagnostics include animal species identification via call recordings, commercial \& military radar, and there are many more. A synthetic application is presented in this report for illustrative purposes. 	
0106047v1	http://arxiv.org/pdf/cond-mat/0106047v1	2001	Oscillatory Finite-Time Singularities in Finance, Population and Rupture	K. D. Ide|D. Sornette	  We present a simple two-dimensional dynamical system where two nonlinear terms, exerting respectively positive feedback and reversal, compete to create a singularity in finite time decorated by accelerating oscillations. The power law singularity results from the increasing growth rate. The oscillations result from the restoring mechanism. As a function of the order of the nonlinearity of the growth rate and of the restoring term, a rich variety of behavior is documented analytically and numerically. The dynamical behavior is traced back fundamentally to the self-similar spiral structure of trajectories in phase space unfolding around an unstable spiral point at the origin. The interplay between the restoring mechanism and the nonlinear growth rate leads to approximately log-periodic oscillations with remarkable scaling properties. Three domains of applications are discussed: (1) the stock market with a competition between nonlinear trend-followers and nonlinear value investors; (2) the world human population with a competition between a population-dependent growth rate and a nonlinear dependence on a finite carrying capacity; (3) the failure of a material subjected to a time-varying stress with a competition between positive geometrical feedback on the damage variable and nonlinear healing. 	
0106096v1	http://arxiv.org/pdf/cond-mat/0106096v1	2001	Statistical mechanics of complex networks	Reka Albert|Albert-Laszlo Barabasi	  Complex networks describe a wide range of systems in nature and society, much quoted examples including the cell, a network of chemicals linked by chemical reactions, or the Internet, a network of routers and computers connected by physical links. While traditionally these systems were modeled as random graphs, it is increasingly recognized that the topology and evolution of real networks is governed by robust organizing principles. Here we review the recent advances in the field of complex networks, focusing on the statistical mechanics of network topology and dynamics. After reviewing the empirical data that motivated the recent interest in networks, we discuss the main models and analytical tools, covering random graphs, small-world and scale-free networks, as well as the interplay between topology and the network's robustness against failures and attacks. 	
0608730v1	http://arxiv.org/pdf/cond-mat/0608730v1	2006	Nanoscale damage during fracture in silica glass	Daniel Bonamy|Silke Prades|Cindy Rountree|Laurent Ponson|Davy Dalmas|Elisabeth Bouchaud|K. Ravi-Chandar|Claude Guillot	  We report here atomic force microscopy experiments designed to uncover the nature of failure mechanisms occuring within the process zone at the tip of a crack propagating into a silica glass specimen under stress corrosion. The crack propagates through the growth and coalescence of nanoscale damage spots. This cavitation process is shown to be the key mechanism responsible for damage spreading within the process zone. The possible origin of the nucleation of cavities, as well as the implications on the selection of both the cavity size at coalescence and the process zone extension are finally discussed. 	
0206061v2	http://arxiv.org/pdf/physics/0206061v2	2002	Fundamental Disagreement of Wave Mechanics with Relativity	Ezzat G. Bakhoum	  A number of well-known difficulties in physics resulted from merging the theory of relativity with the Compton-de Broglie wave mechanics. Two such problems were the failure of Dirac's relativistic wave equation to predict the correct velocity of the electron, and the fact that the measured yield from nuclear fission was found to be substantially less than the theoretical yield. It is shown that the origin of these and other problems stem from the inconsistency of the relativistic mass-energy equivalence principle with the fundamental assumptions of wave mechanics. An alternative view of the concept of mass-energy equivalence that results in a very good agreement between theory and experiment is demonstrated. The conclusions of this paper will be quite important for ongoing research, such as the current problem of the neutrino's mass. 	
0509061v3	http://arxiv.org/pdf/quant-ph/0509061v3	2006	From Einstein's Theorem to Bell's Theorem: A History of Quantum   Nonlocality	H. M. Wiseman	  In this Einstein Year of Physics it seems appropriate to look at an important aspect of Einstein's work that is often down-played: his contribution to the debate on the interpretation of quantum mechanics. Contrary to popular opinion, Bohr had no defence against Einstein's 1935 attack (the EPR paper) on the claimed completeness of orthodox quantum mechanics. I suggest that Einstein's argument, as stated most clearly in 1946, could justly be called Einstein's reality-locality-completeness theorem, since it proves that one of these three must be false. Einstein's instinct was that completeness of orthodox quantum mechanics was the falsehood, but he failed in his quest to find a more complete theory that respected reality and locality. Einstein's theorem, and possibly Einstein's failure, inspired John Bell in 1964 to prove his reality-locality theorem. This strengthened Einstein's theorem (but showed the futility of his quest) by demonstrating that either reality or locality is a falsehood. This revealed the full nonlocality of the quantum world for the first time. 	
0711.2993v1	http://arxiv.org/pdf/0711.2993v1	2007	Fragmentation processes in impact of spheres	H. A. Carmona|F. K. Wittel|F. Kun|H. J. Herrmann	  We study the brittle fragmentation of spheres by using a three-dimensional Discrete Element Model. Large scale computer simulations are performed with a model that consists of agglomerates of many particles, interconnected by beam-truss elements. We focus on the detailed development of the fragmentation process and study several fragmentation mechanisms. The evolution of meridional cracks is studied in detail. These cracks are found to initiate in the inside of the specimen with quasi-periodic angular distribution. The fragments that are formed when these cracks penetrate the specimen surface give a broad peak in the fragment mass distribution for large fragments that can be fitted by a two-parameter Weibull distribution. This mechanism can only be observed in 3D models or experiments. The results prove to be independent of the degree of disorder in the model. Our results significantly improve the understanding of the fragmentation process for impact fracture since besides reproducing the experimental observations of fragment shapes, impact energy dependence and mass distribution, we also have full access to the failure conditions and evolution. 	
0712.3918v1	http://arxiv.org/pdf/0712.3918v1	2007	Digital Image Mechanical Identification (DIMI)	François Hild|Stéphane Roux	  A continuous pathway from digital images acquired during a mechanical test to quantitative identification of a constitutive law is presented herein based on displacement field analysis. From images, displacement fields are directly estimated within a finite element framework. From the latter, the application of the equilibrium gap method provides the means for rigidity field evaluation. In the present case, a reconditioned formulation is proposed for a better stability. Last, postulating a specific form of a damage law, a linear system is formed that gives a direct access to the (non-linear) damage growth law in one step. The two last procedures are presented, validated on an artificial case, and applied to the case of a biaxial tension of a composite sample driven up to failure. A quantitative estimate of the quality of the determination is proposed, and in the last application, it is shown that no more than 7% of the displacement field fluctuations are not accounted for by the determined damage law. 	
1011.2287v2	http://arxiv.org/pdf/1011.2287v2	2012	Causal Symmetry and the Transactional Interpretation	Peter W. Evans	  Cramer's (1986) transactional interpretation of quantum mechanics posits retrocausal influences in quantum processes in an attempt to alleviate some of the interpretational difficulties of the Copenhagen interpretation. In response to Cramer's theory, Maudlin (2002) has levelled a significant objection against any retrocausal model of quantum mechanics. I present here an examination of the transactional interpretation of quantum mechanics and an analysis of Maudlin's critique. I claim that, although Maudlin correctly isolates the weaknesses of Cramer's theory, his justification for this weakness is off the mark. The cardinal vice of the transactional interpretation is its failure to provide a sufficient causal structure to constrain uniquely the behaviour of quantum systems and I contend that this is due to a lack of causal symmetry in the theory. In contrast, Maudlin attributes this shortcoming to retrocausality itself and emphasises an apparently fundamental incongruence between retrocausality and his own metaphysical picture of reality. I conclude by arguing that the problematic aspect of this incongruence is Maudlin's assumptions about what is appropriate for such a metaphysical picture. 	
1102.0468v2	http://arxiv.org/pdf/1102.0468v2	2011	Gauge invariant accounts of the Higgs mechanism	Ward Struyve	  The Higgs mechanism gives mass to Yang-Mills gauge bosons. According to the conventional wisdom, this happens through the spontaneous breaking of gauge symmetry. Yet, gauge symmetries merely reflect a redundancy in the state description and therefore the spontaneous breaking can not be an essential ingredient. Indeed, as already shown by Higgs and Kibble, the mechanism can be explained in terms of gauge invariant variables, without invoking spontaneous symmetry breaking. In this paper, we present a general discussion of such gauge invariant treatments for the case of the Abelian Higgs model, in the context of classical field theory. We thereby distinguish between two different notions of gauge: one that takes all local transformations to be gauge and one that relates gauge to a failure of determinism. 	
1107.1952v1	http://arxiv.org/pdf/1107.1952v1	2011	Impairment of double exchange mechanism in electron transport of iron   pnictides	Lei Hao|Chi-Cheng Lee|T. K. Lee	  Double exchange mechanism is believed to favor transport along ferromagnetic directions, the failure of which in explaining the unusual resistivity anisotropy in iron pnictides is investigated. Several factors intrinsic to the microscopic mechanism of transport in iron pnictides are identified and analyzed, including the moderate Hund's coupling, low local moment, and presence of two anisotropic degenerate orbitals xz and yz. In particular, the substantial second neighbor hoppings are found to be decisive in giving results opposite to the double exchange picture. In high temperature nonmagnetic phase, orbital ordering is shown to give the right trend of resistivity anisotropy as observed experimentally, advocating its essential role in electron transport of iron pnictides. 	
1203.0083v1	http://arxiv.org/pdf/1203.0083v1	2012	Molecular Dynamics Study of the Mechanical Behavior of Few Layer   Graphene	Young In Jhon|Myung S. Jhon	  Atomistic simulation was performed to study the mechanical properties of few layer graphene (FLG) in conjunction with monlayer graphene (MLG) under uniaxial elongation by systematically increasing the layer number from one to six. We found that the ultimate tensile strength and strain increased in these FLGs for both zigzag and armchair-directional elongations when compared with the results of MLG. We also found that the largest increments were obtained in bi- or tri-layer graphene for all the FLG systems we studied. Using atomic stress distribution analysis, it is observed that the width of the distribution became narrower, thus the maximum stress decreased in FLG compared to MLG at respective stages of identical tensile stress. It indicates that locally-driven highly elevated atomic stress of FLG has been effectively relaxed to the atoms in other layers through cooperative interlayer interaction. This effect explains the reason for synergetic mechanical strengthening of FLG since tensile failure is critically influenced by maximum atomic stress. Furthermore, the Young's moduli were slightly smaller for all FLGs compared to MLG. 	
1204.6010v1	http://arxiv.org/pdf/1204.6010v1	2012	Correlations between mechanical, structural, and dynamical properties of   polymer nanocomposites	Kutvonen Aki|Rossi Giulia|Ala-Nissila Tapio	  We study the structural and dynamical mechanisms of reinforcement of a polymer nanocomposite (PNC) via coarse-grained molecular dynamics simulations. In a regime of strong polymer-filler interactions, the stress at failure of the PNC is clearly correlated to structural quantities, such as the filler loading, the surface area of the polymer-filler interface, and the network structure. Additionally, we find that small fillers, of the size of the polymer monomers, are the most effective at reinforcing the matrix by surrounding the polymer chains and maximizing the number of strong polymer-filler interactions. Such a structural configuration is correlated to a dynamical feature, namely, the minimization of the relative mobility of the fillers with respect to the polymer matrix. 	
1207.3591v3	http://arxiv.org/pdf/1207.3591v3	2012	Microscopic Mechanism of Shear Bands in Amorphous Solids	Ratul Dasgupta|H. George E. Hentschel|Itamar Procaccia	  The fundamental instability responsible for the shear localization which results in shear bands in amorphous solids remains unknown despite enormous amount of research, both experimental and theoretical. As this is the main mechanism for the failure of metallic glasses, understanding the instability is invaluable in finding how to stabilize such materials against the tendency to shear localize. In this Letter we explain the mechanism for shear localization under shear, which is the appearance of highly correlated lines of Eshelby-like quadrupolar singularities which organize the non-affine plastic flow of the amorphous solid into a shear band. We prove analytically that such highly correlated solutions in which $\C N$ quadrupoles are aligned with equal orientations are minimum energy states when the strain is high enough. The line lies at 45 degrees to the compressive stress. 	
1208.1184v1	http://arxiv.org/pdf/1208.1184v1	2012	Payment Rules through Discriminant-Based Classifiers	Paul Duetting|Felix Fischer|Pitchayut Jirapinyo|John K. Lai|Benjamin Lubin|David C. Parkes	  In mechanism design it is typical to impose incentive compatibility and then derive an optimal mechanism subject to this constraint. By replacing the incentive compatibility requirement with the goal of minimizing expected ex post regret, we are able to adapt statistical machine learning techniques to the design of payment rules. This computational approach to mechanism design is applicable to domains with multi-dimensional types and situations where computational efficiency is a concern. Specifically, given an outcome rule and access to a type distribution, we train a support vector machine with a special discriminant function structure such that it implicitly establishes a payment rule with desirable incentive properties. We discuss applications to a multi-minded combinatorial auction with a greedy winner-determination algorithm and to an assignment problem with egalitarian outcome rule. Experimental results demonstrate both that the construction produces payment rules with low ex post regret, and that penalizing classification errors is effective in preventing failures of ex post individual rationality. 	
1303.6260v1	http://arxiv.org/pdf/1303.6260v1	2013	E-HORM: An Energy-efficient Hole Removing Mechanism in Wireless Senor   Networks	M. B. Rasheed|N. Javaid|Z. A. Khan|U. Qasim|M. Ishfaq	  Cluster based routing protocols forWireless Sensor Networks (WSNs) have been widely used for better performance in terms of energy efficiency. Efficient use of energy is challenging task of designing these protocols. Energy holes are created due to quickly drain the energy of a few nodes due to nonuniform node distribution in the network. Normally, energy holes make the data routing failure when nodes transmit data back to the sink. We propose Energy-efficientHOle Removing Mechanism (E-HORM) technique to remove energy holes. In this technique, we use sleep and awake mechanism for sensor nodes to save energy. This approach finds the maximum distance nodes to calculate the maximum energy for data transmission. We consider it as a threshold energy Eth. Every node first checks its energy level for data transmission. If the energy level of node is less than Eth, it cannot transmit data. 	
1305.4032v1	http://arxiv.org/pdf/1305.4032v1	2013	Radiative Mechanisms in GRB prompt emission	Asaf Pe'er	  Motivated by the Fermi gamma-ray space telescope results, in recent years immense efforts were given to understanding the mechanism that leads to the prompt emission observed. The failure of the optically thin emission models (synchrotron and synchrotron self Compton) increased interest in alternative models. Optically thick models, while having several advantages, also face difficulty in capturing several key observables. Theoretical efforts are focused in two main directions: (1) mechanisms that act to broaden the Planck spectrum; and (2) combining the optically thin and optically thick models to a hybrid model that could explain the key observables. 	
1306.1448v1	http://arxiv.org/pdf/1306.1448v1	2013	I am 4 vho: new approach to improve seamless vertical hanover in   heterogeneous wireless networks	Omar Khattab|Omar Alani	  Two mechanisms have been proposed independently by IEEE and 3GPP; namely, Media Independent Handover (MIH) and Access Network Discovery and Selection Function (ANDSF), respectively. These mechanisms enable a seamless Vertical Handover (VHO) between the different types of technologies (3GPP and non-3GPP), such as GSM (Global System for Mobile Communication), Wireless Fidelity (Wi- Fi), Worldwide Interoperability for Microwave Access (WiMAX), Universal Mobile Telecommunications System (UMTS) and Long Term Evolution (LTE). In this paper, we overview these mechanisms and show their components, benefits and drawbacks. Then we present our Imperative Alternative MIH for Vertical Handover (I AM 4 VHO) approach based on the approaches that have been studied in the literature with better performance (packet loss and latency), less connection failure (probability of reject sessions), less complexity and more exhaustive for enhancing VHO heterogeneous wireless networks environment. 	
1308.1888v1	http://arxiv.org/pdf/1308.1888v1	2013	Strand-Based Approach to Patch Security Protocols	Dieter Hutter|Raul Monroy	  In this paper, we introduce a mechanism that aims to speed up the development cycle of security protocols, by adding automated aid for diagnosis and repair. Our mechanism relies on existing verification tools analyzing intermediate protocols and synthesizing potential attacks if the protocol is flawed. The analysis of these attacks (including type flaw attacks) pinpoints the source of the failure and controls the synthesis of appropriate patches to the protocol. Using strand spaces, we have developed general guidelines for protocol repair, and captured them into formal requirements on (sets of) protocol steps. For each requirement, there is a collection of rules that transform a set of protocol steps violating the requirement into a set conforming it. We have implemented our mechanism into a tool, called SHRIMP. We have successfully tested SHRIMP on numerous faulty protocols, all of which were successfully repaired, fully automatically. 	
1309.4990v1	http://arxiv.org/pdf/1309.4990v1	2013	"Superluminal paradox" in wavepacket propagation and its quantum   mechanical resolution	D. Sokolovski|E. Akhmatskaya	  We analyse in detail the reshaping mechanism leading to apparently "superluminal" advancement of a wave packet traversing a classically forbidden region. In the coordinate representation, a barrier is shown to act as an effective beamsplitter, recombining envelopes of the freely propagating pulse with various spacial shifts. Causality ensures that none of the constituent envelopes are advanced with respect to free propagation, yet the resulting pulse is advanced due to a peculiar interference effect, similar to the one responsible for "anomalous" values which occur in Aharonov's "weak measurements". In the momentum space, the effect is understood as a bandwidth phenomenon, where the incident pulse probes local, rather than global, analytical properties of the transmission amplitude T (p). The advancement is achieved when T (p) mimics locally an exponential behaviour, similar to the one occurring in Berry's "superoscillations". Seen in a broader quantum mechanical context, the "paradox" is but a consequence of an attempt to obtain "which way?" information without destroying the interference between the pathways of interest. This explains, to a large extent, the failure to adequately describe tunnelling in terms of a single "tunnelling time". 	
1407.1606v1	http://arxiv.org/pdf/1407.1606v1	2014	Elasticity and Plasticity in Stiff and Flexible Oligomeric Glasses	Oleg Gendelman|H. George E. Hentschel|Pankaj K. Mishra|Itamar Procaccia|Jacques Zylberg	  In this paper we focus on the mechanical properties of oligomeric glasses (waxes), employing a microscopic model that provides, via numerical simulations, information about the shear modulus of such materials, the failure mechanism via plastic instabilities and about the geometric responses of the oligomers themselves to a mechanical load. We present a microscopic theory that explains the numerically observed phenomena, including an exact theory of the shear modulus and of the plastic instabilities, both local and system spanning. In addition we present a model to explain the geometric changes in the oligomeric chains under increasing strains. 	
1407.5927v1	http://arxiv.org/pdf/1407.5927v1	2014	Strain localization in a nanocrystalline metal: Atomic mechanisms and   the effect of testing conditions	Timothy J. Rupert	  Molecular dynamics simulations are used to investigate strain localization in a model nanocrystalline metal. The atomic mechanisms of such catastrophic failure are first studied for two grain sizes of interest. Detailed analysis shows that the formation of a strain path across the sample width is crucial, and can be achieved entirely through grain boundary deformation or through a combination of grain boundary sliding and grain boundary dislocation emission. Pronounced mechanically-induced grain growth is also found within the strain localization region. The effects of testing conditions on strain localization are also highlighted, to understand the conditions that promote shear banding and compare these observations to metallic glass behavior. We observed that, while strain localization occurs at low temperatures and slow strain rates, a shift to more uniform plastic flow is observed when either strain rate or temperature is increased. We also explore how external sample dimensions influence strain localization, but find no size effect for the grain sizes and samples sizes studied here. 	
1408.4831v1	http://arxiv.org/pdf/1408.4831v1	2014	Self-replicating cracks: a collaborative fracture mode in thin films	Joel Marthelot|Benoit Roman|Jose Bico|Jeremie Teisseire|Davy Dalmas|Francisco Melo	  Straight cracks are observed in thin coatings under residual tensile stress, resulting into the classical network pattern observed in china crockery, old paintings or dry mud. Here, we present a novel fracture mechanism where delamination and propagation occur simultaneously, leading to the spontaneous self-replication of an initial template. Surprisingly, this mechanism is active below the standard critical tensile load for channel cracks and selects a robust interaction length scale on the order of 30 times the film thickness. Depending on triggering mechanisms, crescent alleys, spirals or long bands are generated over a wide range of experimental parameters. We describe with a simple physical model the selection of the fracture path and provide a configuration diagram displaying the different failure modes. 	
1411.5258v2	http://arxiv.org/pdf/1411.5258v2	2015	Neuronal Response Impedance Mechanism Implementing Cooperative Networks   with Low Firing Rates and Microseconds Precision	Roni Vardi|Amir Goldental|Hagar Marmari|Haya Brama|Edward Stern|Shira Sardi|Pinhas Sabo|Ido Kanter	  Realizations of low firing rates in neural networks usually require globally balanced distributions among excitatory and inhibitory links, while feasibility of temporal coding is limited by neuronal millisecond precision. We show that cooperation, governing global network features, emerges through nodal properties, as opposed to link distributions. Using in vitro and in vivo experiments we demonstrate microsecond precision of neuronal response timings under low stimulation frequencies, whereas moderate frequencies result in a chaotic neuronal phase characterized by degraded precision. Above a critical stimulation frequency, which varies among neurons, response failures were found to emerge stochastically such that the neuron functions as a low pass filter, saturating the average inter-spike-interval. This intrinsic neuronal response impedance mechanism leads to cooperation on a network level, such that firing rates are suppressed towards the lowest neuronal critical frequency simultaneously with neuronal microsecond precision. Our findings open up opportunities of controlling global features of network dynamics through few nodes with extreme properties. 	
1501.00360v1	http://arxiv.org/pdf/1501.00360v1	2015	Critical length limiting super-low friction	Ming Ma|Andrea Benassi|Andrea Vanossi|Michael Urbakh	  Since the demonstration of super-low friction (superlubricity) in graphite at nanoscale, one of the main challenges in the field of nano- and micro-mechanics was to scale this phenomenon up. A key question to be addressed is to what extent superlubricity could persist, and what mechanisms could lead to its failure. Here, using an edge-driven Frenkel-Kontorova model, we establish a connection between the critical length above which superlubricity disappears and both intrinsic material properties and experimental parameters. A striking boost in dissipated energy with chain length emerges abruptly due to a high-friction stick-slip mechanism caused by deformation of the slider leading to a local commensuration with the substrate lattice. We derived a parameter-free analytical model for the critical length that is in excellent agreement with our numerical simulations. Our results provide a new perspective on friction and nano-manipulation and can serve as a theoretical basis for designing nano-devices with super-low friction, such as carbon nanotubes. 	
1507.03779v3	http://arxiv.org/pdf/1507.03779v3	2016	Failure of the Volume Function in Granular Statistical Mechanics and an   Alternative Formulation	Raphael Blumenfeld|Shahar Amitai|Joe F. Jordan|Rebecca Hihinashvili	  We first show that the currently accepted statistical mechanics for granular matter is flawed. The reason is that it is based on the volume function, which depends only on a minute fraction of all the structural degrees of freedom and is unaffected by most of the configurational microstates. Consequently, the commonly used partition function underestimates the entropy severely. We then propose a new formulation, replacing the volume function with a ${\it connectivity}$ function that depends on all the structural degrees of freedom and accounts correctly for the entire entropy. We discuss the advantages of the new formalism and derive explicit results for two- and three-dimensional systems. We test the formalism by calculating the entropy of an experimental two-dimensional system, as a function of system size, and showing that it is an extensive variable. 	
1509.01950v1	http://arxiv.org/pdf/1509.01950v1	2015	Hierarchical structures for a robustness-oriented capacity design	Enrico Masoero|Falk K. Wittel|Hans J. Herrmann|B. M. Chiaia	  In this paper, we study the response of 2D framed structures made of rectangular cells, to the sudden removal of columns. We employ a simulation algorithm based on the Discrete Element Method, where the structural elements are represented by elasto-plastic Euler Bernoulli beams with elongation-rotation failure threshold. The effect of structural cell slenderness and of topological hierarchy on the dynamic residual strength after damage $\ROne$ is investigated. Topologically \textit{hierarchical} frames have a primary structure made of few massive elements, while \textit{homogeneous} frames are made of many thin elements. We also show how $\ROne$ depends on the activated collapse mechanisms, which are determined by the mechanical hierarchy between beams and columns, i.e. by their relative strength and stiffness. Finally, principles of robustness-oriented capacity design which seem to be in contrast to the conventional anti-seismic capacity design are addressed. 	
1512.04562v1	http://arxiv.org/pdf/1512.04562v1	2015	Multiscale modelling of tumour growth induced by circadian rhythm   disruption in epithelial tissue	D. A. Bratsun|D. V. Merkuriev|A. P. Zakharov|L. M. Pismen	  We propose a multiscale chemo-mechanical model of cancer tumour development in an epithelial tissue. The model is based on transformation of normal cells into the cancerous state triggered by a local failure of spatial synchronisation of the circadian rhythm. The model includes mechanical interactions and chemical signal exchange between neighbouring cells, as well as division of cells and intercalation, and allows for modification of the respective parameters following transformation into the cancerous state. The numerical simulations reproduce different dephasing patterns - spiral waves and quasistationary clustering, with the latter being conducive to cancer formation. Modification of mechanical properties reproduces distinct behaviour of invasive and localised carcinoma. 	
1602.06139v1	http://arxiv.org/pdf/1602.06139v1	2016	Toward a virtual material for lifetime prediction of CMCs	Martin Genet|Pierre Ladevèze|Gilles Lubineau|Emmanuel Baranger|A Mouret	  A first version of a multi-scale, multi-physic hybrid model --called virtual material-- for predictions on Self-Healing Ceramic Matrix Composite's (CMCs) lifetime is presented. The model has a mechanical and a chemical part, which are presented here in their actual state of development. The mechanical part provides precise data for the chemical models through an hybrid --melting continuum damage macro model discrete crack surfaces-- representation of the morphology of the crack network at yarn scale. The chemical part should provide predictions on the structure's lifetime using a model of the self-healing process, not yet achieved then not presented here, and a model of fiber sub-critical failure under mechanical and chemical load. 	
1608.04788v2	http://arxiv.org/pdf/1608.04788v2	2016	High anisotropy of fully hydrogenated borophene	Zhiqiang Wang|Tie-Yu Lü|Hui-Qiong Wang|Yuan-Ping Feng|Jin-Cheng Zheng	  We have studied the mechanical properties and phonon dispersions of fully hydrogenated borophene (borophane) under strains by first principles calculations. Uniaxial tensile strains along the a- and b-direction, respectively, and biaxial tensile strain have been considered. Our results show that the mechanical properties and phonon stability of borophane are both highly anisotropic. The ultimate tensile strain along the a-direction is only 0.12, but it can be as large as 0.30 along the b-direction. Compared to borophene and other 2D materials (graphene, graphane, silicene, silicane, h-BN, phosphorene and MoS2), borophane presents the most remarkable anisotropy in in-plane ultimate strain, which is very important for strain engineering. Furthermore, the phonon dispersions under the three applied strains indicate that borophane can withstand up to 5% and 15% uniaxial tensile strain along the a- and b-direction, respectively, and 9% biaxial tensile strain, indicating that mechanical failure in borophane is likely to originate from phonon instability. 	
1611.04189v3	http://arxiv.org/pdf/1611.04189v3	2016	Emergent gravity from Eguchi-Kawai reduction	Edgar Shaghoulian	  Holographic theories with a local gravitational dual have a number of striking features. Here I argue that many of these features are controlled by the Eguchi-Kawai mechanism, which is proposed to be a hallmark of such holographic theories. Higher-spin holographic duality is presented as a failure of the Eguchi-Kawai mechanism, and its restoration illustrates the deformation of higher-spin theory into a proper string theory with a local gravitational limit. AdS/CFT is used to provide a calculable extension of the Eguchi-Kawai mechanism to field theories on curved manifolds and thereby introduce "topological volume independence." Finally, I discuss implications for a general understanding of the extensivity of the Bekenstein-Hawking-Wald entropy. 	
1707.07473v3	http://arxiv.org/pdf/1707.07473v3	2017	Verifying Policy Enforcers	Oliviero Riganelli|Daniela Micucci|Leonardo Mariani|Yliès Falcone	  Policy enforcers are sophisticated runtime components that can prevent failures by enforcing the correct behavior of the software. While a single enforcer can be easily designed focusing only on the behavior of the application that must be monitored, the effect of multiple enforcers that enforce different policies might be hard to predict. So far, mechanisms to resolve interferences between enforcers have been based on priority mechanisms and heuristics. Although these methods provide a mechanism to take decisions when multiple enforcers try to affect the execution at a same time, they do not guarantee the lack of interference on the global behavior of the system. In this paper we present a verification strategy that can be exploited to discover interferences between sets of enforcers and thus safely identify a-priori the enforcers that can co-exist at run-time. In our evaluation, we experimented our verification method with several policy enforcers for Android and discovered some incompatibilities. 	
0706.0403v1	http://arxiv.org/pdf/0706.0403v1	2007	Asymptotic Behavior of Total Times For Jobs That Must Start Over If a   Failure Occurs	Soeren Asmussen|Pierre Fiorini|Lester Lipsky|Tomasz Rolski|Robert Sheahan	  Many processes must complete in the presence of failures. Different systems respond to task failure in different ways. The system may resume a failed task from the failure point (or a saved checkpoint shortly before the failure point), it may give up on the task and select a replacement task from the ready queue, or it may restart the task. The behavior of systems under the first two scenarios is well documented, but the third ({\em RESTART}) has resisted detailed analysis. In this paper we derive tight asymptotic relations between the distribution of {\em task times} without failures to the {\em total time} when including failures, for any failure distribution. In particular, we show that if the task time distribution has an unbounded support then the total time distribution $H$ is always heavy-tailed. Asymptotic expressions are given for the tail of $H$ in various scenarios. The key ingredients of the analysis are the Cram\'er--Lundberg asymptotics for geometric sums and integral asymptotics, that in some cases are obtained via Tauberian theorems and in some cases by bare-hand calculations. 	
0810.3438v1	http://arxiv.org/pdf/0810.3438v1	2008	Efficient Algorithms and Routing Protocols for Handling Transient Single   Node Failures	Amit M Bhosle|Teofilo F Gonzalez	  Single node failures represent more than 85% of all node failures in the today's large communication networks such as the Internet. Also, these node failures are usually transient. Consequently, having the routing paths globally recomputed does not pay off since the failed nodes recover fairly quickly, and the recomputed routing paths need to be discarded. Instead, we develop algorithms and protocols for dealing with such transient single node failures by suppressing the failure (instead of advertising it across the network), and routing messages to the destination via alternate paths that do not use the failed node. We compare our solution to that of Ref. [11] wherein the authors have presented a "Failure Insensitive Routing" protocol as a proactive recovery scheme for handling transient node failures. We show that our algorithms are faster by an order of magnitude while our paths are equally good. We show via simulation results that our paths are usually within 15% of the optimal for randomly generated graph with 100-1000 nodes. 	
1012.4025v1	http://arxiv.org/pdf/1012.4025v1	2010	Optimal adaptive control of cascading power grid failures	Daniel Bienstock	  We present theoretical results and experiments with parallel algorithms for computing an adaptive, online control with the objective of attenuating a power grid cascading failure. 	
1108.3831v1	http://arxiv.org/pdf/1108.3831v1	2011	A remark on the failure of multiplicity one for GSp(4)	Daniel File|Ramin Takloo-Bighash	  We revisit a classical result of Howe and Pitatski-Shapiro on the failure of strong multiplicity one for $\GSp(4)$. 	
1407.3286v1	http://arxiv.org/pdf/1407.3286v1	2014	Solvability-Based Comparison of Failure Detectors	Srikanth Sastry|Josef Widder	  Failure detectors are oracles that have been introduced to provide processes in asynchronous systems with information about faults. This information can then be used to solve problems otherwise unsolvable in asynchronous systems. A natural question is on the "minimum amount of information" a failure detector has to provide for a given problem. This question is classically addressed using a relation that states that a failure detector D is stronger (that is, provides "more, or better, information") than a failure detector D' if D can be used to implement D'. It has recently been shown that this classic implementability relation has some drawbacks. To overcome this, different relations have been defined, one of which states that a failure detector D is stronger than D' if D can solve all the time-free problems solvable by D'. In this paper we compare the implementability-based hierarchy of failure detectors to the hierarchy based on solvability. This is done by introducing a new proof technique for establishing the solvability relation. We apply this technique to known failure detectors from the literature and demonstrate significant differences between the hierarchies. 	
1409.2223v1	http://arxiv.org/pdf/1409.2223v1	2014	Assessment of classification techniques on predicting success or failure   of Software reusability	Nahid Hajizadeh|Manijeh Keshtgari|Marzieh Ahmadzadeh	  Assessment of classification techniques on predicting success or failure of Software reusability 	
1708.09494v1	http://arxiv.org/pdf/1708.09494v1	2017	An Exploratory Study of Field Failures	Luca Gazzola|Leonardo Mariani|Fabrizio Pastore|Mauro Pezz`e	  Field failures, that is, failures caused by faults that escape the testing phase leading to failures in the field, are unavoidable. Improving verification and validation activities before deployment can identify and timely remove many but not all faults, and users may still experience a number of annoying problems while using their software systems. This paper investigates the nature of field failures, to understand to what extent further improving in-house verification and validation activities can reduce the number of failures in the field, and frames the need of new approaches that operate in the field. We report the results of the analysis of the bug reports of five applications belonging to three different ecosystems, propose a taxonomy of field failures, and discuss the reasons why failures belonging to the identified classes cannot be detected at design time but shall be addressed at runtime. We observe that many faults (70%) are intrinsically hard to detect at design-time. 	
1803.00384v1	http://arxiv.org/pdf/1803.00384v1	2018	Fibres of Failure: Classifying errors in predictive processes	Leo Carlsson|Gunnar Carlsson|Mikael Vejdemo-Johansson	  We describe Fibres of Failure (FiFa), a method to classify failure modes of predictive processes using the Mapper algorithm from Topological Data Analysis.   Our method uses Mapper to build a graph model of input data stratified by prediction error.   Groupings found in high-error regions of the Mapper model then provide distinct failure modes of the predictive process.   We demonstrate FiFa on misclassifications of MNIST images with added noise, and demonstrate two ways to use the failure mode classification: either to produce a correction layer that adjusts predictions by similarity to the failure modes; or to inspect members of the failure modes to illustrate and investigate what characterizes each failure mode. 	
0612141v1	http://arxiv.org/pdf/cs/0612141v1	2006	Exact Failure Frequency Calculations for Extended Systems	Annie Druault-Vicard|Christian Tanguy	  This paper shows how the steady-state availability and failure frequency can be calculated in a single pass for very large systems, when the availability is expressed as a product of matrices. We apply the general procedure to $k$-out-of-$n$:G and linear consecutive $k$-out-of-$n$:F systems, and to a simple ladder network in which each edge and node may fail. We also give the associated generating functions when the components have identical availabilities and failure rates. For large systems, the failure rate of the whole system is asymptotically proportional to its size. This paves the way to ready-to-use formulae for various architectures, as well as proof that the differential operator approach to failure frequency calculations is very useful and straightforward. 	
0906.1328v1	http://arxiv.org/pdf/0906.1328v1	2009	Multidimensional Analysis of System Logs in Large-scale Cluster Systems	Wei Zhou|Jianfeng Zhan|Dan Meng	  It is effective to improve the reliability and availability of large-scale cluster systems through the analysis of failures. Existed failure analysis methods understand and analyze failures from one or few dimension. The analysis results are partial and with less precision because of the limitation of data source. This paper presents multidimensional analysis based on graph mining to analyze multi-source system logs, which is a promising failure analysis method to get more complete and precise failure knowledge. 	
1012.2411v1	http://arxiv.org/pdf/1012.2411v1	2010	Measurement of Reciprocity Failure in Near Infrared Detectors	T. Biesiadzinski|W. Lorenzon|R. Newman|M. Schubnell|G. Tarle|C. Weaverdyck	  Flux dependent non-linearity (reciprocity failure) in HgCdTe near infrared detectors can severely impact an instrument's performance, in particular with respect to precision photometric measurements. The cause of this effect is presently not understood. To investigate reciprocity failure, a dedicated test system was built. For flux levels between 1 and 50,000 photons/s, a sensitivity to reciprocity failure of approximately 0.1%/decade was achieved. A wavelength independent non-linearity due to reciprocity failure of about 0.35%/decade was measured in a 1.7 micron HgCdTe detector. 	
1106.1090v1	http://arxiv.org/pdf/1106.1090v1	2011	Reciprocity Failure in HgCdTe Detectors: Measurements and Mitigation	T. Biesiadzinski|W. Lorenzon|R. Newman|M. Schubnell|G. Tarle|C. Weaverdyck	  A detailed study of reciprocity failure in four 1.7 micron cutoff HgCdTe near-infrared detectors is presented. The sensitivity to reciprocity failure is approximately 0.1%\decade over up to five orders of magnitude in illumination intensity. The four detectors, which represent three successive production runs with modified growth recipes, show large differences in amount and spatial structure of reciprocity failure. Reciprocity failure could be reduced to negligible levels by cooling the detectors to about 110 K. No wavelength dependence was observed. The observed spatial structure appears to be weakly correlated with image persistence. 	
1208.5029v1	http://arxiv.org/pdf/1208.5029v1	2012	Probability of Failure in Hypersonic Engines Using Large Deviations	George Papanicolaou|Nicholas West|Tzu-Wei Yang	  We consider a reduced order model of an air-breathing hypersonic engine with a time-dependent stochastic inflow that may cause the failure of the engine. The probability of failure is analyzed by the Freidlin-Wentzell theory, the large deviation principle for finite dimensional stochastic differential equations. We compute the asymptotic failure probability by numerically solving the constrained optimization related to the large deviation problem. A large-deviation-based importance sampling suggested by the most probable inflow perturbation is also implemented to compute the probability of failure of the engine. The numerical simulations show that the importance sampling method is much more efficient than the basic Monte Carlo method. 	
1402.5899v1	http://arxiv.org/pdf/1402.5899v1	2014	Structural failure of two-density-layer cohesionless biaxial ellipsoids	Masatoshi Hirabayashi	  This paper quantitatively evaluates structural failure of biaxial cohesionless ellipsoids that have a two-density-layer distribution. The internal density layer is modeled as a sphere, while the external density layer is the rest of the part. The density is supposed to be constant in each layer. The present study derives averaged stresses over the whole volume of these bodies and uses limit analysis to determine their global failure. The upper bound condition of global failure is considered in terms of the size of the internal layer and the aspect ratio of the shape. The result shows that the two-density-layer causes the body to have different strength against structural failure. 	
1404.7565v2	http://arxiv.org/pdf/1404.7565v2	2014	Investigating SCADA Failures in Interdependent Critical Infrastructure   Systems	Razgar Ebrahimy	  This paper is based on the initial ideas of a PhD proposal which will investigate SCADA failures in physical infrastructure systems. The results will be used to develop a new notation to help risk assessment using dependable computing concepts. SCADA systems are widely used within critical infrastructures to perform system controls and deliver services to linked and dependent systems. Failures in SCADA systems will be investigated to help us understand and prevent cascading failures in future. 	
1709.10166v1	http://arxiv.org/pdf/1709.10166v1	2017	Emergent failures and cascades in power grids: a statistical physics   perspective	Tommaso Nesti|Alessandro Zocca|Bert Zwart	  We consider complex networks where line failures occur indirectly as line flows are influenced by fluctuating input at nodes, a prime example being a power grid where power is generated by renewable sources. We examine the propagation of such emergent failures in the small noise regime, combining concepts from statistical physics and the physics of power flow. In particular we characterize rigorously and explicitly the configuration of inputs responsible for failures and cascades, and analyze the propagation of failures, which often is not of nearest-neighbor type. 	
1605.09350v1	http://arxiv.org/pdf/1605.09350v1	2016	Computing backup forwarding rules in Software-Defined Networks	Niels L. M. van Adrichem|Farabi Iqbal|Fernando A. Kuipers	  The past century of telecommunications has shown that failures in networks are prevalent. Although much has been done to prevent failures, network nodes and links are bound to fail eventually. Failure recovery processes are therefore needed. Failure recovery is mainly influenced by (1) detection of the failure, and (2) circumvention of the detected failure. However, especially in SDNs where controllers recompute network state reactively, this leads to high delays. Hence, next to primary rules, backup rules should be installed in the switches to quickly detour traffic once a failure occurs. In this work, we propose algorithms for computing an all-to-all primary and backup network forwarding configuration that is capable of circumventing link and node failures. Omitting the high delay invoked by controller recomputation through preconfiguration, our proposal's recovery delay is close to the detection time which is significantly below the 50 ms rule of thumb. After initial recovery, we recompute network configuration to guarantee protection from future failures. Our algorithms use packet-labeling to guarantee correct and shortest detour forwarding. The algorithms and labeling technique allow packets to return to the primary path and are able to discriminate between link and node failures. The computational complexity of our solution is comparable to that of all-to-all-shortest paths computations. Our experimental evaluation on both real and generated networks shows that network configuration complexity highly decreases compared to classic disjoint paths computations. Finally, we provide a proof-of-concept OpenFlow controller in which our proposed configuration is implemented, demonstrating that it readily can be applied in production networks. 	
1608.04002v2	http://arxiv.org/pdf/1608.04002v2	2016	Survivable Cloud Network Design Against Multiple Failures Through   Protecting Spanning Trees	Zhili Zhou|Tachun Lin|Krishnaiyan Thulasiraman	  Survivable design of cross-layer networks, such as the cloud computing infrastructure, lies in its resource deployment and allocation and mapping of the logical (virtual datacenter/IP) network into the physical infrastructure (cloud backbone/WDM) such that link or node failure(s) in the physical infrastructure would not result in cascading failures in the logical network. Most of the prior approaches for survivable cross-layer network design aim at single-link failure scenario, which are not applicable to the more challenging multi-failure scenarios. Also, as many of these approaches use the cross-layer cut concept, enumeration of all cuts in the network is required and thus introducing exponential number of constraints. To overcome these difficulties, we investigate in this paper survivable mapping approaches against multiple physical link failures and its special case, Shared Risk Link Group (SRLG) failure. We present the necessary and sufficient conditions based on both cross-layer spanning trees and cutsets to guarantee a survivable mapping when multiple physical link failures occur. Based on the necessary and sufficient conditions, we propose to solve the problem through (1) mixed-integer linear programs which avoid enumerating all combinations of link failures, and (2) an algorithm which generates/adds logical spanning trees sequentially. Our simulation results show that the proposed approaches can produce survivable mappings effectively against both $k$- and SRLG-failures. 	
0902.2121v1	http://arxiv.org/pdf/0902.2121v1	2009	The 1/r singularity in weakly nonlinear fracture mechanics	Eran Bouchbinder|Ariel Livne|Jay Fineberg	  Material failure by crack propagation essentially involves a concentration of large displacement-gradients near a crack's tip, even at scales where no irreversible deformation and energy dissipation occurs. This physical situation provides the motivation for a systematic gradient expansion of general nonlinear elastic constitutive laws that goes beyond the first order displacement-gradient expansion that is the basis for linear elastic fracture mechanics (LEFM). A weakly nonlinear fracture mechanics theory was recently developed by considering displacement-gradients up to second order. The theory predicts that, at scales within a dynamic lengthscale $\ell$ from a crack's tip, significant $\log{r}$ displacements and $1/r$ displacement-gradient contributions arise. Whereas in LEFM the $1/r$ singularity generates an unbalanced force and must be discarded, we show that this singularity not only exists but is {\em necessary} in the weakly nonlinear theory. The theory generates no spurious forces and is consistent with the notion of the autonomy of the near-tip nonlinear region. The J-integral in the weakly nonlinear theory is also shown to be path-independent, taking the same value as the linear elastic J-integral. Thus, the weakly nonlinear theory retains the key tenets of fracture mechanics, while providing excellent quantitative agreement with measurements near the tip of single propagating cracks. As $\ell$ is consistent with lengthscales that appear in crack tip instabilities, we suggest that this theory may serve as a promising starting point for resolving open questions in fracture dynamics. 	
1301.1614v1	http://arxiv.org/pdf/1301.1614v1	2013	Polycrystal model of the mechanical behavior of a Mo-TiC30vol.%   metal-ceramic composite using a 3D microstructure map obtained by a dual beam   FIB-SEM	Denis Cédat|Olivier Fandeur|Colette Rey|Dierk Raabe	  The mechanical behavior of a Mo-TiC30 vol.% ceramic-metal composite was investigated over a large temperature range (25^{\circ}C to 700^{\circ}C). High-energy X-ray tomography was used to reveal the percolation of the hard titanium carbide phase through the composite. Using a polycrystal approach for a two-phase material, finite element simulations were performed on a real 3D aggregate of the material. The 3D microstructure, used as starting configuration for the predictions, was obtained by serial-sectioning in a dual beam Focused Ion Beam (FIB)-Scanning Electron Microscope (SEM) coupled to an Electron Back Scattering Diffraction system (3D EBSD, EBSD tomography). The 3D aggregate consists of a molybdenum matrix and a percolating TiC skeleton. As most BCC metals, the molybdenum matrix phase is characterized by a change in the plasticity mechanisms with temperature. We used a polycrystal model for the BCC material, which was extended to two phases (TiC and Mo). The model parameters of the matrix were determined from experiments on pure molydenum. For all temperatures investigated, the TiC particles were considered as brittle. Gradual damage of the TiC particles was treated, based on an accumulative failure law that is approximated by an evolution of the apparent particle elastic stiffness. The model enabled us to determine the evolution of the local mechanical fields with deformation and temperature. We showed that a 3D aggregate representing the actual microstructure of the composite is required to understand the local and global mechanical properties of the studied composite. 	
1308.4696v1	http://arxiv.org/pdf/1308.4696v1	2013	Mechanical properties of hydrogen functionalized graphene allotropes	Yinfeng Li|Dibakar Datta|Zhonghua Li|Vivek B. Shenoy	  Molecular dynamics (MD) simulations have been performed to investigate the mechanical properties of hydrogen functionalized graphene allotropes (GAs) for H-coverage spanning the entire range (0-100%). Four allotropes (graphyne, cyclic graphene, octagonal graphene, and biphenylene) with larger unit lattice size than graphene are considered. The effect of the degree of functionalization and molecular structure on the Young's modulus and strength are investigated, and the failure processes of some new GAs are reported for the first time. We show that the mechanical properties of the hydrogenated GAs deteriorate drastically with increasing H-coverage within the sensitive threshold, beyond which the mechanical properties remain insensitive to the increase in H-coverage. This drastic deterioration arises both from the conversion of sp2 to sp3 bonding and easy rotation of unsupported sp3 bonds. Allotropes with different lattice structures correspond to different sensitive thresholds. The Young's moduli deterioration of fully hydrogenated allotropes can be up to 70% smaller than that of the corresponding pristine structure. Moreover the tensile strength shows an even larger drop of about 90% and higher sensitivity to H-coverage even if it is small. Our results suggest that the unique coverage-dependent deterioration of the mechanical properties must be taken into account when analyzing the performance characteristics of nanodevices fabricated from functionalized GAs. 	
1506.07792v1	http://arxiv.org/pdf/1506.07792v1	2015	Strain-controlled criticality governs the nonlinear mechanics of fibre   networks	A. Sharma|A. J. Licup|R. Rens|M. Sheinman|K. A. Jansen|G. H. Koenderink|F. C. MacKintosh	  Disordered fibrous networks are ubiquitous in nature as major structural components of living cells and tissues. The mechanical stability of networks generally depends on the degree of connectivity: only when the average number of connections between nodes exceeds the isostatic threshold are networks stable (Maxwell, J. C., Philosophical Magazine 27, 294 (1864)). Upon increasing the connectivity through this point, such networks undergo a mechanical phase transition from a floppy to a rigid phase. However, even sub-isostatic networks become rigid when subjected to sufficiently large deformations. To study this strain-controlled transition, we perform a combination of computational modeling of fibre networks and experiments on networks of type I collagen fibers, which are crucial for the integrity of biological tissues. We show theoretically that the development of rigidity is characterized by a strain-controlled continuous phase transition with signatures of criticality. Our experiments demonstrate mechanical properties consistent with our model, including the predicted critical exponents. We show that the nonlinear mechanics of collagen networks can be quantitatively captured by the predictions of scaling theory for the strain-controlled critical behavior over a wide range of network concentrations and strains up to failure of the material. 	
1511.03477v1	http://arxiv.org/pdf/1511.03477v1	2015	Two-dimensional Graphene Heterojunctions: the Tunable Mechanical   Properties	Kang Xia|Haifei Zhan|Yuantong Gu	  We report the mechanical properties of different two-dimensional carbon heterojunctions (HJs) made from graphene and various stable graphene allotropes, including {\alpha}-, {\beta}-, {\gamma}- and 6612-graphyne (GY), and graphdiyne (GDY). It is found that all HJs exhibit a brittle behaviour except the one with {\alpha}-GY, which however shows a hardening process due to the formation of triple carbon rings. Such hardening process has greatly deferred the failure of the structure. The yielding of the HJs is usually initiated at the interface between graphene and graphene allotropes, and monoatomic carbon rings are normally formed after yielding. By varying the locations of graphene (either in the middle or at the two ends of the HJs), similar mechanical properties have been obtained, suggesting insignificant impacts from location of graphene allotropes. Whereas, changing the types and percentages of the graphene allotropes, the HJs exhibit vastly different mechanical properties. In general, with the increasing graphene percentage, the yield strain decreases and the effective Young's modulus increases. Meanwhile, the yield stress appears irrelevant with the graphene percentage. This study provides a fundamental understanding of the tensile properties of the heterojunctions that are crucial for the design and engineering of their mechanical properties, in order to facilitate their emerging future applications in nanoscale devices, such as flexible/stretchable electronics. 	
1603.02862v1	http://arxiv.org/pdf/1603.02862v1	2016	Is the World Local or Nonlocal? Towards an Emergent Quantum Mechanics in   the 21st Century	Jan Walleczek|Gerhard Groessing	  What defines an emergent quantum mechanics (EmQM)? Can new insight be advanced into the nature of quantum nonlocality by seeking new links between quantum and emergent phenomena as described by self-organization, complexity, or emergence theory? Could the development of a future EmQM lead to a unified, relational image of the cosmos? One key motivation for adopting the concept of emergence in relation to quantum theory concerns the persistent failure in standard physics to unify the two pillars in the foundations of physics: quantum theory and general relativity theory (GRT). The total contradiction in the foundational, metaphysical assumptions that define orthodox quantum theory versus GRT might render inter-theoretic unification impossible. On the one hand, indeterminism and non-causality define orthodox quantum mechanics, and, on the other hand, GRT is governed by causality and determinism. How could these two metaphysically-contradictory theories ever be reconciled? The present work argues that metaphysical contradiction necessarily implies physical contradiction. The contradictions are essentially responsible also for the measurement problem in quantum mechanics. A common foundation may be needed for overcoming the contradictions between the two foundational theories. The concept of emergence, and the development of an EmQM, might help advance a common foundation - physical and metaphysical - as required for successful inter-theory unification. 	
1605.06719v3	http://arxiv.org/pdf/1605.06719v3	2017	(Modular) Effect Algebras are Equivalent to (Frobenius) Antispecial   Algebras	Dusko Pavlovic|Peter-Michael Seidel	  Effect algebras are one of the generalizations of Boolean algebras proposed in the quest for a quantum logic. Frobenius algebras are a tool of categorical quantum mechanics, used to present various families of observables in abstract, often nonstandard frameworks. Both effect algebras and Frobenius algebras capture their respective fragments of quantum mechanics by elegant and succinct axioms; and both come with their conceptual mysteries. A particularly elegant and mysterious constraint, imposed on Frobenius algebras to characterize a class of tripartite entangled states, is the antispecial law. A particularly contentious issue on the quantum logic side is the modularity law, proposed by von Neumann to mitigate the failure of distributivity of quantum logical connectives. We show that, if quantum logic and categorical quantum mechanics are formalized in the same framework, then the antispecial law of categorical quantum mechanics corresponds to the natural requirement of effect algebras that the units are each other's unique complements; and that the modularity law corresponds to the Frobenius condition. These correspondences lead to the equivalence announced in the title. Aligning the two formalisms, at the very least, sheds new light on the concepts that are more clearly displayed on one side than on the other (such as e.g. the orthogonality). Beyond that, it may also open up new approaches to deep and important problems of quantum mechanics (such as the classification of complementary observables). 	
1703.00113v2	http://arxiv.org/pdf/1703.00113v2	2017	Modeling of internal mechanical failure of all-solid-state batteries   during electrochemical cycling, and implications for battery design	Giovanna Bucci|Tushar Swamy|Yet-Ming Chiang|W. Craig Carter	  This is the first quantitative analysis of mechanical reliability of all-solid state batteries. Mechanical degradation of the solid electrolyte (SE) is caused by intercalation-induced expansion of the electrode particles, within the constrain of a dense microstructure. A coupled electro-chemo-mechanical model was implemented to quantify the material properties that cause a SE to fracture. The treatment of microstructural details is essential to the understanding of stress-localization phenomena and fracture. A cohesive zone model is employed to simulate the evolution of damage. In the numerical tests, fracture is prevented only if electrode-particle's expansion is lower than 7.5% and the solid-electrolyte's fracture energy higher than $G_c = 4$ J m$^{-2}$. Perhaps counter-intuitively, the analyses show that compliant solid electrolytes (with Young's modulus in the order of E$_{SE} = 15$ GPa) are more prone to micro-cracking. This result, captured by our non-linear kinematics model, contradicts the speculations that sulfide SEs are more suitable for the design of bulk-type batteries than oxide SEs. Mechanical degradation is linked to the battery power-density. Fracture in solid Li-ion conductors represents a barrier for Li transport, and accelerates the decay of rate performance. 	
1704.08605v1	http://arxiv.org/pdf/1704.08605v1	2017	Failsafe Mechanism Design of Multicopters Based on Supervisory Control   Theory	Quan Quan|Zhiyao Zhao|Liyong Lin|Peng Wang|Walter Murray Wonham|Kai-Yuan Cai	  In order to handle undesirable failures of a multicopter which occur in either the pre-flight process or the in-flight process, a failsafe mechanism design method based on supervisory control theory is proposed for the semi-autonomous control mode. Failsafe mechanism is a control logic that guides what subsequent actions the multicopter should take, by taking account of real-time information from guidance, attitude control, diagnosis, and other low-level subsystems. In order to design a failsafe mechanism for multicopters, safety issues of multicopters are introduced. Then, user requirements including functional requirements and safety requirements are textually described, where function requirements determine a general multicopter plant, and safety requirements cover the failsafe measures dealing with the presented safety issues. In order to model the user requirements by discrete-event systems, several multicopter modes and events are defined. On this basis, the multicopter plant and control specifications are modeled by automata. Then, a supervisor is synthesized by monolithic supervisory control theory. In addition, we present three examples to demonstrate the potential blocking phenomenon due to inappropriate design of control specifications. Also, we discuss the meaning of correctness and the properties of the obtained supervisor. This makes the failsafe mechanism convincingly correct and effective. Finally, based on the obtained supervisory controller generated by TCT software, an implementation method suitable for multicopters is presented, in which the supervisory controller is transformed into decision-making codes. 	
9801204v1	http://arxiv.org/pdf/cond-mat/9801204v1	1998	How Sandcastles Fall	Thomas C. Halsey|Alex J. Levine	  Capillary forces significantly affect the stability of sandpiles. We analyze the stability of sandpiles with such forces, and find that the critical angle is unchanged in the limit of an infinitely large system; however, this angle is increased for finite-sized systems. The failure occurs in the bulk of the sandpile rather than at the surface. This is related to a standard result in soil mechanics. The increase in the critical angle is determined by the surface roughness of the particles, and exhibits three regimes as a function of the added-fluid volume. Our theory is in qualitative agreement with the recent experimental results of Hornbaker et al., although not with the interpretation they make of these results. 	
9912162v2	http://arxiv.org/pdf/cond-mat/9912162v2	1999	Partitioning of a polymer chain between two confining cavities: the role   of electrostatic interactions	S. Tsonchev|R. D. Coalson|A. Duncan	  A recently developed lattice field theory approach to the statistical mechanics of charged polymers in electrolyte solutions [S. Tsonchev, R. D. Coalson, and A. Duncan, Phys. Rev. E {\bf{60}}, 4257, (1999)] is generalized to the case where ground-state dominance in the polymer's Green's function does not apply. The full mean-field equations for the system are derived and are shown to possess a unique solution. The approach is applied to the problem of a charged Gaussian polymer chain confined to move within the region defined by two fused spheres. The failure of the notion of ground-state dominance under certain conditions even in the limit of large number of monomers is demonstrated. 	
0004022v2	http://arxiv.org/pdf/cond-mat/0004022v2	2000	Quantum-Mechanical Non-Perturbative Response of Driven Chaotic   Mesoscopic Systems	Doron Cohen|Tsampikos Kottos	  Consider a time-dependent Hamiltonian $H(Q,P;x(t))$ with periodic driving $x(t)=A\sin(\Omega t)$. It is assumed that the classical dynamics is chaotic, and that its power-spectrum extends over some frequency range $|\omega|<\omega_{cl}$. Both classical and quantum-mechanical (QM) linear response theory (LRT) predict a relatively large response for $\Omega<\omega_{cl}$, and a relatively small response otherwise, independently of the driving amplitude $A$. We define a non-perturbative regime in the $(\Omega,A)$ space, where LRT fails, and demonstrate this failure numerically. For $A>A_{prt}$, where $A_{prt}\propto\hbar$, the system may have a relatively strong response for $\Omega>\omega_{cl}$, and the shape of the response function becomes $A$ dependent. 	
0301350v2	http://arxiv.org/pdf/cond-mat/0301350v2	2003	Magnetic Polarons in the 1D FM Kondo Model	Winfried Koller|Alexander Prüll|Hans Gerd Evertz|Wolfgang von der Linden	  The ferromagnetic Kondo model with classical corespins is studied via unbiased Monte-Carlo simulations. We show that with realistic parameters for the manganites and at low temperatures, the double-exchange mechanism does not lead to phase separation in one-dimensional chains but rather stabilizes individual ferromagnetic polarons. Within the ferromagnetic polaron picture, the pseudogap in the one-particle spectral function A_k(\omega) can easily be explained. Ferromagnetic polarons also clear up a seeming failure of the double-exchange mechanism in explaining the comparable bandwidths in the ferromagnetic and paramagnetic phase. For our analysis, we extend a simplified model, the finite temperature uniform hopping approach (UHA), to include polarons. It can easily be evaluated numerically and provides a simple quantitative understanding of the physical features of the ferromagnetic Kondo model. 	
0304365v2	http://arxiv.org/pdf/cond-mat/0304365v2	2003	Error Diagrams and Temporal Correlations in a Fracture Model with   Characteristic and Power-Law Distributed Avalanches	Yamir Moreno|Miguel Vazquez-Prada|Javier B. Gomez|Amalio F. Pacheco	  Forecasting failure events is one of the most important problems in fracture mechanics and related sciences. In this paper, we use the Molchan scheme to investigate the error diagrams in a fracture model which has the notable advantage of displaying two completely different regimes according to the heterogeneity of the system. In one regime, a characteristic event is observed while for the second regime a power-law spectrum of avalanches is obtained reminiscent of self-organized criticality. We find that both regimes are different when predicting large avalanches and that, in the second regime, there are non-trivial temporal correlations associated to clustering of large events. Finally, we extend the discussion to seismology, where both kinds of avalanche size distributions can be seen. 	
0310701v1	http://arxiv.org/pdf/cond-mat/0310701v1	2003	Non trivial behavior of the linear response function in phase ordering   kinetics	Federico Corberi|Nicola Fusco|Eugenio Lippiello|Marco Zannetti	  Drawing from exact, approximate and numerical results an overview of the properties of the out of equilibrium response function in phase ordering kinetics is presented. Focusing on the zero field cooled magnetization, emphasis is on those features of this quantity which display non trivial behavior when relaxation proceeds by coarsening. Prominent among these is the dimensionality dependence of the scaling exponent $a_{\chi}$ which leads to failure of the connection between static and dynamic properties at the lower dimensionality $d_L$, where $a_{\chi}=0$. We also analyse the mean spherical model as an explicit example of a stochastic unstable system, for which the connection between statics and dynamics fails at all dimensionalities. 	
0311123v2	http://arxiv.org/pdf/cond-mat/0311123v2	2004	Statistical Mechanical Approach to Error Exponents of Lossy Data   Compression	Tadaaki Hosaka|Yoshiyuki Kabashima	  We present herein a scheme by which to accurately evaluate the error exponents of a lossy data compression problem, which characterize average probabilities over a code ensemble of compression failure and success above or below a critical compression rate, respectively, utilizing the replica method (RM). Although the existing method used in information theory (IT) is, in practice, limited to ensembles of randomly constructed codes, the proposed RM-based approach can be applied to a wider class of ensembles. This approach reproduces the optimal expressions of the error exponents achieved by the random code ensembles, which are known in IT. In addition, the proposed framework is used to show that codes composed of non-monotonic perceptrons of a specific type can provide the optimal exponents in most cases, which is supported by numerical experiments. 	
0411556v1	http://arxiv.org/pdf/cond-mat/0411556v1	2004	Quantized Failure Criteria and Indirect Observation for Predicting the   Nanoscale Strength of Materials: The Example of the Ultra Nano Crystalline   Diamond	Nicola M. Pugno	  In this paper theoretical and statistical/experimental criteria for determining the nanoscale strength of materials are proposed. In particular, quantized criteria in fracture mechanics, dynamic fracture mechanics and fatigue, as well as an experimental indirect observation of the nanoscale strength, are proposed. The increasing of the dynamic resistance and the role of a fractal crack surface formation are also rationalized. The analysis shows that materials can be sensitive to flaws also at nanoscale (as demonstrated for carbon nanotubes), in contrast to the conclusion of a recently published paper, and that the surfaces are weaker than the inner parts of a solid by a factor of about 10%. In addition, the proposed statistical/experimental procedure is applied for predicting the nanoscale strength of the ultrananocrystalline diamond (UNCD), an innovative material only recently developed. 	
0412233v1	http://arxiv.org/pdf/cond-mat/0412233v1	2004	On nonextensive thermo-statistics: systematization, clarification of   scope and interpretation, and illustrations	Roberto Luzzi|Áurea R. Vasconcellos|J. Galvão Ramos	  When dealing with certain kind of complex phenomena the theoretician may face some difficulties -- typically a failure to have access to information for properly characterize the system -- for applying the full power of the standard approach to the well established, physically and logically sound, Boltzmann-Gibbs statistics. To circumvent such difficulties, in order to make predictions on properties of the system and looking for an understanding of the physics involved (for example in analyzing the technological characteristics of fractal-structured devices) can be introduced large families of auxiliary statistics. We present here a systematization of these different styles in what can be termed as Unconventional Statistical Mechanics, accompanied of an analysis of the construction and a clarification of its scope and interpretation. As illustrations are derived heterotypical Bose-Einstein, Fermi-Dirac and Maxwell-Boltzmann distributions, and several applications including studies of experimental works are briefly described. 	
0607452v1	http://arxiv.org/pdf/cond-mat/0607452v1	2006	Dielectric breakdown in underoxidized magnetic tunnel junctions:   Dependence on oxidation time and area	J. Ventura|R. Ferreira|J. B. Sousa|P. P. Freitas	  Magnetic tunnel junctions (MTJs) with partially oxidized 9 \AA AlO$_x$-barriers were recently shown to have the necessary characteristics to be used as magnetoresistive sensors in high-density storage devices. Here we study dielectric breakdown in such underoxidized magnetic tunnel junctions, focusing on its dependence on tunnel junction area and oxidation time. A clear relation between breakdown mechanism and junction area is observed for the MTJs with the highest studied oxidation time: samples with large areas fail usually due to extrinsic causes (characterized by a smooth resistance decrease at dielectric breakdown). Small area junctions fail mainly through an intrinsic mechanism (sharp resistance decrease at breakdown). However, this dependence changes for lower oxidation times, with extrinsic breakdown becoming dominant. In fact, in the extremely underoxidized magnetic tunnel junctions, failure is exclusively related with extrinsic causes, independently of MTJ-area. These results are related with the presence of defects in the barrier (weak spots that lead to intrinsic breakdown) and of metallic unoxidized Al nanoconstrictions (leading to extrinsic breakdown). 	
0107070v1	http://arxiv.org/pdf/nlin/0107070v1	2001	Spatiotemporal dynamics of pacing in models of anatomical reentry	Sitabhra Sinha|David J. Christini	  Reentry around non-conducting ventricular scar tissue, which often causes lethal arrhythmias, is typically treated by rapid stimulation from an implantable defibrillator. However, the mechanisms of termination (success and failure) are poorly understood. To elucidate such mechanisms, we studied pacing of anatomical reentry in 1-D and 2-D excitable cardiac media models. Our results suggest that the existence of inhomogeneity in the reentry circuit is essential for pacing termination of tachycardia to be successful. Considering the role of such inhomogeneities may lead to more effective pacing algorithms. 	
0406046v1	http://arxiv.org/pdf/nlin/0406046v1	2004	Terminating ventricular tachycardia by pacing induced dynamical   inhomogeneities in the reentry circuit	Sitabhra Sinha|Johannes Breuer	  Formation of feedback loops of excitation waves (reentrant circuit) around non-conducting ventricular scar tissue is a common cause of lethal cardiac arrhythmias, such as ventricular tachycardia. This is typically treated by rapid stimulation from an implantable device (ICD). However, the mechanisms of reentry termination success and, more importantly, failure, are poorly understood. To study such mechanisms, we simulated pacing of anatomical reentry in an ionic model of cardiac tissue, having significant restitution and dispersion properties. Our results show that rapid pacing causes inhomogeneous conduction in the reentrant circuit, leading to successful pacing termination of tachycardia. The study suggests that more effective pacing algorithms can be designed by taking into account the role of such dynamical inhomogeneities. 	
0301044v3	http://arxiv.org/pdf/quant-ph/0301044v3	2003	Mixing quantum and classical mechanics and uniqueness of Planck's   constant	Debendranath Sahoo	  Observables of quantum or classical mechanics form algebras called quantum or classical Hamilton algebras respectively (Grgin E and Petersen A (1974) {\it J Math Phys} {\bf 15} 764\cite{grginpetersen}, Sahoo D (1977) {\it Pramana} {\bf 8} 545\cite{sahoo}). We show that the tensor-product of two quantum Hamilton algebras, each characterized by a different Planck's constant is an algebra of the same type characterized by yet another Planck's constant. The algebraic structure of mixed quantum and classical systems is then analyzed by taking the limit of vanishing Planck's constant in one of the component algebras. This approach provides new insight into failures of various formalisms dealing with mixed quantum-classical systems. It shows that in the interacting mixed quantum-classical description, there can be no back-reaction of the quantum system on the classical. A natural algebraic requirement involving restriction of the tensor product of two quantum Hamilton algebras to their components proves that Planck's constant is unique. 	
0305137v5	http://arxiv.org/pdf/quant-ph/0305137v5	2003	An Explanation of Spin Based on Classical Mechanics and Electrodynamics	O. Chavoya-Aceves	  It is proved that, according to Classical Mechanics and Electrodynamics, the trajectory of the center of mass of a neutral system of electrical charges can be deflected by an inhomogeneous magnetic field, even if its internal angular momentum is zero. This challenges the common view about the function of the Stern-Gerlach apparatus, as resolving the eigen-states of an intrinsic angular momentum. Doubts are cast also on the supposed failure of Schrodinger's theory to explain the properties of atoms in presence of magnetic fields without introducing spin variables. 	
0308114v1	http://arxiv.org/pdf/quant-ph/0308114v1	2003	The Bell-Kochen-Specker Theorem	D. M. Appleby	  Meyer, Kent and Clifton (MKC) claim to have nullified the Bell-Kochen-Specker (Bell-KS) theorem. It is true that they invalidate KS's account of the theorem's physical implications. However, they do not invalidate Bell's point, that quantum mechanics is inconsistent with the classical assumption, that a measurement tells us about a property previously possessed by the system. This failure of classical ideas about measurement is, perhaps, the single most important implication of quantum mechanics. In a conventional colouring there are some remaining patches of white. MKC fill in these patches, but only at the price of introducing patches where the colouring becomes ''pathologically'' discontinuous. The discontinuities mean that the colours in these patches are empirically unknowable. We prove a general theorem which shows that their extent is at least as great as the patches of white in a conventional approach. The theorem applies, not only to the MKC colourings, but also to any other such attempt to circumvent the Bell-KS theorem (Pitowsky's colourings, for example). We go on to discuss the implications. MKC do not nullify the Bell-KS theorem. They do, however, show that we did not, hitherto, properly understand the theorem. For that reason their results (and Pitowsky's earlier results) are of major importance. 	
0406075v1	http://arxiv.org/pdf/quant-ph/0406075v1	2004	Symmetric Triple Well with Non-Equivalent Vacua: Simple Quantum   Mechanical Approach	H. A. Alhendi|E. I. Lashin	  The structure of the energy levels in a deep triple well is analyzed using simple quantum mechanical considerations. The resultant spectra of the first three energy levels are found to be composed of a ground state localized at the central well and the two other states are distributed only among the left and right well in anti-symmetric and symmetric way respectively. Due to the tunneling effects the energy eigenvalue of the ground state is approximately equal to the ground state energy for a harmonic oscillator localized at the central well, while the two others are nearly degenerate with approximate values equal to the ground state energy of a harmonic oscillator localized at the left or right well. The resulting pattern of the spectra are confirmed numerically. The failure of the instantonic approach recently applied for predicting the correct spectra is commented 	
0509048v1	http://arxiv.org/pdf/quant-ph/0509048v1	2005	The Grammar of Teleportation	Christopher G. Timpson	  Whilst a straightforward consequence of the formalism of non-relativistic quantum mechanics, the phenomenon of quantum teleportation has given rise to considerable puzzlement. In this paper, the teleportation protocol is reviewed and these puzzles dispelled. It is suggested that they arise from two primary sources: 1) the familiar error of hypostatizing an abstract noun (in this case, `information') and 2) failure to differentiate interpretation dependent from interpretation independent features of quantum mechanics. A subsidiary source of error, the simulation fallacy, is also identified. The resolution presented of the puzzles of teleportation illustrates the benefits of paying due attention to the logical status of `information' as an abstract noun. 	
0704.0761v2	http://arxiv.org/pdf/0704.0761v2	2007	Failure of the work-Hamiltonian connection for free energy calculations	Jose M. G. Vilar|J. Miguel Rubi	  Extensions of statistical mechanics are routinely being used to infer free energies from the work performed over single-molecule nonequilibrium trajectories. A key element of this approach is the ubiquitous expression dW/dt=\partial H(x,t)/ \partial t which connects the microscopic work W performed by a time-dependent force on the coordinate x with the corresponding Hamiltonian H(x,t) at time t. Here we show that this connection, as pivotal as it is, cannot be used to estimate free energy changes. We discuss the implications of this result for single-molecule experiments and atomistic molecular simulations and point out possible avenues to overcome these limitations. 	
0707.1333v2	http://arxiv.org/pdf/0707.1333v2	2007	Disproof of Bell's Theorem: Further Consolidations	Joy Christian	  The failure of Bell's theorem for Clifford algebra valued local variables is further consolidated by proving that the conditions of remote parameter independence and remote outcome independence are duly respected within the recently constructed exact, local realistic model for the EPR-Bohm correlations. Since the conjunction of these two conditions is equivalent to the locality condition of Bell, this provides an independent geometric proof of the local causality of the model, at the level of microstates. In addition to local causality, the model respects at least seven other conceptual and operational requirements, arising either from the predictions of quantum mechanics or the premises of Bell's theorem, including the Malus's law for sequential spin measurements. Since the agreement between the predictions of the model and those of quantum mechanics is quantitatively precise in all respects, the ensemble interpretation of the entangled singlet state becomes amenable. 	
0709.2172v1	http://arxiv.org/pdf/0709.2172v1	2007	The influence of metallic particle size on the mechanical properties of   PTFE-Al-W powder composites	J. Cai|V. F. Nesterenko|K. S. Vecchio|E. B. Herbold|D. J. Benson|F. Jiang|J. W. Addiss|S. M. Walley|W. G. Proud	  The dynamic mechanical properties of reactive materials (e.g., high density mixtures of polytetraflouroethylene (PTFE), aluminum (Al) and tungsten (W) powders) can be tailored by changing the morphology of the particles and porosity. Cold isostatically pressed PTFE-Al-W powder composites with fine metallic particles and a higher porosity exhibited higher ultimate compressive strength than less porous composites having equivalent mass ratios with coarse W particles. The mesoscale force chains between the fine metallic particles are responsible for this unusual phenomenon. We observed macrocracks below the critical failure strain for the matrix and a competition between densification and fracture in some porous samples in dynamic tests. 	
0803.2362v1	http://arxiv.org/pdf/0803.2362v1	2008	Failure of feedback as a putative common mechanism of spreading   depolarizations in migraine and stroke	Markus A. Dahlem|Felix M. Schneider|Eckehard Schoell	  The stability of cortical function depends critically on proper regulation. Under conditions of migraine and stroke a breakdown of transmembrane chemical gradients can spread through cortical tissue. A concomitant component of this emergent spatio-temporal pattern is a depolarization of cells detected as slow voltage variations. The velocity of ~3 mm/min indicates a contribution of diffusion. We propose a mechanism for spreading depolarizations (SD) that rests upon a nonlocal or non-instantaneous feedback in a reaction-diffusion system. Depending upon the characteristic space and time scales of the feedback, the propagation of cortical SD can be suppressed by shifting the bifurcation line, which separates the parameter regime of pulse propagation from the regime where a local disturbance dies out. The optimisation of this feedback is elaborated for different control schemes and ranges of control parameters. 	
0808.3886v2	http://arxiv.org/pdf/0808.3886v2	2009	Stress dependent thermal pressurization of a fluid-saturated rock	Siavash Ghabezloo|Jean Sulem	  Temperature increase in saturated porous materials under undrained conditions leads to thermal pressurization of the pore fluid due to the discrepancy between the thermal expansion coefficients of the pore fluid and of the solid matrix. This increase in the pore fluid pressure induces a reduction of the effective mean stress and can lead to shear failure or hydraulic fracturing. The equations governing the phenomenon of thermal pressurization are presented and this phenomenon is studied experimentally for a saturated granular rock in an undrained heating test under constant isotropic stress. Careful analysis of the effect of mechanical and thermal deformation of the drainage and pressure measurement system is performed and a correction of the measured pore pressure is introduced. The test results are modelled using a non-linear thermo-poro-elastic constitutive model of the granular rock with emphasis on the stress-dependent character of the rock compressibility. The effects of stress and temperature on thermal pressurization observed in the tests are correctly reproduced by the model. 	
0809.4616v1	http://arxiv.org/pdf/0809.4616v1	2008	Low-resolution measurements induced classicality	R. M. Angelo	  The classical limit of quantum mechanics is discussed for closed quantum systems in terms of observational aspects. Initially, the failure of the limit h->0 is explicitly demonstrated in a model of two quantum mechanically interacting oscillators by showing that neither quantum expectations reduce to Newtonian trajectories nor entanglement vanishes. This result suggests that the quantum-to-classical transition occurs only at an approximative level, which is regulated by the low accuracy of the measurements. In order to verify the consistence of these ideas we take into account the experimental resolution of physical measurements by introducing a discretized formulation for the quantum structure of wave functions. As a result, in the low-resolution limit the quasi-determinism is recovered and hence the quantum-to-classical transition is shown to occur adequately. Other puzzling problems, such as the classical limit of quantum superpositions and nonlocal correlations, are naturally address as well. 	
0812.4162v1	http://arxiv.org/pdf/0812.4162v1	2008	Kinetic roughening in a realistic model of non-conserved interface   growth	Matteo Nicoli|Mario Castro|Rodolfo Cuerno	  We provide a quantitative picture of non-conserved interface growth from a diffusive field making special emphasis on two main issues, the range of validity of the effective small-slopes (interfacial) theories and the interplay between the emergence of morphologically instabilities in the aggregate dynamics, and its kinetic roughening properties. Taking for definiteness electrochemical deposition as our experimental field of reference, our theoretical approach makes use of two complementary approaches: interfacial effective equations and a phase-field formulation of the electrodeposition process. Both descriptions allow us to establish a close quantitative connection between theory and experiments. Moreover, we are able to correlate the anomalous scaling properties seen in some experiments with the failure of the small slope approximation, and to assess the effective re-emergence of standard kinetic roughening properties at very long times under appropriate experimental conditions. 	
0901.4407v2	http://arxiv.org/pdf/0901.4407v2	2010	A dynamic model of time-dependent complex networks	Scott A. Hill|Dan Braha	  The characterization of the "most connected" nodes in static or slowly evolving complex networks has helped in understanding and predicting the behavior of social, biological, and technological networked systems, including their robustness against failures, vulnerability to deliberate attacks, and diffusion properties. However, recent empirical research of large dynamic networks (characterized by connections that are irregular and evolve rapidly) has demonstrated that there is little continuity in degree centrality of nodes over time, even when their degree distributions follow a power law. This unexpected dynamic centrality suggests that the connections in these systems are not driven by preferential attachment or other known mechanisms. We present a novel approach to explain real-world dynamic networks and qualitatively reproduce these dynamic centrality phenomena. This approach is based on a dynamic preferential attachment mechanism, which exhibits a sharp transition from a base pure random walk scheme. 	
0905.1829v1	http://arxiv.org/pdf/0905.1829v1	2009	An experimental test of volume-equilibration between granular systems	Frederic Lechenault|Karen E. Daniels	  Understanding granular and other athermal systems requires the identification of state variables which consistently predict their bulk properties. A promising approach has been to draw on the techniques of equilibrium statistical mechanics, but to consider alternate conserved quantities in place of energy. The Edwards ensemble, based on volume conservation, provides a temperaturelike intensive parameter called compactivity. We present experiments which demonstrate the failure of compactivity to equilibrate (via volume-exchange) between a pair of externally-agitated granular subsystems with different material properties. Nonetheless, we identify a material-independent relationship between the mean and fluctuations of the local packing fraction which forms the basis for an equation of state. This relationship defines an intensive parameter that decouples from the volume statistics. 	
0910.3850v1	http://arxiv.org/pdf/0910.3850v1	2009	General principles in the interpretation of quantum mechanics	Casey Blood	  The three major theoretical principles of quantum mechanics relevant to its interpretation are: (T1), linearity; (T2), invariance under certain groups; and (T3) the orthogonality and isolation of the different branches of the state vector. These three imply the particle-like properties of mass, energy, momentum, spin, charge, and locality are actually properties of the state vector; and this in turn implies there is no evidence for the existence of particles. Experimentally there is no evidence for collapse (E1) and theoretically linearity prohibits collapse. One also has the experimentally verified probability law (E2), which is found to rule out the many-worlds interpretation. The failure of these three major interpretation, particles, collapse, and many-worlds, apparently implies an acceptable interpretation must be based on perception. Rather than being a separate principle, probability follows in this interpretation from a weak assumption on perception plus the combinatorics when an experiment is run many times. This suggests a relatively simple experimental test of the perception interpretation. 	
1003.5023v1	http://arxiv.org/pdf/1003.5023v1	2010	Collective Helping and Bystander Effects in Coevolving Helping Networks	Hang-Hyun Jo|Hyun Keun Lee|Hyunggyu Park	  We study collective helping behavior and bystander effects in a coevolving helping network model. A node and a link of the network represents an agent who renders or receives help and a friendly relation between agents, respectively. A helping trial of an agent depends on relations with other involved agents and its result (success or failure) updates the relation between the helper and the recipient. We study the network link dynamics and its steady states analytically and numerically. The full phase diagram is presented with various kinds of active and inactive phases and the nature of phase transitions are explored. We find various interesting bystander effects, consistent with the field study results, of which the underlying mechanism is proposed. 	
1011.1529v1	http://arxiv.org/pdf/1011.1529v1	2010	A Survey on Wireless Sensor Network Security	Jaydip Sen	  Wireless sensor networks (WSNs) have recently attracted a lot of interest in the research community due their wide range of applications. Due to distributed nature of these networks and their deployment in remote areas, these networks are vulnerable to numerous security threats that can adversely affect their proper functioning. This problem is more critical if the network is deployed for some mission-critical applications such as in a tactical battlefield. Random failure of nodes is also very likely in real-life deployment scenarios. Due to resource constraints in the sensor nodes, traditional security mechanisms with large overhead of computation and communication are infeasible in WSNs. Security in sensor networks is, therefore, a particularly challenging task. This paper discusses the current state of the art in security mechanisms for WSNs. Various types of attacks are discussed and their countermeasures presented. A brief discussion on the future direction of research in WSN security is also included. 	
1011.6048v1	http://arxiv.org/pdf/1011.6048v1	2010	Noisy-threshold control of cell death	Jose M. G. Vilar	  Cellular responses to death-promoting stimuli typically proceed through a differentiated multistage process, involving a lag phase, extensive death, and potential adaptation. Deregulation of this chain of events is at the root of many diseases. Improper adaptation is particularly important because it allows cell sub-populations to survive even in the continuous presence of death conditions, which results, among others, in the eventual failure of many targeted anticancer therapies. Here, I show that these typical responses arise naturally from the interplay of intracellular variability with a threshold-based control mechanism that detects cellular changes in addition to just the cellular state itself. Implementation of this mechanism in a quantitative model for T-cell apoptosis, a prototypical example of programmed cell death, captures with exceptional accuracy experimental observations for different expression levels of the oncogene Bcl-xL and directly links adaptation with noise in an ATP threshold below which cells die. These results indicate that oncogenes like Bcl-xL, besides regulating absolute death values, can have a novel role as active controllers of cell-cell variability and the extent of adaptation. 	
1101.0114v2	http://arxiv.org/pdf/1101.0114v2	2013	Behavioral subtyping through typed assertions	Herbert Toth	  This paper presents a critical discussion of popular approaches to ensure the Liskov substitution principle in class hierarchies (e.g. Design by Contract(TM), specification inheritance). It will be shown that they have some deficiencies which are due to the way how effective constraints are calculated for subclass methods. A new mechanism, called client conformance, is introduced that takes the client's view on the program state into account more properly: The client's static type determines the context in which reasoning about program state is to be done. This is the context to which the runtime assertion checking (RAC) of server methods must be adapted appropriately. In a stepwise argumentation we show the improvements for RAC that can be reached following this approach in a natural way, preserving the percolation pattern mechanism: Clients will neither be confronted with unsafe or surprising executions, nor with surprising failures of server methods. 	
1104.0802v1	http://arxiv.org/pdf/1104.0802v1	2011	Dielectric insulation and high-voltage issues	D. Tommasini	  Electrical faults are in most cases dramatic events for magnets, due to the large stored energy which is potentially available to be dissipated at the fault location. After a reminder of the principles of electrostatics in Section 1, the basic mechanisms of conduction and breakdown in dielectrics are summarized in Section 2. Section 3 introduces the types and function of the electrical insulation in magnets, and Section 4 its relevant failure mechanisms. Section 5 deals with ageing and, finally, Section 6 gives some principles for testing. Though the School specifically dealt with warm magnets, for completeness some principles of dielectric insulation for superconducting accelerator magnets are briefly summarized in a dedicated appendix. 	
1202.2551v1	http://arxiv.org/pdf/1202.2551v1	2012	A Simulation Model for Evaluating Distributed Systems Dependability	Ciprian Dobre|Florin Pop|Valentin Cristea	  In this paper we present a new simulation model designed to evaluate the dependability in distributed systems. This model extends the MONARC simulation model with new capabilities for capturing reliability, safety, availability, security, and maintainability requirements. The model has been implemented as an extension of the multithreaded, process oriented simulator MONARC, which allows the realistic simulation of a wide-range of distributed system technologies, with respect to their specific components and characteristics. The extended simulation model includes the necessary components to inject various failure events, and provides the mechanisms to evaluate different strategies for replication, redundancy procedures, and security enforcement mechanisms, as well. The results obtained in simulation experiments presented in this paper probe that the use of discrete-event simulators, such as MONARC, in the design and development of distributed systems is appealing due to their efficiency and scalability. 	
1203.0242v1	http://arxiv.org/pdf/1203.0242v1	2012	Network Physiology reveals relations between network topology and   physiological function	Amir Bashan|Ronny P. Bartsch|Jan W. Kantelhardt|Shlomo Havlin|Plamen Ch. Ivanov	  The human organism is an integrated network where complex physiologic systems, each with its own regulatory mechanisms, continuously interact, and where failure of one system can trigger a breakdown of the entire network. Identifying and quantifying dynamical networks of diverse systems with different types of interactions is a challenge. Here, we develop a framework to probe interactions among diverse systems, and we identify a physiologic network. We find that each physiologic state is characterized by a specific network structure, demonstrating a robust interplay between network topology and function. Across physiologic states the network undergoes topological transitions associated with fast reorganization of physiologic interactions on time scales of a few minutes, indicating high network flexibility in response to perturbations. The proposed system-wide integrative approach may facilitate the development of a new field, Network Physiology. 	
1203.1374v2	http://arxiv.org/pdf/1203.1374v2	2012	Steady Periodic Shear Flow is Stable in Two Space Dimensions .   Nonequilibrium Molecular Dynamics vs Navier-Stokes-Fourier Stability Theory   -- A Comment on two Arxiv Contributions	Wm. G. Hoover|Carol G. Hoover	  Dufty, Lee, Lutsko, Montanero, and Santos have carried out stability analyses of steady stationary shear flows. Their approach is based on the compressible and heat conducting Navier-Stokes-Fourier model. It predicts the unstable exponential growth of long-wavelength transverse perturbations for both two- and three-dimensional fluids. We point out that the patently-stable two-dimensional periodic shear flows studied earlier by Petravic, Posch, and ourselves contradict these predicted instabilities. The stable steady-state shear flows are based on nonequilibrium molecular dynamics with simple thermostats maintaining nonequilibrium stationary states in two space dimensions. The failure of the stability analyses remains unexplained. 	
1209.1078v2	http://arxiv.org/pdf/1209.1078v2	2013	Electromagnetic potentials and Aharonov-Bohm effect	Alexander Ershkovich	  Hamilton-Jacobi equation which governs classical mechanics and electrodynamics explicitly depends on the electromagnetic potentials (A,{\phi}), similar to Schroedinger equation. We derived the Aharonov-Bohm effect from Hamilton-Jacobi equation thereby having proved that this effect is of classical origin. These facts enable us to arrive at the following conclusions: a) the very idea of special role of potentials (A,{\phi}) in quantum mechanics (different from their role in classical physics) lost the ground, and becomes dubious, as this idea is based on the Aharonov-Bohm effect, b) failure to find any signs of a special role of these potentials in the appropriate experiments (Feinberg, 1963) is thereby explained, and c) discovery of classical analogues of the Aharonov-Bohm effect (Berry et al., 1980) is also explained by a classical nature of this effect. Elucidation of the "unlocal" interaction problem was made. 	
1209.3410v1	http://arxiv.org/pdf/1209.3410v1	2012	Work hardening behavior in a steel with multiple TRIP mechanisms	M. C. McGrath|D. C. Van Aken|N. I. Medvedeva|J. E. Medvedeva	  Transformation induced plasticity (TRIP) behavior was studied in steel with composition Fe-0.07C-2.85Si-15.3Mn-2.4Al-0.017N that exhibited two TRIP mechanisms. The initial microstructure consisted of both {\epsilon}- and {\alpha}-martensites with 27% retained austenite. TRIP behavior in the first 5% strain was predominately austenite transforming to {\epsilon}-martensite (Stage I), but upon saturation of Stage I, the {\epsilon}-martensite transformed to {\alpha}-martensite (Stage II). Alloy segregation also affected the TRIP behavior with alloy rich regions producing TRIP just prior to necking. This behavior was explained by first principle calculations that revealed aluminum significantly affected the stacking fault energy in Fe-Mn-Al-C steels by decreasing the unstable stacking fault energy and promoting easy nucleation of {\epsilon}-martensite. The addition of aluminum also raised the intrinsic stacking fault energy and caused the {\epsilon}-martensite to be unstable and transform to {\alpha}-martensite under further deformation. The two stage TRIP behavior produced a high strain hardening exponent of 1.4 and led to ultimate tensile strength of 1165 MPa and elongation to failure of 35%. 	
1210.1505v2	http://arxiv.org/pdf/1210.1505v2	2014	A Comparative Study of SIP Overload Control Algorithms	Yang Hong|Changcheng Huang|James Yan	  Recent collapses of SIP servers in the carrier networks indicates two potential problems of SIP: (1) the current SIP design does not easily scale up to large network sizes, and (2) the built-in SIP overload control mechanism cannot handle overload conditions effectively. In order to help carriers prevent widespread SIP network failure effectively, this chapter presents a systematic investigation of current state-of-the-art overload control algorithms. To achieve this goal, this chapter first reviews two basic mechanisms of SIP, and summarizes numerous experiment results reported in the literatures which demonstrate the impact of overload on SIP networks. After surveying the approaches for modeling the dynamic behaviour of SIP networks experiencing overload, the chapter presents a comparison and assessment of different types of SIP overload control solutions. Finally it outlines some research opportunities for managing SIP overload control. 	
1212.4335v1	http://arxiv.org/pdf/1212.4335v1	2012	Influence of nanoparticle size, loading, and shape on the mechanical   properties of polymer nanocomposites	Aki Kutvonen|Giulia Rossi|Sakari R. Puisto|Niko K. J. Rostedt|Tapio Ala-Nissila	  We study the influence of spherical, triangular, and rod-like nanoparticles on the mechanical properties of a polymer nanocomposite (PNC), via coarse-grained molecular dynamics simulations. We focus on how the nanoparticle size, loading, mass, and shape influence the PNC's elastic modulus, stress at failure and resistance against cavity formation and growth, under external stress. We find that in the regime of strong polymer-nanoparticle interactions, the formation of a polymer network via temporary polymer-nanoparticle crosslinks has a predominant role on the PNC reinforcement. Spherical nanoparticles, whose size is comparable to that of the polymer monomers, are more effective at toughening the PNC than larger spherical particles. When comparing particles of spherical, triangular, and rod-like geometries, the rod-like nanoparticles emerge as the best PNC toughening agents. 	
1212.6060v1	http://arxiv.org/pdf/1212.6060v1	2012	Analytic prognostic for petrochemical pipelines	Abdo Abou Jaoude|Seifedine Kadry|Khaled El-Tawil|Hassan Noura|Mustapha Ouladsine	  Pipelines tubes are part of vital mechanical systems largely used in petrochemical industries. They serve to transport natural gases or liquids. They are cylindrical tubes and are submitted to the risks of corrosion due to high PH concentrations of the transported liquids in addition to fatigue cracks due to the alternation of pressure-depression of gas along the time, initiating therefore in the tubes body micro-cracks that can propagate abruptly to lead to failure. The development of the prognostic process for such systems increases largely their performance and their availability, as well decreases the global cost of their missions. Therefore, this paper deals with a new prognostic approach to improve the performance of these pipelines. Only the first mode of crack, that is, the opening mode, is considered. 	
1303.5365v2	http://arxiv.org/pdf/1303.5365v2	2013	Improving Network Efficiency by Removing Energy Holes in WSNs	M. B. Rasheedl|N. Javaid|A. Javaid|M. A. Khan|S. H. Bouk|Z. A. Khan	  Cluster based Wireless Sensor Networks (WSNs) have been widely used for better performance in terms of energy efficiency. Efficient use of energy is challenging task of designing these protocols. Energy holedare created due to quickly drain the energy of a few nodes due to non-uniform distribution in the network. Normally, energy holes make the data routing failure when nodes transmit data back to the base station. We proposedEnergy-efficient HOleRemoving Mechanism (E-HORM) technique to remove energy holes. In this technique, we use sleep and awake mechanism for sensor nodes to save energy. This approach finds the maximum distance node to calculate the maximum energy for data transmission. We considered it as a threshold energy Eth. Every node first checks its energy level for data transmission. If the energy level is less than Eth, it cannot transmit data. We also explain mathematically the energy consumption and average energy saving of sensor nodes in each round. Extensive simulations showed that when use this approach for WSNs significantly helps to extend the network lifetime and stability period. 	
1304.3675v2	http://arxiv.org/pdf/1304.3675v2	2014	Self-assembly of colloidal polymers via depletion-mediated lock and key   binding	Douglas J. Ashton|Robert L. Jack|Nigel B. Wilding	  We study the depletion-induced self-assembly of indented colloids. Using state-of-the-art Monte Carlo simulation techniques that treat the depletant particles explicitly, we demonstrate that colloids assemble by a lock-and-key mechanism, leading to colloidal polymerization. The morphology of the chains that are formed depends sensitively on the size of the colloidal indentation, with smaller values additionally permitting chain branching. In contrast to the case of spheres with attractive patches, Wertheim's thermodynamic perturbation theory fails to provide a fully quantitative description of the polymerization transition. We trace this failure to a neglect of packing effects and we introduce a modified theory that accounts better for the shape of the colloids, yielding improved agreement with simulation. 	
1306.4575v1	http://arxiv.org/pdf/1306.4575v1	2013	A Mathematical Model for Predicting the Life of PEM Fuel Cell Membranes   Subjected to Hydration Cycling	S. F. Burlatsky|M. Gummalla|J. O'Neill|V. V. Atrazhev|A. N. Varyukhin|D. V. Dmitriev|N. S. Erikhman	  Under typical PEM fuel cell operating conditions, part of membrane electrode assembly is subjected to humidity cycling due to variation of inlet gas RH and/or flow rate. Cyclic membrane hydration/dehydration would cause cyclic swelling/shrinking of the unconstrained membrane. In a constrained membrane, it causes cyclic stress resulting in mechanical failure in the area adjacent to the gas inlet. A mathematical modeling framework for prediction of the lifetime of a PEM FC membrane subjected to hydration cycling is developed in this paper. The model predicts membrane lifetime as a function of RH cycling amplitude and membrane mechanical properties. The modeling framework consists of three model components: a fuel cell RH distribution model, a hydration/dehydration induced stress model that predicts stress distribution in the membrane, and a damage accrual model that predicts membrane life-time. Short descriptions of the model components along with overall framework are presented in the paper. The model was used for lifetime prediction of a GORE-SELECT membrane. 	
1307.4541v1	http://arxiv.org/pdf/1307.4541v1	2013	The resilience of interdependent transportation networks under targeted   attack	Peng Zhang|Baisong Cheng|Zhuang Zhao|Daqing Li|Guangquan Lu|Yunpeng Wang|Jinghua Xiao	  Modern world builds on the resilience of interdependent infrastructures characterized as complex networks. Recently, a framework for analysis of interdependent networks has been developed to explain the mechanism of resilience in interdependent networks. Here we extend this interdependent network model by considering flows in the networks and study the system's resilience under different attack strategies. In our model, nodes may fail due to either overload or loss of interdependency. Under the interaction between these two failure mechanisms, it is shown that interdependent scale-free networks show extreme vulnerability. The resilience of interdependent SF networks is found in our simulation much smaller than single SF network or interdependent SF networks without flows. 	
1309.6000v1	http://arxiv.org/pdf/1309.6000v1	2013	A New Sentinel Approach for Energy Efficient and Hole Aware Wireless   Sensor Networks	Dame Diongue|Ousmane Thiare	  Recent advances in micro-sensor and communication technology have enabled the emergence of a new technology, Wireless Sensor Networks (WSN). WSN have emerging recently as a key solution to monitor remote or hostile environments and concern a wide range of applications. These networks are faced with many challenges such as energy efficiency usage, topology maintenance, network lifetime maximization, etc. Experience shows that sensing and communications tasks consume energy, therefore judicious power management can effectively extend network lifetime. Moreover, the low cost of sensor devices will allows deployment of huge number nodes that can permit a high redundancy degree. In this paper, we focus on the problem of energy efficiency and topology maintenance in a densely deployed network context. Hence we propose an energy aware sleep scheduling and rapid topology healing scheme for long life wireless sensor networks. Our scheme is a strong node scheduling based mechanism for lifetime maximization in wireless sensor networks and has a fast maintenance method to cover nodes failure. Our sentinel scheme is based on a probabilistic model which provides a distributed sleep scheduling and topology control algorithm. Simulations and experimental results are presented to verify our approach and the performance of our mechanism. 	
1311.0318v1	http://arxiv.org/pdf/1311.0318v1	2013	Creep dynamics of viscoelastic interfaces	E. A. Jagla	  The movement of a purely elastic interface driven on a disordered energy potential is characterized by a depinning transition: when the pulling force S is larger than some critical value S_1 the system is in a flowing regime and moves at a finite velocity. If S < S_1 the interface remains pinned and its velocity is zero. We show that for a one-dimensional interface, the inclusion of viscoelastic relaxation produces the appearance of an intervening regime between the pinned and the flowing phases in a well defined stress interval S_0<S<S_1, in which the interface evolves through a sequence of avalanches that give rise to a creep process. As S --> S_0 the creep velocity vanishes in an universal way that is governed by a directed percolation process. As S --> S_1 the creep velocity increases as a power law due to the increase of the typical size of the avalanches. The present observations may serve to improve the understanding of fatigue failure mechanisms. 	
1401.3381v1	http://arxiv.org/pdf/1401.3381v1	2014	Promises, Impositions, and other Directionals	Jan A. Bergstra|Mark Burgess	  Promises, impositions, proposals, predictions, and suggestions are categorized as voluntary co-operational methods. The class of voluntary co-operational methods is included in the class of so-called directionals. Directionals are mechanisms supporting the mutual coordination of autonomous agents.   Notations are provided capable of expressing residual fragments of directionals. An extensive example, involving promises about the suitability of programs for tasks imposed on the promisee is presented. The example illustrates the dynamics of promises and more specifically the corresponding mechanism of trust updating and credibility updating. Trust levels and credibility levels then determine the way certain promises and impositions are handled.   The ubiquity of promises and impositions is further demonstrated with two extensive examples involving human behaviour: an artificial example about an agent planning a purchase, and a realistic example describing technology mediated interaction concerning the solution of pay station failure related problems arising for an agent intending to leave the parking area. 	
1402.1217v2	http://arxiv.org/pdf/1402.1217v2	2014	Entanglement, scaling, and the meaning of the wave function in   protective measurement	Maximilian Schlosshauer|Tangereen V. B. Claringbold	  We examine the entanglement and state disturbance arising in a protective measurement and argue that these inescapable effects doom the claim that protective measurement establishes the reality of the wave function. An additional challenge to this claim results from the exponential number of protective measurements required to reconstruct multi-qubit states. We suggest that the failure of protective measurement to settle the question of the meaning of the wave function is entirely expected, for protective measurement is but an application of the standard quantum formalism, and none of the hard foundational questions can ever be settled in this way. 	
1404.5539v2	http://arxiv.org/pdf/1404.5539v2	2014	Electricity Pooling Markets with Strategic Producers Possessing   Asymmetric Information II: Inelastic Demand	Mohammad Rasouli|Demosthenis Teneketzis	  In the restructured electricity industry, electricity pooling markets are an oligopoly with strategic producers possessing private information (private production cost function). We focus on pooling markets where aggregate demand is represented by a non-strategic agent.   Inelasticity of demand is a main difficulty in electricity markets which can potentially result in market failure and high prices. We consider demand to be inelastic.   We propose a market mechanism that has the following features. (F1) It is individually rational. (F2) It is budget balanced. (F3) It is price efficient, that is, at equilibrium the price of electricity is equal to the marginal cost of production. (F4) The energy production profile corresponding to every non-zero Nash equilibrium of the game induced by the mechanism is a solution of the corresponding centralized problem where the objective is the maximization of the sum of the producers' and consumers' utilities.   We identify some open problems associated with our approach to electricity pooling markets. 	
1406.3526v5	http://arxiv.org/pdf/1406.3526v5	2017	Quantum Logic as Classical Logic	Simon Kramer	  We propose a semantic representation of the standard quantum logic QL within a classical, normal modal logic, and this via a lattice-embedding of orthomodular lattices into Boolean algebras with one modal operator. Thus our classical logic is a completion of the quantum logic QL. In other words, we refute Birkhoff and von Neumann's classic thesis that the logic (the formal character) of Quantum Mechanics would be non-classical as well as Putnam's thesis that quantum logic (of his kind) would be the correct logic for propositional inference in general. The propositional logic of Quantum Mechanics is modal but classical, and the correct logic for propositional inference need not have an extroverted quantum character. One normal necessity modality suffices to capture the subjectivity of observation in quantum experiments, and this thanks to its failure to distribute over classical disjunction. The key to our result is the translation of quantum negation as classical negation of observability. 	
1411.5710v1	http://arxiv.org/pdf/1411.5710v1	2014	Quantum annealing: the fastest route to quantum computation?	C. R. Laumann|R. Moessner|A. Scardicchio|S. L. Sondhi	  In this review we consider the performance of the quantum adiabatic algorithm for the solution of decision problems. We divide the possible failure mechanisms into two sets: small gaps due to quantum phase transitions and small gaps due to avoided crossings inside a phase. We argue that the thermodynamic order of the phase transitions is not predictive of the scaling of the gap with the system size. On the contrary, we also argue that, if the phase surrounding the problem Hamiltonian is a Many-Body Localized (MBL) phase, the gaps are going to be typically exponentially small and that this follows naturally from the existence of local integrals of motion in the MBL phase. 	
1412.1020v2	http://arxiv.org/pdf/1412.1020v2	2015	A new failure mechanism in thin film by collaborative fracture and   delamination: interacting duos of cracks	Joel Marthelot|Jose Bico|Francisco Melo|Benoit Roman	  When a thin film moderately adherent to a substrate is subjected to residual stress, the cooperation between fracture and delamination leads to unusual fracture patterns, such as spirals, alleys of crescents and various types of strips, all characterized by a robust characteristic length scale. We focus on the propagation of a duo of cracks: two fractures in the film connected by a delamination front and progressively detaching a strip. We show experimentally that the system selects an equilibrium width on the order of 25 times the thickness of the coating and independent of both fracture and adhesion energies. We investigate numerically the selection of the width and the condition for propagation by considering Griffith's criterion and the principle of local symmetry. In addition, we propose a simplified model based on the criterion of maximum of energy release rate, which provides insights of the physical mechanisms leading to these regular patterns, and predicts the effect of material properties on the selected width of the detaching strip. 	
1501.06590v1	http://arxiv.org/pdf/1501.06590v1	2015	Fluctuating Nonlinear Spring Model of Mechanical Deformation of   Biological Particles	Olga Kononova|Joost Snijder|Kenneth A. Marx|Gijs J. L. Wuite|Wouter H. Roos|Valeri Barsegov	  We present a new theory for modeling forced indentation spectral lineshapes of biological particles, which considers non-linear Hertzian deformation due to an indenter-particle physical contact and bending deformations of curved beams modeling the particle structure. The bending of beams beyond the critical point triggers the particle dynamic transition to the collapsed state, an extreme event leading to the catastrophic force drop as observed in the force (F)-deformation (X) spectra. The theory interprets fine features of the spectra: the slope of the FX curves and the position of force-peak signal, in terms of mechanical characteristics --- the Young's moduli for Hertzian and bending deformations E_H and E_b, and the probability distribution of the maximum strength with the strength of the strongest beam F_b^* and the beams' failure rate m. The theory is applied to successfully characterize the $FX$ curves for spherical virus particles --- CCMV, TrV, and AdV. 	
1505.05163v3	http://arxiv.org/pdf/1505.05163v3	2015	Inflexibility and independence: Phase transitions in the majority-rule   model	Nuno Crokidakis|Paulo Murilo Castro de Oliveira	  In this work we study opinion formation in a population participating of a public debate with two distinct choices. We considered three distinct mechanisms of social interactions and individuals' behavior: conformity, nonconformity and inflexibility. The conformity is ruled by the majority-rule dynamics, whereas the nonconformity is introduced in the population as an independent behavior, implying the failure to attempted group influence. Finally, the inflexible agents are introduced in the population with a given density. These individuals present a singular behavior, in a way that their stubbornness makes them reluctant to change their opinions. We consider these effects separately and all together, with the aim to analyze the critical behavior of the system. We performed numerical simulations in some lattice structures and for distinct population sizes, and our results suggest that the different formulations of the model undergo order-disorder phase transitions in the same universality class of the Ising model. Some of our results are complemented by analytical calculations. 	
1509.00979v1	http://arxiv.org/pdf/1509.00979v1	2015	Mechanisms in Impact Fragmentation	Falk K. Wittel|Humberto A. Carmona|Ferenc Kun|Hans J. Herrmann	  The brittle fragmentation of spheres is studied numerically by a 3D Discrete Element Model. Large scale computer simulations are performed with models that consist of agglomerates of many spherical particles, interconnected by beam-truss elements. We focus on a detailed description of the fragmentation process and study several fragmentation mechanisms involved. The evolution of meridional cracks is studied in detail. These cracks are found to initiate in the inside of the specimen with quasi-periodic angular distribution and give a broad peak in the fragment mass distribution for large fragments that can be fitted by a two-parameter Weibull distribution. The results prove to be independent of the degree of disorder in the model, but mean fragment sizes scale with velocity. Our results reproduce many experimental observations of fragment shapes, impact energy dependence or mass distribution, and significantly improve the understanding of the fragmentation process for impact fracture since we have full access to the failure conditions and evolution. 	
1510.00374v1	http://arxiv.org/pdf/1510.00374v1	2015	Study on the fragmentation of shells	Falk K. Wittel|Ferenc Kun|Hans J. Herrmann|Bernd H. Kröplin	  Fragmentation can be observed in nature and in everyday life on a wide range of length scales and for all kinds of technical applications. Most studies on dynamic failure focus on the behaviour of bulk systems in one, two and three dimensions under impact and explosive loading, showing universal power law behaviour of fragment size distribution. However, hardly any studies have been devoted to fragmentation of shells. We present a detailed experimental and theoretical study on the fragmentation of closed thin shells of various materials, due to an excess load inside the system and impact with a hard wall. Characteristic fragmentation mechanisms are identified by means of a high speed camera and fragment shapes and mass distributions are evaluated. Theoretical rationalisation is given by means of stochastic break-up models and large-scale discrete element simulations with spherical shell systems under different extreme loading situations. By this we ex-plain fragment shapes and distributions and prove a power law for the fragment mass distribution. Satisfactory agreement between experimental findings and nu-merical predictions of the exponents of the power laws for the fragment shapes is obtained. 	
1511.04562v1	http://arxiv.org/pdf/1511.04562v1	2015	The breakdown of superlubricity by driving-induced commensurate   dislocations	Andrea Benassi|Ming Ma|Michael Urbakh|Andrea Vanossi	  In the framework of a Frenkel-Kontorova-like model, we address the robustness of the superlubricity phenomenon in an edge-driven system at large scales, highlighting the dynamical mechanisms leading to its failure due to the slider elasticity. The results of the numerical simulations perfectly match the length critical size derived from a parameter-free analytical model. By considering different driving and commensurability interface configurations, we explore the distinctive nature of the transition from superlubric to high-friction sliding states which occurs above the critical size, discovering the occurrence of previously undetected multiple dissipative jumps in the friction force as a function of the slider length. These driving-induced commensurate dislocations in the slider are then characterized in relation to their spatial localization and width, depending on the system parameters. Setting the ground to scale superlubricity up, this investigation provides a novel perspective on friction and nanomanipulation experiments and can serve as a theoretical basis for designing high-tech devices with specific superlow frictional features. 	
1601.00787v1	http://arxiv.org/pdf/1601.00787v1	2016	Intrinsic strength and failure behaviors of ultra-small single-walled   carbon nanotubes	Nguyen Tuan Hung|Do Van Truong|Vuong Van Thanh|Riichiro Saito	  The intrinsic mechanical strength of single-walled carbon nanotubes (SWNTs) within the diameter range of 0.3-0.8 nm has been studied based on ab initio density functional theory calculations. In contrast to predicting "smaller is stronger and more elastic" in nanomaterials, the strength of the SWNTs is significantly reduced when decreasing the tube diameter. The results obtained show that the Young`s modulus E significantly reduced in the ultra-small SWNTs with the diameter less than 0.4 nm originates from their very large curvature effect, while it is a constant of about 1.0 TPa, and independent of the diameter and chiral index for the large tube. We find that the Poisson`s ratio, ideal strength and ideal strain are dependent on the diameter and chiral index. Furthermore, the relations between E and ideal strength indicate that Griffith`s estimate of brittle fracture could break down in the smallest (2, 2) nanotube, with the breaking strength of 15% of E. Our results provide important insights into intrinsic mechanical behavior of ultra-small SWNTs under their curvature effect. 	
1604.08244v2	http://arxiv.org/pdf/1604.08244v2	2016	Cosmological Galaxy Evolution with Superbubble Feedback II: The Limits   of Supernovae	B. W. Keller|J. Wadsley|H. M. P Couchman	  We explore when supernovae can (and cannot) regulate the star formation and bulge growth in galaxies based on a sample of 18 simulated galaxies. The simulations include key physics such as evaporation and conduction, neglected in prior work, and required to correctly model superbubbles resulting from stellar feedback. We show that for galaxies with virial masses $>10^{12}\;M_\odot$, supernovae alone cannot prevent excessive star formation. This failure occurs due to a shutdown of galactic winds, with wind mass loadings falling from $\eta\sim10$ to $\eta<1$. In more massive systems, this transfer of baryons to the circumgalactic medium falters earlier on and the galaxies diverge significantly from observed galaxy scaling relations and morphologies. The decreasing efficiency is simply due to a deepening potential well preventing gas escape. This implies that non-supernova feedback mechanisms must become dominant for galaxies with stellar masses greater than $\sim4\times10^{10}\;M_\odot$. The runaway growth of the central stellar bulge, strongly linked to black hole growth, suggests that feedback from active galactic nuclei is the probable mechanism. Below this mass, supernovae alone are able to produce a realistic stellar mass fraction, star formation history and disc morphology. 	
1605.01944v2	http://arxiv.org/pdf/1605.01944v2	2016	SDNsec: Forwarding Accountability for the SDN Data Plane	Takayuki Sasaki|Christos Pappas|Taeho Lee|Torsten Hoefler|Adrian Perrig	  SDN promises to make networks more flexible, programmable, and easier to manage. Inherent security problems in SDN today, however, pose a threat to the promised benefits. First, the network operator lacks tools to proactively ensure that policies will be followed or to reactively inspect the behavior of the network. Second, the distributed nature of state updates at the data plane leads to inconsistent network behavior during reconfigurations. Third, the large flow space makes the data plane susceptible to state exhaustion attacks.   This paper presents SDNsec, an SDN security extension that provides forwarding accountability for the SDN data plane. Forwarding rules are encoded in the packet, ensuring consistent network behavior during reconfigurations and limiting state exhaustion attacks due to table lookups. Symmetric-key cryptography is used to protect the integrity of the forwarding rules and enforce them at each switch. A complementary path validation mechanism allows the controller to reactively examine the actual path taken by the packets. Furthermore, we present mechanisms for secure link-failure recovery and multicast/broadcast forwarding. 	
1606.03474v2	http://arxiv.org/pdf/1606.03474v2	2016	On degeneracy control in overcomplete ICA	Jesse A. Livezey|Alejandro F. Bujan|Friedrich T. Sommer	  Understanding the effects of degeneracy control mechanisms when learning overcomplete representations is crucial for applying Independent Components Analysis (ICA) in machine learning and theoretical neuroscience. A number of approaches to degeneracy control have been proposed which can learn non-degenerate complete representations, however some of these methods can fall into bad local minima when extended to overcomplete ICA. Furthermore, they may have unintended side-effects on the distribution of learned basis elements, which may lead to a biased exploration of the data manifold. In this work, we identify and theoretically analyze the cause of these failures and propose a framework that can be used to evaluate arbitrary degeneracy control mechanisms. We evaluate different methods for degeneracy control in overcomplete ICA and suggest two novel approaches, one of which can learn highly orthonormal bases. Finally, we compare all methods on the task of estimating an overcomplete basis on natural images. 	
1608.00198v2	http://arxiv.org/pdf/1608.00198v2	2016	Existence of localizing solutions in plasticity via geometric singular   perturbation theory	Min-Gi Lee|Athanasios Tzavaras	  Shear bands are narrow zones of intense shear observed during plastic deformations of metals at high strain rates. Because they often precede rupture, their study attracted attention as a mechanism of material failure. Here, we aim to reveal the onset of localization into shear bands using a simple model developed from viscoplasticity. We exploit the properties of scale invariance of the model to construct a family of self-similar focusing solutions that capture the nonlinear mechanism of shear band formation. The key step is to de-singularize a reduced system of singular ordinary differential equations and reduce the problem into the construction of a heteroclinic orbit for an autonomous system of three first-order equations. The associated dynamical system has fast and slow time scales, forming a singularly perturbed problem. The geometric singular perturbation theory is applied to this problem to achieve an invariant surface. The flow on the invariant surface is analyzed via the Poincar\'{e}-Bendixson theorem to construct a heteroclinic orbit. 	
1611.04748v3	http://arxiv.org/pdf/1611.04748v3	2017	Improved Handover Through Dual Connectivity in 5G mmWave Mobile Networks	Michele Polese|Marco Giordani|Marco Mezzavilla|Sundeep Rangan|Michele Zorzi	  The millimeter wave (mmWave) bands offer the possibility of orders of magnitude greater throughput for fifth generation (5G) cellular systems. However, since mmWave signals are highly susceptible to blockage, channel quality on any one mmWave link can be extremely intermittent. This paper implements a novel dual connectivity protocol that enables mobile user equipment (UE) devices to maintain physical layer connections to 4G and 5G cells simultaneously. A novel uplink control signaling system combined with a local coordinator enables rapid path switching in the event of failures on any one link. This paper provides the first comprehensive end-to-end evaluation of handover mechanisms in mmWave cellular systems. The simulation framework includes detailed measurement-based channel models to realistically capture spatial dynamics of blocking events, as well as the full details of MAC, RLC and transport protocols. Compared to conventional handover mechanisms, the study reveals significant benefits of the proposed method under several metrics. 	
1612.09230v2	http://arxiv.org/pdf/1612.09230v2	2017	Ideal strength of two-dimensional stanene may reach or exceed Griffith   strength estimate	Zhe Shi|Chandra Veer Singh	  The ideal strength is the maximum stress a material can withstand, and it is an important intrinsic property for structural applications. Griffith strength limit ~E/9 is the best known upper bound of this property for a material loaded in tension. Here we report that stanene, a recently fabricated two-dimensional material, could approach and possibly exceed this limit from a theoretical perspective. Utilizing first-principles density functional theory, we investigated the nonlinear elastic behavior of stanene and found that its strength could reach ~E/7.4 under uniaxial tension in both armchair and zigzag directions without incurring phonon instability or mechanical failure. The unique mechanical properties of stanene are further appreciated by comparisons with two other Group-IV 2D materials, graphene and silicene. 	
1702.07783v1	http://arxiv.org/pdf/1702.07783v1	2017	Generalized Knudsen Number for Unsteady Fluid Flow	Vural Kara|Victor Yakhot|Kamil L. Ekinci	  We explore the scaling behavior of an unsteady flow that is generated by an oscillating body of finite size in a gas. If the gas is gradually rarefied, the Navier-Stokes equations begin to fail and a kinetic description of the flow becomes more appropriate. The failure of the Navier-Stokes equations can be thought to take place via two different physical mechanisms: either the continuum hypothesis breaks down as a result of a finite size effect; or local equilibrium is violated due to the high rate of strain. By independently tuning the relevant linear dimension and the frequency of the oscillating body, we can experimentally observe these two different physical mechanisms. All the experimental data, however, can be collapsed using a single dimensionless scaling parameter that combines the relevant linear dimension and the frequency of the body. This proposed Knudsen number for an unsteady flow is rooted in a fundamental symmetry principle, namely Galilean invariance. 	
1703.02245v2	http://arxiv.org/pdf/1703.02245v2	2017	Design of the Artificial: lessons from the biological roots of general   intelligence	Nima Dehghani	  Our desire and fascination with intelligent machines dates back to the antiquity's mythical automaton Talos, Aristotle's mode of mechanical thought (syllogism) and Heron of Alexandria's mechanical machines and automata. However, the quest for Artificial General Intelligence (AGI) is troubled with repeated failures of strategies and approaches throughout the history. This decade has seen a shift in interest towards bio-inspired software and hardware, with the assumption that such mimicry entails intelligence. Though these steps are fruitful in certain directions and have advanced automation, their singular design focus renders them highly inefficient in achieving AGI. Which set of requirements have to be met in the design of AGI? What are the limits in the design of the artificial? Here, a careful examination of computation in biological systems hints that evolutionary tinkering of contextual processing of information enabled by a hierarchical architecture is the key to build AGI. 	
1703.10045v1	http://arxiv.org/pdf/1703.10045v1	2017	Machining of Spherical Component Fabricated by Selected Laser Melting,   Part II: Application of Ti in Biomedical	AmirMahyar Khorasani	  Ti and Ti-Based alloys have unique properties such as high strength, low density and excellent corrosion resistance. These properties are essential for the manufacture of lightweight and high strength components for biomedical applications. In this paper, Ti properties such as metallurgy, mechanical properties, surface modification, corrosion resistance, biocompatibility and osseointegration in biomedical applications have been discussed. This paper also analyses the advantages and disadvantages of various Ti manufacturing processes for biomedical applications such as casting, powder metallurgy, cold and hot working, machining, laser engineering net shaping, superplastic forming, forging and ring rolling. The contributions of this research are twofold, firstly scrutinizing the behaviour of Ti and Ti-Based alloys in-vivo and in-vitro experiments in biomedical applications to determine the factors leading to failure, and secondly strategies to achieve desired properties essential to improving the quality of patient outcomes after receiving surgical implants. Future research will be directed toward manufacturing of Ti for medical applications by improving the production process, for example using optimal design approaches in additive manufacturing and investigating alloys containing other materials in order to obtain better medical and mechanical characteristics. 	
1704.00617v1	http://arxiv.org/pdf/1704.00617v1	2017	$α$Check: A mechanized metatheory model-checker	James Cheney|Alberto Momigliano	  The problem of mechanically formalizing and proving metatheoretic properties of programming language calculi, type systems, operational semantics, and related formal systems has received considerable attention recently. However, the dual problem of searching for errors in such formalizations has attracted comparatively little attention. In this article, we present $\alpha$Check, a bounded model-checker for metatheoretic properties of formal systems specified using nominal logic. In contrast to the current state of the art for metatheory verification, our approach is fully automatic, does not require expertise in theorem proving on the part of the user, and produces counterexamples in the case that a flaw is detected. We present two implementations of this technique, one based on negation-as-failure and one based on negation elimination, along with experimental results showing that these techniques are fast enough to be used interactively to debug systems as they are developed. 	
1706.00536v2	http://arxiv.org/pdf/1706.00536v2	2017	Modeling Latent Attention Within Neural Networks	Christopher Grimm|Dilip Arumugam|Siddharth Karamcheti|David Abel|Lawson L. S. Wong|Michael L. Littman	  Deep neural networks are able to solve tasks across a variety of domains and modalities of data. Despite many empirical successes, we lack the ability to clearly understand and interpret the learned internal mechanisms that contribute to such effective behaviors or, more critically, failure modes. In this work, we present a general method for visualizing an arbitrary neural network's inner mechanisms and their power and limitations. Our dataset-centric method produces visualizations of how a trained network attends to components of its inputs. The computed "attention masks" support improved interpretability by highlighting which input attributes are critical in determining output. We demonstrate the effectiveness of our framework on a variety of deep neural network architectures in domains from computer vision, natural language processing, and reinforcement learning. The primary contribution of our approach is an interpretable visualization of attention that provides unique insights into the network's underlying decision-making process irrespective of the data modality. 	
1706.02703v1	http://arxiv.org/pdf/1706.02703v1	2017	Topological resilience in non-normal networked systems	Malbor Asllani|Timoteo Carletti	  The network of interactions in complex systems, strongly influences their resilience, the system capability to resist to external perturbations or structural damages and to promptly recover thereafter. The phenomenon manifests itself in different domains, e.g. cascade failures in computer networks or parasitic species invasion in ecosystems. Understanding the networks topological features that affect the resilience phenomenon remains a challenging goal of the design of robust complex systems. We prove that the non-normality character of the network of interactions amplifies the response of the system to exogenous disturbances and can drastically change the global dynamics. We provide an illustrative application to ecology by proposing a mechanism to mute the Allee effect and eventually a new theory of patterns formation involving a single diffusing species. 	
1709.03286v1	http://arxiv.org/pdf/1709.03286v1	2017	Strand plasticity governs fatigue in colloidal gels	Jan Maarten van Doorn|Joanne E. Verweij|Joris Sprakel|Jasper van der Gucht	  Repeated loading of a solid leads to microstructural damage that ultimately results in catastrophic material failure. While posing a major threat to the stability of virtually all materials, the microscopic origins of fatigue, especially for soft solids, remain elusive. Here we explore fatigue in colloidal gels as prototypical inhomogeneous soft solids by combining experiments and computer simulations. Our results reveal how mechanical loading leads to irreversible strand stretching, which builds slack into the network that softens the solid at small strains and causes strain hardening at larger deformations. We thus find that microscopic plasticity governs fatigue at much larger scales. This gives rise to a new picture of fatigue in soft thermal solids and calls for new theoretical descriptions of soft gel mechanics in which local plasticity is taken into account. 	
1710.01846v1	http://arxiv.org/pdf/1710.01846v1	2017	Revisiting the Deformation-Induced Damage in Filled Elastomers: Effect   of Network Polydispersity	Mohammad Tehrani|Mohammad Hossein Moshaei|Alireza Sarvestani	  A priori assumption in micromechanical analysis of polymeric networks is that the constitutive polymer strands are of equal length. Monodisperse distribution of strands, however, is merely a simplifying assumption. In this paper, we relax this assumption and consider a vulcanized network with a broad distribution of strand length. In the light of this model, we predict the damage initiation and stress-stretch dependency in a filled polymer network with random internal structures. The degradation of network mechanical behavior is assumed to be controlled by the adhesive failure of the strands adsorbed to the filler surface. We show that the short adsorbed strands are the culprits for damage initiation and their finite extensibility is a key determinant of mechanical strength. 	
1712.05896v1	http://arxiv.org/pdf/1712.05896v1	2017	Impression Network for Video Object Detection	Congrui Hetang|Hongwei Qin|Shaohui Liu|Junjie Yan	  Video object detection is more challenging compared to image object detection. Previous works proved that applying object detector frame by frame is not only slow but also inaccurate. Visual clues get weakened by defocus and motion blur, causing failure on corresponding frames. Multi-frame feature fusion methods proved effective in improving the accuracy, but they dramatically sacrifice the speed. Feature propagation based methods proved effective in improving the speed, but they sacrifice the accuracy. So is it possible to improve speed and performance simultaneously?   Inspired by how human utilize impression to recognize objects from blurry frames, we propose Impression Network that embodies a natural and efficient feature aggregation mechanism. In our framework, an impression feature is established by iteratively absorbing sparsely extracted frame features. The impression feature is propagated all the way down the video, helping enhance features of low-quality frames. This impression mechanism makes it possible to perform long-range multi-frame feature fusion among sparse keyframes with minimal overhead. It significantly improves per-frame detection baseline on ImageNet VID while being 3 times faster (20 fps). We hope Impression Network can provide a new perspective on video feature enhancement. Code will be made available. 	
1801.02447v1	http://arxiv.org/pdf/1801.02447v1	2018	Synchronized oscillations and acoustic fluidization in confined granular   materials	F. Giacco|L. de Arcangelis|M. Pica Ciamarra|E. Lippiello	  According to the acoustic fluidization hypothesis, elastic waves at a characteristic frequency form inside seismic faults even in the absence of an external perturbation. These waves are able to generate a normal stress which contrasts the confining pressure and promotes failure. Here, we study the mechanisms responsible for this wave activation via numerical simulations of a granular fault model. We observe the particles belonging to the percolating backbone, which sustains the stress, to perform synchronized oscillations over ellipticlike trajectories in the fault plane. These oscillations occur at the characteristic frequency of acoustic fluidization. As the applied shear stress increases, these oscillations become perpendicular to the fault plane just before the system fails, opposing the confining pressure, consistently with the acoustic fluidization scenario. The same change of orientation can be induced by external perturbations at the acoustic fluidization frequency. 	
1802.03921v1	http://arxiv.org/pdf/1802.03921v1	2018	Test Agents: Adaptive, Autonomous and Intelligent Test Cases	Eduard Enoiu|Mirgita Frasheri	  Growth of software size, lack of resources to perform regression testing, and failure to detect bugs faster have seen increased reliance on continuous integration and test automation. Even with greater hardware and software resources dedicated to test automation, software testing is faced with enormous challenges, resulting in increased dependence on complex mechanisms for automated test case selection and prioritization as part of a continuous integration framework. These mechanisms are currently using simple entities called test cases that are concretely realized as executable scripts. Our key idea is to provide test cases with more reasoning, adaptive behavior and learning capabilities by using the concepts of intelligent software agents. We refer to such test cases as test agents. The model that underlie a test agent is capable of flexible and autonomous actions in order to meet overall testing objectives. Our goal is to increase the decentralization of regression testing by letting test agents to know for themselves when they should be executing, how they should update their purpose, and when they should interact with each other. In this paper, we envision software test agents that display such adaptive autonomous behavior. Emerging developments and challenges regarding the use of test agents are explored-in particular, new research that seeks to use adaptive autonomous agents in software testing. 	
1802.06246v1	http://arxiv.org/pdf/1802.06246v1	2018	Backlash Identification in Two-Mass Systems by Delayed Relay Feedback	Michael Ruderman|Shota Yamada|Hiroshi Fujimoto	  Backlash, also known as mechanical play, is a piecewise differentiable nonlinearity which exists in several actuated systems, comprising, e.g., rack-and-pinion drives, shaft couplings, toothed gears, and other elements. Generally, the backlash is nested between the moving elements of a complex dynamic system, which handicaps its proper detection and identification. A classical example is the two-mass system which can approximate numerous mechanisms connected by a shaft (or link) with relatively high stiffness and backlash in series. Information about the presence and extent of the backlash is seldom exactly known and is rather conditional upon factors such as wear, fatigue and incipient failures in components. This paper proposes a novel backlash identification method using one-side sensing of a twomass system. The method is based on the delayed relay operator in feedback that allows stable and controllable limit cycles to be induced, operating within the unknown backlash gap. The system model, with structural transformations required for the one-side backlash measurements, is given, along with the analysis of the delayed relay in velocity feedback. Experimental evaluations are shown for a two-inertia motor bench with gear coupling, with a low backlash gap of about one degree. 	
1409.0540v2	http://arxiv.org/pdf/1409.0540v2	2015	The Landscape of the Neutrino Mechanism of Core-Collapse Supernovae:   Neutron Star and Black Hole Mass Functions, Explosion Energies and Nickel   Yields	Ondrej Pejcha|Todd A. Thompson	  If the neutrino luminosity from the proto-neutron star formed during a massive star core collapse exceeds a critical threshold, a supernova (SN) results. Using spherical quasi-static evolutionary sequences for hundreds of progenitors over a range of metallicities, we study how the explosion threshold maps onto observables, including the fraction of successful explosions, the neutron star (NS) and black hole (BH) mass functions, the explosion energies (E_SN) and nickel yields (M_Ni), and their mutual correlations. Successful explosions are intertwined with failures in a complex pattern that is not simply related to initial progenitor mass or compactness. We predict that progenitors with initial masses of 15 +/- 1, 19 +/- 1, and 21-26 M_Sun are most likely to form BHs, that the BH formation probability is non-zero at solar-metallicity and increases significantly at low metallicity, and that low luminosity, low Ni-yield SNe come from progenitors close to success/failure interfaces. We qualitatively reproduce the observed E_SN-M_Ni correlation, we predict a correlation between the mean and width of the NS mass and E_SN distributions, and that the means of the NS and BH mass distributions are correlated. We show that the observed mean NS mass of ~1.33 M_Sun implies that the successful explosion fraction is higher than 0.35. Overall, we show that the neutrino mechanism can in principle explain the observed properties of SNe and their compact objects. We argue that the rugged landscape of progenitors and outcomes mandates that SN theory should focus on reproducing the wide ranging distributions of observed SN properties. 	
0306217v4	http://arxiv.org/pdf/cond-mat/0306217v4	2004	Competing Styles of Statistical Mechanics: I. Systematization and   Clarification in a General Theory	Roberto Luzzi|Áurea R. Vasconcellos|J. Galvão Ramos	  Competing styles of Statistical Mechanics have been introduced as practical succedaneous to the conventional well established Boltzmann-Gibbs statistical mechanics, when in the use of the latter the researcher is impaired in his/her capacity for satisfying the Criteria of Efficiency and/or Sufficiency in statistics [Fisher, 1922], that is, a failure in the characterization (presence of fractality, scaling, etc.) of the system related to some aspect relevant to the given physical situation. To patch this limitation on the part of the observer, in order to make predictions on the values of observables and response functions, are introduced unconventional approaches. We present a detailed description of their construction and a clarification of its scope and interpretation. Also, resorting to the use of the particular case of Renyi's unconventional statistics is built a nonequilibrium ensemble formalism. The unconventional distribution functions of fermions and bosons are obtained, and in a follow-up article [cond-mat/0306247] we describe applications to the study of experimental results in semiconductor physics and in electro-chemistry involving nanometric scales and fractal-like structures, and some additional theoretical analysis is added. PACS: 05.70.Ln, 82.20.Mj, 82.20.Db Keywords: Nonequilibrium Ensemble Formalism; Generalized Informational Entropies; Generalized Statistics; Nonextensive Statistics; Renyi Statistics; Escort Probability. 	
nonequilibrium ensemble formalism, generalized informational entropies,
generalized statistics, nonextensive statistics, renyi statistics, escort
probability 

0607103v1	http://arxiv.org/pdf/cs/0607103v1	2006	Ideas by Statistical Mechanics (ISM)	Lester Ingber	  Ideas by Statistical Mechanics (ISM) is a generic program to model evolution and propagation of ideas/patterns throughout populations subjected to endogenous and exogenous interactions. The program is based on the author's work in Statistical Mechanics of Neocortical Interactions (SMNI), and uses the author's Adaptive Simulated Annealing (ASA) code for optimizations of training sets, as well as for importance-sampling to apply the author's copula financial risk-management codes, Trading in Risk Dimensions (TRD), for assessments of risk and uncertainty. This product can be used for decision support for projects ranging from diplomatic, information, military, and economic (DIME) factors of propagation/evolution of ideas, to commercial sales, trading indicators across sectors of financial markets, advertising and political campaigns, etc. A statistical mechanical model of neocortical interactions, developed by the author and tested successfully in describing short-term memory and EEG indicators, is the proposed model. Parameters with a given subset of macrocolumns will be fit using ASA to patterns representing ideas. Parameters of external and inter-regional interactions will be determined that promote or inhibit the spread of these ideas. Tools of financial risk management, developed by the author to process correlated multivariate systems with differing non-Gaussian distributions using modern copula analysis, importance-sampled using ASA, will enable bona fide correlations and uncertainties of success and failure to be calculated. Marginal distributions will be evolved to determine their expected duration and stability using algorithms developed by the author, i.e., PATHTREE and PATHINT codes. 	
0501011v2	http://arxiv.org/pdf/quant-ph/0501011v2	2005	Contribution from stochastic electrodynamics to the understanding of   quantum mechanics	L. de la Pena|A. M. Cetto	  During the last decades there has been a relatively extensive attempt to develop the theory of stochastic electrodynamics (SED) with a view to establishing it as the foundation for quantum mechanics. The theory had several important successes, but failed when applied to the study of particles subject to nonlinear forces. An analysis of the failure showed that its reasons are not to be ascribed to the principles of SED, but to the methods used to construct the theory, particularly the use of a Fokker-Planck approximation and perturbation theory. A new, non perturbative approach has been developed, called linear stochastic electrodynamics (LSED), of which a clean form is presented here. After introducing the fundamentals of SED, we discuss in detail the principles on which LSED is constructed. We pay attention to the fundamental issue of the mechanism that leads to the quantum behaviour of field and matter, and demonstrate that indeed LSED is a natural way to the quantum formalism by demanding its solutions to comply with a limited number of principles, each one with a clear physical meaning. As a further application of the principles of LSED we derive also the Planck distribution. In a final section we revisit some of the most tantalizing quandaries of quantum mechanics from the point of view offered by the present theory, and show that it offers a clear physical answer to them. 	
1302.3022v1	http://arxiv.org/pdf/1302.3022v1	2013	Characterising the Anisotropic Mechanical Properties of Excised Human   Skin	Aisling Ni Annaidh|Karine Bruyere|Michel Destrade|Michael D. Gilchrist|Melanie Ottenio	  The mechanical properties of skin are important for a number of applications including surgery, dermatology, impact biomechanics and forensic science. In this study we have investigated the influence of location and orientation on the deformation characteristics of 56 samples of excised human skin. Uniaxial tensile tests were carried out at a strain rate of 0.012s$^{-1}$ on skin from the back. Digital Image Correlation was used for 2D strain measurement and a histological examination of the dermis was also performed. The mean ultimate tensile strength (UTS) was 21.6$\pm$8.4MPa, the mean failure strain 54$\pm$17%, the mean initial slope 1.18$\pm$0.88MPa, the mean elastic modulus 83.3$\pm$34.9MPa and the mean strain energy was 3.6$\pm$1.6MJ/m$^3$. A multivariate analysis of variance has shown that these mechanical properties of skin are dependent upon the orientation of Langer lines (P$<$0.0001-P=0.046). The location of specimens on the back was also found to have a significant effect on the UTS (P =0.0002), the elastic modulus (P=0.001) and the strain energy (P=0.0052). The histological investigation concluded that there is a definite correlation between the orientation of Langer Lines and the preferred orientation of collagen fibres in the dermis (P$<$0.001). The data obtained in this study will provide essential information for those wishing to model the skin using a structural constitutive model. 	
1303.6520v1	http://arxiv.org/pdf/1303.6520v1	2013	Activation and radiation damage in the environment of hadron   accelerators	Daniela Kiselev	  A component which suffers radiation damage usually also becomes radioactive, since the source of activation and radiation damage is the interaction of the material with particles from an accelerator or with reaction products. However, the underlying mechanisms of the two phenomena are different. These mechanisms are described here. Activation and radiation damage can have far-reaching consequences. Components such as targets, collimators, and beam dumps are the first candidates for failure as a result of radiation damage. This means that they have to be replaced or repaired. This takes time, during which personnel accumulate dose. If the dose to personnel at work would exceed permitted limits, remote handling becomes necessary. The remaining material has to be disposed of as radioactive waste, for which an elaborate procedure acceptable to the authorities is required. One of the requirements of the authorities is a complete nuclide inventory. The methods used for calculation of such inventories are presented, and the results are compared with measured data. In the second part of the paper, the effect of radiation damage on material properties is described. The mechanism of damage to a material due to irradiation is described. The amount of radiation damage is quantified in terms of displacements per atom. Its calculation and deficiencies in explaining and predicting the changes in mechanical and thermal material properties are discussed, and examples are given. 	
1408.2093v2	http://arxiv.org/pdf/1408.2093v2	2014	Why Current Interpretations of Quantum Mechanics are Deficient	Elliott Tammaro	  Quantum mechanics under the Copenhagen interpretation is one of the most experimentally well verified formalisms. However, it is known that the interpretation makes explicit reference to external observation or "measurement." One says that the Copenhagen interpretation suffers from the measurement problem. This deficiency of the interpretation excludes it as a viable fundamental formalism and prevents the use of standard quantum mechanics in discussions of quantum cosmology. Numerous alternative interpretations have been developed with the goals of reproducing its predictive success while obviating the measurement problem. While several interpretations make distinct, falsifiable, predictions, many claim to precisely reproduce the results of standard quantum mechanics. The sheer number of interpretations raises several issues. If the experimental predictions are identical, how are they to be assessed? On what grounds can an interpretation be said to trump another? Without recourse to experimental findings, one may continue to assess an interpretation on its logical structure, self-consistency, and simplicity (number and plausibility of its assumptions). We argue, and where possible, demonstrate, that all common interpretations have unresolved deficiencies. Among these deficiencies are failures to resolve the measurement problem, fine-tuning problems, logical/mathematical inconsistencies, disagreement with experiment, and others. Shortcomings as severe as these call into question the viability of any of the common interpretations. When appropriate, we indicate where future work may resolve some of these issues. 	
1508.02137v1	http://arxiv.org/pdf/1508.02137v1	2015	Stochastic modeling and survival analysis of marginally trapped neutrons   for a magnetic trapping neutron lifetime experiment	K. J. Coakley|M. S. Dewey|M. G. Huber|P. R. Huffman|C. R. Huffer|D. E. Marley|H. P. Mumm|C. M. O'Shaughnessy|K. W. Schelhammer|A. K. Thompson|A. T. Yue	  In a variety of neutron lifetime experiments, in addition to $\beta-$decay, neutrons can be lost by other mechanisms including wall losses. Failure to account for these other loss mechanisms produces systematic measurement error and associated systematic uncertainties in neutron lifetime measurements. In this work, we develop a physical model for neutron wall losses and construct a competing risks survival analysis model to account for losses due to the joint effect of $\beta-$decay losses, wall losses of marginally trapped neutrons, and an additional absorption mechanism. We determine the survival probability function associated with the wall loss mechanism by a Monte Carlo method. Based on a fit of the competing risks model to a subset of the NIST experimental data, we determine the mean lifetime of trapped neutrons to be approximately 700 s -- considerably less than the current best estimate of (880.1 $\pm$ 1.1) s promulgated by the Particle Data Group [1]. Currently, experimental studies are underway to determine if this discrepancy can be explained by neutron capture by ${}^3$He impurities in the trapping volume. Analysis of the full NIST data will be presented in a later publication. 	
1604.06485v1	http://arxiv.org/pdf/1604.06485v1	2016	A review of the effects of chemical and phase segregation on the   mechanical behaviour of multi-phase steels	Bernard Ennis	  In the drive towards higher strength alloys, a diverse range of alloying elements is employed to enhance their strength and ductility. Limited solid solubility of these elements in steel leads to segregation during casting which affects the entire down-stream processing and eventually the mechanical properties of the finished product. Although it is thought that the presence of continuous bands lead to premature failure, it has not been possible to verify this link. This poses as increasingly greater risk for higher alloyed, higher strength steels which are prone to centre-line segregation: it is thus vital to be able to predict the mechanical behaviour of multi-phase (MP) steels under loading.   This review covers the microstructure and properties of galvanised advanced high strength steels with particular emphasis to their use in automotive applications. In order to understand the origins of banding, the origins of segregation of alloying elements during casting and partitioning in the solid state will be discussed along with the effects on the mechanical behaviour and damage evolution under (tensile) loading. Attention will also be paid to the application of microstructural models in tailoring the production process to enable suppression of the effects of segregation upon banding. Finally, the theory and application of the experimental techniques used in this work to elucidate the structure and properties will be examined. 	
1702.02145v1	http://arxiv.org/pdf/1702.02145v1	2017	Structure--property relationships of cell clusters in biotissues: 2D   analysis	Xiaohua Zhou|Erhu Zhang|Minggang Xia|Jianlin Liu|Shengli Zhang	  To insight the relationships between the self-organizing structures of cells, such as the cell clusters, and the properties of biotissues is helpful in revealing the function and designing biomaterial. Traditional random foam model neglects several important details of the frameworks of cell clusters, in this study we use a more complete model, cell adhesion model, to investigate the mechanical and morphological properties of the two-dimensional (2D) dry foams composed by cells. Supposing these structures are formed due to adhesion between cells, the equilibrium formations result from the minimum of the free energy. The equilibrium shape equations for high symmetrical structures without the volume constraint are derived, and the analytical results of the corresponding mechanical parameters, such as the Young's modulus, bulk modulus and failure strength, are obtained. Numerical simulation method is applied to study the complex shapes with the volume constraint and several stable multicellular structures are obtained. Symmetry-breaking due to the volume change is founded and typical periodic shapes and the corresponding phase transformations are explored. Our study provides a potential method to connect the microstructure with the macro-mechanical parameters of biotissues. The results also are helpful to understand the physical mechanism of how the structures of biotissues are formed. 	
1712.02074v1	http://arxiv.org/pdf/1712.02074v1	2017	Tensile rupture of medial arterial tissue studied by X-ray   micro-tomography on stained samples	Clémentine Helfenstein-Didier|Damien Taïnoff|Julien Viville|Jérôme Adrien|Éric Maire|Pierre Badel	  Detailed characterization of damage and rupture mechanics of arteries is one the current challenges in vascular biomechanics, which requires developing suitable experimental approaches. This paper introduces an approach using in situ tensile tests in an X-ray micro-tomography setup to observe mechanisms of damage initiation and progression in medial layers of porcine aortic samples. The technique requires the use of sodium polytungstate as a contrast agent, of which the conditions for use are detailed in this paper. Immersion of the samples during 24 hours in a 15 g.L-1 concentrated solution provided the best compromise for viewing musculo-elastic units in this tissue. The process of damage initiation, delamination and rupture of medial tissue under tensile loading was observed and can be described as an elementary process repeating several times until complete failure. This elementary process initiates with a sudden mode I fracture of a group of musculo-elastic units, followed by an elastic recoil of these units, causing mode II separation of these, hence a delamination plane. The presented experimental approach constitutes a basis for observation of other constituents, or for investigations on other tissues and damage mechanisms. 	
1801.04269v1	http://arxiv.org/pdf/1801.04269v1	2018	Mechanical Properties of Pentagraphene-based Nanotubes: A Molecular   Dynamics Study	Jose M. de Sousa|Acrisio L. Aguiar|Eduardo C. Girão|Alexandre F. Fonseca|Antonio G. Sousa Filho|Douglas S. Galvao	  The study of the mechanical properties of nanostructured systems has gained importance in theoretical and experimental research in recent years. Carbon nanotubes (CNTs) are one of the strongest nanomaterials found in nature, with Young's Modulus (YM) in the order 1.25 TPa. One interesting question is about the possibility of generating new nanostructures with 1D symmetry and with similar and/or superior CNT properties. In this work, we present a study on the dynamical, structural, mechanical properties, fracture patterns and YM values for one class of these structures, the so-called pentagraphene nanotubes (PGNTs). These tubes are formed rolling up pentagraphene membranes (which are quasi-bidimensional structures formed by densely compacted pentagons of carbon atoms in sp3 and sp2 hybridized states) in the same form that CNTs are formed from rolling up graphene membranes. We carried out fully atomistic molecular dynamics simulations using the ReaxFF force field. We have considered zigzag-like and armchair-like PGNTs of different diameters. Our results show that PGNTs present YM ~ 800 GPa with distinct elastic behavior in relation to CNTs, mainly associated with mechanical failure, chirality dependent fracture patterns and extensive structural reconstructions. 	
0901.4591v1	http://arxiv.org/pdf/0901.4591v1	2009	Network Coding-Based Protection Strategy Against Node Failures	Salah A. Aly|Ahmed E. Kamal	  The enormous increase in the usage of communication networks has made protection against node and link failures essential in the deployment of reliable networks. To prevent loss of data due to node failures, a network protection strategy is proposed that aims to withstand such failures. Particularly, a protection strategy against any single node failure is designed for a given network with a set of $n$ disjoint paths between senders and receivers. Network coding and reduced capacity are deployed in this strategy without adding extra working paths to the readily available connection paths. This strategy is based on protection against node failures as protection against multiple link failures. In addition, the encoding and decoding operational aspects of the premeditated protection strategy are demonstrated. 	
0907.1182v1	http://arxiv.org/pdf/0907.1182v1	2009	Catastrophic cascade of failures in interdependent networks	Sergey V. Buldyrev|Roni Parshani|Gerald Paul|H. Eugene Stanley|Shlomo Havlin	  Many systems, ranging from engineering to medical to societal, can only be properly characterized by multiple interdependent networks whose normal functioning depends on one another. Failure of a fraction of nodes in one network may lead to a failure in another network. This in turn may cause further malfunction of additional nodes in the first network and so on. Such a cascade of failures, triggered by a failure of a small faction of nodes in only one network, may lead to the complete fragmentation of all networks. We introduce a model and an analytical framework for studying interdependent networks. We obtain interesting and surprising results that should significantly effect the design of robust real-world networks. For two interdependent Erdos-Renyi (ER) networks, we find that the critical average degree below which both networks collapse is <k_c>=2.445, compared to <k_c>=1 for a single ER network. Furthermore, while for a single network a broader degree distribution of the network nodes results in higher robustness to random failure, for interdependent networks, the broader the distribution is, the more vulnerable the networks become to random failure. 	
1003.0951v2	http://arxiv.org/pdf/1003.0951v2	2013	LogMaster: Mining Event Correlations in Logs of Large scale Cluster   Systems	Rui Ren|Xiaoyu Fu|Jianfeng Zhan|Wei Zhou	  This paper presents a methodology and a system, named LogMaster, for mining correlations of events that have multiple attributions, i.e., node ID, application ID, event type, and event severity, in logs of large-scale cluster systems. Different from traditional transactional data, e.g., supermarket purchases, system logs have their unique characteristic, and hence we propose several innovative approaches to mine their correlations. We present a simple metrics to measure correlations of events that may happen interleavedly. On the basis of the measurement of correlations, we propose two approaches to mine event correlations; meanwhile, we propose an innovative abstraction: event correlation graphs (ECGs) to represent event correlations, and present an ECGs based algorithm for predicting events. For two system logs of a production Hadoop-based cloud computing system at Research Institution of China Mobile and a production HPC cluster system at Los Alamos National Lab (LANL), we evaluate our approaches in three scenarios: (a) predicting all events on the basis of both failure and non-failure events; (b) predicting only failure events on the basis of both failure and non-failure events; (c) predicting failure events after removing non-failure events. 	
1003.3880v2	http://arxiv.org/pdf/1003.3880v2	2010	Lessons from the Failure and Subsequent Success of a Complex Healthcare   Sector IT Project	David Greenwood|Ali Khajeh-Hosseini|Ian Sommerville	  This paper argues that IT failures diagnosed as errors at the technical or project management level are often mistakenly pointing to symptoms of failure rather than a project's underlying socio-complexity (complexity resulting from the interactions of people and groups) which is usually the actual source of failure. We propose a novel method, Stakeholder Impact Analysis, that can be used to identify risks associated with socio-complexity as it is grounded in insights from the social sciences, psychology and management science. This paper demonstrates the effectiveness of Stakeholder Impact Analysis by using the 1992 London Ambulance Service Computer Aided Dispatch project as a case study, and shows that had our method been used to identify the risks and had they been mitigated, it would have reduced the risk of project failure. This paper's original contribution comprises expanding upon existing accounts of failure by examining failures at a level of granularity not seen elsewhere that enables the underlying socio-complexity sources of risk to be identified. 	
1011.4535v2	http://arxiv.org/pdf/1011.4535v2	2010	Cascading Link Failure in the Power Grid: A Percolation-Based Analysis	Hongda Xiao|Edmund Yeh	  Large-scale power blackouts caused by cascading failure are inflicting enormous socioeconomic costs. We study the problem of cascading link failures in power networks modelled by random geometric graphs from a percolation-based viewpoint. To reflect the fact that links fail according to the amount of power flow going through them, we introduce a model where links fail according to a probability which depends on the number of neighboring links. We devise a mapping which maps links in a random geometric graph to nodes in a corresponding dual covering graph. This mapping enables us to obtain the first-known analytical conditions on the existence and non-existence of a large component of operational links after degree-dependent link failures. Next, we present a simple but descriptive model for cascading link failure, and use the degree-dependent link failure results to obtain the first-known analytical conditions on the existence and non-existence of cascading link failures. 	
1105.5903v3	http://arxiv.org/pdf/1105.5903v3	2011	Probabilistic Analysis of the Network Reliability Problem on a Random   Graph Ensemble	Akiyuki Yano|Tadashi Wadayama	  In the field of computer science, the network reliability problem for evaluating the network failure probability has been extensively investigated. For a given undirected graph $G$, the network failure probability is the probability that edge failures (i.e., edge erasures) make $G$ unconnected. Edge failures are assumed to occur independently with the same probability. The main contributions of the present paper are the upper and lower bounds on the expected network failure probability. We herein assume a simple random graph ensemble that is closely related to the Erd\H{o}s-R\'{e}nyi random graph ensemble. These upper and lower bounds exhibit the typical behavior of the network failure probability. The proof is based on the fact that the cut-set space of $G$ is a linear space over $\Bbb F_2$ spanned by the incident matrix of $G$. The present study shows a close relationship between the ensemble analysis of the network failure probability and the ensemble analysis of the error detection probability of LDGM codes with column weight 2. 	
1106.1652v1	http://arxiv.org/pdf/1106.1652v1	2011	Distributed Storage Codes through Hadamard Designs	Dimitris S. Papailiopoulos|Alexandros G. Dimakis	  In distributed storage systems that employ erasure coding, the issue of minimizing the total {\it repair bandwidth} required to exactly regenerate a storage node after a failure arises. This repair bandwidth depends on the structure of the storage code and the repair strategies used to restore the lost data. Minimizing it requires that undesired data during a repair align in the smallest possible spaces, using the concept of interference alignment (IA). Here, a points-on-a-lattice representation of the symbol extension IA of Cadambe {\it et al.} provides cues to perfect IA instances which we combine with fundamental properties of Hadamard matrices to construct a new storage code with favorable repair properties. Specifically, we build an explicit $(k+2,k)$ storage code over $\mathbb{GF}(3)$, whose single systematic node failures can be repaired with bandwidth that matches exactly the theoretical minimum. Moreover, the repair of single parity node failures generates at most the same repair bandwidth as any systematic node failure. Our code can tolerate any single node failure and any pair of failures that involves at most one systematic failure. 	
1409.0624v1	http://arxiv.org/pdf/1409.0624v1	2014	A random shock model with mixed effect, including competing soft and   sudden failures, and dependence	Sophie Mercier|H. H. Pham	  A system is considered, which is subject to external and possibly fatal shocks, with dependence between the fatality of a shock and the system age. Apart from these shocks, the system suffers from competing soft and sudden failures, where soft failures refer to the reaching of a given thresh-old for the degradation level, and sudden failures to accidental failures, characterized by a failure rate. A non-fatal shock increases both degradation level and failure rate of a random amount, with possible dependence between the two increments. The system reliability is calculated by four different methods. Conditions under which the system lifetime is New Better than Used are proposed. The in uence of various parameters of the shocks environment on the system lifetime is studied. 	
1701.00898v2	http://arxiv.org/pdf/1701.00898v2	2017	Double Link Failure Protection using a Single P-cycle	Pallavi Athe|Yatindra Nath Singh	  In this letter, we investigate survivability in optical networks for protection from two simultaneous link failures. Failure probability of two links with overlapping protection can be high if these links are geographically close. In a network with deterministic single link protection, simultaneous failure of two links may lead to partial or full loss of traffic on the failed links. Two link failure protection will make the network more resilient by protecting double failures having overlapping protection. A method for achieving double fault tolerance is double cycle method (DB); it uses two pre-configured cycles (p-cycles) to protect a link. Single p-cycle (SG) method, which uses one p-cycle to protect a link from two simultaneous link failure is introduced in this letter. Integer linear programs (ILP) are formulated for the SG method as well as DB method. It has been observed that the SG method provides a solution to bigger networks with lesser computational resources as compared to the DB method. 	
1702.00298v2	http://arxiv.org/pdf/1702.00298v2	2018	Cascading Failures in Interdependent Systems: Impact of Degree   Variability and Dependence	Richard J. La	  We study cascading failures in a system comprising interdependent networks/systems, in which nodes rely on other nodes both in the same system and in other systems to perform their function. The (inter-)dependence among nodes is modeled using a dependence graph, where the degree vector of a node determines the number of other nodes it can potentially cause to fail in each system through aforementioned dependency. In particular, we examine the impact of the variability and dependence properties of node degrees on the probability of cascading failures. We show that larger variability in node degrees hampers widespread failures in the system, starting with random failures. Similarly, positive correlations in node degrees make it harder to set off an epidemic of failures, thereby rendering the system more robust against random failures. 	
1704.06917v1	http://arxiv.org/pdf/1704.06917v1	2017	Identify Critical Branches with Cascading Failure Chain Statistics and   Hypertext-Induced Topic Search Algorithm	Chao Luo|Jun Yang	  An effective way to suppress the cascading failure risk is the branch capacity upgrade, whose optimal decision making, however, may incur high computational burden. A practical way is to find out some critical branches as the candidates in advance. This paper proposes a simulation data oriented approach to identify the critical branches with higher importance in cascading failure propagation. First, a concept of cascading failure chain (CFC) is introduced and numerous samples of CFC are generated with an AC power flow based cascading failure simulator. Then, a directed weighted graph is constructed, whose edges denotes the severities of branch interactions. Third, the weighted hypertext-induced topic search (HITS) algorithm is used to rate and rank this graph's vertices,through which the critical branches can be identified accordingly. Validations on IEEE 118bus and RTS96 systems show that the proposed approach can identify critical branches whose capacity upgrades suppress cascading failure risk more greatly. Moreover, it is also shown that structural importance of a branch does not agree with its importance in cascading failure, which indicates the effectiveness of the proposed approach compared with structure vulnerabilities based identifying methods. 	
1706.10127v1	http://arxiv.org/pdf/1706.10127v1	2017	Studying Cascading Overload Failures under High Penetration of Wind   Generation	Mir Hadi Athari|Zhifang Wang	  While power systems are reliable infrastructures, their complex interconnectivities allow for propagation of disturbances through cascading failures which causes blackouts. Meanwhile the ever increasing penetration level of renewable generation into power grids introduces a massive amount of uncertainty to the grid that might have a severe impact on grid vulnerability to overload cascading failures. There are numerous studies in the literature that focus on modeling cascading failures with different approaches. However, there is a need for studies that simulate cascading failure considering the uncertainty coming from high penetration of renewable generation. In this study, the impacts of wind generation in terms of its penetration and uncertainty levels on grid vulnerability to cascading overload failures are studied. The simulation results on IEEE 300 bus system show that uncertainty coming from wind energy have severe impact on grid vulnerability to cascading overload failures. Results also suggest that higher penetration levels of wind energy if not managed appropriately will add to this severity due to injection of higher uncertainties into the grid. 	
1101.3859v1	http://arxiv.org/pdf/1101.3859v1	2011	OSPF Weight Setting Optimization for Single Link Failures	Mohammed H. Sqalli|Sadiq M. Sait|Syed Asadullah	  In operational networks, nodes are connected via multiple links for load sharing and redundancy. This is done to make sure that a failure of a link does not disconnect or isolate some parts of the network. However, link failures have an effect on routing, as the routers find alternate paths for the traffic originally flowing through the link which has failed. This effect is severe in case of failure of a critical link in the network, such as backbone links or the links carrying higher traffic loads. When routing is done using the Open Shortest Path First (OSPF) routing protocol, the original weight selection for the normal state topology may not be as efficient for the failure state. In this paper, we investigate the single link failure issue with an objective to find a weight setting which results in efficient routing in normal and failure states. We engineer Tabu Search Iterative heuristic using two different implementation strategies to solve the OSPF weight setting problem for link failure scenarios. We evaluate these heuristics and show through experimental results that both heuristics efficiently handle weight setting for the failure state. A comparison of both strategies is also presented. 	
9908329v1	http://arxiv.org/pdf/cond-mat/9908329v1	1999	Material failure time and the fiber bundle model with thermal noise	A. Guarino|R. Scorretti|S. Ciliberto	  The statistical properties of failure are studied in a fiber bundle model with thermal noise. We find that in agreement with recent experiments the macroscopic failure is produced by a thermal activation of microcracks. Most importantly the effective temperature of the system is amplified by the spatial disorder (heterogeneity) of the fiber bundle. 	
1105.0296v1	http://arxiv.org/pdf/1105.0296v1	2011	A Formal Model of Anonymous Systems	Yang D. Li	  We put forward a formal model of anonymous systems. And we concentrate on the anonymous failure detectors in our model. In particular, we give three examples of anonymous failure detectors and show that they can be used to solve the consensus problem and that they are equivalent to their classic counterparts. Moreover, we show some relationship among them and provide a simple classification of anonymous failure detectors. 	
1510.09119v2	http://arxiv.org/pdf/1510.09119v2	2016	From Byzantine Failures to Crash Failures in Message-Passing Systems: a   BG Simulation-based approach	Damien Imbs|Michel Raynal|Julien Stainer	  The BG-simulation is a powerful reduction algorithm designed for asynchronous read/write crash-prone systems. It allows a set of $(t+1)$ asynchronous sequential processes to wait-free simulate (i.e., despite the crash of up to $t$ of them) an arbitrary number $n$ of processes under the assumption that at most $t$ of them may crash. The BG simulation shows that, in read/write systems, the crucial parameter is not the number $n$ of processes, but the upper bound $t$ on the number of process crashes.   The paper extends the concept of BG simulation to asynchronous message-passing systems prone to Byzantine failures. Byzantine failures are the most general type of failure: a faulty process can exhibit any arbitrary behavior. Because of this, they are also the most difficult to analyze and to handle algorithmically. The main contribution of the paper is a signature-free reduction of Byzantine failures to crash failures. Assuming $t<\min(n',n/3)$, the paper presents an algorithm that simulates a system of $n'$ processes where up to $t$ may crash, on top of a basic system of $n$ processes where up to $t$ may be Byzantine. While topological techniques have been used to relate the computability of Byzantine failure-prone systems to that of crash failure-prone ones, this simulation is the first, to our knowledge, that establishes this relation directly, in an algorithmic way.   In addition to extending the basic BG simulation to message-passing systems and failures more severe than process crashes, being modular and direct, this simulation provides us with a deeper insight in the nature and understanding of crash and Byzantine failures in the context of asynchronous message-passing systems. Moreover, it also allows crash-tolerant algorithms, designed for asynchronous read/write systems, to be executed on top of asynchronous message-passing systems prone to Byzantine failures. 	
1609.02956v1	http://arxiv.org/pdf/1609.02956v1	2016	Puzzles in modern biology. I. Male sterility, failure reveals design	Steven A. Frank	  Many human males produce dysfunctional sperm. Various plants frequently abort pollen. Hybrid matings often produce sterile males. Widespread male sterility is puzzling. Natural selection prunes reproductive failure. Puzzling failure implies something that we do not understand about how organisms are designed. Solving the puzzle reveals the hidden processes of design. 	
1702.05849v1	http://arxiv.org/pdf/1702.05849v1	2017	A Platform for Automating Chaos Experiments	Ali Basiri|Aaron Blohowiak|Lorin Hochstein|Casey Rosenthal	  The Netflix video streaming system is composed of many interacting services. In such a large system, failures in individual services are not uncommon. This paper describes the Chaos Automation Platform, a system for running failure injection experiments on the production system to verify that failures in non-critical services do not result in system outages. 	
1708.07379v1	http://arxiv.org/pdf/1708.07379v1	2017	Results of the Survey: Failures in Robotics and Intelligent Systems	Johannes Wienke|Sebastian Wrede	  In January 2015 we distributed an online survey about failures in robotics and intelligent systems across robotics researchers. The aim of this survey was to find out which types of failures currently exist, what their origins are, and how systems are monitored and debugged - with a special focus on performance bugs. This report summarizes the findings of the survey. 	
1710.06832v1	http://arxiv.org/pdf/1710.06832v1	2017	The Origins of Computational Mechanics: A Brief Intellectual History and   Several Clarifications	James P. Crutchfield	  The principle goal of computational mechanics is to define pattern and structure so that the organization of complex systems can be detected and quantified. Computational mechanics developed from efforts in the 1970s and early 1980s to identify strange attractors as the mechanism driving weak fluid turbulence via the method of reconstructing attractor geometry from measurement time series and in the mid-1980s to estimate equations of motion directly from complex time series. In providing a mathematical and operational definition of structure it addressed weaknesses of these early approaches to discovering patterns in natural systems.   Since then, computational mechanics has led to a range of results from theoretical physics and nonlinear mathematics to diverse applications---from closed-form analysis of Markov and non-Markov stochastic processes that are ergodic or nonergodic and their measures of information and intrinsic computation to complex materials and deterministic chaos and intelligence in Maxwellian demons to quantum compression of classical processes and the evolution of computation and language.   This brief review clarifies several misunderstandings and addresses concerns recently raised regarding early works in the field (1980s). We show that misguided evaluations of the contributions of computational mechanics are groundless and stem from a lack of familiarity with its basic goals and from a failure to consider its historical context. For all practical purposes, its modern methods and results largely supersede the early works. This not only renders recent criticism moot and shows the solid ground on which computational mechanics stands but, most importantly, shows the significant progress achieved over three decades and points to the many intriguing and outstanding challenges in understanding the computational nature of complex dynamic systems. 	
0306247v4	http://arxiv.org/pdf/cond-mat/0306247v4	2004	Competing Styles of Statistical Mechanics: II. Comparison of Theory and   Experiment and Further Illustrations	Áurea R. Vasconcellos|J. Galvão Ramos|Roberto Luzzi	  In the present follow-up article of a previous one [1] we illustrate the use of the Unconventional Statistical Mechanics described and discussed in the latter. This is done via the analysis, resorting to Renyi approach, of experimental results in the case of so-called "anomalous" luminescence in nanometric quantum wells in semiconductor heterostructures, and the so-called "anomalous" cyclic voltammetry in fractal-like electrodes in microbatteries. Also a purely theoretical analysis is done in the cases of an ideal gas and of radiation comparing the conventional and unconventional approaches. In all of these situations it is discussed which is the failure to satisfy the Criteria of Efficiency and/or Sufficiency thus requiring to resort to the unconventional approach, and what determines the value of the infoentropic index in each case, and its dependence on the system characteristics. Moreover, on the basis of the results we obtain, it is conjectured that the infoentropic index may satisfy what we call a law defining a "path to sufficiency". PACS: 05.70.Ln; 82.20.Mj; 82.20.Db Keywords: Renyi Statistics; Escort Probability; Fractal Structured Systems; Power Law Properties. 	 renyi statistics, escort probability, fractal structured
systems, power law properties 

0403005v2	http://arxiv.org/pdf/quant-ph/0403005v2	2005	Path integrals from classical momentum paths	John Hegseth	  The path integral formulation of quantum mechanics constructs the propagator by evaluating the action S for all classical paths in coordinate space. A corresponding momentum path integral may also be defined through Fourier transforms in the endpoints. Although these momentum path integrals are especially simple for several special cases, no one has, to my knowledge, ever formally constructed them from all classical paths in momentum space. I show that this is possible because there exists another classical mechanics based on an alternate classical action R. Hamilton's Canonical equations result from a variational principle in both S and R. S uses fixed beginning and ending spatial points while R uses fixed beginning and ending momentum points. This alternative action's classical mechanics also includes a Hamilton-Jacobi equation. I also present some important points concerning the beginning and ending conditions on the action necessary to apply a Canonical transformation. These properties explain the failure of the Canonical transformation in the phase space path integral. It follows that a path integral may be constructed from classical position paths using S in the coordinate representation or from classical momentum paths using R in the momentum representation. Several example calculations are presented that illustrate the simplifications and practical advantages made possible by this broader view of the path integral. In particular, the normalized amplitude for a free particle is found without using the Schrodinger equation, the internal spin degree of freedom is simply and naturally derived, and the simple harmonic oscillator is calculated. 	
1102.0113v1	http://arxiv.org/pdf/1102.0113v1	2011	Drug transport mechanism of P-glycoprotein monitored by single molecule   fluorescence resonance energy transfer	Stefan Ernst|Brandy Verhalen|Nawid Zarrabi|Stephan Wilkens|Michael Boersch	  In this work we monitor the catalytic mechanism of P-glycoprotein (Pgp) using single-molecule fluorescence resonance energy transfer (FRET). Pgp, a member of the ATP binding cassette family of transport proteins, is found in the plasma membrane of animal cells where it is involved in the ATP hydrolysis driven export of hydrophobic molecules. When expressed in the plasma membrane of cancer cells, the transport activity of Pgp can lead to the failure of chemotherapy by excluding the mostly hydrophobic drugs from the interior of the cell. Despite ongoing effort, the catalytic mechanism by which Pgp couples MgATP binding and hydrolysis to translocation of drug molecules across the lipid bilayer is poorly understood. Using site directed mutagenesis, we have introduced cysteine residues for fluorescence labeling into different regions of the nucleotide binding domains (NBDs) of Pgp. Double-labeled single Pgp molecules showed fluctuating FRET efficiencies during drug stimulated ATP hydrolysis suggesting that the NBDs undergo significant movements during catalysis. Duty cycle-optimized alternating laser excitation (DCO-ALEX) is applied to minimize FRET artifacts and to select the appropriate molecules. The data show that Pgp is a highly dynamic enzyme that appears to fluctuate between at least two major conformations during steady state turnover. 	
1202.4796v2	http://arxiv.org/pdf/1202.4796v2	2013	Realistic time-scale fully atomistic simulations of surface nucleation   of dislocations in pristine nanopillars	Pratyush Tiwary|Axel van de Walle	  We use our recently proposed accelerated dynamics algorithm (Tiwary & van de Walle, 2011) to calculate temperature and stress dependence of activation free energy for surface nucleation of dislocations in pristine Gold nanopillars under realistic loads. While maintaining fully atomistic resolution, we achieve the fraction of a second time-scale regime. We find that the activation free energy depends significantly on the driving force (stress or strain) and temperature, leading to very high activation entropies. We also perform compression tests on Gold nanopillars for strain rates varying between 7 orders of magnitudes, reaching as low as 10^3/s. Our calculations show the quantitative effects on the yield point of unrealistic strain-rate Molecular Dynamics calculations: we find that while the failure mechanism for <001> compression of Gold nanopillars remains the same across the entire strain-rate range, the elastic limit (defined as stress for nucleation of the first dislocation) depends significantly on the strain-rate. We also propose a new methodology that overcomes some of the limits in our original accelerated dynamics scheme (and accelerated dynamics methods in general). We lay out our methods in sufficient details so as to be used for understanding and predicting deformation mechanism under realistic driving forces for various problems. 	
1207.1064v2	http://arxiv.org/pdf/1207.1064v2	2012	Fluctuations and differential contraction during regeneration of Hydra   vulgaris tissue toroids	Michael Krahe|Iris Wenzel|Kao-Nung Lin|Julia Fischer|Joseph Goldmann|Markus Kästner|Claus Fütterer	  We studied regenerating bilayered tissue toroids dissected from Hydra vulgaris polyps and relate our macroscopic observations to the dynamics of force-generating mesoscopic cytoskeletal structures. Tissue fragments undergo a specific toroid-spheroid folding process leading to complete regeneration towards a new organism. The time scale of folding is too fast for biochemical signalling or morphogenetic gradients which forced us to assume purely mechanical self-organization. The initial pattern selection dynamics was studied by embedding toroids into hydro-gels allowing us to observe the deformation modes over longer periods of time. We found increasing mechanical fluctuations which break the toroidal symmetry and discuss the evolution of their power spectra for various gel stiffnesses. Our observations are related to single cell studies which explain the mechanical feasibility of the folding process. In addition, we observed switching of cells from a tissue bound to a migrating state after folding failure as well as in tissue injury.   We found a supra-cellular actin ring assembled along the toroid's inner edge. Its contraction can lead to the observed folding dynamics as we could confirm by finite element simulations. This actin ring in the inner cell layer is assembled by myosin- driven length fluctuations of supra-cellular {\alpha}-actin structures (myonemes) in the outer cell-layer. 	
1312.7835v1	http://arxiv.org/pdf/1312.7835v1	2013	Can we advance macroscopic quantum systems outside the framework of   complex decoherence theory?	Mark E. Brezinski|Maria Rupnick	  Macroscopic quantum systems (MQS) are macroscopic systems driven by quantum rather than classical mechanics, a long studied area with minimal success till recently. Harnessing the benefits of quantum mechanics on a macroscopic level would revolutionize fields ranging from telecommunication to biology, the latter focused on here for reasons discussed. Contrary to misconceptions, there are no known physical laws that prevent the development of MQS. Instead, they are generally believed universally lost in complex systems from environmental entanglements (decoherence). But we argue success is achievable MQS with decoherence compensation developed, naturally or artificially, from top-down rather current reductionist approaches. This paper advances the MQS field by a complex systems approach to decoherence. First, why complex system decoherence approaches (top-down) are needed is discussed. Specifically, complex adaptive systems (CAS) are not amenable to reductionist models (and their master equations) because of emergent behavior, approximation failures, not accounting for quantum compensator mechanisms, ignoring path integrals, and the subentity problem. In addition, since MQS must exist within the context of the classical world, rapid decoherence and prolonged coherence are both needed. Nature has already demonstrated this for quantum subsystems such as photosynthesis and magnetoreception. Second, we perform a preliminary study that illustrates a top-down approach to potential MQS. In summary, reductionist arguments against MQS are not justifiable. It is more likely they are not easily detectable in large intact classical systems or has been destroyed by reductionist experimental set-ups. This complex systems decoherence approach, using top down investigations, is critical to paradigm shifts in MQS research both in biological and non-biological systems. 	
1503.04422v1	http://arxiv.org/pdf/1503.04422v1	2015	Making Availability as a Service in the Clouds	Pengfei Chen|Yong Qi|Peipei Wang|Li Su|Xinyi Li	  Cloud computing has achieved great success in modern IT industry as an excellent computing paradigm due to its flexible management and elastic resource sharing. To date, cloud computing takes an irrepalceable position in our socioeconomic system and influences almost every aspect of our daily life. However, it is still in its infancy, many problems still exist.Besides the hotly-debated security problem, availability is also an urgent issue.With the limited power of availability mechanisms provided in present cloud platform, we can hardly get detailed availability information of current applications such as the root causes of availability problem,mean time to failure, etc. Thus a new mechanism based on deep avaliability analysis is neccessary and benificial.Following the prevalent terminology 'XaaS',this paper proposes a new win-win concept for cloud users and providers in term of 'Availability as a Service' (abbreviated as 'AaaS').The aim of 'AaaS' is to provide comprehensive and aimspecific runtime avaliabilty analysis services for cloud users by integrating plent of data-driven and modeldriven approaches. To illustrate this concept, we realize a prototype named 'EagleEye' with all features of 'AaaS'. By subscribing corresponding services in 'EagleEye', cloud users could get specific availability information of their applications deployed in cloud platform. We envision this new kind of service will be merged into the cloud management mechanism in the near future. 	
1509.07801v3	http://arxiv.org/pdf/1509.07801v3	2016	Analysis of the Behavior of Ultra High Performance Concrete at Early Age	Lin Wan|Roman Wendner|Benliang Liang|Gianluca Cusatis	  Ultra high performance concretes (UHPCs) are cementitious composite materials with high level of perfor- mance characterized by high compressive strength, high tensile strength and superior durability, reached by low water-to-binder ratio, optimized aggregate size distribution, thermal activation, and fiber reinforcement. In the past couple of decades, more and more UHPCs have been developed and found their ways into practice. Thus, the demand for computational models capable of describing and predicting relevant aging phenomena to assist design and planning is increasing. This paper presents the early age experimental characterization as well as the results of subsequent simulations of a typical UHPC matrix. Performed and simulated tests include unconfined compression, splitting (Brazilian), and three-point-bending tests. The computational framework is formulated by coupling a hygro-thermo-chemical (HTC) theory and a comprehensive mesoscale discrete model with formulated aging functions. The HTC component allows taking into account various types of curing conditions with varying temperature and relative humidity and predicting the level of concrete aging. The mechanical component, the Lattice Discrete Particle Model (LDPM), permits the simulation of the failure behavior of concrete at the length scale of major heterogeneities. The aging functions relate the mesoscale LDPM mechanical properties in terms of aging degree, defined in this work as the ratio between the quasi-static elastic modulus at a certain age and its asymptotic value. The obtained results provide insights in both UHPC early age mechanisms and a computational model for the analysis of aging UHPC structures. 	
1703.07864v1	http://arxiv.org/pdf/1703.07864v1	2017	Quantifying the structural integrity of nanorod arrays	Florian Thöle|Longjian Xue|Claudia Heß|Reinald Hillebrand|Stanislav N. Gorb|Martin Steinhart	  Arrays of aligned nanorods oriented perpendicular to a support, which are accessible by top-down lithography or by means of shape-defining hard templates, have received increasing interest as sensor components, components for nanophotonics and nanoelectronics, substrates for tissue engineering, sur-faces having specific adhesive or antiadhesive properties and as surfaces with customized wettability. Agglomeration of the nanorods deteriorates the performance of components based on nanorod arrays. A comprehensive body of literature deals with mechanical failure mechanisms of nanorods and design criteria for mechanically stable nanorod arrays. However, the structural integrity of nanorod arrays is commonly evaluated only visually and qualitatively. We use real-space analysis of microscopic images to quantify the fraction of condensed nanorods in nanorod arrays. We suggest the number of array elements apparent in the micrographs divided by the number of array elements a defect-free array would contain in the same area, referred to as integrity fraction, as a measure of structural array integrity. Reproducible procedures to determine the imaged number of array elements are introduced. Thus, quantitative comparisons of different nanorod arrays, or of one nanorod array at different stages of its use, are possible. Structural integrities of identical nanorod arrays differing only in the length of the nanorods are exemplarily analyzed. 	
1712.02320v1	http://arxiv.org/pdf/1712.02320v1	2017	Elastic, strength, and fracture properties of Marcellus shale	Zhefei Jin|Weixin Li|Congrui Jin|James Hambleton|Congrui Jin|Gianluca Cusatis	  Shale, a fine-grained sedimentary rock, is the key source rock for many of the world's most important oil and natural gas deposits. A deep understanding of the mechanical properties of shale is of vital importance in various geotechnical applications, including oil and gas exploitation. In this work, deformability, strength, and fracturing properties of Marcellus shale were investigated through an experimental study. Firstly, uniaxial compression, direct tension, and Brazilian tests were performed on the Marcellus shale specimens in various bedding plane orientations with respect to loading directions to measure the static mechanical properties and their anisotropy. Furthermore, the deformability of Marcellus shale was also studied through seismic velocity measurements for comparison with the static measurements. The experimental results revealed that the transversely isotropic model is applicable for describing the elastic behaviors of Marcellus shale in pure tension and compression. The elastic properties measured from these two experiments, however, were not exactly the same. Strength results showed that differences exist between splitting (Brazilian) and direct tensile strengths, both of which varied with bedding plane orientations and loading directions and were associated with different failure modes. Finally, a series of three-point-bending tests were conducted on specimens of increasing size in three different principal notch orientations to investigate the fracture properties of the material. It was found that there exists a significant size effect on the fracture properties calculated from the measured peak loads and by using the Linear Elastic Fracture Mechanics (LEFM) theory. The fracture properties can be uniquely identified, however, by using Bazant's Size Effect Law and they were found to be anisotropic. 	
0310222v1	http://arxiv.org/pdf/astro-ph/0310222v1	2003	Cooling Flows or Heating Flows?	James Binney	  It is now clear that AGN heat cooling flows, largely by driving winds. The winds may contain a relativistic component that generates powerful synchrotron radiation, but it is not clear that all winds do so. The spatial and temporal stability of the AGN/cooling flow interaction are discussed. Collimation of the winds probably provides spatial stability. Temporal stability may be possible only for black holes with masses above a critical value. Both the failure of cooling flows to have adiabatic cores and the existence of X-ray cavities confirm the importance of collimated outflows. I quantify the scale of the convective flow that the AGN Hydra would need to drive if it balanced radiative inward flow by outward flow parallel to the jets. At least in Virgo any such flow must be confined to r<~20 kpc. Hydrodynamical simulations suggest that AGN outbursts cannot last longer than ~25 Myr. Data for four clusters with well studied X-ray cavities suggests that heating associated with cavity formation approximately balances radiative cooling. The role of cosmic infall and the mechanism of filament formation are briefly touched on. 	
9707013v1	http://arxiv.org/pdf/cond-mat/9707013v1	1997	Multicanonical Methods vs. Molecular Dynamics vs. Monte Carlo:   Comparison for Lennard-Jones Glasses	Kamal K. Bhattacharya|James P. Sethna	  We applied a multicanonical algorithm (entropic sampling) to a two-dimensional and a three-dimensional Lennard-Jones system with quasicrystalline and glassy ground states. Focusing on the ability of the algorithm to locate low lying energy states, we compared the results of the multicanonical simulations with standard Monte Carlo simulated annealing and molecular dynamics methods. We find slight benefits to using entropic sampling in small systems (less than 80 particles), which disappear with larger systems. This is disappointing as the multicanonical methods are designed to surmount energy barriers to relaxation. We analyze this failure theoretically, and show (1) the multicanonical method is reduced in the thermodynamic limit (large systems) to an effective Monte Carlo simulated annealing with a random temperature vs. time, and (2) the multicanonical method gets trapped by unphysical entropy barriers in the same metastable states whose energy barriers trap the traditional quenches. The performance of Monte Carlo and molecular dynamics quenches were remarkably similar. 	
9707114v1	http://arxiv.org/pdf/cond-mat/9707114v1	1997	Scaling with respect to disorder in time-to-failure	D. Sornette|J. V. Andersen	  We revisit a simple dynamical model of rupture in random media with long-range elasticity to test whether rupture can be seen as a first-order or a critical transition. We find a clear scaling of the macroscopic modulus as a function of time-to-rupture and of the amplitude of the disorder, which allows us to collapse neatly the numerical simulations over more than five decades in time and more than one decade in disorder amplitude onto a single master curve. We thus conclude that, at least in this model, dynamical rupture in systems with long-range elasticity is a genuine critical phenomenon occurring as soon as the disorder is non-vanishing. 	
9708220v1	http://arxiv.org/pdf/cond-mat/9708220v1	1997	Fine structure and complex exponents in power law distributions from   random maps	Per Jögi|Didier Sornette|Michael Blank	  Discrete scale invariance (DSI) has recently been documented in time-to-failure rupture, earthquake processes and financial crashes, in the fractal geometry of growth processes and in random systems. The main signature of DSI is the presence of log-periodic oscillations correcting the usual power laws, corresponding to complex exponents. Log-periodic structures are important because they reveal the presence of preferred scaling ratios of the underlying physical processes. Here, we present new evidence of log-periodicity overlaying the leading power law behavior of probability density distributions of affine random maps with parametric noise. The log-periodicity is due to intermittent amplifying multiplicative events. We quantify precisely the progressive smoothing of the log-periodic structures as the randomness increases and find a large robustness. Our results provide useful markers for the search of log-periodicity in numerical and experimental data. 	
9709327v1	http://arxiv.org/pdf/cond-mat/9709327v1	1997	Comment on "Tricritical Behavior in Rupture Induced by Disorder"	Rava da Silveira	  In their letter, Andersen, Sornette, and Leung [Phys. Rev. Lett. 78, 2140 (1997)] describe possible behaviors for rupture in disordered media, based on the mean field-like democratic fiber bundle model. In this model, fibers are pulled with a force which is distributed uniformly. A fiber breaks if the stress on it exceeds a threshold chosen from a probability distribution, and the force is then redistributed over the intact fibers. Andersen et al. claim the existence of a tricritical point, separating a "first-order" regime, characterized by a sudden global failure, from a "second-order" regime, characterized by a divergence in the breaking rate. We show that a first-order transition is an artifact of a (large enough) discontinuity put by hand in the disorder distribution. Thus, in generic physical cases, a first-order regime is not present. This result is obtained from a graphical method, which, unlike Andersen at al.'s analytical solution, enables us to distinguish the various classes of qualitatively different behaviors of the model. 	
9803191v1	http://arxiv.org/pdf/cond-mat/9803191v1	1998	Evidence of discrete scale invariance in DLA and time-to-failure by   canonical averaging	A. Johansen|D. Sornette	  Discrete scale invariance, which corresponds to a partial breaking of the scaling symmetry, is reflected in the existence of a hierarchy of characteristic scales l0, c l0, c^2 l0,... where c is a preferred scaling ratio and l0 a microscopic cut-off. Signatures of discrete scale invariance have recently been found in a variety of systems ranging from rupture, earthquakes, Laplacian growth phenomena, ``animals'' in percolation to financial market crashes. We believe it to be a quite general, albeit subtle phenomenon. Indeed, the practical problem in uncovering an underlying discrete scale invariance is that standard ensemble averaging procedures destroy it as if it was pure noise. This is due to the fact, that while c only depends on the underlying physics, l0 on the contrary is realisation-dependent. Here, we adapt and implement a novel so-called ``canonical'' averaging scheme which re-sets the l0 of different realizations to approximately the same value. The method is based on the determination of a realization-dependent effective critical point obtained from, e.g., a maximum susceptibility criterion. We demonstrate the method on diffusion limited aggregation and a model of rupture. 	
9808140v1	http://arxiv.org/pdf/cond-mat/9808140v1	1998	Evidence for the droplet/scaling picture of spin glasses	M. A. Moore|Hemant Bokil|Barbara Drossel	  We have studied the Parisi overlap distribution for the three dimensional Ising spin glass in the Migdal-Kadanoff approximation. For temperatures T around 0.7Tc and system sizes upto L=32, we found a P(q) as expected for the full Parisi replica symmetry breaking, just as was also observed in recent Monte Carlo simulations on a cubic lattice. However, for lower temperatures our data agree with predictions from the droplet or scaling picture. The failure to see droplet model behaviour in Monte Carlo simulations is due to the fact that all existing simulations have been done at temperatures too close to the transition temperature so that sytem sizes larger than the correlation length have not been achieved. 	
9905329v1	http://arxiv.org/pdf/cond-mat/9905329v1	1999	Multifractality in Human Heartbeat Dynamics	Plamen Ch. Ivanov|Luís A. Nunes Amaral|Ary L. Goldberger|Shlomo Havlin|Michael G. Rosenblum|Zbigniew Struzik|H. Eugene Stanley	  Recent evidence suggests that physiological signals under healthy conditions may have a fractal temporal structure. We investigate the possibility that time series generated by certain physiological control systems may be members of a special class of complex processes, termed multifractal, which require a large number of exponents to characterize their scaling properties. We report on evidence for multifractality in a biological dynamical system --- the healthy human heartbeat. Further, we show that the multifractal character and nonlinear properties of the healthy heart rate are encoded in the Fourier phases. We uncover a loss of multifractality for a life-threatening condition, congestive heart failure. 	
9910211v2	http://arxiv.org/pdf/cond-mat/9910211v2	1999	Dynamics of the 2D two-component plasma near the Kosterlitz-Thouless   transition	Dierk Bormann|Hans Beck|Oliver Gallus|Massimiliano Capezzali	  We study the dynamics of a classical, two-component plasma in two dimensions, in the vicinity of the Kosterlitz-Thouless (KT) transition where the system passes from a dielectric low-temperature phase (consisting of bound pairs) to a conducting phase. We use two ``complementary'' analytical approaches and compare to simulations. The conventional, ``intuitive'' approach is built on the KT picture of independently relaxing, bound pairs. A more formal approach, working with Mori projected dynamic correlation functions, avoids to assume the pair picture from the start. We discuss successes and failures of both approaches, and suggest a way to combine the advantages of both. 	
0003131v1	http://arxiv.org/pdf/cond-mat/0003131v1	2000	Scaling of interfaces in brittle fracture and perfect plasticity	E. T. Seppala|V. I. Raisanen|M. J. Alava	  The roughness properties of two-dimensional fracture surfaces as created by the slow failure of random fuse networks are considered and compared to yield surfaces of perfect plasticity with similar disorder. By studying systems up to a linear size L=350 it is found that in the cases studied the fracture surfaces exhibit self-affine scaling with a roughness exponent close to 2/3, which is asymptotically exactly true for plasticity though finite-size effects are evident for both. The overlap of yield or minimum energy and fracture surfaces with exactly the same disorder configuration is shown to be a decreasing function of the system size and to be of a rather large magnitude for all cases studied. The typical ``overlap cluster'' length between pairs of such interfaces converges to a constant with $L$ increasing. 	
0005284v2	http://arxiv.org/pdf/cond-mat/0005284v2	2000	Scale Invariance in the Nonstationarity of Physiological Signals	Pedro Bernaola-Galvan|Plamen Ch. Ivanov|Luis A. Nunes Amaral|Ary L. Goldberger|H. Eugene Stanley	  We introduce a segmentation algorithm to probe temporal organization of heterogeneities in human heartbeat interval time series. We find that the lengths of segments with different local values of heart rates follow a power-law distribution. This scale-invariant structure is not a simple consequence of the long-range correlations present in the data. We also find that the differences in mean heart rates between consecutive segments display a common functional form, but with different parameters for healthy individuals and for patients with heart failure. This finding may provide information into the way heart rate variability is reduced in cardiac disease. 	
0007300v2	http://arxiv.org/pdf/cond-mat/0007300v2	2000	Network robustness and fragility: Percolation on random graphs	D. S. Callaway|M. E. J. Newman|S. H. Strogatz|D. J. Watts	  Recent work on the internet, social networks, and the power grid has addressed the resilience of these networks to either random or targeted deletion of network nodes. Such deletions include, for example, the failure of internet routers or power transmission lines. Percolation models on random graphs provide a simple representation of this process, but have typically been limited to graphs with Poisson degree distribution at their vertices. Such graphs are quite unlike real world networks, which often possess power-law or other highly skewed degree distributions. In this paper we study percolation on graphs with completely general degree distribution, giving exact solutions for a variety of cases, including site percolation, bond percolation, and models in which occupation probabilities depend on vertex degree. We discuss the application of our theory to the understanding of network resilience. 	
0012007v1	http://arxiv.org/pdf/cond-mat/0012007v1	2000	Dilute Bose gas: short-range particle correlations and ultraviolet   divergence	A. Yu. Cherny|A. A. Shanenko	  The modified Bogoliubov model where the primordial interaction is replaced by the t matrix is reinvestigated. It is shown to provide a negative value of the kinetic energy for a strongly interacting dilute Bose gas, contrary to the original Bogoliubov model. To clear up the origin of this failure, the correct values of the kinetic and interaction energies of a dilute Bose gas are calculated. It is demonstrated that both the problem of the negative kinetic energy and the ultraviolet divergence, dating back to the well-known paper of Lee, Yang and Huang, is connected with an inadequate picture of the short-range boson correlations. These correlations are reconsidered within the thermodynamically consistent model proposed earlier by the present authors. Found results are in absolute agreement with the data of the Monte-Carlo calculations for the hard-sphere Bose gas. 	
0012311v1	http://arxiv.org/pdf/cond-mat/0012311v1	2000	Indivisibility of electron bubbles in helium	Veit Elser	  A recent proposal by Maris[1], that single electron bubbles in helium might fission into separate, particle-like entities, does not properly take into account the failure of the adiabatic approximation when, due to tunneling, there is a long electronic time scale. The point along the fission pathway of a photoexcited p-state bubble, where the adiabatic approximation first breaks down, occurs well before the bubble waist has pinched down forming two cavities. In the connected two-lobed geometry, the p- and s-states are strongly mixed by an antisymmetric vibrational mode, and the excitation decays by the mechanism where one lobe collapses while the other expands into the spherical s-state geometry. The extreme pressure jump in a photoexcited bubble leads to shock formation that may halt the elongation even before adiabaticity is compromised. In this case, the photoexcited bubble decays radiatively from the relaxed p-state geometry.   [1] H.J. Maris, On the fission of elementary particles and the evidence for fractional electrons in liquid helium, J. Low Temp. Phys. 120, 173 (2000) 	
0101378v2	http://arxiv.org/pdf/cond-mat/0101378v2	2001	Nonlinear reactive systems viewed as Boolean dynamical systems	E. Abad|P. Grosfils|G. Nicolis	  We present a stochastic, time-discrete boolean model which mimics the mesoscopic dynamics of the desorption reactions $A+A\to A+S$ and $A+A\to S+S$ in a 1D lattice. In the continuous-time limit, we derive a hierarchy of dynamical equations for the subset of moments involving contiguous lattice sites. The solution of the hierarchy allows to compute the exact dynamics of the mean coverage for both microscopic and coarse-grained initial conditions, which turn out to be different from the mean field predictions. The evolution equations for the mean coverage and the second order moments are shown to be equivalent to those provided by a time-continuous Master equation. The important role of higher order fluctuations is brought out by the failure of a truncation scheme retaining only two-particle fluctuation correlations. 	
0102351v2	http://arxiv.org/pdf/cond-mat/0102351v2	2001	Roughness exponent in the fracture of fibrous materials	I. L. Menezes-Sobrinho	  In this paper, a computational model in (2+1)-dimensions which simulates the rupture process of a fibrous material submitted to a constant force $F$, is analyzed. The roughness exponent $\zeta$ at the boundary that separates two failure regimes, catastrophic and slowly shredding, is evaluated. In the catastrophic (dynamic) regime the initial strain creates a crack which percolates rapidly through the material. In the slowly shredding (quasi-static) regime several cracks of small size appear in all parts of the material, the rupture process is slow and any single crack percolates the sample. At the boundary between these two regimes, we obtained a value $\zeta\simeq 0.42\pm 0.02$ for the roughness exponent, in agreement with results provided by other simulations in three dimension. Also, at this boundary we observed a power law behavior on the number of cracks versus its size. 	
0102397v1	http://arxiv.org/pdf/cond-mat/0102397v1	2001	Simulation of pedestrian dynamics using a 2-dimensional cellular   automaton	C. Burstedde|K. Klauck|A. Schadschneider|J. Zittartz	  We propose a 2-dimensional cellular automaton model to simulate pedestrian traffic. It is a vmax=1 model with exclusion statistics and parallel dynamics. Long-range interactions between the pedestrians are mediated by a so called floor field which modifies the transition rates to neighbouring cells. This field, which can be discrete or continuous, is subject to diffusion and decay. Furthermore it can be modified by the motion of the pedestrians. Therefore the model uses an idea similar to chemotaxis, but with pedestrians following a virtual rather than a chemical trace. Our main goal is to show that the introduction of such a floor field is sufficient to model collective effects and self-organization encountered in pedestrian dynamics, e.g. lane formation in counterflow through a large corridor. As an application we also present simulations of the evacuation of a large room with reduced visibility, e.g. due to failure of lights or smoke. 	
0103232v1	http://arxiv.org/pdf/cond-mat/0103232v1	2001	Creep rupture of viscoelastic fiber bundles	Raul Cruz Hidalgo|Ferenc Kun|Hans. J. Herrmann	  We study the creep rupture of bundles of viscoelastic fibers occurring under uniaxial constant tensile loading. A novel fiber bundle model is introduced which combines the viscoelastic constitutive behaviour and the strain controlled breaking of fibers. Analytical and numerical calculations showed that above a critical external load the deformation of the system monotonically increases in time resulting in global failure at a finite time $t_f$, while below the critical load the deformation tends to a constant value giving rise to an infinite lifetime. Our studies revealed that the nature of the transition between the two regimes, i.e. the behaviour of $t_f$ at the critical load $sigma_c$, strongly depends on the range of load sharing: for global load sharing $t_f$ has a power law divergence at $\sigma_c$ with a universal exponent of 0.5, however, for local load sharing the transition becomes abrupt: at the critical load $t_f$ jumps to a finite value, analogous to second and first order phase transitions, respectively. The acoustic response of the bundle during creep is also studied. 	
0106054v1	http://arxiv.org/pdf/cond-mat/0106054v1	2001	Theory of self-similar oscillatory finite-time singularities in Finance,   Population and Rupture	D. Sornette|K. Ide	  This is a short letter summarizing the long paper cond-mat/0106047 in which we present a simple two-dimensional dynamical system reaching a singularity in finite time decorated by accelerating oscillations due to the interplay between nonlinear positive feedback and reversal in the inertia. This provides a fundamental equation for the dynamics of (1) stock market prices in the presence of nonlinear trend-followers and nonlinear value investors, (2) the world human population with a competition between a population-dependent growth rate and a nonlinear dependence on a finite carrying capacity and (3) the failure of a material subject to a time-varying stress with a competition between positive geometrical feedback on the damage variable and nonlinear healing. The rich fractal scaling properties of the dynamics are traced back to the self-similar spiral structure in phase space unfolding around an unstable spiral point at the origin. 	
0106136v2	http://arxiv.org/pdf/cond-mat/0106136v2	2002	Instability of scale-free networks under node-breaking avalanches	Y. Moreno|J. B. Gómez|A. F. Pacheco	  The instability introduced in a large scale-free network by the triggering of node-breaking avalanches is analyzed using the fiber-bundle model as conceptual framework. We found, by measuring the size of the giant component, the avalanche size distribution and other quantities, the existence of an abrupt transition. This test of strength for complex networks like Internet is more stringent than others recently considered like the random removal of nodes, analyzed within the framework of percolation theory. Finally, we discuss the possible implications of our results and their relevance in forecasting cascading failures in scale-free networks. 	
0106485v3	http://arxiv.org/pdf/cond-mat/0106485v3	2003	The Stable Random Matrix ensembles	M. Tierz	  We address the construction of stable random matrix ensembles as the generalization of the stable random variables (Levy distributions). With a simple method we derive the Cauchy case, which is known to have remarkable properties. These properties allow for such an intuitive method -that relies on taking traces- to hold. Approximate but general results regarding the other distributions are derived as well. Some of the special properties of these ensembles are evidenced by showing partial failure of mean-field approaches. To conclude, we compute the confining potential that gives a Gaussian density of states in the limit of large matrices. The result is an hypergeometric function, in contrast with the simplicity of the Cauchy case. 	
0106569v1	http://arxiv.org/pdf/cond-mat/0106569v1	2001	Density functional theory in the canonical ensemble I General formalism	J. A. Hernando	  Density functional theory stems from the Hohenberg-Kohn-Sham-Mermin (HKSM) theorem in the grand canonical ensemble (GCE). However, as recent work shows, although its extension to the canonical ensemble (CE) is not straightforward, work in nanopore systems could certainly benefit from a mesoscopic DFT in the CE. The stumbling block is the fixed $N$ constraint which is responsible for the failure in proving the interchangeability of density profiles and external potentials as independent variables. Here we prove that, if in the CE the correlation functions are stripped off of their asymptotic behaviour (which is not in the form of a properly irreducible $n$-body function), the HKSM theorem can be extended to the CE. In proving that, we generate a new {\it hierarchy} of $N$-modified distribution and correlation functions which have the same formal structure that the more conventional ones have (but with the proper irreducible $n$-body behaviour) and show that, if they are employed, either a modified external field or the density profiles can indistinctly be used as independent variables. We also write down the $N$-modified free energy functional and prove that the thermodynamic potential is minimized by the equilibrium values of the new hierarchy. 	
0107467v1	http://arxiv.org/pdf/cond-mat/0107467v1	2001	The Heumann-Hotzel model for aging revisited	Nazareno G. F. de Medeiros|Roberto N. Onody	  Since its proposition in 1995, the Heumann-Hotzel model has remained as an obscure model of biological aging. The main arguments used against it were its apparent inability to describe populations with many age intervals and its failure to prevent a population extinction when only deleterious mutations are present. We find that with a simple and minor change in the model these difficulties can be surmounted. Our numerical simulations show a plethora of interesting features: the catastrophic senescence, the Gompertz law and that postponing the reproduction increases the survival probability, as has already been experimentally confirmed for the Drosophila fly. 	
0107476v3	http://arxiv.org/pdf/cond-mat/0107476v3	2003	New Results for the Nonlocal Kardar-Parisi-Zhang Equation	Eytan Katzav	  In this paper various predictions for the scaling exponents of the Nonlocal Kardar-Parisi-Zhang (NKPZ) equation are discussed. I use the Self-Consistent Expansion (SCE), and obtain results that are quite different from result obtained in the past, using Dynamic Renormalization Group analysis (DRG), a Scaling Approach (SA) and a self-consistent Mode Coupling approach (MC). It is shown that the results obtained using SCE recover an exact result for a subfamily of the NKPZ models in one dimension, while all the other methods fail to do so. It is also shown that the SCE result is the only one that is compatible with simple observations on the dependence of the dynamic exponent $z$ in the NKPZ model on the exponent $\rho$ characterizing the decay of the nonlinear interaction. The reasons for the failure of other methods to deal with NKPZ are also discussed. 	
0108258v1	http://arxiv.org/pdf/cond-mat/0108258v1	2001	Large-scale simulation of adhesion dynamics for end-graphed polymers	Scott W. Sides|Gary S. Grest|Mark J. Stevens	  The adhesion between a polymer melt and substrate is studied in the presence of chemically attached chains on the substrate surface. Extensive molecular dynamics simulations have been carried out to study the effect of temperature, tethered chain areal density ($\Sigma$), tethered chain length ($N_{t}$), chain bending energy ($k_{\theta}$) and tensile pull velocity ($v$) on the adhesive failure mechanisms of pullout and/or scission of the tethered chains. We observe a crossover from pure chain pullout to chain scission as $N_{t}$ is increased. Below the glass transition, the value of $N_{t}$ for which this crossover begins approaches the bulk entanglement length $N_{e}$. For the values of $N_{t}$ and $\Sigma$ used here, no crossover to crazing is observed. 	
0109119v1	http://arxiv.org/pdf/cond-mat/0109119v1	2001	A simple model of bank bankruptcies	A. Aleksiejuk|J. A. Holyst	  Interbank deposits (loans and credits) are quite common in banking system all over the world. Such interbank co-operation is profitable for banks but it can also lead to collective financial failures. In this paper we introduce a new model of directed percolation as a simple representation for contagion process and mass bankruptcies in banking systems. Directed connections that are randomly distributed between junctions of bank lattice simulate flows of money in our model. Critical values of a mean density of interbank connections as well as static and dynamic scaling laws for the statistic of avalange bankruptcies are found. Results of computer simulations for the universal profile of bankruptcies spreading are in a qualitative agreement with the third wave of bank suspensions during The Great Depression in the USA. 	
0111586v2	http://arxiv.org/pdf/cond-mat/0111586v2	2001	Self-organized criticality in a model of collective bank bankruptcies	Agata Aleksiejuk|Janusz A. Holyst|Gueorgi Kossinets	  The question we address here is of whether phenomena of collective bankruptcies are related to self-organized criticality. In order to answer it we propose a simple model of banking networks based on the random directed percolation. We study effects of one bank failure on the nucleation of contagion phase in a financial market. We recognize the power law distribution of contagion sizes in 3d- and 4d-networks as an indicator of SOC behavior. The SOC dynamics was not detected in 2d-lattices. The difference between 2d- and 3d- or 4d-systems is explained due to the percolation theory. 	
0202231v1	http://arxiv.org/pdf/cond-mat/0202231v1	2002	Regular binary thermal lattice-gases	Ronald Blaak|David Dubbeldam	  We analyze the power spectrum of a regular binary thermal lattice gas in two dimensions and derive a Landau-Placzek formula, describing the power spectrum in the low-wavelength, low frequency domain, for both the full mixture and a single component in the binary mixture. The theoretical results are compared with simulations performed on this model and show a perfect agreement. The power spectrums are found to be similar in structure as the ones obtained for the continuous theory, in which the central peak is a complicated superposition of entropy and concentration contributions, due to the coupling of the fluctuations in these quantities. Spectra based on the relative difference between both components have in general additional Brillouin peaks as a consequence of the equipartition failure. 	
0207311v1	http://arxiv.org/pdf/cond-mat/0207311v1	2002	A model of large-scale proteome evolution	Ricard V. Sole|Romualdo Pastor-Satorras|Eric Smith|Thomas B. Kepler	  The next step in the understanding of the genome organization, after the determination of complete sequences, involves proteomics. The proteome includes the whole set of protein-protein interactions, and two recent independent studies have shown that its topology displays a number of surprising features shared by other complex networks, both natural and artificial. In order to understand the origins of this topology and its evolutionary implications, we present a simple model of proteome evolution that is able to reproduce many of the observed statistical regularities reported from the analysis of the yeast proteome. Our results suggest that the observed patterns can be explained by a process of gene duplication and diversification that would evolve proteome networks under a selection pressure, favoring robustness against failure of its individual components. 	
0207402v1	http://arxiv.org/pdf/cond-mat/0207402v1	2002	The process of irreversible nucleation in multilayer growth. I. Failure   of the mean-field approach	Paolo Politi|Claudio Castellano	  The formation of stable dimers on top of terraces during epitaxial growth is investigated in detail. In this paper we focus on mean-field theory, the standard approach to study nucleation. Such theory is shown to be unsuitable for the present problem, because it is equivalent to considering adatoms as independent diffusing particles. This leads to an overestimate of the correct nucleation rate by a factor N, which has a direct physical meaning: in average, a visited lattice site is visited N times by a diffusing adatom. The dependence of N on the size of the terrace and on the strength of step-edge barriers is derived from well known results for random walks. The spatial distribution of nucleation events is shown to be different from the mean-field prediction, for the same physical reason. In the following paper we develop an exact treatment of the problem. 	
0210119v1	http://arxiv.org/pdf/cond-mat/0210119v1	2002	Time evolution of damage under variable ranges of load transfer	Oluwole E. Yewande|Yamir Moreno|Ferenc Kun|Raul Cruz Hidalgo|Hans J. Herrmann	  We study the time evolution of damage in a fiber bundle model in which the range of interaction of fibers varies through an adjustable stress transfer function recently introduced. We find that the lifetime of the material exhibits a crossover from mean field to short range behavior as in the static case. Numerical calculations showed that the value at which the transition takes place depends on the system's disorder. Finally, we have performed a microscopic analysis of the failure process. Our results confirm that the growth dynamics of the largest crack is radically different in the two limiting regimes of load transfer during the first stages of breaking. 	
0211331v1	http://arxiv.org/pdf/cond-mat/0211331v1	2002	Universality classes in creep rupture	Ferenc Kun|Yamir Moreno|Raul Cruz Hidalgo|Hans. J. Herrmann	  We study the creep response of solids to a constant external load in the framework of a novel fiber bundle model introduced. Analytical and numerical calculations showed that increasing the external load on a specimen a transition takes place from a partially failed state of infinite lifetime to a state where global failure occurs at a finite time. Two universality classes of creep rupture were identified depending on the range of interaction of fibers: in the mean field limit the transition between the two states is continuous characterized by power law divergences, while for local interactions it becomes abrupt with no scaling. Varying the range of interaction a sharp transition is revealed between the mean field and short range regimes. The creeping system evolves into a macroscopic stationary state accompanied by the emergence of a power law distribution of inter-event times of the microscopic relaxation process, which indicates self organized criticality in creep. 	
0212187v1	http://arxiv.org/pdf/cond-mat/0212187v1	2002	Risk and Utility in Portfolio Optimization	Morrel H. Cohen|Vincent D. Natoli	  Modern portfolio theory(MPT) addresses the problem of determining the optimum allocation of investment resources among a set of candidate assets. In the original mean-variance approach of Markowitz, volatility is taken as a proxy for risk, conflating uncertainty with risk. There have been many subsequent attempts to alleviate that weakness which, typically, combine utility and risk. We present here a modification of MPT based on the inclusion of separate risk and utility criteria. We define risk as the probability of failure to meet a pre-established investment goal. We define utility as the expectation of a utility function with positive and decreasing marginal value as a function of yield. The emphasis throughout is on long investment horizons for which risk-free assets do not exist. Analytic results are presented for a Gaussian probability distribution. Risk-utility relations are explored via empirical stock-price data, and an illustrative portfolio is optimized using the empirical data. 	
0301086v1	http://arxiv.org/pdf/cond-mat/0301086v1	2003	Cascade-based attacks on complex networks	Adilson E. Motter|Ying-Cheng Lai	  We live in a modern world supported by large, complex networks. Examples range from financial markets to communication and transportation systems. In many realistic situations the flow of physical quantities in the network, as characterized by the loads on nodes, is important. We show that for such networks where loads can redistribute among the nodes, intentional attacks can lead to a cascade of overload failures, which can in turn cause the entire or a substantial part of the network to collapse. This is relevant for real-world networks that possess a highly heterogeneous distribution of loads, such as the Internet and power grids. We demonstrate that the heterogeneity of these networks makes them particularly vulnerable to attacks in that a large-scale cascade may be triggered by disabling a single key node. This brings obvious concerns on the security of such systems. 	
0301576v1	http://arxiv.org/pdf/cond-mat/0301576v1	2003	Orientation Dependence of Step Stiffness: Failure of SOS and Ising   Models to Describe Experimental Data	Sabine Dieluweit|Harald Ibach|Margret Giesen|T. L. Einstein	  We have investigated the step stiffness on Cu(001) surfaces as a function of step orientation by two independent methods at several temperatures near 300 K. Both sets of data agree well and show a substantial dependence of the stiffness on the angle of orientation. With the exception of steps oriented along $<110>$, the experimental stiffness is significantly larger than the stiffness calculated within the solid-on-solid (SOS) model and the Ising-model, even if next nearest-neighbor interactions are taken into account. Our results have considerable consequences for the understanding and for the theoretical modeling of equilibrium and growth phenomena, such as step meandering instabilities. 	
0307029v1	http://arxiv.org/pdf/cond-mat/0307029v1	2003	Assessing Interaction Networks with Applications to Catastrophe Dynamics   and Disaster Management	Dirk Helbing|Christian Kuehnert	  In this paper we present a versatile method for the investigation of interaction networks and show how to use it to assess effects of indirect interactions and feedback loops. The method allows to evaluate the impact of optimization measures or failures on the system. Here, we will apply it to the investigation of catastrophes, in particular to the temporal development of disasters (catastrophe dynamics). The mathematical methods are related to the master equation, which allows the application of well-known solution methods. We will also indicate connections of disaster management with excitable media and supply networks. This facilitates to study the effects of measures taken by the emergency management or the local operation units. With a fictious, but more or less realistic example of a spreading epidemic disease or a wave of influenza, we illustrate how this method can, in principle, provide decision support to the emergency management during such a disaster. Similar considerations may help to assess measures to fight the SARS epidemics, although immunization is presently not possible. 	
0308610v1	http://arxiv.org/pdf/cond-mat/0308610v1	2003	Bond valence calculation for several perovskites and the evidences for a   valence charge transfer process in these compounds	Hoang Nam Nhat	  This paper presents the bond valence calculation for several perovskite systems and describes the evidences for a valence charge transfer process in these compounds. The reviewing of the crystal structures of La1-xPbxMnO3 (x=0.1-0.5), La0.6Sr0.4-xTixMnO3 (x=0.0-0.25) and La1-xSrxCoO3 (x=0.1-0.5) is also presented. On the basis of testing samples, the distribution of valence charge has been evaluated which showed the failure of elastic bonding mechanism on all studied systems and revealed the general deficit of valence charge in the unit cell. This deficit was not equally localized on all coordination spheres but proved asymmetrically distributed between the spheres. As the content of substitution increased, the charge deficit declined systematically from balanced level, signifying the continuous transfer of valence charge from the B-O6 to A-O12 spheres. The transfered charge varied from system to system, depending on the valence deviation of spheres and was not small. The total valence deviation reached near 2electron/unit cell in the studied systems. The local deviation may be more larger than this average value. The possible impact of the limitted accuracy of the available structural data on the bond valence results has been considered. 	
0309449v1	http://arxiv.org/pdf/cond-mat/0309449v1	2003	First passage and arrival time densities for Lévy flights and the   failure of the method of images	Aleksei V. Chechkin|Ralf Metzler|Vsevolod Y. Gonchar|Joseph Klafter|Leonid V. Tanatarov	  We discuss the first passage time problem in the semi-infinite interval, for homogeneous stochastic Markov processes with L{\'e}vy stable jump length distributions $\lambda(x)\sim\ell^{\alpha}/|x|^{1+\alpha}$ ($|x|\gg\ell$), namely, L{\'e}vy flights (LFs). In particular, we demonstrate that the method of images leads to a result, which violates a theorem due to Sparre Andersen, according to which an arbitrary continuous and symmetric jump length distribution produces a first passage time density (FPTD) governed by the universal long-time decay $\sim t^{-3/2}$. Conversely, we show that for LFs the direct definition known from Gaussian processes in fact defines the probability density of first arrival, which for LFs differs from the FPTD. Our findings are corroborated by numerical results. 	
0311046v1	http://arxiv.org/pdf/cond-mat/0311046v1	2003	Generic features of the fluctuation dissipation relation in coarsening   systems	Federico Corberi|Claudio Castellano|Eugenio Lippiello|Marco Zannetti	  The integrated response function in phase-ordering systems with scalar, vector, conserved and non conserved order parameter is studied at various space dimensionalities. Assuming scaling of the aging contribution $\chi_{ag} (t,t_w)= t_w ^{-a_\chi} \hat \chi (t/t_w)$ we obtain, by numerical simulations and analytical arguments, the phenomenological formula describing the dimensionality dependence of $a_\chi$ in all cases considered. The primary result is that $a_\chi$ vanishes continuously as $d$ approaches the lower critical dimensionality $d_L$. This implies that i) the existence of a non trivial fluctuation dissipation relation and ii) the failure of the connection between statics and dynamics are generic features of phase ordering at $d_L$. 	
0311284v2	http://arxiv.org/pdf/cond-mat/0311284v2	2004	Percolation and localization in the random fuse model	Phani Kumar V. V. Nukala|Srdan Simunovic|Stefano Zapperi	  We analyze damage nucleation and localization in the random fuse model with strong disorder using numerical simulations. In the initial stages of the fracture process, damage evolves in an uncorrelated manner, resembling percolation. Subsequently, as the damage starts to accumulate, current enhancement at the tips of the microcracks leads eventually to catastrophic failure. We study this behavior quantifying the deviations from percolation and discussing alternative scaling laws for damage. The analysis of damage profiles confirms that localization occurs abruptly starting from an uniform damage landscape. Finally, we show that the cumulative damage distribution follows the normal distribution, suggesting that damage is uncorrelated on large length scales. 	
0401017v1	http://arxiv.org/pdf/cond-mat/0401017v1	2004	A Predator Prey Approach to Diversity Based Defenses in Heterogeneous   Networks	Sean P. Gorman|Rajendra G. Kulkarni|Laurie A. Schintler|Roger R. Stough	  In light of the rise of malicious attacks on the Internet, and the various networks and applications attached to it, new approaches towards modeling predatory activity in networks could be useful. Past research has simulated networks assuming that all vertices are homogenously susceptible to attack or infection. Often times in real world networks only subsets of vertices are susceptible to attack or infection in a heterogeneous population of vertices. One approach to examining a heterogeneous network susceptible to attack is modeling a network as a predator prey landscape. If each type of vulnerable device is considered a heterogeneous species what level of species diversification is needed to keep a malicious attack from a causing a catastrophic failure to the entire network. This paper explores the predator prey analogy for the Internet and presents findings on how different levels of species diversification effects network resilience. The paper will also discuss the connection between diversification, competition, anti-trust, and national security. 	
0401592v1	http://arxiv.org/pdf/cond-mat/0401592v1	2004	Damage Growth in Random Fuse Networks	F. Reurings|M. J. Alava	  The correlations among elements that break in random fuse network fracture are studied, for disorder strong enough to allow for volume damage before final failure. The growth of microfractures is found to be uncorrelated above a lengthscale, that increases as the the final breakdown is approached. Since the fuse network strength decreases with sample size, asymptotically the process resembles more and more mean-field-like (``democratic fiber bundle'') fracture. This is found from the microscopic dynamics of avalanches or microfractures, from a study of damage localization via entropy, and from the final damage profile. In particular, the last one is statistically constant, except exactly at the final crack zone (in contrast to recent results by Hansen et al., Phys. Rev. Lett. 90, 045504 (2003)), in spite of the fact that the fracture surfaces are self-affine. 	
0404226v1	http://arxiv.org/pdf/cond-mat/0404226v1	2004	Network-Induced Oscillatory Behavior in Material Flow Networks	Dirk Helbing|Ulrich Witt|Stefan Laemmer|Thomas Brenner	  Network theory is rapidly changing our understanding of complex systems, but the relevance of topological features for the dynamic behavior of metabolic networks, food webs, production systems, information networks, or cascade failures of power grids remains to be explored. Based on a simple model of supply networks, we offer an interpretation of instabilities and oscillations observed in biological, ecological, economic, and engineering systems. We find that most supply networks display damped oscillations, even when their units - and linear chains of these units - behave in a non-oscillatory way. Moreover, networks of damped oscillators tend to produce growing oscillations. This surprising behavior offers, for example, a new interpretation of business cycles and of oscillating or pulsating processes. The network structure of material flows itself turns out to be a source of instability, and cyclical variations are an inherent feature of decentralized adjustments. 	
0406567v1	http://arxiv.org/pdf/cond-mat/0406567v1	2004	Optimization of Network Robustness to Waves of Targeted and Random   Attack	T. Tanizawa|G. Paul|R. Cohen|S. Havlin|H. E. Stanley	  We study the robustness of complex networks to multiple waves of simultaneous (i) targeted attacks in which the highest degree nodes are removed and (ii) random attacks (or failures) in which fractions $p_t$ and $p_r$ respectively of the nodes are removed until the network collapses. We find that the network design which optimizes network robustness has a bimodal degree distribution, with a fraction $r$ of the nodes having degree $k_2= (\kav - 1 +r)/r$ and the remainder of the nodes having degree $k_1=1$, where $\kav$ is the average degree of all the nodes. We find that the optimal value of $r$ is of the order of $p_t/p_r$ for $p_t/p_r\ll 1$. 	
0407568v1	http://arxiv.org/pdf/cond-mat/0407568v1	2004	Crack roughness and avalanche precursors in the random fuse model	Stefano Zapperi|Phani Kumar V. V. Nukala|Srdan Simunovic	  We analyze the scaling of the crack roughness and of avalanche precursors in the two dimensional random fuse model by numerical simulations, employing large system sizes and extensive sample averaging. We find that the crack roughness exhibits anomalous scaling, as recently observed in experiments. The roughness exponents ($\zeta$, $\zeta_{loc}$) and the global width distributions are found to be universal with respect to the lattice geometry. Failure is preceded by avalanche precursors whose distribution follows a power law up to a cutoff size. While the characteristic avalanche size scales as $s_0 \sim L^D$, with a universal fractal dimension $D$, the distribution exponent $\tau$ differs slightly for triangular and diamond lattices and, in both cases, it is larger than the mean-field (fiber bundle) value $\tau=5/2$. 	
0408064v2	http://arxiv.org/pdf/cond-mat/0408064v2	2004	Pair Contact Process with Diffusion: Failure of Master Equation Field   Theory	Hans-Karl Janssen|Frederic van Wijland|Olivier Deloubriere|Uwe C. Tauber	  We demonstrate that the `microscopic' field theory representation, directly derived from the corresponding master equation, fails to adequately capture the continuous nonequilibrium phase transition of the Pair Contact Process with Diffusion (PCPD). The ensuing renormalization group (RG) flow equations do not allow for a stable fixed point in the parameter region that is accessible by the physical initial conditions. There exists a stable RG fixed point outside this regime, but the resulting scaling exponents, in conjunction with the predicted particle anticorrelations at the critical point, would be in contradiction with the positivity of the equal-time mean-square particle number fluctuations. We conclude that a more coarse-grained effective field theory approach is required to elucidate the critical properties of the PCPD. 	
0410684v2	http://arxiv.org/pdf/cond-mat/0410684v2	2006	Robustness of the avalanche dynamics in data packet transport on   scale-free networks	E. J. Lee|K. -I. Goh|B. Kahng|D. Kim	  We study the avalanche dynamics in the data packet transport on scale-free networks through a simple model. In the model, each vertex is assigned a capacity proportional to the load with a proportionality constant $1+a$. When the system is perturbed by a single vertex removal, the load of each vertex is redistributed, followed by subsequent failures of overloaded vertices. The avalanche size depends on the parameter $a$ as well as which vertex triggers it. We find that there exists a critical value $a_c$ at which the avalanche size distribution follows a power law. The critical exponent associated with it appears to be robust as long as the degree exponent is between 2 and 3, and is close in value to that of the distribution of the diameter changes by single vertex removal. 	
0412038v2	http://arxiv.org/pdf/cond-mat/0412038v2	2007	Stress relaxation in a perfect nanocrystal by coherent ejection of   lattice layers	Abhishek Chaudhuri|Surajit Sengupta|Madan Rao	  We show that a small crystal trapped within a potential well and in contact with its own fluid, responds to large compressive stresses by a novel mechanism -- the transfer of complete lattice layers across the solid-fluid interface. Further, when the solid is impacted by a momentum impulse set up in the fluid, a coherently ejected lattice layer carries away a definite quantity of energy and momentum, resulting in a sharp peak in the calculated phonon absorption spectrum. Apart from its relevance to studies of stability and failure of small sized solids, such coherent nanospallation may be used to make atomic wires or monolayer films. 	
0412395v1	http://arxiv.org/pdf/cond-mat/0412395v1	2004	The failure of the master equation for the reactive systems	Maria K. Koleva	  Two crucial for the breakdown of the master equation arguments are put forward. The first one is related to the violence of a fundamental requirement to the notion of state (thermodynamical) variable, namely: a state variable is defined provided it is insensitive to the particularities of the spatio-temporal configurations upon which the averaging over the dynamical variables proceeds. The second one is related to a ubiquitous divergence of the scattering length in the low-energy limit. In turn, it makes the rates of all the elementary processes divergent as well. Though radically novel viewpoints to the low-energy limit and to the evolution ensure the boundedness of the rates and hold the notion of a state variable available, the master equation remains inappropriate. 	
0501420v2	http://arxiv.org/pdf/cond-mat/0501420v2	2005	Spatial small-world networks: A wiring-cost perspective	Thomas Petermann|Paolo De Los Rios	  Supplementing a lattice with long-range connections effectively models small-world networks characterized by a high local and global interconnectedness observed in systems ranging from society to the brain. If the links have a wiring cost associated to their length l, the corresponding distribution q(l) plays a crucial role. Uniform length distributions have received most attention despite indications that q(l) ~ l^{-\alpha} exist, e.g. for integrated circuits, the Internet and cortical networks. Here we discuss for such systems the emergence of small-world topology, its relationship to the wiring costs, the distribution of flows as well as the robustness with respect to random failures and overload. The main finding is that the choice of such a distribution leads to favorable attributes in most of the investigated properties. 	
0501737v1	http://arxiv.org/pdf/cond-mat/0501737v1	2005	Langevin Simulation of the Chirally Decomposed Sine-Gordon Model	L. Moriconi|M. Moriconi	  A large class of quantum and statistical field theoretical models, encompassing relevant condensed matter and non-abelian gauge systems, are defined in terms of complex actions. As the ordinary Monte-Carlo methods are useless in dealing with these models, alternative computational strategies have been proposed along the years. The Langevin technique, in particular, is known to be frequently plagued with difficulties such as strong numerical instabilities or subtle ergodic behavior. Regarding the chirally decomposed version of the sine-Gordon model as a prototypical case for the failure of the Langevin approach, we devise a truncation prescription in the stochastic differential equations which yields numerical stability and is assumed not to spoil the Berezinskii-Kosterlitz-Thouless transition. This conjecture is supported by a finite size scaling analysis, whereby a massive phase ending at a line of critical points is clearly observed for the truncated stochastic model. 	
0504212v1	http://arxiv.org/pdf/cond-mat/0504212v1	2005	Possibility of Prediction of Avalanches in Power Law Systems	Rumi De|G. Ananthakrishna	  We consider a modified Burridge-Knopoff model with a view to understand results of acoustic emission (AE) relevant to earthquakes by adding a dissipative term which mimics bursts of acoustic signals. Interestingly, we find a precursor effect in the cumulative energy dissipated which allows identification of a large slip event. Further, the AE activity for several large slip events follows a universal stretched exponential behavior with corrections in terms of time-to-failure. We find that many features of the statistics of AE signals such as their amplitudes, durations and the intervals between successive AE bursts obey power laws consistent with recent experimental results. Large magnitude events have different power law from that of the small ones, the latter being sensitive to the pulling speed. 	
0601310v2	http://arxiv.org/pdf/cond-mat/0601310v2	2006	Heat conduction in a confined solid strip: Response to external strain	Debasish Chaudhuri|Abhishek Dhar	  We study heat conduction in a system of hard disks confined to a narrow two dimensional channel. The system is initially in a high density solid-like phase. We study, through nonequilibrium molecular dynamics simulations, the dependence of the heat current on an externally applied elongational strain. The strain leads to deformation and failure of the solid and we find that the changes in internal structure can lead to very sharp changes in the heat current. A simple free-volume type calculation of the heat current in a finite hard-disk system is proposed. This reproduces some qualitative features of the current-strain graph for small strains. 	
0604473v3	http://arxiv.org/pdf/cond-mat/0604473v3	2007	Critical packing in granular shear bands	S. Fazekas|J. Török|J. Kertész	  In a realistic three-dimensional setup, we simulate the slow deformation of idealized granular media composed of spheres undergoing an axisymmetric triaxial shear test. We follow the self-organization of the spontaneous strain localization process leading to a shear band and demonstrate the existence of a critical packing density inside this failure zone. The asymptotic criticality arising from the dynamic equilibrium of dilation and compaction is found to be restricted to the shear band, while the density outside of it keeps the memory of the initial packing. The critical density of the shear band depends on friction (and grain geometry) and in the limit of infinite friction it defines a specific packing state, namely the \emph{dynamic random loose packing}. 	
0611313v2	http://arxiv.org/pdf/cond-mat/0611313v2	2007	Statistical Neurodynamics for sequence processing neural networks with   finite dilution	Pan Zhang|Yong Chen	  We extend the statistical neurodynamics to study transient dynamics of sequence processing neural networks with finite dilution, and the theoretical results is supported by the extensive numerical simulations. It is found that the order parameter equations are completely equivalent to those of the Generating Functional Method, which means that crosstalk noise is normal distribution even in the case of failure in retrieval process. In order to verify the gaussian assumption of crosstalk noise, we numerically obtain the cumulants of crosstalk noise, and third- and fourth-order cumulants are found to be indeed zero even in non-retrieval case. 	
0701707v1	http://arxiv.org/pdf/cond-mat/0701707v1	2007	Relaxation dynamics in strained fiber bundles	Srutarshi Pradhan|Per C. Hemmer	  Under an applied external load the global load-sharing fiber bundle model, with individual fiber strength thresholds sampled randomly from a probability distribution, will relax to an equilibrium state, or to complete bundle breakdown. The relaxation can be viewed as taking place in a sequence of steps. In the first step all fibers weaker than the applied stress fail. As the total load is redistributed on the surviving fibers, a group of secondary fiber failures occur, etc. For a bundle with a finite number of fibers the process stops after a finite number of steps, $t$. By simulation and theoretical estimates, it is determined how $t$ depends upon the stress, the initial load per fiber, both for subcritical and supercritical stress. The two-sided critical divergence is characterized by an exponent -1/2, independent of the probability distribution of the fiber thresholds. 	
0703420v2	http://arxiv.org/pdf/cond-mat/0703420v2	2007	Use and Abuse of a Fractional Fokker-Planck Dynamics for Time-Dependent   Driving	E. Heinsalu|M. Patriarca|I. Goychuk|P. Hänggi	  We investigate a subdiffusive, fractional Fokker-Planck dynamics occurring in time-varying potential landscapes and thereby disclose the failure of the fractional Fokker-Planck equation (FFPE) in its commonly used form when generalized in an {\it ad hoc} manner to time-dependent forces. A modified FFPE (MFFPE) is rigorously derived, being valid for a family of dichotomously alternating force-fields. This MFFPE is numerically validated for a rectangular time-dependent force with zero average bias. For this case subdiffusion is shown to become enhanced as compared to the force free case. We question, however, the existence of any physically valid FFPE for arbitrary varying time-dependent fields that differ from this dichotomous varying family. 	
9604102v1	http://arxiv.org/pdf/cs/9604102v1	1996	Practical Methods for Proving Termination of General Logic Programs	E. Marchiori	  Termination of logic programs with negated body atoms (here called general logic programs) is an important topic. One reason is that many computational mechanisms used to process negated atoms, like Clark's negation as failure and Chan's constructive negation, are based on termination conditions. This paper introduces a methodology for proving termination of general logic programs w.r.t. the Prolog selection rule. The idea is to distinguish parts of the program depending on whether or not their termination depends on the selection rule. To this end, the notions of low-, weakly up-, and up-acceptable program are introduced. We use these notions to develop a methodology for proving termination of general logic programs, and show how interesting problems in non-monotonic reasoning can be formalized and implemented by means of terminating general logic programs. 	
0403042v2	http://arxiv.org/pdf/cs/0403042v2	2004	Protecting Public-Access Sites Against Distributed Denial-of-Service   Attacks	Katerina J. Argyraki|David R. Cheriton	  A distributed denial-of-service (DDoS) attack can flood a victim site with malicious traffic, causing service disruption or even complete failure. Public-access sites like amazon or ebay are particularly vulnerable to such attacks, because they have no way of a priori blocking unauthorized traffic.   We present Active Internet Traffic Filtering (AITF), a mechanism that protects public-access sites from highly distributed attacks by causing undesired traffic to be blocked as close as possible to its sources. We identify filters as a scarce resource and show that AITF protects a significant amount of the victim's bandwidth, while requiring from each participating router a number of filters that can be accommodated by today's routers. AITF is incrementally deployable, because it offers a substantial benefit even to the first sites that deploy it. 	
9703019v1	http://arxiv.org/pdf/dg-ga/9703019v1	1997	Can We Look at The Quantisation Rules as Constraints?	Ennio Gozzi	  In this paper we explore the idea of looking at the Dirac quantisation conditions as $\hbar$-dependent constraints on the tangent bundle to phase-space. Starting from the path-integral version of classical mechanics and using the natural Poisson brackets structure present in the cotangent bundle to the tangent bundle of phase- space, we handle the above constraints using the standard theory of Dirac for constrained systems. The hope is to obtain, as total Hamiltonian, the Moyal operator of time-evolution and as Dirac brackets the Moyal ones. Unfortunately the program fails indicating that something is missing. We put forward at the end some ideas for future work which may overcome this failure. 	
9801003v3	http://arxiv.org/pdf/gr-qc/9801003v3	1998	Nonholonomic Mapping Principle for Classical Mechanics in Spaces with   Curvature and Torsion. New Covariant Conservation Law for Energy-Momentum   Tensor	Hagen Kleinert	  The lecture explains the geometric basis for the recently-discovered nonholonomic mapping principle which specifies certain laws of nature in spacetimes with curvature and torsion from those in flat spacetime, thus replacing and extending Einstein's equivalence principle. An important consequence is a new action principle for determining the equation of motion of a free spinless point particle in such spacetimes. Surprisingly, this equation contains a torsion force, although the action involves only the metric. This force changes geodesic into autoparallel trajectories, which are a direct manifestation of inertia. The geometric origin of the torsion force is a closure failure of parallelograms. The torsion force changes the covariant conservation law of the energy-momentum tensor whose new form is derived. 	
0203029v1	http://arxiv.org/pdf/gr-qc/0203029v1	2002	Nonholonomic Mapping Principle for Classical and Quantum Mechanics in   Spaces with Curvature and Torsion	Hagen Kleinert	  I explain the geometric basis for the recently-discovered nonholonomic mapping principle which permits deriving laws of nature in spacetimes with curvature and torsion from those in flat spacetime, thus replacing and extending Einstein's equivalence principle. As an important consequence, it yields a new action principle for determining the equation of motion of a free spinless point particle in such spacetimes. Surprisingly, this equation contains a torsion force, although the action involves only the metric. This force makes trajectories autoparallel rather than geodesic. Its geometric origin is the closure failure of parallelograms in the presence of torsion, A simple generalization of the mapping principle transforms path integrals from flat spacetimes to those with curvature and torsion, thus playing the role of a quantum equivalence principle, applicable at present only to spaces with gradient torsion. 	
0410036v1	http://arxiv.org/pdf/hep-lat/0410036v1	2004	Failure of Mean Field Theory at Large N	Shailesh Chandrasekharan|Costas G. Strouthos	  We study strongly coupled lattice QCD with $N$ colors of staggered fermions in 3+1 dimensions. While mean field theory describes the low temperature behavior of this theory at large $N$, it fails in the scaling region close to the finite temperature second order chiral phase transition. The universal critical region close to the phase transition belongs to the 3d XY universality class even when $N$ becomes large. This is in contrast to Gross-Neveu models where the critical region shrinks as $N$ (the number of flavors) increases and mean field theory is expected to describe the phase transition exactly in the limit of infinite $N$. Our work demonstrates that close to second order phase transitions infrared fluctuations can sometimes be important even when $N$ is strictly infinite. 	
9607230v1	http://arxiv.org/pdf/hep-ph/9607230v1	1996	End-Point Behavior of Exclusive Processes: The Twilight Regime of   Perturbative QCD	N. G. Stefanis	  A selected set of topics along the borderline between perturbative and nonperturbative QCD in exclusive reactions are studied. Specific problems, related to different mechanisms of momentum transfer to an intact hadron, are discussed. Calculations of the space-like form factors of the pion and the nucleon are reviewed within a convolution scheme of short-distance (hard) and large-distance (soft) contributions which takes into account soft gluon emission and the intrinsic transverse hadron size. The failure of this scheme to reproduce the existing experimental data signals sizeable higher-order perturbative corrections (a K-factor of order two) and/or higher-twist contributions. 	
9702385v1	http://arxiv.org/pdf/hep-ph/9702385v1	1997	Charmonium Production at ELFE Energies	Paul Hoyer	  I discuss issues related to charmonium production, in view of physics possibilities at a 15 ... 30 GeV continuous beam electron facility. High energy photo- and hadroproduction of heavy quarkonia presents several challenges to QCD models concerning cross sections, polarization and nuclear target dependence. Theoretical approaches based on color evaporation as well as on color singlet and color octet mechanisms have met with both successes and failures, indicating that charmonium production is a sensitive probe of color dynamics. Experiments close to charm kinematic threshold will be sensitive also to target substructure since only unusual, compact target configurations contribute. In particular, subthreshold production on nuclei should identify nuclear hot spots of high energy density. At low energies, charmonium will form inside the target nucleus, allowing a determination of c cbar bound state interactions in nuclear matter. 	
9708314v1	http://arxiv.org/pdf/hep-ph/9708314v1	1997	Massiveness of Glueballs as Origin of the OZI Rule	Wei-Shu Hou|Cheng-Ying Ko	  The heaviness of the glueball mass scale is suggested as the source of the OZI rule at low energy. The $J/\psi \to \rho\pi$ decay "anomaly" implies the vector glueball $O$ has mass $m_O \approx m_{J/\psi}$. Such a heavy mass is supported by other glueball studies. Glueball-meson matrix elements turn out to be not suppressed at all at the 1 GeV scale, and a simple and intuitive picture emerges which is consistent with the Gell-Mann-Okubo mass formula as well as the measured sign of $\phi$-$\omega$ mixing. The suppression of glueball mediated $\bar q_iq_i \longleftrightarrow \bar q_jq_j$ transitions and the cancellation mechanism in two-step meson rescatterings are viewed as related by duality. Extensions to the $2^{++}$, $3^{--}$ meson sectors, and failure for $0^{\pm +}$ mesons are also discussed. 	
9806424v2	http://arxiv.org/pdf/hep-ph/9806424v2	1998	Quarkonium Production through Hard Comover Scattering	Paul Hoyer|Stephane Peigne	  We propose a qualitatively new mechanism for quarkonium production, motivated by the global features of the experimental data and by the successes/failures of existing models. In QCD, heavy quarks are created in conjunction with a bremsstrahlung color field emitted by the colliding partons. We study the effects of perturbative gluon exchange between the quark pair and a comoving color field. Such scattering can flip the spin and color of the quarks to create a non-vanishing overlap with the wave function of physical quarkonium. Several observed features that are difficult to understand in current models find simple explanations. Transverse gluon exchange produces unpolarized J/psi's, the chi_c1 and chi_c2 states are produced at similar rates, and the anomalous dependence of the J/psi cross section on the nuclear target size can be qualitatively understood. 	
9908382v2	http://arxiv.org/pdf/hep-ph/9908382v2	1999	Probing Supersymmetric Flavor Models with $ε'/ε$	G. Eyal|A. Masiero|Y. Nir|L. Silvestrini	  We discuss the supersymmetric contribution to $\epsilon'/\epsilon$ in various supersymmetric flavor models. We find that in alignment models the supersymmetric contribution could be significant while in heavy squark models it is expected to be small. The situation is particularly interesting in models that solve the flavor problems by either of the above mechanisms and the remaining CP problems by means of approximate CP, that is, all CP violating phases are small. In such models, the standard model contributions cannot account for $\epsilon'/\epsilon$ and a failure of the supersymmetric contributions to do so would exclude the model. In models of alignment and approximate CP, the supersymmetric contributions can account for $\epsilon'/\epsilon$ only if both the supersymmetric model parameters and the hadronic parameters assume rather extreme values. Such models are then strongly disfavored by the $\epsilon'/\epsilon$ measurements. Models of heavy squarks and approximate CP are excluded. 	
9909332v1	http://arxiv.org/pdf/hep-ph/9909332v1	1999	Coherence of neutrino flavor mixing in quantum field theory	Christian Y. Cardall	  In the simplistic quantum mechanical picture of flavor mixing, conditions on the maximum size and minimum coherence time of the source and detector regions for the observation of interference---as well as the very viability of the approach---can only be argued in an ad hoc way from principles external to the formalism itself. To examine these conditions in a more fundamental way, the quantum field theoretical $S$-matrix approach is employed in this paper, without the unrealistic assumption of microscopic stationarity. The fully normalized, time-dependent neutrino flavor mixing event rates presented here automatically reveal the coherence conditions in a natural, self-contained, and physically unambiguous way, while quantitatively describing the transition to their failure. 	
0004227v4	http://arxiv.org/pdf/hep-ph/0004227v4	2003	Neutrino Oscillations v.s. Leptogenesis in SO(10) Models	Emmanuel Nezri|Jean Orloff	  We study the link between neutrino oscillations and leptogenesis in the minimal framework assuming an SO(10) see-saw mechanism with 3 families. Dirac neutrino masses being fixed, the solar and atmospheric data then generically induce a large mass-hierarchy and a small mixing between the lightest right-handed neutrinos, which fails to produce sufficient lepton asymmetry by 5 orders of magnitudes at least. This failure can be attenuated for a very specific value of the mixing sin^2(2\theta_{e3})=0.1, which interestingly lies at the boundary of the CHOOZ exclusion region, but will be accessible to future long baseline experiments. 	
0012336v1	http://arxiv.org/pdf/hep-ph/0012336v1	2000	A Few Aspects of Heavy Quark Expansion	Nikolai Uraltsev	  Two topics in heavy quark expansion are discussed. The heavy quark potential in perturbation theory is reviewed in connection to the problem of the heavy quark mass. The nontrivial reason behind the failure of the "potential subtracted" mass in higher orders is elucidated. The heavy quark sum rules are the second subject. The physics behind the new exact sum rules is described and a simple quantum mechanical derivation is given. The question of saturation of sum rules is discussed. A comment on the nonstandard possibility which would affect analysis of BR_sl(B) vs. n_c is made. 	
0505222v1	http://arxiv.org/pdf/hep-ph/0505222v1	2005	Split Fermions Baryogenesis from the Kobayashi-Maskawa Phase	Gilad Perez|Tomer Volansky	  A new scenario of baryogenesis is presented, within the split fermions framework. Our model employs a first order phase transition of the localizer field. The standard model (SM), Kobayashi-Maskawa phase induces a sizable CP asymmetry. The usual suppression of CP violation which arises in the SM baryogenesis is absent due to the existence of order one Yukawa couplings before the fermions are localized in the extra dimension. Models of the above type naturally contain B-L violating operators, allowed by the SM symmetries, which induce the baryon asymmetry. Our mechanism demonstrates the following concept: the flavor puzzle and the SM failure to create the baryon asymmetry are linked and may have a common resolution which does not rely on introduction of new CP violating sources. 	
9808151v1	http://arxiv.org/pdf/hep-th/9808151v1	1998	BRST Inner Product Spaces and the Gribov Obstruction	Norbert Duechting|Sergei V. Shabanov|Thomas Strobl	  A global extension of the Batalin-Marnelius proposal for a BRST inner product to gauge theories with topologically nontrivial gauge orbits is discussed. It is shown that their (appropriately adapted) method is applicable to a large class of mechanical models with a semisimple gauge group in the adjoint and fundamental representation. This includes cases where the Faddeev-Popov method fails. Simple models are found also, however, which do not allow for a well-defined global extension of the Batalin-Marnelius inner product due to a Gribov obstruction. Reasons for the partial success and failure are worked out and possible ways to circumvent the problem are briefly discussed. 	
0103197v2	http://arxiv.org/pdf/hep-th/0103197v2	2001	Modular transformation and boundary states in logarithmic conformal   field theory	Shinsuke Kawai|John F. Wheater	  We study the $c=-2$ model of logarithmic conformal field theory in the presence of a boundary using symplectic fermions. We find boundary states with consistent modular properties. A peculiar feature of this model is that the vacuum representation corresponding to the identity operator is a sub-representation of a ``reducible but indecomposable'' larger representation. This leads to unusual properties, such as the failure of the Verlinde formula. Despite such complexities in the structure of modules, our results suggest that logarithmic conformal field theories admit bona fide boundary states. 	
0302197v2	http://arxiv.org/pdf/hep-th/0302197v2	2003	SO(2,1) conformal anomaly: Beyond contact interactions	Gino N. J. Ananos|Horacio E. Camblong|Carlos R. Ordonez	  The existence of anomalous symmetry-breaking solutions of the SO(2,1) commutator algebra is explicitly extended beyond the case of scale-invariant contact interactions. In particular, the failure of the conservation laws of the dilation and special conformal charges is displayed for the two-dimensional inverse square potential. As a consequence, this anomaly appears to be a generic feature of conformal quantum mechanics and not merely an artifact of contact interactions. Moreover, a renormalization procedure traces the emergence of this conformal anomaly to the ultraviolet sector of the theory, within which lies the apparent singularity. 	
0303166v2	http://arxiv.org/pdf/hep-th/0303166v2	2003	Anomaly in conformal quantum mechanics: From molecular physics to black   holes	Horacio E. Camblong|Carlos R. Ordonez	  A number of physical systems exhibit a particular form of asymptotic conformal invariance: within a particular range of distances, they are characterized by a long-range conformal interaction (inverse square potential), the absence of dimensional scales, and an SO(2,1) symmetry algebra. Examples from molecular physics to black holes are provided and discussed within a unified treatment. When such systems are physically realized in the appropriate strong-coupling regime,the occurrence of quantum symmetry breaking is possible. This anomaly is revealed by the failure of the symmetry generators to close the algebra in a manner shown to be independent of the renormalization procedure. 	
0512023v1	http://arxiv.org/pdf/hep-th/0512023v1	2005	Infrared properties of boundaries in 1-d quantum systems	Daniel Friedan|Anatoly Konechny	  We present some partial results on the general infrared behavior of bulk-critical 1-d quantum systems with boundary. We investigate whether the boundary entropy, s(T), is always bounded below as the temperature T decreases towards 0, and whether the boundary always becomes critical in the IR limit. We show that failure of these properties is equivalent to certain seemingly pathological behaviors far from the boundary. One of our approaches uses real time methods, in which locality at the boundary is expressed by analyticity in the frequency. As a preliminary, we use real time methods to prove again that the boundary beta-function is the gradient of the boundary entropy, which implies that s(T) decreases with T. The metric on the space of boundary couplings is interpreted as the renormalized susceptibility matrix of the boundary, made finite by a natural subtraction. 	
0311225v1	http://arxiv.org/pdf/math/0311225v1	2003	Compactness in the d-bar Neumann problem, magnetic Schrodinger   operators, and the Aharonov-Bohm effect	Michael Christ|Siqi Fu	  Compactness of the d-bar Neumann operator is studied for weakly pseudoconvex bounded Hartogs domains in two dimensions. A nonsmooth example is constructed in which condition (P) fails to hold, yet the Neumann operator is compact. The main result, in contrast, is that for smoothly bounded Hartogs domains, condition (P) of Catlin and Sibony is equivalent to compactness.   The analyses of both compactness and condition (P) boil down to properties of the lowest eigenvalues of certain sequences of Schrodinger operators, with and without magnetic fields, parametrized by a Fourier variable resulting from the Hartogs symmetry. The nonsmooth counterexample is based on the Aharonov-Bohm phenomenon of quantum mechanics. For smooth domains, we prove that there always exists an exceptional sequence of Fourier variables for which the Aharonov-Bohm effect is quite weak. This sequence can be quite sparse, so that the failure of compactness is due to a rather subtle effect. 	
0011006v1	http://arxiv.org/pdf/nlin/0011006v1	2000	Spatiotemporal Patterns in Arrays of Coupled Nonlinear Oscillators	M. Lakshmanan|P. Muruganandam	  Nonlinear reaction-diffusion systems admit a wide variety of spatiotemporal patterns or structures. In this lecture, we point out that there is certain advantage in studying discrete arrays, namely cellular neural/nonlinear networks (CNNs), over continuous systems. Then, to illustrate these ideas, the dynamics of diffusively coupled one and two dimensional cellular nonlinear networks (CNNs), involving Murali-Lakshmanan-Chua circuit as the basic element, is considered. Propagation failure in the case of uniform diffusion and propagation blocking in the case of defects are pointed out. The mechanism behind these phenomena in terms of loss of stability is explained. Various spatiotemporal patterns arising from diffusion driven instability such as hexagons, rhombous and rolls are considered when external forces are absent. Existence of penta-hepta defects and removal of them due to external forcing is discussed. The transition from hexagonal to roll structure and breathing oscillations in the presence of external forcing is also demonstrated. Further spatiotemporal chaos, synchronization and size instability in the coupled chaotic systems are elucidated. 	
0512248v2	http://arxiv.org/pdf/physics/0512248v2	2006	A simple mechanism for controlling vortex breakdown in a closed flow	C. Cabeza|Gustavo Sarasua|Arturo C. Marti|Italo Bove	  This work is focused to study the development and control of the laminar vortex breakdown of a flow enclosed in a cylinder. We show that vortex breakdown can be controlled by the introduction of a small fixed rod in the axis of the cylinder. Our method to control the onset of vortex breakdown is simpler than those previously proposed, since it does not require any auxiliary device system. The effect of the fixed rods may be understood using a simple model based on the failure of the quasi-cylindrical approximation. We report experimental results of the critical Reynolds number for the appearance of vortex breakdown for different radius of the fixed rods and different aspect ratios of the system. Good agreement is found between the theoretical and experimental results. 	
0701290v1	http://arxiv.org/pdf/physics/0701290v1	2007	The rich-club phenomenon across complex network hierarchies	Julian J. McAuley|Luciano da Fontoura Costa|Tiberio S. Caetano	  The so-called rich-club phenomenon in a complex network is characterized when nodes of higher degree (hubs) are better connected among themselves than are nodes with smaller degree. The presence of the rich-club phenomenon may be an indicator of several interesting high-level network properties, such as tolerance to hub failures. Here we investigate the existence of the rich-club phenomenon across the hierarchical degrees of a number of real-world networks. Our simulations reveal that the phenomenon may appear in some hierarchies but not in others and, moreover, that it may appear and disappear as we move across hierarchies. This reveals the interesting possibility of non-monotonic behavior of the phenomenon; the possible implications of our findings are discussed. 	
0401016v1	http://arxiv.org/pdf/q-bio/0401016v1	2004	Extinction times for birth-death processes: exact results, continuum   asymptotics, and the failure of the Fokker-Planck approximation	Charles R. Doering|Khachik V. Sargsyan|Leonard M. Sander	  We consider extinction times for a class of birth-death processes commonly found in applications, where there is a control parameter which determines whether the population quickly becomes extinct, or rather persists for a long time. We give an exact expression for the discrete case and its asymptotic expansion for large values of the population. We have results below the threshold, at the threshold, and above the threshold (where there is a quasi-stationary state and the extinction time is very long.) We show that the Fokker-Planck approximation is valid only quite near the threshold. We compare our analytical results to numerical simulations for the SIS epidemic model, which is in the class that we treat. This is an interesting example of the delicate relationship between discrete and continuum treatments of the same problem. 	
0403015v1	http://arxiv.org/pdf/q-bio/0403015v1	2004	Edge vulnerability in neural and metabolic networks	Marcus Kaiser|Claus C. Hilgetag	  Biological networks, such as cellular metabolic pathways or networks of corticocortical connections in the brain, are intricately organized, yet remarkably robust toward structural damage. Whereas many studies have investigated specific aspects of robustness, such as molecular mechanisms of repair, this article focuses more generally on how local structural features in networks may give rise to their global stability. In many networks the failure of single connections may be more likely than the extinction of entire nodes, yet no analysis of edge importance (edge vulnerability) has been provided so far for biological networks. We tested several measures for identifying vulnerable edges and compared their prediction performance in biological and artificial networks. Among the tested measures, edge frequency in all shortest paths of a network yielded a particularly high correlation with vulnerability, and identified inter-cluster connections in biological but not in random and scale-free benchmark networks. We discuss different local and global network patterns and the edge vulnerability resulting from them. 	
0503011v1	http://arxiv.org/pdf/q-bio/0503011v1	2005	On the Statistical Law of Life	N. M. Pugno	  In this paper we derive a statistical law of Life. It governs the probability of death, or complementary of survival, of the living organisms. We have deduced such a law coupling the widely used Weibull statistics, developed for describing the distribution of the strength of solids, with the universal model for ontogenetic growth only recently proposed by West and co-authors. The main idea presented in this paper is that cracks can propagate in solids and cause their failure as sick cells in living organisms can cause their death. Making a rough analogy, living organisms are found to behave as growing mechanical components under cyclic, i.e., fatigue, loadings and composed by a dynamic evolutionary material that, as an ineluctable fate, deteriorates. The implications on biological scaling laws are discussed. As an example of application, we apply such a statistical law to large data collections on human deaths due to cancer of various types recorded in Italy: a relevant agreement is observed. 	
0601094v1	http://arxiv.org/pdf/quant-ph/0601094v1	2006	Casimir effect for curved geometries: PFA validity limits	Holger Gies|Klaus Klingmuller	  We compute Casimir interaction energies for the sphere-plate and cylinder-plate configuration induced by scalar-field fluctuations with Dirichlet boundary conditions. Based on a high-precision calculation using worldline numerics, we quantitatively determine the validity bounds of the proximity force approximation (PFA) on which the comparison between all corresponding experiments and theory are based. We observe the quantitative failure of the PFA on the 1% level for a curvature parameter a/R > 0.00755. Even qualitatively, the PFA fails to predict reliably the correct sign of genuine Casimir curvature effects. We conclude that data analysis of future experiments aiming at a precision of 0.1% must no longer be based on the PFA. 	
0603171v1	http://arxiv.org/pdf/quant-ph/0603171v1	2006	Hardy's criterion of nonlocality for mixed states	GianCarlo Ghirardi|Luca Marinatto	  We generalize Hardy's proof of nonlocality to the case of bipartite mixed statistical operators, and we exhibit a necessary condition which has to be satisfied by any given mixed state $\sigma$ in order that a local and realistic hidden variable model exists which accounts for the quantum mechanical predictions implied by $\sigma$. Failure of this condition will imply both the impossibility of any local explanation of certain joint probability distributions in terms of hidden variables and the nonseparability of the considered mixed statistical operator. Our result can be also used to determine the maximum amount of noise, arising from imperfect experimental implementations of the original Hardy's proof of nonlocality, in presence of which it is still possible to put into evidence the nonlocal features of certain mixed states. 	
0706.0170v1	http://arxiv.org/pdf/0706.0170v1	2007	On the origin of the $λ$-transition in liquid Sulphur	Tullio Scopigno|Spyros Yannopoulos|Filippo Scarponi|Kostas Andrikopoulos|Daniele Fioretto|Giancarlo Ruocco	  Developing a novel experimental technique, we applied photon correlation spectroscopy using infrared radiation in liquid Sulphur around $T_\lambda$, i.e. in the temperature range where an abrupt increase in viscosity by four orders of magnitude is observed upon heating within few degrees. This allowed us - overcoming photo-induced and absorption effects at visible wavelengths - to reveal a chain relaxation process with characteristic time in the ms range. These results do rehabilitate the validity of the Maxwell relation in Sulphur from an apparent failure, allowing rationalizing the mechanical and thermodynamic behavior of this system within a viscoelastic scenario. 	
0708.4296v1	http://arxiv.org/pdf/0708.4296v1	2007	Nuclear physics with spherically symmetric supernova models	M. Liebendoerfer|T. Fischer|C. Fröhlich|F. -K. Thielemann|S. Whitehouse	  Few years ago, Boltzmann neutrino transport led to a new and reliable generation of spherically symmetric models of stellar core collapse and postbounce evolution. After the failure to prove the principles of the supernova explosion mechanism, these sophisticated models continue to illuminate the close interaction between high-density matter under extreme conditions and the transport of leptons and energy in general relativistically curved space-time. We emphasize that very different input physics is likely to be relevant for the different evolutionary phases, e.g. nuclear structure for weak rates in collapse, the equation of state of bulk nuclear matter during bounce, multidimensional plasma dynamics in the postbounce evolution, and neutrino cross sections in the explosive nucleosynthesis. We illustrate the complexity of the dynamics using preliminary 3D MHD high-resolution simulations based on parameterized deleptonization. With established spherically symmetric models we show that typical features of the different phases are reflected in the predicted neutrino signal and that a consistent neutrino flux leads to electron fractions larger than 0.5 in neutrino-driven supernova ejecta. 	
0710.1535v1	http://arxiv.org/pdf/0710.1535v1	2007	Charmonium dynamics in heavy ion collisions	O. Linnyk|E. L. Bratkovskaya|W. Cassing|H. Stoecker	  Applying the HSD transport approach to charmonium dynamics within the 'hadronic comover model' and the 'QGP melting scenario', we show that the suppression pattern seen at RHIC cannot be explained by the interaction with baryons, comoving mesons and/or by color screening mechanism. The interaction with hadrons in the late stages of the collision (when the energy density falls below the critical) gives a sizable contribution to the suppression. On the other hand, it does not account for the observed additional charmonium dissociation and its dependence on rapidity. Together with the failure of the hadron-string models to reproduce high v2 of open charm mesons, this suggests strong pre-hadronic interaction of c-cbar with the medium at high energy densities. 	
0710.1917v3	http://arxiv.org/pdf/0710.1917v3	2007	The heating of the cooling flow (The feedback effervescent heating   model)	Nasser Mohamed Ahmed	  The standard cooling flow model has predicted a large amount of cool gas in the clusters of galaxies. The failure of the Chandra and XXM-Newton telescopes to detect cooling gas (below 1-2 keV) in clusters of galaxies has suggested that some heating process must work to suppress the cooling. The most likely heating source is the heating by AGNs. There are many heating mechanisms, but we will adopt the effervescent heating model which is a result of the interaction of the bubbles inflated by AGN with the intra-cluster medium(ICM).   Using the FLASH code, we have carried out time dependent simulations to investigate the effect of the heating on the suppression of the cooling in cooling flow clusters. We have found that the effervescent heating model can not balance the radiative cooling and it is an artificial model. Furthermore, the effervescent heating is a function of the ICM pressure gradient but the cooling is proportional to the gas density square and square root of the gas temperature. 	
0710.4008v1	http://arxiv.org/pdf/0710.4008v1	2007	Scale-free networks resistant to intentional attacks	Lazaros K. Gallos|Panos Argyrakis	  We study the detailed mechanism of the failure of scale-free networks under intentional attacks. Although it is generally accepted that such networks are very sensitive to targeted attacks, we show that for a particular type of structure such networks surprisingly remain very robust even under removal of a large fraction of their nodes, which in some cases can be up to 70%. The degree distribution $P(k)$ of these structures is such that for small values of the degree $k$ the distribution is constant with $k$, up to a critical value $k_c$, and thereafter it decays with $k$ with the usual power law. We describe in detail a model for such a scale-free network with this modified degree distribution, and we show both analytically and via simulations, that this model can adequately describe all the features and breakdown characteristics of these attacks. We have found several experimental networks with such features, such as for example the IMDB actors collaboration network or the citations network, whose resilience to attacks can be accurately described by our model. 	
0711.1191v1	http://arxiv.org/pdf/0711.1191v1	2007	High Strain and Strain-Rate Behaviour of PTFE/Aluminium/Tungsten   Mixtures	John Addiss|Jing Cai|Stephen Walley|William Proud|Vitali F. Nesterenko	  Conventional drop-weight techniques were modified to accommodate low-amplitude force transducer signals from low-strength, cold isostatically pressed 'heavy' composites of polytetrafluoroethylene, aluminum and tungsten. The failure strength, strain and the post-critical behavior of failed samples were measured for samples of different porosity and tungsten grain size. Unusual phenomenon of significantly higher strength (55 MPa) of porous composites (density 5.9 g/cc) with small W particles (less than 1 micron) in comparison with strength (32 MPa) of dense composites (7.1 g/cc) with larger W particles (44 microns) at the same volume content of components was observed. This is attributed to force chains created by a network of small W particles. Interrupted tests at different levels of strain revealed the mechanisms of fracture under dynamic compression. 	
0801.1927v1	http://arxiv.org/pdf/0801.1927v1	2008	Asynchronous Remote Medical Consultation for Ghana	Rowena Luk|Melissa Ho|Paul M. Aoki	  Computer-mediated communication systems can be used to bridge the gap between doctors in underserved regions with local shortages of medical expertise and medical specialists worldwide. To this end, we describe the design of a prototype remote consultation system intended to provide the social, institutional and infrastructural context for sustained, self-organizing growth of a globally-distributed Ghanaian medical community. The design is grounded in an iterative design process that included two rounds of extended design fieldwork throughout Ghana and draws on three key design principles (social networks as a framework on which to build incentives within a self-organizing network; optional and incremental integration with existing referral mechanisms; and a weakly-connected, distributed architecture that allows for a highly interactive, responsive system despite failures in connectivity). We discuss initial experiences from an ongoing trial deployment in southern Ghana. 	
0801.2468v1	http://arxiv.org/pdf/0801.2468v1	2008	The Construction of the CMS Silicon Strip Tracker	Giacomo Sguazzoni	  The CMS Silicon Strip tracker is a very large scale tracker entirely based on silicon strip detectors technology. The integration of modules, electronics, mechanics and services has been completed within the last eighteen months; first large standalone sub-structures (shells, disks, rods, petals depending on the tracker subdetector) have been integrated and verified; then they have been brought together into the final configuration. The CMS silicon tracker design and its construction is reviewed with particular emphasis on the procedures and quality checks deployed to successfully assembly several silicon strip modules and all ancillary components into these large sub-structures. An overview of the results and the lesson learned from the tracker integration are given, also in terms of failure and damage rates. 	
0802.3003v1	http://arxiv.org/pdf/0802.3003v1	2008	Discrete Fracture Model with Anisotropic Load Sharing	R. C. Hidalgo|S. Zapperi|H. J. Herrmann	  A two-dimensional fracture model where the interaction among elements is modeled by an anisotropic stress-transfer function is presented. The influence of anisotropy on the macroscopic properties of the samples is clarified, by interpolating between several limiting cases of load sharing. Furthermore, the critical stress and the distribution of failure avalanches are obtained numerically for different values of the anisotropy parameter $\alpha$ and as a function of the interaction exponent $\gamma$. From numerical results, one can certainly conclude that the anisotropy does not change the crossover point $\gamma_c=2$ in 2D. Hence, in the limit of infinite system size, the crossover value $\gamma_c=2$ between local and global load sharing is the same as the one obtained in the isotropic case. In the case of finite systems, however, for $\gamma\le2$, the global load sharing behavior is approached very slowly. 	
0803.1913v1	http://arxiv.org/pdf/0803.1913v1	2008	Rupture sismique des fondations par perte de capacité portante: Le cas   des semelles circulaires	Charisis Chatzigogos|Alain Pecker|J. Salençon	  Within the context of earthquake-resistant design of shallow foundations, the present study is concerned with the determination of the seismic bearing capacity of a circular footing resting on the surface of a heterogene-ous purely cohesive semi-infinite soil layer. In the first part of the paper, a database, containing case histories of civil engineering structures that sustained a foundation seismic bearing capacity failure, is briefly pre-sented, aiming at a better understanding of the studied phenomenon and offering a number of case studies useful for validation of theoretical computations. In the second part of the paper, the aforementioned problem is addressed using the kinematic approach of the Yield Design theory, thus establishing optimal upper bounds for the ultimate seismic loads supported by the soil-footing system. The results lead to the establishment of some very simple guidelines that extend the existing formulae for the seismic bearing capacity contained in the European norms (proposed for strip footings on homogeneous soils) to the case of circular footings and to that of heterogeneous cohesive soils. 	
0808.0709v1	http://arxiv.org/pdf/0808.0709v1	2008	Fine Structure of Avalanches in the Abelian Sandpile Model	Amir Abdolvand|Afshin Montakhab	  We study the two-dimensional Abelian Sandpile Model on a square lattice of linear size L. We introduce the notion of avalanche's fine structure and compare the behavior of avalanches and waves of toppling. We show that according to the degree of complexity in the fine structure of avalanches, which is a direct consequence of the intricate superposition of the boundaries of successive waves, avalanches fall into two different categories. We propose scaling ans\"{a}tz for these avalanche types and verify them numerically. We find that while the first type of avalanches has a simple scaling behavior, the second (complex) type is characterized by an avalanche-size dependent scaling exponent. This provides a framework within which one can understand the failure of a consistent scaling behavior in this model. 	
0809.2844v1	http://arxiv.org/pdf/0809.2844v1	2008	Interplay of local hydrogen-bonding and long-ranged dipolar forces in   simulations of confined water	Jocelyn M. Rodgers|John D. Weeks	  Spherical truncations of Coulomb interactions in standard models for water permit efficient molecular simulations and can give remarkably accurate results for the structure of the uniform liquid. However truncations are known to produce significant errors in nonuniform systems, particularly for electrostatic properties. Local molecular field (LMF) theory corrects such truncations by use of an effective or restructured electrostatic potential that accounts for effects of the remaining long-ranged interactions through a density-weighted mean field average and satisfies a modified Poisson's equation defined with a Gaussian-smoothed charge density. We apply LMF theory to three simple molecular systems that exhibit different aspects of the failure of a naive application of spherical truncations -- water confined between hydrophobic walls, water confined between atomically-corrugated hydrophilic walls, and water confined between hydrophobic walls with an applied electric field. Spherical truncations of 1/r fail spectacularly for the final system in particular, and LMF theory corrects the failings for all three. Further, LMF theory provides a more intuitive way to understand the balance between local hydrogen bonding and longer-ranged electrostatics in molecular simulations involving water. 	
0810.2055v1	http://arxiv.org/pdf/0810.2055v1	2008	The performance of Minima Hopping and Evolutionary Algorithms for   cluster structure prediction	Sandro E. Schoenborn|Stefan Goedecker|Shantanu Roy|Artem R. Oganov	  We compare Evolutionary Algorithms with Minima Hopping for global optimization in the field of cluster structure prediction. We introduce a new {\em average offspring} recombination operator and compare it with previously used operators. Minima Hopping is improved with a {\em softening} method and a stronger feedback mechanism. Test systems are atomic clusters with Lennard-Jones interaction as well as silicon and gold clusters described by force fields. The improved Minima Hopping is found to be well-suited to all these homoatomic problems. The evolutionary algorithm is more efficient for systems with compact and symmetric ground states, including LJ$_{150}$, but it fails for systems with very complex energy landscapes and asymmetric ground states, such as LJ$_{75}$ and silicon clusters with more than 30 atoms. Both successes and failures of the evolutionary algorithm suggest ways for its improvement. 	
0811.3611v1	http://arxiv.org/pdf/0811.3611v1	2008	Nucleation of interfacial shear cracks in thin films on disordered   substrates	Michael Zaiser|Paolo Moretti|Avraam Konstantinidis|Elias C Aifantis	  We formulate a theoretical model of the shear failure of a thin film tethered to a rigid substrate. The interface between film and substrate is modeled as a cohesive layer with randomly fluctuating shear strength/fracture energy. We demonstrate that, on scales large compared with the film thickness, the internal shear stresses acting on the interface can be approximated by a second-order gradient of the shear displacement across the interface. The model is used to study one-dimensional shear cracks, for which we evaluate the stress-dependent probability of nucleation of a critical crack. This is used to determine the interfacial shear strength as a function of film geometry and statistical properties of the interface. 	
0901.3759v2	http://arxiv.org/pdf/0901.3759v2	2009	On the relationship between structure and dynamics in a supercooled   liquid	Asaph Widmer-Cooper|Peter Harrowell	  We present the dynamic propensity distribution as an explicit measure of the degree to which the dynamics in a liquid over the time scale of structural relaxation is determined by the initial configuration. We then examine, for a binary mixture of soft discs in two dimensions, the correlation between the spatial distribution of propensity and that of two localmeasures of configuration structure: the local composition and local free volume. While the small particles dominate the high propensity population,we find no strong correlation between either the local composition or the local free volume and the propensity. It is argued that this is a generic failure of purely local structural measures to capture the inherently non-local character of collective behaviour. 	
0901.4692v1	http://arxiv.org/pdf/0901.4692v1	2009	Effectiveness of Ninth-Grade Physics in Maine: Conceptual Understanding	Michael O'Brien|John Thompson	  The Physics First movement - teaching a true physics course to ninth grade students - is gaining popularity in high schools. There are several different rhetorical arguments for and against this movement, and it is quite controversial in physics education. However, there is no actual evidence to assess the success, or failure, of this substantial shift in the science teaching sequence. We have undertaken a comparison study of physics classes taught in ninth- and 12th grade classes in Maine. Comparisons of student understanding and gains with respect to mechanics concepts were made with excerpts from well-known multiple-choice surveys and individual student interviews. Results indicate that both populations begin physics courses with similar content knowledge and specific difficulties, but that in the learning of the concepts ninth graders are more sensitive to the instructional method used. 	
0903.3279v1	http://arxiv.org/pdf/0903.3279v1	2009	Generalisation of the fractal Einstein law relating conduction and   diffusion on networks	Anthony P. Roberts|Christophe P. Haynes	  In the 1980s an important goal of the emergent field of fractals was to determine the relationships between their physical and geometrical properties. The fractal-Einstein and Alexander-Orbach laws, which interrelate electrical, diffusive and fractal properties, are two key theories of this type. Here we settle a long standing controversy about their exactness by showing that the properties of a class of fractal trees violate both laws. A new formula is derived which unifies the two classical results by proving that if one holds, then so must the other, and resolves a puzzling discrepancy in the properties of Eden trees and diffusion limited aggregates. The failure of the classical laws is attributed to anisotropic exploration of the network by a random walker. The occurrence of this newly revealed behaviour means that numerous theories, such as recent first passage time results, are restricted to a narrower range of networks than previously thought. 	
0904.1611v1	http://arxiv.org/pdf/0904.1611v1	2009	Shear Unzipping of DNA	Buddhapriya Chakrabarti|David R. Nelson	  We study theoretically the mechanical failure of a simple model of double stranded DNA under an applied shear. Starting from a more microscopic Hamiltonian that describes a sheared DNA, we arrive at a nonlinear generalization of a ladder model of shear unzipping proposed earlier by deGennes [deGennes P. G. C. R. Acad. Sci., Ser. IV; Phys., Astrophys. 2001, 1505]. Using this model and a combination of analytical and numerical methods, we study the DNA "unzipping" transition when the shearing force exceeds a critical threshold at zero temperature. We also explore the effects of sequence heterogeneity and finite temperature and discuss possible applications to determine the strength of colloidal nanoparticle assemblies functionalized by DNA. 	
0904.1986v1	http://arxiv.org/pdf/0904.1986v1	2009	The Breaking Strain of Neutron Star Crust and Gravitational Waves	C. J. Horowitz|Kai Kadau	  Mountains on rapidly rotating neutron stars efficiently radiate gravitational waves. The maximum possible size of these mountains depends on the breaking strain of neutron star crust. With multi-million ion molecular dynamics simulations of Coulomb solids representing the crust, we show that the breaking strain of pure single crystals is very large and that impurities, defects, and grain boundaries only modestly reduce the breaking strain to around 0.1. Due to the collective behavior of the ions during failure found in our simulations, the neutron star crust is likely very strong and can support mountains large enough so that their gravitational wave radiation could limit the spin periods of some stars and might be detectable in large scale interferometers. Furthermore, our microscopic modeling of neutron star crust material can help analyze mechanisms relevant in magnetar giant and micro flares. 	
0906.1013v1	http://arxiv.org/pdf/0906.1013v1	2009	Dielectrophoretic Assembly of High-Density Arrays of Individual Graphene   Devices for Rapid Screening	Aravind Vijayaraghavan|Calogero Sciascia|Simone Dehm|Antonio Lombardo|Alessandro Bonetti|Andrea C. Ferrari|Ralph Krupke	  We establish the use of dielectrophoresis for the directed parallel assembly of individual flakes and nanoribbons of few-layer graphene into electronic devices. This is a bottom-up approach where source and drain electrodes are prefabricated and the flakes are deposited from a solution using an alternating electric field applied between the electrodes. These devices are characterized by scanning electron microscopy, atomic force microscopy, Raman spectroscopy and electron transport measurements. They are shown to be electrically active and their current carrying capacity and subsequent failure mechanism is revealed. Akin to carbon nanotubes, we show that the dielectrophoretic deposition is self-limiting to one flake per device and is scalable to ultra-large-scale integration densities, thereby enabling the rapid screening of a large number of devices. 	
0908.0424v1	http://arxiv.org/pdf/0908.0424v1	2009	The work value of information	Oscar C. O. Dahlsten|Renato Renner|Elisabeth Rieper|Vlatko Vedral	  We present quantitative relations between work and information that are valid both for finite sized and internally correlated systems as well in the thermodynamical limit. We suggest work extraction should be viewed as a game where the amount of work an agent can extract depends on how well it can guess the micro-state of the system. In general it depends both on the agent's knowledge and risk-tolerance, because the agent can bet on facts that are not certain and thereby risk failure of the work extraction. We derive strikingly simple expressions for the extractable work in the extreme cases of effectively zero- and arbitrary risk tolerance respectively, thereby enveloping all cases. Our derivation makes a connection between heat engines and the smooth entropy approach. The latter has recently extended Shannon theory to encompass finite sized and internally correlated bit strings, and our analysis points the way to an analogous extension of statistical mechanics. 	
0909.0685v1	http://arxiv.org/pdf/0909.0685v1	2009	In-Network Outlier Detection in Wireless Sensor Networks	Joel W. Branch|Chris Giannella|Boleslaw Szymanski|Ran Wolff|Hillol Kargupta	  To address the problem of unsupervised outlier detection in wireless sensor networks, we develop an approach that (1) is flexible with respect to the outlier definition, (2) computes the result in-network to reduce both bandwidth and energy usage,(3) only uses single hop communication thus permitting very simple node failure detection and message reliability assurance mechanisms (e.g., carrier-sense), and (4) seamlessly accommodates dynamic updates to data. We examine performance using simulation with real sensor data streams. Our results demonstrate that our approach is accurate and imposes a reasonable communication load and level of power consumption. 	
0909.3482v1	http://arxiv.org/pdf/0909.3482v1	2009	Schumpeterian economic dynamics as a quantifiable minimum model of   evolution	Stefan Thurner|Peter Klimek|Rudolf Hanel	  We propose a simple quantitative model of Schumpeterian economic dynamics. New goods and services are endogenously produced through combinations of existing goods. As soon as new goods enter the market they may compete against already existing goods, in other words new products can have destructive effects on existing goods. As a result of this competition mechanism existing goods may be driven out from the market - often causing cascades of secondary defects (Schumpeterian gales of destruction). The model leads to a generic dynamics characterized by phases of relative economic stability followed by phases of massive restructuring of markets - which could be interpreted as Schumpeterian business `cycles'. Model timeseries of product diversity and productivity reproduce several stylized facts of economics timeseries on long timescales such as GDP or business failures, including non-Gaussian fat tailed distributions, volatility clustering etc. The model is phrased in an open, non-equilibrium setup which can be understood as a self organized critical system. Its diversity dynamics can be understood by the time-varying topology of the active production networks. 	
0909.3701v1	http://arxiv.org/pdf/0909.3701v1	2009	Anharmonicity and quasi-localization of the excess low-frequency   vibrations in jammed solids	Ning Xu|Vincenzo Vitelli|Andrea J. Liu|Sidney R. Nagel	  We compare the harmonic and anharmonic properties of the vibrational modes in 3-dimensional jammed packings of frictionless spheres interacting via repulsive, finite range potentials. A crossover frequency is apparent in the density of states, the diffusivity and the participation ratio of the modes. At this frequency, which shifts to zero at the jamming threshold, the vibrational modes have a very small participation ratio implying that the modes are quasi-localized. The most anharmonic modes occur at low frequency which is opposite to what is normally found in crystals. The lowest frequency modes have the strongest response to the pressure and the lowest energy barriers to mechanical failure. 	
0911.2010v1	http://arxiv.org/pdf/0911.2010v1	2009	Critical Kondo destruction and the violation of the quantum-to-classical   mapping of quantum criticality	Stefan Kirchner|Qimiao Si	  Antiferromagnetic heavy fermion metals close to their quantum critical points display a richness in their physical properties unanticipated by the traditional approach to quantum criticality, which describes the critical properties solely in terms of fluctuations of the order parameter. This has led to the question as to how the Kondo effect gets destroyed as the system undergoes a phase change. In one approach to the problem, Kondo lattice systems are studied through a self-consistent Bose-Fermi Kondo model within the Extended Dynamical Mean Field Theory. The quantum phase transition of the Kondo lattice is thus mapped onto that of a sub-Ohmic Bose-Fermi Kondo model. In the present article we address some aspects of the failure of the standard order-parameter functional for the the Kondo-destroying quantum critical point of the Bose-Fermi Kondo model. 	
0912.0549v1	http://arxiv.org/pdf/0912.0549v1	2009	Modular Workflow Engine for Distributed Services using Lightweight Java   Clients	R. -M. Vetter|W. Lennartz|J. -V. Peetz	  In this article we introduce the concept and the first implementation of a lightweight client-server-framework as middleware for distributed computing. On the client side an installation without administrative rights or privileged ports can turn any computer into a worker node. Only a Java runtime environment and the JAR files comprising the workflow client are needed. To connect all clients to the engine one open server port is sufficient. The engine submits data to the clients and orchestrates their work by workflow descriptions from a central database. Clients request new task descriptions periodically, thus the system is robust against network failures. In the basic set-up, data up- and downloads are handled via HTTP communication with the server. The performance of the modular system could additionally be improved using dedicated file servers or distributed network file systems.   We demonstrate the design features of the proposed engine in real-world applications from mechanical engineering. We have used this system on a compute cluster in design-of-experiment studies, parameter optimisations and robustness validations of finite element structures. 	
1002.0392v1	http://arxiv.org/pdf/1002.0392v1	2010	A novel and precise time domain description of MOSFET low frequency   noise due to random telegraph signals	Roberto da Silva|Gilson Wirth|Lucas Brusamarello	  Nowadays, random telegraph signals play an important role in integrated circuit performance variability, leading for instance to failures in memory circuits. This problem is related to the successive captures and emissions of electrons at the many traps stochastically distributed at the silicon-oxide (Si-SiO2) interface of MOS transistors. In this paper we propose a novel analytical and numerical approach to statistically describe the fluctuations of current due to random telegraph signal in time domain. Our results include two distinct situations: when the density of interface trap density is uniform in energy, and when it is an u-shape curve as prescribed in literature, here described as simple quadratic function. We establish formulas for relative error as function of the parameters related to capture and emission probabilities. For a complete analysis experimental u-shape curves are used and compared with the theoretical aproach. 	
1003.5206v1	http://arxiv.org/pdf/1003.5206v1	2010	Chaos and Thermalization in the one-dimensional Bose-Hubbard model in   the classical-field approximation	Amy C. Cassidy	  In this thesis, we present a comprehensive study of chaos and thermalization of the one-dimensional Bose-Hubbard Model (BHM) within the classical field approximation. Two quantitative measures are compared: the ensemble-averaged Finite-time Maximal Lyapunov exponent, a measures of chaos and the normalized spectral entropy, a measure of the distance between the numerical time-averaged momentum distribution and the one predicted by thermodynamics. A threshold for chaos is found, which depends on two parameters, the nonlinearity and the total energy-per-particle. Below the threshold, the dynamics are regular, while far above the threshold, complete thermalization is observed, as measured by the normalized spectral entropy. We study individual resonances in the Bose-Hubbard model to determine the criterion for chaos. The criterion based on Chirikov's method of overlapping resonances diverges in the thermodynamic limit, in contrast to the criterion parameters inferred from numerical calculations, signifying the failure of the standard Chirikov's approach. The Ablowitz-Ladik lattice is one of several integrable models that are close to the BHM. We outline the method of Inverse Scattering Transform and generate the integrals of motion of the Ablowitz-Ladik lattice. Furthermore, we discuss the possible role of these quantities in the relaxation dynamics of the BHM. 	
1005.0796v1	http://arxiv.org/pdf/1005.0796v1	2010	Structure-Sensitive Mechanism of Nanographene Failure	Elena F. Sheka|Nadezhda A. Popova|Vera A. Popova|Ekaterina A. Nikitina|Landysh H. Shaymardanova	  The response of a nanographene sheet to external stresses is considered in terms of a mechanochemical reaction. The quantum chemical realization of the approach is based on a coordinate-of-reaction concept for the purpose of introducing a mechanochemical internal coordinate (MIC) that specifies a deformational mode. The related force of response is calculated as the energy gradient along the MIC, while the atomic configuration is optimized over all of the other coordinates under the MIC constant-pitch elongation. The approach is applied to the benzene molecule and (5, 5) nanographene. A drastic anisotropy in the microscopic behavior of both objects under elongation along a MIC has been observed when the MIC is oriented either along or normally to the C-C bonds chain. Both the anisotropy and high stiffness of the nanographene originate at the response of the benzenoid unit to stress. 	
1005.1119v1	http://arxiv.org/pdf/1005.1119v1	2010	Coherent Manipulation of Multilevel Atoms for Quantum Information   Processing	Juan D. Serna	  In quantum information processing, quantum cavities play an important role by providing the mechanisms to transfer information between atom qubits and photon qubits, or to couple single atoms with the optical modes of the cavity field. We explore numerically the population transfer in an atom + cavity system by using the $\pi$-pulse and adiabatic passage methods. While the first method is very efficient transferring the atomic population for no radiative decay of the intermediate level, the second method shows very interesting nonadiabatic, resonance-like properties that can be used to achieve very large transfer efficiencies without needing very large Rabi frequencies or very long interaction times. We introduce a simple analytical model to explore the origin of these properties and describe "qualitatively" the power-law dependence of the failure probability on the product of the pulse amplitude and the interaction time. We also examine numerically the transfer of interatomic coherence in a two-atom + cavity system by using adiabatic methods. For some specific symmetry conditions, we show that the dynamics of the original system can be studied as the individual evolution of a symmetric and an antisymmetric system, interacting separately with the classical field and the cavity mode, but mutually exchanging the atomic coherence. 	
1005.4932v2	http://arxiv.org/pdf/1005.4932v2	2010	Failure of Bell's Theorem and the Local Causality of the Entangled   Photons	Joy Christian	  A counterexample to Bell's theorem is presented which uses a pair of photons instead of spin-1/2 particles used in our previous counterexamples. A locally causal protocol is provided for Alice and Bob, which allows them to simulate observing photon polarizations at various angles, and record their results as A=+/-1 in S^3 and B=+/-1 in S^3, respectively. When these results are compared, the correlations are seen to be exactly those predicted by quantum mechanics; namely cos 2(alpha - beta), where alpha and beta are the angles of polarizers. The key ingredient in our counterexample is the topology of 3-sphere, which remains closed under multiplication, thus preserving the locality condition of Bell. 	
1006.3521v1	http://arxiv.org/pdf/1006.3521v1	2010	Business fluctuations in a credit-network economy	Domenico Delli Gatti|Mauro Gallegati|Bruce Greenwald|Alberto Russo|Joseph E. Stiglitz	  We model a network economy with three sectors: downstream firms, upstream firms, and banks. Agents are linked by productive and credit relationships so that the behavior of one agent influences the behavior of the others through network connections. Credit interlinkages among agents are a source of bankruptcy diffusion: in fact, failure of fulfilling debt commitments would lead to bankruptcy chains. All in all, the bankruptcy in one sector can diffuse to other sectors through linkages creating a vicious cycle and bankruptcy avalanches in the network economy. Our analysis show how the choices of credit supply by both banks and firms are interrelated. While the initial impact of monetary policy is on bank behaviour, we show the interactive play between the choices made by banks, the choices made by firms in their role as providers of credit, and the choices made by firms in their role as producers. 	
1007.1780v1	http://arxiv.org/pdf/1007.1780v1	2010	A subluminous Schroedinger equation	Philip Rosenau|Zeev Schuss	  The standard derivation of Schroedinger's equation from a Lorentz-invariant Feynman path integral consists in taking first the limit of infinite speed of light and then the limit of short time slice. In this order of limits the light cone of the path integral disappears, giving rise to an instantaneous spread of the wave function to the entire space. We ascribe the failure of Schroedinger's equation to retain the light cone of the path integral to the very nature of the limiting process: it is a regular expansion of a singular approximation problem, because the boundary conditions of the path integral on the light cone are lost in this limit. We propose a distinguished limit, which produces an intermediate model between non-relativistic and relativistic quantum mechanics: it produces Schroedinger's equation and preserves the zero boundary conditions on and outside the original light cone of the path integral. These boundary conditions relieve the Schroedinger equation of several annoying, seemingly unrelated unphysical artifacts, including non-analytic wave functions, spontaneous appearance of discontinuities, non-existence of moments when the initial wave function has a jump discontinuity (e.g., a collapsed wave function after a measurement), the EPR paradox, and so on. The practical implications of the present formulation are yet to be seen. 	
1007.2155v3	http://arxiv.org/pdf/1007.2155v3	2011	Evidence for Anthropogenic Surface Loading as Trigger Mechanism of the   2008 Wenchuan Earthquake	Christian D. Klose	  Two and a half years prior to China's M7.9 Wenchuan earthquake of May 2008, at least 300 million metric tons of water accumulated with additional seasonal water level changes in the Minjiang River Valley at the eastern margin of the Longmen Shan. This article shows that static surface loading in the Zipingpu water reservoir induced Coulomb failure stresses on the nearby Beichuan thrust fault system at <17km depth. Triggering stresses exceeded levels of daily lunar and solar tides and perturbed a fault area measuring 416+/-96km^2. These stress perturbations, in turn, likely advanced the clock of the mainshock and directed the initial rupture propagation upward towards the reservoir on the "Coulomb-like" Beichuan fault with rate-and-state dependent frictional behavior. Static triggering perturbations produced up to 60 years (0.6%) of equivalent tectonic loading, and show strong correlations to the coseismic slip. Moreover, correlations between clock advancement and coseismic slip, observed during the mainshock beneath the reservoir, are strongest for a longer seismic cycle (10kyr) of M>7 earthquakes. Finally, the daily event rate of the micro-seismicity (M>0.5) correlates well with the static stress perturbations, indicating destabilization. 	
1008.0464v1	http://arxiv.org/pdf/1008.0464v1	2010	Backward Causation in Complex Action Model --- Superdeterminism and   Transactional Interpretations	Holger B. Nielsen|Masao Ninomiya	  It is shown that the transactional interpretation of quantum mechanics being referred back to Feynman-Wheeler's time reversal symmetric radiation theory has reminiscences to our complex action model. In this complex action model the initial conditions are in principle even calculable. Thus it philosophically points towards superdeterminism, but really the Bell theorem problem is solved in our model of complex action by removing the significance of signals running slower than by light velocity.   Our model as earlier published predicts that LHC should have some failure before reaching to have produced as many Higgs-particles as would have been produced the SSC accelerator. In the present article, we point out that a cardgame involving whether to restrict LHC-running as we have proposed to test our model will under all circumstances be a success. 	
1008.2236v2	http://arxiv.org/pdf/1008.2236v2	2010	Efficient energy transfer in light-harvesting systems, I: optimal   temperature, reorganization energy, and spatial-temporal correlations	Jianlan Wu|Fan Liu|Young Shen|Jianshu Cao|Robert J. Silbey	  Understanding the mechanisms of efficient and robust energy transfer in light-harvesting systems provides new insights for the optimal design of artificial systems. In this paper, we use the Fenna-Matthews-Olson (FMO) protein complex and phycocyanin 645 (PC 645) to explore the general dependence on physical parameters that help maximize the efficiency and maintain its stability. With the Haken-Strobl model, the maximal energy transfer efficiency (ETE) is achieved under an intermediate optimal value of dephasing rate. To avoid the infinite temperature assumption in the Haken-Strobl model and the failure of the Redfield equation in predicting the Forster rate behavior, we use the generalized Bloch-Redfield (GBR) equation approach to correctly describe dissipative exciton dynamics and find that maximal ETE can be achieved under various physical conditions, including temperature, reorganization energy, and spatial-temporal correlations in noise. We also identify regimes of reorganization energy where the ETE changes monotonically with temperature or spatial correlation and therefore cannot be optimized with respect to these two variables. 	
1009.2556v2	http://arxiv.org/pdf/1009.2556v2	2011	Securing Dynamic Distributed Storage Systems against Eavesdropping and   Adversarial Attacks	Sameer Pawar|Salim El Rouayheb|Kannan Ramchandran	  We address the problem of securing distributed storage systems against eavesdropping and adversarial attacks. An important aspect of these systems is node failures over time, necessitating, thus, a repair mechanism in order to maintain a desired high system reliability. In such dynamic settings, an important security problem is to safeguard the system from an intruder who may come at different time instances during the lifetime of the storage system to observe and possibly alter the data stored on some nodes. In this scenario, we give upper bounds on the maximum amount of information that can be stored safely on the system. For an important operating regime of the distributed storage system, which we call the 'bandwidth-limited regime', we show that our upper bounds are tight and provide explicit code constructions. Moreover, we provide a way to short list the malicious nodes and expurgate the system. 	
1009.5454v1	http://arxiv.org/pdf/1009.5454v1	2010	Study of gold induced heavy ion collisions using isospin dependent QMD   model	Bhawna Sharma|Sanjeev Kumar|Suneel Kumar|Rajeev K. Puri	  We have studied the fragment production mechanism in the set of four reactions 197Au79+12C6, 197Au79+26Al13, 197Au79+63Cu29 and 197Au79+208Pb82. The reactions are simulated at an energy 600 MeV/nucleon and collision geometry is varied from central to peripheral (= b/bmax = 0 to 1). A theoretical investigation has been carried out on the study of mass dependence of intermediate mass fragments (5\leqA\leqAtot/6) and other fragments. It is observed that multiplicity shows a good agreement for low Zbound, but it fails for high Zbound. This failure is due the method of analysis MST which we had used in our analysis, because MST method gives one heavy cluster at the time of high density. The discrepancy between theory and experiments can be removed by using reduced isospin dependent NN cross section and sophisticated clustrization algorithm SACA. 	
1011.0063v1	http://arxiv.org/pdf/1011.0063v1	2010	Multiscales and cascade in isotropic turbulence	Zheng Ran	  The central problem of fully developed turbulence is the energy cascading process. It has revisited all attempts at a full physical understanding or mathematical formulation. The main reason for this failure are related to the large hierarchy of scales involved, the highly nonlinear character inherent in the Navier-Stokes equations, and the spatial intermittency of the dynamically active regions. Richardson has described the interplay between large and small scales and the phenomena so described are known as the Richardson cascade. This local interplay also forms the basis of a theory by Kolmogorov. In this letter, we use the explicit map method to analyze the nonlinear dynamical behavior for cascade in isotropic turbulence. This deductive scale analysis is shown to provide the first visual evidence of the celebrated Richardson cascade, and reveals in particular its multiscale character. The results also indicate that the energy cascading process has remarkable similarities with the deterministic construction rules of the logistic map. Cascade of period-doubling bifurcations have been seen in this isotropic turbulent systems that exhibit chaotic behavior. The `cascade' appears as an infinite sequence of period-doubling bifurcations. 	
1012.0203v1	http://arxiv.org/pdf/1012.0203v1	2010	Enhancing synchronization by directionality in complex networks	An Zeng|Seung-Woo Son|Chi Ho Yeung|Ying Fan|Zengru Di	  We proposed a method called residual edge-betweenness gradient (REBG) to enhance synchronizability of networks by assignment of link direction while keeping network topology and link weight unchanged. Direction assignment has been shown to improve the synchronizability of undirected networks in general, but we find that in some cases incommunicable components emerge and networks fail to synchronize. We show that the REBG method can effectively avoid the synchronization failure ($R=\lambda_{2}^{r}/\lambda_{N}^{r}=0$) which occurs in the residual degree gradient (RDG) method proposed in Phys. Rev. Lett. 103, 228702 (2009). Further experiments show that REBG method enhance synchronizability in networks with community structure as compared with the RDG method. 	
1012.4632v2	http://arxiv.org/pdf/1012.4632v2	2011	Quantum theory of fermion production after inflation	J. Berges|D. Gelfand|J. Pruschke	  We show that quantum effects dramatically enhance the production of fermions following preheating after inflation in the early Universe in the presence of high excitations of bosonic quanta. As a consequence fermions rapidly approach a quasistationary distribution with a thermal occupancy in the infrared, while the inflaton enters a turbulent scaling regime. The failure of standard semiclassical descriptions based on the Dirac equation with a homogeneous background field is caused by nonperturbatively high boson occupation numbers. During preheating the inflaton occupation number increases, thus leading to a dynamical mechanism for the enhanced production of fermions from the rescattering of the inflaton quanta. We comment on related phenomena in heavy-ion collisions for the production of quark matter fields from highly occupied gauge bosons. 	
1101.2272v1	http://arxiv.org/pdf/1101.2272v1	2011	Logical Consensus for Distributed and Robust Intrusion Detection	Adriano Fagiolini|Antonio Bicchi	  In this paper we introduce a novel consensus mech- anism where agents of a network are able to share logical values, or Booleans, representing their local opinions on e.g. the presence of an intruder or of a fire within an indoor environment. We first formulate the logical consensus problem, and then we review relevant results in the literature on cellular automata and convergence of finite-state iteration maps. Under suitable joint conditions on the visibility of agents and their communication capability, we provide an algorithm for generating a logical linear consensus system that is globally stable. The solution is optimal in terms of the number of messages to be exchanged and the time needed to reach a consensus. Moreover, to cope with possible sensor failure, we propose a second design approach that produces robust logical nonlinear consensus systems tolerating a given maximum number of faults. Finally, we show applicability of the agreement mechanism to a case study consisting of a distributed Intrusion Detection System (IDS). 	
1101.5564v1	http://arxiv.org/pdf/1101.5564v1	2011	Generating Functions and Stability Study of Multivariate Self-Excited   Epidemic Processes	A. Saichev|D. Sornette	  We present a stability study of the class of multivariate self-excited Hawkes point processes, that can model natural and social systems, including earthquakes, epileptic seizures and the dynamics of neuron assemblies, bursts of exchanges in social communities, interactions between Internet bloggers, bank network fragility and cascading of failures, national sovereign default contagion, and so on. We present the general theory of multivariate generating functions to derive the number of events over all generations of various types that are triggered by a mother event of a given type. We obtain the stability domains of various systems, as a function of the topological structure of the mutual excitations across different event types. We find that mutual triggering tends to provide a significant extension of the stability (or subcritical) domain compared with the case where event types are decoupled, that is, when an event of a given type can only trigger events of the same type. 	
1102.0141v2	http://arxiv.org/pdf/1102.0141v2	2011	Dynamics of the directed Ising chain	Claude Godreche	  The study by Glauber of the time-dependent statistics of the Ising chain is extended to the case where each spin is influenced unequally by its nearest neighbours. The asymmetry of the dynamics implies the failure of the detailed balance condition. The functional form of the rate at which an individual spin changes its state is constrained by the global balance condition with respect to the equilibrium measure of the Ising chain. The local magnetization, the equal-time and two-time correlation functions and the linear response to an external magnetic field obey linear equations which are solved explicitly. The behaviour of these quantities and the relation between the correlation and response functions are analyzed both in the stationary state and in the zero-temperature scaling regime. In the stationary state, a transition between two behaviours of the correlation function occurs when the amplitude of the asymmetry crosses a critical value, with the consequence that the limit fluctuation-dissipation ratio decays continuously from the value 1, for the equilibrium state in the absence of asymmetry, to 0 for this critical value. At zero temperature, under asymmetric dynamics, the system loses its critical character, yet keeping many of the characteristic features of a coarsening system. 	
1103.1207v1	http://arxiv.org/pdf/1103.1207v1	2011	Framework to Solve Load Balancing Problem in Heterogeneous Web Servers	Ms. Deepti Sharma|Ms. Archana B. Saxena	  For popular websites most important concern is to handle incoming load dynamically among web servers, so that they can respond to their client without any wait or failure. Different websites use different strategies to distribute load among web servers but most of the schemes concentrate on only one factor that is number of requests, but none of the schemes consider the point that different type of requests will require different level of processing efforts to answer, status record of all the web servers that are associated with one domain name and mechanism to handle a situation when one of the servers is not working. Therefore, there is a fundamental need to develop strategy for dynamic load allocation on web side. In this paper, an effort has been made to introduce a cluster based frame work to solve load distribution problem. This framework aims to distribute load among clusters on the basis of their operational capabilities. Moreover, the experimental results are shown with the help of example, algorithm and analysis of the algorithm. 	
1103.1897v1	http://arxiv.org/pdf/1103.1897v1	2011	On the Properties of Hydrogen Terminated Diamond as a Photocathode	Jonathan Rameau|John Smedley|Eric Muller|Tim Kidd|Peter Johnson	  Electron emission from the negative electron affinity (NEA) surface of hydrogen terminated, boron doped diamond in the [100] orientation is investigated using angle resolved photoemission spectroscopy (ARPES). ARPES measurements using 16 eV synchrotron and 6 eV laser light are compared and found to show a catastrophic failure of the sudden approximation. While the high energy photoemission is found to yield little information regarding the NEA, low energy laser ARPES reveals for the first time that the NEA results from a novel Franck-Condon mechanism coupling electrons in the conduction band to the vacuum. The result opens the door to development of a new class of NEA electron emitters based on this effect. 	
1104.0121v1	http://arxiv.org/pdf/1104.0121v1	2011	Complex network analysis of water distribution systems	A. Yazdani|P. Jeffrey	  This paper explores a variety of strategies for understanding the formation, structure, efficiency and vulnerability of water distribution networks. Water supply systems are studied as spatially organized networks for which the practical applications of abstract evaluation methods are critically evaluated. Empirical data from benchmark networks are used to study the interplay between network structure and operational efficiency, reliability and robustness. Structural measurements are undertaken to quantify properties such as redundancy and optimal-connectivity, herein proposed as constraints in network design optimization problems. The role of the supply-demand structure towards system efficiency is studied and an assessment of the vulnerability to failures based on the disconnection of nodes from the source(s) is undertaken. The absence of conventional degree-based hubs (observed through uncorrelated non-heterogeneous sparse topologies) prompts an alternative approach to studying structural vulnerability based on the identification of network cut-sets and optimal connectivity invariants. A discussion on the scope, limitations and possible future directions of this research is provided. 	
1104.4209v2	http://arxiv.org/pdf/1104.4209v2	2012	Modeling the clustering in citation networks	Fu-Xin Ren|Xue-Qi Cheng|Hua-Wei Shen	  For the study of citation networks, a challenging problem is modeling the high clustering. Existing studies indicate that the promising way to model the high clustering is a copying strategy, i.e., a paper copies the references of its neighbour as its own references. However, the line of models highly underestimates the number of abundant triangles observed in real citation networks and thus cannot well model the high clustering. In this paper, we point out that the failure of existing models lies in that they do not capture the connecting patterns among existing papers. By leveraging the knowledge indicated by such connecting patterns, we further propose a new model for the high clustering in citation networks. Experiments on two real world citation networks, respectively from a special research area and a multidisciplinary research area, demonstrate that our model can reproduce not only the power-law degree distribution as traditional models but also the number of triangles, the high clustering coefficient and the size distribution of co-citation clusters as observed in these real networks. 	
1106.2275v1	http://arxiv.org/pdf/1106.2275v1	2011	Byzantine Fault Tolerance of Regenerating Codes	Frédérique Oggier|Anwitaman Datta	  Recent years have witnessed a slew of coding techniques custom designed for networked storage systems. Network coding inspired regenerating codes are the most prolifically studied among these new age storage centric codes. A lot of effort has been invested in understanding the fundamental achievable trade-offs of storage and bandwidth usage to maintain redundancy in presence of different models of failures, showcasing the efficacy of regenerating codes with respect to traditional erasure coding techniques. For practical usability in open and adversarial environments, as is typical in peer-to-peer systems, we need however not only resilience against erasures, but also from (adversarial) errors. In this paper, we study the resilience of generalized regenerating codes (supporting multi-repairs, using collaboration among newcomers) in the presence of two classes of Byzantine nodes, relatively benign selfish (non-cooperating) nodes, as well as under more active, malicious polluting nodes. We give upper bounds on the resilience capacity of regenerating codes, and show that the advantages of collaborative repair can turn to be detrimental in the presence of Byzantine nodes. We further exhibit that system mechanisms can be combined with regenerating codes to mitigate the effect of rogue nodes. 	
1106.4090v1	http://arxiv.org/pdf/1106.4090v1	2011	Discovery of Invariants through Automated Theory Formation	Maria Teresa Llano|Andrew Ireland|Alison Pease	  Refinement is a powerful mechanism for mastering the complexities that arise when formally modelling systems. Refinement also brings with it additional proof obligations -- requiring a developer to discover properties relating to their design decisions. With the goal of reducing this burden, we have investigated how a general purpose theory formation tool, HR, can be used to automate the discovery of such properties within the context of Event-B. Here we develop a heuristic approach to the automatic discovery of invariants and report upon a series of experiments that we undertook in order to evaluate our approach. The set of heuristics developed provides systematic guidance in tailoring HR for a given Event-B development. These heuristics are based upon proof-failure analysis, and have given rise to some promising results. 	
1108.0579v1	http://arxiv.org/pdf/1108.0579v1	2011	Damage of Cross-Linked Rubbers as the Scission of Polymer Chains:   Modeling and Tensile Experiments	Alexei Y. Melnikov|A. I. Leonov	  This paper develops a damage model for unfilled cross-linked rubbers based on the concept of scission of polymer chains. The model is built up on the well-known Gent elastic potential complemented by a kinetic equation describing effects of polymer chain scission. The macroscopic parameters in the damage model are evaluated through the parameters for undamaged elastomer. Qualitative analysis of changing molecular parameters of rubbers under scission of polymer chains resulted in easy scaling modeling the dependences of these parameters on the damage factor. It makes possible to predict the rubber failure in molecular terms as mechanical de-vulcanization. The model was tested in tensile quasi-static experiments with both the monotonous loading and repeated loading-unloading. 	
1108.1545v1	http://arxiv.org/pdf/1108.1545v1	2011	Damage of Cross-Linked Rubbers as the Scission of Polymer Chains:   Modeling and Tensile Experiments, Report 1	Alexei Y. Melnikov|A. I. Leonov	  This paper develops a damage model for unfilled cross-linked rubbers based on the concept of scission of polymer chains. The model is built up on the well-known Gent elastic potential complemented by a kinetic equation describing effects of polymer chain scission. The macroscopic parameters in the damage model are evaluated through the parameters for undamaged elastomer. Qualitative analysis of changing molecular parameters of rubbers under scission of polymer chains resulted in easy scaling modeling the dependences of these parameters on the damage factor. It makes possible to predict the rubber failure in molecular terms as mechanical de-vulcanization. The model was tested in tensile quasi-static experiments with both the monotonous loading and repeated loading-unloading. 	
1108.2570v1	http://arxiv.org/pdf/1108.2570v1	2011	Hardness of T-carbon: Density functional theory calculations	Xing-Qiu Chen|Haiyang Niu|Cesare Franchini|Dianzhong Li|Yiyi Li	  We revisit and interpret the mechanical properties of the recently proposed allotrope of carbon, T-carbon [Sheng \emph{et al.}, Phys. Rev. Lett., \textbf{106}, 155703 (2011)], using density functional theory in combination with different empirical hardness models. In contrast with the early estimation based on the Gao's model, which attributes to T-carbon an high Vickers hardness of 61 GPa comparable to that of superhard cubic boron nitride (\emph{c}-BN), we find that T-carbon is not a superhard material, since its Vickers hardenss does not exceed 10 GPa. Besides providing clear evidence for the absence of superhardenss in T-carbon, we discuss the physical reasons behind the failure of Gao's and \v{S}im$\rm\mathring{u}$nek and Vack\'a\v{r}'s (SV) models in predicting the hardness of T-carbon, residing on their improper treatment of the highly anisotropic distribution of quasi-\emph{sp}$^3$-like C-C hybrids. A possible remedy to the Gao and SV models based on the concept of superatom is suggest, which indeed yields a Vickers hardness of about 8 GPa. 	
1109.0839v1	http://arxiv.org/pdf/1109.0839v1	2011	Percolation on correlated random networks	Elena Agliari|Claudia Cioli|Enore Guadagnini	  We consider a class of random, weighted networks, obtained through a redefinition of patterns in an Hopfield-like model and, by performing percolation processes, we get information about topology and resilience properties of the networks themselves. Given the weighted nature of the graphs, different kinds of bond percolation can be studied: stochastic (deleting links randomly) and deterministic (deleting links based on rank weights), each mimicking a different physical process. The evolution of the network is accordingly different, as evidenced by the behavior of the largest component size and of the distribution of cluster sizes. In particular, we can derive that weak ties are crucial in order to maintain the graph connected and that, when they are the most prone to failure, the giant component typically shrinks without abruptly breaking apart; these results have been recently evidenced in several kinds of social networks. 	
1110.1449v1	http://arxiv.org/pdf/1110.1449v1	2011	Teleportation of the one-qubit state with environment-disturbed recovery   operations	Ming-Liang Hu	  We study standard protocol $\mathcal{P}_0$ for teleporting the one-qubit state with both the transmission process of the two qubits constitute the quantum channel and the recovery operations performed by Bob disturbed by the decohering environment. The results revealed that Bob's imperfect operations do not eliminate the possibility of nonclassical teleportation fidelity provided he shares an ideal channel state with Alice, while the transmission process is constrained by a critical time $t_{0,c}$ longer than which will result in failure of $\mathcal{P}_0$ if the two qubits are corrupted by the decohering environment. Moreover, we found that under the condition of the same decoherence rate $\gamma$, the teleportation protocol is significantly more fragile when it is executed under the influence of the noisy environment than those under the influence of the dissipative and dephasing environments. 	
1110.3832v1	http://arxiv.org/pdf/1110.3832v1	2011	Distributed flow optimization and cascading effects in weighted complex   networks	Andrea Asztalos|Sameet Sreenivasan|Boleslaw K. Szymanski|G. Korniss	  We investigate the effect of a specific edge weighting scheme $\sim (k_i k_j)^{\beta}$ on distributed flow efficiency and robustness to cascading failures in scale-free networks. In particular, we analyze a simple, yet fundamental distributed flow model: current flow in random resistor networks. By the tuning of control parameter $\beta$ and by considering two general cases of relative node processing capabilities as well as the effect of bandwidth, we show the dependence of transport efficiency upon the correlations between the topology and weights. By studying the severity of cascades for different control parameter $\beta$, we find that network resilience to cascading overloads and network throughput is optimal for the same value of $\beta$ over the range of node capacities and available bandwidth. 	
1110.5246v1	http://arxiv.org/pdf/1110.5246v1	2011	Fluctuation-induced traffic congestion in heterogeneous networks	A. S. Stepanenko|I. V. Yurkevich|C. C. Constantinou|I. V. Lerner	  In studies of complex heterogeneous networks, particularly of the Internet, significant attention was paid to analyzing network failures caused by hardware faults or overload, where the network reaction was modeled as rerouting of traffic away from failed or congested elements. Here we model another type of the network reaction to congestion -- a sharp reduction of the input traffic rate through congested routes which occurs on much shorter time scales. We consider the onset of congestion in the Internet where local mismatch between demand and capacity results in traffic losses and show that it can be described as a phase transition characterized by strong non-Gaussian loss fluctuations at a mesoscopic time scale. The fluctuations, caused by noise in input traffic, are exacerbated by the heterogeneous nature of the network manifested in a scale-free load distribution. They result in the network strongly overreacting to the first signs of congestion by significantly reducing input traffic along the communication paths where congestion is utterly negligible. 	
1111.2452v1	http://arxiv.org/pdf/1111.2452v1	2011	A nonlinear symmetry breaking effect in shear cracks	Roi Harpaz|Eran Bouchbinder	  Shear cracks propagation is a basic dynamical process that mediates interfacial failure. We develop a general weakly nonlinear elastic theory of shear cracks and show that these experience tensile-mode crack tip deformation, including possibly opening displacements, in agreement with Stephenson's prediction. We quantify this nonlinear symmetry breaking effect, under two-dimensional deformation conditions, by an explicit inequality in terms of the first and second order elastic constants in the quasi-static regime and semi-analytic calculations in the fully dynamic regime. Our general results are applied to various materials. Finally, we discuss available works in the literature and note the potential relevance of elastic nonlinearities for frictional cracks. 	
1112.0387v2	http://arxiv.org/pdf/1112.0387v2	2012	Sandpiles on multiplex networks	Kyu-Min Lee|K. -I. Goh|I. -M. Kim	  We introduce the sandpile model on multiplex networks with more than one type of edge and investigate its scaling and dynamical behaviors. We find that the introduction of multiplexity does not alter the scaling behavior of avalanche dynamics; the system is critical with an asymptotic power-law avalanche size distribution with an exponent $\tau = 3/2$ on duplex random networks. The detailed cascade dynamics, however, is affected by the multiplex coupling. For example, higher-degree nodes such as hubs in scale-free networks fail more often in the multiplex dynamics than in the simplex network counterpart in which different types of edges are simply aggregated. Our results suggest that multiplex modeling would be necessary in order to gain a better understanding of cascading failure phenomena of real-world multiplex complex systems, such as the global economic crisis. 	
1112.2067v1	http://arxiv.org/pdf/1112.2067v1	2011	Ontology-Based Emergency Management System in a Social Cloud	Bhuvaneswari. A|Dr. G. R. Karpagam	  The need for Emergency Management continually grows as the population and exposure to catastrophic failures increase. The ability to offer appropriate services at these emergency situations can be tackled through group communication mechanisms. The entities involved in the group communication include people, organizations, events, locations and essential services. Cloud computing is a "as a service" style of computing that enables on-demand network access to a shared pool of resources. So this work focuses on proposing a social cloud constituting group communication entities using an open source platform, Eucalyptus. The services are exposed as semantic web services, since the availability of machine-readable metadata (Ontology) will enable the access of these services more intelligently. The objective of this paper is to propose an Ontology-based Emergency Management System in a social cloud and demonstrate the same using emergency healthcare domain. 	
1112.4231v2	http://arxiv.org/pdf/1112.4231v2	2012	Student Understanding of Taylor Series Expansions in Statistical   Mechanics	Trevor I. Smith|John R. Thompson|Donald B. Mountcastle	  One goal of physics instruction is to have students learn to make physical meaning of specific mathematical ideas, concepts, and procedures in different physical settings. As part of research investigating student learning in statistical physics, we are developing curriculum materials that guide students through a derivation of the Boltzmann factor, using a Taylor series expansion of entropy. Using results from written surveys, classroom observations, and both individual think-aloud and teaching interviews, we present evidence that many students can recognize and interpret series expansions, but they often lack fluency with the Taylor series despite previous exposures in both calculus and physics courses. We present students' successes and failures both using and interpreting Taylor series expansions in a variety of contexts. 	
1112.5667v1	http://arxiv.org/pdf/1112.5667v1	2011	Arithmetic of Potts model hypersurfaces	Matilde Marcolli|Jessica Su	  We consider Potts model hypersurfaces defined by the multivariate Tutte polynomial of graphs (Potts model partition function). We focus on the behavior of the number of points over finite fields for these hypersurfaces, in comparison with the graph hypersurfaces of perturbative quantum field theory defined by the Kirchhoff graph polynomial. We give a very simple example of the failure of the "fibration condition" in the dependence of the Grothendieck class on the number of spin states and of the polynomial countability condition for these Potts model hypersurfaces. We then show that a period computation, formally similar to the parametric Feynman integrals of quantum field theory, arises by considering certain thermodynamic averages. One can show that these evaluate to combinations of multiple zeta values for Potts models on polygon polymer chains, while silicate tetrahedral chains provide a candidate for a possible occurrence of non-mixed Tate periods. 	
1202.6049v1	http://arxiv.org/pdf/1202.6049v1	2012	Attack Detection and Identification in Cyber-Physical Systems -- Part   II: Centralized and Distributed Monitor Design	Fabio Pasqualetti|Florian Dörfler|Francesco Bullo	  Cyber-physical systems integrate computation, communication, and physical capabilities to interact with the physical world and humans. Besides failures of components, cyber-physical systems are prone to malicious attacks so that specific analysis tools and monitoring mechanisms need to be developed to enforce system security and reliability. This paper builds upon the results presented in our companion paper [1] and proposes centralized and distributed monitors for attack detection and identification. First, we design optimal centralized attack detection and identification monitors. Optimality refers to the ability of detecting (respectively identifying) every detectable (respectively identifiable) attack. Second, we design an optimal distributed attack detection filter based upon a waveform relaxation technique. Third, we show that the attack identification problem is computationally hard, and we design a sub-optimal distributed attack identification procedure with performance guarantees. Finally, we illustrate the robustness of our monitors to system noise and unmodeled dynamics through a simulation study. 	
1202.6144v2	http://arxiv.org/pdf/1202.6144v2	2012	Attack Detection and Identification in Cyber-Physical Systems -- Part I:   Models and Fundamental Limitations	Fabio Pasqualetti|Florian Dörfler|Francesco Bullo	  Cyber-physical systems integrate computation, communication, and physical capabilities to interact with the physical world and humans. Besides failures of components, cyber-physical systems are prone to malignant attacks, and specific analysis tools as well as monitoring mechanisms need to be developed to enforce system security and reliability. This paper proposes a unified framework to analyze the resilience of cyber-physical systems against attacks cast by an omniscient adversary. We model cyber-physical systems as linear descriptor systems, and attacks as exogenous unknown inputs. Despite its simplicity, our model captures various real-world cyber-physical systems, and it includes and generalizes many prototypical attacks, including stealth, (dynamic) false-data injection and replay attacks. First, we characterize fundamental limitations of static, dynamic, and active monitors for attack detection and identification. Second, we provide constructive algebraic conditions to cast undetectable and unidentifiable attacks. Third, by using the system interconnection structure, we describe graph-theoretic conditions for the existence of undetectable and unidentifiable attacks. Finally, we validate our findings through some illustrative examples with different cyber-physical systems, such as a municipal water supply network and two electrical power grids. 	
1203.1979v2	http://arxiv.org/pdf/1203.1979v2	2012	Icebergs in the Clouds: the Other Risks of Cloud Computing	Bryan Ford	  Cloud computing is appealing from management and efficiency perspectives, but brings risks both known and unknown. Well-known and hotly-debated information security risks, due to software vulnerabilities, insider attacks, and side-channels for example, may be only the "tip of the iceberg." As diverse, independently developed cloud services share ever more fluidly and aggressively multiplexed hardware resource pools, unpredictable interactions between load-balancing and other reactive mechanisms could lead to dynamic instabilities or "meltdowns." Non-transparent layering structures, where alternative cloud services may appear independent but share deep, hidden resource dependencies, may create unexpected and potentially catastrophic failure correlations, reminiscent of financial industry crashes. Finally, cloud computing exacerbates already-difficult digital preservation challenges, because only the provider of a cloud-based application or service can archive a "live," functional copy of a cloud artifact and its data for long-term cultural preservation. This paper explores these largely unrecognized risks, making the case that we should study them before our socioeconomic fabric becomes inextricably dependent on a convenient but potentially unstable computing model. 	
1203.2062v1	http://arxiv.org/pdf/1203.2062v1	2012	Meta-models for structural reliability and uncertainty quantification	Bruno Sudret	  A meta-model (or a surrogate model) is the modern name for what was traditionally called a response surface. It is intended to mimic the behaviour of a computational model M (e.g. a finite element model in mechanics) while being inexpensive to evaluate, in contrast to the original model which may take hours or even days of computer processing time. In this paper various types of meta-models that have been used in the last decade in the context of structural reliability are reviewed. More specifically classical polynomial response surfaces, polynomial chaos expansions and kriging are addressed. It is shown how the need for error estimates and adaptivity in their construction has brought this type of approaches to a high level of efficiency. A new technique that solves the problem of the potential biasedness in the estimation of a probability of failure through the use of meta-models is finally presented. 	
1203.6154v1	http://arxiv.org/pdf/1203.6154v1	2012	Finite element modelling of shock-induced damages on ceramic hip   prostheses	Juliana Uribe|Jérôme Hausselle|Jean Geringer	  The aim of this work was to simulate the behaviour of hip prostheses under mechanical shocks. When hip joint is replaced by prosthesis, during the swing phase of the leg, a microseparation between the prosthetic head and the cup could occur. Two different sizes of femoral heads were studied: 28 and 32 mm diameter, made, respectively, in alumina and zirconia. The shock-induced stress was determined numerically using finite element analysis (FEA), Abaqus software. The influence of inclination, force, material, and microseparation was studied. In addition, an algorithm was developed from a probabilistic model, Todinov's approach, to predict lifetime of head and cup. Simulations showed maximum tensile stresses were reached on the cup's surfaces near to rim. The worst case was the cup-head mounted at 30^{\circ}. All simulations and tests showed bulk zirconia had a greater resistance to shocks than bulk alumina. The probability of failure could be bigger than 0.9 when a porosity greater than 0.7% vol. is present in the material. Simulating results showed good agreement with experimental results. The tests and simulations are promising for predicting the lifetime of ceramic prostheses. 	
1204.3888v1	http://arxiv.org/pdf/1204.3888v1	2012	Sustainable institutionalized punishment requires elimination of   second-order free-riders	Matjaz Perc	  Although empirical and theoretical studies affirm that punishment can elevate collaborative efforts, its emergence and stability remain elusive. By peer-punishment the sanctioning is something an individual elects to do depending on the strategies in its neighborhood. The consequences of unsustainable efforts are therefore local. By pool-punishment, on the other hand, where resources for sanctioning are committed in advance and at large, the notion of sustainability has greater significance. In a population with free-riders, punishers must be strong in numbers to keep the "punishment pool" from emptying. Failure to do so renders the concept of institutionalized sanctioning futile. We show that pool-punishment in structured populations is sustainable, but only if second-order free-riders are sanctioned as well, and to a such degree that they cannot prevail. A discontinuous phase transition leads to an outbreak of sustainability when punishers subvert second-order free-riders in the competition against defectors. 	
1204.4822v2	http://arxiv.org/pdf/1204.4822v2	2012	Entropy production from stochastic dynamics in discrete full phase space	Ian J. Ford|Richard E. Spinney	  The stochastic entropy generated during the evolution of a system interacting with an environment may be separated into three components, but only two of these have a non-negative mean. The third component of entropy production is associated with the relaxation of the system probability distribution towards a stationary state and with nonequilibrium constraints within the dynamics that break detailed balance. It exists when at least some of the coordinates of the system phase space change sign under time reversal, and when the stationary state is asymmetric in these coordinates. We illustrate the various components of entropy production, both in detail for particular trajectories and in the mean, using simple systems defined on a discrete phase space of spatial and velocity coordinates. These models capture features of the drift and diffusion of a particle in a physical system, including the processes of injection and removal and the effect of a temperature gradient. The examples demonstrate how entropy production in stochastic thermodynamics depends on the detail that is included in a model of the dynamics of a process. Entropy production from such a perspective is a measure of the failure of such models to meet Loschmidt's expectation of dynamic reversibility. 	
1205.1428v1	http://arxiv.org/pdf/1205.1428v1	2012	High Velocity Penetration/Perforation Using Coupled Smooth Particle   Hydrodynamics-Finite Element Method	S. Swaddiwudhipong|M. J. Islam|Z. S. Liu	  Finite element method (FEM) suffers from a serious mesh distortion problem when used for high velocity impact analyses. The smooth particle hydrodynamics (SPH) method is appropriate for this class of problems involving severe damages but at considerable computational cost. It is beneficial if the latter is adopted only in severely distorted regions and FEM further away. The coupled smooth particle hydrodynamics - finite element method (SFM) has been adopted in a commercial hydrocode LS-DYNA to study the perforation of Weldox 460E steel and AA5083-H116 aluminum plates with varying thicknesses and various projectile nose geometries including blunt, conical and ogival noses. Effects of the SPH domain size and particle density are studied considering the friction effect between the projectile and the target materials. The simulated residual velocities and the ballistic limit velocities from the SFM agree well with the published experimental data. The study shows that SFM is able to emulate the same failure mechanisms of the steel and aluminum plates as observed in various experimental investigations for initial impact velocity of 170 m/s and higher. 	
1205.1826v1	http://arxiv.org/pdf/1205.1826v1	2012	Ion irradiation tolerance of graphene as studied by atomistic   simulations	E. H. Åhlgren|J. Kotakoski|O. Lehtinen|A. V. Krasheninnikov	  As impermeable to gas molecules and at the same time transparent to high-energy ions, graphene has been suggested as a window material for separating a high-vacuum ion beam system from targets kept at ambient conditions. However, accumulation of irradiation-induced damage in the graphene membrane may give rise to its mechanical failure. Using atomistic simulations, we demonstrate that irradiated graphene even with a high vacancy concentration does not show signs of such instability, indicating a considerable robustness of graphene windows. We further show that upper and lower estimates for the irradiation damage in graphene can be set using a simple model. 	
1205.6440v1	http://arxiv.org/pdf/1205.6440v1	2012	Monitoring Software Reliability using Statistical Process Control An   Ordered Statistics Approach	Bandla Srinivasa Rao|R. Satya Prasad|R. R. L. Kantham	  The nature and complexity of software have changed significantly in the last few decades. With the easy availability of computing power, deeper and broader applications are made. It has been extremely necessary to produce good quality software with high precession of reliability right in the first place. Olden day's software errors and bugs were fixed at a later stage in the software development. Today to produce high quality reliable software and to keep a specific time schedule is a big challenge. To cope up the challenge many concepts, methodology and practices of software engineering have been evolved for developing reliable software. Better methods of controlling the process of software production are underway. One of such methods to assess the software reliability is using control charts. In this paper we proposed an NHPP based control mechanism by using order statistics with cumulative quantity between observations of failure data using mean value function of exponential distribution. 	
1206.0103v1	http://arxiv.org/pdf/1206.0103v1	2012	Cooperation in Carrier Sense Based Wireless Ad Hoc Networks - Part I:   Reactive Schemes	Andrea Munari|Marco Levorato|Michele Zorzi	  Cooperative techniques have been shown to significantly improve the performance of wireless systems. Despite being a mature technology in single communication link scenarios, their implementation in wider, and practical, networks poses several challenges which have not been fully identified and understood so far. In this two-part paper, the implementation of cooperative communications in non-centralized ad hoc networks with sensing-based channel access is extensively discussed. Both analysis and simulation are employed to provide a clear understanding of the mutual influence between the link layer contention mechanism and collaborative protocols. Part I of this work focuses on reactive cooperation, in which relaying is triggered by packet delivery failure events, while Part II addresses proactive approaches, preemptively initiated by the source based on channel state information. Results show that sensing-based channel access significantly hampers the effectiveness of cooperation by biasing the spatial distribution of available relays, and by inducing a level of spatial and temporal correlation of the interference that diminishes the diversity improvement on which cooperative gains are founded. Moreover, the efficiency reduction entailed by several practical protocol issues related to carrier sense multiple access which are typically neglected in the literature is thoroughly investigated. 	
1206.1918v1	http://arxiv.org/pdf/1206.1918v1	2012	Routing Protocols for Mobile and Vehicular Ad-Hoc Networks: A   Comparative Analysis	Preetida Vinayakray-Jani|Sugata Sanyal	  We present comparative analysis of MANET (Mobile Ad-Hoc Network) and VANET (Vehicular Ad-Hoc Network) routing protocols, in this paper. The analysis is based on various design factors. The traditional routing protocols of AODV (Ad hoc On-Demand Distance Vector), DSR (Dynamic Source Routing), and DSDV (Destination-Sequenced Distance-Vector) of MANET are utilizing node centric routing which leads to frequent breaking of routes, causing instability in routing. Usage of these protocols in high mobility environment like VANET may eventually cause many packets to drop. Route repairs and failures notification overheads increase significantly leading to low throughput and long delays. Such phenomenon is not suitable for Vehicular Ad hoc Networks (VANET) due to high mobility of nodes where network can be dense or sparse. Researchers have proposed various routing algorithms or mechanism for MANET and VANET. This paper describes the relevant protocols, associated algorithm and the strength and weakness of these routing protocols. 	
1206.2187v1	http://arxiv.org/pdf/1206.2187v1	2012	An Empirical Study of the Repair Performance of Novel Coding Schemes for   Networked Distributed Storage Systems	Lluis Pamies-Juarez|Frédérique Oggier|Anwitaman Datta	  Erasure coding techniques are getting integrated in networked distributed storage systems as a way to provide fault-tolerance at the cost of less storage overhead than traditional replication. Redundancy is maintained over time through repair mechanisms, which may entail large network resource overheads. In recent years, several novel codes tailor-made for distributed storage have been proposed to optimize storage overhead and repair, such as Regenerating Codes that minimize the per repair traffic, or Self-Repairing Codes which minimize the number of nodes contacted per repair. Existing studies of these coding techniques are however predominantly theoretical, under the simplifying assumption that only one object is stored. They ignore many practical issues that real systems must address, such as data placement, de/correlation of multiple stored objects, or the competition for limited network resources when multiple objects are repaired simultaneously. This paper empirically studies the repair performance of these novel storage centric codes with respect to classical erasure codes by simulating realistic scenarios and exploring the interplay of code parameters, failure characteristics and data placement with respect to the trade-offs of bandwidth usage and speed of repairs. 	
1207.3822v1	http://arxiv.org/pdf/1207.3822v1	2012	Mechanical design of ceramic beam tube braze joints for NOvA kicker   magnets	C. R. Ader|R. E. Reilly|J. H. Wilson	  The NO{\nu}A Experiment will construct a detector optimized for electron neutrino detection in the existing NuMI neutrino beam. The NuMI beam line is capable of operating at 400 kW of primary beam power and the upgrade will allow up to 700 kW. Ceramic beam tubes are utilized in numerous kicker magnets in different accelerator rings at Fermi National Accelerator Laboratory. Kovar flanges are brazed onto each beam tube end, since kovar and high alumina ceramic have similar expansion curves. The tube, kovar flange, end piece, and braze foil alloy brazing material are stacked in the furnace and then brazed. The most challenging aspect of fabricating kicker magnets in recent years have been making hermetic vacuum seals on the braze joints between the ceramic and flange. Numerous process variables can influence the robustness of conventional metal/ceramic brazing processes. The ceramic-filler metal interface is normally the weak layer when failure does not occur within the ceramic. Differences between active brazing filler metal and the moly-manganese process will be discussed along with the applicable results of these techniques used for Fermilab production kicker tubes. 	
1208.4172v1	http://arxiv.org/pdf/1208.4172v1	2012	Transaction Log Based Application Error Recovery and Point In-Time Query	Tomas Talius|Robin Dhamankar|Andrei Dumitrache|Hanuma Kodavalla	  Database backups have traditionally been used as the primary mechanism to recover from hardware and user errors. High availability solutions maintain redundant copies of data that can be used to recover from most failures except user or application errors. Database backups are neither space nor time efficient for recovering from user errors which typically occur in the recent past and affect a small portion of the database. Moreover periodic full backups impact user workload and increase storage costs. In this paper we present a scheme that can be used for both user and application error recovery starting from the current state and rewinding the database back in time using the transaction log. While we provide a consistent view of the entire database as of a point in time in the past, the actual prior versions are produced only for data that is accessed. We make the as of data accessible to arbitrary point in time queries by integrating with the database snapshot feature in Microsoft SQL Server. 	
1210.5913v1	http://arxiv.org/pdf/1210.5913v1	2012	Bio-Thentic Card: Authentication concept for RFID Card	Ikuesan R. Adeyemi|Norafida Bt Ithnin	  Radio frequency identification (RFID) is a technology that employs basic identifier of an object embedded in a chip, transmitted via radio wave, for identification. An RFID Card responds to query or interrogation irrespective of "Who" holds the Card; like a key to a door. Since an attacker can possess the card, access to such object can therefore be easily compromised. This security breach is classified as an unauthorized use of Card, and it forms the bedrock for RFID Card compromise especially in access control. As an on-card authentication mechanism, this research proposed a concept termed Bio-Thentic Card, which can be adopted to prevent this single point of failure of RFID Card. The Bio-Thentic Card was fabricated, tested and assessed in line with the known threats, and attacks; and it was observed to proffer substantive solution to unauthorized use of RFID Card vulnerability 	
1210.8372v1	http://arxiv.org/pdf/1210.8372v1	2012	Electronic Strengthening of Graphene by Charge Doping	Chen Si|Wenhui Duan|Zheng Liu|Feng Liu	  Graphene is known as the strongest 2D material in nature, yet we show that moderate charge doping of either electrons or holes can further enhance its ideal strength by up to ~17%, based on first principles calculations. This unusual electronic enhancement, versus conventional structural enhancement, of material's strength is achieved by an intriguing physical mechanism of charge doping counteracting on strain induced enhancement of Kohn anomaly, which leads to an overall stiffening of zone boundary K1 phonon mode whose softening under strain is responsible for graphene failure. Electrons and holes work in the same way due to the high electron-hole symmetry around the Dirac point of graphene, while over doping may weaken the graphene by softening other phonon modes. Our findings uncover another fascinating property of graphene with broad implications in graphene-based electromechanical devices. 	
1211.6796v1	http://arxiv.org/pdf/1211.6796v1	2012	Polymer Welding: Strength Through Entanglements	Ting Ge|Flint Pierce|Dvora Perahia|Gary S. Grest|Mark O. Robbins	  Large-scale simulations of thermal welding of polymers are performed to investigate the rise of mechanical strength at the polymer-polymer interface with the welding time. The welding process is in the core of integrating polymeric elements into devices as well as in thermal induced healing of polymers; processes that require development of interfacial strength equal to that of the bulk. Our simulations show that the interfacial strength saturates at the bulk shear strength much before polymers diffuse by their radius of gyration. Along with the strength increase, the dominant failure mode changes from chain pullout at the interface to chain scission as in the bulk. Formation of sufficient entanglements across the interface, which we track using a Primitive Path Analysis is required to arrest catastrophic chain pullout at the interface. The bulk response is not fully recovered until the density of entanglements at the interface reaches the bulk value. Moreover, the increase of interfacial strength before saturation is proportional to the number of interfacial entanglements between chains from opposite sides. 	
1212.1551v2	http://arxiv.org/pdf/1212.1551v2	2013	From microstructural features to effective toughness in disordered   brittle solids	Vincent Démery|Laurent Ponson|Alberto Rosso	  The relevant parameters at the microstructure scale that govern the macroscopic toughness of disordered brittle materials are investigated theoretically. We focus on planar crack propagation and describe the front evolution as the propagation of a long-range elastic line within a plane with random distribution of toughness. Our study reveals two regimes: in the collective pinning regime, the macroscopic toughness can be expressed as a function of a few parameters only, namely the average and the standard deviation of the local toughness distribution and the correlation lengths of the heterogeneous toughness field; in the individual pinning regime, the passage from micro to macroscale is more subtle and the full distribution of local toughness is required to be predictive. Beyond the failure of brittle solids, our findings illustrate the complex filtering process of microscale quantities towards the larger scales into play in a broad range of systems governed by the propagation of an elastic interface in a disordered medium. 	
1301.6115v1	http://arxiv.org/pdf/1301.6115v1	2013	DebtRank-transparency: Controlling systemic risk in financial networks	Stefan Thurner|Sebastian Poledna	  Banks in the interbank network can not assess the true risks associated with lending to other banks in the network, unless they have full information on the riskiness of all the other banks. These risks can be estimated by using network metrics (for example DebtRank) of the interbank liability network which is available to Central Banks. With a simple agent based model we show that by increasing transparency by making the DebtRank of individual nodes (banks) visible to all nodes, and by imposing a simple incentive scheme, that reduces interbank borrowing from systemically risky nodes, the systemic risk in the financial network can be drastically reduced. This incentive scheme is an effective regulation mechanism, that does not reduce the efficiency of the financial network, but fosters a more homogeneous distribution of risk within the system in a self-organized critical way. We show that the reduction of systemic risk is to a large extent due to the massive reduction of cascading failures in the transparent system. An implementation of this minimal regulation scheme in real financial networks should be feasible from a technical point of view. 	
1301.6375v3	http://arxiv.org/pdf/1301.6375v3	2013	Increased Network Interdependency Leads to Aging	Dervis Can Vural|Greg Morrison|L. Mahadevan	  Although species longevity is subject to a diverse range of selective forces, the mortality curves of a wide variety of organisms are rather similar. We argue that aging and its universal characteristics may have evolved by means of a gradual increase in the systemic interdependence between a large collection of biochemical or mechanical components. Modeling the organism as a dependency network which we create using a constructive evolutionary process, we age it by allowing nodes to be broken or repaired according to a probabilistic algorithm that accounts for random failures/repairs and dependencies. Our simulations show that the network slowly accumulates damage and then catastrophically collapses. We use our simulations to fit experimental data for the time dependent mortality rates of a variety of multicellular organisms and even complex machines such as automobiles. Our study suggests that aging is an emergent finite-size effect in networks with dynamical dependencies and that the qualitative and quantitative features of aging are not sensitively dependent on the details of system structure. 	
1302.2418v1	http://arxiv.org/pdf/1302.2418v1	2013	Effective Mean Field Approach to Kinetic Monte Carlo Simulations in   Limit Cycle Dynamics with Reactive and Diffusive Rewiring	E. Panagakou|G. C. Boulougouris|A. Provata	  The dynamics of complex reactive schemes is known to deviate from the Mean Field (MF) theory when restricted on low dimensional spatial supports. This failure has been attributed to the limited number of species-neighbours which are available for interactions. In the current study, we introduce effective reactive parameters, which depend on the type of the spatial support and which allow for an effective MF description. As working example the Lattice Limit Cycle dynamics is used, restricted on a 2D square lattice with nearest neighbour interactions. We show that the MF steady state results are recovered when the kinetic rates are replaced with their effective values. The same conclusion holds when reactive stochastic rewiring is introduced in the system via long distance reactive coupling. Instead, when the stochastic coupling becomes diffusive the effective parameters no longer predict the steady state. This is attributed to the diffusion process which is an additional factor introduced into the dynamics and is not accounted for, in the kinetic MF scheme. 	
1303.0965v1	http://arxiv.org/pdf/1303.0965v1	2013	On the validity of the method of reduction of dimensionality: area of   contact, average interfacial separation and contact stiffness	I. A. Lyashenko|Lars Pastewka|Bo N. J. Persson	  It has recently been suggested that many contact mechanics problems between solids can be accurately studied by mapping the problem on an effective one dimensional (1D) elastic foundation model. Using this 1D mapping we calculate the contact area and the average interfacial separation between elastic solids with nominally flat but randomly rough surfaces. We show, by comparison to exact numerical results, that the 1D mapping method fails even qualitatively. We also calculate the normal interfacial stiffness $K$ and compare it with the result of an analytical study. We attribute the failure of the elastic foundation model to the neglect of the long-range elastic coupling between the asperity contact regions. 	
1304.2362v1	http://arxiv.org/pdf/1304.2362v1	2013	A Comparison of Decision Analysis and Expert Rules for Sequential   Diagnosis	Jayant Kalagnanam|Max Henrion	  There has long been debate about the relative merits of decision theoretic methods and heuristic rule-based approaches for reasoning under uncertainty. We report an experimental comparison of the performance of the two approaches to troubleshooting, specifically to test selection for fault diagnosis. We use as experimental testbed the problem of diagnosing motorcycle engines. The first approach employs heuristic test selection rules obtained from expert mechanics. We compare it with the optimal decision analytic algorithm for test selection which employs estimated component failure probabilities and test costs. The decision analytic algorithm was found to reduce the expected cost (i.e. time) to arrive at a diagnosis by an average of 14% relative to the expert rules. Sensitivity analysis shows the results are quite robust to inaccuracy in the probability and cost estimates. This difference suggests some interesting implications for knowledge acquisition. 	
1305.5525v1	http://arxiv.org/pdf/1305.5525v1	2013	Time in Quantum Mechanics	Curt A. Moyer	  The failure of conventional quantum theory to recognize time as an observable and to admit time operators is addressed. Instead of focusing on the existence of a time operator for a given Hamiltonian, we emphasize the role of the Hamiltonian as the generator of translations in time to construct time states. Taken together, these states constitute what we call a timeline, or quantum history, that is adequate for the representation of any physical state of the system. Such timelines appear to exist even for the semi-bounded and discrete Hamiltonian systems ruled out by Pauli's theorem. However, the step from a timeline to a valid time operator requires additional assumptions that are not always met. Still, this approach illuminates the crucial issue surrounding the construction of time operators, and establishes quantum histories as legitimate alternatives to the familiar coordinate and momentum bases of standard quantum theory. 	
1306.3704v1	http://arxiv.org/pdf/1306.3704v1	2013	How interbank lending amplifies overlapping portfolio contagion: A case   study of the Austrian banking network	Fabio Caccioli|J. Doyne Farmer|Nick Foti|Daniel Rockmore	  In spite of the growing theoretical literature on cascades of failures in interbank lending networks, empirical results seem to suggest that networks of direct exposures are not the major channel of financial contagion. In this paper we show that networks of interbank exposures can however significantly amplify contagion due to overlapping portfolios. To illustrate this point, we consider the case of the Austrian interbank network and perform stress tests on it according to different protocols. We consider in particular contagion due to (i) counterparty loss; (ii) roll-over risk; and (iii) overlapping portfolios. We find that the average number of bankruptcies caused by counterparty loss and roll-over risk is fairly small if these contagion mechanisms are considered in isolation. Once portfolio overlaps are also accounted for, however, we observe that the network of direct interbank exposures significantly contributes to systemic risk. 	
1306.5448v2	http://arxiv.org/pdf/1306.5448v2	2013	On the efficiency at maximum cooling power	Yann Apertet|Henni Ouerdane|Aurelie Michot|Christophe Goupil|Philippe Lecoeur	  The efficiency at maximum power (EMP) of heat engines operating as generators is one corner stone of finite-time thermodynamics, the Curzon-Ahlborn efficiency $\eta_{\rm CA}$ being considered as a universal upper bound. Yet, no valid counterpart to $\eta_{\rm CA}$ has been derived for the efficiency at maximum cooling power (EMCP) for heat engines operating as refrigerators. In this Letter we analyse the reasons of the failure to obtain such a bound and we demonstrate that, despite the introduction of several optimisation criteria, the maximum cooling power condition should be considered as the genuine equivalent of maximum power condition in the finite-time thermodynamics frame. We then propose and discuss an analytic expression for the EMCP in the specific case of exoreversible refrigerators. 	
1307.2466v2	http://arxiv.org/pdf/1307.2466v2	2013	Tearing of Free-Standing Graphene	Maria J. B. Moura|Michael Marder	  We examine the fracture mechanics of tearing graphene. We present a molecular dynamics simulation of the propagation of cracks in clamped, free-standing graphene as a function of the out-of-plane force. The geometry is motivated by experimental configurations that expose graphene sheets to out-of-plane forces, such as back-gate voltage. We establish the geometry and basic energetics of failure, and obtain approximate analytical expressions for critical crack lengths and forces. We also propose a method to obtain graphene's toughness. We observe that the cracks' path and the edge structure produced are dependent on the initial crack length. This work may help avoid the tearing of graphene sheets and aid the production of samples with specific edge structures. 	
1308.4302v2	http://arxiv.org/pdf/1308.4302v2	2013	Shocks Generate Crossover Behaviour In Lattice Avalanches	James Burridge	  A spatial avalanche model is introduced, in which avalanches increase stability in the regions where they occur. Instability is driven globally by a driving process that contains shocks. The system is typically subcritical, but the shocks occasionally lift it into a near or super critical state from which it rapidly retreats due to large avalanches. These shocks leave behind a signature -- a distinct power--law crossover in the avalanche size distribution. The model is inspired by landslide field data, but the principles may be applied to any system that experiences stabilizing failures, possesses a critical point, and is subject to an ongoing process of destabilization which includes occasional dramatic destabilizing events. 	
1309.3660v1	http://arxiv.org/pdf/1309.3660v1	2013	(Failure of the) Wisdom of the crowds in an endogenous opinion dynamics   model with multiply biased agents	Steffen Eger	  We study an endogenous opinion (or, belief) dynamics model where we endogenize the social network that models the link (`trust') weights between agents. Our network adjustment mechanism is simple: an agent increases her weight for another agent if that agent has been close to truth (whence, our adjustment criterion is `past performance'). Moreover, we consider multiply biased agents that do not learn in a fully rational manner but are subject to persuasion bias - they learn in a DeGroot manner, via a simple `rule of thumb' - and that have biased initial beliefs. In addition, we also study this setup under conformity, opposition, and homophily - which are recently suggested variants of DeGroot learning in social networks - thereby taking into account further biases agents are susceptible to. Our main focus is on crowd wisdom, that is, on the question whether the so biased agents can adequately aggregate dispersed information and, consequently, learn the true states of the topics they communicate about. In particular, we present several conditions under which wisdom fails. 	
1309.5960v4	http://arxiv.org/pdf/1309.5960v4	2014	Warm dark matter does not do better than cold dark matter in solving   small-scale inconsistencies	Aurel Schneider|Donnino Anderhalden|Andrea Maccio|Juerg Diemand	  Over the last decade, warm dark matter (WDM) has been repeatedly proposed as an alternative scenario to the standard cold dark matter (CDM) one, potentially resolving several disagreements between the CDM model and observations on small scales. Here, we reconsider the most important CDM small-scale discrepancies in the light of recent observational constraints on WDM. As a result, we find that a conventional thermal (or thermal-like) WDM cosmology with a particle mass in agreement with Lyman-$\alpha$ is nearly indistinguishable from CDM on the relevant scales and therefore fails to alleviate any of the small-scale problems. The reason for this failure is that the power spectrum of conventional WDM falls off too rapidly. To maintain WDM as a significantly different alternative to CDM, more evolved production mechanisms leading to multiple dark matter components or a gradually decreasing small-scale power spectrum have to be considered. 	
1309.7795v2	http://arxiv.org/pdf/1309.7795v2	2015	Condensation phenomena in fat-tailed distributions: a characterization   by means of an order parameter	Mario Filiasi|Elia Zarinelli|Erik Vesselli|Matteo Marsili	  Condensation phenomena are ubiquitous in nature and are found in condensed matter, disordered systems, networks, finance, etc. In the present work we investigate one of the best frameworks in which condensation phenomena take place, namely, the sum of independent and fat-tailed distributed random variables. For large deviations of the sum, this system undergoes a phase transition and shifts from a democratic phase to a condensed phase, where a single variable (the condensate) carries a finite fraction of the sum. This phenomenon yields the failure of the standard results of the Large Deviation Theory. In this work we exploit the Density Functional Method to overcome the limitation of the Large Deviation Theory and characterize the condensation transition in terms of an order parameter, i.e. the Inverse Participation Ratio (IPR). This procedure leads us to investigate the system in the large-deviation regime where both the sum and the IPR are constrained, observing new phase transitions. As a sample application, the case of condensation phenomena in financial time-series is briefly discussed. 	
1310.0996v1	http://arxiv.org/pdf/1310.0996v1	2013	Spatially localized attacks on interdependent networks: the existence of   a finite critical attack size	Yehiel Berezin|Amir Bashan|Michael M. Danziger|Daqing Li|Shlomo Havlin	  Many real world complex systems such as infrastructure, communication and transportation networks are embedded in space, where entities of one system may depend on entities of other systems. These systems are subject to geographically localized failures due to malicious attacks or natural disasters. Here we study the resilience of a system composed of two interdependent spatially embedded networks to localized geographical attacks. We find that if an attack is larger than a finite (zero fraction of the system) critical size, it will spread through the entire system and lead to its complete collapse. If the attack is below the critical size, it will remain localized. In contrast, under random attack a finite fraction of the system needs to be removed to initiate system collapse. We present both numerical simulations and a theoretical approach to analyze and predict the effect of local attacks and the critical attack size. Our results demonstrate the high risk of local attacks on interdependent spatially embedded infrastructures and can be useful for designing more resilient systems. 	
1310.2702v3	http://arxiv.org/pdf/1310.2702v3	2014	Emergent irreversibility and entanglement spectrum statistics	Claudio Chamon|Alioscia Hamma|Eduardo R. Mucciolo	  We study the problem of irreversibility when the dynamical evolution of a many-body system is described by a stochastic quantum circuit. Such evolution is more general than a Hamiltonian one, and since energy levels are not well defined, the well-established connection between the statistical fluctuations of the energy spectrum and irreversibility cannot be made. We show that the entanglement spectrum provides a more general connection. Irreversibility is marked by a failure of a disentangling algorithm and is preceded by the appearance of Wigner-Dyson statistical fluctuations in the entanglement spectrum. This analysis can be done at the wave-function level and offers an alternative route to study quantum chaos and quantum integrability. 	
1310.3882v1	http://arxiv.org/pdf/1310.3882v1	2013	Structure and Strength at Immiscible Polymer Interfaces	Ting Ge|Gary S. Grest|Mark O. Robbins	  Thermal welding of polymer-polymer interfaces is important for integrating polymeric elements into devices. When two different polymers are joined, the strength of the weld depends critically on the degree of immiscibility. We perform large-scale molecular dynamics simulations of the structure-strength relation at immiscible polymer interfaces. Our simulations show that immiscibility arrests interdiffusion and limits the equilibrium interfacial width. Even for weakly immiscible films, the narrow interface is unable to transfer stress upon deformation as effectively as the bulk material, and chain pullout at the interface becomes the dominant failure mechanism. This greatly reduces the interfacial strength. The weak response of immiscible interfaces is shown to arise from an insufficient density of entanglements across the interface. We demonstrate that there is a threshold interfacial width below which no significant entanglements can form between opposite sides to strengthen the interface. 	
1310.8293v1	http://arxiv.org/pdf/1310.8293v1	2013	Dimensions, Structures and Security of Networks	Angsheng Li|Wei Zhang|Yicheng Pan	  One of the main issues in modern network science is the phenomenon of cascading failures of a small number of attacks. Here we define the dimension of a network to be the maximal number of functions or features of nodes of the network. It was shown that there exist linear networks which are provably secure, where a network is linear, if it has dimension one, that the high dimensions of networks are the mechanisms of overlapping communities, that overlapping communities are obstacles for network security, and that there exists an algorithm to reduce high dimensional networks to low dimensional ones which simultaneously preserves all the network properties and significantly amplifies security of networks. Our results explore that dimension is a fundamental measure of networks, that there exist linear networks which are provably secure, that high dimensional networks are insecure, and that security of networks can be amplified by reducing dimensions. 	
1312.7530v2	http://arxiv.org/pdf/1312.7530v2	2014	Heisenberg Uncertainty Relation Revisited	Kazuo Fujikawa	  It is shown that all the known uncertainty relations are the secondary consequences of Robertson's relation. The basic idea is to use the Heisenberg picture so that the time development of quantum mechanical operators incorporate the effects of the measurement interaction. A suitable use of triangle inequalities then gives rise to various forms of uncertainty relations. The assumptions of unbiased measurement and unbiased disturbance are important to simplify the resulting uncertainty relations and to give the familiar uncertainty relations such as a naive Heisenberg error-disturbance relation. These simplified uncertainty relations are however valid only conditionally. Quite independently of uncertainty relations, it is shown that the notion of precise measurement is incompatible with the assumptions of unbiased measurement and unbiased disturbance. We can thus naturally understand the failure of the naive Heisenberg's error-disturbance relation, as was demonstrated by the recent spin-measurement by J. Erhart, et al.. 	
1401.0592v1	http://arxiv.org/pdf/1401.0592v1	2014	The influence of van der Waals forces on the waveguide deformation and   power limit of nanoscale optomechanical systems	Fei Xu|Bi-cai Zheng|WEi Luo|Yan-qing Lu	  The ultra-short range force, van der Waals force (VWF), will rise rapidly when one nanoscale waveguide is close to another one, and be stronger than the external transverse gradient force (TGF). We theoretically investigate the giant influence of the VWF on the device performance in a typical optomechanical system consisting of a suspended silicon waveguide and a silica substrate including waveguide deformation stiction and failure mechanism. The device shows unique optically-activated plastic/elastic behaviors and stiction due to the VWF. When the input optical power is above the critical power, the waveguide is sticking to the substrate and the deformation is plastic and unrecoverable, even though the total force is less than the yield strength of the waveguide material. This is important and helpful for the design and applications of optomechanical devices. 	
1401.3281v1	http://arxiv.org/pdf/1401.3281v1	2014	A Creepy World	Didier Sornette|Peter Cauwels	  Using the mechanics of creep in material sciences as a metaphor, we present a general framework to understand the evolution of financial, economic and social systems and to construct scenarios for the future. In a nutshell, highly non-linear out-of-equilibrium systems subjected to exogenous perturbations tend to exhibit a long phase of slow apparent stable evolution, which are nothing but slow maturations towards instabilities, failures and changes of regimes. With examples from history where a small event had a cataclysmic consequence, we propose a novel view of the current state of the world via the logical scenarios that derive, avoiding the traps of an illusionary stability and simple linear extrapolation. The endogenous scenarios are "muddling along", "managing through" and "blood red abyss". The exogenous scenarios are "painful adjustment" and "golden east". 	
1401.6807v1	http://arxiv.org/pdf/1401.6807v1	2014	Nonconvex bundle method with application to a delamination problem	M. N. Dao|J. Gwinner|D. Noll|N. Ovcharova	  Delamination is a typical failure mode of composite materials caused by weak bonding. It arises when a crack initiates and propagates under a destructive loading. Given the physical law characterizing the properties of the interlayer adhesive between the bonded bodies, we consider the problem of computing the propagation of the crack front and the stress field along the contact boundary. This leads to a hemivariational inequality, which after discretization by finite elements we solve by a nonconvex bundle method, where upper-$C^1$ criteria have to be minimized. As this is in contrast with other classes of mechanical problems with non-monotone friction laws and in other applied fields, where criteria are typically lower-$C^1$, we propose a bundle method suited for both types of nonsmoothness. We prove its global convergence in the sense of subsequences and test it on a typical delamination problem of material sciences. 	
1401.8131v1	http://arxiv.org/pdf/1401.8131v1	2014	Failure Detection and Recovery in Hierarchical Network Using FTN   Approach	Bhagvan Krishna Gupta|Ankit Mundra|Nitin Rakesh	  In current scenario several commercial and social organizations are using computer networks for their business and management purposes. In order to meet the business requirements networks are also grow. The growth of network also promotes the handling capability of large networks because it counter raises the possibilities of various faults in the network. A fault in network degrades its performance by affecting parameters like throughput, delay, latency, reliability etc. In hierarchical network models any possibility of fault may collapse entire network. If a fault occurrence disables a device in hierarchical network then it may distresses all the devices underneath. Thus it affects entire networks performance. In this paper we propose Fault Tolerable hierarchical Network (FTN) approach as a solution to the problems of hierarchical networks. The proposed approach firstly detects possibilities of fault in the network and accordingly provides specific recovery mechanism. We have evaluated the performance of FTN approach in terms of delay and throughput of network. 	
1402.2057v2	http://arxiv.org/pdf/1402.2057v2	2014	Robustness of complex many-body networks: Novel perspective in 2D   metal-insulator transition	Chung-Pin Chou	  We present a novel theoretical framework established by complex network analysis for understanding the phase transition beyond the Landau symmetry breaking paradigm. In this paper we take a two-dimensional metal-insulator transition driven by electron correlations for example. Passing through the transition point, we find a hidden symmetry broken in the network space, which is invisible in real space. This symmetry is nothing but a kind of robustness of the network to random failures. We then show that a network quantity, small-worldness, is capable of identifying the phase transition with/without any symmetry breaking in the real space and behaving as a new order parameter in the network space. We demonstrate that whether or not the symmetry is broken in real space a variety of phase transitions in condensed matters can be characterized by the hidden symmetry breaking in the weighted network, that is to say, a decline in network robustness. 	
1402.5342v2	http://arxiv.org/pdf/1402.5342v2	2014	Scattering nonlocality in quantum charge transport: Application to   semiconductor nanostructures	Roberto Rosati|Fausto Rossi	  Our primary goal is to provide a rigorous treatment of scattering nonlocality in semiconductor nanostructures. On the one hand, starting from the conventional density-matrix formulation and employing as ideal instrument for the study of the semiclassical limit the well-known Wigner-function picture, we shall perform a fully quantum-mechanical derivation of the space-dependent Boltzmann equation. On the other hand, we shall examine the validity limits of such semiclassical framework, pointing out, in particular, regimes where scattering-nonlocality effects may play a relevant role; to this end we shall supplement our analytical investigation with a number of simulated experiments, discussing and further expanding preliminary studies of scattering-induced quantum diffusion in GaN-based nanomaterials. As for the case of carrier-carrier relaxation in photoexcited semiconductors, our analysis will show the failure of simplified dephasing models in describing phonon-induced scattering nonlocality, pointing out that such limitation is particularly severe for the case of quasielastic dissipation processes. 	
1402.6228v2	http://arxiv.org/pdf/1402.6228v2	2014	Numerical evidence for nucleated self-assembly of DNA brick structures	Aleks Reinhardt|Daan Frenkel	  The observation by Ke et al. [Science 338, 1177 (2012)] that large numbers of short, pre-designed DNA strands can assemble into three-dimensional target structures came as a great surprise, as no colloidal self-assembling system has ever achieved the same degree of complexity. That failure seemed easy to rationalise: the larger the number of distinct building blocks, the higher the expected error rate for self-assembly. The experiments of Ke et al. have disproved this argument. Here, we report Monte Carlo simulations of the self-assembly of a DNA brick cube, comprising approximately 1000 types of DNA strand, using a simple model. We model the DNA strands as lattice tetrahedra with attractive patches, the interaction strengths of which are computed using a standard thermodynamic model. We find that, within a narrow temperature window, the target structure assembles with high probability. Our simulations suggest that mis-assembly is disfavoured because of a slow nucleation step. As our model incorporates no aspect of DNA other than its binding properties, these simulations suggest that, with proper design of the building blocks, other systems, such as colloids, may also assemble into truly complex structures. 	
1403.1887v2	http://arxiv.org/pdf/1403.1887v2	2015	Ultra-ductile and low friction epoxy matrix composites	Anderson O. Okonkwo|Pravin Jagadale|John E. García Herrera|Viktor G. Hadjiev|Juan Muñoz Saldaña|Alberto Taglafierro|Francisco C. Robles Hernandez	  We present the results of an effective reinforcement of epoxy resin matrix with fullerene carbon soot. The optimal carbon soot addition of 1 wt. % results in a toughness improvement of almost 20 times. The optimized soot-epoxy composites also show an increase in tensile elongation of more than 13 %, thus indicating a change of the failure mechanism in tension from brittle to ductile. Additionally, the coefficient of friction is reduced from its 0.91 value in plain epoxy resin to 0.15 in the optimized composite. In the optimized composite, the lateral forces during nanoscratching decrease as much as 80 % with enhancement of the elastic modulus and hardness by 43 % and 94%, respectively. The optimized epoxy resin fullerene soot composite can be a strong candidate for coating applications where toughness, low friction, ductility and light weight are important. 	
1403.2599v2	http://arxiv.org/pdf/1403.2599v2	2014	Anderson localization and momentum-space entanglement	Eric C. Andrade|Mark Steudtner|Matthias Vojta	  We consider Anderson localization and the associated metal-insulator transition for non-interacting fermions in D = 1, 2 space dimensions in the presence of spatially correlated on-site random potentials. To assess the nature of the wavefunction, we follow a recent proposal to study momentum-space entanglement. For a D = 1 model with long-range disorder correlations, both the entanglement spectrum and the entanglement entropy allow us to clearly distinguish between extended and localized states based upon a single realization of disorder. However, for other models including the D = 2 case with long-range correlated disorder, we find that the method is not similarly successful. We analyze the reasons for its failure, concluding that the much desired generalization to higher dimensions may be problematic. 	
1404.6790v2	http://arxiv.org/pdf/1404.6790v2	2014	Assessing T cell clonal size distribution: a non-parametric approach	O. V. Bolkhovskaya|D. Yu. Zorin|M. V. Ivanchenko	  Clonal structure of the human peripheral T-cell repertoire is shaped by a number of homeostatic mechanisms, including antigen presentation, cytokine and cell regulation. Its accurate tuning leads to a remarkable ability to combat pathogens in all their variety, while systemic failures may lead to severe consequences like autoimmune diseases. Here we develop and make use of a non-parametric statistical approach to assess T cell clonal size distributions from recent next generation sequencing data. For 41 healthy individuals and a patient with ankylosing spondylitis, who undergone treatment, we invariably find power law scaling over several decades and for the first time calculate quantitatively meaningful values of decay exponent. It has proved to be much the same among healthy donors, significantly different for an autoimmune patient before the therapy, and converging towards a typical value afterwards. We discuss implications of the findings for theoretical understanding and mathematical modeling of adaptive immunity. 	
1405.0483v2	http://arxiv.org/pdf/1405.0483v2	2014	Percolation on sparse networks	Brian Karrer|M. E. J. Newman|Lenka Zdeborová	  We study percolation on networks, which is used as a model of the resilience of networked systems such as the Internet to attack or failure and as a simple model of the spread of disease over human contact networks. We reformulate percolation as a message passing process and demonstrate how the resulting equations can be used to calculate, among other things, the size of the percolating cluster and the average cluster size. The calculations are exact for sparse networks when the number of short loops in the network is small, but even on networks with many short loops we find them to be highly accurate when compared with direct numerical simulations. By considering the fixed points of the message passing process, we also show that the percolation threshold on a network with few loops is given by the inverse of the leading eigenvalue of the so-called non-backtracking matrix. 	
1405.0528v2	http://arxiv.org/pdf/1405.0528v2	2014	Brightest Cluster Galaxies in Cosmological Simulations with Adaptive   Mesh Refinement: Successes and Failures	Davide Martizzi| Jimmy|Romain Teyssier|Ben Moore	  A large sample of cosmological hydrodynamical zoom-in simulations with Adaptive Mesh Refinement (AMR) is analysed to study the properties of simulated Brightest Cluster Galaxies (BCGs). Following the formation and evolution of BCGs requires modeling an entire galaxy cluster, because the BCG properties are largely influenced by the state of the gas in the cluster and by interactions and mergers with satellites. BCG evolution is also deeply influenced by the presence of gas heating sources such as Active Galactic Nuclei (AGNs) that prevent catastrophic cooling of large amounts of gas. We show that AGN feedback is one of the most important mechanisms in shaping the properties of BCGs at low redshift by analysing our statistical sample of simulations with and without AGN feedback. When AGN feedback is included BCG masses, sizes, star formation rates and kinematic properties are closer to those of the observed systems. Some small discrepancies are observed only for the most massive BCGs and in the fraction of star-forming BCGs, effects that might be due to physical processes that are not included in our model. 	
1405.0932v1	http://arxiv.org/pdf/1405.0932v1	2014	Experimental investigation of the elastoplastic response of aluminum   silicate spray dried powder during cold compaction	F. Bosi|A. Piccolroaz|M. Gei|F. Dal Corso|A. Cocquio|D. Bigoni	  Mechanical experiments have been designed and performed to investigate the elasto-plastic behaviour of green bodies formed from an aluminum silicate spray dried powder used for tiles production. Experiments have been executed on samples obtained from cold compaction into a cylindrical mould and include: uniaxial strain, equi-biaxial flexure and high-pressure triaxial compression/extension tests. Two types of powders have been used to realize the green body samples, differing in the values of water content, which have been taken equal to those usually employed in the industrial forming of traditional ceramics. Yielding of the green body during compaction has been characterized in terms of yield surface shape, failure envelope, and evolution of cohesion and void ratio with the forming pressure, confirming the validity of previously proposed constitutive models for dense materials obtained through cold compaction of granulates. 	
1405.1385v1	http://arxiv.org/pdf/1405.1385v1	2014	Quasi Steady-State Model for Power System Stability: Limitations,   Analysis and a Remedy	Xiaozhe Wang|Hsiao-Dong Chiang	  The quasi steady-state (QSS) model tries to reach a good compromise between accuracy and efficiency in long-term stability analysis. However, the QSS model is unable to provide correct approximations and stability assessment for the long-term stability model consistently. In this paper, some numerical examples in which the QSS model was stable while the long-term stability model underwent instabilities are presented with analysis in nonlinear system framework. At the same time, a hybrid model which serves as a remedy to the QSS model is proposed according to causes for failure of the QSS model and dynamic mechanisms of long-term instabilities. Numerical examples are given to show that the developed hybrid model can successfully capture unstable behaviors of the long-term stability model while the QSS model fails. 	
1405.2843v2	http://arxiv.org/pdf/1405.2843v2	2014	Correlations after quantum quenches in the XXZ spin chain: Failure of   the Generalized Gibbs Ensemble	B. Pozsgay|M. Mestyán|M. A. Werner|M. Kormos|G. Zaránd|G. Takács	  We study the nonequilibrium time evolution of the spin-1/2 anisotropic Heisenberg (XXZ) spin chain, with a choice of dimer product and Neel states as initial states. We investigate numerically various short-ranged spin correlators in the long-time limit and find that they deviate significantly from predictions based on the generalized Gibbs ensemble (GGE) hypotheses. By computing the asymptotic spin correlators within the recently proposed quench-action formalism [Phys. Rev. Lett. 110, 257203 (2013)], however, we find excellent agreement with the numerical data. We, therefore, conclude that the GGE cannot give a complete description even of local observables, while the quench-action formalism correctly captures the steady state in this case. 	
1407.4419v2	http://arxiv.org/pdf/1407.4419v2	2014	Irreversibility and Entanglement Spectrum Statistics in Quantum Circuits	Daniel Shaffer|Claudio Chamon|Alioscia Hamma|Eduardo R. Mucciolo	  We show that in a quantum system evolving unitarily under a stochastic quantum circuit the notions of irreversibility, universality of computation, and entanglement are closely related. As the state evolves from an initial product state, it gets asymptotically maximally entangled. We define irreversibility as the failure of searching for a disentangling circuit using a Metropolis-like algorithm. We show that irreversibility corresponds to Wigner-Dyson statistics in the level spacing of the entanglement eigenvalues, and that this is obtained from a quantum circuit made from a set of universal gates for quantum computation. If, on the other hand, the system is evolved with a non-universal set of gates, the statistics of the entanglement level spacing deviates from Wigner-Dyson and the disentangling algorithm succeeds. These results open a new way to characterize irreversibility in quantum systems. 	
1407.7899v1	http://arxiv.org/pdf/1407.7899v1	2014	Failure of steady state thermodynamics in lattice gases under nonuniform   drive	Ronald Dickman	  To be useful, steady state thermodynamics (SST) must be self-consistent and have predictive value. Although consistency of SST was recently verified for driven lattice gases under global weak exchange, I show here that it does not predict the coexisting densities in athermal stochastic lattice gases under a nonuniform drive. I consider the lattice gas with nearest-neighbor exclusion on the square lattice, with nearest-neighbor hopping (NNE dynamics), and with hopping to both nearest and next-nearest neighbors (NNE2 dynamics). Part of the system is subject to a drive $D$ that favors hopping along one direction, while the other part is free of the drive. Thus the steady state represents coexistence between two subsystems, one far from equilibrium and the other in equilibrium, which exchange particles along the interfaces. The dimensionless chemical potential $\mu^*(\rho,D) = \mu/k_B T$ for the lattice gas with density $\rho$ is readily determined in studies of a uniform system. Under the nonuniform drive, however, equating the chemical potentials of the coexisting subsystems does not yield the coexisting densities. The steady state chemical potential is, moreover, different in the coexisting bulk regions, contrary to the basic principles of thermodynamics. These results cast serious doubt on the predictive value of SST. 	
1408.1183v1	http://arxiv.org/pdf/1408.1183v1	2014	Network Robustness: Detecting Topological Quantum Phases	Chung-Pin Chou	  Can the topology of a network that consists of many particles interacting with each other change in complexity when a phase transition occurs? The answer to this question is particularly interesting to understand the nature of phase transitions if the distinct phases do not break any symmetry, such as topological phase transitions. Here we present a novel theoretical framework established by complex network analysis for demonstrating that across a transition point of the topological superconductors, the network space experiences a homogeneous-heterogeneous transition invisible in real space. This transition is nothing but related to the robustness of a network to random failures. We suggest that the idea of the network robustness can be applied to characterizing various phase transitions whether or not the symmetry is broken. 	
1409.2298v1	http://arxiv.org/pdf/1409.2298v1	2014	Fragmentation of colliding planetesimals with water content	Thomas I. Maindl|Rudolf Dvorak|Christoph Schäfer|Roland Speith	  We investigate the outcome of collisions of Ceres-sized planetesimals composed of a rocky core and a shell of water ice. These collisions are not only relevant for explaining the formation of planetary embryos in early planetary systems, but also provide insight into the formation of asteroid families and possible water transport via colliding small bodies. Earlier studies show characteristic collision velocities exceeding the bodies' mutual escape velocity which - along with the distribution of the impact angles - cover the collision outcome regimes 'partial accretion', 'erosion', and 'hit-and-run' leading to different expected fragmentation scenarios. Existing collision simulations use bodies composed of strengthless material; we study the distribution of fragments and their water contents considering the full elasto-plastic continuum mechanics equations also including brittle failure and fragmentation. 	
1409.2680v2	http://arxiv.org/pdf/1409.2680v2	2015	Anomalous transport of impurities in inelastic Maxwell gases	Vicente Garzó|Nagi Khalil|Emmanuel Trizac	  A mixture of dissipative hard grains generically exhibits a breakdown of kinetic energy equipartition. The undriven and thus freely cooling binary problem, in the tracer limit where the density of one species becomes minute, may exhibit an extreme form of this breakdown, with the minority species carrying a finite fraction of the total kinetic energy of the system. We investigate the fingerprint of this non-equilibrium phase transition, akin to an ordering process, on transport properties. The analysis, performed by solving the Boltzmann kinetic equation from a combination of analytical and Monte Carlo techniques, hints at the possible failure of hydrodynamics in the ordered region. As a relevant byproduct of the study, the behaviour of the second and fourth-degree velocity moments is also worked out. 	
1409.5539v2	http://arxiv.org/pdf/1409.5539v2	2015	Uninformed Hawking Radiation	I. Sakalli|A. Ovgun	  We show in detail that the Parikh-Wilczek tunneling method (PWTM), which was designed for resolving the information loss problem in Hawking radiation (HR)fails whenever the radiation occurs from an isothermal process. The PWTM aims to produce a non-thermal HR which adumbrates the resolution of the problem of unitarity in quantum mechanics (QM), and consequently the entropy (or information) conservation problem. The effectiveness of the method has been satisfactorily tested on numerous black holes (BHs). However, it has been shown that the isothermal HR, which results from the emission of the uncharged particles of the linear dilaton BH (LDBH) described in the Einstein-Maxwell-Dilaton (EMD) theory, the PWTM has vulnerability in having non-thermal radiation. In particular, we consider Painlev\'e-Gullstrand coordinates (PGCs) and isotropic coordinates (ICs) in order to prove the aformentioned failure in the PWTM. While carrying out calculations in the ICs, we also highlight the effect of the refractive index on the null geodesics. 	
1410.0138v1	http://arxiv.org/pdf/1410.0138v1	2014	Characterization of Single-Walled Carbon Nanotubes with Nodal Structural   Defects	Young I. Jhon|Woonjo Cho|Seok Lee|Young Min Jhon	  Recently experiments showed that nodal structural defects are readily formed in the synthesis of single-walled carbon nanotubes (SWNTs) and consequently, SWNTs are likely to deviate from well-defined seamless tubular structures. Here, using graphene-helix growth model, we describe structural details of feasible nodal defects in SWNTs and investigate how mechanical and electronic properties of SWNTs would change in the presence of them using computational methods. Surprisingly atomistic simulations of SWNTs with nodal defects show excellent agreement with previous structural, tensile, and ball-milling experiments whose results cannot be explained using conventional models. The tensile failure of SWNTs with nodal defects requires about four- or six-fold lower strength than pristine ones and these SWNTs are comparatively prone to damage under a lateral compressive biting. We reveal that electronic band-gap of SWNT(12,8) would be remarkably reduced in the presence of nodal defects. This study strongly indicates universality of nodal defects in SWNTs requesting new theoretical framework in SWNT modelling for proper characteristics prediction. 	
1410.8616v1	http://arxiv.org/pdf/1410.8616v1	2014	Data Driven Prognosis: A multi-physics approach verified via balloon   burst experiment	Abhijit Chandra|Oliva Kar	  A multi-physics formulation for Data Driven Prognosis (DDP) is developed. Unlike traditional predictive strategies that require controlled off-line measurements or training for determination of constitutive parameters to derive the transitional statistics, the proposed DDP algorithm relies solely on in situ measurements. It utilizes a deterministic mechanics framework, but the stochastic nature of the solution arises naturally from the underlying assumptions regarding the order of the conservation potential as well as the number of dimensions involved. The proposed DDP scheme is capable of predicting onset of instabilities. Since the need for off-line testing (or training) is obviated, it can be easily implemented for systems where such a priori testing is difficult or even impossible to conduct. The prognosis capability is demonstrated here via a balloon burst experiment where the instability is predicted utilizing only on-line visual observations. The DDP scheme never failed to predict the incipient failure, and no false positives were issued. The DDP algorithm is applicable to others types of datasets. Time horizons of DDP predictions can be adjusted by using memory over different time windows. Thus, a big dataset can be parsed in time to make a range of predictions over varying time horizons. 	
1412.1211v1	http://arxiv.org/pdf/1412.1211v1	2014	Criticality in Fiber Bundle Model	Subhadeep Roy|Purusattam Ray	  We report a novel critical behavior in the breakdown of an equal load sharing fiber bundle model at a dispersion $\delta_c$ of the breaking threshold of the fibers. For $\delta < \delta_c$, there is a finite probability $P_b$, that rupturing of the weakest fiber leads to the failure of the entire system. For $\delta \geq \delta_c$, $P_b = 0$. At $\delta_c, P_b \sim L^{-\eta}$, with $\eta \approx 1/3$, where $L$ is the size of the system. As $\delta \rightarrow \delta_c$, the relaxation time $\tau$ diverges obeying the finite size scaling law: $\tau \sim L^{\beta}(|\delta-\delta_c| L^{\alpha})$ with $\alpha, \beta = 0.33 \pm 0.05$. At $\delta_c$, the system fails, at the critical load, in avalanches (of rupturing fibers) of all sizes $s$ following the distribution $P(s) \sim s^{-\kappa}$, with $\kappa = 0.50 \pm 0.01$. We relate this critical behavior to brittle to quasi-brittle transition. 	
1412.1523v2	http://arxiv.org/pdf/1412.1523v2	2015	Information Exchange and Learning Dynamics over Weakly-Connected   Adaptive Networks	Bicheng Ying|Ali H. Sayed	  The paper examines the learning mechanism of adaptive agents over weakly-connected graphs and reveals an interesting behavior on how information flows through such topologies. The results clarify how asymmetries in the exchange of data can mask local information at certain agents and make them totally dependent on other agents. A leader-follower relationship develops with the performance of some agents being fully determined by the performance of other agents that are outside their domain of influence. This scenario can arise, for example, due to intruder attacks by malicious agents or as the result of failures by some critical links. The findings in this work help explain why strong-connectivity of the network topology, adaptation of the combination weights, and clustering of agents are important ingredients to equalize the learning abilities of all agents against such disturbances. The results also clarify how weak-connectivity can be helpful in reducing the effect of outlier data on learning performance. 	
1412.4787v1	http://arxiv.org/pdf/1412.4787v1	2014	Quenching the XXZ spin chain: quench action approach versus generalized   Gibbs ensemble	M. Mestyan|B. Pozsgay|G. Takacs|M. A. Werner	  Following our previous work [PRL 113 (2014) 09020] we present here a detailed comparison of the quench action approach and the predictions of the generalized Gibbs ensemble, with the result that while the quench action formalism correctly captures the steady state, the GGE does not give a correct description of local short-distance correlation functions. We extend our studies to include another initial state, the so-called q-dimer state. We present important details of our construction, including new results concerning exact overlaps for the dimer and q-dimer states, and we also give an exact solution of the quench-action-based overlap-TBA for the q-dimer. Furthermore, we extend our computations to include the xx spin correlations besides the zz correlations treated previously, and give a detailed discussion of the underlying reasons for the failure of the GGE, especially in the light of new developments. 	
1412.5571v1	http://arxiv.org/pdf/1412.5571v1	2014	From Co- Toward Multi-Simulation of Smart Grids based on HLA and FMI   Standards	Martin Lévesque|Christophe Béchet|Eric Suignard|Martin Maier|Anne Picault|Géza Joós	  In this article, a multi-simulation model is proposed to measure the performance of all Smart Grid perspectives as defined in the IEEE P2030 standard. As a preliminary implementation, a novel information technology (IT) and communication multi-simulator is developed following an High Level Architecture (HLA). To illustrate the usefulness of such a multi-simulator, a case study of a distribution network operation application is presented using real-world topology configurations with realistic communication traffic based on IEC 61850. The multi-simulator allows to quantify, in terms of communication delay and system reliability, the impacts of aggregating all traffic on a low-capacity wireless link based on Digital Mobile Radio (DMR) when a Long Term Evolution (LTE) network failure occurs. The case study illustrates that such a multi-simulator can be used to experiment new smart grid mechanisms and verify their impacts on all smart grid perspectives in an automated manner. Even more importantly, multi-simulation can prevent problems before modifying/upgrading a smart grid and thus potentially reduce utility costs. 	
1412.5970v1	http://arxiv.org/pdf/1412.5970v1	2014	Excitation Waves on a Minimal Small-World Model	Thomas Isele|Benedikt Hartung|Philipp Hövel|Eckehard Schöll	  We examine traveling-wave solutions on a regular ring network with one additional long-range link that spans a distance d. The nodes obey the FitzHugh-Nagumo kinetics in the excitable regime. The additional shortcut induces a plethora of spatio-temporal behavior that is not present without it. We describe the underlying mechanisms for different types of patterns: propagation failure, period decreasing, bistability, shortcut blocking and period multiplication. For this purpose, we investigate the dependence on d, the network size, the coupling range in the original ring and the global coupling strength and present a phase diagram summarizing the different scenarios. Furthermore, we discuss the scaling behavior of the critical distance by analytical means and address the connection to spatially continuous excitable media. 	
1501.00202v1	http://arxiv.org/pdf/1501.00202v1	2014	Comb models for transport along spiny dendrites	V. Méndez|A. Iomin	  This chapter is a contribution in the "Handbook of Applications of Chaos Theory" ed. by Prof. Christos H Skiadas. The chapter is organized as follows. First we study the statistical properties of combs and explain how to reduce the effect of teeth on the movement along the backbone as a waiting time distribution between consecutive jumps. Second, we justify an employment of a comb-like structure as a paradigm for further exploration of a spiny dendrite. In particular, we show how a comb-like structure can sustain the phenomenon of the anomalous diffusion, reaction-diffusion and L\'evy walks. Finally, we illustrate how the same models can be also useful to deal with the mechanism of ta translocation wave / translocation waves of CaMKII and its propagation failure. We also present a brief introduction to the fractional integro-differentiation in appendix at the end of the chapter. 	
1501.05473v2	http://arxiv.org/pdf/1501.05473v2	2015	Persistent crust-core spin lag in neutron stars	Kostas Glampedakis|Paul Lasky	  It is commonly believed that the magnetic field threading a neutron star provides the ultimate mechanism (on top of fluid viscosity) for enforcing long-term corotation between the slowly spun down solid crust and the liquid core. We show that this argument fails for axisymmetric magnetic fields with closed field lines in the core, the commonly used `twisted torus' field being the most prominent example. The failure of such magnetic fields to enforce global crust-core corotation leads to the development of a persistent spin lag between the core region occupied by the closed field lines and the rest of the crust and core. We discuss the repercussions of this spin lag for the evolution of the magnetic field, suggesting that, in order for a neutron star to settle to a stable state of crust-core corotation, the bulk of the toroidal field component should be deposited into the crust soon after the neutron star's birth. 	
1501.06024v1	http://arxiv.org/pdf/1501.06024v1	2015	Strain localization and shear banding in ductile materials	N. Bordignon|A. Piccolroaz|F. Dal Corso|D. Bigoni	  A model of a shear band as a zero-thickness nonlinear interface is proposed and tested using finite element simulations. An imperfection approach is used in this model where a shear band, that is assumed to lie in a ductile matrix material (obeying von Mises plasticity with linear hardening), is present from the beginning of loading and is considered to be a zone in which yielding occurs before the rest of the matrix. This approach is contrasted with a perturbative approach, developed for a J$_2$-deformation theory material, in which the shear band is modelled to emerge at a certain stage of a uniform deformation. Both approaches concur in showing that the shear bands (differently from cracks) propagate rectilinearly under shear loading and that a strong stress concentration should be expected to be present at the tip of the shear band, two key features in the understanding of failure mechanisms of ductile materials. 	
1502.00244v2	http://arxiv.org/pdf/1502.00244v2	2015	Multiple Tipping Points and Optimal Repairing in Interacting Networks	Antonio Majdandzic|Lidia A. Braunstein|Chester Curme|Irena Vodenska|Sary Levy-Carciente|H. Eugene Stanley|Shlomo Havlin	  Systems that comprise many interacting dynamical networks, such as the human body with its biological networks or the global economic network consisting of regional clusters, often exhibit complicated collective dynamics. To understand the collective behavior of such systems, we investigate a model of interacting networks exhibiting the fundamental processes of failure, damage spread, and recovery. We find a very rich phase diagram that becomes exponentially more complex as the number of networks is increased. In the simplest example of $n=2$ interacting networks we find two critical points, 4 triple points, 10 allowed transitions, and two "forbidden" transitions, as well as complex hysteresis loops. Remarkably, we find that triple points play the dominant role in constructing the optimal repairing strategy in damaged interacting systems. To support our model, we analyze an example of real interacting financial networks and find evidence of rapid dynamical transitions between well-defined states, in agreement with the predictions of our model. 	
1503.00254v2	http://arxiv.org/pdf/1503.00254v2	2017	Many-body critical Casimir interactions in colloidal suspensions	Hendrik Hobrecht|Alfred Hucht	  We study the fluctuation-induced Casimir interactions in colloidal suspensions, especially between colloids immersed in a binary liquid close to its critical demixing point. To simulate these systems, we present a highly efficient cluster Monte Carlo algorithm based on geometric symmetries of the Hamiltonian. Utilizing the principle of universality, the medium is represented by an Ising system while the colloids are areas of spins with fixed orientation. Our results for the Casimir interaction potential between two particles at the critical point in two dimensions perfectly agree with the exact predictions. However, we find that in finite systems the behavior strongly depends on whether the $Z_{2}$ symmetry of the system is broken by the particles. Eventually we present Monte Carlo results for the three-body Casimir interaction potential and take a close look onto the case of one particle in the vicinity of two adjacent particles, which can be calculated from the two-particle interaction by a conformal mapping. These results emphasize the failure of the common decomposition approach for many-particle critical Casimir interactions. 	
1503.01172v1	http://arxiv.org/pdf/1503.01172v1	2015	Soil cracking modelling using the mesh-free SPH method	H. H. Bui|G. D. Nguyen|J. Kodikara|M. Sanchez	  The presence of desiccation cracks in soils can significantly alter their mechanical and hydrological properties. In many circumstances, desiccation cracking in soils can cause significant damage to earthen or soil supported structures. For example, desiccation cracks can act as the preference path way for water flow, which can facilitate seepage flow causing internal erosion inside earth structures. Desiccation cracks can also trigger slope failures and landslides. Therefore, developing a computational procedure to predict desiccation cracking behaviour in soils is vital for dealing with key issues relevant to a range of applications in geotechnical and geo-environment engineering. In this paper, the smoothed particle hydrodynamics (SPH) method will be extended for the first time to simulate shrinkage-induced soil cracking. The main objective of this work is to examine the performance of the proposed numerical approach in simulating the strong discontinuity in material behaviour and to learn about the crack formation in soils, looking at the effects of soil thickness on the cracking patterns. Results show that the SPH is a promising numerical approach for simulating crack formation in soils 	
1504.02578v1	http://arxiv.org/pdf/1504.02578v1	2015	Blade: A Data Center Garbage Collector	David Terei|Amit Levy	  An increasing number of high-performance distributed systems are written in garbage collected languages. This removes a large class of harmful bugs from these systems. However, it also introduces high tail-latency do to garbage collection pause times. We address this problem through a new technique of garbage collection avoidance which we call Blade. Blade is an API between the collector and application developer that allows developers to leverage existing failure recovery mechanisms in distributed systems to coordinate collection and bound the latency impact. We describe Blade and implement it for the Go programming language. We also investigate two different systems that utilize Blade, a HTTP load-balancer and the Raft consensus algorithm. For the load-balancer, we eliminate any latency introduced by the garbage collector, for Raft, we bound the latency impact to a single network round-trip, (48 {\mu}s in our setup). In both cases, latency at the tail using Blade is up to three orders of magnitude better. 	
1504.02956v1	http://arxiv.org/pdf/1504.02956v1	2015	Liquidity crises on different time scales	Francesco Corradi|Andrea Zaccaria|Luciano Pietronero	  We present an empirical analysis of the microstructure of financial markets and, in particular, of the static and dynamic properties of liquidity. We find that on relatively large time scales (15 minutes) large price fluctuations are connected to the failure of the subtle mechanism of compensation between the flows of market and limit orders: in other words, the missed revelation of the latent order book breaks the dynamical equilibrium between the flows, triggering the large price jumps. On smaller time scales (30 seconds), instead, the static depletion of the limit order book is an indicator of an intrinsic fragility of the system, which is related to a strongly non linear enhancement of the response. In order to quantify this phenomenon we introduce a measure of the liquidity imbalance present in the book and we show that it is correlated to both the sign and the magnitude of the next price movement. These findings provide a quantitative definition of the effective liquidity, which results to be strongly dependent on the considered time scales. 	
1504.05646v2	http://arxiv.org/pdf/1504.05646v2	2015	The New South Wales iVote System: Security Failures and Verification   Flaws in a Live Online Election	J. Alex Halderman|Vanessa Teague	  In the world's largest-ever deployment of online voting, the iVote Internet voting system was trusted for the return of 280,000 ballots in the 2015 state election in New South Wales, Australia. During the election, we performed an independent security analysis of parts of the live iVote system and uncovered severe vulnerabilities that could be leveraged to manipulate votes, violate ballot privacy, and subvert the verification mechanism. These vulnerabilities do not seem to have been detected by the election authorities before we disclosed them, despite a pre-election security review and despite the system having run in a live state election for five days. One vulnerability, the result of including analytics software from an insecure external server, exposed some votes to complete compromise of privacy and integrity. At least one parliamentary seat was decided by a margin much smaller than the number of votes taken while the system was vulnerable. We also found protocol flaws, including vote verification that was itself susceptible to manipulation. This incident underscores the difficulty of conducting secure elections online and carries lessons for voters, election officials, and the e-voting research community. 	
1505.02577v1	http://arxiv.org/pdf/1505.02577v1	2015	A mathematical model for plasticity and damage: A discrete calculus   formulation	Ioannis Dassios|Andrey Jivkov|Andrew Abu-Muharib|Peter James	  In this article we propose a discrete lattice model to simulate the elastic, plastic and failure behaviour of isotropic materials. Focus is given on the mathematical derivation of the lattice elements, nodes and edges, in the presence of plastic deformations and damage, i.e. stiffness degradation. By using discrete calculus and introducing non-local potential for plasticity, a force-based approach, we provide a matrix formulation necessary for software implementation. The output is a non-linear system with allowance for elasticity, plasticity and damage in lattices. This is the key tool for explicit analysis of micro-crack generation and population growth in plastically deforming metals, leading to macroscopic degradation of their mechanical properties and fitness for service. An illustrative example, analysing a local region of a node, is given to demonstrate the system performance. 	
1505.03724v1	http://arxiv.org/pdf/1505.03724v1	2015	Generalized model of blockage in particulate flow limited by channel   carrying capacity	C. Barré|J. Talbot|P. Viot L. Angelani|A. Gabrielli	  We investigate stochastic models of particles entering a channel with a random time distribution. When the number of particles present in the channel exceeds a critical value $N$, a blockage occurs and the particle flux is definitively interrupted. By introducing an integral representation of the $n$ particle survival probabilities, we obtain exact expressions for the survival probability, the distribution of the number of particles that pass before failure, the instantaneous flux of exiting particle and their time correlation. We generalize previous results for $N=2$ to an arbitrary distribution of entry times and obtain new, exact solutions for $N=3$ for a Poisson distribution and partial results for $N\ge 4$. 	
1505.04775v1	http://arxiv.org/pdf/1505.04775v1	2015	A Microstructural View of Burrowing with RoboClam	Kerstin Nordstrom|Dan Dorsch|Wolfgang Losert|Amos Winter	  RoboClam is a burrowing technology inspired by Ensis directus, the Atlantic razor clam. Atlantic razor clams should only be strong enough to dig a few centimeters into the soil, yet they burrow to over 70 cm. The animal uses a clever trick to achieve this: by contracting its body, it agitates and locally fluidizes the soil, reducing the drag and energetic cost of burrowing. RoboClam technology, which is based on the digging mechanics of razor clams, may be valuable for subsea applications that could benefit from efficient burrowing, such as anchoring, mine detonation, and cable laying. We directly visualize the movement of soil grains during the contraction of RoboClam, using a novel index-matching technique along with particle tracking. We show that the size of the failure zone around contracting RoboClam, can be theoretically predicted from the substrate and pore fluid properties, provided that the timescale of contraction is sufficiently large. We also show that the nonaffine motions of the grains are a small fraction of the motion within the fluidized zone, affirming the relevance of a continuum model for this system, even though the grain size is comparable to the size of RoboClam. 	
1506.00439v1	http://arxiv.org/pdf/1506.00439v1	2015	Scaling of discrete element model parameters for cohesionless and   cohesive solid	Subhash C. Thakur|Jin Y. Ooi|Hossein Ahmadian	  One of the major shortcomings of discrete element modelling (DEM) is the computational cost required when the number of particles is huge, especially for fine powders and/or industry scale simulations. This study investigates the scaling of model parameters that is necessary to produce scale independent predictions for cohesionless and cohesive solid under quasi-static simulation of confined compression and unconfined compression to failure in uniaxial test. A bilinear elasto-plastic adhesive frictional contact model was used. The results show that contact stiffness (both normal and tangential) for loading and unloading scales linearly with the particle size and the adhesive force scales very well with the square of the particle size. This scaling law would allow scaled up particle DEM model to exhibit bulk mechanical loading response in uniaxial test that is similar to a material comprised of much smaller particles. This is a first step towards a mesoscopic representation of a cohesive powder that is phenomenological based to produce the key bulk characteristics of a cohesive solid and has the potential to gain considerable computational advantage for industry scale DEM simulations. 	
1506.00832v1	http://arxiv.org/pdf/1506.00832v1	2015	Nonmonotonic fracture behavior of polymer nanocomposites	Janaina G. de Castro|Rojman Zargar|Mehdi Habibi|Samet H. Varol|Sapun H. Parekh|Babak Hosseinkhani|Mokhtar Adda-Bedia|Daniel Bonn	  Polymer composite materials are widely used for their exceptional mechanical properties, notably their ability to resist large deformations. Here we examine the failure stress and strain of rubbers reinforced by varying amounts of nano-sized silica particles. We find that small amounts of silica increase the fracture stress and strain, but too much filler makes the material become brittle and consequently fracture happens at small deformations. We thus find that as a function of the amount of filler there is an optimum in the breaking resistance at intermediate filler concentrations. We use a modified Griffith theory to establish a direct relation between the material properties and the fracture behavior that agrees with the experiment. 	
1507.00481v2	http://arxiv.org/pdf/1507.00481v2	2015	Self-Elongation with Sequential Folding of a Filament of Bacterial Cells	Ryojiro Honda|Jun-ichi Wakita|Makoto Katori	  Under hard-agar and nutrient-rich conditions, a cell of $Bacillus$ $subtilis$ grows as a single filament owing to the failure of cell separation after each growth and division cycle. The self-elongating filament of cells shows sequential folding processes, and multifold structures extend over an agar plate. We report that the growth process from the exponential phase to the stationary phase is well described by the time evolution of fractal dimensions of the filament configuration. We propose a method of characterizing filament configurations using a set of lengths of multifold parts of a filament. Systems of differential equations are introduced to describe the folding processes that create multifold structures in the early stage of the growth process. We show that the fitting of experimental data to the solutions of equations is excellent, and the parameters involved in our model systems are determined., 	
1507.00672v1	http://arxiv.org/pdf/1507.00672v1	2015	The Elusive Present: Hidden Past and Future Dependency and Why We Build   Models	Pooneh M. Ara|Ryan G. James|James P. Crutchfield	  Modeling a temporal process as if it is Markovian assumes the present encodes all of the process's history. When this occurs, the present captures all of the dependency between past and future. We recently showed that if one randomly samples in the space of structured processes, this is almost never the case. So, how does the Markov failure come about? That is, how do individual measurements fail to encode the past? And, how many are needed to capture dependencies between the past and future? Here, we investigate how much information can be shared between the past and future, but not be reflected in the present. We quantify this elusive information, give explicit calculational methods, and draw out the consequences. The most important of which is that when the present hides past-future dependency we must move beyond sequence-based statistics and build state-based models. 	
1507.01939v2	http://arxiv.org/pdf/1507.01939v2	2016	Nonequilibrium many-body steady states via Keldysh formalism	Mohammad F. Maghrebi|Alexey V. Gorshkov	  Many-body systems with both coherent dynamics and dissipation constitute a rich class of models which are nevertheless much less explored than their dissipationless counterparts. The advent of numerous experimental platforms that simulate such dynamics poses an immediate challenge to systematically understand and classify these models. In particular, nontrivial many-body states emerge as steady states under non-equilibrium dynamics. While these states and their phase transitions have been studied extensively with mean field theory, the validity of the mean field approximation has not been systematically investigated. In this paper, we employ a field-theoretic approach based on the Keldysh formalism to study nonequilibrium phases and phase transitions in a variety of models. In all cases, a complete description via the Keldysh formalism indicates a partial or complete failure of the mean field analysis. Furthermore, we find that an effective temperature emerges as a result of dissipation, and the universal behavior including the dynamics near the steady state is generically described by a thermodynamic universality class. 	
1507.04679v1	http://arxiv.org/pdf/1507.04679v1	2015	Non-equilibrium first order transition marks the mechanical failure of   glasses	D. V. Denisov|M. T. Dang|B. Struth|A. Zaccone|G. H. Wegdam|P. Schall	  Glasses acquire their solid-like properties by cooling from the supercooled liquid via a continuous transition known as the glass transition. Recent research on soft glasses indicates that besides temperature, another route to liquify glasses is by application of stress that forces relaxation and flow. Here we provide experimental evidence that the stress-induced onset of flow of glasses occurs via a sharp first order-like transition. Using simultaneous x-ray scattering during the oscillatory rheology of a colloidal glass, we identify a sharp symmetry change from anisotropic solid to isotropic liquid structure at the transition from the linear to the nonlinear regime. Concomitantly, intensity fluctuations sharply acquire liquid distributions. These observations identify the yielding of glasses to increasing stress as sharp affine-to-nonaffine transition, providing a new conceptual paradigm of the yielding of this technologically important class of materials, and offering new perspectives on the glass transition. 	
1508.00699v1	http://arxiv.org/pdf/1508.00699v1	2015	The roles of jets: CF, CCSN, PN, CEE, GEE, ILOT	Noam Soker	  I review the roles of jet-inflated bubbles in determining the evolution of different astrophysical objects. I discuss astrophysical systems where jets are known to inflate bubbles (cooling flow [CF] clusters; young galaxies; intermediate luminosity optical transients [ILOTs]; bipolar planetary nebulae [PNe]), and systems that are speculated to have jet-inflated bubbles (core collapse supernovae [CCSNe]; common envelope evolution [CEE]; grazing envelope evolution [GEE]). The jets in many of these cases act through a negative jet feedback mechanism (JFM). I discuss the outcomes when the JFM fizzle, or does not work at all. According to this perspective, some very interesting and energetic events owe their existence to the failure of the JFM, including stellar black holes, gamma ray bursts, and type Ia supernovae. 	
1508.02945v1	http://arxiv.org/pdf/1508.02945v1	2015	Gene expression dynamics with stochastic bursts: exact results for a   coarse-grained model	Yen Ting Lin|Charles R. Doering	  We present a theoretical framework to analyze the dynamics of gene expression with stochastic bursts. Beginning with an individual-based model which fully accounts for the messenger RNA (mRNA) and protein populations, we propose a novel expansion of the master equation for the joint process. The resulting coarse-grained model reduces the dimensionality of the system, describing only the protein population while fully accounting for the effects of discrete and fluctuating mRNA population. Closed form expressions for the stationary distribution of the protein population and mean first-passage times of the coarse-grained model are derived and large-scale Monte Carlo simulations show that the analysis accurately describes the individual-based process accounting for mRNA population, in contrast to the failure of commonly proposed diffusion-type models. 	
1509.01566v2	http://arxiv.org/pdf/1509.01566v2	2015	Fast Compact Laser Shutter Using a Direct Current Motor and 3D Printing	Grace H. Zhang|Boris Braverman|Akio Kawasaki|Vladan Vuletić	  We present a mechanical laser shutter design that utilizes a DC electric motor to rotate a blade which blocks and unblocks a light beam. The blade and the main body of the shutter are modeled with computer aided design (CAD) and are produced by 3D printing. Rubber flaps are used to limit the blade's range of motion, reducing vibrations and preventing undesirable blade oscillations. At its nominal operating voltage, the shutter achieves a switching speed of (1.22 $\pm$ 0.02) m/s with 1 ms activation delay and 10 $\mu$s jitter in its timing performance. The shutter design is simple, easy to replicate, and highly reliable, showing no failure or degradation in performance over more than $10^8$ cycles. 	
1509.03158v1	http://arxiv.org/pdf/1509.03158v1	2015	Exploring structural inhomogeneities in glasses during cavitation	Pinaki Chaudhuri|Jürgen Horbach	  Using large-scale molecular dynamics simulations for a system of $10^6$ particles, the response of a dense amorphous solid to the continuous expansion of its volume is investigated. We find that the spatially uniform glassy state becomes unstable via the formation of cavities, which eventually leads to failure. By scanning through a wide range of densities and temperatures, we determine the state points at which the instability occurs and thereby provide estimates of the co-existence density of the resultant glass phase. Evidence for long-lived, inhomogeneous configurations with a negative pressure is found, where the frozen-in glass structure contains spherical cavities or a network of void space. Furthermore, we demonstrate the occurrence of hysteretic effects when the cavitated solid is compressed to regain the dense glassy state. As a result, a new glass state is obtained, the pressure of which is different from the initial one due to small density inhomogeneities that are generated by the dilation-compression cycle. 	
1510.04046v1	http://arxiv.org/pdf/1510.04046v1	2015	On Arthur Eddington's Theory of Everything	Helge Kragh	  From 1929 to his death in 1944, A. Eddington worked on developing a highly ambitious theory of fundamental physics that covered everything in the physical world, from the tiny electron to the universe at large. His unfinished theory included abstract mathematics and spiritual philosophy in a mix which was peculiar to Eddington but hardly intelligible to other scientists. The constants of nature, which he claimed to be able to deduce purely theoretically, were of particular significance to his project. Although highly original, Eddington's attempt to provide physics with a new foundation had to some extent parallels in the ideas of other British physicists, including P. Dirac and E. A. Milne. Eddington's project was however a grand failure in so far that it was rejected by the large majority of physicists. A major reason was his unorthodox view of quantum mechanics. 	
1511.09295v2	http://arxiv.org/pdf/1511.09295v2	2016	Exploiting Path Diversity in Datacenters using MPTCP-aware SDN	Savvas Zannettou|Michael Sirivianos|Fragkiskos Papadopoulos	  Recently, Multipath TCP (MPTCP) has been proposed as an alternative transport approach for datacenter networks. MPTCP provides the ability to split a flow into multiple paths thus providing better performance and resilience to failures. Usually, MPTCP is combined with flow-based Equal-Cost Multi-Path Routing (ECMP), which uses random hashing to split the MPTCP subflows over different paths. However, random hashing can be suboptimal as distinct subflows may end up using the same paths, while other available paths remain unutilized. In this paper, we explore an MPTCP-aware SDN controller that facilitates an alternative routing mechanism for the MPTCP subflows. The controller uses packet inspection to provide deterministic subflow assignment to paths. Using the controller, we show that MPTCP can deliver significantly improved performance when connections are not limited by the access links of hosts. To lessen the effect of throughput limitation due to access links, we also investigate the usage of multiple interfaces at the hosts. We demonstrate, using our modification of the MPTCP Linux Kernel, that using multiple subflows per pair of IP addresses can yield improved performance in multi-interface settings. 	
1512.04503v3	http://arxiv.org/pdf/1512.04503v3	2016	How Soft Gamma Repeaters May Make Fast Radio Bursts	J. I. Katz	  There are several phenomenological similarities between Soft Gamma Repeaters and Fast Radio Bursts, including duty factors, time scales and probable repetition. The sudden release of magnetic energy in a neutron star magnetosphere, as in popular models of SGR, can meet the energy requirements of FRB but requires both the presence of magnetospheric plasma in order that dissipation occur in a transparent region and a mechanism for releasing much of that energy quickly. FRB sources and SGR are distinguished by long-lived (up to thousands of years) current-carrying coronal arches remaining from formation of the young neutron star and their decay ends the phase of SGR/AXP/FRB activity even though "magnetar" fields may persist. Runaway increase in resistance when the current density exceeds a threshold releases magnetostatic energy in a sudden burst and produces high brightness GHz emission of FRB by a coherent process; SGR are produced when released energy thermalizes as an equilibrium pair plasma. Failures of some alternative FRB models and the non-detection of SGR~1806-20 at radio frequencies are discussed in appendices. 	
1512.08335v2	http://arxiv.org/pdf/1512.08335v2	2016	Hybrid Phase Transition into an Absorbing State: Percolation and   Avalanches	Deokjae Lee|S. Choi|M. Stippinger|J. Kertész|B. Kahng	  Interdependent networks are more fragile under random attacks than simplex networks, because interlayer dependencies lead to cascading failures and finally to a sudden collapse. This is a hybrid phase transition (HPT), meaning that at the transition point the order parameter has a jump but there are also critical phenomena related to it. Here we study these phenomena on the Erd\H{o}s--R\'enyi and the two dimensional interdependent networks and show that the hybrid percolation transition exhibits two kinds of critical behaviors: divergence of the fluctuations of the order parameter and power-law size distribution of finite avalanches at a transition point. At the transition point, avalanches of infinite size occur thus the avalanche statistics also has the nature of a HPT. The exponent $\beta_m$ of the order parameter is $1/2$ under general conditions, while the value of the exponent $\gamma_m$ characterizing the fluctuations of the order parameter depends on the system. The critical behavior of the finite avalanches can be described by another set of exponents, $\beta_a$ and $\gamma_a$. These two critical behaviors are coupled by a scaling law: $1-\beta_m=\gamma_a$. 	
1601.07036v1	http://arxiv.org/pdf/1601.07036v1	2016	Coded Packet Transport for Optical Packet/Burst Switched Networks	Katina Kralevska|Harald Oeverby|Danilo Gligoroski	  This paper presents the Coded Packet Transport (CPT) scheme, a novel transport mechanism for Optical Packet/Burst Switched (OPS/OBS) networks. The CPT scheme exploits the combined benefits of source coding by erasure codes and path diversity to provide efficient means for recovering from packet loss due to contentions and path failures, and to provide non-cryptographic secrecy. In the CPT scheme, erasure coding is employed at the OPS/OBS ingress node to form coded packets, which are transmitted on disjoint paths from the ingress node to an egress node in the network. The CPT scheme allows for a unified view of Quality of Service (QoS) in OPS/OBS networks by linking the interactions between survivability, performance and secrecy. We provide analytical models that illustrate how QoS aspects of CPT are affected by the number of disjoint paths, packet overhead and processing delay. 	
1602.06161v1	http://arxiv.org/pdf/1602.06161v1	2016	Abaqus/Standard-based quantification of human cardiac mechanical   properties	Martin Genet|Lik Chuan Lee|Ellen Kuhl|Julius Guccione	  Computational modeling can provide critical insight into existing and potential new surgical procedures, medical or minimally-invasive treatments for heart failure, one of the leading causes of deaths in the world that has reached epidemic proportions. In this paper, we present our Abaqus/Standard-based pipeline to create subject-specific left ventricular models. We first review our generic left ventricular model, and then the personalization process based on magnetic resonance images. Identification of subject-specific cardiac material properties is done by coupling Abaqus/Standard to the python optimization library NL-Opt. Compared to previous studies from our group, the emphasis is here on the fully implicit solving of the model, and the two-parameter optimization of the passive cardiac material properties. 	
1602.06238v2	http://arxiv.org/pdf/1602.06238v2	2016	Emergence of Robustness in Network of Networks	Kevin Roth|Flaviano Morone|Byungjoon Min|Hernán A. Makse	  A model of interdependent networks of networks (NoN) has been introduced recently in the context of brain activation to identify the neural collective influencers in the brain NoN. Here we develop a new approach to derive an exact expression for the random percolation transition in Erd\"{o}s-R\'enyi NoN. Analytical calculations are in excellent agreement with numerical simulations and highlight the robustness of the NoN against random node failures. Interestingly, the phase diagram of the model unveils particular patterns of interconnectivity for which the NoN is most vulnerable. Our results help to understand the emergence of robustness in such interdependent architectures. 	
1602.08461v2	http://arxiv.org/pdf/1602.08461v2	2017	A One-Hop Information Based Geographic Routing Protocol for Delay   Tolerant MANETs	Lei You|Jianbo Li|Changjiang We|Chenqu Dai	  Delay and Disruption Tolerant Networks (DTNs) may lack continuous network connectivity. Routing in DTNs is thus a challenge since it must handle network partitioning, long delays, and dynamic topology. Meanwhile, routing protocols of the traditional Mobile Ad hoc NETworks (MANETs) cannot work well due to the failure of its assumption that most network connections are available. In this article, a geographic routing protocol is proposed for MANETs in delay tolerant situations, by using no more than one-hop information. A utility function is designed for implementing the under-controlled replication strategy. To reduce the overheads caused by message flooding, we employ a criterion so as to evaluate the degree of message redundancy. Consequently a message redundancy coping mechanism is added to our routing protocol. Extensive simulations have been conducted and the results show that when node moving speed is relatively low, our routing protocol outperforms the other schemes such as Epidemic, Spray and Wait, FirstContact in delivery ratio and average hop count, while introducing an acceptable overhead ratio into the network. 	
1603.04099v1	http://arxiv.org/pdf/1603.04099v1	2016	Contagion and Stability in Financial Networks	Seyyed Mostafa Mousavi|Robert Mackay|Alistair Tucker	  This paper investigates two mechanisms of financial contagion that are, firstly, the correlated exposure of banks to the same source of risk, and secondly the direct exposure of banks in the interbank market. It will consider a random network of banks which are connected through the inter-bank market and will discuss the desirable level of banks exposure to the same sources of risk, that is investment in similar portfolios, for different levels of network connectivity when peering through the lens of the systemic cost incurred to the economy from the banks simultaneous failure. It demonstrates that for all levels of network connectivity, certain levels of diversifying individual banks diversifications are not optimum under any condition. So, given an acceptable level of systemic cost, the regulator could let banks decrease their capital buffers by moving away from the non-optimum area. 	
1603.05165v2	http://arxiv.org/pdf/1603.05165v2	2016	Non-Markovianity in atom-surface dispersion forces	F. Intravaia|R. O. Behunin|C. Henkel|K. Busch|D. A. R. Dalvit	  We discuss the failure of the Markov approximation in the description of atom-surface fluctuation-induced interactions, both at equilibrium (Casimir-Polder forces) and out-of-equilibrium (quantum friction). Using general theoretical arguments, we show that the Markov approximation can lead to erroneous predictions of such phenomena with regard to both strength and functional dependencies on system parameters. Our findings highlight the importance of non-Markovian effects in dispersion interactions. In particular, we show that the long-time power-law tails of temporal correlations, and the corresponding low-frequency behavior, of two-time dipole correlations, neglected in the Markovian limit, dramatically affect the prediction of the force. 	
1603.06287v3	http://arxiv.org/pdf/1603.06287v3	2016	Large deviations of radial statistics in the two-dimensional   one-component plasma	Fabio Deelan Cunden|Francesco Mezzadri|Pierpaolo Vivo	  The two-dimensional one-component plasma is an ubiquitous model for several vortex systems. For special values of the coupling constant $\beta q^2$ (where $q$ is the particles charge and $\beta$ the inverse temperature), the model also corresponds to the eigenvalues distribution of normal matrix models. Several features of the system are discussed in the limit of large number $N$ of particles for generic values of the coupling constant. We show that the statistics of a class of radial observables produces a rich phase diagram, and their asymptotic behaviour in terms of large deviation functions is calculated explicitly, including next-to-leading terms up to order 1/N. We demonstrate a split-off phenomenon associated to atypical fluctuations of the edge density profile. We also show explicitly that a failure of the fluid phase assumption of the plasma can break a genuine $1/N$-expansion of the free energy. Our findings are corroborated by numerical comparisons with exact finite-N formulae valid for $\beta q^2=2$. 	
1604.04414v2	http://arxiv.org/pdf/1604.04414v2	2016	Partial reconstruction of the rotational motion of Philae spacecraft   during its landing on comet 67P/Churyumov-Gerasimenko	Tamás Baranyai|András Balázs|Péter L. Várkonyi	  This paper presents a partial reconstruction of the rotational dynamics of the Philae spacecraft upon landing on comet 67P/Churyumov-Gerasimenko as part of ESA's Rosetta mission. We analyze the motion and the events triggered by the failure to fix the spacecraft to the comet surface at the time of the first touchdown. Dynamic trajectories obtained by numerical simulation of a 7 degree-of-freedom mechanical model of the spacecraft are fitted to directions of incoming solar radiation inferred from in-situ measurements of the electric power provided by the solar panels. The results include a lower bound of the angular velocity of the lander immediately after its first touchdown. Our study also gives insight into the effect of the programmed turn-off of the stabilizing gyroscope after touchdown; the important dynamical consequences of a small collision during Philae's journey; and the probability that a similar landing scenario harms the operability of this type of spacecraft. 	
1604.06881v1	http://arxiv.org/pdf/1604.06881v1	2016	Micro-mechanical Failure Analysis of Wet Granular Matter	Konstantin Melnikov|Falk K. Wittel|Hans J. Herrmann	  We employ a novel fluid-particle model to study the shearing behavior of granular soils under different saturation levels, ranging from the dry material via the capillary bridge regime to higher saturation levels with percolating clusters. The full complexity of possible liquid morphologies is taken into account, implying the formation of isolated arbitrary-sized liquid clusters with individual Laplace pressures that evolve by liquid exchange via films on the grain surface. Liquid clusters can grow in size, shrink, merge and split, depending on local conditions, changes of accessible liquid and the pore space morphology determined by the granular phase. This phase is represented by a discrete particle model based on Contact Dynamics, where capillary forces exerted from a liquid phase add to the motion of spherical particles. We study the macroscopic response of the system due to an external compression force at various liquid contents with the help of triaxial shear tests. Additionally, the change in liquid cluster distributions during the compression due to the deformation of the pore space is evaluated close to the critical load. 	
1604.08094v1	http://arxiv.org/pdf/1604.08094v1	2016	Optimal processes for probabilistic work extraction beyond the second   law	Vasco Cavina|Andrea Mari|Vittorio Giovannetti	  According to the second law of thermodynamics, for every transformation performed on a system which is in contact with an environment of fixed temperature, the extracted work is bounded by the decrease of the free energy of the system. However, in a single realization of a generic process, the extracted work is subject to statistical fluctuations which may allow for probabilistic violations of the previous bound. We are interested in enhancing this effect, i.e. we look for thermodynamic processes that maximize the probability of extracting work above a given arbitrary threshold. For any process obeying the Jarzynski identity, we determine an upper bound for the work extraction probability that depends also on the minimum amount of work that we are willing to extract in case of failure, or on the average work we wish to extract from the system. Then we show that this bound can be saturated within the thermodynamic formalism of quantum discrete processes composed by sequences of unitary quenches and complete thermalizations. We explicitly determine the optimal protocol which is given by two quasi-static isothermal transformations separated by a finite unitary quench. 	
1605.04069v1	http://arxiv.org/pdf/1605.04069v1	2016	Availability Aware Continuous Replica Placement Problem	Abdullah Yousafzai|Abdullah Gani|Rafidah Md Noor	  Replica placement (RP) intended at producing a set of duplicated data items across the nodes of a distributed system in order to optimize fault tolerance, availability, system performance load balancing. Typically, RP formulations employ dynamic methods to change the replica placement in the system potentially upon user request profile. Continuous Replica Placement Problem (CRPP) is an extension of replica placement problem that takes into consideration the current replication state of the distributed system along with user request profile to define a new replication scheme, subject to optimization criteria and constraints. This paper proposes an alternative technique, named Availability Aware Continuous Replica Placement Problem (AACRPP).AACRPP can be defined as: Given an already defined replica placement scheme, a user request profile, and a node failure profile define a new replication scheme, subject to optimization criteria and constraints. In this effort we use modified greedy heuristics from the CRPP and investigated the proposed mechanism using a trace driven java based simulation. 	
1605.05378v2	http://arxiv.org/pdf/1605.05378v2	2016	Frictional sliding without geometrical reflection symmetry	Michael Aldam|Yohai Bar-Sinai|Ilya Svetlizky|Efim A. Brener|Jay Fineberg|Eran Bouchbinder	  The dynamics of frictional interfaces play an important role in many physical systems spanning a broad range of scales. It is well-known that frictional interfaces separating two dissimilar materials couple interfacial slip and normal stress variations, a coupling that has major implications on their stability, failure mechanism and rupture directionality. In contrast, interfaces separating identical materials are traditionally assumed not to feature such a coupling due to symmetry considerations. We show, combining theory and experiments, that interfaces which separate bodies made of macroscopically identical materials, but lack geometrical reflection symmetry, generically feature such a coupling. We discuss two applications of this novel feature. First, we show that it accounts for a distinct, and previously unexplained, experimentally observed weakening effect in frictional cracks. Second, we demonstrate that it can destabilize frictional sliding which is otherwise stable. The emerging framework is expected to find applications in a broad range of systems. 	
1606.00315v3	http://arxiv.org/pdf/1606.00315v3	2017	Universal Finite-Size Scaling for Percolation Theory in High Dimensions	Ralph Kenna|Bertrand Berche	  We present a unifying, consistent, finite-size-scaling picture for percolation theory bringing it into the framework of a general, renormalization-group-based, scaling scheme for systems above their upper critical dimensions $d_c$. Behaviour at the critical point is non-universal in $d>d_c=6$ dimensions. Proliferation of the largest clusters, with fractal dimension $4$, is associated with the breakdown of hyperscaling there when free boundary conditions are used. But when the boundary conditions are periodic, the maximal clusters have dimension $D=2d/3$, and obey random-graph asymptotics. Universality is instead manifest at the pseudocritical point, where the failure of hyperscaling in its traditional form is universally associated with random-graph-type asymptotics for critical cluster sizes, independent of boundary conditions. 	
1606.00919v4	http://arxiv.org/pdf/1606.00919v4	2017	Global warming: Temperature estimation in annealers	Jack Raymond|Sheir Yarkoni|Evgeny Andriyash	  Sampling from a Boltzmann distribution is NP-hard and so requires heuristic approaches. Quantum annealing is one promising candidate. The failure of annealing dynamics to equilibrate on practical time scales is a well understood limitation, but does not always prevent a heuristically useful distribution from being generated. In this paper we evaluate several methods for determining a useful operational temperature range for annealers. We show that, even where distributions deviate from the Boltzmann distribution due to ergodicity breaking, these estimates can be useful. We introduce the concepts of local and global temperatures that are captured by different estimation methods. We argue that for practical application it often makes sense to analyze annealers that are subject to post-processing in order to isolate the macroscopic distribution deviations that are a practical barrier to their application. 	
1606.03390v1	http://arxiv.org/pdf/1606.03390v1	2016	Microscopic description for the emergence of collective dissipation in   extended quantum systems	Fernando Galve|Antonio Mandarino|Matteo G. A. Paris|Claudia Benedetti|Roberta Zambrini	  Practical implementations of quantum technology are limited by unavoidable effects of decoherence and dissipation. With achieved experimental control for individual atoms and photons, more complex platforms composed by several units can be assembled enabling distinctive forms of dissipation and decoherence, in independent heat baths or collectively into a common bath, with dramatic consequences for the preservation of quantum coherence. The cross-over between these two regimes has been widely attributed in the literature to the system units being farther apart than the bath's correlation length. Starting from a microscopic model of a structured environment (a crystal) sensed by two bosonic probes, here we show the failure of such conceptual relation, and identify the exact physical mechanism underlying this cross-over, displaying a sharp contrast between dephasing and dissipative baths. Depending on the frequency of the system and, crucially, on its orientation with respect to the crystal axes, collective dissipation becomes possible for very large distances between probes, opening new avenues to deal with decoherence in phononic baths. 	
1607.08725v1	http://arxiv.org/pdf/1607.08725v1	2016	Recurrent Neural Machine Translation	Biao Zhang|Deyi Xiong|Jinsong Su	  The vanilla attention-based neural machine translation has achieved promising performance because of its capability in leveraging varying-length source annotations. However, this model still suffers from failures in long sentence translation, for its incapability in capturing long-term dependencies. In this paper, we propose a novel recurrent neural machine translation (RNMT), which not only preserves the ability to model varying-length source annotations but also better captures long-term dependencies. Instead of the conventional attention mechanism, RNMT employs a recurrent neural network to extract the context vector, where the target-side previous hidden state serves as its initial state, and the source annotations serve as its inputs. We refer to this new component as contexter. As the encoder, contexter and decoder in our model are all derivable recurrent neural networks, our model can still be trained end-to-end on large-scale corpus via stochastic algorithms. Experiments on Chinese-English translation tasks demonstrate the superiority of our model to attention-based neural machine translation, especially on long sentences. Besides, further analysis of the contexter revels that our model can implicitly reflect the alignment to source sentence. 	
1608.01606v1	http://arxiv.org/pdf/1608.01606v1	2016	Particle Traces for Detecting Divergent Robot Behavior	Samuel Zapolsky|Evan Drumwright	  The motion of robots and objects in our world is often highly dependent upon contact. When contact is expected but does not occur or when contact is not expected but does occur, robot behavior diverges from plan, often disastrously. This paper describes an approach that uses simulation to detect possible such behavioral divergences on real robots. This approach, and others like it, could be applied to validation of robot behaviors, mechanism design, and even online planning.   The particle trace approach samples robot modeling parameters, sensory readings, and state estimates to evaluate a robot's behavior statistically over a range of conditions. We demonstrate that combining even coarse estimates of state and modeling parameters with fast multibody simulation can be sufficient to detect divergent robot behavior and characterize robot performance in the real world. Correspondingly, this approach could be used to assess risk and find and analyze likely failures, given the extensive data that such simulations can generate.   We assess this approach on actuated, high degree-of-freedom robot locomotion examples, a picking task with a fixed-base manipulator, and an unpowered passive dynamic walker. This research works toward understanding how multi-rigid body simulations can better characterize the behavior of robots without significantly compliant elements. 	
1608.03096v1	http://arxiv.org/pdf/1608.03096v1	2016	Protection of Accelerator Hardware: RF systems	S. -H. Kim	  The radio-frequency (RF) system is the key element that generates electric fields for beam acceleration. To keep the system reliable, a highly sophisticated protection scheme is required, which also should be designed to ensure a good balance between beam availability and machine safety. Since RF systems are complex, incorporating high-voltage and high-power equipment, a good portion of machine downtime typically comes from RF systems. Equipment and component damage in RF systems results in long and expensive repairs. Protection of RF system hardware is one of the oldest machine protection concepts, dealing with the protection of individual high-power RF equipment from breakdowns. As beam power increases in modern accelerators, the protection of accelerating structures from beam-induced faults also becomes a critical aspect of protection schemes. In this article, an overview of the RF system is given, and selected topics of failure mechanisms and examples of protection requirements are introduced. 	
1608.03398v1	http://arxiv.org/pdf/1608.03398v1	2016	Robust Relativistic Bit Commitment	Kaushik Chakraborty|André Chailloux|Anthony Leverrier	  Relativistic cryptography exploits the fact that no information can travel faster than the speed of light in order to obtain security guarantees that cannot be achieved from the laws of quantum mechanics alone. Recently, Lunghi et al [Phys. Rev. Lett. 2015] presented a bit commitment scheme where each party uses two agents that exchange classical information in a synchronized fashion, and that is both hiding and binding. A caveat is that the commitment time is intrinsically limited by the spatial configuration of the players, and increasing this time requires the agents to exchange messages during the whole duration of the protocol. While such a solution remains computationally attractive, its practicality is severely limited in realistic settings since all communication must remain perfectly synchronized at all times.   In this work, we introduce a robust protocol for relativistic bit commitment that tolerates failures of the classical communication network. This is done by adding a third agent to both parties. Our scheme provides a quadratic improvement in terms of expected sustain time compared to the original protocol, while retaining the same level of security. 	
1608.04144v2	http://arxiv.org/pdf/1608.04144v2	2016	A multiscale framework for the simulation of the anisotropic mechanical   behavior of shale	Weixin Li|Roozbeh Rezakhani|Congrui Jin|Xinwei Zhou|Gianluca Cusatis	  Shale, like many other sedimentary rocks, is typically heterogeneous, anisotropic, and is characterized by partial alignment of anisotropic clay minerals and naturally formed bedding planes. In this study, a micromechanical framework based on the Lattice Discrete Particle Model (LDPM) is formulated to capture these features. Material anisotropy is introduced through an approximated geometric description of shale internal structure, which includes representation of material property variation with orientation and explicit modeling of parallel lamination. The model is calibrated by carrying out numerical simulations to match various experimental data, including the ones relevant to elastic properties, Brazilian tensile strength, and unconfined compressive strength. Furthermore, parametric study is performed to investigate the relationship between the mesoscale parameters and the macroscopic properties. It is shown that the dependence of the elastic stiffness, strength, and failure mode on loading orientation can be captured successfully. Finally, a homogenization approach based on the asymptotic expansion of field variables is applied to upscale the proposed micromechanical model, and the properties of the homogenized model are analyzed. 	
1609.01142v1	http://arxiv.org/pdf/1609.01142v1	2016	Neurofilaments function as shock absorbers: compression response arising   from disordered proteins	Micha Kornreich|Eti Malka-Gibor|Ben Zuker|Adi Laser-Azogui|Roy Beck	  What can cells gain by using disordered, rather than folded, proteins in the architecture of their skeleton? Disordered proteins take multiple co-existing conformations, and often contain segments which act as random-walk-shaped polymers. Using X-ray scattering we measure the compression response of disordered protein hydrogels, which are the main stress-responsive component of neuron cells. We find that at high compression their mechanics are dominated by gas-like steric and ionic repulsions. At low compression, specific attractive interactions dominate. This is demonstrated by the considerable hydrogel expansion induced by the truncation of critical short protein segments. Accordingly, the floppy disordered proteins form a weakly cross-bridged hydrogel, and act as shock absorbers that sustain large deformations without failure. 	
1609.03157v1	http://arxiv.org/pdf/1609.03157v1	2016	A centralized reinforcement learning method for multi-agent job   scheduling in Grid	Milad Moradi	  One of the main challenges in Grid systems is designing an adaptive, scalable, and model-independent method for job scheduling to achieve a desirable degree of load balancing and system efficiency. Centralized job scheduling methods have some drawbacks, such as single point of failure and lack of scalability. Moreover, decentralized methods require a coordination mechanism with limited communications. In this paper, we propose a multi-agent approach to job scheduling in Grid, named Centralized Learning Distributed Scheduling (CLDS), by utilizing the reinforcement learning framework. The CLDS is a model free approach that uses the information of jobs and their completion time to estimate the efficiency of resources. In this method, there are a learner agent and several scheduler agents that perform the task of learning and job scheduling with the use of a coordination strategy that maintains the communication cost at a limited level. We evaluated the efficiency of the CLDS method by designing and performing a set of experiments on a simulated Grid system under different system scales and loads. The results show that the CLDS can effectively balance the load of system even in large scale and heavy loaded Grids, while maintains its adaptive performance and scalability. 	
1609.06587v1	http://arxiv.org/pdf/1609.06587v1	2016	Comment on "On Uniqueness of SDE Decomposition in A-type Stochastic   Integration" [arXiv:1603.07927v1]	Peijie Zhou|Tiejun Li	  The uniqueness issue of SDE decomposition theory proposed by Ao and his co-workers has recently been discussed. A comprehensive study to investigate connections among different landscape theories [J. Chem. Phys. 144, 094109 (2016)] has pointed out that the decomposition is generally not unique, while Ao et al. (arXiv:1603.07927v1) argues that such conclusions are "incorrect" because of the missing boundary conditions. In this comment, we will combine literatures research and concrete examples to show that the concrete and effective boundary conditions have not been proposed to guarantee the uniqueness, hence the arguments in [arXiv:1603.07927v1] are not sufficient. Moreover, we show that the "uniqueness" of the O-U process decomposition referred by YTA paper is unable to serve as a counterexample to ZL's result since additional assumptions have been made implicitly beyond the original SDE decomposition framework, which cannot be applied to general nonlinear cases. Some other issues such as the failure of gradient expansion method will also be discussed. Our demonstration contributes to better understanding of the relevant papers as well as the SDE decomposition theory. 	
1609.07275v1	http://arxiv.org/pdf/1609.07275v1	2016	Critical behavior of $k$-core percolation: Numerical studies	Deokjae Lee|Minjae Jo|B. Kahng	  $k$-Core percolation has served as a paradigmatic model of discontinuous percolation for a long time. Recently it was revealed that the order parameter of $k$-core percolation of random networks additionally exhibits critical behavior. Thus $k$-core percolation exhibits a hybrid phase transition. Unlike the critical behaviors of ordinary percolation that are well understood, those of hybrid percolation transitions have not been thoroughly understood yet. Here, we investigate the critical behavior of $k$-core percolation of Erd\H{o}s-R\'enyi networks. We find numerically that the fluctuations of the order parameter and the mean avalanche size diverge in different ways. Thus, we classify the critical exponents into two types: those associated with the order parameter and those with finite avalanches. The conventional scaling relations hold within each set, however, these two critical exponents are coupled. Finally we discuss some universal features of the critical behaviors of $k$-core percolation and the cascade failure model on multiplex networks. 	
1609.07850v1	http://arxiv.org/pdf/1609.07850v1	2016	A fiber-bundle model for the continuum deformation of brittle material	K. Z. Nanjo	  The deformation of brittle material is primarily accompanied by micro-cracking and faulting. However, it has often been found that continuum fluid models, usually based on a non-Newtonian viscosity, are applicable. To explain this rheology, we use a fiber-bundle model, which is a model of damage mechanics. In our analyses, yield stress was introduced. Above this stress, we hypothesize that the fibers begin to fail and a failed fiber is replaced by a new fiber. This replacement is analogous to a micro-crack or an earthquake and its iteration is analogous to stick-slip motion. Below the yield stress, we assume that no fiber failure occurs, and the material behaves elastically. We show that deformation above yield stress under a constant strain rate for a sufficient amount of time can be modeled as an equation similar to that used for non-Newtonian viscous flow. We expand our rheological model to treat viscoelasticity and consider a stress relaxation problem. The solution can be used to understand aftershock temporal decay following an earthquake. Our results provide justification for the use of a non-Newtonian viscous flow to model the continuum deformation of brittle materials. 	
1610.08550v1	http://arxiv.org/pdf/1610.08550v1	2016	First principles study of band line up at defective metal-oxide   interface: oxygen point defects at Al/SiO_2 interface	Eric Tea|Jianqiu Huang|Celine Hin	  The dielectric breakdown at metal-oxide interfaces is a critical electronic device failure mechanism. Electronic tunneling through dielectric layers is a well-accepted explanation for this phenomenon. Theoretical band alignment studies, providing information about tunneling, have already been conducted in the literature for metal-oxide interfaces. However, most of the time materials were assumed defect free. Oxygen vacancies being very common in oxides, their effect on band lineup is of prime importance in understanding electron tunneling in realistic materials and devices. This work explores the effect of oxygen vacancy and oxygen di-vacancy at the Al/SiO2 interface on the band line up within Density Functional Theory using PBE0 hybrid exchange and correlation functional. It is found that the presence of defects at the interface, and their charge state, strongly alters the band line up. 	
1611.03065v2	http://arxiv.org/pdf/1611.03065v2	2016	Toward Smart Moving Target Defense for Linux Container Resiliency	Mohamed Azab|Bassem Mokhtar|Amr S. Abed|Mohamed Eltoweissy	  This paper presents ESCAPE, an informed moving target defense mechanism for cloud containers. ESCAPE models the interaction between attackers and their target containers as a "predator searching for a prey" search game. Live migration of Linux-containers (prey) is used to avoid attacks (predator) and failures. The entire process is guided by a novel host-based behavior-monitoring system that seamlessly monitors containers for indications of intrusions and attacks. To evaluate ESCAPE effectiveness, we simulated the attack avoidance process based on a mathematical model mimicking the prey-vs-predator search game. Simulation results show high container survival probabilities with minimal added overhead. 	
1611.04269v2	http://arxiv.org/pdf/1611.04269v2	2017	Exactly solvable model for a velocity jump observed in crack propagation   in viscoelastic solids	Naoyuki Sakumichi|Ko Okumura	  Needs to impart appropriate elasticity and high toughness to viscoelastic polymer materials are ubiquitous in industries such as concerning automobiles and medical devices. One of the major problems to overcome for toughening is catastrophic failure linked to a velocity jump, i.e., a sharp transition in the velocity of crack propagation occurred in a narrow range of the applied load. However, its physical origin has remained an enigma despite previous studies over 35 years. Here, we propose an exactly solvable model that exhibits the velocity jump incorporating linear viscoelasticity with a cutoff length for a continuum description. With the exact solution, we elucidate the physical origin of the velocity jump: it emerges from a dynamic glass transition in the vicinity of the propagating crack tip. We further quantify the velocity jump together with slow- and fast-velocity regimes of crack propagation, which would stimulate the development of tough polymer materials. 	
1611.05113v2	http://arxiv.org/pdf/1611.05113v2	2017	Efficient Diffusion on Region Manifolds: Recovering Small Objects with   Compact CNN Representations	Ahmet Iscen|Giorgos Tolias|Yannis Avrithis|Teddy Furon|Ondrej Chum	  Query expansion is a popular method to improve the quality of image retrieval with both conventional and CNN representations. It has been so far limited to global image similarity. This work focuses on diffusion, a mechanism that captures the image manifold in the feature space. The diffusion is carried out on descriptors of overlapping image regions rather than on a global image descriptor like in previous approaches. An efficient off-line stage allows optional reduction in the number of stored regions. In the on-line stage, the proposed handling of unseen queries in the indexing stage removes additional computation to adjust the precomputed data. We perform diffusion through a sparse linear system solver, yielding practical query times well below one second. Experimentally, we observe a significant boost in performance of image retrieval with compact CNN descriptors on standard benchmarks, especially when the query object covers only a small part of the image. Small objects have been a common failure case of CNN-based retrieval. 	
1612.01260v1	http://arxiv.org/pdf/1612.01260v1	2016	Real-time Collision Handling in Railway Network:An Agent-based Approach	Poulami Dalapati|Abhijeet Padhy|Bhawana Mishra|Animesh Dutta|Swapan Bhattacharya	  Advancement in intelligent transportation systems with complex operations requires autonomous planning and management to avoid collisions in day-to-day traffic. As failure and/or inadequacy in traffic safety system are life-critical, such collisions must be detected and resolved in an efficient way to manage continuously rising traffic. In this paper, we address different types of collision scenarios along with their early detection and resolution techniques in a complex railway system. In order to handle collisions dynamically in distributed manner, a novel agent based solution approach is proposed using the idea of max-sum algorithm, where each agent (train agent, station agent, and junction agent) communicates and cooperates with others to generate a good feasible solution that keeps the system in a safe state, i.e., collision free. We implement the proposed mechanism in Java Agent DEvelopment Framework (JADE). The results are evaluated with exhaustive experiments and compared with different existing collision handling methods to show the efficiency of our proposed approach. 	
1612.02807v1	http://arxiv.org/pdf/1612.02807v1	2016	Random versus maximum entropy models of neural population activity	Ulisse Ferrari|Tomoyuki Obuchi|Thierry Mora	  The principle of maximum entropy provides a useful method for inferring statistical mechanics models from observations in correlated systems, and is widely used in a variety of fields where accurate data are available. While the assumptions underlying maximum entropy are intuitive and appealing, its adequacy for describing complex empirical data has been little studied in comparison to alternative approaches. Here data from the collective spiking activity of retinal neurons is reanalysed. The accuracy of the maximum entropy distribution constrained by mean firing rates and pairwise correlations is compared to a random ensemble of distributions constrained by the same observables. In general, maximum entropy approximates the true distribution better than the typical or mean distribution from that ensemble. This advantage improves with population size, with groups as small as 8 being almost always better described by maximum entropy. Failure of maximum entropy to outperform random models is found to be associated with strong correlations in the population. 	
1701.01702v1	http://arxiv.org/pdf/1701.01702v1	2017	Virtual Network Migration on the GENI Wide-Area SDN-Enabled   Infrastructure	Yimeng Zhao|Samantha Lo|Ellen Zegura|Niky Riga|Mostafa Ammar	  A virtual network (VN) contains a collection of virtual nodes and links assigned to underlying physical resources in a network substrate. VN migration is the process of remapping a VN's logical topology to a new set of physical resources to provide failure recovery, energy savings, or defense against attack. Providing VN migration that is transparent to running applications is a significant challenge. Efficient migration mechanisms are highly dependent on the technology deployed in the physical substrate. Prior work has considered migration in data centers and in the PlanetLab infrastructure. However, there has been little effort targeting an SDN-enabled wide-area networking environment - an important building block of future networking infrastructure. In this work, we are interested in the design, implementation and evaluation of VN migration in GENI as a working example of such a future network. We identify and propose techniques to address key challenges: the dynamic allocation of resources during migration, managing hosts connected to the VN, and flow table migration sequences to minimize packet loss. We find that GENI's virtualization architecture makes transparent and efficient migration challenging. We suggest alternatives that might be adopted in GENI and are worthy of adoption by virtual network providers to facilitate migration. 	
1701.03404v3	http://arxiv.org/pdf/1701.03404v3	2017	Generalized $k$-core pruning process on directed networks	Jin-Hua Zhao	  The resilience of a complex interconnected system concerns the size of the macroscopic functioning node clusters after external perturbations based on a random or designed scheme. For a representation of the interconnected systems with directional or asymmetrical interactions among constituents, the directed network is a convenient choice. Yet how the interaction directions affect the network resilience still lacks thorough exploration. Here, we study the resilience of directed networks with a generalized $k$-core pruning process as a simple failure procedure based on both the in- and out-degrees of nodes, in which any node with an in-degree $< k_{in}$ or an out-degree $< k_{ou}$ is removed iteratively. With an explicitly analytical framework, we can predict the relative sizes of residual node clusters on uncorrelated directed random graphs. We show that the discontinuous transitions rise for cases with $k_{in} \geq 2$ or $k_{ou} \geq 2$, and the unidirectional interactions among nodes drive the networks more vulnerable against perturbations based on in- and out-degrees separately. 	
1702.01318v1	http://arxiv.org/pdf/1702.01318v1	2017	Concurrent factors determine toughening in the hydraulic fracture of   poroelastic composites	Alessandro Lucantonio|Giovanni Noselli	  Brittle materials fail catastrophically. In consequence of their limited flaw-tolerance, failure occurs by localized fracture and is typically a dynamic process. Recently, experiments on epithelial cell monolayers have revealed that this scenario can be significantly modified when the material susceptible to cracking is adhered to a hydrogel substrate. Thanks to the hydraulic coupling between the brittle layer and the poroelastic substrate, such a composite can develop a toughening mechanism that relies on the simultaneous growth of multiple cracks. Here, we study this remarkable behaviour by means of a detailed model, and explore how the material and loading parameters concur in determining the macroscopic toughness of the system. By extending a previous study, our results show that rapid loading conveys material toughness by promoting distributed cracking. Moreover, our theoretical findings may suggest innovative architectures of flaw-insensitive materials with higher toughness. 	
1702.05828v1	http://arxiv.org/pdf/1702.05828v1	2017	Failure and Scaling of Graphene Nanocomposites	Cory Hage Mefford|Yao Qiao|Marco Salviato	  This work proposes an investigation on the scaling of the structural strength of polymer/graphene nanocomposites. To this end, fracture tests on geometrically scaled Single Edge Notch Bending (SENB) specimens with varying contents of graphene were conducted to study the effects of nanomodification on the scaling.   It is shown that, while the strength of the pristine polymer scales according to Linear Elastic Fracture Mechanics (LEFM), this is not the case for nanocomposites, even for very low graphene contents. In fact, small specimens exhibited a more pronounced ductility with limited scaling and a significant deviation from LEFM whereas larger specimens behaved in a more brittle way, with scaling of nominal strength closer to the one predicted by LEFM.   This behavior, due to the significant size of the Fracture Process Zone (FPZ) compared to the specimen size, needs to be taken into serious consideration. In facts, it is shown that, for the specimen sizes investigated in this work, neglecting the non-linear effects of the FPZ can lead to an underestimation of the fracture energy as high as 113%, this error decreasing for increasing specimen sizes. 	
1703.06740v1	http://arxiv.org/pdf/1703.06740v1	2017	Heterogeneous micro-structure of percolation in sparse networks	Reimer Kuehn|Tim Rogers	  We examine the heterogeneous responses of individual nodes in sparse networks to the random removal of a fraction of edges. Using the message-passing formulation of percolation, we discover considerable variation across the network in the probability of a particular node to remain part of the giant component, and in the expected size of small clusters containing that node. In the vicinity of the percolation threshold, weakly non-linear analysis reveals that node-to-node heterogeneity is captured by the recently introduced notion of non-backtracking centrality. We supplement these results for fixed finite networks by a population dynamics approach to analyse random graph models in the infinite system size limit, also providing closed-form approximations for the large mean degree limit of Erd\H{o}s-R\'enyi random graphs. Interpreted in terms of the application of percolation to real-world processes, our results shed light on the heterogeneous exposure of different nodes to cascading failures, epidemic spread, and information flow. 	
1705.01670v1	http://arxiv.org/pdf/1705.01670v1	2017	Qubit-loss-free fusion of W states employing weak cross-Kerr   nonlinearities	Meiyu Wang|Quanzhi Hao|Fengli Yan|Ting Gao	  With the assistance of weak cross-Kerr nonlinearities, we introduce an optical scheme to fuse two small-size polarization entangled W states into a large-scale W state without qubit loss, i.e.,$\mathrm{W}_{n+m}$ state can be generated from an $n$-qubit W state and a $m$-qubit W state. To complete the fusion task, two polarization entanglement processes and one spatial entanglement process are applied. The fulfillments of the above processes are contributed by a cross-Kerr nonlinear interaction between the signal photons and a coherent state via Kerr media. We analyze the resource cost and the success probability of the scheme. There is no complete failure output in our fusion mechanism, and all the garbage states are recyclable. In addition, there is no need for any controlled quantum gate and any ancillary photon, so it is simple and feasible under the current experiment technology. 	
1705.02561v1	http://arxiv.org/pdf/1705.02561v1	2017	A Reconnaissance Attack Mechanism for Fixed-Priority Real-Time Systems	Chien-Ying Chen|AmirEmad Ghassami|Sibin Mohan|Negar Kiyavash|Rakesh B. Bobba|Rodolfo Pellizzoni|Man-Ki Yoon	  In real-time embedded systems (RTS), failures due to security breaches can cause serious damage to the system, the environment and/or injury to humans. Therefore, it is very important to understand the potential threats and attacks against these systems. In this paper we present a novel reconnaissance attack that extracts the exact schedule of real-time systems designed using fixed priority scheduling algorithms. The attack is demonstrated on both a real hardware platform and a simulator, with a high success rate. Our evaluation results show that the algorithm is robust even in the presence of execution time variation. 	
1705.08884v1	http://arxiv.org/pdf/1705.08884v1	2017	Uncovering the Flop of the EU Cookie Law	Martino Trevisan|Stefano Traverso|Hassan Metwalley|Marco Mellia	  In 2002, the European Union (EU) introduced the ePrivacy Directive to regulate the usage of online tracking technologies. Its aim is to make tracking mechanisms explicit while increasing privacy awareness in users. It mandates websites to ask for explicit consent before using any kind of profiling methodology, e.g., cookies. Starting from 2013 the Directive is mandatory, and now most of European websites embed a "Cookie Bar" to explicitly ask user's consent. To the best of our knowledge, no study focused in checking whether a website respects the Directive. For this, we engineer CookieCheck, a simple tool that makes this check automatic. We use it to run a measure- ment campaign on more than 35,000 websites. Results depict a dramatic picture: 65% of websites do not respect the Directive and install tracking cookies before the user is even offered the accept button. In few words, we testify the failure of the ePrivacy Directive. Among motivations, we identify the absence of rules enabling systematic auditing procedures, the lack of tools to verify its implementation by the deputed agencies, and the technical difficulties of webmasters in implementing it. 	
1705.09292v2	http://arxiv.org/pdf/1705.09292v2	2017	Coannihilation without chemical equilibrium	Mathias Garny|Jan Heisig|Benedikt Lülf|Stefan Vogl	  Chemical equilibrium is a commonly made assumption in the freeze-out calculation of coannihilating dark matter. We explore the possible failure of this assumption and find a new conversion-driven freeze-out mechanism. Considering a representative simplified model inspired by supersymmetry with a neutralino- and sbottom-like particle we find regions in parameter space with very small couplings accommodating the measured relic density. In this region freeze-out takes place out of chemical equilibrium and dark matter self-annihilation is thoroughly inefficient. The relic density is governed primarily by the size of the conversion terms in the Boltzmann equations. Due to the small dark matter coupling the parameter region is immune to direct detection but predicts an interesting signature of disappearing tracks or displaced vertices at the LHC. Unlike freeze-in or superWIMP scenarios, conversion-driven freeze-out is not sensitive to the initial conditions at the end of reheating. 	
1706.04451v2	http://arxiv.org/pdf/1706.04451v2	2017	Correlations between thresholds and degrees: An analytic approach to   model attacks and failure cascades	Rebekka Burkholz|Frank Schweitzer	  Two node variables determine the evolution of cascades in random networks: a node's degree and threshold. Correlations between both fundamentally change the robustness of a network, yet, they are disregarded in standard analytic methods as local tree or heterogeneous mean field approximations because of the bad tractability of order statistics. We show how they become tractable in the thermodynamic limit of infinite network size. This enables the analytic description of node attacks that are characterized by threshold allocations based on node degree. Using two examples, we discuss possible implications of irregular phase transitions and different speeds of cascade evolution for the control of cascades. 	
1706.05536v1	http://arxiv.org/pdf/1706.05536v1	2017	dSDiVN: a distributed Software-Defined Networking architecture for   Infrastructure-less Vehicular Networks	Ahmed Alioua|Sidi-Mohammed Senouci|Samira Moussaoui	  In the last few years, the emerging network architecture paradigm of Software-Defined Networking (SDN), has become one of the most important technology to manage large scale networks such as Vehicular Ad-hoc Networks (VANETs). Recently, several works have shown interest in the use of SDN paradigm in VANETs. SDN brings flexibility, scalability and management facility to current VANETs. However, almost all of proposed Software-Defined VANET (SDVN) architectures are infrastructure-based. This paper will focus on how to enable SDN in infrastructure-less vehicular environments. For this aim, we propose a novel distributed SDN-based architecture for uncovered infrastructure-less vehicular scenarios. It is a scalable cluster-based architecture with distributed mobile controllers and a reliable fall back recovery mechanism based on self-organized clustering and failure anticipation. 	
1707.01098v2	http://arxiv.org/pdf/1707.01098v2	2017	Dual lattice functional renormalization group for the   Berezinskii-Kosterlitz-Thouless transition: irrelevance of amplitude and   out-of-plane fluctuations	Jan Krieg|Peter Kopietz	  We develop a new functional renormalization group (FRG) approach for the two-dimensional XY-model by combining the lattice FRG proposed by Machado and Dupuis [Phys. Rev. E 82, 041128 (2010)] with a duality transformation which explicitly introduces vortices via an integer-valued field. We show that the hierarchy of FRG flow equations for the infinite set of relevant and marginal couplings of the model can be reduced to the well-known Kosterlitz-Thouless renormalization group equations for the renormalized temperature and the vortex fugacity. Within our approach it is straightforward to include weak amplitude as well as out-of-plane fluctuations of the spins, which lead to additional interactions between the vortices that do not spoil the Berezinskii-Kosterlitz-Thouless transition. This demonstrates that previous failures to obtain a line of true fixed points within the FRG are a mathematical artifact of insufficient truncation schemes. 	
1707.02487v1	http://arxiv.org/pdf/1707.02487v1	2017	The energy spectrum of ultraheavy nuclei above 10$^{20}$ eV	Antonio Codino	  A major feature of the energy spectrum of the cosmic radiation above 10$^{19}$ eV is the increasing fraction of heavy nuclei with respect to light nuclei. This fact, along with other simple assumptions, is adopted to calculate the energy spectrum of the cosmic radiation up to 2.4$\times$10$^{21}$ eV. The predicted spectrum maintains the index of 2.67 observed at lower energies which is the basic, known, empirical well-assessed feature of the physical mechanism accelerating cosmic rays in the Galaxy. Indeed above 10$^{19}$ eV the injection of nuclei is inhibited by some filter and this inhibition causes a staircase profile of the energy spectrum. It is argued that particle injection failure versus energy commences with protons, followed by Helium and then by other heavier nuclei up to Uranium. Around 7.5$\times$10$^{20}$ the cosmic radiation consists solely of nuclei heavier than Copper and the estimated intensity is 1.8$\times$10$^{-30}$ particles/GeV s sr m$^2$. 	
1709.06632v1	http://arxiv.org/pdf/1709.06632v1	2017	Biomechanical analysis of a cranial Patient Specific Implant on the   interface with the bone using the Finite Element Method	J. Díaz|Octavio Andrés González-Estrada|C. López	  - New advance technologies based on reverse engineering , design and additive manufacturing, have expanded design capabilities for biomedical applications to include Patient Specific Implants (PSI). This change in design paradigms needs advanced tools to assess the mechanical performance of the product, and simulate the impact on the patient. In this work, we perform a structural analysis on the interface of a cranial PSI under static loading conditions. Based on those simulations, we have identified the regions with high stress and strain and checked the failure criteria both in the implant and the skull. We evaluate the quality of the design of the implant and determine their response given different materials, in order to ensure optimality of the final product to be manufactured . 	
1709.07047v1	http://arxiv.org/pdf/1709.07047v1	2017	Patterns and Thresholds of Magnetoelectric Switching in Spin Logic   Devices	Dmitri E. Nikonov|Sasikanth Manipatruni|Ian A. Young	  In the quest to develop spintronic logic, it was discovered that magnetoelectric switching results in lower energy and shorter switching time than other mechanisms. Magnetoelectric (ME) field due to exchange bias at the interface with a multi-ferroic (such as BiFeO3) is well suited for 180 degree switching of magnetization. The ME field is determined by the direction of canted magnetization in BiFeO3 which can point at an angle to the plane, to which voltage is applied. Dependence of switching time and the threshold of ME field on its angles was determined by micromagnetic simulations. Switching occurs by formation of a domain wall on the side of the nanomagnet on top of BFO and its propagation to the rest of the magnet. For in-plane magnetization, switching occurs over a wide range of angles and at all magnitudes of ME field above threshold. For out-of-plane magnetization failure occurs (with an exception of a narrow range of angles and magnitudes of ME field) due to the domain wall reflecting from the opposite end of the nanomagnet. 	
1710.03783v1	http://arxiv.org/pdf/1710.03783v1	2017	Why Black Hole Information Loss is Paradoxical	David Wallace	  I distinguish between two versions of the black hole information-loss paradox. The first arises from apparent failure of unitarity on the spacetime of a completely evaporating black hole, which appears to be non-globally-hyperbolic; this is the most commonly discussed version of the paradox in the foundational and semipopular literature, and the case for calling it `paradoxical' is less than compelling. But the second arises from a clash between a fully-statistical-mechanical interpretation of black hole evaporation and the quantum-field-theoretic description used in derivations of the Hawking effect. This version of the paradox arises long before a black hole completely evaporates, seems to be the version that has played a central role in quantum gravity, and is genuinely paradoxical. After explicating the paradox, I discuss the implications of more recent work on AdS/CFT duality and on the `Firewall paradox', and conclude that the paradox is if anything now sharper. The article is written at a (relatively) introductory level and does not assume advanced knowledge of quantum gravity. 	
1710.04914v1	http://arxiv.org/pdf/1710.04914v1	2017	Atomic force microscopy study of the tetragonal to monoclinic   transformation behaviour of silica doped yttria-stabilized zirconia	Sylvain Deville|Jérôme Chevalier|Laurent Gremillard	  The tetragonal to monoclinic phase transformation of zirconia has been the subject of extensive studies over the last 20 years [1-4]. The main features of the transformation have been identified and its martensitic nature is now widely recognised [5-8]. More specifically, the relevance of a nucleation and growth model to describe the transformation is widely accepted. Recent fracture episodes [9] of zirconia hip joint heads were reported, failures related to the t-m transformation degradation. Among the materials solutions considered for decreasing the sensitivity to t-m phase transformation, the possibility of adding silica as a dopant appears as an appealing one. Previous studies have revealed the beneficial effect of silica addition by the formation of a glassy phase at the grain boundaries and triple points. This glassy phase has been proven to reduce the residual stresses level [10], slowing down the transformation kinetics. Preliminary quantitative investigations by XRD have shown these materials are less susceptible to transformation. However, the mechanism by which the transformation propagated has still to be assessed. 	
1710.05195v1	http://arxiv.org/pdf/1710.05195v1	2017	Thermomechanical properties of zirconium tungstate/hydrogenated nitrile   butadiene rubber (HNBR) composites for low-temperature applications	Anton G. Akulichev|Ben Alcock|Avinash Tiwari|Andreas T. Echtermeyer	  Rubber compounds for pressure sealing application typically have inferior dimensional stability with temperature fluctuations compared with their steel counterparts. This effect may result in seal leakage failures when subjected to decreases in temperature. Composites of hydrogenated nitrile butadiene rubber (HNBR) and zirconium tungstate as a negative thermal expansion filler were prepared in order to control the thermal expansivity of the material. The amount of zirconium tungstate (ZrW2O8) was varied in the range of 0 to about 40 vol%. The coefficient of thermal expansion (CTE), bulk modulus, uniaxial extension and compression set properties were measured. The CTE of the ZrW2O8-filled HNBR decreases with the filler content and it is reduced by a factor of 2 at the highest filler concentration used. The filler effect on CTE is found to be stronger when HNBR is below the glass transition temperature. The experimental thermal expansion data of the composites are compared with the theoretical estimates and predictions given by FEA. The effect of ZrW2O8 on the mechanical characteristics and compression set of these materials is also discussed. 	
1710.06055v1	http://arxiv.org/pdf/1710.06055v1	2017	Evolution in Virtual Worlds	Tim Taylor	  This chapter discusses the possibility of instilling a virtual world with mechanisms for evolution and natural selection in order to generate rich ecosystems of complex organisms in a process akin to biological evolution. Some previous work in the area is described, and successes and failures are discussed. The components of a more comprehensive framework for designing such worlds are mapped out, including the design of the individual organisms, the properties and dynamics of the environmental medium in which they are evolving, and the representational relationship between organism and environment. Some of the key issues discussed include how to allow organisms to evolve new structures and functions with few restrictions, and how to create an interconnectedness between organisms in order to generate drives for continuing evolutionary activity. 	
1711.02732v1	http://arxiv.org/pdf/1711.02732v1	2017	Cohomological rigidity and the Anosov-Katok construction	Nikolaos Karaliolios	  We provide a general argument for the failure of Anosov-Katok-like constructions to produce Cohomologically Rigid diffeomorphisms in manifolds other than tori. A $C^{\infty }$ smooth diffeomorphism $f $ of a compact manifold $M$ is Cohomologically Rigid iff the equation, known as Linear Cohomological one, \begin{equation*} \psi \circ f - \psi = \varphi \end{equation*} admits a $C^{\infty }$ smooth solution $\psi$ for every $\varphi$ in a codimension $1$ closed subspace of $C^{\infty } (M, \mathbb{C} )$. As an application, we show that no Cohomologically Rigid diffeomorphisms exist in the Almost Reducibility regime for quasi-periodic cocycles in homogeneous spaces of compact type, even though the Linear Cohomological equation over a generic such system admits a solution for a dense subset of functions $\varphi$. We thus confirm a conjecture by M. Herman and A. Katok in that context and provide some insight in the mechanism obstructing the construction of counterexamples. 	
1711.02926v1	http://arxiv.org/pdf/1711.02926v1	2017	Zero-Crossing Statistics for Non-Markovian Time Series	Markus Nyberg|Ludvig Lizana|Tobias Ambjörnsson	  In applications spaning from image analysis and speech recognition, to energy dissipation in turbulence and time-to failure of fatigued materials, researchers and engineers want to calculate how often a stochastic observable crosses a specific level, such as zero. At first glance this problem looks simple, but it is in fact theoretically very challenging. And therefore, few exact results exist. One exception is the celebrated Rice formula that gives the mean number of zero-crossings in a fixed time interval of a zero-mean Gaussian stationary processes. In this study we use the so-called Independent Interval Approximation to go beyond Rice's result and derive analytic expressions for all higher-order zero-crossing cumulants and moments. Our results agrees well with simulations for the non-Markovian autoregressive model. 	
1711.06007v1	http://arxiv.org/pdf/1711.06007v1	2017	Increasing power-law range in avalanche amplitude and energy   distributions	Víctor Navas-Portella|Isabel Serra|Álvaro Corral|Eduard Vives	  Power-law type probability density functions spanning several orders of magnitude are found for different avalanche properties. We propose a methodology to overcome empirical constrains that limit the power-law range for the distributions of different avalanche observables like amplitude, energy, duration or size. By considering catalogs of events that cover different observation windows, maximum likelihood estimation of a global power-law exponent is computed. This methodology is applied to amplitude and energy distributions of acoustic emission avalanches in failure-under- compression experiments of a nanoporous silica glass, finding in some cases global exponents in an unprecedented broad range: 4.5 decades for amplitudes and 9.5 decades for energies. In the later case, however, strict statistical analysis suggests experimental limitations might alter the power-law behavior. 	
1711.08965v1	http://arxiv.org/pdf/1711.08965v1	2017	Improving Reliability of Service Function Chains with Combined VNF   Migrations and Replications	Francisco Carpio|Admela Jukan	  The Network Function Virtualization (NFV) paradigm is enabling flexibility, programmability and implementation of traditional network functions into generic hardware, in form of Virtual Network Functions (VNFs). To provide services, the VNFs are commonly concatenated in a certain ordered sequence, known as Service Function Chains (SFCs). SFCs are usually required to meeting a certain level of reliability. This creates the need to place the VNFs while optimizing reliability jointly with other objectives, such as network and server load balancing. Traditional migration and replication mechanisms, commonly used for Virtual Machines (VM) in data centers, can be used to improve SFC reliability. We study how to improve service reliability using jointly replications and migrations, considering the chaining problem inherent in NFV. While replications provide reliability, performing migrations to more reliable servers decreases the resource overhead. A Linear Programming (LP) model is presented to study the impact of active-active configurations on the network and server resources. Additionally, to provide a fast recovery from server failures, we consider N-to-N configurations in NFV networks and study its impact on server resources. The results show that replications do not only improve reliability, but can also be used to achieving a better server and network load balancing, and when used jointly with migrations can improve resource utilization without degrading reliability. 	
1711.09594v1	http://arxiv.org/pdf/1711.09594v1	2017	FCLT - A Fully-Correlational Long-Term Tracker	Alan Lukežič|Luka Čehovin Zajc|Tomáš Vojíř|Jiří Matas|Matej Kristan	  We propose FCLT - a fully-correlational long-term tracker. The two main components of FCLT are a short-term tracker which localizes the target in each frame and a detector which re-detects the target when it is lost. Both the short-term tracker and the detector are based on correlation filters. The detector exploits properties of the recent constrained filter learning and is able to re-detect the target in the whole image efficiently. A failure detection mechanism based on correlation response quality is proposed. The FCLT is tested on recent short-term and long-term benchmarks. It achieves state-of-the-art results on the short-term benchmarks and it outperforms the current best-performing tracker on the long-term benchmark by over 18%. 	
1711.10165v1	http://arxiv.org/pdf/1711.10165v1	2017	Enhanced Communication With the Assistance of Indefinite Causal Order	Daniel Ebler|Sina Salek|Giulio Chiribella	  In quantum Shannon theory, the way information is encoded and decoded takes advantage of the laws of quantum mechanics, while the way communication channels are interlinked is assumed to be classical. In this Letter we relax the assumption that quantum channels are combined classically, showing that a quantum communication network where channels are combined in an indefinite causal order can achieve tasks that are impossible in conventional quantum Shannon theory. In particular, we show that two identical copies of a completely depolarizing channel become able to transmit information when they are combined in a quantum superposition of two alternative orders. This finding runs counter to the intuition that if two communication channels are identical, using them in different orders should not make any difference. The failure of such intuition stems from the fact that a single noisy channel can be a random mixture of many elementary, non-commuting processes, whose order (or lack thereof) can affect the ability to transmit information. 	
1712.01291v1	http://arxiv.org/pdf/1712.01291v1	2017	Machine learning for predictive estimation of qubit dynamics subject to   dephasing	Riddhi Swaroop Gupta|Michael J. Biercuk	  Decoherence remains a major challenge in quantum computing hardware and a variety of physical-layer controls provide opportunities to mitigate the impact of this phenomenon through feedback and feedforward control. In this work, we compare a variety of machine learning algorithms derived from diverse fields for the task of state estimation (retrodiction) and forward prediction of future qubit state evolution for a single qubit subject to classical, non-Markovian dephasing. Our approaches involve the construction of a dynamical model capturing qubit dynamics via autoregressive or Fourier-type protocols using only a historical record of projective measurements. A detailed comparison of achievable prediction horizons, model robustness, and measurement-noise-filtering capabilities for Kalman Filters (KF) and Gaussian Process Regression (GPR) algorithms is provided. We demonstrate superior performance from the autoregressive KF relative to Fourier-based KF approaches and focus on the role of filter optimization in achieving suitable performance. Finally, we examine several realizations of GPR using different kernels and discover that these approaches are generally not suitable for forward prediction. We highlight the underlying failure mechanism in this application and identify ways in which the output of the algorithm may be misidentified numerical artefacts. 	
1712.03690v2	http://arxiv.org/pdf/1712.03690v2	2017	Cascading collapse of online social networks	János Török|János Kertész	  Online social networks have increasing influence on our society, they may play decisive roles in politics and can be crucial for the fate of companies. Such services compete with each other and some may even break down rapidly. Using social network datasets we show the main factors leading to such a dramatic collapse. At early stage mostly the loosely bound users disappear, later collective effects play the main role leading to cascading failures. We present a theory based on a generalised threshold model to explain the findings and show how the collapse time can be estimated in advance using the dynamics of the churning users. Our results shed light to possible mechanisms of instabilities in other competing social processes. 	
1712.03785v1	http://arxiv.org/pdf/1712.03785v1	2017	A primer on noise-induced transitions in applied dynamical systems	Eric Forgoston|Richard O. Moore	  Noise plays a fundamental role in a wide variety of physical and biological dynamical systems. It can arise from an external forcing or due to random dynamics internal to the system. It is well established that even weak noise can result in large behavioral changes such as transitions between or escapes from quasi-stable states. These transitions can correspond to critical events such as failures or extinctions that make them essential phenomena to understand and quantify, despite the fact that their occurrence is rare. This article will provide an overview of the theory underlying the dynamics of rare events for stochastic models along with some example applications. 	
1712.10230v1	http://arxiv.org/pdf/1712.10230v1	2017	On quality of implementation of Fortran 2008 complex intrinsic functions   on branch cuts	Anton Shterenlikht	  Branch cuts in complex functions in combination with signed zero and signed infinity have important uses in fracture mechanics, jet flow and aerofoil analysis. We present benchmarks for validating Fortran 2008 complex functions - LOG, SQRT, ASIN, ACOS, ATAN, ASINH, ACOSH and ATANH - on branch cuts with arguments of all 3 IEEE floating point binary formats: binary32, binary64 and binary128. Results are reported with 8 Fortran 2008 compilers: GCC, Flang, Cray, Oracle, PGI, Intel, NAG and IBM. Multiple test failures were revealed, e.g. wrong signs of results or unexpected overflow, underflow, or NaN. We conclude that the quality of implementation of these Fortran 2008 intrinsics in many compilers is not yet sufficient to remove the need for special code for branch cuts. The test results are complemented by conformal maps of the branch cuts and detailed derivations of the values of these functions on branch cuts, to be used as a reference. The benchmarks are freely available from cmplx.sf.net. This work will be of interest to engineers who use complex functions, as well as to compiler and maths library developers. 	
1801.00376v1	http://arxiv.org/pdf/1801.00376v1	2018	Radiative thermal runaway due to negative differential thermal emission   across a solid-solid phase transition	David M. Bierman|Andrej Lenert|Mikhail A. Kats|You Zhou|Shuyan Zhang|Matthew De La Ossa|Shriram Ramanathan|Federico Capasso|Evelyn N. Wang	  Thermal runaway occurs when a rise in system temperature results in heat generation rates exceeding dissipation rates. Here we demonstrate that thermal runaway occurs in thermal radiative systems, given a sufficient level of negative differential thermal emission. By exploiting the insulator-to-metal phase transition of vanadium dioxide, we show that a small increase in heat generation (e.g., 10 nW/mm2) can result in a large change in surface temperature (e.g., ~35 K), as the thermal emitter switches from high emissivity to low emissivity. While thermal runaway is typically associated with catastrophic failure mechanisms, detailed understanding and control of this phenomenon may give rise to new opportunities in infrared sensing, camouflage, and rectification. 	
1801.01580v1	http://arxiv.org/pdf/1801.01580v1	2018	Binder migration during drying of lithium-ion battery electrodes:   modelling and comparison to experiment	Francesc Font|Bartosz Protas|Giles Richardson|Jamie M. Foster	  The drying process is a crucial step in electrode manufacture as it can affect the component distribution within the electrode. Phenomena such as binder migration can have negative effects in the form of poor cell performance (e.g. capacity fade) or mechanical failure (e.g. electrode delamination from the current collector). We present a mathematical model that tracks the evolution of the binder concentration in the electrode during drying. Solutions to the model predict that low drying rates lead to a favourable homogeneous binder profile across the electrode film, whereas high drying rates result in an unfavourable accumulation of binder near the evaporation surface. These results show strong qualitative agreement with experimental observations and provide a cogent explanation for why fast drying conditions result in poorly performing electrodes. Finally, we provide some guidelines on how the drying process could be optimised to offer relatively short drying times whilst simultaneously maintaining a roughly homogeneous binder distribution. 	
1801.06039v1	http://arxiv.org/pdf/1801.06039v1	2017	Reason and Method in Einstein's Relativity	Hisham Ghassib	  Relativity was Einstein's main research program and scientific project. It was an open-ended program that developed throughout Einstein's scientific career, giving rise to special relativity, general relativity and unified field theory. In this paper, we want to uncover the methodological logic of the Einsteinian program, which animated the whole program and its development, and as it was revealed in SR, GR, and unified field theory. We aver that the same methodological logic animated all these theories as Einstein's work progressed. Each of these theories contributed towards constructing Einstein's ambitious program. This paper is not a paper in the history of Relativity, but, rather, it utilizes our knowledge of this history to uncover the methodological logic of the relativity program and its development. This logic is latent in the historical narrative, but is not identical to it. We hope to show that the Einsteinian relativity project is still relevant today as a theoretical scheme, despite its failures and despite quantum mechanics. 	
1801.09250v1	http://arxiv.org/pdf/1801.09250v1	2018	Virtual Breakpoints for x86/64	Gregory Michael Price	  Efficient, reliable trapping of execution in a program at the desired location is a hot area of research for security professionals. The progression of debuggers and malware is akin to a game of cat and mouse - each are constantly in a state of trying to thwart one another. At the core of most efficient debuggers today is a combination of virtual machines and traditional binary modification breakpoints (int3). In this paper, we present a design for Virtual Breakpoints, a modification to the x86 MMU which brings breakpoint management into hardware alongside page tables. We demonstrate the fundamental abstraction failures of current trapping methods, and rebuild the mechanism from the ground up. Our design delivers fast, reliable trapping without the pitfalls of binary modification. 	
1802.04040v1	http://arxiv.org/pdf/1802.04040v1	2018	Yield precursor dislocation avalanches in small crystals: the   irreversibility transition	Xiaoyue Ni|Haolu Zhang|Danilo B. Liarte|Louis W. McFaul|Karin A. Dahmen|James P. Sethna|Julia R. Greer	  The transition from elastic to plastic deformation in crystalline metals shares history dependence and scale-invariant avalanche signature with other non-equilibrium systems under external loading: dilute colloidal suspensions, plastically-deformed amorphous solids, granular materials, and dislocation-based simulations of crystals. These other systems exhibit transitions with clear analogies to work hardening and yield stress, with many typically undergoing purely elastic behavior only after 'training' through repeated cyclic loading; studies in these other systems show a power law scaling of the hysteresis loop extent and of the training time as the peak load approaches a so-called reversible-irreversible transition (RIT). We discover here that deformation of small crystals shares these key characteristics: yielding and hysteresis in uniaxial compression experiments of single-crystalline Cu nano- and micro-pillars decay under repeated cyclic loading. The amplitude and decay time of the yield precursor avalanches diverge as the peak stress approaches failure stress for each pillar, with a power law scaling virtually equivalent to RITs in other nonequilibrium systems. 	
0404593v4	http://arxiv.org/pdf/cond-mat/0404593v4	2004	The shortest path to complex networks	S. N. Dorogovtsev|J. F. F. Mendes	  1. The birth of network science. 2. What are random networks? 3. Adjacency matrix. 4. Degree distribution. 5. What are simple networks? Classical random graphs. 6. Birth of the giant component. 7. Topology of the Web. 8.Uncorrelated networks. 9. What are small worlds? 10. Real networks are mesoscopic objects. 11. What are complex networks? 12. The configuration model. 13. The absence of degree--degree correlations. 14.Networks with correlated degrees.15.Clustering. 16. What are small-world networks? 17. `Small worlds' is not the same as `small-world networks'. 18. Fat-tailed degree distributions. 19.Reasons for the fat-tailed degree distributions. 20. Preferential linking. 21. Condensation of edges. 22. Cut-offs of degree distributions. 23. Reasons for correlations in networks. 24. Classical random graphs cannot be used for comparison with real networks. 25. How to measure degree--degree correlations. 26. Assortative and disassortative mixing. 27. Disassortative mixing does not mean that vertices of high degrees rarely connect to each other. 28. Reciprocal links in directed nets. 29. Ultra-small-world effect. 30. Tree ansatz. 31.Ultraresilience against random failures. 32. When correlated nets are ultraresilient. 33. Vulnerability of complex networks. 34. The absence of an epidemic threshold. 35. Search based on local information. 36.Ultraresilience disappears in finite nets. 37.Critical behavior of cooperative models on networks. 38. Berezinskii-Kosterlitz-Thouless phase transitions in networks. 39.Cascading failures. 40.Cliques & communities. 41. Betweenness. 42.Extracting communities. 43. Optimal paths. 44.Distributions of the shortest-path length & of the loop's length are narrow. 45. Diffusion on networks. 46. What is modularity? 47.Hierarchical organization of networks. 48. Convincing modelling of real-world networks:Is it possible? 49. The small Web.. 	
0509487v1	http://arxiv.org/pdf/cond-mat/0509487v1	2005	Fluctuations of power injection in randomly driven granular gases	P. Visco|A. Puglisi|A. Barrat|E. Trizac|F. van Wijland	  We investigate the large deviation function pi(w) for the fluctuations of the power W(t)=w t, integrated over a time t, injected by a homogeneous random driving into a granular gas, in the infinite time limit. Starting from a generalized Liouville equation we obtain an equation for the generating function of the cumulants mu(lambda) which appears as a generalization of the inelastic Boltzmann equation and has a clear physical interpretation. Reasonable assumptions are used to obtain mu(lambda) in a closed analytical form. A Legendre transform is sufficient to get the large deviation function pi(w). Our main result, apart from an estimate of all the cumulants of W(t) at large times t, is that pi(w) has no negative branch. This immediately results in the failure of the Gallavotti-Cohen Fluctuation Relation (GCFR), that in previous studies had been suggested to be valid for injected power in driven granular gases. We also present numerical results, in order to discuss the finite time behavior of the fluctuations of W(t). We discover that their probability density function converges extremely slowly to its asymptotic scaling form: the third cumulant saturates after a characteristic time larger than 50 mean free times and the higher order cumulants evolve even slower. The asymptotic value is in good agreement with our theory. Remarkably, a numerical check of the GCFR is feasible only at small times, since negative events disappear at larger times. At such small times this check leads to the misleading conclusion that GCFR is satisfied for pi(w). We offer an explanation for this remarkable apparent verification. In the inelastic Maxwell model, where a better statistics can be achieved, we are able to numerically observe the failure of GCFR. 	
0505033v2	http://arxiv.org/pdf/physics/0505033v2	2005	A physical model for aftershocks triggered by dislocation on a   rectangular fault	R. Console|F. Catalli	  We find the static displacement, stress, strain and the modified Columb failure stress produced in an elastic medium by a finite size rectangular fault after its dislocation with uniform stress drop but a non uniform dislocation on the source. The time-dependent rate of triggered earthquakes is estimated by a rate-state model applied to a uniformly distributed population of faults whose equilibrium is perturbated by a stress change caused only by the first dislocation. The rate of triggered events in our simulations is exponentially proportional to the stress change, but the time at which the maximum rate begins to decrease is variable from fractions of hour for positive stress changes of the order of some MPa, up to more than a year for smaller stress changes. As a consequence, the final number of triggered events is proportional to the stress change. The model predicts that the total number of events triggered on a plane containing the fault is proportional to the 2/3 power of the seismic moment. Indeed, the total number of aftershocks produced on the fault plane scales in magnitude as 10^{M}. Including the negative contribution of the stress drop inside the source, we observe that the number of events inhibited on the fault is, at long term, nearly identical to the number of those induced outside, representing a sort of conservative natural rule. Considering its behaviour in time, our model doesn't completely match the popular Omori law; in fact it has been shown that the seismicity induced closely to the fault edges is intense but of short duration, while that expected at large distances (up to some tens times the fault dimensions) exhibits a much slower decay. 	
0508147v3	http://arxiv.org/pdf/quant-ph/0508147v3	2010	Fault Models for Quantum Mechanical Switching Networks	Jacob Biamonte|Jeff S. Allen|Marek A. Perkowski	  The difference between faults and errors is that, unlike faults, errors can be corrected using control codes. In classical test and verification one develops a test set separating a correct circuit from a circuit containing any considered fault. Classical faults are modelled at the logical level by fault models that act on classical states. The stuck fault model, thought of as a lead connected to a power rail or to a ground, is most typically considered. A classical test set complete for the stuck fault model propagates both binary basis states, 0 and 1, through all nodes in a network and is known to detect many physical faults. A classical test set complete for the stuck fault model allows all circuit nodes to be completely tested and verifies the function of many gates. It is natural to ask if one may adapt any of the known classical methods to test quantum circuits. Of course, classical fault models do not capture all the logical failures found in quantum circuits. The first obstacle faced when using methods from classical test is developing a set of realistic quantum-logical fault models. Developing fault models to abstract the test problem away from the device level motivated our study. Several results are established. First, we describe typical modes of failure present in the physical design of quantum circuits. From this we develop fault models for quantum binary circuits that enable testing at the logical level. The application of these fault models is shown by adapting the classical test set generation technique known as constructing a fault table to generate quantum test sets. A test set developed using this method is shown to detect each of the considered faults. 	
1009.3183v1	http://arxiv.org/pdf/1009.3183v1	2010	Interdependent networks with correlated degrees of mutually dependent   nodes	Sergey V. Buldyrev|Nathaniel Shere|Gabriel A. Cwilich	  We study a problem of failure of two interdependent networks in the case of correlated degrees of mutually dependent nodes. We assume that both networks (A and B) have the same number of nodes $N$ connected by the bidirectional dependency links establishing a one-to-one correspondence between the nodes of the two networks in a such a way that the mutually dependent nodes have the same number of connectivity links, i.e. their degrees coincide. This implies that both networks have the same degree distribution $P(k)$. We call such networks correspondently coupled networks (CCN). We assume that the nodes in each network are randomly connected. We define the mutually connected clusters and the mutual giant component as in earlier works on randomly coupled interdependent networks and assume that only the nodes which belong to the mutual giant component remain functional. We assume that initially a $1-p$ fraction of nodes are randomly removed due to an attack or failure and find analytically, for an arbitrary $P(k)$, the fraction of nodes $\mu(p)$ which belong to the mutual giant component. We find that the system undergoes a percolation transition at certain fraction $p=p_c$ which is always smaller than the $p_c$ for randomly coupled networks with the same $P(k)$. We also find that the system undergoes a first order transition at $p_c>0$ if $P(k)$ has a finite second moment. For the case of scale free networks with $2<\lambda \leq 3$, the transition becomes a second order transition. Moreover, if $\lambda<3$ we find $p_c=0$ as in percolation of a single network. For $\lambda=3$ we find an exact analytical expression for $p_c>0$. Finally, we find that the robustness of CCN increases with the broadness of their degree distribution. 	
1209.0959v3	http://arxiv.org/pdf/1209.0959v3	2013	How big is too big? Critical Shocks for Systemic Failure Cascades	Claudio J. Tessone|Antonios Garas|Beniamino Guerra|Frank Schweitzer	  External or internal shocks may lead to the collapse of a system consisting of many agents. If the shock hits only one agent initially and causes it to fail, this can induce a cascade of failures among neighoring agents. Several critical constellations determine whether this cascade remains finite or reaches the size of the system, i.e. leads to systemic risk. We investigate the critical parameters for such cascades in a simple model, where agents are characterized by an individual threshold \theta_i determining their capacity to handle a load \alpha\theta_i with 1-\alpha being their safety margin. If agents fail, they redistribute their load equally to K neighboring agents in a regular network. For three different threshold distributions P(\theta), we derive analytical results for the size of the cascade, X(t), which is regarded as a measure of systemic risk, and the time when it stops. We focus on two different regimes, (i) EEE, an external extreme event where the size of the shock is of the order of the total capacity of the network, and (ii) RIE, a random internal event where the size of the shock is of the order of the capacity of an agent. We find that even for large extreme events that exceed the capacity of the network finite cascades are still possible, if a power-law threshold distribution is assumed. On the other hand, even small random fluctuations may lead to full cascades if critical conditions are met. Most importantly, we demonstrate that the size of the "big" shock is not the problem, as the systemic risk only varies slightly for changes of 10 to 50 percent of the external shock. Systemic risk depends much more on ingredients such as the network topology, the safety margin and the threshold distribution, which gives hints on how to reduce systemic risk. 	
1702.08695v2	http://arxiv.org/pdf/1702.08695v2	2017	Online Robot Introspection via Wrench-based Action Grammars	Juan Rojas|Shuangqi Luo|Dingqiao Zhu|Yunlong Du|Hongbin Lin|Zhengjie Huang|Wenwei Kuang|Kensuke Harada	  Robotic failure is all too common in unstructured robot tasks. Despite well-designed controllers, robots often fail due to unexpected events. How do robots measure unexpected events? Many do not. Most robots are driven by the sense-plan act paradigm, however more recently robots are undergoing a sense-plan-act-verify paradigm. In this work, we present a principled methodology to bootstrap online robot introspection for contact tasks. In effect, we are trying to enable the robot to answer the question: what did I do? Is my behavior as expected or not? To this end, we analyze noisy wrench data and postulate that the latter inherently contains patterns that can be effectively represented by a vocabulary. The vocabulary is generated by segmenting and encoding the data. When the wrench information represents a sequence of sub-tasks, we can think of the vocabulary forming a sentence (set of words with grammar rules) for a given sub-task; allowing the latter to be uniquely represented. The grammar, which can also include unexpected events, was classified in offline and online scenarios as well as for simulated and real robot experiments. Multiclass Support Vector Machines (SVMs) were used offline, while online probabilistic SVMs were are used to give temporal confidence to the introspection result. The contribution of our work is the presentation of a generalizable online semantic scheme that enables a robot to understand its high-level state whether nominal or abnormal. It is shown to work in offline and online scenarios for a particularly challenging contact task: snap assemblies. We perform the snap assembly in one-arm simulated and real one-arm experiments and a simulated two-arm experiment. This verification mechanism can be used by high-level planners or reasoning systems to enable intelligent failure recovery or determine the next most optima manipulation skill to be used. 	
1705.09701v1	http://arxiv.org/pdf/1705.09701v1	2017	SMORE: A Cold Data Object Store for SMR Drives (Extended Version)	Peter Macko|Xiongzi Ge|John Haskins Jr.|James Kelley|David Slik|Keith A. Smith|Maxim G. Smith	  Shingled magnetic recording (SMR) increases the capacity of magnetic hard drives, but it requires that each zone of a disk be written sequentially and erased in bulk. This makes SMR a good fit for workloads dominated by large data objects with limited churn. To explore this possibility, we have developed SMORE, an object storage system designed to reliably and efficiently store large, seldom-changing data objects on an array of host-managed or host-aware SMR disks.   SMORE uses a log-structured approach to accommodate the constraint that all writes to an SMR drive must be sequential within large shingled zones. It stripes data across zones on separate disks, using erasure coding to protect against drive failure. A separate garbage collection thread reclaims space by migrating live data out of the emptiest zones so that they can be trimmed and reused. An index stored on flash and backed up to the SMR drives maps object identifiers to on-disk locations. SMORE interleaves log records with object data within SMR zones to enable index recovery after a system crash (or failure of the flash device) without any additional logging mechanism.   SMORE achieves full disk bandwidth when ingesting data---with a variety of object sizes---and when reading large objects. Read performance declines for smaller object sizes where inter- object seek time dominates. With a worst-case pattern of random deletions, SMORE has a write amplification (not counting RAID parity) of less than 2.0 at 80% occupancy. By taking an index snapshot every two hours, SMORE recovers from crashes in less than a minute. More frequent snapshots allow faster recovery. 	
1707.05063v1	http://arxiv.org/pdf/1707.05063v1	2017	Optimal Storage under Unsynchrononized Mobile Byzantine Faults	Silvia Bonomi|Antonella Del Pozzo|Maria Potop-Butucaru|Sébastien Tixeuil	  In this paper we prove lower and matching upper bounds for the number of servers required to implement a regular shared register that tolerates unsynchronized Mobile Byzantine failures. We consider the strongest model of Mobile Byzantine failures to date: agents are moved arbitrarily by an omniscient adversary from a server to another in order to deviate their computation in an unforeseen manner. When a server is infected by an Byzantine agent, it behaves arbitrarily until the adversary decides to move the agent to another server. Previous approaches considered asynchronous servers with synchronous mobile Byzantine agents (yielding impossibility results), and synchronous servers with synchronous mobile Byzantine agents (yielding optimal solutions for regular register implementation, even in the case where servers and agents periods are decoupled). We consider the remaining open case of synchronous servers with unsynchronized agents, that can move at their own pace, and change their pace during the execution of the protocol. Most of our findings relate to lower bounds, and characterizing the model parameters that make the problem solvable. It turns out that unsynchronized mobile Byzantine agent movements requires completely new proof arguments, that can be of independent interest when studying other problems in this model. Additionally, we propose a generic server-based algorithm that emulates a regular register in this model, that is tight with respect to the number of mobile Byzantine agents that can be tolerated. Our emulation spans two awareness models: servers with and without self-diagnose mechanisms. In the first case servers are aware that the mobile Byzantine agent has left and hence they can stop running the protocol until they recover a correct state while in the second case, servers are not aware of their faulty state and continue to run the protocol using an incorrect local state. 	
1708.07422v1	http://arxiv.org/pdf/1708.07422v1	2017	Resilience Design Patterns: A Structured Approach to Resilience at   Extreme Scale	Saurabh Hukerikar|Christian Engelmann	  Reliability is a serious concern for future extreme-scale high-performance computing (HPC) systems. While the HPC community has developed various resilience solutions, the solution space remains fragmented. There are no formal methods and metrics to integrate the various HPC resilience techniques into composite solutions, nor are there methods to holistically evaluate the adequacy and efficacy of such solutions in terms of their protection coverage, and their performance & power efficiency characteristics. In this paper, we develop a structured approach to the design, evaluation and optimization of HPC resilience using the concept of design patterns. A design pattern is a general repeatable solution to a commonly occurring problem. We identify the problems caused by various types of faults, errors and failures in HPC systems and the techniques used to deal with these events. Each well-known solution that addresses a specific HPC resilience challenge is described in the form of a pattern. We develop a complete catalog of such resilience design patterns, which may be used as essential building blocks when designing and deploying resilience solutions. We also develop a design framework that enhances a designer's understanding the opportunities for integrating multiple patterns across layers of the system stack and the important constraints during implementation of the individual patterns. It is also useful for defining mechanisms and interfaces to coordinate flexible fault management across hardware and software components. The overall goal of this work is to establish a systematic methodology for the design and evaluation of resilience technologies in extreme-scale HPC systems that keep scientific applications running to a correct solution in a timely and cost-efficient manner despite frequent faults, errors, and failures of various types. 	
0506717v2	http://arxiv.org/pdf/cond-mat/0506717v2	2005	A simple microscopic model for the dynamics of adhesive failure	Dominic Vella|L. Mahadevan	  We consider a microscopic model for the failure of soft adhesives in tension based on ideas of bond rupture under dynamic loading. Focusing on adhesive failure under loading at constant velocity, we demonstrate that bi-modal curves of stress against strain may occur due to effects of finite polymer chain or bond length and characterise the loading conditions under which such bi-modal behaviour is observed. The results of this analysis are in qualitative agreement with experiments performed on unconfined adhesives in which failure does not occur by cavitation. 	
0603618v1	http://arxiv.org/pdf/cond-mat/0603618v1	2006	Precursors and prediction of catastrophic avalanches	Srutarshi Pradhan|Bikas K. Chakrabarti	  In this work we review the precursors of catastrophic avalanches (global failures) in several failure models, namely (a) Fiber Bundle Model (FBM), (b) Random Fuse Model (RFM), (c) Sandpile Models and (d) Fractal Overlap Model. The precursor parameters identified here essentially reflect the growing correlations within such systems as they approach their respective failure points. As we show, often they help us to predict the global failure points in advance. 	
0702120v1	http://arxiv.org/pdf/physics/0702120v1	2007	Failure in Complex Social Networks	Damon Centola	  Tolerance against failures and errors is an important feature of many complex networked systems [1,2]. It has been shown that a class of inhomogeneously wired networks called scale-free[1,3] networks can be surprisingly robust to failures, suggesting that socially self-organized systems such as the World-Wide Web, the Internet, and other kinds of social networks [4] may have significant tolerance against failures by virtue of their scale-free degree distribution. I show that this finding only holds on the assumption that the diffusion process supported by the network is a simple one, requiring only a single contact in order for transmission to be successful. 	
1205.6561v1	http://arxiv.org/pdf/1205.6561v1	2012	Cascade Failure in a Phase Model of Power Grids	Hidetsugu Sakaguchi|Tatsuma Matsuo	  We propose a phase model to study cascade failure in power grids composed of generators and loads. If the power demand is below a critical value, the model system of power grids maintains the standard frequency by feedback control. On the other hand, if the power demand exceeds the critical value, an electric failure occurs via step out (loss of synchronization) or voltage collapse. The two failures are incorporated as two removal rules of generator nodes and load nodes. We perform direct numerical simulation of the phase model on a scale-free network and compare the results with a mean-field approximation. 	
1211.0592v1	http://arxiv.org/pdf/1211.0592v1	2012	Requirements of a Recovery Solution for Failure of Composite Web   Services	Hadi Saboohi|Sameem Abdul Kareem	  Web services are building blocks of interoperable systems. Composing Web services makes the processes capable of doing complex tasks. Composite services may fail during their execution which can be diagnosed by a mediator. The mediator adapts the structure so that the failure is recovered. Moreover, future executions should avoid the situation or organize a strategy to repair the structure with a minimum delay. In this paper the failure reasons of a composite service are reviewed. Furthermore, the requirements of a solution for recovery of a system from a failure are investigated. 	
1301.4287v1	http://arxiv.org/pdf/1301.4287v1	2013	Maximizing Reliability in WDM Networks through Lightpath Routing	Hyang-Won Lee|Kayi Lee|Eytan Modiano	  We study the reliability maximization problem in WDM networks with random link failures. Reliability in these networks is defined as the probability that the logical network is connected, and it is determined by the underlying lightpath routing, network topologies and the link failure probability. By introducing the notion of lexicographical ordering for lightpath routings, we characterize precise optimization criteria for maximum reliability in the low failure probability regime. Based on the optimization criteria, we develop lightpath routing algorithms that maximize the reliability, and logical topology augmentation algorithms for further improving reliability. We also study the reliability maximization problem in the high failure probability regime. 	
1302.1256v2	http://arxiv.org/pdf/1302.1256v2	2013	Repairing Multiple Failures in the Suh-Ramchandran Regenerating Codes	Junyu Chen|Kenneth W. Shum	  Using the idea of interference alignment, Suh and Ramchandran constructed a class of minimum-storage regenerating codes which can repair one systematic or one parity-check node with optimal repair bandwidth. With the same code structure, we show that in addition to single node failure, double node failures can be repaired collaboratively with optimal repair bandwidth as well. We give an example of how to repair double failures in the Suh-Ramchandran regenerating code with six nodes, and give the proof for the general case. 	
1302.4779v1	http://arxiv.org/pdf/1302.4779v1	2013	Failure Data Analysis of HPC Systems	Charng-Da Lu	  Continuous availability of HPC systems built from commodity components have become a primary concern as system size grows to thousands of processors. In this paper, we present the analysis of 8-24 months of real failure data collected from three HPC systems at the National Center for Supercomputing Applications (NCSA) during 2001-2004. The results show that the availability is 98.7-99.8% and most outages are due to software halts. On the other hand, the downtime are mostly contributed by hardware halts or scheduled maintenance. We also used failure clustering analysis to identify several correlated failures. 	
1309.7498v1	http://arxiv.org/pdf/1309.7498v1	2013	Most probable failure scenario in a model power grid with random power   demand	Misha Stepanov|Aditya Sundarrajan	  We consider a simple system with a local synchronous generator and a load whose power consumption is a random process. The most probable scenario of system failure (synchronization loss) is considered, and it is argued that its knowledge is virtually enough to estimate the probability of failure per unit time. We discuss two numerical methods to obtain the "optimal" evolution leading to failure. 	
1404.4225v1	http://arxiv.org/pdf/1404.4225v1	2014	Efficient rare event simulation for failure problems in random media	Jingchen Liu|Jianfeng Lu|Xiang Zhou	  In this paper we study rare events associated to solutions of elliptic partial differential equations with spatially varying random coefficients. The random coefficients follow the lognormal distribution, which is determined by a Gaussian process. This model is employed to study the failure problem of elastic materials in random media in which the failure is characterized by that the strain field exceeds a high threshold. We propose an efficient importance sampling scheme to compute small failure probabilities in the high threshold limit. The change of measure in our scheme is parametrized by two density functions. The efficiency of the importance sampling scheme is validated by numerical examples. 	
1502.01509v1	http://arxiv.org/pdf/1502.01509v1	2015	OS-level Failure Injection with SystemTap	Camille Coti|Nicolas Greneche	  Failure injection in distributed systems has been an important issue to experiment with robust, resilient distributed systems. In order to reproduce real-life conditions, parts of the application must be killed without letting the operating system close the existing network communications in a "clean" way. When a process is simply killed, the OS closes them. SystemTap is a an infrastructure that probes the Linux kernel's internal calls. If processes are killed at kernel-level, they can be destroyed without letting the OS do anything else. In this paper, we present a kernel-level failure injection system based on SystemTap. We present how it can be used to implement deterministic and probabilistic failure scenarios. 	
1503.00912v1	http://arxiv.org/pdf/1503.00912v1	2015	Exploring Beta-Like Distributions	H. R. N. van Erp|R. O. Linger|P. H. A. J. M. van Gelder	  The most well known probability distribution of probabilities is the Beta distribution. If we have observed $r$ `successes', each having a probability $\theta$, and $n-r$ `failures', each having a probability $1-\theta$. In this paper we will derive a whole family of Beta-like distributions, which take as their data not only the number of successes and failures, but also values on predictor variables and time to failure or time without failure. 	
1604.07474v1	http://arxiv.org/pdf/1604.07474v1	2016	Advancing Dynamic Fault Tree Analysis	Matthias Volk|Sebastian Junges|Joost-Pieter Katoen	  This paper presents a new state space generation approach for dynamic fault trees (DFTs) together with a technique to synthesise failures rates in DFTs. Our state space generation technique aggressively exploits the DFT structure --- detecting symmetries, spurious non-determinism, and don't cares. Benchmarks show a gain of more than two orders of magnitude in terms of state space generation and analysis time. Our approach supports DFTs with symbolic failure rates and is complemented by parameter synthesis. This enables determining the maximal tolerable failure rate of a system component while ensuring that the mean time of failure stays below a threshold. 	
1706.03664v1	http://arxiv.org/pdf/1706.03664v1	2017	Concurrent risks of dam failure due to internal degradation, strong   winds, snow and drought	Chris Collier|Alan Gadian|Ralph Burton|James Groves	  The chance (or probability) of a dam failure can change for various reasons such as structural degradation, the impacts of climate change and land-use change. Similarly the consequences of dam failure (flooding) can change for many reasons such as growth in the population in areas below a dam. Consequently both the chance that a dam might fail and the likely consequences of that failure can change over time. It is therefore crucial that reservoir safety risk analysis methods and decision-making processes are able to support (as a minimum) what-if testing (or sensitivity testing) to take into account these changes over time to gauge their effect on the estimated risk of dam failure. The consequences of a dam failure relate to the vulnerability and exposure of the receptors (for example, people, property and environment) to floodwater. Also the probability of dam failure varies with age, design and construction of the dam. Spillway failure may be caused by the dissipation of energy from water flowing down the spillway, and embankment erosion (scour) may be caused by a dam overtopping. The occurrence of these events depends upon the dam design and the likelihood of extreme rainfall, also in the case of overtopping wind-driven waves on the reservoir surface. In this study the meteorological situations of notable recent events i.e. the Boltby, North Yorkshire incident, 19 June 2005 in which the dam almost overtopped, and the spillway failure of the Ulley Dam near Rotherham at the end of June 2007, are studied. The WRF numerical model will be used to indicate how these meteorological situations might be maximized, and be coupled with the occurrence of other failure modes such as the likelihood of internal dam failure assessed from previous work by government panel engineers. 	
1102.2616v1	http://arxiv.org/pdf/1102.2616v1	2011	An Improved Multiple Faults Reassignment based Recovery in Cluster   Computing	Sanjay Bansal|Sanjeev Sharma	  In case of multiple node failures performance becomes very low as compare to single node failure. Failures of nodes in cluster computing can be tolerated by multiple fault tolerant computing. Existing recovery schemes are efficient for single fault but not with multiple faults. Recovery scheme proposed in this paper having two phases; sequentially phase, concurrent phase. In sequentially phase, loads of all working nodes are uniformly and evenly distributed by proposed dynamic rank based and load distribution algorithm. In concurrent phase, loads of all failure nodes as well as new job arrival are assigned equally to all available nodes by just finding the least loaded node among the several nodes by failure nodes job allocation algorithm. Sequential and concurrent executions of algorithms improve the performance as well better resource utilization. Dynamic rank based algorithm for load redistribution works as a sequential restoration algorithm and reassignment algorithm for distribution of failure nodes to least loaded computing nodes works as a concurrent recovery reassignment algorithm. Since load is evenly and uniformly distributed among all available working nodes with less number of iterations, low iterative time and communication overheads hence performance is improved. Dynamic ranking algorithm is low overhead, high convergence algorithm for reassignment of tasks uniformly among all available nodes. Reassignments of failure nodes are done by a low overhead efficient failure job allocation algorithm. Test results to show effectiveness of the proposed scheme are presented. 	
1103.3671v2	http://arxiv.org/pdf/1103.3671v2	2011	Easy Impossibility Proofs for k-Set Agreement in Message Passing Systems	Martin Biely|Peter Robinson|Ulrich Schmid	  Despite of being quite similar agreement problems, consensus and general k-set agreement require surprisingly different techniques for proving the impossibility in asynchronous systems with crash failures: Rather than relatively simple bivalence arguments as in the impossibility proof for consensus (= 1-set agreement) in the presence of a single crash failure, known proofs for the impossibility of k-set agreement in systems with at least k>1 crash failures use algebraic topology or a variant of Sperner's Lemma. In this paper, we present a generic theorem for proving the impossibility of k-set agreement in various message passing settings, which is based on a simple reduction to the consensus impossibility in a certain subsystem. We demonstrate the broad applicability of our result by exploring the possibility/impossibility border of k-set agreement in several message-passing system models: (i) asynchronous systems with crash failures, (ii) partially synchronous processes with (initial) crash failures, and (iii) asynchronous systems augmented with failure detectors. In (i) and (ii), the impossibility part is just an instantiation of our main theorem, whereas the possibility of achieving k-set agreement in (ii) follows by generalizing the consensus algorithm for initial crashes by Fisher, Lynch and Patterson. In (iii), applying our technique yields the exact border for the parameter k where k-set agreement is solvable with the failure detector class (Sigma_k,Omega_k), for (1<= k<= n-1), of Bonnet and Raynal. Considering that Sigma_k was shown to be necessary for solving k-set agreement, this result yields new insights on the quest for the weakest failure detector. 	
1401.0750v4	http://arxiv.org/pdf/1401.0750v4	2014	An Interaction Model for Simulation and Mitigation of Cascading Failures	Junjian Qi|Kai Sun|Shengwei Mei	  In this paper the interactions between component failures are quantified and the interaction matrix and interaction network are obtained. The quantified interactions can capture the general propagation patterns of the cascades from utilities or simulation, thus helping to better understand how cascading failures propagate and to identify key links and key components that are crucial for cascading failure propagation. By utilizing these interactions a high-level probabilistic model called interaction model is proposed to study the influence of interactions on cascading failure risk and to support online decision-making. It is much more time efficient to first quantify the interactions between component failures with fewer original cascades from a more detailed cascading failure model and then perform the interaction model simulation than it is to directly simulate a large number of cascades with a more detailed model. Interaction-based mitigation measures are suggested to mitigate cascading failure risk by weakening key links, which can be achieved in real systems by wide area protection such as blocking of some specific protective relays. The proposed interaction quantifying method and interaction model are validated with line outage data generated by the AC OPA cascading simulations on the IEEE 118-bus system. 	
1401.2282v1	http://arxiv.org/pdf/1401.2282v1	2014	How do heterogeneities in operating environments affect field failure   predictions and test planning?	Zhi-Sheng Ye|Yili Hong|Yimeng Xie	  The main objective of accelerated life tests (ALTs) is to predict fraction failings of products in the field. However, there are often discrepancies between the predicted fraction failing from the lab testing data and that from the field failure data, due to the yet unobserved heterogeneities in usage and operating conditions. Most previous research on ALT planning and data analysis ignores the discrepancies, resulting in inferior test plans and biased predictions. In this paper we model the heterogeneous environments together with their effects on the product failures as a frailty term to link the lab failure time distribution and field failure time distribution of a product. We show that in the presence of the heterogeneous operating conditions, the hazard rate function of the field failure time distribution exhibits a range of shapes. Statistical inference procedure for the frailty models is developed when both the ALT data and the field failure data are available. Based on the frailty models, optimal ALT plans aimed at predicting the field failure time distribution are obtained. The developed methods are demonstrated through a real life example. 	
1406.5282v2	http://arxiv.org/pdf/1406.5282v2	2014	STAIR Codes: A General Family of Erasure Codes for Tolerating Device and   Sector Failures	Mingqiang Li|Patrick P. C. Lee	  Practical storage systems often adopt erasure codes to tolerate device failures and sector failures, both of which are prevalent in the field. However, traditional erasure codes employ device-level redundancy to protect against sector failures, and hence incur significant space overhead. Recent sector-disk (SD) codes are available only for limited configurations. By making a relaxed but practical assumption, we construct a general family of erasure codes called \emph{STAIR codes}, which efficiently and provably tolerate both device and sector failures without any restriction on the size of a storage array and the numbers of tolerable device failures and sector failures. We propose the \emph{upstairs encoding} and \emph{downstairs encoding} methods, which provide complementary performance advantages for different configurations. We conduct extensive experiments on STAIR codes in terms of space saving, encoding/decoding speed, and update cost. We demonstrate that STAIR codes not only improve space efficiency over traditional erasure codes, but also provide better computational efficiency than SD codes based on our special code construction. Finally, we present analytical models that characterize the reliability of STAIR codes, and show that the support of a wider range of configurations by STAIR codes is critical for tolerating sector failure bursts discovered in the field. 	
1610.00997v1	http://arxiv.org/pdf/1610.00997v1	2016	Impact of embedding on predictability of failure-recovery dynamics in   networks	Lucas Böttcher|Mirko Lukovic|Jan Nagler|Shlomo Havlin|Hans J. Herrmann	  Failure, damage spread and recovery crucially underlie many spatially embedded networked systems ranging from transportation structures to the human body. Here we study the interplay between spontaneous damage, induced failure and recovery in both embedded and non-embedded networks. In our model the network's components follow three realistic processes that capture these features: (i) spontaneous failure of a component independent of the neighborhood (internal failure), (ii) failure induced by failed neighboring nodes (external failure) and (iii) spontaneous recovery of a component.We identify a metastable domain in the global network phase diagram spanned by the model's control parameters where dramatic hysteresis effects and random switching between two coexisting states are observed. The loss of predictability due to these effects depend on the characteristic link length of the embedded system. For the Euclidean lattice in particular, hysteresis and switching only occur in an extremely narrow region of the parameter space compared to random networks. We develop a unifying theory which links the dynamics of our model to contact processes. Our unifying framework may help to better understand predictability and controllability in spatially embedded and random networks where spontaneous recovery of components can mitigate spontaneous failure and damage spread in the global network. 	
1610.01364v1	http://arxiv.org/pdf/1610.01364v1	2016	FMEA Based Risk Assessment of Component Failure Modes in Industrial   Radiography	Alok Pandey|Meghraj Singh|A. U. Sonawane|Prashant S. Rawat	  Industrial radiography has its inimitable role in non-destructive examinations. Industrial radiography devices, consisting of significantly high activity of the radioisotopes, are operated manually by remotely held control unit. Malfunctioning of these devices may cause potential exposure to the operator and nearby public, and thus should be practiced under a systematic risk control. To ensure the radiation safety, proactive risk assessment should be implemented. Risk assessment in industrial radiography using the Failure Modes & Effect Analysis (FMEA) for the design and operation of industrial radiography exposure devices has been carried out in this study. Total 56 component failure modes were identified and Risk Priority Numbers (RPNs) were assigned by the FMEA expert team, based on the field experience and reported failure data of various components. Results shows all the identified failure modes have RPN in the range of 04 to 216 and most of the higher RPN are due to low detectability and high severity levels. Assessment reveals that increasing failure detectability is a practical and feasible approach to reduce the risk in most of the failure modes of industrial radiography devices. Actions for reducing RPN for each failure mode have been suggested. Feasibility of FMEA for risk assessment in industrial radiography has been established by this study 	
1701.01539v4	http://arxiv.org/pdf/1701.01539v4	2017	Algorithms for Optimal Replica Placement Under Correlated Failure in   Hierarchical Failure Domains	K. Alex Mills|R. Chandrasekaran|Neeraj Mittal	  In data centers, data replication is the primary method used to ensure availability of customer data. To avoid correlated failure, cloud storage infrastructure providers model hierarchical failure domains using a tree, and avoid placing a large number of data replicas within the same failure domain (i.e. on the same branch of the tree). Typical best practices ensure that replicas are distributed across failure domains, but relatively little is known concerning optimization algorithms for distributing data replicas. Using a hierarchical model, we answer how to distribute replicas across failure domains optimally. We formulate a novel optimization problem for replica placement in data centers. As part of our problem, we formalize and explain a new criterion for optimizing a replica placement. Our overall goal is to choose placements in which correlated failures disable as few replicas as possible. We provide two optimization algorithms for dependency models represented by trees. We first present an $O(n + \rho \log \rho)$ time dynamic programming algorithm for placing $\rho$ replicas of a single file on the leaves (representing servers) of a tree with $n$ vertices. We next consider the problem of placing replicas of $m$ blocks of data, where each block may have different replication factors. For this problem, we give an exact algorithm which runs in polynomial time when the skew, the difference in the number of replicas between the largest and smallest blocks of data, is constant. 	
1709.07302v2	http://arxiv.org/pdf/1709.07302v2	2018	Influence of Clustering on Cascading Failures in Interdependent Systems	Richard J. La	  We study the influence of clustering, more specifically triangles, on cascading failures in interdependent networks or systems, in which we model the dependence between comprising systems using a dependence graph. First, we propose a new model that captures how the presence of triangles in the dependence graph alters the manner in which failures transmit from affected systems to others. Unlike existing models, the new model allows us to approximate the failure propagation dynamics using a multi-type branching process, even with triangles. Second, making use of the model, we provide a simple condition that indicates how increasing clustering will affect the likelihood that a random failure triggers a cascade of failures, which we call the probability of cascading failures (PoCF). In particular, our condition reveals an intriguing observation that the influence of clustering on PoCF depends on the vulnerability of comprising systems to an increasing number of failed neighboring systems and the current PoCF, starting with different types of failed systems. Our numerical studies hint that increasing clustering impedes cascading failures under both (truncated) power law and Poisson degree distributions. Furthermore, our finding suggests that, as the degree distribution becomes more concentrated around the mean degree with smaller variance, increasing clustering will have greater impact on the PoCF. A numerical investigation of networks with Poisson and power law degree distributions reflects this finding and demonstrates that increasing clustering reduces the PoCF much faster under Poisson degree distributions in comparison to power law degree distributions. 	
1712.09666v1	http://arxiv.org/pdf/1712.09666v1	2017	A Fast and Accurate Failure Frequency Approximation for $k$-Terminal   Reliability Systems	Anoosheh Heidarzadeh|Alex Sprintson|Chanan Singh	  This paper considers the problem of approximating the failure frequency of large-scale composite $k$-terminal reliability systems. In such systems, the nodes ($k$ of which are terminals) are connected through components which are subject to random failure and repair processes. At any time, a system failure occurs if the surviving system fails to connect all the k terminals together. We assume that each component's up-times and down-times follow statistically independent stationary random processes, and these processes are statistically independent across the components. In this setting, the exact computation of failure frequency is known to be computationally intractable (NP-hard). In this work, we present an algorithm to approximate the failure frequency for any given multiplicative error factor that runs in polynomial time in the number of (minimal) cutsets. Moreover, for the special case of all-terminal reliability systems, i.e., where all nodes are terminals, we propose an algorithm for approximating the failure frequency within an arbitrary multiplicative error that runs in polynomial time in the number of nodes (which can be much smaller than the number of cutsets). In addition, our simulation results confirm that the proposed method is much faster and more accurate than the Monte Carlo simulation technique for approximating the failure frequency. 	
1801.10321v1	http://arxiv.org/pdf/1801.10321v1	2018	Derivative-Free Failure Avoidance Control for Manipulation using Learned   Support Constraints	Jonathan Lee|Michael Laskey|Roy Fox|Ken Goldberg	  Learning to accomplish tasks such as driving, grasping or surgery from supervisor demonstrations can be risky when the execution of the learned policy leads to col- lisions and other costly failures. Adding explicit constraints to stay within safe zones is often not possible when the state representations are complex. Furthermore, enforcing these constraints during execution of the learned policy can be difficult in environments where dynamics are not known. In this paper, we propose a two-phase method for safe control from demonstrations in robotic manipulation tasks where changes in state are limited by the magnitude of control applied. In the first phase, we use support estimation of supervisor demonstrations to infer implicit constraints on states in addition to learning a policy directly from the observed controls. We also propose a time-varying modification to the support estimation problem allowing for accurate estimation on sequential tasks. In the second phase, we present a switching policy to prevent the robot from leaving safe regions of the state space during run time using the decision function of the estimated support. The policy switches between the robot's learned policy and a novel failure avoidance policy depending on the distance to the boundary of the support. We prove that inferred constraints are guaranteed to be enforced using this failure avoidance policy if the support is well-estimated. A simulated pushing task suggests that support estimation and failure avoidance control can reduce failures by 87% while sacrificing only 40% of performance. On a line tracking task using a da Vinci Surgical Robot, failure avoidance control reduced failures by 84%. 	
0511134v1	http://arxiv.org/pdf/cond-mat/0511134v1	2005	Optimal Prediction of Time-to-Failure from Information Revealed by   Damage	D. Sornette|J. V. Andersen	  We present a general prediction scheme of failure times based on updating continuously with time the probability for failure of the global system, conditioned on the information revealed on the pre-existing idiosyncratic realization of the system by the damage that has occurred until the present time. Its implementation on a simple prototype system of interacting elements with unknown random lifetimes undergoing irreversible damage until a global rupture occurs shows that the most probable predicted failure time (mode) may evolve non-monotonically with time as information is incorporated in the prediction scheme. In addition, both the mode, its standard deviation and, in fact, the full distribution of predicted failure times exhibit sensitive dependence on the realization of the system, similarly to ``chaos'' in spinglasses, providing a multi-dimensional dynamical explanation for the broad distribution of failure times observed in many empirical situations. 	
0801.0559v1	http://arxiv.org/pdf/0801.0559v1	2008	Failure patterns caused by localized rise in pore-fluid overpressure and   effective strength of rocks	Alexander Rozhko|Yuri Podladchikov|François Renard	  In order to better understand the interaction between pore-fluid overpressure and failure patterns in rocks we consider a porous elasto-plastic medium in which a laterally localized overpressure line source is imposed at depth below the free surface. We solve numerically the fluid filtration equation coupled to the gravitational force balance and poro-elasto-plastic rheology equations. Systematic numerical simulations, varying initial stress, intrinsic material properties and geometry, show the existence of five distinct failure patterns caused by either shear banding or tensile fracturing. The value of the critical pore-fluid overpressure at the onset of failure is derived from an analytical solution that is in excellent agreement with numerical simulations. Finally, we construct a phase-diagram that predicts the domains of the different failure patterns and at the onset of failure. 	
0809.4107v1	http://arxiv.org/pdf/0809.4107v1	2008	Modelling interdependencies between the electricity and information   infrastructures	Jean-Claude Laprie|Karama Kanoun|Mohamed Kaaniche	  The aim of this paper is to provide qualitative models characterizing interdependencies related failures of two critical infrastructures: the electricity infrastructure and the associated information infrastructure. The interdependencies of these two infrastructures are increasing due to a growing connection of the power grid networks to the global information infrastructure, as a consequence of market deregulation and opening. These interdependencies increase the risk of failures. We focus on cascading, escalating and common-cause failures, which correspond to the main causes of failures due to interdependencies. We address failures in the electricity infrastructure, in combination with accidental failures in the information infrastructure, then we show briefly how malicious attacks in the information infrastructure can be addressed. 	
0902.4447v2	http://arxiv.org/pdf/0902.4447v2	2009	Percolation Processes and Wireless Network Resilience to   Degree-Dependent and Cascading Node Failures	Zhenning Kong|Edmund M. Yeh	  We study the problem of wireless network resilience to node failures from a percolation-based perspective. In practical wireless networks, it is often the case that the failure probability of a node depends on its degree (number of neighbors). We model this phenomenon as a degree-dependent site percolation process on random geometric graphs. In particular, we obtain analytical conditions for the existence of phase transitions within this model. Furthermore, in networks carrying traffic load, the failure of one node can result in redistribution of the load onto other nearby nodes. If these nodes fail due to excessive load, then this process can result in a cascading failure. Using a simple but descriptive model, we show that the cascading failure problem for large-scale wireless networks is equivalent to a degree-dependent site percolation on random geometric graphs. We obtain analytical conditions for cascades in this model. 	
0908.4290v1	http://arxiv.org/pdf/0908.4290v1	2009	Bridging the Gap between Crisis Response Operations and Systems	Khaled M. Khalil|M. Abdel-Aziz|Taymour T. Nazmy|Abdel-Badeeh M. Salem	  There exist huge problems in the current practice of crisis response operations. Response problems are projected as a combination of failure in communication, failure in technology, failure in methodology, failure of management, and finally failure of observation. In this paper we compare eight crisis response systems namely: DrillSim [2, 13], DEFACTO [12, 17], ALADDIN [1, 6], RoboCup Rescue [11, 15], FireGrid [3, 8, 18], WIPER [16], D-AESOP [4], and PLAN C [14]. Comparison results will disclose the cause of failure of current crisis response operations (the response gap). Based on comparison results; we provide recommendations for bridging this gap between response operations and systems. 	
1008.3369v1	http://arxiv.org/pdf/1008.3369v1	2010	Investigating reciprocity failure in 1.7-micron cut-off HgCdTe detectors	Michael Schubnell|Tomasz Biesiadzinski|Wolfgang Lorenzon|Robert Newman|Greg Tarle	  Flux dependent non-linearity (reciprocity failure) in HgCdTe NIR detectors with 1.7 micron cut-off was investigated. A dedicated test station was designed and built to measure reciprocity failure over the full dynamic range of near infrared detectors. For flux levels between 1 and 100,000 photons/sec a limiting sensitivity to reciprocity failure of 0.3%/decade was achieved. First measurements on several engineering grade 1.7 micron cut-off HgCdTe detectors show a wide range of reciprocity failure, from less than 0.5%/decade to about 10%/decade. For at least two of the tested detectors, significant spatial variation in the effect was observed. No indication for wavelength dependency was found. The origin of reciprocity failure is currently not well understood. In this paper we present details of our experimental set-up and show the results of measurements for several detectors. 	
1012.5961v2	http://arxiv.org/pdf/1012.5961v2	2010	Vulnerability of Networks Against Critical Link Failures	Serdar Çolak|Hilmi Luş|Ali Rana Atılgan	  Networks are known to be prone to link failures. In this paper we set out to investigate how networks of varying connectivity patterns respond to different link failure schemes in terms of connectivity, clustering coefficient and shortest path lengths. We then propose a measure, which we call the vulnerability of a network, for evaluating the extent of the damage these failures can cause. Accepting the disconnections of node pairs as a damage indicator, vulnerability simply represents how quickly the failure of the critical links cause the network to undergo a specified damage extent. Analyzing the vulnerabilities under varying damage specifications shows that scale free networks are relatively more vulnerable for small failures, but more efficient; whereas Erd\"os-R\'enyi networks are the least vulnerable despite lacking any clustered structure. 	
1109.3056v3	http://arxiv.org/pdf/1109.3056v3	2012	Wait-Freedom with Advice	Carole Delporte-Gallet|Hugues Fauconnier|Eli Gafni|Petr Kuznetsov	  We motivate and propose a new way of thinking about failure detectors which allows us to define, quite surprisingly, what it means to solve a distributed task \emph{wait-free} \emph{using a failure detector}. In our model, the system is composed of \emph{computation} processes that obtain inputs and are supposed to output in a finite number of steps and \emph{synchronization} processes that are subject to failures and can query a failure detector. We assume that, under the condition that \emph{correct} synchronization processes take sufficiently many steps, they provide the computation processes with enough \emph{advice} to solve the given task wait-free: every computation process outputs in a finite number of its own steps, regardless of the behavior of other computation processes. Every task can thus be characterized by the \emph{weakest} failure detector that allows for solving it, and we show that every such failure detector captures a form of set agreement. We then obtain a complete classification of tasks, including ones that evaded comprehensible characterization so far, such as renaming or weak symmetry breaking. 	
1212.0311v1	http://arxiv.org/pdf/1212.0311v1	2012	Enhanced Multiple Routing Configurations For Fast IP Network Recovery   From Multiple Failures	T. Anji Kumar|M. H. M. Krishna Prasad	  Now a days, Internet plays a major role in our day to day activities e.g., for online transactions, online shopping, and other network related applications. Internet suffers from slow convergence of routing protocols after a network failure which becomes a growing problem. Multiple Routing Configurations [MRC] recovers network from single node/link failures, but does not support network from multiple node/link failures. In this paper, we propose Enhanced MRC [EMRC], to support multiple node/link failures during data transmission in IP networks without frequent global re-convergence. By recovering these failures, data transmission in network will become fast. 	
1302.4984v1	http://arxiv.org/pdf/1302.4984v1	2013	Modeling Failure Priors and Persistence in Model-Based Diagnosis	Sampath Srinivas	  Probabilistic model-based diagnosis computes the posterior probabilities of failure of components from the prior probabilities of component failure and observations of system behavior. One problem with this method is that such priors are almost never directly available. One of the reasons is that the prior probability estimates include an implicit notion of a time interval over which they are specified -- for example, if the probability of failure of a component is 0.05, is this over the period of a day or is this over a week? A second problem facing probabilistic model-based diagnosis is the modeling of persistence. Say we have an observation about a system at time t_1 and then another observation at a later time t_2. To compute posterior probabilities that take into account both the observations, we need some model of how the state of the system changes from time t_1 to t_2. In this paper, we address these problems using techniques from Reliability theory. We show how to compute the failure prior of a component from an empirical measure of its reliability -- the Mean Time Between Failure (MTBF). We also develop a scheme to model persistence when handling multiple time tagged observations. 	
1306.6818v3	http://arxiv.org/pdf/1306.6818v3	2014	Tetrahedral Elliptic Curves and the local-global principle for Isogenies	Barinder Singh Banwait|John Cremona	  We study the failure of a local-global principle for the existence of $l$-isogenies for elliptic curves over number fields $K$. Sutherland has shown that over $\mathbb{Q}$ there is just one failure, which occurs for $l=7$ and a unique $j$-invariant, and has given a classification of such failures when $K$ does not contain the quadratic subfield of the $l$'th cyclotomic field. In this paper we provide a classification of failures for number fields which do contain this quadratic field, and we find a new `exceptional' source of such failures arising from the exceptional subgroups of $\mbox{PGL}_2(\mathbb{F}_l)$. By constructing models of two modular curves, $X_{\text{s}}(5)$ and $X_{S_4}(13)$, we find two new families of elliptic curves for which the principle fails, and we show that, for quadratic fields, there can be no other exceptional failures. 	
1404.6415v1	http://arxiv.org/pdf/1404.6415v1	2014	The Impact Failure Detector	Anubis G. M. Rossetto|Cláudio F. R. Geyer|Luciana Arantes|Pierre Sens	  This work proposes a new and flexible unreliable failure detector whose output is related to the trust level of a set of processes. By expressing the relevance of each process of the set by an impact factor value, our approach allows the tuning of the detector output, making possible a softer or stricter monitoring. The idea behind our proposal is that, according to an acceptable margin of failures and the impact factor assigned to processes, in some scenarios, the failure of some low impact processes may not change the user confidence in the set of processes, while the crash of a high impact factor process may seriously affect it. We outline the application scenarios and the proposed unreliable failure detector, giving a detailed account of the concept on which it is based. 	
1407.5925v1	http://arxiv.org/pdf/1407.5925v1	2014	Emergence of localized plasticity and failure through shear banding   during microcompression of a nanocrystalline alloy	Amirhossein Khalajhedayati|Timothy J. Rupert	  Microcompression testing is used to probe the uniaxial stress-strain response of a nanocrystalline alloy, with an emphasis on exploring how grain size and grain boundary relaxation state impact the complete flow curve and failure behavior. The yield strength, strain hardening, strain-to-failure, and failure mode of nanocrystalline Ni-W films with mean grain sizes of 5, 15, and 90 nm are studied using taper-free micropillars that are large enough to avoid extrinsic size effects. Strengthening is observed with grain refinement, but catastrophic failure through strain localization is found as well. Shear banding is found to cause failure, resembling the deformation of metallic glasses. Finally, we study the influence of grain boundary state by employing heat treatments that relax nonequilibrium boundary structure but leave grain size unchanged. A pronounced strengthening effect and increased strain localization is observed after relaxation in the finer grained samples. 	
1408.5951v5	http://arxiv.org/pdf/1408.5951v5	2016	Fragility of the Commons under Prospect-Theoretic Risk Attitudes	Ashish R. Hota|Siddharth Garg|Shreyas Sundaram	  We study a common-pool resource game where the resource experiences failure with a probability that grows with the aggregate investment in the resource. To capture decision making under such uncertainty, we model each player's risk preference according to the value function from prospect theory. We show the existence and uniqueness of a pure Nash equilibrium when the players have heterogeneous risk preferences and under certain assumptions on the rate of return and failure probability of the resource. Greater competition, vis-a-vis the number of players, increases the failure probability at the Nash equilibrium; we quantify this effect by obtaining bounds on the ratio of the failure probability at the Nash equilibrium to the failure probability under investment by a single user. We further show that heterogeneity in attitudes towards loss aversion leads to higher failure probability of the resource at the equilibrium. 	
1409.5401v1	http://arxiv.org/pdf/1409.5401v1	2014	Failure Detection and Isolation in Integrator Networks	Mohammad Amin Rahimian|Victor M. Preciado	  Detection and isolation of link failures under the Laplacian consensus dynamics have been the focus of our previous study. Our results relate the failure of links in the network to jump discontinuities in the derivatives of the output responses of the nodes and exploit that relation to propose failure detection and isolation (FDI) techniques, accordingly. In this work, we extend the results to general linear networked dynamics. In particular, we show that with additional niceties of the integrator networks and the enhanced proofs, we are able to incorporate both unidirectional and bidirectional link failures. At the next step, we extend the available FDI techniques to accommodate the cases of bidirectional link failures and undirected topologies. Computer experiments with large networks and both directed and undirected topologies provide interesting insights as to the role of directionality, as well as the scalability of the proposed FDI techniques with the network size. 	
1411.3197v1	http://arxiv.org/pdf/1411.3197v1	2014	Warranty Cost Estimation Using Bayesian Network	Karamjit Singh|Puneet Agarwal|Gautam Shroff	  All multi-component product manufacturing companies face the problem of warranty cost estimation. Failure rate analysis of components plays a key role in this problem. Data source used for failure rate analysis has traditionally been past failure data of components. However, failure rate analysis can be improved by means of fusion of additional information, such as symptoms observed during after-sale service of the product, geographical information (hilly or plains areas), and information from tele-diagnostic analytics. In this paper, we propose an approach, which learns dependency between part-failures and symptoms gleaned from such diverse sources of information, to predict expected number of failures with better accuracy. We also indicate how the optimum warranty period can be computed. We demonstrate, through empirical results, that our method can improve the warranty cost estimates significantly. 	
1412.3687v1	http://arxiv.org/pdf/1412.3687v1	2014	Modelling common cause failures of large digital I&C systems with   coloured Petri nets	Gilles Deleuze|Nicolae Brinzei|Nicolas Villaume	  The purpose of this study is the representation of Common Cause Failures (CCF) in large digital systems. The system under study is representative of a control system of a nuclear plant. The model for CCF is the generalized Atwood model. It can represent independent failures, CCF non-lethal for some system elements and CCF lethal to all. The Atwood model was modified to "direct" non-lethal DCC on certain parts of the system and take into account the different possible origins of DCC. Maintenance and repairs are taken into account in the model that is thus dynamic. The main evaluation results are probabilistic, the considered indicator is the probability of failure on demand (PFD). A comparison is made between the estimator of the PFD taking into account all the failures and the estimator taking into account only the detected failures. 	
1501.04227v2	http://arxiv.org/pdf/1501.04227v2	2016	Tunable failure: control of rupture through rigidity	Michelle M. Driscoll|Bryan Gin-ge Chen|Thomas H. Beuman|Stephan Ulrich|Sidney R. Nagel|Vincenzo Vitelli	  We investigate how material rigidity acts as a key control parameter for the failure of solids under stress. In both experiments and simulations, we demonstrate that material failure can be continuously tuned by varying the underlying rigidity of the material while holding the amount of disorder constant. As the rigidity transition is approached, failure due to the application of uniaxial stress evolves from brittle cracking to system-spanning diffuse breaking. This evolution in failure behavior can be parameterized by the width of the crack. As a system becomes more and more floppy, this crack width increases until it saturates at the system size. Thus, the spatial extent of the failure zone can be used as a direct probe for material rigidity. 	
1505.03469v1	http://arxiv.org/pdf/1505.03469v1	2015	The Weakest Failure Detector for Eventual Consistency	Swan Dubois|Rachid Guerraoui|Petr Kuznetsov|Franck Petit|Pierre Sens	  In its classical form, a consistent replicated service requires all replicas to witness the same evolution of the service state. Assuming a message-passing environment with a majority of correct processes, the necessary and sufficient information about failures for implementing a general state machine replication scheme ensuring consistency is captured by the {\Omega} failure detector. This paper shows that in such a message-passing environment, {\Omega} is also the weakest failure detector to implement an eventually consistent replicated service, where replicas are expected to agree on the evolution of the service state only after some (a priori unknown) time. In fact, we show that {\Omega} is the weakest to implement eventual consistency in any message-passing environment, i.e., under any assumption on when and where failures might occur. Ensuring (strong) consistency in any environment requires, in addition to {\Omega}, the quorum failure detector {\Sigma}. Our paper thus captures, for the first time, an exact computational difference be- tween building a replicated state machine that ensures consistency and one that only ensures eventual consistency. 	
1508.01122v1	http://arxiv.org/pdf/1508.01122v1	2015	On Bivariate Generalized Linear Failure Rate-Power Series Class of   Distributions	Rasool Roozegar|Ali Akbar Jafari	  Recently it has been observed that the bivariate generalized linear failure rate distribution can be used quite effectively to analyze lifetime data in two dimensions. This paper introduces a more general class of bivariate distributions. We refer to this new class of distributions as bivariate generalized linear failure rate power series model. This new class of bivariate distributions contains several lifetime models such as: generalized linear failure rate-power series, bivariate generalized linear failure rate and bivariate generalized linear failure rate geometric distributions as special cases among others. The construction and characteristics of the proposed bivariate distribution are presented along with estimation procedures for the model parameters based on maximum likelihood. The marginal and conditional laws are also studied. We present an application to the real data set where our model provides a better fit than other models. 	
1510.08380v1	http://arxiv.org/pdf/1510.08380v1	2015	Towards a Realistic Model for Failure Propagation in Interdependent   Networks	Agostino Sturaro|Simone Silvestri|Mauro Conti|Sajal K. Das	  Modern networks are becoming increasingly interdependent. As a prominent example, the smart grid is an electrical grid controlled through a communications network, which in turn is powered by the electrical grid. Such interdependencies create new vulnerabilities and make these networks more susceptible to failures. In particular, failures can easily spread across these networks due to their interdependencies, possibly causing cascade effects with a devastating impact on their functionalities.   In this paper we focus on the interdependence between the power grid and the communications network, and propose a novel realistic model, HINT (Heterogeneous Interdependent NeTworks), to study the evolution of cascading failures. Our model takes into account the heterogeneity of such networks as well as their complex interdependencies. We compare HINT with previously proposed models both on synthetic and real network topologies. Experimental results show that existing models oversimplify the failure evolution and network functionality requirements, resulting in severe underestimations of the cascading failures. 	
1604.03677v1	http://arxiv.org/pdf/1604.03677v1	2016	Robustness of Power-law Behavior in Cascading Failure Models	F. Sloothaak|S. C. Borst|A. P. Zwart	  Inspired by reliability issues in electric transmission networks, we use a probabilistic approach to study the occurrence of large failures in a stylized cascading failure model. In this model, lines have random capacities that initially meet the load demands imposed on the network. Every single line failure changes the load distribution in the surviving network, possibly causing further lines to become overloaded and trip as well. An initial single line failure can therefore potentially trigger massive cascading effects, and in this paper we measure the risk of such cascading events by the probability that the number of failed lines exceeds a certain large threshold. Under particular critical conditions, the exceedance probability follows a power-law distribution, implying a significant risk of severe failures. We examine the robustness of the power-law behavior by exploring under which assumptions this behavior prevails. 	
1606.00892v1	http://arxiv.org/pdf/1606.00892v1	2016	Developing a Methodology for Online Service Failure Prevention:   Reporting on an Action Design Research Project-in-Progress	Jacques Louis Du Preez|Mary Tate|Alireza Nili	  The increasing use of online channels for service delivery raises new challenges in service failure prevention. This work-in-progress paper reports on the first phase of an action-design research project to develop a service failure prevention methodology. In this paper we review the literature on online services, failure prevention and failure recovery and develop a theoretical framework for online service failure prevention. This provides the theoretical grounding for the artefact (the methodology) to be developed. We use this framework to develop an initial draft of our methodology. We then outline the remaining phases of the research, and offer some initial conclusions gained from the project to date. 	
1801.04523v1	http://arxiv.org/pdf/1801.04523v1	2018	Shrink or Substitute: Handling Process Failures in HPC Systems using   In-situ Recovery	Rizwan A. Ashraf|Saurabh Hukerikar|Christian Engelmann	  Efficient utilization of today's high-performance computing (HPC) systems with complex hardware and software components requires that the HPC applications are designed to tolerate process failures at runtime. With low mean time to failure (MTTF) of current and future HPC systems, long running simulations on these systems require capabilities for gracefully handling process failures by the applications themselves. In this paper, we explore the use of fault tolerance extensions to Message Passing Interface (MPI) called user-level failure mitigation (ULFM) for handling process failures without the need to discard the progress made by the application. We explore two alternative recovery strategies, which use ULFM along with application-driven in-memory checkpointing. In the first case, the application is recovered with only the surviving processes, and in the second case, spares are used to replace the failed processes, such that the original configuration of the application is restored. Our experimental results demonstrate that graceful degradation is a viable alternative for recovery in environments where spares may not be available. 	
9612107v1	http://arxiv.org/pdf/astro-ph/9612107v1	1996	An Investigation of Neutrino-Driven Convection and the Core Collapse   Supernova Mechanism Using Multigroup Neutrino Transport	A. Mezzacappa|A. C. Calder|S. W. Bruenn|J. M. Blondin|M. W. Guidry|M. R. Strayer|A. S. Umar	  We investigate neutrino-driven convection in core collapse supernovae and its ramifications for the explosion mechanism. We begin with an ``optimistic'' 15 solar mass precollapse model, which is representative of the class of stars with compact iron cores. This model is evolved through core collapse and bounce in one dimension using multigroup (neutrino-energy--dependent) flux-limited diffusion (MGFLD) neutrino transport and Lagrangian hydrodynamics, providing realistic initial conditions for the postbounce convection and evolution. Our two-dimensional simulation begins at 106 ms after bounce at a time when there is a well-developed gain region, and proceeds for 400 ms. We couple two-dimensional (PPM) hydrodynamics to one-dimensional MGFLD neutrino transport. At 225 ms after bounce we see large-scale convection behind the shock, characterized by high-entropy, mushroom-like, expanding upflows and dense, low-entropy, finger-like downflows. The upflows reach the shock and distort it from sphericity. The radial convection velocities become supersonic just below the shock, reaching magnitudes in excess of 10^9 cm/sec. Eventually, however, the shock recedes to smaller radii, and at about 500 ms after bounce there is no evidence in our simulation of an explosion or of a developing explosion. Failure in our ``optimistic'' 15 solar mass Newtonian model leads us to conclude that it is unlikely, at least in our approximation, that neutrino-driven convection will lead to explosions for more massive stars with fatter iron cores or in cases in which general relativity is included. 	
0005366v5	http://arxiv.org/pdf/astro-ph/0005366v5	2000	Simulation of the Spherically Symmetric Stellar Core Collapse, Bounce,   and Postbounce Evolution of a 13 Solar Mass Star with Boltzmann Neutrino   Transport, and Its Implications for the Supernova Mechanism	Anthony Mezzacappa|Matthias Liebendoerfer|O. E. Bronson Messer|W. Raphael Hix|Friederich-Karl Thielemann|Stephen W. Bruenn	  With exact three-flavor Boltzmann neutrino transport, we simulate the stellar core collapse, bounce, and postbounce evolution of a 13 solar mass star in spherical symmetry, the Newtonian limit, without invoking convection. In the absence of convection, prior spherically symmetric models, which implemented approximations to Boltzmann transport, failed to produce explosions. We are motivated to consider exact transport to determine if these failures were due to the transport approximations made and to answer remaining fundamental questions in supernova theory. The model presented here is the first in a sequence of models beginning with different progenitors. In this model, a supernova explosion is not obtained. We discuss the ramifications of our results for the supernova mechanism. 	
9812232v1	http://arxiv.org/pdf/cond-mat/9812232v1	1998	Coupling of Length Scales and Atomistic Simulation of MEMS Resonators	Robert E. Rudd|Jeremy Q. Broughton	  We present simulations of the dynamic and temperature dependent behavior of Micro-Electro-Mechanical Systems (MEMS) by utilizing recently developed parallel codes which enable a coupling of length scales. The novel techniques used in this simulation accurately model the behavior of the mechanical components of MEMS down to the atomic scale. We study the vibrational behavior of one class of MEMS devices: micron-scale resonators made of silicon and quartz. The algorithmic and computational avenue applied here represents a significant departure from the usual finite element approach based on continuum elastic theory. The approach is to use an atomistic simulation in regions of significantly anharmonic forces and large surface area to volume ratios or where internal friction due to defects is anticipated. Peripheral regions of MEMS which are well-described by continuum elastic theory are simulated using finite elements for efficiency. Thus, in central regions of the device, the motion of millions of individual atoms is simulated, while the relatively large peripheral regions are modeled with finite elements. The two techniques run concurrently and mesh seamlessly, passing information back and forth. This coupling of length scales gives a natural domain decomposition, so that the code runs on multiprocessor workstations and supercomputers. We present novel simulations of the vibrational behavior of micron-scale silicon and quartz oscillators. Our results are contrasted with the predictions of continuum elastic theory as a function of size, and the failure of the continuum techniques is clear in the limit of small sizes. We also extract the Q value for the resonators and study the corresponding dissipative processes. 	
0610080v2	http://arxiv.org/pdf/cond-mat/0610080v2	2007	Computer simulation of fatigue under diametrical compression	H. A. Carmona|F. Kun|J. S. Andrade Jr.|H. J. Herrmann	  We study the fatigue fracture of disordered materials by means of computer simulations of a discrete element model. We extend a two-dimensional fracture model to capture the microscopic mechanisms relevant for fatigue, and we simulate the diametric compression of a disc shape specimen under a constant external force. The model allows to follow the development of the fracture process on the macro- and micro-level varying the relative influence of the mechanisms of damage accumulation over the load history and healing of microcracks. As a specific example we consider recent experimental results on the fatigue fracture of asphalt. Our numerical simulations show that for intermediate applied loads the lifetime of the specimen presents a power law behavior. Under the effect of healing, more prominent for small loads compared to the tensile strength of the material, the lifetime of the sample increases and a fatigue limit emerges below which no macroscopic failure occurs. The numerical results are in a good qualitative agreement with the experimental findings. 	
0703069v1	http://arxiv.org/pdf/cond-mat/0703069v1	2007	A physics-based life prediction methodology for thermal barrier coating   systems	Esteban Busso|L. Wright|H. E. Evans|L. N. McCartney|S. R. J Saunders|S. Osgerby|J. Nunn	  A novel mechanistic approach is proposed for the prediction of the life of thermal barrier coating (TBC) systems. The life prediction methodology is based on a criterion linked directly to the dominant failure mechanism. It relies on a statistical treatment of the TBC's morphological characteristics, non-destructive stress measurements and on a continuum mechanics framework to quantify the stresses that promote the nucleation and growth of microcracks within the TBC. The last of these accounts for the effects of TBC constituents' elasto-visco-plastic properties, the stiffening of the ceramic due to sintering and the oxidation at the interface between the thermally insulating yttria stabilized zirconia (YSZ) layer and the metallic bond coat. The mechanistic approach is used to investigate the effects on TBC life of the properties and morphology of the top YSZ coating, metallic low-pressure plasma sprayed bond coat and the thermally grown oxide. Its calibration is based on TBC damage inferred from non-destructive fluorescence measurements using piezo-spectroscopy and on the numerically predicted local TBC stresses responsible for the initiation of such damage. The potential applicability of the methodology to other types of TBC coatings and thermal loading conditions is also discussed. 	
0404014v1	http://arxiv.org/pdf/cs/0404014v1	2004	A Modular and Fault-Tolerant Data Transport Framework	Timm M. Steinbeck	  The High Level Trigger (HLT) of the future ALICE heavy-ion experiment has to reduce its input data rate of up to 25 GB/s to at most 1.25 GB/s for output before the data is written to permanent storage. To cope with these data rates a large PC cluster system is being designed to scale to several 1000 nodes, connected by a fast network. For the software that will run on these nodes a flexible data transport and distribution software framework, described in this thesis, has been developed. The framework consists of a set of separate components, that can be connected via a common interface. This allows to construct different configurations for the HLT, that are even changeable at runtime. To ensure a fault-tolerant operation of the HLT, the framework includes a basic fail-over mechanism that allows to replace whole nodes after a failure. The mechanism will be further expanded in the future, utilizing the runtime reconnection feature of the framework's component interface. To connect cluster nodes a communication class library is used that abstracts from the actual network technology and protocol used to retain flexibility in the hardware choice. It contains already two working prototype versions for the TCP protocol as well as SCI network adapters. Extensions can be added to the library without modifications to other parts of the framework. Extensive tests and measurements have been performed with the framework. Their results as well as conclusions drawn from them are also presented in this thesis. Performance tests show very promising results for the system, indicating that it can fulfill ALICE's requirements concerning the data transport. 	
0405159v2	http://arxiv.org/pdf/hep-th/0405159v2	2004	Supersymmetric Unification Without Low Energy Supersymmetry And   Signatures for Fine-Tuning at the LHC	Nima Arkani-Hamed|Savas Dimopoulos	  The cosmological constant problem is a failure of naturalness and suggests that a fine-tuning mechanism is at work, which may also address the hierarchy problem. An example -- supported by Weinberg's successful prediction of the cosmological constant -- is the potentially vast landscape of vacua in string theory, where the existence of galaxies and atoms is promoted to a vacuum selection criterion. Then, low energy SUSY becomes unnecessary, and supersymmetry -- if present in the fundamental theory -- can be broken near the unification scale. All the scalars of the supersymmetric standard model become ultraheavy, except for a single finely tuned Higgs. Yet, the fermions of the supersymmetric standard model can remain light, protected by chiral symmetry, and account for the successful unification of gauge couplings. This framework removes all the difficulties of the SSM: the absence of a light Higgs and sparticles, dimension five proton decay, SUSY flavor and CP problems, and the cosmological gravitino and moduli problems. High-scale SUSY breaking raises the mass of the light Higgs to about 120-150 GeV. The gluino is strikingly long lived, and a measurement of its lifetime can determine the ultraheavy scalar mass scale. Measuring the four Yukawa couplings of the Higgs to the gauginos and higgsinos precisely tests for high-scale SUSY. These ideas, if confirmed, will demonstrate that supersymmetry is present but irrelevant for the hierarchy problem -- just as it has been irrelevant for the cosmological constant problem -- strongly suggesting the existence of a fine-tuning mechanism in nature. 	
0710.5236v1	http://arxiv.org/pdf/0710.5236v1	2007	Saturation Throughput Analysis of IEEE 802.11 in Presence of Non Ideal   Transmission Channel and Capture Effects	F. Daneshgaran|Massimiliano Laddomada|F. Mesiti|M. Mondin	  In this paper, we provide a saturation throughput analysis of the IEEE 802.11 protocol at the data link layer by including the impact of both transmission channel and capture effects in Rayleigh fading environment. Impacts of both non-ideal channel and capture effects, specially in an environment of high interference, become important in terms of the actual observed throughput. As far as the 4-way handshaking mechanism is concerned, we extend the multi-dimensional Markovian state transition model characterizing the behavior at the MAC layer by including transmission states that account for packet transmission failures due to errors caused by propagation through the channel. This way, any channel model characterizing the physical transmission medium can be accommodated, including AWGN and fading channels. We also extend the Markov model in order to consider the behavior of the contention window when employing the basic 2-way handshaking mechanism.   Under the usual assumptions regarding the traffic generated per node and independence of packet collisions, we solve for the stationary probabilities of the Markov chain and develop expressions for the saturation throughput as a function of the number of terminals, packet sizes, raw channel error rates, capture probability, and other key system parameters. The theoretical derivations are then compared to simulation results confirming the effectiveness of the proposed models. 	
0802.1366v1	http://arxiv.org/pdf/0802.1366v1	2008	Rise and fall of the old quantum theory	Manfred Bucher	  The old quantum theory of Bohr and Sommerfeld was abandonned for the wrong reason. Its contradictions were caused not by the orbit concept but by a mental barrier--the inconceivability that an electron might collide with the atomic nucleus. Removing that barrier resolves the theory's main failures--incorrect orbital momenta, He atom, H2+ molecule ion. The inclusion of electron oscillations through the nucleus--a concept called "Coulomb oscillator"--renders the old quantum theory consistent with quantum mechanics (although devoid of wave character). The triple success of the Bohr-Sommerfeld model is its correct description of the H atom (and one-electron ions) concerning (1) the energy levels Enl, (2) the orbital angular momenta Lnl--if corrected as Lnl^2 = l(l+1) hbar^2 and with the Coulomb oscillator included--and (3) the orbits' space quantization--with (Lnl)z = ml hbar. These achievements are succinctly represented by the principal, angular and magnetic quantum numbers (n, l, ml) and visualized by orbital ellipse geometry--major axis, vertex curvature, and tilt angle, respectively. Orbit geometry also accounts for the average orbit size. Moreover, the Coulomb oscillator provides a natural explanation of (isotropic) hyperfine interaction. The shortcomings of the old quantum theory lie in its neglect of three properties of particles--their spin, their wave nature and their quantum statistics. These deficiencies notwithstanding, the visual appeal of the Bohr-Sommerfeld model remains a pedagogical asset to complement the abstract character of quantum mechanics. 	
0805.3292v2	http://arxiv.org/pdf/0805.3292v2	2009	Spontaneous dissipation of elastic energy by self-localizing thermal   runaway	S. Braeck|Y. Y. Podladchikov|S. Medvedev	  Thermal runaway instability induced by material softening due to shear heating represents a potential mechanism for mechanical failure of viscoelastic solids. In this work we present a model based on a continuum formulation of a viscoelastic material with Arrhenius dependence of viscosity on temperature, and investigate the behavior of the thermal runaway phenomenon by analytical and numerical methods. Approximate analytical descriptions of the problem reveal that onset of thermal runaway instability is controlled by only two dimensionless combinations of physical parameters. Numerical simulations of the model independently verify these analytical results and allow a quantitative examination of the complete time evolutions of the shear stress and the spatial distributions of temperature and displacement during runaway instability. Thus we find that thermal runaway processes may well develop under nonadiabatic conditions. Moreover, nonadiabaticity of the unstable runaway mode leads to continuous and extreme localization of the strain and temperature profiles in space, demonstrating that the thermal runaway process can cause shear banding. Examples of time evolutions of the spatial distribution of the shear displacement between the interior of the shear band and the essentially nondeforming material outside are presented. Finally, a simple relation between evolution of shear stress, displacement, shear-band width and temperature rise during runaway instability is given. 	
0806.0593v1	http://arxiv.org/pdf/0806.0593v1	2008	Laws of crack motion and phase-field models of fracture	Vincent Hakim|Alain Karma	  Recently proposed phase-field models offer self-consistent descriptions of brittle fracture. Here, we analyze these theories in the quasistatic regime of crack propagation. We show how to derive the laws of crack motion either by using solvability conditions in a perturbative treatment for slight departure from the Griffith threshold, or by generalizing the Eshelby tensor to phase-field models. The analysis provides a simple physical interpretation of the second component of the classic Eshelby integral in the limit of vanishing crack propagation velocity: it gives the elastic torque on the crack tip that is needed to balance the Herring torque arising from the anisotropic interface energy. This force balance condition reduces in this limit to the principle of local symmetry in isotropic media and to the principle of maximum energy release rate for smooth curvilinear cracks in anisotropic media. It can also be interpreted physically in this limit based on energetic considerations in the traditional framework of continuum fracture mechanics, in support of its general validity for real systems beyond the scope of phase-field models. Analytical predictions of crack paths in anisotropic media are validated by numerical simulations. Simulations also show that these predictions hold even if the phase-field dynamics is modified to make the failure process irreversible. In addition, the role of dissipative forces on the process zone scale as well as the extension of the results to motion of planar cracks under pure antiplane shear are discussed. 	
0904.4426v1	http://arxiv.org/pdf/0904.4426v1	2009	Word-of-mouth and dynamical inhomogeneous markets: Efficiency measure   and optimal sampling policies for the pre-launch stage	Elena Agliari|Raffaella Burioni|Davide Cassi|Franco Maria Neri	  An important assumption lying behind innovation diffusion models and word-of-mouth processes is that of homogeneous mixing: at any time, the individuals making up the market are uniformly distributed in space. When the geographical parameters of the market, such as its area extension, become important, the movement of individuals must be explicitly taken into account. The authors introduce a model for a "micro-level" process for the diffusion of an innovative product, based on a word-of-mouth mechanism, and they explicitly consider the inhomogeneity of markets and the spatial extent of the geographical region where the process takes place. This results in an unexpected behaviour of macro (aggregate) level measurable quantities. The authors study the particular case of the pre-launch stage, where a product is first presented to the market through free sample distribution. The first triers of the samples then inform the other potential customers via word-of-mouth; additional advertising is absent. The authors find an unexpected general failure of the word-of-mouth mechanism for high market densities and they obtain quantitative results for the optimal sampling policy. By introducing a threshold to discriminate between individuals who will purchase and those who will not purchase according to their individual goodwill, they calculate the length of the pre-launch campaign and the final goodwill as a function of the firm's expenditure. These results are applied to a set of major US urban areas. 	
1004.0542v3	http://arxiv.org/pdf/1004.0542v3	2011	Cognitive Interference Management in Retransmission-Based Wireless   Networks	Marco Levorato|Urbashi Mitra|Michele Zorzi	  Cognitive radio methodologies have the potential to dramatically increase the throughput of wireless systems. Herein, control strategies which enable the superposition in time and frequency of primary and secondary user transmissions are explored in contrast to more traditional sensing approaches which only allow the secondary user to transmit when the primary user is idle. In this work, the optimal transmission policy for the secondary user when the primary user adopts a retransmission based error control scheme is investigated. The policy aims to maximize the secondary users' throughput, with a constraint on the throughput loss and failure probability of the primary user. Due to the constraint, the optimal policy is randomized, and determines how often the secondary user transmits according to the retransmission state of the packet being served by the primary user. The resulting optimal strategy of the secondary user is proven to have a unique structure. In particular, the optimal throughput is achieved by the secondary user by concentrating its transmission, and thus its interference to the primary user, in the first transmissions of a primary user packet. The rather simple framework considered in this paper highlights two fundamental aspects of cognitive networks that have not been covered so far: (i) the networking mechanisms implemented by the primary users (error control by means of retransmissions in the considered model) react to secondary users' activity; (ii) if networking mechanisms are considered, then their state must be taken into account when optimizing secondary users' strategy, i.e., a strategy based on a binary active/idle perception of the primary users' state is suboptimal. 	
1006.4741v1	http://arxiv.org/pdf/1006.4741v1	2010	Compression Behavior of Single-layer Graphene	Otakar Frank|Georgia Tsoukleri|John Parthenios|Konstantinos Papagelis|Ibtsam Riaz|Rashid Jalil|Kostya S. Novoselov|Costas Galiotis	  Central to most applications involving monolayer graphene is its mechanical response under various stress states. To date most of the work reported is of theoretical nature and refers to tension and compression loading of model graphene. Most of the experimental work is indeed limited to bending of single flakes in air and the stretching of flakes up to typically ~1% using plastic substrates. Recently we have shown that by employing a cantilever beam we can subject single graphene into various degrees of axial compression. Here we extend this work much further by measuring in detail both stress uptake and compression buckling strain in single flakes of different geometries. In all cases the mechanical response is monitored by simultaneous Raman measurements through the shift of either the G or 2D phonons of graphene. In spite of the infinitely small thickness of the monolayers, the results show that graphene embedded in plastic beams exhibit remarkable compression buckling strains. For large length (l)-to-width (w) ratios (> 0.2) the buckling strain is of the order of -0.5% to -0.6%. However, for l/w <0.2 no failure is observed for strains even higher than -1%. Calculations based on classical Euler analysis show that the buckling strain enhancement provided by the polymer lateral support is more than six orders of magnitude compared to suspended graphene in air. 	
1011.1400v1	http://arxiv.org/pdf/1011.1400v1	2010	Electrical transport through a mechanically gated molecular wire	C. Toher|R. Temirov|A. Greuling|F. Pump|M. Kaczmarski|M. Rohlfing|G. Cuniberti|F. S. Tautz	  A surface-adsorbed molecule is contacted with the tip of a scanning tunneling microscope (STM) at a pre-defined atom. On tip retraction, the molecule is peeled off the surface. During this experiment, a two-dimensional differential conductance map is measured on the plane spanned by the bias voltage and the tip-surface distance. The conductance map demonstrates that tip retraction leads to mechanical gating of the molecular wire in the STM junction. The experiments are compared with a detailed ab initio simulation. We find that density functional theory (DFT) in the local density approximation (LDA) describes the tip-molecule contact formation and the geometry of the molecular junction throughout the peeling process with predictive power. However, a DFT-LDA-based transport simulation following the non-equilibrium Green's functions (NEGF) formalism fails to describe the behavior of the differential conductance as found in experiment. Further analysis reveals that this failure is due to the mean-field description of electron correlation in the local density approximation. The results presented here are expected to be of general validity and show that, for a wide range of common wire configurations, simulations which go beyond the mean-field level are required to accurately describe current conduction through molecules. Finally, the results of the present study illustrate that well-controlled experiments and concurrent ab initio transport simulations that systematically sample a large configuration space of molecule-electrode couplings allow the unambiguous identification of correlation signatures in experiment. 	
1011.2578v2	http://arxiv.org/pdf/1011.2578v2	2011	Theoretical perspective on the glass transition and amorphous materials	Ludovic Berthier|Giulio Biroli	  We provide a theoretical perspective on the glass transition in molecular liquids at thermal equilibrium, on the spatially heterogeneous and aging dynamics of disordered materials, and on the rheology of soft glassy materials. We start with a broad introduction to the field and emphasize its connections with other subjects and its relevance. The important role played by computer simulations to study and understand the dynamics of systems close to the glass transition at the molecular level is spelled out. We review the recent progress on the subject of the spatially heterogeneous dynamics that characterizes structural relaxation in materials with slow dynamics. We then present the main theoretical approaches describing the glass transition in supercooled liquids, focusing on theories that have a microscopic, statistical mechanics basis. We describe both successes and failures, and critically assess the current status of each of these approaches. The physics of aging dynamics in disordered materials and the rheology of soft glassy materials are then discussed, and recent theoretical progress is described. For each section, we give an extensive overview of the most recent advances, but we also describe in some detail the important open problems that, we believe, will occupy a central place in this field in the coming years. 	
1112.5593v1	http://arxiv.org/pdf/1112.5593v1	2011	Extending and Evaluating and Novel Course Reform of introductory   Mechanics	Marcos D. Caballero	  The research presented in this thesis was motivated by the need to improve introductory physics courses. Introductory physics courses are generally the first courses in which students learn to create models to solve complex problems. However, many students taking introductory physics courses fail to acquire a command of the concepts, methods and tools presented in these courses. The reforms proposed by this thesis focus on altering the content of introductory courses rather than content delivery methods as most reforms do.   This thesis explores how the performance on a widely used test of conceptual understanding in mechanics compares between students taking a course with updated and modified content and students taking a traditional course. Better performance by traditional students was found to stem from their additional practice on the types of items which appeared on the test. The results of this work brought into question the role of the introductory physics course for non-majors.   One aspect of this new role is the teaching of new methods such as computation (the use of a computer to solve numerically, simulate and visualize physical problems). This thesis explores the potential benefits for students who learn computation as part of physics course. After students worked through a suite of computational homework problems, many were able to model a new physical situation with which they had no experience.   The failure of some students to model this new situation might have stemmed from their unfavorable attitudes towards learning computation. In this thesis, we present the development of a new tool for characterizing students' attitudes. Preliminary measurements indicated significant differences between successful and unsuccessful students. 	
1206.6808v1	http://arxiv.org/pdf/1206.6808v1	2012	A Multi-State Power Model for Adequacy Assessment of Distributed   Generation via Universal Generating Function	Yan-Fu Li|Enrico Zio	  The current and future developments of electric power systems are pushing the boundaries of reliability assessment to consider distribution networks with renewable generators. Given the stochastic features of these elements, most modeling approaches rely on Monte Carlo simulation. The computational costs associated to the simulation approach force to treating mostly small-sized systems, i.e. with a limited number of lumped components of a given renewable technology (e.g. wind or solar, etc.) whose behavior is described by a binary state, working or failed. In this paper, we propose an analytical multi-state modeling approach for the reliability assessment of distributed generation (DG). The approach allows looking to a number of diverse energy generation technologies distributed on the system. Multiple states are used to describe the randomness in the generation units, due to the stochastic nature of the generation sources and of the mechanical degradation/failure behavior of the generation systems. The universal generating function (UGF) technique is used for the individual component multi-state modeling. A multiplication-type composition operator is introduced to combine the UGFs for the mechanical degradation and renewable generation source states into the UGF of the renewable generator power output. The overall multi-state DG system UGF is then constructed and classical reliability indices (e.g. loss of load expectation (LOLE), expected energy not supplied (EENS)) are computed from the DG system generation and load UGFs. An application of the model is shown on a DG system adapted from the IEEE 34 nodes distribution test feeder. 	
1309.7429v1	http://arxiv.org/pdf/1309.7429v1	2013	Quorum Sensing for Regenerating Codes in Distributed Storage	Mit Sheth|Krishna Gopal Benerjee|Manish K. Gupta	  Distributed storage systems with replication are well known for storing large amount of data. A large number of replication is done in order to provide reliability. This makes the system expensive. Various methods have been proposed over time to reduce the degree of replication and yet provide same level of reliability. One recently suggested scheme is of Regenerating codes, where a file is divided in to parts which are then processed by a coding mechanism and network coding to provide large number of parts. These are stored at various nodes with more than one part at each node. These codes can generate whole file and can repair a failed node by contacting some out of total existing nodes. This property ensures reliability in case of node failure and uses clever replication. This also optimizes bandwidth usage. In a practical scenario, the original file will be read and updated many times. With every update, we will have to update the data stored at many nodes. Handling multiple requests at the same time will bring a lot of complexity. Reading and writing or multiple writing on the same data at the same time should also be prevented. In this paper, we propose an algorithm that manages and executes all the requests from the users which reduces the update complexity. We also try to keep an adequate amount of availability at the same time. We use a voting based mechanism and form read, write and repair quorums. We have also done probabilistic analysis of regenerating codes. 	
1402.4180v1	http://arxiv.org/pdf/1402.4180v1	2014	Effect of Deck Deterioration on Overall System Behavior, Resilience and   Remaining Life of Composite Steel Girder Bridges	Amir Gheitasi|Devin K. Harris	  During past few decades, several studies have been conducted to characterize the performance of in-service girder-type bridge superstructures under operating conditions. Few of these efforts have focused on evaluating the actual response of the bridge systems, especially beyond the elastic limit of their behavior, and correlating the impact of damage to the overall system behavior. In practice, most of the in-service bridge superstructures behave elastically under the routine daily traffic; however, existing damage and deteriorating conditions would significantly influence different aspects of the structural performance including reserve capacity, resilience and remaining service-life. The main purpose of this study is to evaluate the response of composite steel girder bridges under the effect of subsurface delamination in the reinforced concrete deck. Commercial finite element computer software, ANSYS, was implemented to perform a nonlinear analysis on a representative single-span simply supported bridge superstructure. The system failure characteristics were captured in the numerical models by incorporating different sources of material non-linearities including cracking/crushing in the concrete and plasticity in steel components. Upon validation, non-linear behavior of the system with both intact and degraded configurations was used to evaluate the impact of integrated damage mechanism on the overall system performance. Reserve capacity of this bridge superstructure was also determined with respect to the nominal element-level design capacity. As vision to the future path, this framework can be implemented to evaluate the performance of other in-service bridges degraded under the effect of different damage scenarios, thus providing a mechanism to determine a measure of capacity, resilience and remaining service-life. 	
1406.7547v1	http://arxiv.org/pdf/1406.7547v1	2014	Influence Process Structural Learning and the Emergence of Collective   Intelligence	James Hazy|Baran Curuklu	  Recent work [Hazy 2012] has demonstrated computationally that collectives that are organized into networks which govern the flow of resources can learn to recognize newly emerging opportunities distributed in the environment. This paper argues that the system does this through a process analogous to neural network learning with relative status playing the role of synaptic weights. Hazy showed computationally that learning of this type can occur even when resource allocation decision makers have no direct visibility into the environment, have no direct understanding of the opportunity, and are not involved in their exploitation except to the extent that they evaluate the success or failure of funded projects. Effectively, the system of interactions learns which individuals have the best access to information and other resources within the ecosystem. Hazy [2012] calls this previously unidentified emergence phenomenon: Influence Process Structural Learning (IPSL). In the prior model of IPSL, a three-tiered organizational structure was predetermined in the model design [Hazy 2012]. These initial conditions delimit the extent to which the emergence of collective intelligence can be posited because the model itself assumes a defined structure. This work contributes to the field by extending the IPSL argument for collective intelligence to a holistic emergence argument. It begins by briefly reviewing previously published work. It continues the conversation by adding two additional steps: Firstly, it shows how a three-tier organizing structure might emerge through known complexity mechanisms. In this case the mechanism identified is preferential attachment [Barabasi 2002]. Secondly, the paper shows how collective intelligence can emerge within a system of agents when the influence structure among these agents is treated as a the genetic algorithm. 	
1409.1998v1	http://arxiv.org/pdf/1409.1998v1	2014	Surface tension and the mechanics of liquid inclusions in compliant   solids	Robert W. Style|John S. Wettlaufer|Eric R. Dufresne	  Eshelby's theory of inclusions has wide-reaching implications across the mechanics of materials and structures including the theories of composites, fracture, and plasticity. However, it does not include the effects of surface stress, which has recently been shown to control many processes in soft materials such as gels, elastomers and biological tissue. To extend Eshelby's theory of inclusions to soft materials, we consider liquid inclusions within an isotropic, compressible, linear-elastic solid. We solve for the displacement and stress fields around individual stretched inclusions, accounting for the bulk elasticity of the solid and the surface tension (\textit{i.e.} isotropic strain-independent surface stress) of the solid-liquid interface. Surface tension significantly alters the inclusion's shape and stiffness as well as its near- and far-field stress fields. These phenomenon depend strongly on the ratio of inclusion radius, $R$, to an elastocapillary length, $L$. Surface tension is significant whenever inclusions are smaller than $100L$. While Eshelby theory predicts that liquid inclusions generically reduce the stiffness of an elastic solid, our results show that liquid inclusions can actually stiffen a solid when $R<3L/2$. Intriguingly, surface tension cloaks the far-field signature of liquid inclusions when $R=3L/2$. These results are have far-reaching applications from measuring local stresses in biological tissue, to determining the failure strength of soft composites. 	
1412.0780v1	http://arxiv.org/pdf/1412.0780v1	2014	Emergence of Anti-Cancer Drug Resistance: Exploring the Importance of   the Microenvironmental Niche via a Spatial Model	Jana L. Gevertz|Zahra Aminzare|Kerri-Ann Norton|Judith Perez-Velazquez|Alexandria Volkening|Katarzyna A. Rejniak	  Practically, all chemotherapeutic agents lead to drug resistance. Clinically, it is a challenge to determine whether resistance arises prior to, or as a result of, cancer therapy. Further, a number of different intracellular and microenvironmental factors have been correlated with the emergence of drug resistance. With the goal of better understanding drug resistance and its connection with the tumor microenvironment, we have developed a hybrid discrete-continuous mathematical model. In this model, cancer cells described through a particle-spring approach respond to dynamically changing oxygen and DNA damaging drug concentrations described through partial differential equations. We thoroughly explored the behavior of our self-calibrated model under the following common conditions: a fixed layout of the vasculature, an identical initial configuration of cancer cells, the same mechanism of drug action, and one mechanism of cellular response to the drug. We considered one set of simulations in which drug resistance existed prior to the start of treatment, and another set in which drug resistance is acquired in response to treatment. This allows us to compare how both kinds of resistance influence the spatial and temporal dynamics of the developing tumor, and its clonal diversity. We show that both pre-existing and acquired resistance can give rise to three biologically distinct parameter regimes: successful tumor eradication, reduced effectiveness of drug during the course of treatment (resistance), and complete treatment failure. 	
1501.03194v2	http://arxiv.org/pdf/1501.03194v2	2015	The cavity method for analysis of large-scale penalized regression	Mohammad Ramezanali|Partha P. Mitra|Anirvan M. Sengupta	  Penalized regression methods aim to retrieve reliable predictors among a large set of putative ones from a limited amount of measurements. In particular, penalized regression with singular penalty functions is important for sparse reconstruction algorithms. For large-scale problems, these algorithms exhibit sharp phase transition boundaries where sparse retrieval breaks down. Large optimization problems associated with sparse reconstruction have been analyzed in the literature by setting up corresponding statistical mechanical models at a finite temperature. Using replica method for mean field approximation, and subsequently taking a zero temperature limit, this approach reproduces the algorithmic phase transition boundaries. Unfortunately, the replica trick and the non-trivial zero temperature limit obscure the underlying reasons for the failure of a sparse reconstruction algorithm, and of penalized regression methods, in general. In this paper, we employ the ``cavity method'' to give an alternative derivation of the mean field equations, working directly in the zero-temperature limit. This derivation provides insight into the origin of the different terms in the self-consistency conditions. The cavity method naturally involves a quantity, the average local susceptibility, whose behavior distinguishes different phases in this system. This susceptibility can be generalized for analysis of a broader class of sparse reconstruction algorithms. 	
1504.04169v1	http://arxiv.org/pdf/1504.04169v1	2015	Fault Tolerant BFS Structures: A Reinforcement-Backup Tradeoff	Merav Parter|David Peleg	  This paper initiates the study of fault resilient network structures that mix two orthogonal protection mechanisms: (a) {\em backup}, namely, augmenting the structure with many (redundant) low-cost but fault-prone components, and (b) {\em reinforcement}, namely, acquiring high-cost but fault-resistant components. To study the trade-off between these two mechanisms in a concrete setting, we address the problem of designing a $(b,r)$ {\em fault-tolerant} BFS (or $(b,r)$ FT-BFS for short) structure, namely, a subgraph $H$ of the network $G$ consisting of two types of edges: a set $E' \subseteq E$ of $r(n)$ fault-resistant {\em reinforcement} edges, which are assumed to never fail, and a (larger) set $E(H) \setminus E'$ of $b(n)$ fault-prone {\em backup} edges, such that subsequent to the failure of a single fault-prone backup edge $e \in E \setminus E'$, the surviving part of $H$ still contains an BFS spanning tree for (the surviving part of) $G$, satisfying $dist(s,v,H\setminus \{e\}) \leq dist(s,v,G\setminus \{e\})$ for every $v \in V$ and $e \in E \setminus E'$. We establish the following tradeoff between $b(n)$ and $r(n)$: For every real $\epsilon \in (0,1]$, if $r(n) = {\tilde\Theta}(n^{1-\epsilon})$, then $b(n) = {\tilde\Theta}(n^{1+\epsilon})$ is necessary and sufficient. 	
1505.03066v3	http://arxiv.org/pdf/1505.03066v3	2016	Globally disruptive events show predictable timing patterns	Michael Gillman|Hilary Erenler	  Globally disruptive events include asteroid/comet impacts, large igneous provinces and glaciations, all of which have been considered as contributors to mass extinctions. Understanding the overall relationship between the timings of the largest extinctions and their potential proximal causes remains one of science's great unsolved mysteries. Cycles of about 60 million years in both fossil diversity and environmental data suggest external drivers such as the passage of the Solar System through the galactic plane. While cyclic phenomena are recognised statistically, a lack of coherent mechanisms and a failure to link key events has hampered wider acceptance of multi-million year periodicity and its relevance to earth science and evolution. The generation of a robust predictive model of timings, with a clear plausible primary mechanism, would signal a paradigm shift. Here, we present a model of the timings of globally disruptive events and a possible explanation of their ultimate cause. The proposed model is a symmetrical pattern of 63 million-year sequences around a central value, interpreted as the occurrence of events along, and parallel to, the galactic midplane. The symmetry is consistent with multiple dark matter disks, aligned parallel to the midplane. One implication of the precise pattern of timings and the underlying physical model is the ability to predict future events, such as a major extinction in one to two million years. 	
1505.04500v1	http://arxiv.org/pdf/1505.04500v1	2015	Unusual plastic deformation and damage features in Titanium:   experimental tests and constitutive modeling	Benoit Revil-Baudard|Oana Cazacu|Philip Flater|Nitin Chandola|J. L. Alves	  In this paper, we present an experimental study on plastic deformation and damage of polycrystalline pure Ti, as well as modeling of the observed behavior. From the mechanical characterization data, it can be concluded that the material displays anisotropy and tension-compression asymmetry. As concerns damage, the X-ray tomography measurements conducted reveal that damage distribution and evolution in this HCP Ti material is markedly different than in a typical FCC material such as copper. Stewart and Cazacu (2011) anisotropic elastic/plastic damage model is used to describe the behavior. All material parameters involved in this model have a clear physical significance, being related to plastic properties, and are determined based on very few simple mechanical tests. It is shown that this model predicts correctly the anisotropy in plastic deformation, and its strong influence on damage distribution and damage accumulation in Ti. Specifically, for a smooth axisymmetric specimen subject to uniaxial tension, damage initiates at the center of the specimen and is diffuse; the level of damage close to failure is very low. On the other hand, for a notched specimen subject to the same loading, the model predicts that damage initiates at the outer surface of the specimen, and further grows from the outer surface to the center of the specimen, which corroborates with the in-situ tomography data. 	
1506.03049v2	http://arxiv.org/pdf/1506.03049v2	2015	Formation and growth of shear bands in glasses: existence of an   underlying directed percolation transition	Gaurav Prakash Shrivastav|Pinaki Chaudhuri|Jürgen Horbach	  The response of glasses to mechanical loading often leads to the formation of inhomogeneous flow patterns that strongly affect materials properties. Among them, shear bands are ubiquitous in a wide variety of materials, ranging from soft matter systems to metallic alloys. Shear banding is associated with strain localization, i.e. the deformation of the sheared glassy solid is localized in space in form of band-like structures. These structures are often precursors to catastrophic failure, implying that a proper understanding of the underlying mechanisms could lead to the design of smarter materials. However, despite its importance in material science, the microscopic origin of shear banding in glassy materials is only poorly understood. Here, the formation of shear banding in glassy systems is revealed by non-equilibrium molecular dynamics simulations (NEMD) of a binary Lennard-Jones mixture, subject to a constant strain rate. In its glass state, this system exhibits for all considered strain rates the formation of a percolating cluster of mobile regions at a critical strain. We show that this percolation transition belongs to the universality class of directed percolation. Only at low shear rates, where the steady-state stress is close to the yielding threshold, the percolating cluster evolves into a transient (but long-lived) shear band with a diffusive growth of its width. 	
1506.03639v4	http://arxiv.org/pdf/1506.03639v4	2017	Mean-field description of plastic flow in amorphous solids	Jie Lin|Matthieu Wyart	  Failure and flow of amorphous materials are central to various phenomena including earthquakes and landslides. There is accumulating evidence that the yielding transition between a flowing and an arrested phase is a critical phenomenon, but the associated exponents are not understood, even at a mean-field level where the validity of popular models is debated. Here we solve a mean-field model that captures the broad distribution of the mechanical noise generated by plasticity, whose behavior is related to biased L\'evy flights near an absorbing boundary. We compute the exponent $\theta$ characterising the density of shear transformation $P(x)\sim x^{\theta}$, where $x$ is the stress increment beyond which they yield. We find that after an isotropic thermal quench, $\theta=1/2$. However, $\theta$ depends continuously on the applied shear stress, this dependence is not monotonic, and its value at the yield stress is not universal. The model rationalizes previously unexplained observations, and captures reasonably well the value of exponents in three dimensions. Values of exponents in four dimensions are accurately predicted. These results support that it is the true mean-field model that applies in large dimension, and raise fundamental questions on the nature of the yielding transition. 	
1509.06778v1	http://arxiv.org/pdf/1509.06778v1	2015	On the impact of Masking and Blocking Hypotheses for measuring efficacy   of new tuberculosis vaccines	Sergio Arregui|Joaquín Sanz|Dessislava Marinova|Carlos Martín|Yamir Moreno	  Over the past 60 years, the Mycobacterium bovis bacille Calmette-Gu\'erin (BCG) has been used worldwide to prevent tuberculosis (TB). However, BCG has shown a very variable efficacy in different trials, showing a wide range of protection in adults against pulmonary TB. Previous studies indicate that this failure is related to pre-existing immune response to antigens that are common to environmental sources of mycobacterial antigens and Mycobacterium tuberculosis. Specifically, two different mechanisms have been hypothesized: the masking, (previous sensitization confers some level of protection against TB), and the blocking (previous immune response prevent vaccine taking of a new TB vaccine), effects. In this work we introduce a series of models to discriminate between masking and blocking mechanisms and address their relative likelihood. The application of our models to interpret the results coming from the BCG-REVAC clinical trials, specifically designed for the study of sources of efficacy variability yields estimates that are consistent with high levels of blocking (41% in Manaus -95% C.I. [14%-68%]- and 96% in Salvador -95% C.I. [52%-100%]-), and no support for masking to play any relevant role in modifying vaccine efficacy either alone or aside blocking. The quantification of these effects around a plausible model constitutes a relevant step towards impact evaluation of novel anti-tuberculosis vaccines, which are susceptible of being affected by similar effects if applied on individuals previously exposed to mycobacterial antigens. 	
1512.06673v2	http://arxiv.org/pdf/1512.06673v2	2017	Photodissociation of ultracold diatomic strontium molecules with quantum   state control	M. McDonald|B. H. McGuyer|F. Apfelbeck|C. -H. Lee|I. Majewska|R. Moszynski|T. Zelevinsky	  Chemical reactions at ultracold temperatures are expected to be dominated by quantum mechanical effects. Although progress towards ultracold chemistry has been made through atomic photoassociation, Feshbach resonances and bimolecular collisions, these approaches have been limited by imperfect quantum state selectivity. In particular, attaining complete control of the ground or excited continuum quantum states has remained a challenge. Here we achieve this control using photodissociation, an approach that encodes a wealth of information in the angular distribution of outgoing fragments. By photodissociating ultracold 88Sr2 molecules with full control of the low-energy continuum, we access the quantum regime of ultracold chemistry, observing resonant and nonresonant barrier tunneling, matter-wave interference of reaction products and forbidden reaction pathways. Our results illustrate the failure of the traditional quasiclassical model of photodissociation and instead are accurately described by a quantum mechanical model. The experimental ability to produce well-defined quantum continuum states at low energies will enable high-precision studies of long-range molecular potentials for which accurate quantum chemistry models are unavailable, and may serve as a source of entangled states and coherent matter waves for a wide range of experiments in quantum optics. 	
1601.03130v1	http://arxiv.org/pdf/1601.03130v1	2016	A Micrometer-sized Heat Engine Operating Between Bacterial Reservoirs	Sudeesh Krishnamurthy|Subho Ghosh|Dipankar Chatterji|Rajesh Ganapathy|A. K. Sood	  Artificial micro heat engines are prototypical models to explore and elucidate the mechanisms of energy transduction in a regime that is dominated by fluctuations [1-2]. Micro heat engines realized hitherto mimicked their macroscopic counterparts and operated between reservoirs that were effectively thermal [3-7]. For such reservoirs, temperature is a well-defined state variable and stochastic thermodynamics provides a precise framework for quantifying engine performance [8-9]. It remains unclear whether these concepts readily carry over to situations where the reservoirs are out-of-equilibrium [10], a scenario of particular importance to the functioning of synthetic [11-12] and biological [13] micro engines and motors. Here we experimentally realized a micrometer-sized active Stirling engine by periodically cycling a colloidal particle in a time-varying harmonic optical potential across bacterial baths at different activities. Unlike in equilibrium thermal reservoirs, the displacement statistics of the trapped particle becomes increasingly non-Gaussian with activity. We show that as much as $\approx$ 85\% of the total power output and $\approx$ 50\% of the overall efficiency stems from large non-Gaussian particle displacements alone. Most remarkably, at the highest activities investigated, the efficiency of our quasi-static active heat engines surpasses the equilibrium saturation limit of Stirling efficiency - the maximum efficiency of a Stirling engine with the ratio of cold and hot reservoir temperatures ${T_C\over T_H} \to 0$. Crucially, the failure of effective temperature descriptions [14-16] for active reservoirs highlights the dire need for theories that can better capture the physics of micro motors and heat engines that operate in strongly non-thermal environments. 	
1604.06567v1	http://arxiv.org/pdf/1604.06567v1	2016	Concurrent Regenerating Codes and Scalable Application in Network   Storage	Huayu Zhang|Hui Li|Hanxu Hou|K. W. Shum|ShuoYen Robert Li	  To recover simultaneous multiple failures in erasure coded storage systems, Patrick Lee et al introduce concurrent repair based minimal storage regenerating codes to reduce repair traffic. The architecture of this approach is simpler and more practical than that of the cooperative mechanism in non-fully distributed environment, hence this paper unifies such class of regenerating codes as concurrent regenerating codes and further studies its characteristics by analyzing cut-based information flow graph in the multiple-node recovery model. We present a general storage-bandwidth tradeoff and give closed-form expressions for the points on the curve, including concurrent repair mechanism based on minimal bandwidth regenerating codes. We show that the general concurrent regenerating codes can be constructed by reforming the existing single-node regenerating codes or multiplenode cooperative regenerating codes. Moreover, a connection to strong-MDS is also analyzed. On the other respect, the application of RGC is hardly limited to "repairing". It is of great significance for "scaling", a scenario where we need to increase(decrease) nodes to upgrade(degrade) redundancy and reliability. Thus, by clarifying the similarities and differences, we integrate them into a unified model to adjust to the dynamic storage network. 	
1604.08041v1	http://arxiv.org/pdf/1604.08041v1	2016	Reducing DRAM Latency at Low Cost by Exploiting Heterogeneity	Donghyuk Lee	  In modern systems, DRAM-based main memory is significantly slower than the processor. Consequently, processors spend a long time waiting to access data from main memory, making the long main memory access latency one of the most critical bottlenecks to achieving high system performance. Unfortunately, the latency of DRAM has remained almost constant in the past decade. This is mainly because DRAM has been optimized for cost-per-bit, rather than access latency. As a result, DRAM latency is not reducing with technology scaling, and continues to be an important performance bottleneck in modern and future systems.   This dissertation seeks to achieve low latency DRAM-based memory systems at low cost in three major directions. First, based on the observation that long bitlines in DRAM are one of the dominant sources of DRAM latency, we propose a new DRAM architecture, Tiered-Latency DRAM (TL-DRAM), which divides the long bitline into two shorter segments using an isolation transistor, allowing one segment to be accessed with reduced latency. Second, we propose a fine-grained DRAM latency reduction mechanism, Adaptive-Latency DRAM, which optimizes DRAM latency for the common operating conditions for individual DRAM module. Third, we propose a new technique, Architectural-Variation-Aware DRAM (AVA-DRAM), which reduces DRAM latency at low cost, by profiling and identifying only the inherently slower regions in DRAM to dynamically determine the lowest latency DRAM can operate at without causing failures.   This dissertation provides a detailed analysis of DRAM latency by using both circuit-level simulation with a detailed DRAM model and FPGA-based profiling of real DRAM modules. Our latency analysis shows that our low latency DRAM mechanisms enable significant latency reductions, leading to large improvement in both system performance and energy efficiency. 	
1605.09678v1	http://arxiv.org/pdf/1605.09678v1	2016	Level Up Your Strategy: Towards a Descriptive Framework for Meaningful   Enterprise Gamification	Umar Ruhi	  Gamification initiatives are currently top-of-mind for many organizations seeking to engage their employees in creative ways, improve their productivity, and drive positive behavioural outcomes in their workforce - ultimately leading to positive business outcomes on the whole. Despite its touted benefits, little empirical research has been done to date to investigate technological and individual personal factors that determine the success or failure of enterprise gamification initiatives. In this article, we provide a summary of our preliminary research findings from three case studies of gamification initiatives across different business contexts and present an empirically validated descriptive framework that details the key success factors for enterprise gamification. Our adaptation of the mechanics, dynamics, and aesthetics (MDA) framework for enterprise gamification aims to explicate the connections between end-user motivations, interactive gameplay elements, and technology features and functions that constitute effective gamification interventions in the enterprise. Following a discussion of the core elements in the framework and their interrelationships, the implications of our research are presented in the form of guidelines for the management and design of gamification initiatives and applications. The research findings presented in this article can potentially aid in the development of game mechanics that translate into positive user experiences and foster higher levels of employee engagement. Additionally, our research findings provide insights on key success factors for the effective adoption and institutionalization of enterprise gamification initiatives in organizations, and subsequently help them enhance the performance of their employees and drive positive business outcomes. 	
1606.08779v2	http://arxiv.org/pdf/1606.08779v2	2016	Interface propagation in fiber bundles: Local, mean-field and   intermediate range-dependent statistics	Soumyajyoti Biswas|Lucas Goehring	  The fiber bundle model is essentially an array of elements that break when sufficient load is applied on them. With a local loading mechanism, this can serve as a model for a one-dimensional interface separating the broken and unbroken parts of a solid in mode-I fracture. The interface can propagate through the system depending on the loading rate and disorder present in the failure thresholds of the fibers. In the presence of a quasi-static drive, the intermittent dynamics of the interface mimic front propagation in disordered media. Such situations appear in diverse physical systems such as mode-I crack propagation, domain wall dynamics in magnets, charge density waves, contact line in wetting etc. We study the effect of the range of interaction, i.e. the neighborhood of the interface affected following a local perturbation, on the statistics of the intermittent dynamics of the front. There exists a crossover from local to global behavior as the range of interaction grows and a continuously varying `universality' in the intermediate range. This means that the interaction range is a relevant parameter of any resulting physics. This is particularly relevant in view of the fact that there is a scatter in the experimental observations of the exponents, in even idealized experiments on fracture fronts, and also a possibility in changing the interaction range in real samples. 	
1607.05699v1	http://arxiv.org/pdf/1607.05699v1	2016	Stochastic Epidemic Networks with Strategic Link Formation	Jie Xu	  Understanding cascading failures or epidemics in networks is crucial for developing effective defensive mechanisms for many critical systems and infrastructures (e.g. biological, social and cyber networks). Most of the existing works treat the network topology as being exogenously given and study under what conditions an epidemic breaks out and/or extinguishes. However, if agents are able to strategically decide their connections according to their own self-interest, the network will instead be endogenously formed and evolving. In such systems, the epidemic, agents' strategic decisions and the network structure become complexly coupled and co-evolve. As a result, existing knowledge may no longer be applicable. Built on a continuous time Susceptible-Infected-Susceptible epidemic model with strong mixing, this paper studies stochastic epidemic networks consisting of strategic agents, who decide the number of links to form based on a careful evaluation of its current obtainable benefit and the potential future cost due to infection by forming links. A game theoretical framework is developed to analyze such networks and a number of important insights are obtained. One key result is that whereas an epidemic eventually dies out if the effective spreading rate is sufficiently low in exogenously given networks, it never dies out when agents are strategic regardless of the effective spreading rate. This property leads to reduced achievable system efficiency and considerably different optimal protection mechanisms. Without understanding the strategic behavior of agents, significant security cost may incur. 	
1610.08301v1	http://arxiv.org/pdf/1610.08301v1	2016	Characterization of Surface Deformation Behavior, Mechanical and   Physical Properties of Modified-clay Bricks	David N Githinji|Charles K Nzila|John T Githaiga|David R Tuigong|Albert O Osiemo|Peter O Ayoro	  The demand for building material is ever increasing owing to population growth. Compacted clay bricks are an important integral building material especially for low cost durable and affordable housing segment. This is a valued building material since its properties can be modified to suit various loading conditions. In this paper, the mechanical and physical properties of clay bricks modified with varying proportions of sawdust and polystyrene are determined. Increment of non-clay material proportion in the modified-clay bricks increases their porosity and water absorbency while their bulk densities, compressive and flexural strengths decreases. The use is made of Particle Image Velocimetry (PIV) method to assess the surface deformation behavior of the modified-clay bricks under uniaxial compressive loading. The distribution of surface deformation as assessed through PIV method is relatively uniform in pure-clay bricks while modified-clay bricks indicates a non-uniform deformation localized near the loading point at low strains. The strain distribution progressively spread out in the modifiedclay brick as the failure point is approached. 	
1706.01560v1	http://arxiv.org/pdf/1706.01560v1	2017	Stateless Puzzles for Real Time Online Fraud Preemption	Mizanur Rahman|Ruben Recabarren|Bogdan Carbunar|Dongwon Lee	  The profitability of fraud in online systems such as app markets and social networks marks the failure of existing defense mechanisms. In this paper, we propose FraudSys, a real-time fraud preemption approach that imposes Bitcoin-inspired computational puzzles on the devices that post online system activities, such as reviews and likes. We introduce and leverage several novel concepts that include (i) stateless, verifiable computational puzzles, that impose minimal performance overhead, but enable the efficient verification of their authenticity, (ii) a real-time, graph-based solution to assign fraud scores to user activities, and (iii) mechanisms to dynamically adjust puzzle difficulty levels based on fraud scores and the computational capabilities of devices. FraudSys does not alter the experience of users in online systems, but delays fraudulent actions and consumes significant computational resources of the fraudsters. Using real datasets from Google Play and Facebook, we demonstrate the feasibility of FraudSys by showing that the devices of honest users are minimally impacted, while fraudster controlled devices receive daily computational penalties of up to 3,079 hours. In addition, we show that with FraudSys, fraud does not pay off, as a user equipped with mining hardware (e.g., AntMiner S7) will earn less than half through fraud than from honest Bitcoin mining. 	
1709.05228v1	http://arxiv.org/pdf/1709.05228v1	2017	Spontaneous surface reserve formation in wicked membranes bestow extreme   stretchability	Paul Grandgeorge|Natacha Krins|Aurélie Hourlier-Fargette|Christel Laberty|Sébastien Neukirch|Arnaud Antkowiak	  Soft stretchable materials are key for arising technologies such as stretchable electronics or batteries, smart textiles, biomedical devices, tissue engineering and soft robotics. Recent attempts to design such materials, via e.g. micro-patterning of wavy fibres on soft substrates, polymer engineering at the molecular level or even kirigami techniques, provide appealing prospects but suffer drawbacks impacting the material viability: complexity of manufacturing, fatigue or failure upon cycling, restricted range of materials or biological incompatibility. Here, we report a universal strategy to design highly stretchable, self-assembling and fatigue-resistant synthetic fabrics. Our approach finds its inspiration in the mechanics of living animal cells that routinely encounter and cope with extreme deformations, e.g. with the engulfment of large intruders by macrophages, squeezing and stretching of immune cells in tiny capillaries or shrinking/swelling of neurons upon osmotic stimuli. All these large instant deformations are actually mediated and buffered by membrane reserves available in the form of microvilli, membrane folds or endomembrane that can be recruited on demand. We synthetically mimicked this behavior by creating nanofibrous liquid-infused tissues spontaneously forming surface reserves whose unfolding fuels any imposed shape change. Our process, relying only on geometry, elasticity and capillarity, allows to endow virtually any material with high stretchability and reversibility, making it straightforward to implement additional mechanical, electrical or chemical functions. We illustrate this with proof-of-concept activable capillary muscles, adaptable slippery liquid infused porous surfaces and stretchable basic printed electronic circuits. 	
1709.08269v1	http://arxiv.org/pdf/1709.08269v1	2017	Particle Acceleration and Fractional Transport in Turbulent Reconnection	Heinz Isliker|Theophilos Pisokas|Loukas Vlahos|Anastasios Anastasiadis	  We consider a large scale environment of turbulent reconnection that is fragmented into a number of randomly distributed Unstable Current Sheets (UCS), and we statistically analyze the acceleration of particles within this environment. We address two important cases of acceleration mechanisms when the particles interact with the UCS: (a) electric field acceleration, and (b) acceleration through reflection at contracting islands. Electrons and ions are accelerated very efficiently, attaining an energy distribution of power-law shape with an index $1-2$, depending on the acceleration mechanism. The transport coefficients in energy space are estimated from the test-particle simulation data, and we show that the classical Fokker-Planck (FP) equation fails to reproduce the simulation results when the transport coefficients are inserted into it and it is solved numerically. The cause for this failure is that the particles perform Levy flights in energy space, the distributions of energy increments exhibit power-law tails. We then use the fractional transport equation (FTE) derived by Isliker et al., 2017, whose parameters and the order of the fractional derivatives are inferred from the simulation data, and, solving the FTE numerically, we show that the FTE successfully reproduces the kinetic energy distribution of the test-particles. We discuss in detail the analysis of the simulation data and the criteria that allow judging the appropriateness of either an FTE or a classical FP equation as a transport model. 	
1710.01996v1	http://arxiv.org/pdf/1710.01996v1	2017	Network approach towards understanding the crazing in glassy amorphous   polymers	Sudarkodi Venkatesan|R. P. Vivek-Ananth|R. P. Sreejith|P. Mangalapandi|Ali A. Hassanali|Areejit Samal	  We have used molecular dynamics to simulate an amorphous glassy polymer with long chains to study deformation mechanism of crazing and associated void statistics. The Van der Waals interactions and the entanglements between chains constituting the polymer play a crucial role in crazing. Thus, we have reconstructed two underlying weighted networks, namely, the Van der Waals network and the Entanglement network from polymer configurations extracted from the molecular dynamics simulation. Subsequently, we have performed graph-theoretic analysis of the two reconstructed networks to reveal the role played by them in crazing of polymers. Our analysis captured various stages of crazing through specific trends in the network measures for Van der Waals networks and entanglement networks. To further corroborate the effectiveness of network analysis in unraveling the underlying physics of crazing in polymers, we have contrasted the trends in network measures for Van der Waals networks and entanglement networks in the light of stress-strain behaviour and voids statistics during deformation. We find that Van der Waals network plays a crucial role in craze initiation and growth. Although, the entanglement network was found to maintain its structure during craze initiation stage, it was found to progressively weaken and undergo dynamic changes during the hardening and failure stages of crazing phenomena. Our work demonstrates the utility of network theory in quantifying the underlying physics of polymer crazing and widens the scope of applications of network science to characterization of deformation mechanisms in diverse polymers. 	
1801.05378v1	http://arxiv.org/pdf/1801.05378v1	2018	Quasi-static and Dynamic Behavior of Additively Manufactured Metallic   Lattice Cylinders	Hossein Sadeghi|Dhruv Bhate|Joseph Abraham|Joseph Magallanes	  Lattice structures have tailorable mechanical properties which allows them to exhibit superior mechanical properties (per unit weight) beyond what is achievable through natural materials. In this paper, quasi-static and dynamic behavior of additively manufactured stainless steel lattice cylinders is studied. Cylindrical samples with internal lattice structure are fabricated by a laser powder bed fusion system. Equivalent hollow cylindrical samples with the same length, outer diameter, and mass (larger wall thickness) are also fabricated. Split Hopkinson bar is used to study the behavior of the specimens under high strain rate loading. It is observed that lattice cylinders reduce the transmitted wave amplitude up to about 21% compared to their equivalent hollow cylinders. However, the lower transmitted wave energy in lattice cylinders comes at the expense of a greater reduction in their stiffness, when compared to their equivalent hollow cylinder. In addition, it is observed that increasing the loading rate by five orders of magnitude leads to up to about 36% increase in the peak force that the lattice cylinder can carry, which is attributed to strain rate hardening effect in the bulk stainless steel material. Finite element simulations of the specimens under dynamic loads are performed to study the effect of strain rate hardening, thermal softening, and the failure mode on dynamic behavior of the specimens. Numerical results are compared with experimental data and good qualitative agreement is observed. 	
1003.4929v2	http://arxiv.org/pdf/1003.4929v2	2010	Dynamics of the contact between a ruthenium surface with a single   nanoasperity and a flat ruthenium surface: Molecular dynamics simulations	Alan Barros de Oliveira|Andrea Fortini|Sergey V. Buldyrev|David Srolovitz	  We study the dynamics of the contact between a pair of surfaces (with properties designed to mimic ruthenium) via molecular dynamics simulations. In particular, we study the contact between a ruthenium surface with a single nanoasperity and a flat ruthenium surface. The results of such simulations suggest that contact behavior is highly variable. The goal of this study is to investigate the source and degree of this variability. We find that during compression, the behavior of the contact force displacement curves is reproducible, while during contact separation, the behavior is highly variable. Examination of the contact surfaces suggest that two separation mechanism are in operation and give rise to this variability. One mechanism corresponds to the formation of a bridge between the two surfaces that plastically stretches as the surfaces are drawn apart and eventually separates in shear. This leads to a morphology after separation in which there are opposing asperities on the two surfaces. This plastic separation/bridge formation mechanism leads to a large work of separation. The other mechanism is a more brittle-like mode in which a crack propagates across the base of the asperity (slightly below the asperity/substrate junction) leading to most of the asperity on one surface or the other after separation and a slight depression facing this asperity on the opposing surface. This failure mode corresponds to a smaller work of separation. those in which a single mechanism operates. Furthermore, contacts made from materials that exhibit predominantly brittle-like behavior will tend to require lower work of separation than those made from ductile-like contact materials. 	
1610.09604v2	http://arxiv.org/pdf/1610.09604v2	2017	Understanding and Exploiting Design-Induced Latency Variation in Modern   DRAM Chips	Donghyuk Lee|Samira Khan|Lavanya Subramanian|Saugata Ghose|Rachata Ausavarungnirun|Gennady Pekhimenko|Vivek Seshadri|Onur Mutlu	  Variation has been shown to exist across the cells within a modern DRAM chip. We empirically demonstrate a new form of variation that exists within a real DRAM chip, induced by the design and placement of different components in the DRAM chip. Our goals are to understand design-induced variation that exists in real, state-of-the-art DRAM chips, exploit it to develop low-cost mechanisms that can dynamically find and use the lowest latency at which to operate a DRAM chip reliably, and, thus, improve overall system performance while ensuring reliable system operation.   To this end, we first experimentally demonstrate and analyze designed-induced variation in modern DRAM devices by testing and characterizing 96 DIMMs (768 DRAM chips). Our characterization identifies DRAM regions that are vulnerable to errors, if operated at lower latency, and finds consistency in their locations across a given DRAM chip generation, due to design-induced variation. Based on our extensive experimental analysis, we develop two mechanisms that reliably reduce DRAM latency. First, DIVA Profiling uses runtime profiling to dynamically identify the lowest DRAM latency that does not introduce failures. DIVA Profiling exploits design-induced variation and periodically profiles only the vulnerable regions to determine the lowest DRAM latency at low cost. Our second mechanism, DIVA Shuffling, shuffles data such that values stored in vulnerable regions are mapped to multiple error-correcting code (ECC) codewords. Combined together, our two mechanisms reduce read/write latency by 40.0%/60.5%, which translates to an overall system performance improvement of 14.7%/13.7%/13.8% (in 2-/4-/8-core systems) across a variety of workloads, while ensuring reliable operation. 	
0610165v1	http://arxiv.org/pdf/cs/0610165v1	2006	Decentralized Failure Diagnosis of Stochastic Discrete Event Systems	Fuchun Liu|Daowen Qiu|Hongyan Xing|Zhujun Fan	  Recently, the diagnosability of {\it stochastic discrete event systems} (SDESs) was investigated in the literature, and, the failure diagnosis considered was {\it centralized}. In this paper, we propose an approach to {\it decentralized} failure diagnosis of SDESs, where the stochastic system uses multiple local diagnosers to detect failures and each local diagnoser possesses its own information. In a way, the centralized failure diagnosis of SDESs can be viewed as a special case of the decentralized failure diagnosis presented in this paper with only one projection. The main contributions are as follows: (1) We formalize the notion of codiagnosability for stochastic automata, which means that a failure can be detected by at least one local stochastic diagnoser within a finite delay. (2) We construct a codiagnoser from a given stochastic automaton with multiple projections, and the codiagnoser associated with the local diagnosers is used to test codiagnosability condition of SDESs. (3) We deal with a number of basic properties of the codiagnoser. In particular, a necessary and sufficient condition for the codiagnosability of SDESs is presented. (4) We give a computing method in detail to check whether codiagnosability is violated. And (5) some examples are described to illustrate the applications of the codiagnosability and its computing method. 	
1206.6701v2	http://arxiv.org/pdf/1206.6701v2	2014	Evaluating the dependence of a non-leaky intervention's partial efficacy   on a categorical mark	Paul T. Edlefsen	  We address discrete-marks survival analysis, also known as categorical sieve analysis, for a setting of a randomized placebo-controlled treatment intervention to prevent infection by a pathogen to which multiple exposures are possible, with a finite number of types of "failure". In particular, we address the case of interventions that are partially efficacious due to a combination of failure-type-dependent efficacy and subject-dependent efficacy, for an intervention that is "non-leaky" (where "leaky" interventions are those for which each exposure event has a chance of resulting in a "failure" outcome, so multiple exposures to pathogens of a single type increase the chance of failure). We introduce the notion of some-or-none interventions, which are completely effective only against some of the failure types, and are completely ineffective against the others. Under conditions of no intervention-induced failures, we introduce a framework and Bayesian and frequentist methods to detect and quantify the extent to which an intervention's partial efficacy is attributable to uneven efficacy across the failure types rather than to incomplete "take" of the intervention. These new methods provide more power than existing methods to detect sieve effects when the conditions hold. We demonstrate the new framework and methods with simulation results and new analyses of genomic signatures of HIV-1 vaccine effects in the STEP and RV144 vaccine efficacy trials. 	
1310.1050v1	http://arxiv.org/pdf/1310.1050v1	2013	The failure tolerance of mechatronic software systems to random and   targeted attacks	Dharshana Kasthurirathna|Andy Dong|Mahendrarajah Piraveenan|Irem Y. Tumer	  This paper describes a complex networks approach to study the failure tolerance of mechatronic software systems under various types of hardware and/or software failures. We produce synthetic system architectures based on evidence of modular and hierarchical modular product architectures and known motifs for the interconnection of physical components to software. The system architectures are then subject to various forms of attack. The attacks simulate failure of critical hardware or software. Four types of attack are investigated: degree centrality, betweenness centrality, closeness centrality and random attack. Failure tolerance of the system is measured by a 'robustness coefficient', a topological 'size' metric of the connectedness of the attacked network. We find that the betweenness centrality attack results in the most significant reduction in the robustness coefficient, confirming betweenness centrality, rather than the number of connections (i.e. degree), as the most conservative metric of component importance. A counter-intuitive finding is that "designed" system architectures, including a bus, ring, and star architecture, are not significantly more failure-tolerant than interconnections with no prescribed architecture, that is, a random architecture. Our research provides a data-driven approach to engineer the architecture of mechatronic software systems for failure tolerance. 	
1402.2680v1	http://arxiv.org/pdf/1402.2680v1	2014	Unveiling Potential Failure Propagation Scenarios in Core Transport   Networks	Marc Manzano|Anna Manolova Fagertun|Sarah Ruepp|Eusebi Calle|Caterina Scoglio|Ali Sydney|Antonio de la Oliva|Alfonso Muñoz	  The contemporary society has become more dependent on telecommunication networks. Novel services and technologies supported by such networks, such as cloud computing or e-Health, hold a vital role in modern day living. Large-scale failures are prone to occur, thus being a constant threat to business organizations and individuals. To the best of our knowledge, there are no publicly available reports regarding failure propagation in core transport networks. Furthermore, Software Defined Networking (SDN) is becoming more prevalent in our society and we can envision more SDN-controlled Backbone Transport Networks (BTNs) in the future. For this reason, we investigate the main motivations that could lead to epidemic-like failures in BTNs and SDNTNs. To do so, we enlist the expertise of several research groups with significant background in epidemics, network resiliency, and security. In addition, we consider the experiences of three network providers. Our results illustrate that Dynamic Transport Networks (DTNs) are prone to epidemic-like failures. Moreover, we propose different situations in which a failure can propagate in SDNTNs. We believe that the key findings will aid network engineers and the scientific community to predict this type of disastrous failure scenario and plan adequate survivability strategies. 	
1402.5282v1	http://arxiv.org/pdf/1402.5282v1	2014	The Compound Class of Linear Failure Rate-Power Series Distributions:   Model, Properties and Applications	Eisa Mahmoudi|Ali Akbar Jafari	  We introduce in this paper a new class of distributions which generalizes the linear failure rate (LFR) distribution and is obtained by compounding the LFR distribution and power series (PS) class of distributions. This new class of distributions is called the linear failure rate-power series (LFRPS) distributions and contains some new distributions such as linear failure rate geometric (LFRG) distribution, linear failure rate Poisson (LFRP) distribution, linear failure rate logarithmic (LFRL) distribution, linear failure rate binomial (LFRB) distribution and Raylight-power series (RPS) class of distributions. Some former works such as exponential-power series (EPS) class of distributions, exponential geometric (EG) distribution, exponential Poisson (EP) distribution and exponential logarithmic (EL) distribution are special cases of the new proposed model.   The ability of the LFRPS class of distributions is in covering five possible hazard rate function i.e., increasing, decreasing, upside-down bathtub (unimodal), bathtub and increasing-decreasing-increasing shaped. Several properties of the LFRPS distributions such as moments, maximum likelihood estimation procedure via an EM-algorithm and inference for a large sample, are discussed in this paper. In order to show the flexibility and potentiality of the new class of distributions, the fitted results of the new class of distributions and some its submodels are compared using a real data set. 	
1408.0409v1	http://arxiv.org/pdf/1408.0409v1	2014	Vertex Fault Tolerant Additive Spanners	Merav Parter	  A {\em fault-tolerant} structure for a network is required to continue functioning following the failure of some of the network's edges or vertices. In this paper, we address the problem of designing a {\em fault-tolerant} additive spanner, namely, a subgraph $H$ of the network $G$ such that subsequent to the failure of a single vertex, the surviving part of $H$ still contains an \emph{additive} spanner for (the surviving part of) $G$, satisfying $dist(s,t,H\setminus \{v\}) \leq dist(s,t,G\setminus \{v\})+\beta$ for every $s,t,v \in V$. Recently, the problem of constructing fault-tolerant additive spanners resilient to the failure of up to $f$ \emph{edges} has been considered by Braunschvig et. al. The problem of handling \emph{vertex} failures was left open therein. In this paper we develop new techniques for constructing additive FT-spanners overcoming the failure of a single vertex in the graph. Our first result is an FT-spanner with additive stretch $2$ and $\widetilde{O}(n^{5/3})$ edges. Our second result is an FT-spanner with additive stretch $6$ and $\widetilde{O}(n^{3/2})$ edges. The construction algorithm consists of two main components: (a) constructing an FT-clustering graph and (b) applying a modified path-buying procedure suitably adopted to failure prone settings. Finally, we also describe two constructions for {\em fault-tolerant multi-source additive spanners}, aiming to guarantee a bounded additive stretch following a vertex failure, for every pair of vertices in $S \times V$ for a given subset of sources $S\subseteq V$. The additive stretch bounds of our constructions are 4 and 8 (using a different number of edges). 	
1506.03555v1	http://arxiv.org/pdf/1506.03555v1	2015	Automatic Generation of Minimal Cut Sets	Sentot Kromodimoeljo|Peter A. Lindsay	  A cut set is a collection of component failure modes that could lead to a system failure. Cut Set Analysis (CSA) is applied to critical systems to identify and rank system vulnerabilities at design time. Model checking tools have been used to automate the generation of minimal cut sets but are generally based on checking reachability of system failure states. This paper describes a new approach to CSA using a Linear Temporal Logic (LTL) model checker called BT Analyser that supports the generation of multiple counterexamples. The approach enables a broader class of system failures to be analysed, by generalising from failure state formulae to failure behaviours expressed in LTL. The traditional approach to CSA using model checking requires the model or system failure to be modified, usually by hand, to eliminate already-discovered cut sets, and the model checker to be rerun, at each step. By contrast, the new approach works incrementally and fully automatically, thereby removing the tedious and error-prone manual process and resulting in significantly reduced computation time. This in turn enables larger models to be checked. Two different strategies for using BT Analyser for CSA are presented. There is generally no single best strategy for model checking: their relative efficiency depends on the model and property being analysed. Comparative results are given for the A320 hydraulics case study in the Behavior Tree modelling language. 	
1510.02735v2	http://arxiv.org/pdf/1510.02735v2	2015	Reliability and Survivability Analysis of Data Center Network Topologies	Rodrigo de Souza Couto|Stefano Secci|Miguel Elias Mitre Campista|Luís Henrique Maciel Kosmalski Costa	  The architecture of several data centers have been proposed as alternatives to the conventional three-layer one.Most of them employ commodity equipment for cost reduction. Thus, robustness to failures becomes even more important, because commodity equipment is more failure-prone. Each architecture has a different network topology design with a specific level of redundancy. In this work, we aim at analyzing the benefits of different data center topologies taking the reliability and survivability requirements into account. We consider the topologies of three alternative data center architecture: Fat-tree, BCube, and DCell. Also, we compare these topologies with a conventional three-layer data center topology. Our analysis is independent of specific equipment, traffic patterns, or network protocols, for the sake of generality. We derive closed-form formulas for the Mean Time To Failure of each topology. The results allow us to indicate the best topology for each failure scenario. In particular, we conclude that BCube is more robust to link failures than the other topologies, whereas DCell has the most robust topology when considering switch failures. Additionally, we show that all considered alternative topologies outperform a three-layer topology for both types of failures. We also determine to which extent the robustness of BCube and DCell is influenced by the number of network interfaces per server. 	
1606.00521v1	http://arxiv.org/pdf/1606.00521v1	2016	Initial and Eventual Software Quality Relating to Continuous Integration   in GitHub	Yue Yu|Bogdan Vasilescu|Huaimin Wang|Vladimir Filkov|Premkumar Devanbu	  The constant demand for new features and bug fixes are forcing software projects to shorten cycles and deliver updates ever faster, while sustaining software quality. The availability of inexpensive, virtualized, cloud-computing has helped shorten schedules, by enabling continuous integration (CI) on demand. Platforms like GitHub support CI in-the-cloud. In projects using CI, a user submitting a pull request triggers a CI step. Besides speeding up build and test, this fortuitously creates voluminous archives of build and test successes and failures. CI is a relatively new phenomenon, and these archives allow a detailed study of CI. How many problems are exposed? Where do they occur? What factors affect CI failures? Does the "initial quality" as ascertained by CI predict how many bugs will later appear ("eventual quality") in the code? In this paper, we undertake a large-scale, fine resolution study of these records, to better understand CI processes, the nature, and predictors of CI failures, and the relationship of CI failures to the eventual quality of the code. We find that: a) CI failures appear to be concentrated in a few files, just like normal bugs; b) CI failures are not very highly correlated with eventual failures; c) The use of CI in a pull request doesn't necessarily mean the code in that request is of good quality. 	
1708.08309v2	http://arxiv.org/pdf/1708.08309v2	2017	A Dual Digraph Approach for Leaderless Atomic Broadcast (Extended   Version)	Marius Poke|Colin W. Glass	  Many distributed systems work on a common shared state; in such systems, distributed agreement is necessary for consistency. With an increasing number of servers, systems become more susceptible to single-server failures, increasing the relevance of fault-tolerance. Atomic broadcast enables fault-tolerant distributed agreement, yet it is costly to solve. Most practical algorithms entail linear work per broadcast message. AllConcur -- a leaderless approach -- reduces the work by connecting the servers via a resilient overlay network; yet, this resiliency entails redundancy, which reduces performance. In this work, we propose AllConcur+, an extension of AllConcur. During intervals with no failures, it uses an overlay network with no redundancy and automatically switches to a resilient overlay network when failures occur. Our performance estimation shows that if no failures occur, AllConcur+ achieves up to 10x higher throughput and up to 5x lower latency than AllConcur. In the presence of occasional failures, AllConcur+ still outperforms AllConcur significantly. In the worst case, AllConcur+'s performance is worse than AllConcur's, yet, this requires frequent failures at very specific intervals. Thus, for realistic use cases, leveraging redundancy-free distributed agreement during intervals with no failures, increases the expected performance. 	
0712.4258v2	http://arxiv.org/pdf/0712.4258v2	2008	Two dogmas about quantum mechanics	Jeffrey Bub|Itamar Pitowsky	  We argue that the intractable part of the measurement problem -- the 'big' measurement problem -- is a pseudo-problem that depends for its legitimacy on the acceptance of two dogmas. The first dogma is John Bell's assertion that measurement should never be introduced as a primitive process in a fundamental mechanical theory like classical or quantum mechanics, but should always be open to a complete analysis, in principle, of how the individual outcomes come about dynamically. The second dogma is the view that the quantum state has an ontological significance analogous to the significance of the classical state as the 'truthmaker' for propositions about the occurrence and non-occurrence of events, i.e., that the quantum state is a representation of physical reality. We show how both dogmas can be rejected in a realist information-theoretic interpretation of quantum mechanics as an alternative to the Everett interpretation. The Everettian, too, regards the 'big' measurement problem as a pseudo-problem, because the Everettian rejects the assumption that measurements have definite outcomes, in the sense that one particular outcome, as opposed to other possible outcomes, actually occurs in a quantum measurement process. By contrast with the Everettians, we accept that measurements have definite outcomes. By contrast with the Bohmians and the GRW 'collapse' theorists who add structure to the theory and propose dynamical solutions to the 'big' measurement problem, we take the problem to arise from the failure to see the significance of Hilbert space as a new kinematic framework for the physics of an indeterministic universe, in the sense that Hilbert space imposes kinematic (i.e., pre-dynamic) objective probabilistic constraints on correlations between events. 	
1609.07395v1	http://arxiv.org/pdf/1609.07395v1	2016	A study of cascading failures in real and synthetic power grid   topologies using DC power flows	Russell Spiewak|Sergey V. Buldyrev|Yakir Forman|Saleh Soltan|Gil Zussman	  Using the linearized DC power flow model, we study cascading failures and their spatial and temporal properties in the US Western Interconnect (USWI) power grid. We also introduce the preferential Degree And Distance Attachment (DADA) model, with similar degree distributions, resistances, and currents to the USWI. We investigate the behavior of both grids resulting from the failure of a single line. We find that the DADA model and the USWI model react very similarly to that failure, and that their blackout characteristics resemble each other. In many cases, the failure of a single line can cause cascading failures, which impact the entire grid. We characterize the resilience of the grid by three parameters, the most important of which is tolerance ${\alpha}$, which is the ratio of the maximal load a line can carry to its initial load. We characterize a blackout by its yield, which we define as the ratio of the final to the initial consumed currents. We find that if ${\alpha}\leq2$, the probability of a large blackout occurring is very small. By contrast, in a broad range of $1 < {\alpha} < 2$, the initial failure of a single line can result, with a high probability, in cascading failures leading to a massive blackout with final yield less than 80%. The yield has a bimodal distribution typical of a first-order transition, i.e., the failure of a randomly selected line leads either to an insignificant current reduction or to a major blackout. We find that there is a latent period in the development of major blackouts during which few lines are overloaded, and the yield remains high. The duration of this latent period is proportional to the tolerance. The existence of the latent period suggests that intervention during early time steps of a cascade can significantly reduce the risk of a major blackout. 	
0008388v1	http://arxiv.org/pdf/cond-mat/0008388v1	2000	Atomistics of Tensile Failure in Fused Silica: Weakest Link Models   Revisited	J. I. Katz	  In weakest link models the failure of a single microscopic element of a brittle material causes the failure of an entire macroscopic specimen, just as a chain fails if one link fails. Pristine samples of glass, such as optical communications fiber, approach their ideal strength, and their brittle tensile failure has been described by this model. The statistics of weakest link models are calculable in terms of the statistics of the individual links, which, unfortunately, are poorly known. Use of the skewness of the failure distribution may permit simultaneous determination of the statistics of the individual weak links and of their number density, which indicates their physical origin. However, the applicability of weakest link models to real materials remains unproven. 	
0506725v5	http://arxiv.org/pdf/cond-mat/0506725v5	2005	Entropy Optimization of Scale-Free Networks Robustness to Random   Failures	Bing Wang|Huanwen Tang|Chonghui Guo|Zhilong Xiu	  Many networks are characterized by highly heterogeneous distributions of links, which are called scale-free networks and the degree distributions follow $p(k)\sim ck^{-\alpha}$. We study the robustness of scale-free networks to random failures from the character of their heterogeneity. Entropy of the degree distribution can be an average measure of a network's heterogeneity. Optimization of scale-free network robustness to random failures with average connectivity constant is equivalent to maximize the entropy of the degree distribution. By examining the relationship of entropy of the degree distribution, scaling exponent and the minimal connectivity, we get the optimal design of scale-free network to random failures. We conclude that entropy of the degree distribution is an effective measure of network's resilience to random failures. 	
0411297v1	http://arxiv.org/pdf/math/0411297v1	2004	On Representing the Mean Residual Life in Terms of the Failure Rate	Ramesh C. Gupta|David M. Bradley	  In survival or reliability studies, the mean residual life or life expectancy is an important characteristic of the model. Whereas the failure rate can be expressed quite simply in terms of the mean residual life and its derivative, the inverse problem--namely that of expressing the mean residual life in terms of the failure rate--typically involves an integral of a complicated expression. In this paper, we obtain simple expressions for the mean residual life in terms of the failure rate for certain classes of distributions which subsume many of the standard cases. Several results in the literature can be obtained using our approach. Additionally, we develop an expansion for the mean residual life in terms of Gaussian probability functions for a broad class of ultimately increasing failure rate distributions. Some examples are provided to illustrate the procedure. 	
0509100v1	http://arxiv.org/pdf/physics/0509100v1	2005	Constitutive Laws and Failure Models for Compact Bones Subjected to   Dynamic Loading	Martine Pithioux|P. Chabrand|M. Jean	  Many biological tissues, such as bones and ligaments, are fibrous. The geometrical structure of these tissues shows that they exhibit a similar hierarchy in their ultra-structure and macro-structure. The aim of this work is to develop a model to study the failure of fibrous structures subjected to dynamic loading. The important feature of this model is that it describes failure in terms of the loss of cohesion between fibres. We have developed a model based on the lamellar structure of compact bone with fibres oriented at 0 degrees, 45 degrees and 90 degrees to the longitudinal axis of the bone, and have studied the influence of the model parameters on the failure process. Bone porosity and joint stress force at failure were found to be the most significant parameters. Using least square resolution, we deduced a phenomenological model of the lamellar structure. Finally, experimental results were found to be comparable with our numerical model. 	
0704.1952v2	http://arxiv.org/pdf/0704.1952v2	2008	Dynamic Effects Increasing Network Vulnerability to Cascading Failures	Ingve Simonsen|Lubos Buzna|Karsten Peters|Stefan Bornholdt|Dirk Helbing	  We study cascading failures in networks using a dynamical flow model based on simple conservation and distribution laws to investigate the impact of transient dynamics caused by the rebalancing of loads after an initial network failure (triggering event). It is found that considering the flow dynamics may imply reduced network robustness compared to previous static overload failure models. This is due to the transient oscillations or overshooting in the loads, when the flow dynamics adjusts to the new (remaining) network structure. We obtain {\em upper} and {\em lower} limits to network robustness, and it is shown that {\it two} time scales $\tau$ and $\tau_0$, defined by the network dynamics, are important to consider prior to accurately addressing network robustness or vulnerability. The robustness of networks showing cascading failures is generally determined by a complex interplay between the network topology and flow dynamics, where the ratio $\chi=\tau/\tau_0$ determines the relative role of the two of them. 	
0811.1301v1	http://arxiv.org/pdf/0811.1301v1	2008	Distributed Algorithms for Computing Alternate Paths Avoiding Failed   Nodes and Links	Amit M. Bhosle|Teofilo F. Gonzalez	  A recent study characterizing failures in computer networks shows that transient single element (node/link) failures are the dominant failures in large communication networks like the Internet. Thus, having the routing paths globally recomputed on a failure does not pay off since the failed element recovers fairly quickly, and the recomputed routing paths need to be discarded. In this paper, we present the first distributed algorithm that computes the alternate paths required by some "proactive recovery schemes" for handling transient failures. Our algorithm computes paths that avoid a failed node, and provides an alternate path to a particular destination from an upstream neighbor of the failed node. With minor modifications, we can have the algorithm compute alternate paths that avoid a failed link as well. To the best of our knowledge all previous algorithms proposed for computing alternate paths are centralized, and need complete information of the network graph as input to the algorithm. 	
0905.1778v1	http://arxiv.org/pdf/0905.1778v1	2009	Encoding of Network Protection Codes Against Link and Node Failures Over   Finite Fields	Salah A. Aly|Ahmed E. Kamal	  Link and node failures are common two fundamental problems that affect operational networks. Hence, protection of communication networks is essential to increase their reliability, performance, and operations. Much research work has been done to protect against link and node failures, and to provide reliable solutions based on pre-defined provision or dynamic restoration of the domain. In this paper we develop network protection strategies against multiple link failures using network coding and joint capacities. In these strategies, the source nodes apply network coding for their transmitted data to provide backup copies for recovery at the receivers' nodes. Such techniques can be applied to optical, IP, and mesh networks. The encoding operations of protection codes are defined over finite fields. Furthermore, the normalized capacity of the communication network is given by $(n-t)/n$ in case of $t$ link failures. In addition, a bound on the minimum required field size is derived. 	
1006.0671v2	http://arxiv.org/pdf/1006.0671v2	2010	Predicting Failures in Power Grids: The Case of Static Overloads	Michael Chertkov|Feng Pan|Mikhail G. Stepanov	  Here we develop an approach to predict power grid weak points, and specifically to efficiently identify the most probable failure modes in static load distribution for a given power network. This approach is applied to two examples: Guam's power system and also the IEEE RTS-96 system, both modeled within the static Direct Current power flow model. Our algorithm is a power network adaption of the worst configuration heuristics, originally developed to study low probability events in physics and failures in error-correction. One finding is that, if the normal operational mode of the grid is sufficiently healthy, the failure modes, also called instantons, are sufficiently sparse, i.e. the failures are caused by load fluctuations at only a few buses. The technique is useful for discovering weak links which are saturated at the instantons. It can also identify generators working at the capacity and generators under capacity, thus providing predictive capability for improving the reliability of any power network. 	
1006.5101v1	http://arxiv.org/pdf/1006.5101v1	2010	Probabilistic Model-Based Safety Analysis	Matthias Güdemann|Frank Ortmeier	  Model-based safety analysis approaches aim at finding critical failure combinations by analysis of models of the whole system (i.e. software, hardware, failure modes and environment). The advantage of these methods compared to traditional approaches is that the analysis of the whole system gives more precise results. Only few model-based approaches have been applied to answer quantitative questions in safety analysis, often limited to analysis of specific failure propagation models, limited types of failure modes or without system dynamics and behavior, as direct quantitative analysis is uses large amounts of computing resources. New achievements in the domain of (probabilistic) model-checking now allow for overcoming this problem.   This paper shows how functional models based on synchronous parallel semantics, which can be used for system design, implementation and qualitative safety analysis, can be directly re-used for (model-based) quantitative safety analysis. Accurate modeling of different types of probabilistic failure occurrence is shown as well as accurate interpretation of the results of the analysis. This allows for reliable and expressive assessment of the safety of a system in early design stages. 	
1106.0489v2	http://arxiv.org/pdf/1106.0489v2	2011	Recovery from Link Failures in Networks with Arbitrary Topology via   Diversity Coding	S. N. Avci|X. Hu|E. Ayanoglu	  Link failures in wide area networks are common. To recover from such failures, a number of methods such as SONET rings, protection cycles, and source rerouting have been investigated. Two important considerations in such approaches are the recovery time and the needed spare capacity to complete the recovery. Usually, these techniques attempt to achieve a recovery time less than 50 ms. In this paper we introduce an approach that provides link failure recovery in a hitless manner, or without any appreciable delay. This is achieved by means of a method called diversity coding. We present an algorithm for the design of an overlay network to achieve recovery from single link failures in arbitrary networks via diversity coding. This algorithm is designed to minimize spare capacity for recovery. We compare the recovery time and spare capacity performance of this algorithm against conventional techniques in terms of recovery time, spare capacity, and a joint metric called Quality of Recovery (QoR). QoR incorporates both the spare capacity percentages and worst case recovery times. Based on these results, we conclude that the proposed technique provides much shorter recovery times while achieving similar extra capacity, or better QoR performance overall. 	
1110.2289v1	http://arxiv.org/pdf/1110.2289v1	2011	Enhancing congestion control to address link failure loss over mobile   ad-hoc network	Mohammad Amin Kheirandish Fard|Sasan Karamizadeh|Mohammad Aflaki	  Standard congestion control cannot detect link failure losses which occur due to mobility and power scarcity in multi-hop Ad-Hoc network (MANET). Moreover, successive executions of Back-off algorithm deficiently grow Retransmission Timeout (RTO) exponentially for new route. The importance of detecting and responding link failure losses is to prevent sender from remaining idle unnecessarily and manage number of packet retransmission overhead. In contrast to Cross-layer approaches which require feedback information from lower layers, this paper operates purely in Transport layer. This paper explores an end-to-end threshold-based algorithm which enhances congestion control to address link failure loss in MANET. It consists of two phases. First, threshold-based loss classification algorithm distinguishes losses due to link failure by estimating queue usage based on Relative One-way Trip Time (ROTT). Second phase adjusts RTO for new route by comparing capabilities of new route to the broken route using available information in Transport layer such as ROTT and number of hops. 	
1203.0415v1	http://arxiv.org/pdf/1203.0415v1	2012	On Compositional Reasoning for Guaranteeing Probabilistic Properties	Jan Olaf Blech	  We present a framework to formally describe probabilistic system behavior and symbolically reason about it. In particular we aim at reasoning about possible failures and fault tolerance. We regard systems which are composed of different units: sensors, computational parts and actuators. Considering worst-case failure behavior of system components, our framework is most suited to derive reliability guarantees for composed systems. The behavior of system components is modeled using monad like constructs that serve as an abstract representation for system behavior. We introduce rules to reason about these representations and derive results like guaranteed upper bounds for system failure. Our approach is characterized by the fact that we do not just map a certain component to a failure probability, but regard distributions of error behavior and their evolvement over system runs. This serves as basis for deriving probabilities of events, in particular failure probabilities. The work presented in this paper slightly extends a complete framework and a case study which has been previously published. One focus of this report is a more detailed explanation of definitions and a more comprehensive description of examples. 	
1203.4324v3	http://arxiv.org/pdf/1203.4324v3	2012	Distributed Consensus Resilient to Both Crash Failures and Strategic   Manipulations	Xiaohui Bei|Wei Chen|Jialin Zhang	  In this paper, we study distributed consensus in synchronous systems subject to both unexpected crash failures and strategic manipulations by rational agents in the system. We adapt the concept of collusion-resistant Nash equilibrium to model protocols that are resilient to both crash failures and strategic manipulations of a group of colluding agents. For a system with $n$ distributed agents, we design a deterministic protocol that tolerates 2 colluding agents and a randomized protocol that tolerates $n - 1$ colluding agents, and both tolerate any number of failures. We also show that if colluders are allowed an extra communication round after each synchronous round, there is no protocol that can tolerate even 2 colluding agents and 1 crash failure. 	
1204.2465v1	http://arxiv.org/pdf/1204.2465v1	2012	Fast emergency paths schema to overcome transient link failures in ospf   routing	Fernando Barreto|Emilio C. G. Wille|Luiz Nacamura Jr	  A reliable network infrastructure must be able to sustain traffic flows, even when a failure occurs and changes the network topology. During the occurrence of a failure, routing protocols, like OSPF, take from hundreds of milliseconds to various seconds in order to converge. During this convergence period, packets might traverse a longer path or even a loop. An even worse transient behaviour is that packets are dropped even though destinations are reachable. In this context, this paper describes a proactive fast rerouting approach, named Fast Emergency Paths Schema (FEP-S), to overcome problems originating from transient link failures in OSPF routing. Extensive experiments were done using several network topologies with different dimensionality degrees. Results show that the recovery paths, obtained by FEPS, are shorter than those from other rerouting approaches and can improve the network reliability by reducing the packet loss rate during the routing protocols convergence caused by a failure. 	
1209.6484v1	http://arxiv.org/pdf/1209.6484v1	2012	Vulnerability Management for an Enterprise Resource Planning System	Shivani Goel|Ravi Kiran|Deepak Garg	  Enterprise resource planning (ERP) systems are commonly used in technical educational institutions(TEIs). ERP systems should continue providing services to its users irrespective of the level of failure. There could be many types of failures in the ERP systems. There are different types of measures or characteristics that can be defined for ERP systems to handle the levels of failure. Here in this paper, various types of failure levels are identified along with various characteristics which are concerned with those failures. The relation between all these is summarized. The disruptions causing vulnerabilities in TEIs are identified .A vulnerability management cycle has been suggested along with many commercial and open source vulnerability management tools. The paper also highlights the importance of resiliency in ERP systems in TEIs. 	
1211.3792v1	http://arxiv.org/pdf/1211.3792v1	2012	Modeling Repairs of Systems with a Bathtub-Shaped Failure Rate Function	Sima Varnosafaderani|Stefanka Chukova	  Most of the reliability literature on modeling the effect of repairs on systems assumes the failure rate functions are monotonically increasing. For systems with non-monotonic failure rate functions, most models deal with minimal repairs (which do not affect the working condition of the system) or replacements (which return the working condition to that of a new and identical system). We explore a new approach to model repairs of a system with a non-monotonic failure rate function; in particular, we consider systems with a bathtub-shaped failure rate function. We propose a repair model specified in terms of modifications to the virtual age function of the system, while preserving the usual definitions of the types of repair (minimal, imperfect and perfect repairs) and distinguishing between perfect repair and replacement. In addition, we provide a numerical illustration of the proposed repair model. 	
1301.2055v4	http://arxiv.org/pdf/1301.2055v4	2013	A Cascading Failure Model by Quantifying Interactions	Junjian Qi|Shengwei Mei	  Cascading failures triggered by trivial initial events are encountered in many complex systems. It is the interaction and coupling between components of the system that causes cascading failures. We propose a simple model to simulate cascading failure by using the matrix that determines how components interact with each other. A careful comparison is made between the original cascades and the simulated cascades by the proposed model. It is seen that the model can capture general features of the original cascades, suggesting that the interaction matrix can well reflect the relationship between components. An index is also defined to identify important links and the distribution follows an obvious power law. By eliminating a small number of most important links the risk of cascading failures can be significantly mitigated, which is dramatically different from getting rid of the same number of links randomly. 	
1302.3344v2	http://arxiv.org/pdf/1302.3344v2	2013	CORE: Augmenting Regenerating-Coding-Based Recovery for Single and   Concurrent Failures in Distributed Storage Systems	Runhui Li|Jian Lin|Patrick P. C. Lee	  Data availability is critical in distributed storage systems, especially when node failures are prevalent in real life. A key requirement is to minimize the amount of data transferred among nodes when recovering the lost or unavailable data of failed nodes. This paper explores recovery solutions based on regenerating codes, which are shown to provide fault-tolerant storage and minimum recovery bandwidth. Existing optimal regenerating codes are designed for single node failures. We build a system called CORE, which augments existing optimal regenerating codes to support a general number of failures including single and concurrent failures. We theoretically show that CORE achieves the minimum possible recovery bandwidth for most cases. We implement CORE and evaluate our prototype atop a Hadoop HDFS cluster testbed with up to 20 storage nodes. We demonstrate that our CORE prototype conforms to our theoretical findings and achieves recovery bandwidth saving when compared to the conventional recovery approach based on erasure codes. 	
1308.5474v2	http://arxiv.org/pdf/1308.5474v2	2013	Changes in Cascading Failure Risk with Generator Dispatch Method and   System Load Level	Pooya Rezaei|Paul D. H. Hines	  Industry reliability rules increasingly require utilities to study and mitigate cascading failure risk in their system. Motivated by this, this paper describes how cascading failure risk, in terms of expected blackout size, varies with power system load level and pre-contingency dispatch. We used Monte Carlo sampling of random branch outages to generate contingencies, and a model of cascading failure to estimate blackout sizes. The risk associated with different blackout sizes was separately estimated in order to separate small, medium, and large blackout risk. Results from $N-1$ secure models of the IEEE RTS case and a 2383 bus case indicate that blackout risk does not always increase with load level monotonically, particularly for large blackout risk. The results also show that risk is highly dependent on the method used for generator dispatch. Minimum cost methods of dispatch can result in larger long distance power transfers, which can increase cascading failure risk. 	
1312.4976v1	http://arxiv.org/pdf/1312.4976v1	2013	Analysis of Asteroid (216) Kleopatra using dynamical and structural   constraints	Masatoshi Hirabayashi|Daniel J. Scheeres	  Given the spin state by Magnusson (1990), the shape model by Ostro et al. (2000), and the mass by Descamps et al. (2011), this paper evaluates a dynamically and structurally stable size of Asteroid (216) Kleopatra. In particular, we investigate two different failure modes: material shedding from the surface and structural failure of the internal body. We construct zero-velocity curves in the vicinity of this asteroid to determine surface shedding, while we utilize a limit analysis to calculate the lower and upper bounds of structural failure under the zero-cohesion assumption. Surface shedding does not occur at the current spin period (5.385 hr) and cannot directly initiate the formation of the satellites. On the other hand, this body may be close to structural failure; in particular, the neck may be situated near a plastic state. In addition, the neck's sensitivity to structural failure changes as the body size varies. We conclude that plastic deformation has probably occurred around the neck part in the past. If the true size of this body is established through additional measurements, this method will provide strong constraints on the current friction angle for the body. 	
1405.0455v1	http://arxiv.org/pdf/1405.0455v1	2014	Epidemic and Cascading Survivability of Complex Networks	Marc Manzano|Eusebi Calle|Jordi Ripoll|Anna Manolova Fagertun|Victor Torres-Padrosa|Sakshi Pahwa|Caterina Scoglio	  Our society nowadays is governed by complex networks, examples being the power grids, telecommunication networks, biological networks, and social networks. It has become of paramount importance to understand and characterize the dynamic events (e.g. failures) that might happen in these complex networks. For this reason, in this paper, we propose two measures to evaluate the vulnerability of complex networks in two different dynamic multiple failure scenarios: epidemic-like and cascading failures. Firstly, we present \emph{epidemic survivability} ($ES$), a new network measure that describes the vulnerability of each node of a network under a specific epidemic intensity. Secondly, we propose \emph{cascading survivability} ($CS$), which characterizes how potentially injurious a node is according to a cascading failure scenario. Then, we show that by using the distribution of values obtained from $ES$ and $CS$ it is possible to describe the vulnerability of a given network. We consider a set of 17 different complex networks to illustrate the suitability of our proposals. Lastly, results reveal that distinct types of complex networks might react differently under the same multiple failure scenario. 	
1405.4342v1	http://arxiv.org/pdf/1405.4342v1	2014	The robust-yet-fragile nature of interdependent networks	Fei Tan|Yongxiang Xia	  Interdependent networks have been shown to be extremely vulnerable based on the percolation model. Parshani et. al further indicated that the more inter-similar networks are, the more robust they are to random failure. Our understanding of how coupling patterns shape and impact the cascading failures of loads in interdependent networks is limited, but is essential for the design and optimization of the real-world interdependent networked systems. This question, however, is largely unexplored. In this paper, we address this question by investigating the robustness of interdependent ER random graphs and BA scale-free networks under both random failure and intentional attack. It is found that interdependent ER random graphs are robust-yet-fragile under both random failures and intentional attack. Interdependent BA scale-free networks, however, are only robust-yet-fragile under random failure but fragile under intentional attack. These results advance our understanding of the robustness of interdependent networks significantly. 	
1406.3266v1	http://arxiv.org/pdf/1406.3266v1	2014	Event and Anomaly Detection Using Tucker3 Decomposition	Hadi Fanaee-T|Márcia D. B. Oliveira|João Gama|Simon Malinowski|Ricardo Morla	  Failure detection in telecommunication networks is a vital task. So far, several supervised and unsupervised solutions have been provided for discovering failures in such networks. Among them unsupervised approaches has attracted more attention since no label data is required. Often, network devices are not able to provide information about the type of failure. In such cases the type of failure is not known in advance and the unsupervised setting is more appropriate for diagnosis. Among unsupervised approaches, Principal Component Analysis (PCA) is a well-known solution which has been widely used in the anomaly detection literature and can be applied to matrix data (e.g. Users-Features). However, one of the important properties of network data is their temporal sequential nature. So considering the interaction of dimensions over a third dimension, such as time, may provide us better insights into the nature of network failures. In this paper we demonstrate the power of three-way analysis to detect events and anomalies in time-evolving network data. 	
1406.4261v1	http://arxiv.org/pdf/1406.4261v1	2014	Failure Inference and Optimization for Step Stress Model Based on   Bivariate Wiener Model	S. Shemehsavar|Morteza Amini	  In this paper, we consider the situation under a life test, in which the failure time of the test units are not related deterministically to an observable stochastic time varying covariate. In such a case, the joint distribution of failure time and a marker value would be useful for modeling the step stress life test. The problem of accelerating such an experiment is considered as the main aim of this paper. We present a step stress accelerated model based on a bivariate Wiener process with one component as the latent (unobservable) degradation process, which determines the failure times and the other as a marker process, the degradation values of which are recorded at times of failure. Parametric inference based on the proposed model is discussed and the optimization procedure for obtaining the optimal time for changing the stress level is presented. The optimization criterion is to minimize the approximate variance of the maximum likelihood estimator of a percentile of the products' lifetime distribution. 	
1406.7264v1	http://arxiv.org/pdf/1406.7264v1	2014	Repairable Block Failure Resilient Codes	Gokhan Calis|O. Ozan Koyluoglu	  In large scale distributed storage systems (DSS) deployed in cloud computing, correlated failures resulting in simultaneous failure (or, unavailability) of blocks of nodes are common. In such scenarios, the stored data or a content of a failed node can only be reconstructed from the available live nodes belonging to available blocks. To analyze the resilience of the system against such block failures, this work introduces the framework of Block Failure Resilient (BFR) codes, wherein the data (e.g., file in DSS) can be decoded by reading out from a same number of codeword symbols (nodes) from each available blocks of the underlying codeword. Further, repairable BFR codes are introduced, wherein any codeword symbol in a failed block can be repaired by contacting to remaining blocks in the system. Motivated from regenerating codes, file size bounds for repairable BFR codes are derived, trade-off between per node storage and repair bandwidth is analyzed, and BFR-MSR and BFR-MBR points are derived. Explicit codes achieving these two operating points for a wide set of parameters are constructed by utilizing combinatorial designs, wherein the codewords of the underlying outer codes are distributed to BFR codeword symbols according to projective planes. 	
1407.4953v1	http://arxiv.org/pdf/1407.4953v1	2014	A Topological Investigation of Phase Transitions of Cascading Failures   in Power Grids	Yakup Koç|Martijn Warnier|Piet Van Mieghem|Robert E. Kooij|Frances M. T. Brazier	  Cascading failures are one of the main reasons for blackouts in electric power transmission grids. The economic cost of such failures is in the order of tens of billion dollars annually. The loading level of power system is a key aspect to determine the amount of the damage caused by cascading failures. Existing studies show that the blackout size exhibits phase transitions as the loading level increases. This paper investigates the impact of the topology of a power grid on phase transitions in its robustness. Three spectral graph metrics are considered: spectral radius, effective graph resistance and algebraic connectivity. Experimental results from a model of cascading failures in power grids on the IEEE power systems demonstrate the applicability of these metrics to design/optimize a power grid topology for an enhanced phase transition behavior of the system. 	
1409.0025v2	http://arxiv.org/pdf/1409.0025v2	2014	Geometric control of failure behavior in perforated sheets	Michelle M. Driscoll	  Adding perforations to a continuum sheet allows new modes of deformation, and thus modifies its elastic behavior. The failure behavior of such a perforated sheet is explored, using a model experimental system: a material containing a one-dimensional array of rectangular holes. In this model system, a transition in failure mode occurs as the spacing and aspect ratio of the holes are varied: rapid failure via a running crack is completely replaced by quasi-static failure which proceeds via the breaking of struts at random positions in the array of holes. I demonstrate that this transition can be connected to the loss of stress enhancement which occurs as the material geometry is modified. 	
1410.6945v1	http://arxiv.org/pdf/1410.6945v1	2014	What The Trace Distance Security Criterion in Quantum Key Distribution   Does And Does Not Guarantee	Horace Yuen	  Cryptographic security of quantum key distribution is currently based on a trace distance criterion. The widespread misinterpretation of the criterion as failure probability and also its actual scope have been discussed previously. Recently its distinguishability advantage interpretation is re-emphasized as an operational guarantee, and the failure probability misinterpretation is maintained with a further failure probability per bit interpretation. In this paper we explain the basic perpetuating error as a confusion on the correspondence between mathematics and reality. We note that the assignment of equal a priori probability of 1/2 to the real and ideal situations for distinguishability advantage would not lead to operational guarantee. We explain why operational guarantee in terms of Eve's probabilities of getting various key bits is necessary for security, and why the failure probability interpretation misrepresents the security situation. The scope and limits of the trace distance guarantee are summarized. It is shown that there would have been no security problem to begin with if the failure probability per bit interpretation validity. 	
1508.03821v1	http://arxiv.org/pdf/1508.03821v1	2015	Vertical modeling: analysis of competing risks data with a cure   proportion	M. A. Nicolaie|J. M. G. Taylor|C. Legrand	  In this paper, we extend the vertical modeling approach for the analysis of survival data with competing risks to incorporate a cured fraction in the population, that is, a proportion of the population for which none of the competing events can occur. The proposed method has three components: the proportion of cure, the risk of failure, irrespective of the cause, and the relative risk of a certain cause of failure, given a failure occurred. Covariates may affect each of these components. An appealing aspect of the method is that it is a natural extension to competing risks of the semi-parametric mixture cure model in ordinary survival analysis; thus, causes of failure are assigned only if a failure occurs. This contrasts with the existing mixture cure model for competing risks of Larson and Dinse, which conditions at the onset on the future status presumably attained. Regression parameter estimates are obtained using an EM-algorithm. The performance of the estimators is evaluated in a simulation study. The method is illustrated using a melanoma cancer data set. 	
1509.04613v1	http://arxiv.org/pdf/1509.04613v1	2015	Gaussian process surrogates for failure detection: a Bayesian   experimental design approach	Hongqiao Wang|Guang Lin|Jinglai Li	  An important task of uncertainty quantification is to identify {the probability of} undesired events, in particular, system failures, caused by various sources of uncertainties. In this work we consider the construction of Gaussian {process} surrogates for failure detection and failure probability estimation. In particular, we consider the situation that the underlying computer models are extremely expensive, and in this setting, determining the sampling points in the state space is of essential importance. We formulate the problem as an optimal experimental design for Bayesian inferences of the limit state (i.e., the failure boundary) and propose an efficient numerical scheme to solve the resulting optimization problem. In particular, the proposed limit-state inference method is capable of determining multiple sampling points at a time, and thus it is well suited for problems where multiple computer simulations can be performed in parallel. The accuracy and performance of the proposed method is demonstrated by both academic and practical examples. 	
1603.04335v1	http://arxiv.org/pdf/1603.04335v1	2016	The landscape of software failure cause models	Lena Feinbube|Peter Tröger|Andreas Polze	  The software engineering field has a long history of classifying software failure causes. Understanding them is paramount for fault injection, focusing testing efforts or reliability prediction. Since software fails in manifold complex ways, a broad range of software failure cause models is meanwhile published in dependability literature. We present the results of a meta-study that classifies publications containing a software failure cause model in topic clusters. Our results structure the research field and can help to identify gaps. We applied the systematic mapping methodology for performing a repeatable analysis.   We identified 156 papers presenting a model of software failure causes. Their examination confirms the assumption that a large number of the publications discusses source code defects only. Models of fault-activating state conditions and error states are rare. Research seems to be driven mainly by the need for better testing methods and code-based quality improvement. Other motivations such as online error detection are less frequently given. Mostly, the IEEE definitions or orthogonal defect classification is used as base terminology. The majority of use cases comes from web, safety- and security-critical applications. 	
1606.00955v2	http://arxiv.org/pdf/1606.00955v2	2016	Message Passing for Analysis and Resilient Design of Self-Healing   Interdependent Cyber-Physical Networks	Ali Behfarnia|Ali Eslami	  Coupling cyber and physical systems gives rise to numerous engineering challenges and opportunities. An important challenge is the contagion of failure from one system to another, that can lead to large scale cascading failures. On the other hand, self-healing ability emerges as a valuable opportunity where the overlay cyber network can cure failures in the underlying physical network. To capture both self-healing and contagion, we introduce a factor graph representation of inter-dependent cyber-physical systems in which factor nodes represent various node functionalities and the edges capture the interactions between the nodes. We develop a message passing algorithm to study the dynamics of failure propagation and healing in this representation. Through applying a fixed-point analysis to this algorithm, we investigate the network reaction to initial disruptions. Our analysis provides simple yet critical guidelines for choosing network parameters to achieve resiliency against cascading failures. 	
1608.01037v1	http://arxiv.org/pdf/1608.01037v1	2016	The robustness of interdependent networks under the interplay between   cascading failures and virus propagation	Dawei Zhao|Zhen Wang|Gaoxi Xiao|Bo Gao|Lianhai Wang	  Cascading failures and epidemic dynamics, as two successful application realms of network science, are usually investigated separately. How do they affect each other is still one open, interesting problem. In this letter, we couple both processes and put them into the framework of interdependent networks, where each network only supports one dynamical process. Of particular interest, they spontaneously form a feedback loop: virus propagation triggers cascading failures of systems while cascading failures suppress virus propagation. Especially, there exists crucial threshold of virus transmissibility, above which the interdependent networks collapse completely. In addition, the interdependent networks will be more vulnerable if the network supporting virus propagation has denser connections; otherwise the interdependent systems are robust against the change of connections in other layer(s). This discovery differs from previous framework of cascading failure in interdependent networks, where better robustness usually needs denser connections. Finally, to protect interdependent networks we also propose the control measures based on the identification capability. The larger this capability, more robustness the interdependent networks will be. 	
1608.06451v1	http://arxiv.org/pdf/1608.06451v1	2016	Failure Detection for Facial Landmark Detectors	Andreas Steger|Radu Timofte|Luc Van Gool	  Most face applications depend heavily on the accuracy of the face and facial landmarks detectors employed. Prediction of attributes such as gender, age, and identity usually completely fail when the faces are badly aligned due to inaccurate facial landmark detection. Despite the impressive recent advances in face and facial landmark detection, little study is on the recovery from and detection of failures or inaccurate predictions. In this work we study two top recent facial landmark detectors and devise confidence models for their outputs. We validate our failure detection approaches on standard benchmarks (AFLW, HELEN) and correctly identify more than 40% of the failures in the outputs of the landmark detectors. Moreover, with our failure detection we can achieve a 12% error reduction on a gender estimation application at the cost of a small increase in computation. 	
1609.01580v1	http://arxiv.org/pdf/1609.01580v1	2016	Using Natural Language Processing to Screen Patients with Active Heart   Failure: An Exploration for Hospital-wide Surveillance	Shu Dong|R Kannan Mutharasan|Siddhartha Jonnalagadda	  In this paper, we proposed two different approaches, a rule-based approach and a machine-learning based approach, to identify active heart failure cases automatically by analyzing electronic health records (EHR). For the rule-based approach, we extracted cardiovascular data elements from clinical notes and matched patients to different colors according their heart failure condition by using rules provided by experts in heart failure. It achieved 69.4% accuracy and 0.729 F1-Score. For the machine learning approach, with bigram of clinical notes as features, we tried four different models while SVM with linear kernel achieved the best performance with 87.5% accuracy and 0.86 F1-Score. Also, from the classification comparison between the four different models, we believe that linear models fit better for this problem. Once we combine the machine-learning and rule-based algorithms, we will enable hospital-wide surveillance of active heart failure through increased accuracy and interpretability of the outputs. 	
1609.06187v2	http://arxiv.org/pdf/1609.06187v2	2017	Determination of Bond Wire Failure Probabilities in Microelectronic   Packages	Thorben Casper|Ulrich Römer|Sebastian Schöps	  This work deals with the computation of industry-relevant bond wire failure probabilities in microelectronic packages. Under operating conditions, a package is subject to Joule heating that can lead to electrothermally induced failures. Manufacturing tolerances result, e.g., in uncertain bond wire geometries that often induce very small failure probabilities requiring a high number of Monte Carlo (MC) samples to be computed. Therefore, a hybrid MC sampling scheme that combines the use of an expensive computer model with a cheap surrogate is used. The fraction of surrogate evaluations is maximized using an iterative procedure, yielding accurate results at reduced cost. Moreover, the scheme is non-intrusive, i.e., existing code can be reused. The algorithm is used to compute the failure probability for an example package and the computational savings are assessed by performing a surrogate efficiency study. 	
1612.07002v1	http://arxiv.org/pdf/1612.07002v1	2016	A subset multicanonical Monte Carlo method for simulating rare failure   events	Xinjuan Chen|Jinglai Li	  Estimating failure probabilities of engineering systems is an important problem in many engineering fields. In this work we consider such problems where the failure probability is extremely small (e.g $\leq10^{-10}$). In this case, standard Monte Carlo methods are not feasible due to the extraordinarily large number of samples required. To address these problems, we propose an algorithm that combines the main ideas of two very powerful failure probability estimation approaches: the subset simulation (SS) and the multicanonical Monte Carlo (MMC) methods. Unlike the standard MMC which samples in the entire domain of the input parameter in each iteration, the proposed subset MMC algorithm adaptively performs MMC simulations in a subset of the state space and thus improves the sampling efficiency. With numerical examples we demonstrate that the proposed method is significantly more efficient than both of the SS and the MMC methods. Moreover, the proposed algorithm can reconstruct the complete distribution function of the parameter of interest and thus can provide more information than just the failure probabilities of the systems. 	
1701.08787v1	http://arxiv.org/pdf/1701.08787v1	2017	Vulnerability of Clustering under Node Failure in Complex Networks	Alan Kuhnle|Nam P. Nguyen|Thang N. Dinh|My T. Thai	  Robustness in response to unexpected events is always desirable for real-world networks. To improve the robustness of any networked system, it is important to analyze vulnerability to external perturbation such as random failures or adversarial attacks occurring to elements of the network. In this paper, we study an emerging problem in assessing the robustness of complex networks: the vulnerability of the clustering of the network to the failure of network elements. Specifically, we identify vertices whose failures will critically damage the network by degrading its clustering, evaluated through the average clustering coefficient. This problem is important because any significant change made to the clustering, resulting from element-wise failures, could degrade network performance such as the ability for information to propagate in a social network. We formulate this vulnerability analysis as an optimization problem, prove its NP-completeness and non-monotonicity, and we offer two algorithms to identify the vertices most important to clustering. Finally, we conduct comprehensive experiments in synthesized social networks generated by various well-known models as well as traces of real social networks. The empirical results over other competitive strategies show the efficacy of our proposed algorithms. 	
1704.06302v1	http://arxiv.org/pdf/1704.06302v1	2017	Quality of Service of an Asynchronous Crash-Recovery Leader Election   Algorithm	Vinícius A. Reis|Gustavo M. D. Vieira	  In asynchronous distributed systems it is very hard to assess if one of the processes taking part in a computation is operating correctly or has failed. To overcome this problem, distributed algorithms are created using unreliable failure detectors that capture in an abstract way timing assumptions necessary to assess the operating status of a process. One particular type of failure detector is a leader election, that indicates a single process that has not failed. The unreliability of these failure detectors means that they can make mistakes, however if they are to be used in practice there must be limits to the eventual behavior of these detectors. These limits are defined as the quality of service (QoS) provided by the detector. Many works have tackled the problem of creating failure detectors with predictable QoS, but only for crash-stop processes and synchronous systems. This paper presents and analyzes the behavior of a new leader election algorithm named NFD-L for the asynchronous crash-recovery failure model that is efficient in terms of its use of stable memory and message exchanges. 	
1705.05776v1	http://arxiv.org/pdf/1705.05776v1	2017	Shape optimization to decrease failure probability	Matthias Bolten|Hanno Gottschalk|Camilla Hahn|Mohamed Saadi	  Ceramic is a material frequently used in industry because of its favorable properties. Common approaches in shape optimization for ceramic structures aim to minimize the tensile stress acting on the component, as it is the main driver for failure. In contrast to this, we follow a more natural approach by minimizing the component's probability of failure under a given tensile load. Since the fundamental work of Weibull, the probabilistic description of the strength of ceramics is standard and has been widely applied. Here, for the first time, the resulting failure probabilities are used as objective functions in PDE constrained shape optimization.   To minimize the probability of failure, we choose a gradient based method combined with a first discretize then optimize approach. For discretization finite elements are used. Using the Lagrangian formalism, the shape gradient via the adjoint equation is calculated at low computational cost. The implementation is verified by comparison of it with a finite difference method applied to a minimal 2d example. Furthermore, we construct shape flows towards an optimal / improved shape in the case of a simple beam and a bended joint. 	
1709.06537v1	http://arxiv.org/pdf/1709.06537v1	2017	DC-Prophet: Predicting Catastrophic Machine Failures in DataCenters	You-Luen Lee|Da-Cheng Juan|Xuan-An Tseng|Yu-Ting Chen|Shih-Chieh Chang	  When will a server fail catastrophically in an industrial datacenter? Is it possible to forecast these failures so preventive actions can be taken to increase the reliability of a datacenter? To answer these questions, we have studied what are probably the largest, publicly available datacenter traces, containing more than 104 million events from 12,500 machines. Among these samples, we observe and categorize three types of machine failures, all of which are catastrophic and may lead to information loss, or even worse, reliability degradation of a datacenter. We further propose a two-stage framework-DC-Prophet-based on One-Class Support Vector Machine and Random Forest. DC-Prophet extracts surprising patterns and accurately predicts the next failure of a machine. Experimental results show that DC-Prophet achieves an AUC of 0.93 in predicting the next machine failure, and a F3-score of 0.88 (out of 1). On average, DC-Prophet outperforms other classical machine learning methods by 39.45% in F3-score. 	
1710.08500v1	http://arxiv.org/pdf/1710.08500v1	2017	Are Multiagent Systems Resilient to Communication Failures?	Philip N. Brown|Holly P. Borowski|Jason R. Marden	  A challenge in multiagent control systems is to ensure that they are appropriately resilient to communication failures between the various agents. In many common game-theoretic formulations of these types of systems, it is implicitly assumed that all agents have access to as much information about other agents' actions as needed. This paper endeavors to augment these game-theoretic methods with policies that would allow agents to react on-the-fly to losses of this information. Unfortunately, we show that even if a single agent loses communication with one other weakly-coupled agent, this can cause arbitrarily-bad system states to emerge as various solution concepts of an associated game, regardless of how the agent accounts for the communication failure and regardless of how weakly coupled the agents are. Nonetheless, we show that the harm that communication failures can cause is limited by the structure of the problem; when agents' action spaces are richer, problems are more susceptible to these types of pathologies. Finally, we undertake an initial study into how a system designer might prevent these pathologies, and explore a few limited settings in which communication failures cannot cause harm. 	
1712.04053v1	http://arxiv.org/pdf/1712.04053v1	2017	Cascading Failures as Continuous Phase-Space Transitions	Yang Yang|Adilson E. Motter	  In network systems, a local perturbation can amplify as it propagates, potentially leading to a large-scale cascading failure. Here we derive a continuous model to advance our understanding of cascading failures in power-grid networks. The model accounts for both the failure of transmission lines and the desynchronization of power generators, and incorporates the transient dynamics between successive steps of the cascade. In this framework, we show that a cascade event is a phase-space transition from an equilibrium state with high energy to an equilibrium state with lower energy, which can be suitably described in closed form using a global Hamiltonian-like function. From this function we show that a perturbed system cannot always reach the equilibrium state predicted by quasi-steady-state cascade models, which would correspond to a reduced number of failures, and may instead undergo a larger cascade. We also show that in the presence of two or more perturbations, the outcome depends strongly on the order and timing of the individual perturbations. These results offer new insights into the current understanding of cascading dynamics, with potential implications for control interventions. 	
0303593v1	http://arxiv.org/pdf/astro-ph/0303593v1	2003	Numerical Modeling of Gamma Radiation from Galaxy Clusters	Francesco Miniati	  We investigate the spatial and spectral properties of non-thermal emission from clusters of galaxies at gamma-ray energies between 10 keV and 10 TeV due to inverse-Compton (IC) emission, pion-decay and non-thermal bremsstrahlung (NTB) from cosmic-ray(CR) ions and electrons accelerated at cosmic shock and secondary e+- from inelastic p-p collisions. We identify two main emission region, namely the core (also bright in thermal X-ray) and the outskirts region where accretion shocks occur. IC emission from shock accelerated CR electrons dominate the emission in the outer regions of galaxy clusters, provided that at least a fraction of a percent of the shock ram pressure is converted into CR electrons. A clear detection of this component and of its spatial distribution will allow us direct probing of cosmic accretion shocks. In the cluster core, gamma-ray emission above 100 MeV is dominated by pion-decay mechanism and, at lower energies, by IC emission from secondary e+-. However, IC emission from shock accelerated electrons projected onto the cluster core will not be negligible. We emphasize the importance of separating these emission components for a correct interpretation of the experimental data and outline a strategy for that purpose. Failure in addressing this issue will produce unsound estimates of the intra-cluster magnetic field strength and CR ion content. According to our estimate future space borne and ground based gamma-ray facilities should be able to measure the whole nonthermal spectrum both in the cluster core and at its outskirts. The importance of such measurements in advancing our understanding of non-thermal processes in the intra-cluster medium is discussed. 	
0407284v1	http://arxiv.org/pdf/astro-ph/0407284v1	2004	The Chandra view of NGC1800 and the X-ray scaling properties of dwarf   starbursts	Jesper Rasmussen|Ian R. Stevens|Trevor J. Ponman	  The superb spatial resolution of Chandra is utilized to study the X-ray morphology of the dwarf starburst galaxy NGC1800 embedded in a small group of galaxies. Diffuse galactic emission is detected, extending several kpc above the galactic plane, with an overall morphology similar to the galactic winds seen in nearby X-ray bright starburst galaxies. This makes NGC1800 the most distant dwarf starburst with a clear detection of diffuse X-ray emission. The diffuse X-ray luminosity of 1.3+/-0.3 *10^38 erg/s accounts for at least 60 per cent of the total soft X-ray output of the galaxy. A hot gas temperature of kT=0.25 keV and metallicity Z~0.05Z_Sun are derived, the latter in consistency with results from optical spectroscopy of the interstellar medium. Our failure to detect any hot gas associated with the embedding galaxy group translates into an upper limit to the group X-ray luminosity of L_X<10^41 erg/s. There is no convincing evidence that the outflowing wind of NGC1800 is currently interacting with any intragroup gas, and mechanical considerations indicate that the wind can escape the galaxy and its surrounding HI halo, eventually delivering energy and metals to the intragroup gas. Properties of NGC1800 are compared to those of other dwarf starburst galaxies, and a first detailed discussion of the X-ray scaling properties of this population of objects is given, set against the equivalent results obtained for normal starburst galaxies. Results indicate that dwarf starbursts to a large degree behave as down-scaled versions of normal starburst galaxies. 	
9803204v1	http://arxiv.org/pdf/cond-mat/9803204v1	1998	An Observational Test of the Critical Earthquake Concept	D. D. Bowman|G. Ouillon|C. G. Sammis|A. Sornette|D. Sornette	  We test the concept that seismicity prior to a large earthquake can be understood in terms of the statistical physics of a critical phase transition. In this model, the cumulative seismic strain release increases as a power-law time-to-failure before the final event. Furthermore, the region of correlated seismicity predicted by this model is much greater than would be predicted from simple elasto-dynamic interactions. We present a systematic procedure to test for the accelerating seismicity predicted by the critical point model and to identify the region approaching criticality, based on a comparison between the observed cumulative energy (Benioff strain) release and the power-law behavior predicted by theory. This method is used to find the critical region before all earthquakes along the San Andreas system since 1950 with M 6.5. The statistical significance of our results is assessed by performing the same procedure on a large number of randomly generated synthetic catalogs. The null hypothesis, that the observed acceleration in all these earthquakes could result from spurious patterns generated by our procedure in purely random catalogs, is rejected with 99.5% confidence. An empirical relation between the logarithm of the critical region radius (R) and the magnitude of the final event (M) is found, such that log R \mu 0.5 M, suggesting that the largest probable event in a given region scales with the size of the regional fault network. 	
9906277v2	http://arxiv.org/pdf/cond-mat/9906277v2	1999	The fraction of condensed counterions around a charged rod: Comparison   of Poisson-Boltzmann theory and computer simulations	Markus Deserno|Christian Holm|Sylvio May	  We investigate the phenomenon of counterion condensation in a solution of highly charged rigid polyelectrolytes within the cell model. A method is proposed which -- based on the charge distribution function -- identifies both the fraction of condensed ions and the radial extension of the condensed layer. Within salt-free Poisson-Boltzmann (PB) theory it reproduces the well known fraction 1-1/xi of condensed ions for a Manning parameter xi>1. Furthermore, it predicts a weak salt dependence of this fraction and a breakdown of the concept of counterion condensation in the high salt limit. We complement our theoretical investigations with molecular dynamics simulations of a cell-like model, which constantly yield a stronger condensation than predicted by PB theory. While the agreement between theory and simulation is excellent in the monovalent, weakly charged case, it deteriorates with increasing electrostatic interaction strength and, in particular, increasing valence. For instance, at a high concentration of divalent salt and large xi our computer simulations predict charge oscillations, which mean-field theory is unable to reproduce. 	
0105322v2	http://arxiv.org/pdf/cond-mat/0105322v2	2001	Velocity Distributions and Correlations in Homogeneously Heated Granular   Media	Sung Joon Moon|M. D. Shattuck|J. B. Swift	  We compare the steady state velocity distributions from our three-dimensional inelastic hard sphere molecular dynamics simulation for homogeneously heated granular media, with the predictions of a mean field-type Enskog-Boltzmann equation for inelastic hard spheres [van Noije & Ernst, Gran. Matt. {\bf 1}, 57 (1998)]. Although we find qualitative agreement for all values of density and inelasticity, the quantitative disagreement approaches $\sim 40%$ at high inelasticity or density. By contrast the predictions of the pseudo-Maxwell molecule model [Carrillo, Cercignani & Gamba, Phys. Rev. E, {\bf 62}, 7700 (2000)] are both qualitatively and quantitatively different from those of our simulation. We also measure short-range and long-range velocity correlations exhibiting non-zero correlations at contact before the collision, and being consistent with a slow algebraic decay over a decade in the unit of the diameter of the particle, proportional to $r^{-(1+\alpha)}$, where $0.2 < \alpha < 0.3$. The existence of these correlations imply the failure of the molecular chaos assumption and the mean field approximation, which is responsible for the quantitative disagreement of the inelastic hard sphere kinetic theory. 	
0306224v1	http://arxiv.org/pdf/cond-mat/0306224v1	2003	The Error and Repair Catastrophes: A Two-Dimensional Phase Diagram in   the Quasispecies Model	Emmanuel Tannenbaum|Eugene I. Shakhnovich	  This paper develops a two gene, single fitness peak model for determining the equilibrium distribution of genotypes in a unicellular population which is capable of genetic damage repair. The first gene, denoted by $ \sigma_{via} $, yields a viable organism with first order growth rate constant $ k > 1 $ if it is equal to some target ``master'' sequence $ \sigma_{via, 0} $. The second gene, denoted by $ \sigma_{rep} $, yields an organism capable of genetic repair if it is equal to some target ``master'' sequence $ \sigma_{rep, 0} $. This model is analytically solvable in the limit of infinite sequence length, and gives an equilibrium distribution which depends on $ \mu \equiv L\eps $, the product of sequence length and per base pair replication error probability, and $ \eps_r $, the probability of repair failure per base pair. The equilibrium distribution is shown to exist in one of three possible ``phases.'' In the first phase, the population is localized about the viability and repairing master sequences. As $ \eps_r $ exceeds the fraction of deleterious mutations, the population undergoes a ``repair'' catastrophe, in which the equilibrium distribution is still localized about the viability master sequence, but is spread ergodically over the sequence subspace defined by the repair gene. Below the repair catastrophe, the distribution undergoes the error catastrophe when $ \mu $ exceeds $ \ln k/\eps_r $, while above the repair catastrophe, the distribution undergoes the error catastrophe when $ \mu $ exceeds $ \ln k/f_{del} $, where $ f_{del} $ denotes the fraction of deleterious mutations. 	
0501512v1	http://arxiv.org/pdf/cond-mat/0501512v1	2005	Spatial Dynamics of Invasion: The Geometry of Introduced Species	G. Korniss|Thomas Caraco	  Many exotic species combine low probability of establishment at each introduction with rapid population growth once introduction does succeed. To analyze this phenomenon, we note that invaders often cluster spatially when rare, and consequently an introduced exotic's population dynamics should depend on locally structured interactions. Ecological theory for spatially structured invasion relies on deterministic approximations, and determinism does not address the observed uncertainty of the exotic-introduction process. We take a new approach to the population dynamics of invasion and, by extension, to the general question of invasibility in any spatial ecology. We apply the physical theory for nucleation of spatial systems to a lattice-based model of competition between plant species, a resident and an invader, and the analysis reaches conclusions that differ qualitatively from the standard ecological theories. Nucleation theory distinguishes between dynamics of single-cluster and multi-cluster invasion. Low introduction rates and small system size produce single-cluster dynamics, where success or failure of introduction is inherently stochastic. Single-cluster invasion occurs only if the cluster reaches a critical size, typically preceded by a number of failed attempts. For this case, we identify the functional form of the probability distribution of time elapsing until invasion succeeds. Although multi-cluster invasion for sufficiently large systems exhibits spatial averaging and almost-deterministic dynamics of the global densities, an analytical approximation from nucleation theory, known as Avrami's law, describes our simulation results far better than standard ecological approximations. 	
0509493v1	http://arxiv.org/pdf/cond-mat/0509493v1	2005	Dynamics of a tracer granular particle as a non-equilibrium Markov   process	Andrea Puglisi|Paolo Visco|Emmanuel Trizac|Frederic van Wijland	  The dynamics of a tracer particle in a stationary driven granular gas is investigated. We show how to transform the linear Boltzmann equation describing the dynamics of the tracer into a master equation for a continuous Markov process. The transition rates depend upon the stationary velocity distribution of the gas. When the gas has a Gaussian velocity probability distribution function (pdf), the stationary velocity pdf of the tracer is Gaussian with a lower temperature and satisfies detailed balance for any value of the restitution coefficient $\alpha$. As soon as the velocity pdf of the gas departs from the Gaussian form, detailed balance is violated. This non-equilibrium state can be characterized in terms of a Lebowitz-Spohn action functional $W(\tau)$ defined over trajectories of time duration $\tau$. We discuss the properties of this functional and of a similar functional $\bar{W}(\tau)$ which differs from the first for a term which is non-extensive in time. On the one hand we show that in numerical experiments, i.e. at finite times $\tau$, the two functionals have different fluctuations and $\bar{W}$ always satisfies an Evans-Searles-like symmetry. On the other hand we cannot observe the verification of the Lebowitz-Spohn-Gallavotti-Cohen (LS-GC) relation, which is expected for $W(\tau)$ at very large times $\tau$. We give an argument for the possible failure of the LS-GC relation in this situation. We also suggest practical recipes for measuring $W(\tau)$ and $\bar{W}(\tau)$ in experiments. 	
0509541v4	http://arxiv.org/pdf/cond-mat/0509541v4	2009	Strong-Coupling Fixed Point of the Kardar-Parisi-Zhang Equation	Léonie Canet	  {\em NOTE: This paper presented the first attempt to tackle the Kardar-Parisi-Zhang (KPZ) equation using non-perturbative renormalisation group (NPRG) methods. It exploited the most natural and frequently used approximation scheme within the NPRG framework, namely the derivative expansion (DE). However, the latter approximation turned out to yield unphysical critical exponents in dimensions $d\ge 2$ and, furthermore, hinted at very poor convergence properties of the DE. The author has since realized that in fact, this approximation may not be valid for the KPZ problem, because of the very nature of the KPZ interaction, which is not {\em potential} but {\em derivative}. The probable failure of the DE is a very unusual -- and instructive -- feature within the NPRG framework. As such, the original work, unpublished, is left available on the arXiv and can be found below.   Added note: the key to deal with the KPZ problem using NPRG lies in not truncating the momentum dependence of the correlation functions, which is investigated in a recent work {\em arXiv:0905.1025}.}   We present a new approach to the Kardar-Parisi-Zhang (KPZ) equation based on the non-perturbative renormalisation group (NPRG). The NPRG flow equations derived here, at the lowest order of the derivative expansion, provide a stable strong-coupling fixed point in all dimensions $d$, embedding in particular the exact results in $d=0$ and $d=1$. However, it yields at this order unreliable dynamical and roughness exponents $z$ and $\chi$ in higher dimensions, which suggests that a richer approximation is needed to investigate the property of the rough phase in $d \ge 2$. 	
0601087v1	http://arxiv.org/pdf/cond-mat/0601087v1	2006	Two-dimensional scaling properties of experimental fracture surfaces	Laurent Ponson|Daniel Bonamy|Elisabeth Bouchaud	  The morphology of fracture surfaces encodes the various complex damage and fracture processes occurring at the microstructure scale that have lead to the failure of a given heterogeneous material. Understanding how to decipher this morphology is therefore of fundamental interest. This has been extensively investigated over these two last decades. It has been established that 1D profiles of these fracture surfaces exhibit properties of scaling invariance. In this paper, we present deeper analysis and investigate the 2D scaling properties of these fracture surfaces. We showed that the properties of scaling invariance are anisotropic and evidenced the existence of two peculiar directions on the post-mortem fracture surface caracterized by two different scaling exponents: the direction of the crack growth and the direction of the crack front. These two exponents were found to be universal, independent of the crack growth velocity, in both silica glass and aluminum alloy, archetype of brittle and ductile material respectively. Moreover, the 2D structure function that fully characterizes the scaling properties of the fracture surface was shown to take a peculiar form similar to the one predicted by some models issued from out-of-equilibrium statistical physics. This suggest some promising analogies between dynamic phase transition models and the stability of a crack front pinned/unpinned by the heterogenities of the material. 	
0604078v2	http://arxiv.org/pdf/cond-mat/0604078v2	2006	First-order Chapman--Enskog velocity distribution function in a granular   gas	J. M. Montanero|A. Santos|V. Garzo	  A method is devised to measure the first-order Chapman-Enskog velocity distribution function associated with the heat flux in a dilute granular gas. The method is based on the application of a homogeneous, anisotropic velocity-dependent external force which produces heat flux in the absence of gradients. The form of the force is found under the condition that, in the linear response regime, the deviation of the velocity distribution function from that of the homogeneous cooling state obeys the same linear integral equation as the one derived from the conventional Chapman-Enskog expansion. The Direct Simulation Monte Carlo method is used to solve the corresponding Boltzmann equation and measure the dependence of the (modified) thermal conductivity on the coefficient of normal restitution $\alpha$. Comparison with previous simulation data obtained from the Green--Kubo relations [Brey et al., J. Phys.: Condens. Matter 17, S2489 (2005)] shows an excellent agreement, both methods consistently showing that the first Sonine approximation dramatically overestimates the thermal conductivity for high inelasticity ($\alpha\lesssim 0.7$). Since our method is tied to the Boltzmann equation, the results indicate that the failure of the first Sonine approximation is not due to velocity correlation effects absent in the Boltzmann framework. This is further confirmed by an analysis of the first-order Chapman-Enskog velocity distribution function and its three first Sonine coefficients obtained from the simulations. 	
0609448v2	http://arxiv.org/pdf/cond-mat/0609448v2	2006	A stochastic flow rule for granular materials	Ken Kamrin|Martin Z. Bazant	  There have been many attempts to derive continuum models for dense granular flow, but a general theory is still lacking. Here, we start with Mohr-Coulomb plasticity for quasi-2D granular materials to calculate (average) stresses and slip planes, but we propose a "stochastic flow rule" (SFR) to replace the principle of coaxiality in classical plasticity. The SFR takes into account two crucial features of granular materials - discreteness and randomness - via diffusing "spots" of local fluidization, which act as carriers of plasticity. We postulate that spots perform random walks biased along slip-lines with a drift direction determined by the stress imbalance upon a local switch from static to dynamic friction. In the continuum limit (based on a Fokker-Planck equation for the spot concentration), this simple model is able to predict a variety of granular flow profiles in flat-bottom silos, annular Couette cells, flowing heaps, and plate-dragging experiments -- with essentially no fitting parameters -- although it is only expected to function where material is at incipient failure and slip-lines are inadmissible. For special cases of admissible slip-lines, such as plate dragging under a heavy load or flow down an inclined plane, we postulate a transition to rate-dependent Bagnold rheology, where flow occurs by sliding shear planes. With different yield criteria, the SFR provides a general framework for multiscale modeling of plasticity in amorphous materials, cycling between continuum limit-state stress calculations, meso-scale spot random walks, and microscopic particle relaxation. 	
0611714v1	http://arxiv.org/pdf/cond-mat/0611714v1	2006	Born-Oppenheimer Breakdown in Graphene	Simone Pisana|Michele Lazzeri|Cinzia Casiraghi|Kostya S. Novoselov|Andre K. Geim|Andrea C. Ferrari|Francesco Mauri	  The Born-Oppenheimer approximation (BO) has proven effective for the accurate determination of chemical reactions, molecular dynamics and phonon frequencies in a wide range of metallic systems. Graphene, recently discovered in the free state, is a zero band-gap semiconductor, which becomes a metal if the Fermi energy is tuned applying a gate-voltage Vg. Graphene electrons near the Fermi energy have twodimensional massless dispersions, described by Dirac cones. Here we show that a change in Vg induces a stiffening of the Raman G peak (i.e. the zone-center E2g optical phonon), which cannot be described within BO. Indeed, the E2g vibrations cause rigid oscillations of the Dirac-cones in the reciprocal space. If the electrons followed adiabatically the Dirac-cone oscillations, no change in the phonon frequency would be observed. Instead, since the electron-momentum relaxation near the Fermi level is much slower than the phonon motion, the electrons do not follow the Dirac-cone displacements. This invalidates BO and results in the observed phonon stiffening. This spectacular failure of BO is quite significant since BO has been the fundamental paradigm to determine crystal vibrations from the early days of quantum mechanics. 	
0701495v3	http://arxiv.org/pdf/cond-mat/0701495v3	2009	New Consideration on Composed Nonextensive Magnetic Systems	F. A. R. Navarro|M. S. Reis|E. K Lenzi|I. S. Olivera	  In this paper a composed A+B magnetic system, with spins J_A=2 and J_B=3/2, is considered within the mean-field approximation, in the framework of Tsallis nonextensive statistics. Our motivation is twofold: (1) to approach the existing experimental data of manganese oxides (manganites), where Mn^{3+} and Mn^{4+} form two magnetic sublattices, and (2) to investigate the structure of nonextensive density matrices of composed systems. By imposing that thermodynamic quantities, such as the magnetization of sublattices A and B, must be invariant weather the calculation is taken over the total Hilbert space or over partial subspaces, we found that the expression for the nonextensive entropy must be adapted. Our argument is supported by calculation of sublattices magnetization M_A and M_B, internal energy, U_A and U_B, and magnetic specific heat, CA and CB. It is shown that only with the modified entropy the two methods of calculation agree to each other. Internal energy and magnetization are additive, but no clear relationship was found between S_A, S_B and the total entropy S_{A+B} for q \neq 1. It is shown that the reason for the failure of the standard way of calculation is the assumption of statistical independence between the two subsystems, which however does not affect the density matrix in the full Hilbert space. 	
0302022v2	http://arxiv.org/pdf/cs/0302022v2	2003	Fault-tolerant routing in peer-to-peer systems	James Aspnes|Zoe Diamadi|Gauri Shah	  We consider the problem of designing an overlay network and routing mechanism that permits finding resources efficiently in a peer-to-peer system. We argue that many existing approaches to this problem can be modeled as the construction of a random graph embedded in a metric space whose points represent resource identifiers, where the probability of a connection between two nodes depends only on the distance between them in the metric space. We study the performance of a peer-to-peer system where nodes are embedded at grid points in a simple metric space: a one-dimensional real line. We prove upper and lower bounds on the message complexity of locating particular resources in such a system, under a variety of assumptions about failures of either nodes or the connections between them. Our lower bounds in particular show that the use of inverse power-law distributions in routing, as suggested by Kleinberg (1999), is close to optimal. We also give efficient heuristics to dynamically maintain such a system as new nodes arrive and old nodes depart. Finally, we give experimental results that suggest promising directions for future work. 	
0411094v2	http://arxiv.org/pdf/cs/0411094v2	2005	On the existence of truly autonomic computing systems and the link with   quantum computing	Radhakrishnan Srinivasan|H. P. Raghunandan	  A theoretical model of truly autonomic computing systems (ACS), with infinitely many constraints, is proposed. An argument similar to Turing's for the unsolvability of the halting problem, which is permitted in classical logic, shows that such systems cannot exist. Turing's argument fails in the recently proposed non-Aristotelian finitary logic (NAFL), which permits the existence of ACS. NAFL also justifies quantum superposition and entanglement, which are essential ingredients of quantum algorithms, and resolves the Einstein-Podolsky-Rosen (EPR) paradox in favour of quantum mechanics and non-locality. NAFL requires that the autonomic manager (AM) must be conceptually and architecturally distinct from the managed element, in order for the ACS to exist as a non-self-referential entity. Such a scenario is possible if the AM uses quantum algorithms and is protected from all problems by (unbreakable) quantum encryption, while the managed element remains classical. NAFL supports such a link between autonomic and quantum computing, with the AM existing as a metamathematical entity. NAFL also allows quantum algorithms to access truly random elements and thereby supports non-standard models of quantum (hyper-) computation that permit infinite parallelism. 	
0508004v1	http://arxiv.org/pdf/cs/0508004v1	2005	A three-valued semantics for logic programmers	Lee Naish	  This paper describes a simpler way for programmers to reason about the correctness of their code. The study of semantics of logic programs has shown strong links between the model theoretic semantics (truth and falsity of atoms in the programmer's interpretation of a program), procedural semantics (for example, SLD resolution) and fixpoint semantics (which is useful for program analysis and alternative execution mechanisms). Most of this work assumes that intended interpretations are two-valued: a ground atom is true (and should succeed according to the procedural semantics) or false (and should not succeed). In reality, intended interpretations are less precise. Programmers consider that some atoms "should not occur" or are "ill-typed" or "inadmissible". Programmers don't know and don't care whether such atoms succeed. In this paper we propose a three-valued semantics for (essentially) pure Prolog programs with (ground) negation as failure which reflects this. The semantics of Fitting is similar but only associates the third truth value with non-termination. We provide tools to reason about correctness of programs without the need for unnatural precision or undue restrictions on programming style. As well as theoretical results, we provide a programmer-oriented synopsis. This work has come out of work on declarative debugging, where it has been recognised that inadmissible calls are important. This paper has been accepted to appear in Theory and Practice of Logic Programming. 	
0204131v1	http://arxiv.org/pdf/hep-th/0204131v1	2002	A tentative theory of large distance physics	Daniel Friedan	  A theoretical mechanism is devised to determine the large distance physics of spacetime. It is a two dimensional nonlinear model, the lambda model, set to govern the string worldsurface to remedy the failure of string theory. The lambda model is formulated to cancel the infrared divergent effects of handles at short distance on the worldsurface. The target manifold is the manifold of background spacetimes. The coupling strength is the spacetime coupling constant. The lambda model operates at 2d distance $\Lambda^{-1}$, very much shorter than the 2d distance $\mu^{-1}$ where the worldsurface is seen. A large characteristic spacetime distance $L$ is given by $L^2=\ln(\Lambda/\mu)$. Spacetime fields of wave number up to 1/L are the local coordinates for the manifold of spacetimes. The distribution of fluctuations at 2d distances shorter than $\Lambda^{-1}$ gives the {\it a priori} measure on the target manifold, the manifold of spacetimes. If this measure concentrates at a macroscopic spacetime, then, nearby, it is a measure on the spacetime fields. The lambda model thereby constructs a spacetime quantum field theory, cutoff at ultraviolet distance $L$, describing physics at distances larger than $L$. The lambda model also constructs an effective string theory with infrared cutoff $L$, describing physics at distances smaller than $L$. The lambda model evolves outward from zero 2d distance, $\Lambda^{-1} = 0$, building spacetime physics starting from $L=\infty$ and proceeding downward in $L$. $L$ can be taken smaller than any distance practical for experiments, so the lambda model, if right, gives all actually observable physics. The harmonic surfaces in the manifold of spacetimes are expected to have novel nonperturbative effects at large distances. 	
0405121v1	http://arxiv.org/pdf/hep-th/0405121v1	2004	Radiation reaction reexamined: bound momentum and Schott term	Dmitri V. Gal'tsov|Pavel Spirin	  We review and compare two different approaches to radiation reaction in classical electrodynamics of point charges: a local calculation of the self-force using the charge equation of motion and a global calculation consisting in integration of the electromagnetic energy-momentum flux through a hypersurface encircling the world-line. Both approaches are complementary and, being combined together, give rise to an identity relating the locally and globally computed forces. From this identity it follows that the Schott terms in the Abraham force should arise from the bound field momentum and can not be introduced by hand as an additional term in the mechanical momentum of an accelerated charge. This is in perfect agreement with the results of Dirac and Teitelboim, but disagrees with the recent calculation of the bound momentum in the retarded coordinates. We perform an independent calculation of the bound electromagnetic momentum and verify explicitly that the Schott term is the derivative of the finite part of the bound momentum indeed. The failure to obtain the same result using the method of retarded coordinates tentatively lies in an inappropriate choice of the integration surface. We also discuss the definition of the delta-function on the semi-axis involved in the local calculation of the radiation reaction force and demonstrate inconsistency of one recent proposal. 	
0412470v1	http://arxiv.org/pdf/math/0412470v1	2004	Wick rotations in 3D gravity: ML(H2)-spacetimes	Riccardo Benedetti|Francesco Bonsante	  "Ends of hyperbolic 3-manifolds should support canonical Wick Rotations, so they realize effective interactions of their ending globally hyperbolic spacetimes of constant curvature." We develop a consistent sector of WR-rescaling theory in 3D gravity, that, in particular, concretizes the above guess for many geometrically finite manifolds. ML(H2)-spacetimes are solutions of pure Lorentzian 3D gravity encoded by measured geodesic laminations of the hyperbolic plane H2, possibly invariant by any given torsion-free discrete isometry group G. The rescalings which correlate spacetimes of different curvature, as well as the conformal Wick rotations towards hyperbolic structures, are directed by the gradient of the respective canonical cosmological times, and have universal rescaling functions that only depend on their value. We get an insight into the WR-rescaling mechanism by studying rays of ML(H2)-spacetimes emanating from the static case. In particular, we determine the "derivatives" at the starting point of each ray. We point out the tamest behaviour of the cocompact G case against the different general one, even when G is of cofinite area, but non-compact. We analyze brocken T-symmetry of AdS ML(H2)-spacetimes and related earthquake failure. This helps us to figure out the main lines of development in order to achieve a complete WR rescaling theory. 	
0511079v1	http://arxiv.org/pdf/math-ph/0511079v1	2005	The Dirichlet Hopf algebra of arithmetics	Bertfried Fauser|P. D. Jarvis	  In this work, we develop systematically the ``Dirichlet Hopf algebra of arithmetics'' by dualizing addition and multiplication maps. We study the additive and multiplicative antipodal convolutions which fail to give rise to Hopf algebra structures, obeying only a weakened (multiplicative) homomorphism axiom. The consequences of the weakened structure, called a Hopf gebra, e.g. on cohomology are explored. This features multiplicativity versus complete multiplicativity of number theoretic arithmetic functions. The deficiency of not being a Hopf algebra is then cured by introducing an `unrenormalized' coproduct and an `unrenormalized' pairing. It is then argued that exactly the failure of the homomorphism property (complete multiplicativity) for non-coprime integers is a blueprint for the problems in quantum field theory (QFT) leading to the need for renormalization. Renormalization turns out to be the morphism from the algebraically sound Hopf algebra to the physical and number theoretically meaningful Hopf gebra. This can be modelled alternatively by employing Rota-Baxter operators. We stress the need for a characteristic-free development where possible, to have a sound starting point for generalizations of the algebraic structures. The last section provides three key applications: symmetric function theory, quantum (matrix) mechanics, and the combinatorics of renormalization in QFT which can be discerned as functorially inherited from the development at the number-theoretic level as outlined here. Hence the occurrence of number theoretic functions in QFT becomes natural. 	
0402025v2	http://arxiv.org/pdf/physics/0402025v2	2004	Underlying mechanism of numerical instability in large eddy simulation   of turbulent flows	Masato Ida|Nobuyuki Taniguchi	  This paper extends our recent theoretical work concerning the feasibility of stable and accurate computation of turbulence using a large eddy simulation [Ida and Taniguchi, Phys. Rev. E 68, 036705 (2003)]. In our previous paper, it was shown, based on a simple assumption regarding the instantaneous streamwise velocity, that the application of the Gaussian filter to the incompressible Navier-Stokes equations can result in the appearance of a numerically unstable term that can be decomposed into positive and negative viscosities. That result raises the question as to whether an accurate solution can be achieved by a numerically stable subgrid-scale model. In the present paper, based on assumptions regarding the statistically averaged velocity, we present similar theoretical investigations to show that in several situations, the shears appearing in the statistically averaged velocity field numerically destabilize the fluctuation components because of the derivation of a numerically unstable term that represents negative diffusion in a fixed direction. This finding can explain the problematic numerical instability that has been encountered in large eddy simulations of wall-bounded flows. The present result suggests that this numerical problem is universal in large eddy simulations, and that if there is no failure in modeling, the resulting subgrid-scale model can still have unstable characteristics; that is, the known instability problems of several existing subgrid-scale models are not something that one may remove simply by an artificial technique, but must be taken seriously so as to treat them accurately. 	
0502100v1	http://arxiv.org/pdf/physics/0502100v1	2005	Use of time-correlated single photon counting detection to measure the   speed of light in water	Pedro L. Muino|Aaron M. Thompson|Robert J. Buenker	  Traditional methods for measuring the speed of light in dispersive media have been based on the detection of interference between light waves emitted from the same source. In the present study the elapsed times for single photons to move from a laser to a photomultiplier tube are measured electronically. Time-correlated single photon counting detection produces a characteristic instrument response which has the same shape independent of both the path length the light travels and the nature of the transparent media through which it passes. This allows for an accurate calibration of the chronograph by observing shifts in the location of the instrument response for different distances traveled by the light. Measurement of the corresponding shift which occurs when light moves the same distance through air and water then enables an accurate determination of the ratio of the photon velocities in these two media. Three different wavelengths of light have been used. In two cases good agreement is found between the present measured light speeds and those which can be inferred from existing refractive index measurements in water. The shortest wavelength studied is too far in the uv to obtain a reliable estimate on the same basis, and so the ng value (1.463) measured in the present work awaits independent confirmation. A theoretical discussion of the present results is undertaken with reference to Newton's original corpuscular theory of light. It is argued that his failure to predict that light travels more slowly in water than in air arose from the inadequacy of his mechanical theory rather than his assumptions about the elementary composition of light. 	
0312022v2	http://arxiv.org/pdf/q-bio/0312022v2	2004	Failed "nonaccelerating" models of prokaryote gene regulatory networks	M. J. Gagen|J. S. Mattick	  Much current network analysis is predicated on the assumption that important biological networks will either possess scale free or exponential statistics which are independent of network size allowing unconstrained network growth over time. In this paper, we demonstrate that such network growth models are unable to explain recent comparative genomics results on the growth of prokaryote regulatory gene networks as a function of gene number. This failure largely results as prokaryote regulatory gene networks are "accelerating" and have total link numbers growing faster than linearly with network size and so can exhibit transitions from stationary to nonstationary statistics and from random to scale-free to regular statistics at particular critical network sizes. In the limit, these networks can undergo transitions so marked as to constrain network sizes to be below some critical value. This is of interest as the regulatory gene networks of single celled prokaryotes are indeed characterized by an accelerating quadratic growth with gene count and are size constrained to be less than about 10,000 genes encoded in DNA sequence of less than about 10 megabases. We develop two "nonaccelerating" network models of prokaryote regulatory gene networks in an endeavor to match observation and demonstrate that these approaches fail to reproduce observed statistics. 	
9908039v1	http://arxiv.org/pdf/quant-ph/9908039v1	1999	Quantum perfect correlations and Hardy's nonlocality theorem	Jose L. Cereceda	  In this paper the failure of Hardy's nonlocality proof for the class of maximally entangled states is considered. A detailed analysis shows that the incompatibility of the Hardy equations for this class of states physically originates from the fact that the existence of quantum perfect correlations for the three pairs of two-valued observables (D_11,D_21), (D_11,D_22) and (D_12,D_21) [in the sense of having with certainty equal (different) readings for a joint measurement of any one of the pairs (D_11,D_21), (D_11,D_22), and (D_12,D_21)], necessarily entails perfect correlation for the pair of observables (D_12,D_22) [in the sense of having with certainty equal (different) readings for a joint measurement of the pair (D_12,D_22)]. Indeed, the set of these four perfect correlations is found to satisfy the CHSH inequality, and then no violations of local realism will arise for the maximally entangled state as far as the four observables D_ij, i,j = 1,2, are concerned. The connection between this fact and the impossibility for the quantum mechanical predictions to give the maximum possible theoretical violation of the CHSH inequality is pointed out. Moreover, it is generally proved that the fulfillment of all the Hardy nonlocality conditions necessarily entails a violation of the resulting CHSH inequality. The largest violation of this latter inequality is determined. 	
0704.2379v1	http://arxiv.org/pdf/0704.2379v1	2007	Fundamental-measure density functional for the fluid of aligned hard   hexagons: New insights in fundamental measure theory	Jose A. Capitan|Jose A. Cuesta	  In this article we obtain a fundamental measure functional for the model of aligned hard hexagons in the plane. Our aim is not just to provide a functional for a new, admittedly academic, model, but to investigate the structure of fundamental measure theory. A model of aligned hard hexagons has similarities with the hard disk model. Both share "lost cases", i.e. admit configurations of three particles in which there is pairwise overlap but not triple overlap. These configurations are known to be problematic for fundamental measure functionals, which are not able to capture their contribution correctly. This failure lies in the inability of these functionals to yield a correct low density limit of the third order direct correlation function. Here we derive the functional by projecting aligned hard cubes on the plane x+y+z=0. The correct dimensional crossover behavior of these functionals permits us to follow this strategy. The functional of aligned hard cubes, however, does not have lost cases, so neither had the resulting functional for aligned hard hexagons. The latter exhibits, in fact, a peculiar structure as compared to the one for hard disks. It depends on a uniparametric family of weighted densities through a new term not appearing in the functional for hard disks. Apart from studying the freezing of this system, we discuss the implications of the functional structure for new developments of fundamental measure theory. 	
0808.1744v1	http://arxiv.org/pdf/0808.1744v1	2008	Our Brothers' Keepers: Secure Routing with High Performance	Alex Brodsky|Scott Lindenberg	  The Trinity (Brodsky et al., 2007) spam classification system is based on a distributed hash table that is implemented using a structured peer-to-peer overlay. Such an overlay must be capable of processing hundreds of messages per second, and must be able to route messages to their destination even in the presence of failures and malicious peers that misroute packets or inject fraudulent routing information into the system. Typically there is tension between the requirements to route messages securely and efficiently in the overlay.   We describe a secure and efficient routing extension that we developed within the I3 (Stoica et al. 2004) implementation of the Chord (Stoica et al. 2001) overlay. Secure routing is accomplished through several complementary approaches: First, peers in close proximity form overlapping groups that police themselves to identify and mitigate fraudulent routing information. Second, a form of random routing solves the problem of entire packet flows passing through a malicious peer. Third, a message authentication mechanism links each message to it sender, preventing spoofing. Fourth, each peer's identifier links the peer to its network address, and at the same time uniformly distributes the peers in the key-space.   Lastly, we present our initial evaluation of the system, comprising a 255 peer overlay running on a local cluster. We describe our methodology and show that the overhead of our secure implementation is quite reasonable. 	
0902.0603v2	http://arxiv.org/pdf/0902.0603v2	2009	Severe Vesico-ureteral Reflux and Urine Sequestration: Mathematical   Relations and Urodynamic Consequences	Lisieux Eyer de Jesus|Paulo de Faria Borges	  Some simple mathematical formulae to calculate the volumes of proximal pyeloureteral reflexive systems are presented, and the results are compared to bladder capacity values. Using the results of the calculi, the author discusses possible implications of severe urinary sequestration in the pyeloureteral systems. Using geometrical and topological approximations we calculate the volumes of ureters and renal pelvises, applying in vivo measurements obtained from conventional ultrasound, retrograde cystourethrograms and topographic anatomic references. Approximations use 2 decimals and assumed $\pi$ value was 3.14. Ureteral and pyelic volumes are calculated, respectively, from the mathematical formula for the cylinder and cone volumes. Dolicomegaureter are compensated using proportional calculi. Bladder volumes are estimated from conventional formulae. Proximal urinary sequestration is compared between infants and older children with VUR. Mechanisms of direct induction of bladder urodynamic failure from VUR are suggested. Sequestration of urine in the ureter and renal pelvis can be estimated from mathematical formulae in patients with VUR. The values used derive from ultrasound examinations, CUM and topographical anatomical references. Primary VUR can determine urodynamic problems. Urine sequestration in the proximal urinary system is worse in infants than in older children. 	
0903.4365v2	http://arxiv.org/pdf/0903.4365v2	2009	CliqueStream: an efficient and fault-resilient live streaming network on   a clustered peer-to-peer overlay	Shah Asaduzzaman|Ying Qiao|Gregor v. Bochmann	  Several overlay-based live multimedia streaming platforms have been proposed in the recent peer-to-peer streaming literature. In most of the cases, the overlay neighbors are chosen randomly for robustness of the overlay. However, this causes nodes that are distant in terms of proximity in the underlying physical network to become neighbors, and thus data travels unnecessary distances before reaching the destination. For efficiency of bulk data transmission like multimedia streaming, the overlay neighborhood should resemble the proximity in the underlying network. In this paper, we exploit the proximity and redundancy properties of a recently proposed clique-based clustered overlay network, named eQuus, to build efficient as well as robust overlays for multimedia stream dissemination. To combine the efficiency of content pushing over tree structured overlays and the robustness of data-driven mesh overlays, higher capacity stable nodes are organized in tree structure to carry the long haul traffic and less stable nodes with intermittent presence are organized in localized meshes. The overlay construction and fault-recovery procedures are explained in details. Simulation study demonstrates the good locality properties of the platform. The outage time and control overhead induced by the failure recovery mechanism are minimal as demonstrated by the analysis. 	
0907.0335v2	http://arxiv.org/pdf/0907.0335v2	2010	Population dynamics on random networks: simulations and analytical   models	Ganna Rozhnova|Ana Nunes	  We study the phase diagram of the standard pair approximation equations for two different models in population dynamics, the susceptible-infective-recovered-susceptible model of infection spread and a predator-prey interaction model, on a network of homogeneous degree $k$. These models have similar phase diagrams and represent two classes of systems for which noisy oscillations, still largely unexplained, are observed in nature. We show that for a certain range of the parameter $k$ both models exhibit an oscillatory phase in a region of parameter space that corresponds to weak driving. This oscillatory phase, however, disappears when $k$ is large. For $k=3, 4$, we compare the phase diagram of the standard pair approximation equations of both models with the results of simulations on regular random graphs of the same degree. We show that for parameter values in the oscillatory phase, and even for large system sizes, the simulations either die out or exhibit damped oscillations, depending on the initial conditions. We discuss this failure of the standard pair approximation model to capture even the qualitative behavior of the simulations on large regular random graphs and the relevance of the oscillatory phase in the pair approximation diagrams to explain the cycling behavior found in real populations. 	
0907.1040v1	http://arxiv.org/pdf/0907.1040v1	2009	Capacitance of graphene nanoribbons	A. A. Shylau|J. W. Klos|I. V. Zozoulenko	  We present an analytical theory for the gate electrostatics and the classical and quantum capacitance of the graphene nanoribbons (GNRs) and compare it with the exact self-consistent numerical calculations based on the tight-binding p-orbital Hamiltonian within the Hartree approximation. We demonstrate that the analytical theory is in a good qualitative (and in some aspects quantitative) agreement with the exact calculations. There are however some important discrepancies. In order to understand the origin of these discrepancies we investigate the self-consistent electronic structure and charge density distribution in the nanoribbons and relate the above discrepancy to the inability of the simple electrostatic model to capture the classical gate electrostatics of the GNRs. In turn, the failure of the classical electrostatics is traced to the quantum mechanical effects leading to the significant modification of the self-consistent charge distribution in comparison to the non-interacting electron description. The role of electron-electron interaction in the electronic structure and the capacitance of the GNRs is discussed. Our exact numerical calculations show that the density distribution and the potential profile in the GNRs are qualitatively different from those in conventional split-gate quantum wires; at the same time, the electron distribution and the potential profile in the GNRs show qualitatively similar features to those in the cleaved-edge overgrown quantum wires. Finally, we discuss an experimental extraction of the quantum capacitance from experimental data. 	
0907.3475v1	http://arxiv.org/pdf/0907.3475v1	2009	BBGKY equations, self-diffusion and 1/f noise in a slightly nonideal gas	Yuriy E. Kuzovlev	  The hypothesis of ``molecular chaos'' is shown to fail when applied to spatially inhomogeneous evolution of a low-density gas, because this hypothesis is incompatible with reduction of interactions of gas particles to ``collisions''. The failure of molecular chaos means existence of statistical correlations between colliding and closely spaced particles in configuration space. If this fact is taken into account, then in the collisional approximation (in the kinetic stage of gas evolution) in the limit of infinitely small gas parameter the Bogolyubov-Born-Green-Kirkwood-Yvon (BBGKY) hierarchy of equations yields an autonomous system of kinetic equations for the many-particle distribution functions of closely spaced particles. This system of equations can produce the Boltzmann equation only in the homogeneous case. It is used to analyze statistical properties of Brownian motion of a test gas particle. The analysis shows that there exist fluctuations with a 1/f spectrum in the diffusivity and mobility of any particle. The physical cause of these fluctuations is randomness of distribution of particles' encounters over the impact parameter values and, consequently, randomness of the rate and efficiency of collisions.   In essence, this is {\bf reprint} of the like author's paper published in Russian in [ Zh. Eksp. Teor. Fiz. {\bf 94} (12), 140-156 (Dec. 1988)] and translated into English in [ Sov. Phys. JETP {\bf 67} (12), 469-2477 (Dec. 1988)] twenty years ago but seemingly still unknown to those to whom it might be very useful. The footnotes contain presently added comments. 	
0908.0499v1	http://arxiv.org/pdf/0908.0499v1	2009	Role of the defect-core in energetics of vacancies	Vikram Gavini	  Electronic structure calculations at macroscopic scales are employed to investigate the crucial role of a defect-core in the energetics of vacancies in aluminum. We find that vacancy core-energy is significantly influenced by the state of deformation at the vacancy-core, especially volumetric strains. Insights from the core electronic structure and computed displacement fields show that this dependence on volumetric strains is closely related to the changing nature of the core-structure under volumetric deformations. These results are in sharp contrast to mechanics descriptions based on elastic interactions that often consider defect core-energies as an inconsequential constant. Calculations suggest that the variation in core-energies with changing macroscopic deformations is quantitatively more significant than the corresponding variation in relaxation energies associated with elastic fields. Upon studying the influence of various macroscopic deformations, which include volumetric, uniaxial, biaxial and shear deformations, on the formation energies of vacancies, we show that volumetric deformations play a dominant role in governing the energetics of these defects. Further, by plotting formation energies of vacancies and di-vacancies against the volumetric strain corresponding to any macroscopic deformation, we find that all variations in the formation energies collapse on to a universal curve. This suggests a universal role of volumetric strains in the energetics of vacancies. Implications of these results in the context of dynamic failure in metals due to spalling are analyzed. 	
1003.0488v2	http://arxiv.org/pdf/1003.0488v2	2010	On Secure Distributed Data Storage Under Repair Dynamics	Sameer Pawar|Salim El Rouayheb|Kannan Ramchandran	  We address the problem of securing distributed storage systems against passive eavesdroppers that can observe a limited number of storage nodes. An important aspect of these systems is node failures over time, which demand a repair mechanism aimed at maintaining a targeted high level of system reliability. If an eavesdropper observes a node that is added to the system to replace a failed node, it will have access to all the data downloaded during repair, which can potentially compromise the entire information in the system. We are interested in determining the secrecy capacity of distributed storage systems under repair dynamics, i.e., the maximum amount of data that can be securely stored and made available to a legitimate user without revealing any information to any eavesdropper. We derive a general upper bound on the secrecy capacity and show that this bound is tight for the bandwidth-limited regime which is of importance in scenarios such as peer-to-peer distributed storage systems. We also provide a simple explicit code construction that achieves the capacity for this regime. 	
1010.5176v1	http://arxiv.org/pdf/1010.5176v1	2010	A Distributed Trust Management Framework for Detecting Malicious Packet   Dropping Nodes in a Mobile Ad Hoc Network	Jaydip Sen	  In a multi-hop mobile ad hoc network (MANET) mobile nodes communicate with each other forming a cooperative radio network. Security remains a major challenge for these networks due to their features of open medium, dynamically changing topologies, reliance on cooperative algorithms, absence of centralized monitoring points, and lack of any clear lines of defense. Most of the currently existing security algorithms designed for these networks are insecure, in efficient, and have low detection accuracy for nodes' misbehaviour. In this paper, a new approach has been proposed to bring out the complementary relationship between key distribution and misbehaviour detection for developing an integrated security solution for MANETs. The redundancy of routing information in ad hoc networks is utilized to develop a highly reliable protocol that works even in presence of transient network partitioning and Byzantine failure of nodes. The proposed mechanism is fully co-operative, and thus it is more robust as the vulnerabilities of the election algorithms used for choosing the subset of nodes for cooperation are absent. Simulation results show the effectiveness of the proposed protocol. 	
1107.4898v4	http://arxiv.org/pdf/1107.4898v4	2011	Local energy: a basis for local electronegativity and local hardness	Tamas Gal	  The traditional approach to establishing a local measure of chemical hardness, by defining a local hardness concept through the derivative of the chemical potential with respect to the electron density, has been found to have limited chemical applicability, and has proved to be an unfeasible approach in principle. Here, we propose a new approach via a unique local energy concept. This local energy is shown to emerge from the Hamilton-Jacobi kind of construction of Schrodinger's quantum mechanics. It then leads to the concepts of a local chemical potential, i.e. negative of local electronegativity, and a local hardness just as the chemical potential and hardness are obtained from the energy, namely via differentiations with respect to the number of electrons. The emerging local hardness adds corrections to a recently proposed local hardness expression that has been found to be a good local measure of hardness for a series of atomic and molecular systems. These corrections become relevant for molecules with a large number of electrons. It is pointed out further that the definition of local softness that yields it as the Fukui function times the softness is not well-established, explaining recent observations of failure of this local softness concept as a proper local reactivity index for hard systems. 	
1110.5969v1	http://arxiv.org/pdf/1110.5969v1	2011	Reliable Provisioning of Spot Instances for Compute-intensive   Applications	William Voorsluys|Rajkumar Buyya	  Cloud computing providers are now offering their unused resources for leasing in the spot market, which has been considered the first step towards a full-fledged market economy for computational resources. Spot instances are virtual machines (VMs) available at lower prices than their standard on-demand counterparts. These VMs will run for as long as the current price is lower than the maximum bid price users are willing to pay per hour. Spot instances have been increasingly used for executing compute-intensive applications. In spite of an apparent economical advantage, due to an intermittent nature of biddable resources, application execution times may be prolonged or they may not finish at all. This paper proposes a resource allocation strategy that addresses the problem of running compute-intensive jobs on a pool of intermittent virtual machines, while also aiming to run applications in a fast and economical way. To mitigate potential unavailability periods, a multifaceted fault-aware resource provisioning policy is proposed. Our solution employs price and runtime estimation mechanisms, as well as three fault tolerance techniques, namely checkpointing, task duplication and migration. We evaluate our strategies using trace-driven simulations, which take as input real price variation traces, as well as an application trace from the Parallel Workload Archive. Our results demonstrate the effectiveness of executing applications on spot instances, respecting QoS constraints, despite occasional failures. 	
1111.0034v3	http://arxiv.org/pdf/1111.0034v3	2012	Diffusion Adaptation Strategies for Distributed Optimization and   Learning over Networks	Jianshu Chen|Ali H. Sayed	  We propose an adaptive diffusion mechanism to optimize a global cost function in a distributed manner over a network of nodes. The cost function is assumed to consist of a collection of individual components. Diffusion adaptation allows the nodes to cooperate and diffuse information in real-time; it also helps alleviate the effects of stochastic gradient noise and measurement noise through a continuous learning process. We analyze the mean-square-error performance of the algorithm in some detail, including its transient and steady-state behavior. We also apply the diffusion algorithm to two problems: distributed estimation with sparse parameters and distributed localization. Compared to well-studied incremental methods, diffusion methods do not require the use of a cyclic path over the nodes and are robust to node and link failure. Diffusion methods also endow networks with adaptation abilities that enable the individual nodes to continue learning even when the cost function changes with time. Examples involving such dynamic cost functions with moving targets are common in the context of biological networks. 	
1111.0385v1	http://arxiv.org/pdf/1111.0385v1	2011	A Distributed Protocol for Detection of Packet Dropping Attack in Mobile   Ad Hoc Networks	Jaydip Sen|M. Girish Chandra|P. Balamuralidhar|Harihara S. G.|Harish Reddy	  In multi-hop mobile ad hoc networks (MANETs),mobile nodes cooperate with each other without using any infrastructure such as access points or base stations. Security remains a major challenge for these networks due to their features of open medium, dynamically changing topologies, reliance on cooperative algorithms, absence of centralized monitoring points, and lack of clear lines of defense. Among the various attacks to which MANETs are vulnerable, malicious packet dropping attack is very common where a malicious node can partially degrade or completely disrupt communication in the network by consistently dropping packets. In this paper, a mechanism for detection of packet dropping attack is presented based on cooperative participation of the nodes in a MANET. The redundancy of routing information in an ad hoc network is utilized to make the scheme robust so that it works effectively even in presence of transient network partitioning and Byzantine failure of nodes. The proposed scheme is fully cooperative and thus more secure as the vulnerabilities of any election algorithm used for choosing a subset of nodes for cooperation are absent. Simulation results show the effectiveness of the protocol. 	
1201.1826v1	http://arxiv.org/pdf/1201.1826v1	2012	Hamiltonian structure of classical N-body systems of finite-size   particles subject to EM interactions	Claudio Cremaschini|Massimo Tessarotto	  An open issue in classical relativistic mechanics is the consistent treatment of the dynamics of classical $N$-body systems of mutually-interacting particles. This refers, in particular, to charged particles subject to EM interactions, including both binary and self interactions (EM-interacting $N$-body systems). In this paper it is shown that such a description can be consistently obtained in the context of classical electrodynamics, for the case of a $N$-body system of classical finite-size charged particles. A variational formulation of the problem is presented, based on the $N$-body hybrid synchronous Hamilton variational principle. Covariant Lagrangian and Hamiltonian equations of motion for the dynamics of the interacting $N$-body system are derived, which are proved to be delay-type ODEs. Then, a representation in both standard Lagrangian and Hamiltonian forms is proved to hold, the latter expressed by means of classical Poisson Brackets. The theory developed retains both the covariance with respect to the Lorentz group and the exact Hamiltonian structure of the problem, which is shown to be intrinsically non-local. Different applications of the theory are investigated. The first one concerns the development of a suitable Hamiltonian approximation of the exact equations that retains finite delay-time effects characteristic of the binary and self EM interactions. Second, basic consequences concerning the validity of Dirac generator formalism are pointed out, with particular reference to the instant-form representation of Poincar\`{e} generators. Finally, a discussion is presented both on the validity and possible extension of the Dirac generator formalism as well as the failure of the so-called Currie \textquotedblleft no-interaction\textquotedblright\ theorem for the non-local Hamiltonian system considered here. 	
1202.0885v1	http://arxiv.org/pdf/1202.0885v1	2012	The Impact of Secure OSs on Internet Security: What Cyber-Insurers Need   to Know	Ranjan Pal|Pan Hui	  In recent years, researchers have proposed \emph{cyber-insurance} as a suitable risk-management technique for enhancing security in Internet-like distributed systems. However, amongst other factors, information asymmetry between the insurer and the insured, and the inter-dependent and correlated nature of cyber risks have contributed in a big way to the failure of cyber-insurance markets. Security experts have argued in favor of operating system (OS) platform switching (ex., from Windows to Unix-based OSs) or secure OS adoption as being one of the techniques that can potentially mitigate the problems posing a challenge to successful cyber-insurance markets. In this regard we model OS platform switching dynamics using a \emph{social gossip} mechanism and study three important questions related to the nature of the dynamics, for Internet-like distributed systems: (i) which type of networks should cyber-insurers target for insuring?, (ii) what are the bounds on the asymptotic performance level of a network, where the performance parameter is an average function of the long-run individual user willingness to adopt secure OSs?, and (iii) how can cyber-insurers use the topological information of their clients to incentivize/reward them during offering contracts? Our analysis is important to a profit-minded cyber-insurer, who wants to target the right network, design optimal contracts to resolve information asymmetry problems, and at the same time promote the increase of overall network security through increasing secure OS adoption amongst users. 	
1202.5208v2	http://arxiv.org/pdf/1202.5208v2	2012	Structure of finite sphere packings via exact enumeration: Implications   for colloidal crystal nucleation	Robert S. Hoy|Jared Harwayne-Gidansky|Corey S. O'Hern	  We analyze the geometric structure and mechanical stability of a complete set of isostatic and hyperstatic sphere packings obtained via exact enumeration. The number of nonisomorphic isostatic packings grows exponentially with the number of spheres $N$, and their diversity of structure and symmetry increases with increasing $N$ and decreases with increasing hyperstaticity $H \equiv N_c - N_{ISO}$, where $N_c$ is the number of pair contacts and $N_{ISO} = 3N-6$. Maximally contacting packings are in general neither the densest nor the most symmetric. Analyses of local structure show that the fraction $f$ of nuclei with order compatible with the bulk (RHCP) crystal decreases sharply with increasing $N$ due to a high propensity for stacking faults, 5- and near-5-fold symmetric structures, and other motifs that preclude RHCP order. While $f$ increases with increasing $H$, a significant fraction of hyperstatic nuclei for $N$ as small as 11 retain non-RHCP structure. Classical theories of nucleation that consider only spherical nuclei, or only nuclei with the same ordering as the bulk crystal, cannot capture such effects. Our results provide an explanation for the failure of classical nucleation theory for hard-sphere systems of $N\lesssim 10$ particles; we argue that in this size regime, it is essential to consider nuclei of unconstrained geometry. Our results are also applicable to understanding kinetic arrest and jamming in systems that interact via hard-core-like repulsive and short-ranged attractive interactions. 	
1203.5086v1	http://arxiv.org/pdf/1203.5086v1	2012	"Selfish" algorithm for optimizing the network survivability analysis	Svetlana V. Poroseva	  In Nature, the primary goal of any network is to survive. This is less obvious for engineering networks (electric power, gas, water, transportation systems etc.) that are expected to operate under normal conditions most of time. As a result, the ability of a network to withstand massive sudden damage caused by adverse events (or survivability) has not been among traditional goals in the network design. Reality, however, calls for the adjustment of design priorities. As modern networks develop toward increasing their size, complexity, and integration, the likelihood of adverse events increases too due to technological development, climate change, and activities in the political arena among other factors. Under such circumstances, a network failure has an unprecedented effect on lives and economy. To mitigate the impact of adverse events on the network operability, the survivability analysis must be conducted at the early stage of the network design. Such analysis requires the development of new analytical and computational tools. Computational analysis of the network survivability is the exponential time problem at least. The current paper describes a new algorithm, in which the reduction of the computational complexity is achieved by mapping an initial network topology with multiple sources and sinks onto a set of simpler smaller topologies with multiple sources and a single sink. Steps for further reducing the time and space expenses of computations are also discussed. 	
1204.0320v1	http://arxiv.org/pdf/1204.0320v1	2012	Two-dimensional finite element simulation of fracture and fatigue   behaviours of alumina microstructures for hip prosthesis	Kyungmok Kim|Bernard Forest|Jean Géringer	  This paper describes a two-dimensional (2D) finite element simulation for fracture and fatigue behaviours of pure alumina microstructures such as those found at hip prostheses. Finite element models are developed using actual Al2O3 microstructures and a bilinear cohesive zone law. Simulation conditions are similar to those found at a slip zone in a dry contact between a femoral head and an acetabular cup of hip prosthesis. Contact stresses are imposed to generate cracks in the models. Magnitudes of imposed stresses are higher than those found at the microscopic scale. Effects of microstructures and contact stresses are investigated in terms of crack formation. In addition, fatigue behaviour of the microstructure is determined by performing simulations under cyclic loading conditions. It is shown that crack density observed in a microstructure increases with increasing magnitude of applied contact stress. Moreover, crack density increases linearly with respect to the number of fatigue cycles within a given contact stress range. Meanwhile, as applied contact stress increases, number of cycles to failure decreases gradually. Finally, this proposed finite element simulation offers an effective method for identifying fracture and fatigue behaviours of a microstructure provided that microstructure images are available. 	
1204.0974v2	http://arxiv.org/pdf/1204.0974v2	2012	Traditional formation scenarios fail to explain 4:3 mean motion   resonances	Hanno Rein|Matthew J. Payne|Dimitri Veras|Eric B. Ford	  At least two multi-planetary systems in a 4:3 mean motion resonance have been found by radial velocity surveys. These planets are gas giants and the systems are only stable when protected by a resonance. Additionally the Kepler mission has detected at least 4 strong candidate planetary systems with a period ratio close to 4:3.   This paper investigates traditional dynamical scenarios for the formation of these systems. We systematically study migration scenarios with both N-body and hydro-dynamic simulations. We investigate scenarios involving the in-situ formation of two planets in resonance. We look at the results from finely tuned planet-planet scattering simulations with gas disk damping. Finally, we investigate a formation scenario involving isolation-mass embryos.   Although the combined planet-planet scattering and damping scenario seems promising, none of the above scenarios is successful in forming enough systems in 4:3 resonance with planetary masses similar to the observed ones. This is a negative result but it has important implications for planet formation. Previous studies were successful in forming 2:1 and 3:2 resonances. This is generally believed to be evidence of planet migration. We highlight the main differences between those studies and our failure in forming a 4:3 resonance. We also speculate on more exotic and complicated ideas. These results will guide future investigators toward exploring the above scenarios and alternative mechanisms in a more general framework. 	
1205.3892v1	http://arxiv.org/pdf/1205.3892v1	2012	Reconsideration of the uncertainty relations and quantum measurements	Spiridon Dumitru	  Discussions on uncertainty relations (UR) and quantum measurements (QMS) persisted until nowadays in publications about quantum mechanics (QM). They originate mainly from the conventional interpretation of UR (CIUR). In the most of the QM literarure, it is underestimated the fact that, over the years, a lot of deficiencies regarding CIUR were signaled. As a rule the alluded deficiencies were remarked disparately and discussed as punctual and non-essential questions. Here we approach an investigation of the mentioned deficiencies collected in a conclusive ensemble. Subsequently we expose a reconsideration of the major problems referring to UR and QMS. We reveal that all the basic presumption of CIUR are troubled by insurmountable deficiencies which require the indubitable failure of CIUR and its necessary abandonment. Therefore the UR must be deprived of their statute of crucial pieces for physics. So, the aboriginal versions of UR appear as being in postures of either (i) thought-experimental fictions or (ii) simple QM formulae and, any other versions of them, have no connection with the QMS. Then the QMS must be viewed as an additional subject comparatively with the usual questions of QM. For a theoretical description of QMS we propose an information-transmission model, in which the quantum observables are considered as random variables. Our approach directs to natural solutions and simplifications for many problems regarding UR and QMS. 	
1206.4361v1	http://arxiv.org/pdf/1206.4361v1	2012	Tracing the Progression of Retinitis Pigmentosa via Photoreceptor   Interactions	Erika T. Camacho|Stephen Wirkus	  Retinitis pigmentosa (RP) is a group of inherited degenerative eye diseases characterized by mutations in the genetic structure of the photoreceptors that leads to the premature death of both rod and cone photoreceptors. Defects in particular genes encoding proteins that are involved in either the photoreceptor structure, phototransduction cascades, or visual cycle are expressed in the rods but ultimately affect both types of cells. RP is "typically" manifested by a steady death of rods followed by a period of stability in which cones survive initially and then inevitably die too. In some RP cases, rods and cones die off simultaneously or even cone death precedes rod death (reverse RP). The mechanisms and factors involved in the development of the different types of RP are not well understood nor have researchers been able to provide more than a limited number of short-term therapies. In this work we trace the progression of RP to complete blindness through each subtype via bifurcation theory. We show that the evolution of RP from one stage to another often requires the failure of multiple components. Our results indicate that a delicate balance between the availability of nutrients and the rates of shedding and renewal of photoreceptors is needed at every stage of RP to halt its progression. This work provides a framework for future physiological investigations potentially leading to long-term targeted multi-facet interventions and therapies dependent on the particular stage and subtype of RP under consideration. The results of this mathematical model may also give insight into the progression of many other degenerative eye diseases involving genetic mutations or secondary photoreceptor death and potential ways to circumvent these diseases. 	
1209.5256v1	http://arxiv.org/pdf/1209.5256v1	2012	Infiltration effects on a two-dimensional molecular dynamics model of   landslides	Gianluca Martelloni|Franco Bagnoli	  In this paper we propose a two-dimensional (2D) computational model, based on a molecular dynamics (MD) approach, for deep landslides triggered by rainfall. Our model is based on interacting particles or grains and describes the behavior of a fictitious granular material along a slope consisting of a vertical section, i.e. with a wide thickness. The triggering of the landslide is caused by the passing of two conditions: a threshold speed and a condition on the static friction of the particles, the latter based on the Mohr-Coulomb failure criterion (Coulomb 1776; Mohr 1914). The inter-particle interactions are through a potential that, in the absence of suitable experimental data and due to the arbitrariness of the grain dimension is modeled by means of a potential similar to the Lennard-Jones one (Lennard-Jones 1924), i.e., with an attractive and a repulsive part. For the updating of the particle positions we use a MD method which results to be very suitable to simulate this type of systems (Herrmann and Luding 1998). In order to take into account the increasing of the pore pressure due to the rainfall, a filtration model is considered. Finally we also introduce in the model the viscosity as a term in the dynamic equations of motion. The outcome of simulations, from the point of view of statistical and dynamic characterization, is quite satisfactory relative to real landslides behavior and we can claim that this types of modeling can represent a new method to simulate landslides triggered by rainfall. 	
1212.2117v1	http://arxiv.org/pdf/1212.2117v1	2012	Functional renormalization-group approach to decaying turbulence	Andrei A. Fedorenko|Pierre Le Doussal|Kay Joerg Wiese	  We reconsider the functional renormalization-group (FRG) approach to decaying Burgers turbulence, and extend it to decaying Navier-Stokes and Surface-Quasi-Geostrophic turbulence. The method is based on a renormalized small-time expansion, equivalent to a loop expansion, and naturally produces a dissipative anomaly and a cascade after a finite time. We explicitly calculate and analyze the one-loop FRG equations in the zero-viscosity limit as a function of the dimension. For Burgers they reproduce the FRG equation obtained in the context of random manifolds, extending previous results of one of us. Breakdown of energy conservation due to shocks and the appearance of a direct energy cascade corresponds to failure of dimensional reduction in the context of disordered systems. For Navier-Stokes in three dimensions, the velocity-velocity correlation function acquires a linear dependence on the distance, zeta_2=1, in the inertial range, instead of Kolmogorov's zeta_2=2/3; however the possibility remains for corrections at two- or higher-loop order. In two dimensions, we obtain a numerical solution which conserves energy and exhibits an inverse cascade, with explicit analytical results both for large and small distances, in agreement with the scaling proposed by Batchelor. In large dimensions, the one-loop FRG equation for Navier-Stokes converges to that of Burgers. 	
1302.0916v1	http://arxiv.org/pdf/1302.0916v1	2013	Quantum Discord, CHSH Inequality and Hidden Variables -- Critical   reassessment of hidden-variables models	Kazuo Fujikawa	  Hidden-variables models are critically reassessed. It is first examined if the quantum discord is classically described by the hidden-variable model of Bell in the Hilbert space with $d=2$. The criterion of vanishing quantum discord is related to the notion of reduction and, surprisingly, the hidden-variable model in $d=2$, which has been believed to be consistent so far, is in fact inconsistent and excluded by the analysis of conditional measurement and reduction. The description of the full contents of quantum discord by the deterministic hidden-variables models is not possible. We also re-examine CHSH inequality. It is shown that the well-known prediction of CHSH inequality $|B|\leq 2$ for the CHSH operator $B$ introduced by Cirel'son is not unique. This non-uniqueness arises from the failure of linearity condition in the non-contextual hidden-variables model in $d=4$ used by Bell and CHSH, in agreement with Gleason's theorem which excludes $d=4$ non-contextual hidden-variables models. If one imposes the linearity condition, their model is converted to a factored product of two $d=2$ models which describes quantum mechanical separable states. The CHSH inequality thus does not test the hidden-variables model in $d=4$. This observation is consistent with an application of the CHSH inequality to quantum cryptography by Ekert, which is based on mixed separable states without referring to dispersion-free representations. As for hidden-variables models, there exist no viable local non-contextual models in any dimensions. 	
1303.2256v2	http://arxiv.org/pdf/1303.2256v2	2013	Migraine generator network and spreading depression dynamics as   neuromodulation targets in episodic migraine	Markus A. Dahlem	  Migraine is a common disabling headache disorder characterized by recurrent episodes sometimes preceded or accompanied by focal neurological symptoms called aura. The relation between two subtypes, migraine without aura (MWoA) and migraine with aura (MWA), is explored with the aim to identify targets for neuromodulation techniques. To this end, a dynamically regulated control system is schematically reduced to a network of the trigeminal nerve, which innervates the cranial circulation, an associated descending modulatory network of brainstem nuclei, and parasympathetic vasomotor efferents. This extends the idea of a migraine generator region in the brainstem to a larger network and is still simple and explicit enough to open up possibilities for mathematical modeling in the future. In this study, it is suggested that the migraine generator network (MGN) is driven and may therefore respond differently to different spatio-temporal noxious input in the migraine subtypes MWA and MWoA. The noxious input is caused by a cortical perturbation of homeostasis, known as spreading depression (SD). The MGN might even trigger SD in the first place by a failure in vasomotor control. As a consequence, migraine is considered as an inherently dynamical disease to which a linear course from upstream to downstream events would not do justice. Minimally invasive and noninvasive neuromodulation techniques are briefly reviewed and their rational is discussed in the context of the proposed mechanism. 	
1304.1933v1	http://arxiv.org/pdf/1304.1933v1	2013	Improvements to Kramers Turnover Theory	Eli Pollak|Joachim Ankerhold	  The Kramers turnover problem, that is obtaining a uniform expression for the rate of escape of a particle over a barrier for any value of the external friction was solved in the eighties. Two formulations were given, one by Melnikov and Meshkov (MM) (J. Chem. Phys. 85, 1018 (1986)), which was based on a perturbation expansion for the motion of the particle in the presence of friction. The other, by Pollak, Grabert and Haenggi (PGH) (J. Chem. Phys. 91, 4073 (1989)), valid also for memory friction, was based on a perturbation expansion for the motion along the collective unstable normal mode of the particle. Both theories did not take into account the temperature dependence of the average energy loss to the bath. Increasing the bath temperature will reduce the average energy loss. In this paper, we analyse this effect, using a novel perturbation theory. We find that within the MM approach, the thermal energy gained from the bath diverges, the average energy gain becomes infinite, implying an essential failure of the theory. Within the PGH approach increasing the bath temperature reduces the average energy loss but only by a finite small amount, of the order of the inverse of the reduced barrier height. This then does not seriously affect the theory. Analysis and application for a cubic potential and Ohmic friction are presented. 	
1305.0727v2	http://arxiv.org/pdf/1305.0727v2	2013	Arterial stiffening provides sufficient explanation for primary   hypertension	Klas H. Pettersen|Scott M. Bugenhagen|Javaid Nauman|Daniel A. Beard|Stig W. Omholt	  Hypertension is one of the most common age-related chronic diseases and by predisposing individuals for heart failure, stroke and kidney disease, it is a major source of morbidity and mortality. Its etiology remains enigmatic despite intense research efforts over many decades. By use of empirically well-constrained computer models describing the coupled function of the baroreceptor reflex and mechanics of the circulatory system, we demonstrate quantitatively that arterial stiffening seems sufficient to explain age-related emergence of hypertension. Specifically, the empirically observed chronic changes in pulse pressure with age, and the impaired capacity of hypertensive individuals to regulate short-term changes in blood pressure, arise as emergent properties of the integrated system. Results are consistent with available experimental data from chemical and surgical manipulation of the cardio-vascular system. In contrast to widely held opinions, the results suggest that primary hypertension can be attributed to a mechanogenic etiology without challenging current conceptions of renal and sympathetic nervous system function. The results support the view that a major target for treating chronic hypertension in the elderly is the reestablishment of a proper baroreflex response. 	
1305.2190v1	http://arxiv.org/pdf/1305.2190v1	2013	Scalable Routing Easy as PIE: a Practical Isometric Embedding Protocol   (Technical Report)	Julien Herzen|Cedric Westphal|Patrick Thiran	  We present PIE, a scalable routing scheme that achieves 100% packet delivery and low path stretch. It is easy to implement in a distributed fashion and works well when costs are associated to links. Scalability is achieved by using virtual coordinates in a space of concise dimensionality, which enables greedy routing based only on local knowledge. PIE is a general routing scheme, meaning that it works on any graph. We focus however on the Internet, where routing scalability is an urgent concern. We show analytically and by using simulation that the scheme scales extremely well on Internet-like graphs. In addition, its geometric nature allows it to react efficiently to topological changes or failures by finding new paths in the network at no cost, yielding better delivery ratios than standard algorithms. The proposed routing scheme needs an amount of memory polylogarithmic in the size of the network and requires only local communication between the nodes. Although each node constructs its coordinates and routes packets locally, the path stretch remains extremely low, even lower than for centralized or less scalable state-of-the-art algorithms: PIE always finds short paths and often enough finds the shortest paths. 	
1306.1529v2	http://arxiv.org/pdf/1306.1529v2	2014	Scaling Properties of Rainfall-Induced Landslides Predicted by a   Physically Based Model	M. Alvioli|F. Guzzetti|M. Rossi	  Natural landslides exhibit scaling properties revealed by power law relationships. These relationships include the frequency of the size (e.g., area, volume) of the landslides, and the rainfall conditions responsible for slope failures in a region. Reasons for the scaling behavior of landslides are poorly known. We investigate the possibility of using the Transient Rainfall Infiltration and Grid-Based Regional Slope-Stability analysis code (TRIGRS), a consolidated, physically-based, numerical model that describes the stability/instability conditions of natural slopes forced by rainfall, to determine the frequency statistics of the area of the unstable slopes and the rainfall intensity (I) - duration (D) conditions that result in landslides in a region. We apply TRIGRS in a portion of the Upper Tiber River Basin, Central Italy. The spatially distributed model predicts the stability/instability conditions of individual grid cells, given the local terrain and rainfall conditions. We run TRIGRS using multiple, synthetic rainfall histories, and we compare the modeling results with empirical evidences of the area of landslides and of the rainfall conditions that have caused landslides in the study area. Our findings revealed that TRIGRS is capable of reproducing the frequency of the size of the patches of terrain predicted as unstable by the model, which match the frequency size statistics of landslides in the study area, and the mean rainfall D, I conditions that result in unstable slopes in the study area, which match rainfall I-D thresholds for possible landslide occurrence. Our results are a step towards understanding the mechanisms that give rise to landslide scaling properties. 	
1307.1097v3	http://arxiv.org/pdf/1307.1097v3	2013	A construction principle for ADM-type theories in maximal slicing gauge	Henrique Gomes	  The differing concepts of time in general relativity and quantum mechanics are widely accused as the main culprits in our persistent failure in finding a complete theory of quantum gravity. Here we address this issue by constructing ADM-type theories \emph{in a particular time gauge} directly from first principles. The principles are expressed as conditions on phase space constraints: we search for two sets of spatially covariant constraints, which generate symmetries (are first class) and gauge-fix each other leaving two propagating degrees of freedom.   One of the sets is the Weyl generator tr$(\pi)$, and the other is a one-parameter family containing the ADM scalar constraint $\lambda R- \beta(\pi^{ab}\pi_{ab}+(\mbox{tr}(\pi))^2/2))$. The two sets of constraints can be seen as defining ADM-type theories with a maximal slicing gauge-fixing. This work provides an independent, first principles derivation of ADM gravity.   The principles above are motivated by a heuristic argument relying in the relation between symmetry doubling and exact renormalization arguments for quantum gravity, aside from compatibility with the spatial diffeomorphisms. As a by-product, these results address one of the most popular criticisms of Shape Dynamics: its construction starts off from the ADM Hamiltonian formulation.   The present work severs this dependence: the set of constraints yield reduced phase space theories that can be naturally represented by either Shape Dynamics or ADM. More precisely, the resulting theories can be naturally "unfixed" to encompass either spatial Weyl invariance (the symmetry of Shape Dynamics) or refoliation symmetry (ADM). 	
1308.0959v4	http://arxiv.org/pdf/1308.0959v4	2017	LORD: Leader-based framework for Resource Discovery in Mobile Device   Clouds	Seyed Mohammad Asghari|Yi-Hsuan Kao|Mohammad Hassan Lotfi|Mohammad Noormohammadpour|Bhaskar Krishnamachari|Babak Hossein Khalaj|Marcos Katz	  We provide a novel solution for Resource Discovery (RD) in mobile device clouds consisting of selfish nodes. Mobile device clouds (MDCs) refer to cooperative arrangement of communication-capable devices formed with resource-sharing goal in mind. Our work is motivated by the observation that with ever-growing applications of MDCs, it is essential to quickly locate resources offered in such clouds, where the resources could be content, computing resources, or communication resources. The current approaches for RD can be categorized into two models: decentralized model, where RD is handled by each node individually; and centralized model, where RD is assisted by centralized entities like cellular network. However, we propose LORD, a Leader-based framewOrk for RD in MDCs which is not only self-organized and not prone to having a single point of failure like the centralized model, but also is able to balance the energy consumption among MDC participants better than the decentralized model. Moreover, we provide a credit-based incentive to motivate participation of selfish nodes in the leader selection process, and present the first energy-aware leader selection mechanism for credit-based models. The simulation results demonstrate that LORD balances energy consumption among nodes and prolongs overall network lifetime compared to decentralized model. 	
1308.1862v1	http://arxiv.org/pdf/1308.1862v1	2013	Percolation of Interdependent Networks with Inter-similarity	Yanqing Hu|Dong Zhou|Rui Zhang|Zhangang Han|Shlomo Havlin	  Real data show that interdependent networks usually involve inter-similarity. Intersimilarity means that a pair of interdependent nodes have neighbors in both networks that are also interdependent (Parshani et al \cite{PAR10B}). For example, the coupled world wide port network and the global airport network are intersimilar since many pairs of linked nodes (neighboring cities), by direct flights and direct shipping lines exist in both networks. Nodes in both networks in the same city are regarded as interdependent. If two neighboring nodes in one network depend on neighboring nodes in the another we call these links common links. The fraction of common links in the system is a measure of intersimilarity. Previous simulation results suggest that intersimilarity has considerable effect on reducing the cascading failures, however, a theoretical understanding on this effect on the cascading process is currently missing. Here, we map the cascading process with inter-similarity to a percolation of networks composed of components of common links and non common links. This transforms the percolation of inter-similar system to a regular percolation on a series of subnetworks, which can be solved analytically. We apply our analysis to the case where the network of common links is an Erd\H{o}s-R\'{e}nyi (ER) network with the average degree $K$, and the two networks of non-common links are also ER networks. We show for a fully coupled pair of ER networks, that for any $K\geq0$, although the cascade is reduced with increasing $K$, the phase transition is still discontinuous. Our analysis can be generalized to any kind of interdependent random networks system. 	
1310.5722v2	http://arxiv.org/pdf/1310.5722v2	2014	Architecture of the Florida Power Grid as a Complex Network	Yan Xu|Aleks Jacob Gurfinkel|Per Arne Rikvold	  We study the Florida high-voltage power grid as a technological network embedded in space. Measurements of geographical lengths of transmission lines, the mixing of generators and loads, the weighted clustering coefficient, as well as the organization of edge conductance weights show a complex architecture quite different from random-graph models usually considered. In particular, we introduce a parametrized mixing matrix to characterize the mixing pattern of generators and loads in the Florida Grid, which is intermediate between the random mixing case and the semi-bipartite case where generator-generator transmission lines are forbidden. Our observations motivate an investigation of optimization (design) principles leading to the structural organization of power grids. We thus propose two network optimization models for the Florida Grid as a case study. Our results show that the Florida Grid is optimized not only by reducing the construction cost (measured by the total length of power lines), but also through reducing the total pairwise edge resistance in the grid, which increases the robustness of power transmission between generators and loads against random line failures. We then embed our models in spatial areas of different aspect ratios and study how this geometric factor affects the network structure, as well as the box-counting fractal dimension of the grids generated by our models. 	
1310.8386v1	http://arxiv.org/pdf/1310.8386v1	2013	Intrinsic VHE Gamma-ray spectra of Blazars as a probe for Extragalactic   Background Light	K K Singh|S Sahayanathan|A K Tickoo|N Bhatt	  Very high energy (VHE) $\gamma$-rays above 10$'$s of GeV energy, emitted from distant blazars, are attenuated by photons from the extragalactic background light (EBL). Unfortunately, neither the EBL nor the intrinsic blazar spectrum is accurately known to derive one quantity from the other. In this work we use a homogeneous one zone model involving synchrotron, synchrotron self Compton (SSC) and external Compton (EC) emission mechanisms to estimate the intrinsic VHE spectra of blazars. The model is applied on three VHE blazars, namely PKS2155-304, RGB J0710+591 and 3C279, for which simultaneous multi-wavelength data are available from various observations. The predicted values of the intrinsic VHE fluxes are then compared with the observations by imaging atmospheric Cherenkov telescopes to determine the optical depth of VHE $\gamma$-rays. On comparing these optical depth values with those predicted by four different EBL models, we observe a somewhat pronounced systematic deviation for PKS2155-304 and 3C279 at higher energies, especially for the EBL model proposed by Finke et al.(2010). We attribute this deviation to be an outcome of either the failure of the extrapolation of blazar SED to VHE energies and/or due to various assumptions buried in the EBL models. 	
1311.0047v2	http://arxiv.org/pdf/1311.0047v2	2014	Bayesian inferences of galaxy formation from the K-band luminosity and   HI mass functions of galaxies: constraining star formation and feedback	Yu Lu|H. J. Mo|Zhankui Lu|Neal Katz|Martin D. Weinberg	  We infer mechanisms of galaxy formation for a broad family of semi-analytic models (SAMs) constrained by the K-band luminosity function and HI mass function of local galaxies using tools of Bayesian analysis. Even with a broad search in parameter space the whole model family fails to match to constraining data. In the best fitting models, the star formation and feedback parameters in low-mass haloes are tightly constrained by the two data sets, and the analysis reveals several generic failures of models that similarly apply to other existing SAMs. First, based on the assumption that baryon accretion follows the dark matter accretion, large mass-loading factors are required for haloes with circular velocities lower than 200 km/s, and most of the wind mass must be expelled from the haloes. Second, assuming that the feedback is powered by Type-II supernovae with a Chabrier IMF, the outflow requires more than 25% of the available SN kinetic energy. Finally, the posterior predictive distributions for the star formation history are dramatically inconsistent with observations for masses similar to or smaller than the Milky-Way mass. The inferences suggest that the current model family is still missing some key physical processes that regulate the gas accretion and star formation in galaxies with masses below that of the Milky Way. 	
1311.6546v1	http://arxiv.org/pdf/1311.6546v1	2013	The Compactness of Presupernova Stellar Cores	Tuguldur Sukhbold|Stan Woosley	  The success or failure of the neutrino-transport mechanism for producing a supernova in an evolved massive star is known to be sensitive not only to the mass of the iron core that collapses, but also to the density gradient in the silicon and oxygen shells surrounding that core. Here we study the systematics of a presupernova core's "compactness" (O'Connor & Ott 2011) as a function of the mass of the star and the physics used in its calculation. Fine-meshed surveys of presupernova evolution are calculated for stars from 15 to 65 Msun. The metallicity and the efficiency of semiconvection and overshoot mixing are both varied and bare carbon-oxygen cores are explored as well as full hydrogenic stars. Two different codes, KEPLER and MESA, are used for the study. A complex interplay of carbon and oxygen burning, especially in shells, can cause rapid variations in the compactness for stars of very nearly the same mass. On larger scales, the distribution of compactness with main sequence mass is found to be robustly non-monotonic, implying islands of "explodability", particularly around 8 to 20 Msun and 25 to 30 Msun. The carbon-oxygen (CO) core mass of a presupernova star is a better, though still ambiguous discriminant of its core structure than the main sequence mass. 	
1312.2323v1	http://arxiv.org/pdf/1312.2323v1	2013	Architectural Pattern of Health Care System Using GSM Networks	Meiappane. A|Dr. V. Prasanna Venkatesan|Selva Murugan. S|Arun. A|Ramachandran. A	  Large-scale networked environments, such as the Internet, possess the characteristics of centralised data, centralised access and centralised control; this gives the user a powerful mechanism for building and integrating large repositories of centralised information from diverse resources set. However, a centralised network system with GSM Networks development for a hospital information systems or a health care information portal is still in its infancy. The shortcomings of the currently available tools have made the use of mobile devices more appealing. In mobile computing, the issues such as low bandwidth, high latency wireless Networks, loss or degradation of wireless connections, and network errors or failures need to be dealt with. Other issues to be addressed include system adaptability, reliability, robustness, extensibility, flexibility, and maintainability. GSM approach has emerged as the most viable approach for development of intelligent software applications for wireless mobile devices in a centralized environment, which gives higher band width of 900 MHz for transmission. The e-healthcare system that we have developed provides support for physicians, nurses, pharmacists and other healthcare professionals, as well as for patients and medical devices used to monitor patients. In this paper, we present the architecture and the demonstration prototype. 	
1312.5543v1	http://arxiv.org/pdf/1312.5543v1	2013	Improvements to the Prototype Micro-Brittle Linear Elasticity Model of   Peridynamics	Georg C. Ganzenmüller|Stefan Hiermaier|Michael May	  This paper assesses the accuracy and convergence of the linear-elastic, bond-based Peridynamic model with brittle failure, known as the prototype micro-brittle (PMB) model. We investigate the discrete equations of this model, suitable for numerical implementation. It is shown that the widely used discretization approach incurs rather large errors. Motivated by this observation, a correction is proposed, which significantly increases the accuracy by cancelling errors associated with the discretization. As an additional result, we derive equations to treat the interactions between differently sized particles, i.e., a non-homogeneous discretization spacing. This presents an important step forward for the applicability of the PMB model to complex geometries, where it is desired to model interesting parts with a fine resolution (small particle spacings) and other parts with a coarse resolution in order to gain numerical efficiency. Validation of the corrected Peridynamic model is performed by comparing longitudinal sound wave propagation velocities with exact theoretical results. We find that the corrected approach correctly reproduces the sound wave velocity, while the original approach severely overestimates this quantity. Additionally, we present simulations for a crack growth problem which can be analytically solved within the framework of Linear Elastic Fracture Mechanics Theory. We find that the corrected Peridynamics model is capable of quantitatively reproducing crack initiation and propagation. 	
1402.2646v1	http://arxiv.org/pdf/1402.2646v1	2014	A Performance-Based Framework for Bridge Preservation Based on   Damage-Integrated System-Level Behavior	Amir Gheitasi|Davin K. Harris	  The safety and condition of transportation infrastructure has been at the forefront of national debates in recent times due to catastrophic bridge failures, but the issue has been a longstanding challenge for transportation agencies for many years as resources continue to diminish. The performance of this infrastructure has a direct influence on the lives of most of citizens in developed regions by providing a critical lifeline between communities and the transportation of goods and services, and as a critical component of the transportation network, bridges have received a lot of attention regarding condition assessment and maintenance practices. Despite successful implementation of advanced evaluation techniques, what is still lacking is a fundamental understanding of the system behavior in the presence of deteriorating conditions that can be used for preservation decision-making. This paper aims to present a performance-based framework that can be used to characterize the behavior of in-service bridge superstructures. In order to measure the bridge system performance with deteriorating conditions, system-level behavior of a representative composite steel girder bridge, degraded with three common damage scenarios was investigated in this study. Results obtained from validated numerical analysis demonstrated significant impact of integrated damage mechanisms on the ultimate capacity, redundancy and system ductility of the simulated bridge superstructure. It is expected that the proposed framework for evaluating system behavior will provide a first step for establishing a critical linkage between design, maintenance, and rehabilitation of highway bridges, which are uncoupled in current infrastructure decision-making processes. 	
1402.5770v1	http://arxiv.org/pdf/1402.5770v1	2014	The Case for Cloud Service Trustmarks and Assurance-as-a-Service	Theo Lynn|Philip Healy|Richard McClatchey|John Morrison|Claus Pahl|Brian Lee	  Cloud computing represents a significant economic opportunity for Europe. However, this growth is threatened by adoption barriers largely related to trust. This position paper examines trust and confidence issues in cloud computing and advances a case for addressing them through the implementation of a novel trustmark scheme for cloud service providers. The proposed trustmark would be both active and dynamic featuring multi-modal information about the performance of the underlying cloud service. The trustmarks would be informed by live performance data from the cloud service provider, or ideally an independent third-party accountability and assurance service that would communicate up-to-date information relating to service performance and dependability. By combining assurance measures with a remediation scheme, cloud service providers could both signal dependability to customers and the wider marketplace and provide customers, auditors and regulators with a mechanism for determining accountability in the event of failure or non-compliance. As a result, the trustmarks would convey to consumers of cloud services and other stakeholders that strong assurance and accountability measures are in place for the service in question and thereby address trust and confidence issues in cloud computing. 	
1402.6557v1	http://arxiv.org/pdf/1402.6557v1	2014	Mean-field dynamos: the old concept and some recent developments	K. -H. Rädler	  This article reproduces the Karl Schwarzschild lecture 2013. Some of the basic ideas of electrodynamics and magnetohydrodynamics of mean fields in turbulently moving conducting fluids are explained. It is stressed that the connection of the mean electromotive force with the mean magnetic field and its first spatial derivatives is in general neither local nor instantaneous and that quite a few claims concerning pretended failures of the mean-field concept result from ignoring this aspect. In addition to the mean-field dynamo mechanisms of $\alpha^2$ and $\alpha$ $\Omega$ type several others are considered. Much progress in mean-field electrodynamics and magnetohydrodynamics results from the test-field method for calculating the coefficients that determine the connection of the mean electromotive force with the mean magnetic field. As an important example the memory effect in homogeneous isotropic turbulence is explained. In magnetohydrodynamic turbulence there is the possibility of a mean electromotive force that is primarily independent of the mean magnetic field and labeled as Yoshizawa effect. Despite of many efforts there is so far no convincing comprehensive theory of $\alpha$ quenching, that is, the reduction of the $\alpha$ effect with growing mean magnetic field, and of the saturation of mean-field dynamos. Steps toward such a theory are explained. Finally, some remarks on laboratory experiments with dynamos are made. 	
1403.6162v1	http://arxiv.org/pdf/1403.6162v1	2014	Entropy is in Flux	Leo P. Kadanoff	  The science of thermodynamics was put together in the Nineteenth Century to describe large systems in equilibrium. One part of thermodynamics defines entropy for equilibrium systems and demands an ever-increasing entropy for non-equilibrium ones. However, starting with the work of Ludwig Boltzmann in 1872, and continuing to the present day, various models of non-equilibrium behavior have been put together with the specific aim of generalizing the concept of entropy to non-equilibrium situations. This kind of entropy has been termed {\em kinetic entropy} to distinguish it from the thermodynamic variety. Knowledge of kinetic entropy started from Boltzmann's insight about his equation for the time dependence of gaseous systems. In this paper, his result is stated as a definition of kinetic entropy in terms of a local equation for the entropy density. This definition is then applied to Landau's theory of the Fermi liquid thereby giving the kinetic entropy within that theory.   Entropy has been defined and used for a wide variety of situations in which a condensed matter system has been allowed to relax for a sufficient period so that the very most rapid fluctuations have been ironed out. One of the broadest applications of non-equilibrium analysis considers quantum degenerate systems using Martin-Schwinger Green's functions\cite{MS} as generalized of Wigner functions, $g^<$ and $g^>$. This paper describes once again these how the quantum kinetic equations for these functions give locally defined conservation laws for mass momentum and energy. In local thermodynamic equilibrium, this kinetic theory enables a reasonable local definition of entropy density. However, when the system is outside of local equilibrium, this definition fails. It is speculated that quantum entanglement is the source of this failure. 	
1404.0696v1	http://arxiv.org/pdf/1404.0696v1	2014	D-P2P-Sim+:A Novel Distributed Framework for P2P Protocols Performance   Testing	S. Sioutas|E. Sakkopoulos|A. Panaretos|D. Tsoumakos|P. Gerolymatos|G. Tzimas|Y. Manolopoulos	  In recent IoT (Internet of Things) and Web 2.0 technologies, a critical problem arises with respect to storing and processing the large amount of collected data. In this paper we develop and evaluate distributed infrastructures for storing and processing large amount of such data. We present a distributed framework that supports customized deployment of a variety of indexing engines over million-node overlays. The proposed framework provides the appropriate integrated set of tools that allows applications processing large amount of data, to evaluate and test the performance of various application protocols for very large scale deployments (multi million nodes - billions of keys). The key aim is to provide the appropriate environment that contributes in taking decisions regarding the choice of the protocol in storage P2P systems for a variety of big data applications. Using lightweight and efficient collection mechanisms, our system enables real-time registration of multiple measures, integrating support for real-life parameters such as node failure models and recovery strategies. Experiments have been performed at the PlanetLab network and at a typical research laboratory in order to verify scalability and show maximum re-usability of our setup. D-P2P-Sim+ framework is publicly available at http://code.google.com/p/d-p2p-sim/downloads/list. 	
1404.2210v2	http://arxiv.org/pdf/1404.2210v2	2014	A kinematic study of energy barriers to crack formation in graphene tilt   boundaries	Matthew Daly|Chandra Veer Singh	  Recent experimental studies have observed a surprisingly wide range of strengths in polycrystalline graphene. Previous computational investigations of graphene tilt boundaries have highlighted the role of interfacial topology in determining mechanical properties. However, a rigorous characterization of deformation energy barriers is lacking, which precludes direct comparison to the available experimental data. In the current study, molecular dynamics tensile studies are performed to quantify kinematic effects on failure initiation in a wide range of graphene tilt boundaries. Specifically, the process of crack formation is investigated to provide a conservative estimate of strength at experimental loading rates. Contrary to previous studies, significant strain rate sensitivity is observed, resulting in reductions of crack formation stresses on the order of 7 to 33%. Activation energies of crack formation are calculated in the range of 0.58 to 2.07 eV based on an Arrhenius relation that is fit to the collected simulation data. Physically, the magnitude of activation energies in graphene tilt boundaries are found to be linearly correlated to the pre-stress found at the critical bonds in graphene tilt boundaries. Predictions reported in the present study provide a possible explanation for the wide range of strengths experimentally observed in polycrystalline graphene and greatly improve upon current theoretical estimates. 	
1404.2713v1	http://arxiv.org/pdf/1404.2713v1	2014	Transaction Handling in COM, EJB and .NET	Ditmar Parmeza|Miraldi Fifo	  The technology evolution has shown a very impressive performance in the last years by introducing several technologies that are based on the concept of component. As time passes, new versions of Component- Based technologies are released in order to improve services provided by previous ones. One important issue that regards these technologies is transactional activity. Transactions are important because they consist in sending different small amounts of information collected properly in a single combined unit which makes the process simpler, less expensive and also improves the reliability of the whole system, reducing its chances to go through possible failures. Different Component-Based technologies offer different ways of handling transactions. In this paper, we will review and discuss how transactions are handled in three of them: COM, EJB and .NET. It can be expected that .NET offers more efficient mechanisms due to the fact of being released later than the other two technologies. Nevertheless, COM and EJB are still present in the market and their services are still widely used. Comparing transaction handling in these technologies will be helpful to analyze the advantages and disadvantages of each of them. This comparison and evaluation will be seen in two main perspectives: performance and security. 	
1405.3558v1	http://arxiv.org/pdf/1405.3558v1	2014	Aspects of Statistical Physics in Computational Complexity	Stefano Gogioso	  The aim of this review paper is to give a panoramic of the impact of spin glass theory and statistical physics in the study of the K-sat problem. The introduction of spin glass theory in the study of the random K-sat problem has indeed left a mark on the field, leading to some groundbreaking descriptions of the geometry of its solution space, and helping to shed light on why it seems to be so hard to solve.   Most of the geometrical intuitions have their roots in the Sherrington-Kirkpatrick model of spin glass. We'll start Chapter 2 by introducing the model from a mathematical perspective, presenting a selection of rigorous results and giving a first intuition about the cavity method. We'll then switch to a physical perspective, to explore concepts like pure states, hierarchical clustering and replica symmetry breaking.   Chapter 3 will be devoted to the spin glass formulation of K-sat, while the most important phase transitions of K-sat (clustering, condensation, freezing and SAT/UNSAT) will be extensively discussed in Chapter 4, with respect their complexity, free-entropy density and the Parisi 1RSB parameter.   The concept of algorithmic barrier will be presented in Chapter 5 and exemplified in detail on the Belief Propagation (BP) algorithm. The BP algorithm will be introduced and motivated, and numerical analysis of a BP-guided decimation algorithm will be used to show the role of the clustering, condensation and freezing phase transitions in creating an algorithmic barrier for BP.   Taking from the failure of BP in the clustered and condensed phases, Chapter 6 will finally introduce the Cavity Method to deal with the shattering of the solution space, and present its application to the development of the Survey Propagation algorithm. 	
1405.3704v1	http://arxiv.org/pdf/1405.3704v1	2014	Internal stresses and breakup of rigid isostatic aggregates in   homogeneous and isotropic turbulence	Jeremias De Bona|Alessandra S. Lanotte|Marco Vanni	  By characterising the hydrodynamic stresses generated by statistically homogeneous and isotropic turbulence in rigid aggregates, we estimate theoretically the rate of turbulent breakup of colloidal aggregates and the size distribution of the formed fragments. The adopted method combines Direct Numerical Simulation of the turbulent field with a Discrete Element Method based on Stokesian dynamics. In this way, not only the mechanics of the aggregate is modelled in detail, but the internal stresses are evaluated while the aggregate is moving in the turbulent flow. We examine doublets and cluster-cluster isostatic aggregates, where the failure of a single contact leads to the rupture of the aggregate and breakup occurs when the tensile force at a contact exceeds the cohesive strength of the bond. Due to the different role of the internal stresses, the functional relationship between breakup frequency and turbulence dissipation rate is very different in the two cases. In the limit of very small and very large values, the frequency of breakup scales exponentially with the turbulence dissipation rate for doublets, while it follows a power law for cluster-cluster aggregates. For the case of large isostatic aggregates it is confirmed that the proper scaling length for maximum stress and breakup is the radius of gyration. The cumulative fragment distribution function is nearly independent of the mean turbulence dissipation rate and can be approximated by the sum of a small erosive component and a term that is quadratic with respect to fragment size. 	
1408.0640v1	http://arxiv.org/pdf/1408.0640v1	2014	Reconstruction of disease transmission rates: applications to measles,   dengue, and influenza	Alexander Lange	  Transmission rates are key in understanding the spread of infectious diseases. Using the framework of compartmental models, we introduce a simple method that enables us to reconstruct time series of transmission rates directly from incidence or disease-related mortality data. The reconstruction exploits differential equations, which model the time evolution of infective stages and strains. Being sensitive to initial values, the method produces asymptotically correct solutions. The computations are fast, with time complexity being quadratic. We apply the reconstruction to data of measles (England and Wales, 1948-67), dengue (Thailand, 1982-99), and influenza (U.S., 1910-27). The Measles example offers comparison with earlier work. Here we re-investigate reporting corrections, include and exclude demographic information. The dengue example deals with the failure of vector-control measures in reducing dengue hemorrhagic fever (DHF) in Thailand. Two competing mechanisms have been held responsible: strain interaction and demographic transitions. Our reconstruction reveals that both explanations are possible, showing that the increase in DHF cases is consistent with decreasing transmission rates resulting from reduced vector counts. The flu example focuses on the 1918/19 pandemic, examining the transmission rate evolution for an invading strain. Our analysis indicates that the pandemic strain could have circulated in the population for many months before the pandemic was initiated by an event of highly increased transmission. 	
1409.1652v2	http://arxiv.org/pdf/1409.1652v2	2014	Elementary analysis of interferometers for wave-particle duality test   and the perspective of going beyond the complementarity principle	Zhi-Yuan Li	  Wave-particle duality and complementarity principle stand at the conceptual core of quantum theory in its orthodox Copenhagen interpretation. They imply that the wave behavior and particle behavior of quantum objects are mutually exclusive to each other in experimental observation. Here we make a systematic analysis using the elementary methodology of quantum mechanics upon   Young`s two-slit interferometer and Mach-Zehnder two-arm interferometer with the focus placed on how to measure the interference pattern (wave nature) and which-way information (particle nature) of quantum objects. We design several schemes to simultaneously acquire the which-way information for an individual quantum object and the high-contrast interference pattern for an ensemble of these quantum objects by placing two sets of measurement instrument that are well separated in space and whose perturbation on each other is negligibly small within the interferometer at the same time. Yet, improper arrangement and cooperation of these two sets of measurement instrument in the interferometer would lead to failure of simultaneous observation of wave and particle behavior.   The internal freedoms of quantum object could be harnessed to probe both the which-way information and interference pattern for the center-of-mass motion. That quantum objects can behave beyond the wave-particle duality and complementarity principle would stimulate new conceptual examination and exploration of quantum theory at a deeper level. 	
1409.3837v1	http://arxiv.org/pdf/1409.3837v1	2014	Instability and network effects in innovative markets	Paolo Sgrignoli|Elena Agliari|Raffaella Burioni|Augusto Schianchi	  We consider a network of interacting agents and we model the process of choice on the adoption of a given innovative product by means of statistical-mechanics tools. The modelization allows us to focus on the effects of direct interactions among agents in establishing the success or failure of the product itself. Mimicking real systems, the whole population is divided into two sub-communities called, respectively, Innovators and Followers, where the former are assumed to display more influence power. We study in detail and via numerical simulations on a random graph two different scenarios: no-feedback interaction, where innovators are cohesive and not sensitively affected by the remaining population, and feedback interaction, where the influence of followers on innovators is non negligible. The outcomes are markedly different: in the former case, which corresponds to the creation of a niche in the market, Innovators are able to drive and polarize the whole market. In the latter case the behavior of the market cannot be definitely predicted and become unstable. In both cases we highlight the emergence of collective phenomena and we show how the final outcome, in terms of the number of buyers, is affected by the concentration of innovators and by the interaction strengths among agents. 	
1409.6883v1	http://arxiv.org/pdf/1409.6883v1	2014	Performance Analysis of Faults Detection in Wind Turbine Generator Based   on High-Resolution Frequency Estimation Methods	Saad Chakkor|Mostafa Baghouri|Abderrahmane Hajraoui	  Electrical energy production based on wind power has become the most popular renewable resources in the recent years because it gets reliable clean energy with minimum cost. The major challenge for wind turbines is the electrical and the mechanical failures which can occur at any time causing prospective breakdowns and damages and therefore it leads to machine downtimes and to energy production loss. To circumvent this problem, several tools and techniques have been developed and used to enhance fault detection and diagnosis to be found in the stator current signature for wind turbines generators. Among these methods, parametric or super-resolution frequency estimation methods, which provides typical spectrum estimation, can be useful for this purpose. Facing on the plurality of these algorithms, a comparative performance analysis is made to evaluate robustness based on different metrics: accuracy, dispersion, computation cost, perturbations and faults severity. Finally, simulation results in MATLAB with most occurring faults indicate that ESPRIT and R-MUSIC algorithms have high capability of correctly identifying the frequencies of fault characteristic components, a performance ranking had been carried out to demonstrate the efficiency of the studied methods in faults detecting. 	
1410.0117v2	http://arxiv.org/pdf/1410.0117v2	2014	Coupling Top-down and Bottom-up Methods for 3D Human Pose and Shape   Estimation from Monocular Image Sequences	Atul Kanaujia	  Until recently Intelligence, Surveillance, and Reconnaissance (ISR) focused on acquiring behavioral information of the targets and their activities. Continuous evolution of intelligence being gathered of the human centric activities has put increased focus on the humans, especially inferring their innate characteristics - size, shapes and physiology. These bio-signatures extracted from the surveillance sensors can be used to deduce age, ethnicity, gender and actions, and further characterize human actions in unseen scenarios. However, recovery of pose and shape of humans in such monocular videos is inherently an ill-posed problem, marked by frequent depth and view based ambiguities due to self-occlusion, foreshortening and misalignment. The likelihood function often yields a highly multimodal posterior that is difficult to propagate even using the most advanced particle filtering(PF) algorithms. Motivated by the recent success of the discriminative approaches to efficiently predict 3D poses directly from the 2D images, we present several principled approaches to integrate predictive cues using learned regression models to sustain multimodality of the posterior during tracking. Additionally, these learned priors can be actively adapted to the test data using a likelihood based feedback mechanism. Estimated 3D poses are then used to fit 3D human shape model to each frame independently for inferring anthropometric bio-signatures. The proposed system is fully automated, robust to noisy test data and has ability to swiftly recover from tracking failures even after confronting with significant errors. We evaluate the system on a large number of monocular human motion sequences. 	
1410.8238v3	http://arxiv.org/pdf/1410.8238v3	2015	The Slicing Theory of Quantum Measurement: Derivation of Transient Many   Worlds Behavior	Clifford Chafin	  An emergent theory of quantum measurement arises directly by considering the particular subset of many body wavefunctions that can be associated with classical condensed matter and its interaction with delocalized wavefunctions. This transfers questions of the "strangeness" of quantum mechanics from the wavefunction to the macroscopic material itself. An effectively many-worlds picture of measurement results for long times and induces a natural arrow of time. The challenging part is then justifying why our macroscopic world is dominated by such far-from-eigenstate matter. Condensing cold mesoscopic clusters provide a pathway to a partitioning of a highly correlated many body wavefunction to long lasting islands composed of classical-like bodies widely separated in Fock space. Low mass rapidly delocalizing matter that recombines with the solids "slice" the system into a set of nearby yet very weakly interacting subsystems weighted according to the Born statistics and yields a kind of many worlds picture but with the possibility of revived phase interference on iterative particle desorption, delocalization and readsorption. A proliferation of low energy photons competes with such a possibility. Causality problems associated with correlated quantum measurement are resolved and conserved quantities are preserved for the overall many body function despite their failure in each observer's bifurcating "slice-path." The necessity of such a state for a two state logic and reliable discrete state machine suggests that later stages of the universe's evolution will destroy the physical underpinnings required for consciousness and the arrow of time even without heat-death or atomic destruction. Some exotic possibilities outside the domain of usual quantum measurement are considered such as measurement with delocalized devices and revival of information from past measurements. 	
1411.1020v1	http://arxiv.org/pdf/1411.1020v1	2014	Model of deep non-volcanic tremor part II: episodic tremor and slip	Naum I. Gershenzon|Gust Bambakidis	  Bursts of tremor accompany a moving slip pulse in Episodic Tremor and Slip (ETS) events. The sources of this non-volcanic tremor (NVT) are largely unknown. We have developed a model describing the mechanism of NTV generation. According to this model, NTV is a reflection of resonant-type oscillations excited in a fault at certain depth ranges. From a mathematical viewpoint, tremor (phonons) and slip pulses (solitons) are two different solutions of the sine-Gordon equation describing frictional processes inside a fault. In an ETS event, a moving slip pulse generates tremor due to interaction with structural heterogeneities in a fault and to failures of small asperities. Observed tremor parameters, such as central frequency and frequency attenuation curve, are associated with fault parameters and conditions, such as elastic modulus, effective normal stress, penetration hardness and friction. Model prediction of NTV frequency content is consistent with observations. In the framework of this model it is possible to explain the complicated pattern of tremor migration, including rapid tremor propagation and reverse tremor migration. Migration along the strike direction is associated with movement of the slip pulse. Rapid tremor propagation in the slip-parallel direction is associated with movement of kinks along a 2D slip pulse. A slip pulse, pinned in some places, can fragment into several pulses, causing tremor associated with some of these pulse fragments to move opposite to the main propagation direction. The model predicts that the frequency content of tremor during an ETS event is slightly different from the frequency content of ambient tremor and tremor triggered by earthquakes. 	
1411.2873v1	http://arxiv.org/pdf/1411.2873v1	2014	Improving Robustness of Next-Hop Routing	Glencora Borradaile|W. Sean Kennedy|Gordon Wilfong|Lisa Zhang	  A weakness of next-hop routing is that following a link or router failure there may be no routes between some source-destination pairs, or packets may get stuck in a routing loop as the protocol operates to establish new routes. In this article, we address these weaknesses by describing mechanisms to choose alternate next hops.   Our first contribution is to model the scenario as the following {\sc tree augmentation} problem. Consider a mixed graph where some edges are directed and some undirected. The directed edges form a spanning tree pointing towards the common destination node. Each directed edge represents the unique next hop in the routing protocol. Our goal is to direct the undirected edges so that the resulting graph remains acyclic and the number of nodes with outdegree two or more is maximized. These nodes represent those with alternative next hops in their routing paths.   We show that {\sc tree augmentation} is NP-hard in general and present a simple $\frac{1}{2}$-approximation algorithm. We also study 3 special cases. We give exact polynomial-time algorithms for when the input spanning tree consists of exactly 2 directed paths or when the input graph has bounded treewidth. For planar graphs, we present a polynomial-time approximation scheme when the input tree is a breadth-first search tree. To the best of our knowledge, {\sc tree augmentation} has not been previously studied. 	
1412.3148v2	http://arxiv.org/pdf/1412.3148v2	2016	Coarse Grained Quantum Dynamics	Cesar Agon|Vijay Balasubramanian|Skyler Kasko|Albion Lawrence	  We consider coarse graining a quantum system divided between short distance and long distance degrees of freedom, which are coupled by the Hamiltonian. Observations using purely long distance observables can be described by the reduced density matrix that arises from tracing out the short-distance observables. The dynamics of this density matrix is that of an open quantum system, and is nonlocal in time, on the order of some short time scale. We describe these dynamics in a model system with a simple hierarchy of energy gaps $\Delta E_{UV} > \Delta E_{IR}$, in which the coupling between high-and low-energy degrees of freedom is treated to second order in perturbation theory. We then describe the equations of motion under suitable time averaging, reflecting the limited time resolution of actual experiments, and find an expansion of the master equation in powers of $\Delta E_{IR}/\Delta E_{UV}$, in which the failure of the system to be Hamiltonian or even Markovian appears at higher orders in this ratio. We compute the evolution of the density matrix in two specific examples -- coupled spins, and linearly coupled simple harmonic oscillators. Finally, we discuss the evolution of the density matrix using the path integral approach, computing the Feynman-Vernon influence functional for the IR degrees of freedom in perturbation theory, and argue that this influence functional is the correct analog of the Wilsonian effective action for this problem. 	
1502.07309v2	http://arxiv.org/pdf/1502.07309v2	2015	Dimensional transitions in thermodynamic properties of ideal   Maxwell-Boltzmann gases	Alhun Aydin|Altug Sisman	  An ideal Maxwell-Boltzmann gas confined in various rectangular nano domains is considered under quantum size effects. Thermodynamic quantities are calculated from their relations with partition function which consists of triple infinite summations over momentum states in each direction. To get analytical expressions, summations are converted to integrals for macro systems by continuum approximation which fails at nanoscales. To avoid both from the numerical calculation of summations and the failure of their integral approximations at nanoscale, a method which gives an analytical expression for single particle partition function (SPPF) is proposed. It's shown that dimensional transition in momentum space occurs at certain magnitude of confinement. Therefore, to represent SPPF by lower-dimensional analytical expressions becomes possible rather than numerical calculation of summations. Considering rectangular domains with different aspect ratios, comparison of the results of derived expressions with those of summation forms of SPPF is done. It's shown that analytical expressions for SPPF give very precise results with maximum relative errors of around 1%, 2% and 3% at just the transition point for single, double and triple transitions respectively. Based on dimensional transitions, expressions for free energy, entropy, internal energy, chemical potential, heat capacity and pressure are given analytically valid for any scale. 	
1503.03944v2	http://arxiv.org/pdf/1503.03944v2	2015	A comprehensive lattice-stability limit surface for graphene	Sandeep Kumar|David Parks	  The limits of reversible deformation in graphene under various loadings are examined using lattice-dynamical stability analysis. This information is then used to construct a comprehensive lattice-stability limit surface for graphene, which provides an analytical description of incipient lattice instabilities of \textit{all kinds}, for arbitrary deformations, parametrized in terms of symmetry-invariants of strain/stress. Symmetry-invariants allow obtaining an accurate parametrization with a minimal number of coefficients. Based on this limit surface, we deduce a general continuum criterion for the onset of all kinds of lattice-stabilities in graphene: an instability appears when the magnitude of the deviatoric strain $\gamma$ reaches a critical value $\gamma^c$ which depends upon the mean hydrostatic strain $\bar {\mathcal E}$ and the directionality $\theta$ of the deviatoric stretch. We also distinguish between the distinct regions of the limit surface that correspond to fundamentally-different mechanisms of lattice instabilities in graphene, such as structural vs material instabilities, and long-wave (elastic) vs short-wave instabilities. Utility of this limit surface is demonstrated in assessment of incipient failures in defect-free graphene via its implementation in a continuum Finite Elements Analysis (FEA). The resulting scheme enables on-the-fly assessments of not only the macroscopic conditions (e.g., load; deflection) but also the microscopic conditions (e.g., local stress/strain; spatial location, temporal proximity, and nature of incipient lattice instability) at which an instability occurs in a defect-free graphene sheet subjected to an arbitrary loading condition. 	
1503.04058v1	http://arxiv.org/pdf/1503.04058v1	2015	Optimal redundancy against disjoint vulnerabilities in networks	Sebastian M. Krause|Michael M. Danziger|Vinko Zlatić	  Redundancy is commonly used to guarantee continued functionality in networked systems. However, often many nodes are vulnerable to the same failure or adversary. A "backup" path is not sufficient if both paths depend on nodes which share a vulnerability.For example, if two nodes of the Internet cannot be connected without using routers belonging to a given untrusted entity, then all of their communication-regardless of the specific paths utilized-will be intercepted by the controlling entity.In this and many other cases, the vulnerabilities affecting the network are disjoint: each node has exactly one vulnerability but the same vulnerability can affect many nodes. To discover optimal redundancy in this scenario, we describe each vulnerability as a color and develop a "color-avoiding percolation" which uncovers a hidden color-avoiding connectivity. We present algorithms for color-avoiding percolation of general networks and an analytic theory for random graphs with uniformly distributed colors including critical phenomena. We demonstrate our theory by uncovering the hidden color-avoiding connectivity of the Internet. We find that less well-connected countries are more likely able to communicate securely through optimally redundant paths than highly connected countries like the US. Our results reveal a new layer of hidden structure in complex systems and can enhance security and robustness through optimal redundancy in a wide range of systems including biological, economic and communications networks. 	
1503.04655v1	http://arxiv.org/pdf/1503.04655v1	2015	Percolation in real interdependent networks	Filippo Radicchi	  The function of a real network depends not only on the reliability of its own components, but is affected also by the simultaneous operation of other real networks coupled with it. Robustness of systems composed of interdependent network layers has been extensively studied in recent years. However, the theoretical frameworks developed so far apply only to special models in the limit of infinite sizes. These methods are therefore of little help in practical contexts, given that real interconnected networks have finite size and their structures are generally not compatible with those of graph toy models. Here, we introduce a theoretical method that takes as inputs the adjacency matrices of the layers to draw the entire phase diagram for the interconnected network, without the need of actually simulating any percolation process. We demonstrate that percolation transitions in arbitrary interdependent networks can be understood by decomposing these system into uncoupled graphs: the intersection among the layers, and the remainders of the layers. When the intersection dominates the remainders, an interconnected network undergoes a continuous percolation transition. Conversely, if the intersection is dominated by the contribution of the remainders, the transition becomes abrupt even in systems of finite size. We provide examples of real systems that have developed interdependent networks sharing a core of "high quality" edges to prevent catastrophic failures. 	
1503.07905v1	http://arxiv.org/pdf/1503.07905v1	2015	D3-Tree: A Dynamic Distributed Deterministic Load - Balancer for   decentralized tree structures	Efrosini Sourla|Spyros Sioutas|Kostas Tsichlas|Christos Zaroliagis	  In this work, we propose D3-Tree, a dynamic distributed deterministic structure for data management in decentralized networks. We present in brief the theoretical algorithmic analysis, in which our proposed structure is based on, and we describe thoroughly the key aspects of the implementation. Conducting experiments, we verify that the implemented structure outperforms other well-known hierarchical tree-based structures, since it provides better complexities regarding load-balancing operations. More specifically, the structure achieves a logarithmic amortized bound, using an efficient deterministic load-balancing mechanism, which is general enough to be applied to other hierarchical tree-based structures. Moreover, we investigate the structure's fault tolerance, which hasn't been sufficiently tackled in previous work, both theoretically and through rigorous experimentation. We prove that D3-Tree is highly fault tolerant, since, even for massive node failures, it achieves a significant success rate in element queries. Afterwards we go one step further, in order to achieve sub-logarithmic complexity and propose the ART+ structure (Autonomous Range Tree), exploiting the excellent performance of D3-Tree. ART+ is a fully dynamic and fault-tolerant structure, which achieves sub-logarithmic performance for query and update operations and performs load-balancing in sub-logarithmic amortized cost. 	
1506.06045v1	http://arxiv.org/pdf/1506.06045v1	2015	Data compression for the First G-APD Cherenkov Telescope	M. L. Ahnen|M. Balbo|M. Bergmann|A. Biland|T. Bretz|J. Buß|D. Dorner|S. Einecke|J. Freiwald|C. Hempfling|D. Hildebrand|G. Hughes|W. Lustermann|E. Lyard|K. Mannheim|K. Meier|S. Mueller|D. Neise|A. Neronov|A. -K. Overkemping|A. Paravac|F. Pauss|W. Rhode|T. Steinbring|F. Temme|J. Thaele|S. Toscano|P. Vogler|R. Walter|A. Wilbert	  The First Geiger-mode Avalanche photodiode (G-APD) Cherenkov Telescope (FACT) has been operating on the Canary island of La Palma since October 2011. Operations were automated so that the system can be operated remotely. Manual interaction is required only when the observation schedule is modified due to weather conditions or in case of unexpected events such as a mechanical failure. Automatic operations enabled high data taking efficiency, which resulted in up to two terabytes of FITS files being recorded nightly and transferred from La Palma to the FACT archive at ISDC in Switzerland. Since long term storage of hundreds of terabytes of observations data is costly, data compression is mandatory. This paper discusses the design choices that were made to increase the compression ratio and speed of writing of the data with respect to existing compression algorithms.   Following a more detailed motivation, the FACT compression algorithm along with the associated I/O layer is discussed. Eventually, the performances of the algorithm is compared to other approaches. 	
1507.04381v2	http://arxiv.org/pdf/1507.04381v2	2015	Complete Characterization of Stability of Cluster Synchronization in   Complex Dynamical Networks	Francesco Sorrentino|Louis M. Pecora|Aaron M. Hagerstrom|Thomas E. Murphy|Rajarshi Roy	  Synchronization is an important and prevalent phenomenon in natural and engineered systems. In many dynamical networks, the coupling is balanced or adjusted in order to admit global synchronization, a condition called Laplacian coupling. Many networks exhibit incomplete synchronization, where two or more clusters of synchronization persist, and computational group theory has recently proved to be valuable in discovering these cluster states based upon the topology of the network. In the important case of Laplacian coupling, additional synchronization patterns can exist that would not be predicted from the group theory analysis alone. The understanding of how and when clusters form, merge, and persist is essential for understanding collective dynamics, synchronization, and failure mechanisms of complex networks such as electric power grids, distributed control networks, and autonomous swarming vehicles. We describe here a method to find and analyze all of the possible cluster synchronization patterns in a Laplacian-coupled network, by applying methods of computational group theory to dynamically-equivalent networks. We present a general technique to evaluate the stability of each of the dynamically valid cluster synchronization patterns. Our results are validated in an electro-optic experiment on a 5 node network that confirms the synchronization patterns predicted by the theory. 	
1507.06278v3	http://arxiv.org/pdf/1507.06278v3	2015	Some Nearly Quantum Theories	Howard Barnum|Matthew A. Graydon|Alexander Wilce	  We consider possible non-signaling composites of probabilistic models based on euclidean Jordan algebras. Subject to some reasonable constraints, we show that no such composite exists having the exceptional Jordan algebra as a direct summand. We then construct several dagger compact categories of such Jordan-algebraic models. One of these neatly unifies real, complex and quaternionic mixed-state quantum mechanics, with the exception of the quaternionic "bit". Another is similar, except in that (i) it excludes the quaternionic bit, and (ii) the composite of two complex quantum systems comes with an extra classical bit. In both of these categories, states are morphisms from systems to the tensor unit, which helps give the categorical structure a clear operational interpretation. A no-go result shows that the first of these categories, at least, cannot be extended to include spin factors other than the (real, complex, and quaternionic) quantum bits, while preserving the representation of states as morphisms. The same is true for attempts to extend the second category to even-dimensional spin-factors. Interesting phenomena exhibited by some composites in these categories include failure of local tomography, supermultiplicativity of the maximal number of mutually distinguishable states, and mixed states whose marginals are pure. 	
1508.05288v3	http://arxiv.org/pdf/1508.05288v3	2015	Dynamics of Human Cooperation in Economic Games	Martin Spanknebel|Klaus Pawelzik	  Human decision behaviour is quite diverse. In many games humans on average do not achieve maximal payoff and the behaviour of individual players remains inhomogeneous even after playing many rounds. For instance, in repeated prisoner dilemma games humans do not always optimize their mean reward and frequently exhibit broad distributions of cooperativity. The reasons for these failures of maximization are not known. Here we show that the dynamics resulting from the tendency to shift choice probabilities towards previously rewarding choices in closed loop interaction with the strategy of the opponent can not only explain systematic deviations from 'rationality', but also reproduce the diversity of choice behaviours. As a representative example we investigate the dynamics of choice probabilities in prisoner dilemma games with opponents using strategies with different degrees of extortion and generosity. We find that already a simple model for human learning can account for a surprisingly wide range of human decision behaviours. It reproduces suppression of cooperation against extortionists and increasing cooperation when playing with generous opponents, explains the broad distributions of individual choices in ensembles of players, and predicts the evolution of individual subjects' cooperation rates over the course of the games. We conclude that important aspects of human decision behaviours are rooted in elementary learning mechanisms realised in the brain. 	
1509.06613v2	http://arxiv.org/pdf/1509.06613v2	2015	Stress channelling in extreme couple-stress materials Part I: Strong   ellipticity, wave propagation, ellipticity, and discontinuity relations	Panos A. Gourgiotis|Davide Bigoni	  Materials with extreme mechanical anisotropy are designed to work near a material instability threshold where they display stress channelling and strain localization, effects that can be exploited in several technologies. Extreme couple stress solids are introduced and for the first time systematically analyzed in terms of several material instability criteria: positive-definiteness of the strain energy (implying uniqueness of the mixed b.v.p.), strong ellipticity (implying uniqueness of the b.v.p. with prescribed kinematics on the whole boundary), plane wave propagation, ellipticity, and the emergence of discontinuity surfaces. Several new and unexpected features are highlighted: (i.) Ellipticity is mainly dictated by the 'Cosserat part' of the elasticity and (ii.) its failure is shown to be related to the emergence of discontinuity surfaces; (iii.) Ellipticity and wave propagation are not interdependent conditions (so that it is possible for waves not to propagate when the material is still in the elliptic range and, in very special cases, for waves to propagate when ellipticity does not hold). The proof that loss of ellipticity induces stress channelling, folding and faulting of an elastic Cosserat continuum (and the related derivation of the infinite-body Green's function under antiplane strain conditions) is deferred to Part II of this study. 	
1511.01583v2	http://arxiv.org/pdf/1511.01583v2	2016	From Brittle to Ductile: A Structure Dependent Ductility of Diamond   Nanothread	Haifei Zhan|Gang Zhang|Vincent BC Tan|Yuan Cheng|John M. Bell|Yong-Wei Zhang|Yuantong Gu	  As a potential building block for the next generation of devices or multifunctional materials that are spreading almost every technology sector, one-dimensional (1D) carbon nanomaterial has received intensive research interests. Recently, a new ultra-thin diamond nanothread (DNT) has joined this palette, which is a 1D structure with poly-benzene sections connected by Stone-Wales (SW) transformation defects. Using large-scale molecular dynamics simulations, we found that this sp3 bonded DNT can transit from a brittle to a ductile characteristic by varying the length of the poly-benzene sections, suggesting that DNT possesses entirely different mechanical responses than other 1D carbon allotropies. Analogously, the SW defects behave like a grain boundary that interrupts the consistency of the poly-benzene sections. For a DNT with a fixed length, the yield strength fluctuates in the vicinity of a certain value and is independent of the "grain size". On the other hand, both yield strength and yield strain show a clear dependence on the total length of DNT, which is due to the fact that the failure of the DNT is dominated by the SW defects. Its highly tunable ductility together with its ultra-light density and high Young's modulus makes diamond nanothread ideal for creation of extremely strong three-dimensional nano-architectures. 	
1511.04385v3	http://arxiv.org/pdf/1511.04385v3	2017	Factoring Safe Semiprimes with a Single Quantum Query	Frédéric Grosshans|Thomas Lawson|François Morain|Benjamin Smith	  Shor's factoring algorithm (SFA), by its ability to efficiently factor large numbers, has the potential to undermine contemporary encryption. At its heart is a process called order finding, which quantum mechanics lets us perform efficiently. SFA thus consists of a \emph{quantum order finding algorithm} (QOFA), bookended by classical routines which, given the order, return the factors. But, with probability up to $1/2$, these classical routines fail, and QOFA must be rerun. We modify these routines using elementary results in number theory, improving the likelihood that they return the factors.   The resulting quantum factoring algorithm is better than SFA at factoring safe semiprimes, an important class of numbers used in cryptography. With just one call to QOFA, our algorithm almost always factors safe semiprimes. As well as a speed-up, improving efficiency gives our algorithm other, practical advantages: unlike SFA, it does not need a randomly picked input, making it simpler to construct in the lab; and in the (unlikely) case of failure, the same circuit can be rerun, without modification.   We consider generalizing this result to other cases, although we do not find a simple extension, and conclude that SFA is still the best algorithm for general numbers (non safe semiprimes, in other words). Even so, we present some simple number theoretic tricks for improving SFA in this case. 	
1511.08661v3	http://arxiv.org/pdf/1511.08661v3	2016	Cascading failures in coupled networks with both inner-dependency and   inter-dependency links	Run-Ran Liu|Ming Li|Chun-Xiao Jia|Bing-Hong Wang	  We study the percolation in coupled networks with both inner-dependency and inter-dependency links, where the inner- and inter-dependency links represent the dependencies between nodes in the same or different networks, respectively. We find that when most of dependency links are inner- or inter-ones, the coupled networks system is fragile and makes a discontinuous percolation transition. However, when the numbers of two types of dependency links are close to each other, the system is robust and makes a continuous percolation transition. This indicates that the high density of dependency links could not always lead to a discontinuous percolation transition as the previous studies. More interestingly, although the robustness of the system can be optimized by adjusting the ratio of the two types of dependency links, there exists a critical average degree of the networks for coupled random networks, below which the crossover of the two types of percolation transitions disappears, and the system will always demonstrate a discontinuous percolation transition. We also develop an approach to analyze this model, which is agreement with the simulation results well. 	
1512.05180v1	http://arxiv.org/pdf/1512.05180v1	2015	Failure Mechanism of True 2D Granular Flows	Cuong T. Nguyen|Ha H. Bui|R. Fukagawa	  Most previous experimental investigations of two-dimensional (2D) granular column collapses have been conducted using three-dimensional (3D) granular materials in narrow horizontal channels (i.e., quasi-2D condition). Our recent research on 2D granular column collapses by using 2D granular materials (i.e., aluminum rods) has revealed results that differ markedly from those reported in the literature. We assume a 2D column with an initial height of h0 and initial width of d0, a defined as their ratio (a =h0/d0), a final height of h , and maximum run-out distance of d . The experimental data suggest that for the low a regime (a <0.65) the ratio of the final height to initial height is 1. However, for the high a regime (a >0.65), the ratio of a to (d-d0)/d0, h0/h , or d/d0 is expressed by power-law relations. In particular, the following power-function ratios (h0/h=1.42a^2/3 and d/d0=4.30a^0.72) are proposed for every a >0.65. In contrast, the ratio (d-d0)/d0=3.25a^0.96 only holds for 0.65< a< 1.5, whereas the ratio (d-d0)/d0=3.80a^0.73 holds for a>1.5. In addition, the influence of ground contact surfaces (hard or soft beds) on the final run-out distance and destruction zone of the granular column under true 2D conditions is investigated. 	
1512.05461v3	http://arxiv.org/pdf/1512.05461v3	2016	A Novel Material for In Situ Construction on Mars: Experiments and   Numerical Simulations	Lin Wan|Roman Wendner|Gianluca Cusatis	  A significant step in space exploration during the 21st century will be human settlement on Mars. Instead of transporting all the construction materials from Earth to the red planet with incredibly high cost, using Martian soil to construct a site on Mars is a superior choice. Knowing that Mars has long been considered a "sulfur-rich planet", a new construction material composed of simulated Martian soil and molten sulfur is developed. In addition to the raw material availability for producing sulfur concrete and a strength reaching similar or higher levels of conventional cementitious concrete, fast curing, low temperature sustainability, acid and salt environment resistance, 100% recyclability are appealing superior characteristics of the developed Martian Concrete. In this study, different percentages of sulfur are investigated to obtain the optimal mixing proportions. Three point bending, unconfined compression and splitting tests were conducted to determine strength development, strength variability, and failure mechanisms. The test results show that the strength of Martian Concrete doubles that of sulfur concrete utilizing regular sand. It is also shown that the particle size distribution plays an important role in the mixture's final strength. Furthermore, since Martian soil is metal rich, sulfates and, potentially, polysulfates are also formed during high temperature mixing, which might contribute to the high strength. The optimal mix developed as Martian Concrete has an unconfined compressive strength of above 50 MPa. The formulated Martian Concrete is simulated by the Lattice Discrete Particle Model (LDPM), which exhibits excellent ability in modeling the material response under various loading conditions. 	
1512.07343v1	http://arxiv.org/pdf/1512.07343v1	2015	Thermal stability of a free nanotube from single-layer black phosphorus	Kun Cai|Jing Wan|Ning Wei|Haifang Cai|Qing-Hua Qin	  Similar to the carbon nanotube fabricated from graphene sheet, a black phosphorus nanotube (BPNT) also can theoretically be produced by curling the rectangular single-layer black phosphorus (SLBP). In present study, the effect of thermal vibration of atoms on the failure of a BPNT is investigated using molecular dynamics simulations. Two types of double-shell BPNTs, which are obtained by curling the rectangular SLBP along its armchair/pucker direction and zigzag direction (in-plane normal) respectively, are involved in simulation. At finite temperature, a bond on the outer shell of tube is under tension due to both of curvature of tube and serious thermal vibration of atoms. As the length of a bond with such elongation approaches its critical value, i.e., 0.279 nm, or the smallest distance between two nonbonding phosphorus atoms is over 0.389nm caused by great variation of bond angle, the tube fails quickly. The critical stable states of either an armchair or a zigzag BPNT at finite temperature are calculated and compared. To achieve a stable BPNT with high robustness, the curvature of the tube should be reduced or the tube should work at a lower temperature. Only when the BPNT has structural stability, it has a potential application as a nanowire in a future nano electro-mechanical system (NEMS). 	
1603.02440v1	http://arxiv.org/pdf/1603.02440v1	2016	Simulation study of a rectifying bipolar ion channel: Detailed model   versus reduced model	Z. Ható|D. Boda|D. Gillespie|J. Vrabec|G. Rutkai|T. Kristóf	  We study a rectifying mutant of the OmpF porin ion channel using both all-atom and reduced models. The mutant was created by Miedema et al. [Nano Lett., 2007, 7, 2886] on the basis of the N-P semiconductor diode, in which an N-P junction is formed. The mutant contains a pore region with positive amino acids on the left-hand side and negative amino acids on the right-hand side. Experiments show that this mutant rectifies. Although we do not know the structure of this mutant, we can build an all-atom model for it on the basis of the structure of the wild type channel. Interestingly, molecular dynamics simulations for this all-atom model do not produce rectification. A reduced model that contains only the important degrees of freedom (the positive and negative amino acids and free ions in an implicit solvent), on the other hand, exhibits rectification. Our calculations for the reduced model (using the Nernst-Planck equation coupled to Local Equilibrium Monte Carlo simulations) reveal a rectification mechanism that is different from that seen for semiconductor diodes. The basic reason is that the ions are different in nature from electrons and holes (they do not recombine). We provide explanations for the failure of the all-atom model including the effect of all the other atoms in the system as a noise that inhibits the response of ions (that would be necessary for rectification) to the polarizing external field. 	
1604.06775v1	http://arxiv.org/pdf/1604.06775v1	2016	Spooky Action at No Distance: On the individuation of quantum mechanical   systems	David Weinbaum	  Recent experiments have perfectly verified the fact that quantum correlations between two entangled particles are stronger than any classical, local pre-quantum worldview allows. This is famously called the EPR paradox first conceived as a thought experiment and decades later realized in the lab. We discuss in depth the nature of the paradox and show that the problematics it presents is first and foremost epistemological. After briefly exploring resolutions to the paradox that after many decades of discourse still remain controversial, we argue that the paradox is rooted in the failure of our current metaphysical scheme, being the foundation of our knowledge, to accommodate and cohere our knowledge of the phenomena of entanglement. We then develop and make the case for a novel and more fundamental resolution of the paradox by changing the underlying metaphysical foundation from one based on individuals to a one based on individuation. We discuss in detail how in the light of this new scheme concepts central to the paradox such as realism, causality and locality are adjusted to the effect that the paradox is resolved without giving up these concepts so fundamental to our thinking. We conclude with a brief note about the important role of metaphysics to the progress of knowledge and our understanding of reality. 	
1605.03106v1	http://arxiv.org/pdf/1605.03106v1	2016	What is Physics: The individual and the universal, and seeing past the   noise	A. R. P. Rau	  Along with weaving together observations, experiments, and theoretical constructs into a coherent mesh of understanding of the world around us, physics over its past five centuries has continuously refined the base concepts on which the whole framework is built. In quantum physics, first in non-relativistic mechanics and later in quantum field theories, even familiar concepts of position, momentum, wave or particle, are derived constructs from the classical limit in which we live but not intrinsic to the underlying physics. Most crucially, the very idea of the individual, whether an object or an event, distinguished only in a mere label of identity from others identical to it in all the physics, exists only as an approximation, not an element of underlying reality. Failure to recognize this and seeking alternative explanations in many worlds or multiverses leads only to incoherent logic and incorrect physics.   As an example, in a physical system such as an atom in a particular state, physics deals with the universal system of all such atoms but makes no meaningful prediction of the position of an electron or the time of decay of any specific atom. Those are incidental, entirely random among all possible positions and times, even while physics makes very precise predictions for the distribution of the outcomes in measurements on atoms in that state. Physics deals with the universal, not the individual. 	
1605.08978v1	http://arxiv.org/pdf/1605.08978v1	2016	Quantile-based optimization under uncertainties using adaptive Kriging   surrogate models	M. Moustapha|B. Sudret|J. -M. Bourinet|B. Guillaume	  Uncertainties are inherent to real-world systems. Taking them into account is crucial in industrial design problems and this might be achieved through reliability-based design optimization (RBDO) techniques. In this paper, we propose a quantile-based approach to solve RBDO problems. We first transform the safety constraints usually formulated as admissible probabilities of failure into constraints on quantiles of the performance criteria. In this formulation, the quantile level controls the degree of conservatism of the design. Starting with the premise that industrial applications often involve high-fidelity and time-consuming computational models, the proposed approach makes use of Kriging surrogate models (a.k.a. Gaussian process modeling). Thanks to the Kriging variance (a measure of the local accuracy of the surrogate), we derive a procedure with two stages of enrichment of the design of computer experiments (DoE) used to construct the surrogate model. The first stage globally reduces the Kriging epistemic uncertainty and adds points in the vicinity of the limit-state surfaces describing the system performance to be attained. The second stage locally checks, and if necessary, improves the accuracy of the quantiles estimated along the optimization iterations. Applications to three analytical examples and to the optimal design of a car body subsystem (minimal mass under mechanical safety constraints) show the accuracy and the remarkable efficiency brought by the proposed procedure. 	
1606.04435v2	http://arxiv.org/pdf/1606.04435v2	2016	Adversarial Perturbations Against Deep Neural Networks for Malware   Classification	Kathrin Grosse|Nicolas Papernot|Praveen Manoharan|Michael Backes|Patrick McDaniel	  Deep neural networks, like many other machine learning models, have recently been shown to lack robustness against adversarially crafted inputs. These inputs are derived from regular inputs by minor yet carefully selected perturbations that deceive machine learning models into desired misclassifications. Existing work in this emerging field was largely specific to the domain of image classification, since the high-entropy of images can be conveniently manipulated without changing the images' overall visual appearance. Yet, it remains unclear how such attacks translate to more security-sensitive applications such as malware detection - which may pose significant challenges in sample generation and arguably grave consequences for failure.   In this paper, we show how to construct highly-effective adversarial sample crafting attacks for neural networks used as malware classifiers. The application domain of malware classification introduces additional constraints in the adversarial sample crafting problem when compared to the computer vision domain: (i) continuous, differentiable input domains are replaced by discrete, often binary inputs; and (ii) the loose condition of leaving visual appearance unchanged is replaced by requiring equivalent functional behavior. We demonstrate the feasibility of these attacks on many different instances of malware classifiers that we trained using the DREBIN Android malware data set. We furthermore evaluate to which extent potential defensive mechanisms against adversarial crafting can be leveraged to the setting of malware classification. While feature reduction did not prove to have a positive impact, distillation and re-training on adversarially crafted samples show promising results. 	
1606.05401v1	http://arxiv.org/pdf/1606.05401v1	2016	Manganite-based three level memristive devices with self-healing   capability	W. Román Acevedo|D. Rubi|J. Lecourt|U. Lüders|F. Gomez-Marlasca|P. Granell|F. Golmar|P. Levy	  We report on non-volatile memory devices based on multifunctional manganites. The electric field induced resistive switching of Ti/$La_{1/3}$$Ca_{2/3}$Mn$O_3$/n-Si devices is explored using different measurement protocols. We show that using current as the electrical stimulus (instead of standard voltage-controlled protocols) improves the electrical performance of our devices and unveils an intermediate resistance state. We observe three discrete resistance levels (low, intermediate and high), which can be set either by the application of current-voltage ramps or by means of single pulses. These states exhibit retention and endurance capabilities exceeding $10^4$ s and 70 cycles, respectively. We rationalize our experimental observations by proposing a mixed scenario were a metallic filament and a Si$O_x$ layer coexist, accounting for the observed resistive switching. Overall electrode area dependence and temperature dependent resistance measurements support our scenario. After device failure takes place, the system can be turned functional again by heating up to low temperature (120 C), a feature that could be exploited for the design of memristive devices with self-healing functionality. These results give insight into the existence of multiple resistive switching mechanisms in manganite-based memristive systems and provide strategies for controlling them. 	
1606.07193v2	http://arxiv.org/pdf/1606.07193v2	2016	A Unified Model for GRB Prompt Emission from Optical to $γ$-Rays:   Exploring GRBs as Standard Candles	S. Guiriec|C. Kouveliotou|D. H. Hartmann|J. Granot|K. Asano|P. Meszaros|R. Gill|N. Gehrels|J. McEnery	  The origin of prompt emission from gamma ray bursts remains to be an open question. Correlated prompt optical and gamma-ray emission observed in a handful of GRBs strongly suggests a common emission region, but failure to adequately fit the broadband GRB spectrum prompted the hypothesis of different emission mechanisms for the low- and high-energy radiations. We demonstrate that our multi-component model for GRB gamma-ray prompt emission provides an excellent fit to GRB 110205A from optical to gamma-ray energies. Our results show that the optical and highest gamma-ray emissions have the same spatial and spectral origin, which is different from the bulk of the X- and softest gamma-ray radiation. Finally, our accurate redshift estimate for GRB 110205A demonstrates promise for using GRBs as cosmological standard candles. 	
1606.08577v1	http://arxiv.org/pdf/1606.08577v1	2016	Reliability analysis of high-dimensional models using low-rank tensor   approximations	K. Konakli|B. Sudret	  Engineering and applied sciences use models of increasing complexity to simulate the behaviour of manufactured and physical systems. Propagation of uncertainties from the input to a response quantity of interest through such models may become intractable in cases when a single simulation is time demanding. Particularly challenging is the reliability analysis of systems represented by computationally costly models, because of the large number of model evaluations that are typically required to estimate small probabilities of failure. In this paper, we demonstrate the potential of a newly emerged meta-modelling technique known as low-rank tensor approximations to address this limitation. This technique is especially promising for high-dimensional problems because: (i) the number of unknowns in the generic functional form of the meta-model grows only linearly with the input dimension and (ii) such approximations can be constructed by relying on a series of minimization problems of small size independent of the input dimension. In example applications involving finite-element models pertinent to structural mechanics and heat conduction, low-rank tensor approximations built with polynomial bases are found to outperform the popular sparse polynomial chaos expansions in the estimation of tail probabilities when small experimental designs are used. It should be emphasized that contrary to methods particularly targeted to reliability analysis, the meta-modelling approach also provides a full probabilistic description of the model response, which can be used to estimate any statistical measure of interest. 	
1607.01510v3	http://arxiv.org/pdf/1607.01510v3	2018	Perturbation Theory for Arbitrary Coupling Strength ?	B. P. Mahapatra|N. B. Pradhan	  We present a \emph{new} formulation of perturbation theory for quantum systems, designated here as: `mean field perturbation theory'(MFPT), which is free from power-series-expansion in any physical parameter, including the coupling strength. Its application is thereby extended to deal with interactions of \textit{arbitrary} strength and to compute system-properties having non-analytic dependence on the coupling, thus overcoming the primary limitations of the `standard formulation of perturbation theory' ( SFPT). MFPT is defined by developing perturbation about a chosen input Hamiltonian, which is exactly solvable but which acquires the non-linearity and the analytic structure~(in the coupling-strength)~of the original interaction through a self-consistent, feedback mechanism. We demonstrate Borel-summability of MFPT for the case of the quartic- and sextic-anharmonic oscillators and the quartic double-well oscillator (QDWO) by obtaining uniformly accurate results for the ground state of the above systems for arbitrary physical values of the coupling strength. The results obtained for the QDWO may be of particular significance since `renormalon'-free, unambiguous results are achieved for its spectrum in contrast to the well-known failure of SFPT in this case.   \pacs{11.15.Bt,11.10.Jj,11.25.Db,12.38.Cy,03.65.Ge} 	
1608.07113v1	http://arxiv.org/pdf/1608.07113v1	2016	Effects of high-power laser irradiation on sub-superficial graphitic   layers in single crystal diamond	F. Picollo|S. Rubanov|C. Tomba|A. Battiato|E. Enrico|A. Perrat-Mabilon|C. Peaucelle|T. N. Tran Thi|L. Boarino|E. Gheeraert|P. Olivero	  We report on the structural modifications induced by a lambda = 532 nm ns-pulsed high-power laser on sub-superficial graphitic layers in single-crystal diamond realized by means of MeV ion implantation. A systematic characterization of the structures obtained under different laser irradiation conditions (power density, number of pulses) and subsequent thermal annealing was performed by different electron microscopy techniques. The main feature observed after laser irradiation is the thickening of the pre-existing graphitic layer. Cross sectional SEM imaging was performed to directly measure the thickness of the modified layers, and subsequent selective etching of the buried layers was employed to both assess their graphitic nature and enhance the SEM imaging contrast. In particular, it was found that for optimal irradiation parameters the laser processing induces a six-fold increase the thickness of sub superficial graphitic layers without inducing mechanical failures in the surrounding crystal. TEM microscopy and EELS spectroscopy allowed a detailed analysis of the internal structure of the laser irradiated layers, highlighting the presence of different nano graphitic and amorphous layers. The obtained results demonstrate the effectiveness and versatility of high-power laser irradiation for an accurate tuning of the geometrical and structural features of graphitic structures embedded in single crystal diamond, and open new opportunities in diamond fabrication. 	
1608.07499v1	http://arxiv.org/pdf/1608.07499v1	2016	Stem Cell Therapy for Alzheimer's Disease	Ankur Patel|Grishma joshi|Rupali Ugile	  The loss of neuronal cells in the central nervous system may happen in numerous neurodegenerative illnesses. Alzheimer's Disease (AD) is an intricate, irreversible, dynamic neurodegenerative sickness. It is the main source of age-related dementia, influencing roughly 5.3 million individuals in the United States alone. Promotion is a typical feeble ailment in individuals more than 65 years, bringing on disability described by decrease in memory, failure to learn and do every day exercises, intellectual weakness and influences the personal satisfaction of patients. Pathologic qualities of AD are an irregular development of specific proteins called Beta-amyloid "plaques" and Tau "Tangles" in the mind. Notwithstanding, current treatments against AD are just to calm manifestations and palliative yet are not the cure and a few promising medications competitors have fizzled in late clinical trials. There is consequently a critical need to enhance our comprehension for pathogenesis of this sickness, making new and creative prescient models with powerful treatments. As of late, stem cell treatment has been appeared to have a potential way to deal with different illnesses, including neurodegenerative disorders. In light of the far reaching nature of AD pathology, stem cell substitution procedures have been seen as an extraordinarily difficult and impossible treatment approach. Stem Cell may likewise offer an effective new way to deal with model and concentrate AD. Patient derived induced Pluripotent Stem Cells (iPSCs), for instance, may propel our comprehension of disease mechanism. In this review we will examine the capability of stem cells to help in these testing tries. 	
1608.08743v2	http://arxiv.org/pdf/1608.08743v2	2017	A Large Scale Analysis of Unreliable Stochastic Networks	Reza Aghajani|Philippe Robert|Wen Sun	  The problem of reliability of a large distributed system is analyzed via a new mathematical model. A typical framework is a system where a set of files are duplicated on several data servers. When one of these servers breaks down, all copies of files stored on it are lost. In this way, repeated failures may lead to losses of files. The efficiency of such a network is directly related to the performances of the mechanism used to duplicate files on servers. In this paper we study the evolution of the network using a natural duplication policy giving priority to the files with the least number of copies.   We investigate the asymptotic behavior of the network when the number $N$ of servers is large. The analysis is complicated by the large dimension of the state space of the empirical distribution of the state of the network. A stochastic model of the evolution of the network which has values in state space whose dimension does not depend on $N$ is introduced. Despite this description does not have the Markov property, it turns out that it is converging in distribution, when the number of nodes goes to infinity, to a nonlinear Markov process. The rate of decay of the network, which is the key characteristic of interest of these systems, can be expressed in terms of this asymptotic process. The corresponding mean-field convergence results are established. A lower bound on the exponential decay, with respect to time, of the fraction of the number of initial files with at least one copy is obtained. 	
1608.08786v2	http://arxiv.org/pdf/1608.08786v2	2016	Age-dependent Size Effect and Fracture Characteristics of Ultra High   Performance Concrete	Lin Wan|Roman Wendner|Gianluca Cusatis	  This paper presents an investigation of the age-dependent size effect and fracture characteristics of an ultra high performance concrete (UHPC). The study is based on a unique set of experimental data connecting aging tests for two curing protocols of one size and scaled size effect tests of one age. Both aging and size effect studies are performed on notched three point bending tests. Experimental data is augmented by state of the art simulations employing a recently developed discrete element based early-age computational framework. The framework is constructed by coupling a hygro-thermo-chemical (HTC) model and the Lattice Discrete Particle Model (LDPM) through a set of aging functions. The HTC component allows taking into account variable curing conditions and predicts the maturity of concrete. The mechanical component, LDPM, simulates the failure behavior of concrete at the length scale of major heterogeneities. After careful calibration and validation the mesoscale HTC-LDPM model is uniquely posed to perform predictive simulations. The ultimate flexural strengths from experiments and simulations are analyzed by the cohesive size effect curve (CSEC) method, and the classical size effect law (SEL). The fracture energies obtained by LDPM, CSEC, SEL, and cohesive crack analyses are compared and an aging formulation for fracture properties is proposed. Based on experiments, simulations, and size effect analyses, the age-dependence of size effect and the robustness of analytical size effect methods are evaluated. 	
1609.02305v2	http://arxiv.org/pdf/1609.02305v2	2018	Survey of Consistent Software-Defined Network Updates	Klaus-Tycho Foerster|Stefan Schmid|Stefano Vissicchio	  Computer networks have become a critical infrastructure. Designing dependable computer networks however is challenging, as such networks should not only meet strict requirements in terms of correctness, availability, and performance, but they should also be flexible enough to support fast updates, e.g., due to a change in the security policy, an increasing traffic demand, or a failure. The advent of Software-Defined Networks (SDNs) promises to provide such flexiblities, allowing to update networks in a fine-grained manner, also enabling a more online traffic engineering. In this paper, we present a structured survey of mechanisms and protocols to update computer networks in a fast and consistent manner. In particular, we identify and discuss the different desirable update consistency properties a network should provide, the algorithmic techniques which are needed to meet these consistency properties, their implications on the speed and costs at which updates can be performed. We also discuss the relationship of consistent network update problems to classic algorithmic optimization problems. While our survey is mainly motivated by the advent of Software-Defined Networks (SDNs), the fundamental underlying problems are not new, and we also provide a historical perspective of the subject. 	
1609.02535v1	http://arxiv.org/pdf/1609.02535v1	2016	Modeling the differentiation of A- and C-type baroreceptor firing   patterns	Jacob Sturdy|Johnny T Ottesen|Mette S Olufsen	  The baroreceptor neurons serve as the primary transducers of blood pressure for the autonomic nervous system and are thus critical in enabling the body to respond effectively to changes in blood pressure. These neurons can be separated into two types (A and C) based on the myelination of their axons and their distinct firing patterns elicited in response to specific pressure stimuli. This study has developed a comprehensive model of the afferent baroreceptor discharge built on physiological knowledge of arterial wall mechanics, firing rate responses to controlled pressure stimuli, and ion channel dynamics within the baroreceptor neurons. With this model, we were able to predict firing rates observed in previously published experiments in both A- and C-type neurons. These results were obtained by adjusting model parameters determining the maximal ion-channel conductances. The observed variation in the model parameters are hypothesized to correspond to physiological differences between A- and C-type neurons. In agreement with published experimental observations, our simulations suggest that a twofold lower potassium conductance in C-type neurons is responsible for the observed sustained basal firing, whereas a tenfold higher mechanosensitive conductance is responsible for the greater firing rate observed in A-type neurons. A better understanding of the difference between the two neuron types can potentially be used to gain more insight into the underlying pathophysiology facilitating development of targeted interventions improving baroreflex function in diseased individuals, e.g. in patients with autonomic failure, a syndrome that is difficult to diagnose in terms of its pathophysiology. 	
1609.04947v1	http://arxiv.org/pdf/1609.04947v1	2016	Robot Introspection via Wrench-based Action Grammars	Juan Rojas|Zhengjie Huang|Shuangqi Luo|Yunlong Du Wenwei Kuang|Dingqiao Zhu|Kensuke Harada	  Robotic failure is all too common in unstructured robot tasks. Despite well designed controllers, robots often fail due to unexpected events. How do robots measure unexpected events? Many do not. Most robots are driven by the senseplan- act paradigm, however more recently robots are working with a sense-plan-act-verify paradigm. In this work we present a principled methodology to bootstrap robot introspection for contact tasks. In effect, we are trying to answer the question, what did the robot do? To this end, we hypothesize that all noisy wrench data inherently contains patterns that can be effectively represented by a vocabulary. The vocabulary is generated by meaningfully segmenting the data and then encoding it. When the wrench information represents a sequence of sub-tasks, we can think of the vocabulary forming sets of words or sentences, such that each subtask is uniquely represented by a word set. Such sets can be classified using statistical or machine learning techniques. We use SVMs and Mondrian Forests to classify contacts tasks both in simulation and in real robots for one and dual arm scenarios showing the general robustness of the approach. The contribution of our work is the presentation of a simple but generalizable semantic scheme that enables a robot to understand its high level state. This verification mechanism can provide feedback for high-level planners or reasoning systems that use semantic descriptors as well. The code, data, and other supporting documentation can be found at: http://www.juanrojas.net/2017icra_wrench_introspection. 	
1610.01180v1	http://arxiv.org/pdf/1610.01180v1	2016	Counterion-Induced Swelling of Ionic Microgels	Alan R. Denton|Qiyun Tang	  Ionic microgel particles, when dispersed in a solvent, swell to equilibrium sizes that are governed by a balance between electrostatic and elastic forces. Tuning of particle size by varying external stimuli, such as $p$H, salt concentration, and temperature, has relevance for drug delivery, microfluidics, and filtration. To model swelling of ionic microgels, we derive a statistical mechanical theorem, which proves exact within the cell model, for the electrostatic contribution to the osmotic pressure inside a permeable colloidal macroion. Applying the theorem, we demonstrate how the distribution of counterions within an ionic microgel determines the internal osmotic pressure. By combining the electrostatic pressure, which we compute via both Poisson-Boltzmann theory and molecular dynamics simulation, with the elastic pressure, modeled via the Flory-Rehner theory of swollen polymer networks, we show how deswelling of ionic microgels with increasing concentration of particles can result from a redistribution of counterions that reduces electrostatic pressure. A linearized approximation for the electrostatic pressure, which proves remarkably accurate, provides physical insight and greatly eases numerical calculations for practical applications. Comparing with experiments, we explain why soft particles in deionized suspensions deswell upon increasing concentration and why this effect may be suppressed at higher ionic strength. The failure of the uniform ideal-gas approximation to adequately account for counterion-induced deswelling below close packing of microgels is attributed to neglect of spatial variation of the counterion density profile and the electrostatic pressure of incompletely neutralized macroions. 	
1611.02617v1	http://arxiv.org/pdf/1611.02617v1	2016	Color-avoiding percolation	Sebastian M. Krause|Michael M. Danziger|Vinko Zlatić	  Many real world networks have groups of similar nodes which are vulnerable to the same failure or adversary. Nodes can be colored in such a way that colors encode the shared vulnerabilities. Using multiple paths to avoid these vulnerabilities can greatly improve network robustness. Color-avoiding percolation provides a theoretical framework for analyzing this scenario, focusing on the maximal set of nodes which can be connected via multiple color-avoiding paths. In this paper we extend the basic theory of color-avoiding percolation that was published in [Krause et. al., Phys. Rev. X 6 (2016) 041022]. We explicitly account for the fact that the same particular link can be part of different paths avoiding different colors. This fact was previously accounted for with a heuristic approximation. We compare this approximation with a new, more exact theory and show that the new theory is substantially more accurate for many avoided colors. Further, we formulate our new theory with differentiated node functions, as senders/receivers or as transmitters. In both functions, nodes can be explicitly trusted or avoided. With only one avoided color we obtain standard percolation. With one by one avoiding additional colors, we can understand the critical behavior of color avoiding percolation. For heterogeneous color frequencies, we find that the colors with the largest frequencies control the critical threshold and exponent. Colors of small frequencies have only a minor influence on color avoiding connectivity, thus allowing for approximations. 	
1611.08859v2	http://arxiv.org/pdf/1611.08859v2	2017	Correlations and diagonal entropy after quantum quenches in XXZ chains	Lorenzo Piroli|Eric Vernier|Pasquale Calabrese|Marcos Rigol	  We study quantum quenches in the XXZ spin-$1/2$ Heisenberg chain from families of ferromagnetic and antiferromagnetic initial states. Using Bethe ansatz techniques, we compute short-range correlators in the complete generalized Gibbs ensemble (GGE), which takes into account all local and quasi-local conservation laws. We compare our results to exact diagonalization and numerical linked cluster expansion calculations for the diagonal ensemble finding excellent agreement and thus providing a very accurate test for the validity of the complete GGE. Furthermore, we compute the diagonal entropy in the post-quench steady state. By careful finite-size scaling analyses of the exact diagonalization results, we show that the diagonal entropy is equal to one half the Yang-Yang entropy corresponding to the complete GGE. Finally, the complete GGE is quantitatively contrasted with the GGE built using only the local conserved charges (local GGE). The predictions of the two ensembles are found to differ significantly in the case of ferromagnetic initial states. Such initial states are better suited than others considered in the literature to experimentally test the validity of the complete GGE and contrast it to the failure of the local GGE. 	
1612.00274v1	http://arxiv.org/pdf/1612.00274v1	2016	Lattice of infinite bending-resistant fibers	V. Kobelev	  This article present the double-periodical lattice made of infinite elastic fibers that withstand bending and tension. The model describes the elastic properties of flat periodic structure. With this model the behavior of a two-dimensional array of infinite fibers is simulated. The material that contains a row of broken fibers is considered. These broken fibers form the failure in the material that shapes like a long straight crack. The lattice is tensioned in the direction, which is orthogonal to the direction of straight crack. The conditions of fracture of this lattice are investigated. The closed form expression for the stress in the first unbroken fiber and the expression for fracture toughness are given. These values are the functions of mechanical parameters of lattice and tensions in both families of fibers. The closed form solution demonstrates a notable behavior of the material. Namely, the fracture behavior of two-dimensional lattice is cardinally depends upon the pre-stress in the material in the direction, parallel to crack direction. If the tension in fibers that parallel to the crack direction exists, it stabilizes the crack growth and makes the load distribution in the unbroken fibers more even. The two-dimensional lattice behaves in the presence of tension in both directions similarly to the plane elastic media. The finite length crack assumes the shape of the elongated elliptic split. Another behavior of lattice occurs if the fibers, parallel to crack direction, are unstressed. The character of stress concentration near the crack differs. The load distribution at the crack tip varies considerably. The first unbroken fiber carries higher load. The crack is lens-shaped and the crack borders form at the tip the finite angle. 	
1612.02219v1	http://arxiv.org/pdf/1612.02219v1	2016	Process Monitoring of Extrusion Based 3D Printing via Laser Scanning	Matthias Faes|Wim Abbeloos|Frederik Vogeler|Hans Valkenaers|Kurt Coppens|Toon Goedemé|Eleonora Ferraris	  Extrusion based 3D Printing (E3DP) is an Additive Manufacturing (AM) technique that extrudes thermoplastic polymer in order to build up components using a layerwise approach. Hereby, AM typically requires long production times in comparison to mass production processes such as Injection Molding. Failures during the AM process are often only noticed after build completion and frequently lead to part rejection because of dimensional inaccuracy or lack of mechanical performance, resulting in an important loss of time and material. A solution to improve the accuracy and robustness of a manufacturing technology is the integration of sensors to monitor and control process state-variables online. In this way, errors can be rapidly detected and possibly compensated at an early stage. To achieve this, we integrated a modular 2D laser triangulation scanner into an E3DP machine and analyzed feedback signals. A 2D laser triangulation scanner was selected here owing to the very compact size, achievable accuracy and the possibility of capturing geometrical 3D data. Thus, our implemented system is able to provide both quantitative and qualitative information. Also, in this work, first steps towards the development of a quality control loop for E3DP processes are presented and opportunities are discussed. 	
1701.02809v1	http://arxiv.org/pdf/1701.02809v1	2017	DyMo: Dynamic Monitoring of Large Scale LTE-Multicast Systems	Yigal Bejerano|Chandru Raman|Chun-Nam Yu|Varun Gupta|Craig Gutterman|Tomas Young|Hugo Infante|Yousef Abdelmalek|Gil Zussman	  LTE evolved Multimedia Broadcast/Multicast Service (eMBMS) is an attractive solution for video delivery to very large groups in crowded venues. However, deployment and management of eMBMS systems is challenging, due to the lack of realtime feedback from the User Equipment (UEs). Therefore, we present the Dynamic Monitoring (DyMo) system for low-overhead feedback collection. DyMo leverages eMBMS for broadcasting Stochastic Group Instructions to all UEs. These instructions indicate the reporting rates as a function of the observed Quality of Service (QoS). This simple feedback mechanism collects very limited QoS reports from the UEs. The reports are used for network optimization, thereby ensuring high QoS to the UEs. We present the design aspects of DyMo and evaluate its performance analytically and via extensive simulations. Specifically, we show that DyMo infers the optimal eMBMS settings with extremely low overhead, while meeting strict QoS requirements under different UE mobility patterns and presence of network component failures. For instance, DyMo can detect the eMBMS Signal-to-Noise Ratio (SNR) experienced by the 0.1% percentile of the UEs with Root Mean Square Error (RMSE) of 0.05% with only 5 to 10 reports per second regardless of the number of UEs. 	
1701.07561v1	http://arxiv.org/pdf/1701.07561v1	2017	Precision Pointing of Antennas in Space Using Arrays of Shape Memory   Alloy Based Linear Actuators	Nikhil S. Sonawane|Jekan Thangavelautham	  Space systems such as communication satellites, earth observation satellites and space telescopes require precise pointing to observe fixed targets over prolonged time. These systems typically use reaction-wheels to slew the spacecraft and gimballing systems containing motors to achieve precise pointing. Motor based actuators have limited life as they contain moving parts that require lubrication in space. Alternate methods have utilized piezoelectric actuators. This paper presents Shape memory alloys (SMA) actuators for control of a deployable antenna placed on a satellite. The SMAs are operated as a series of distributed linear actuators. These distributed linear actuators are not prone to single point failures and although each individual actuator is imprecise due to hysteresis and temperature variation. The system as a whole achieves reliable results. The SMAs can be programmed to perform a series of periodic motion and operate as a mechanical guidance system that is not prone to damage from radiation or space weather. Efforts are focused on developing a system that can achieve one degree pointing accuracy at first, with an ultimate goal of achieving a few arc seconds accuracy. Bench top models of the actuator system has been developed and working towards testing the system under vacuum. A demonstration flight of the technology is planned aboard a CubeSat. 	
1703.08831v1	http://arxiv.org/pdf/1703.08831v1	2017	Token-based Function Computation with Memory	Saber Salehkaleybar|S. Jamaloddin Golestani	  In distributed function computation, each node has an initial value and the goal is to compute a function of these values in a distributed manner. In this paper, we propose a novel token-based approach to compute a wide class of target functions to which we refer as "Token-based function Computation with Memory" (TCM) algorithm. In this approach, node values are attached to tokens and travel across the network. Each pair of travelling tokens would coalesce when they meet, forming a token with a new value as a function of the original token values. In contrast to the Coalescing Random Walk (CRW) algorithm, where token movement is governed by random walk, meeting of tokens in our scheme is accelerated by adopting a novel chasing mechanism. We proved that, compared to the CRW algorithm, the TCM algorithm results in a reduction of time complexity by a factor of at least $\sqrt{n/\log(n)}$ in Erd\"os-Renyi and complete graphs, and by a factor of $\log(n)/\log(\log(n))$ in torus networks. Simulation results show that there is at least a constant factor improvement in the message complexity of TCM algorithm in all considered topologies. Robustness of the CRW and TCM algorithms in the presence of node failure is analyzed. We show that their robustness can be improved by running multiple instances of the algorithms in parallel. 	
1704.00080v2	http://arxiv.org/pdf/1704.00080v2	2017	Discovering Phases, Phase Transitions and Crossovers through   Unsupervised Machine Learning: A critical examination	Wenjian Hu|Rajiv R. P. Singh|Richard T. Scalettar	  We apply unsupervised machine learning techniques, mainly principal component analysis (PCA), to compare and contrast the phase behavior and phase transitions in several classical spin models - the square and triangular-lattice Ising models, the Blume-Capel model, a highly degenerate biquadratic-exchange spin-one Ising (BSI) model, and the 2D XY model, and examine critically what machine learning is teaching us. We find that quantified principal components from PCA not only allow exploration of different phases and symmetry-breaking, but can distinguish phase transition types and locate critical points. We show that the corresponding weight vectors have a clear physical interpretation, which is particularly interesting in the frustrated models such as the triangular antiferromagnet, where they can point to incipient orders. Unlike the other well-studied models, the properties of the BSI model are less well known. Using both PCA and conventional Monte Carlo analysis, we demonstrate that the BSI model shows an absence of phase transition and macroscopic ground-state degeneracy. The failure to capture the `charge' correlations (vorticity) in the BSI model (XY model) from raw spin configurations points to some of the limitations of PCA. Finally, we employ a nonlinear unsupervised machine learning procedure, the `antoencoder method', and demonstrate that it too can be trained to capture phase transitions and critical points. 	
1704.06569v1	http://arxiv.org/pdf/1704.06569v1	2017	SFCSD: A Self-Feedback Correction System for DNS Based on Active and   Passive Measurement	Caiyun Huang|Peng Zhang|Junpeng Liu|Yong Sun|Xueqiang Zou	  Domain Name System (DNS), one of the important infrastructure in the Internet, was vulnerable to attacks, for the DNS designer didn't take security issues into consideration at the beginning. The defects of DNS may lead to users' failure of access to the websites, what's worse, users might suffer a huge economic loss.   In order to correct the DNS wrong resource records, we propose a Self-Feedback Correction System for DNS (SFCSD), which can find and track a large number of common websites' domain name and IP address correct correspondences to provide users with a real-time auto-updated correct (IP, Domain) binary tuple list. By matching specific strings with SSL, DNS and HTTP traffic passively, filtering with the CDN CNAME and non-homepage URL feature strings, verifying with webpage fingerprint algorithm, SFCSD obtains a large number of highly possibly correct IP addresses to make an active manual correction in the end. Its self-feedback mechanism can expand search range and improve performance.   Experiments show that, SFCSD can achieve 94.3% precision and 93.07% recall rate with the optimal threshold selection in the test dataset. It has 8Gbps processing speed stand-alone to find almost 1000 possibly correct (IP, Domain) per day for the each specific string and to correct almost 200. 	
1705.02477v1	http://arxiv.org/pdf/1705.02477v1	2017	Metacognitive Learning Approach for Online Tool Condition Monitoring	Mahardhika Pratama|Eric Dimla|Chow Yin Lai|Edwin Lughofer	  As manufacturing processes become increasingly automated, so should tool condition monitoring (TCM) as it is impractical to have human workers monitor the state of the tools continuously. Tool condition is crucial to ensure the good quality of products: Worn tools affect not only the surface quality but also the dimensional accuracy, which means higher reject rate of the products. Therefore, there is an urgent need to identify tool failures before it occurs on the fly. While various versions of intelligent tool condition monitoring have been proposed, most of them suffer from a cognitive nature of traditional machine learning algorithms. They focus on the how to learn process without paying attention to other two crucial issues: what to learn, and when to learn. The what to learn and the when to learn provide self regulating mechanisms to select the training samples and to determine time instants to train a model. A novel tool condition monitoring approach based on a psychologically plausible concept, namely the metacognitive scaffolding theory, is proposed and built upon a recently published algorithm, recurrent classifier (rClass). The learning process consists of three phases: what to learn, how to learn, when to learn and makes use of a generalized recurrent network structure as a cognitive component. Experimental studies with real-world manufacturing data streams were conducted where rClass demonstrated the highest accuracy while retaining the lowest complexity over its counterparts. 	
1708.04251v2	http://arxiv.org/pdf/1708.04251v2	2018	A learning framework for winner-take-all networks with stochastic   synapses	Hesham Mostafa|Gert Cauwenberghs	  Many recent generative models make use of neural networks to transform the probability distribution of a simple low-dimensional noise process into the complex distribution of the data. This raises the question of whether biological networks operate along similar principles to implement a probabilistic model of the environment through transformations of intrinsic noise processes. The intrinsic neural and synaptic noise processes in biological networks, however, are quite different from the noise processes used in current abstract generative networks. This, together with the discrete nature of spikes and local circuit interactions among the neurons, raises several difficulties when using recent generative modeling frameworks to train biologically motivated models. In this paper, we show that a biologically motivated model based on multi-layer winner-take-all (WTA) circuits and stochastic synapses admits an approximate analytical description. This allows us to use the proposed networks in a variational learning setting where stochastic backpropagation is used to optimize a lower bound on the data log likelihood, thereby learning a generative model of the data. We illustrate the generality of the proposed networks and learning technique by using them in a structured output prediction task, and in a semi-supervised learning task. Our results extend the domain of application of modern stochastic network architectures to networks where synaptic transmission failure is the principal noise mechanism. 	
1709.04075v1	http://arxiv.org/pdf/1709.04075v1	2017	Linear and nonlinear spectroscopy from quantum master equations	Jonathan H. Fetherolf|Timothy C. Berkelbach	  We investigate the accuracy of the second-order time-convolutionless (TCL2) quantum master equation for the calculation of linear and nonlinear spectroscopies of multichromophore systems. We show that, even for systems with non-adiabatic coupling, the TCL2 master equation predicts linear absorption spectra that are accurate over an extremely broad range of parameters and well beyond what would be expected based on the perturbative nature of the approach; non-equilibrium population dynamics calculated with TCL2 for identical parameters are significantly less accurate. For third-order (two-dimensional) spectroscopy, the importance of population dynamics and the violation of the so-called quantum regression theorem degrade the accuracy of TCL2 dynamics. To correct these failures, we combine the TCL2 approach with a classical ensemble sampling of slow microscopic bath degrees of freedom, leading to an efficient hybrid quantum-classical scheme that displays excellent accuracy over a wide range of parameters. In the spectroscopic setting, the success of such a hybrid scheme can be understood through its separate treatment of homogeneous and inhomogeneous broadening. Importantly, the presented approach has the computational scaling of TCL2, with the modest addition of an embarrassingly parallel prefactor associated with ensemble sampling. The presented approach can be understood as a generalized inhomogeneous cumulant expansion technique, capable of treating multilevel systems with non-adiabatic dynamics. 	
1709.06779v1	http://arxiv.org/pdf/1709.06779v1	2017	High speed self-testing quantum random number generation without   detection loophole	Yang Liu|Xiao Yuan|Ming-Han Li|Weijun Zhang|Qi Zhao|Jiaqiang Zhong|Yuan Cao|Yu-Huai Li|Luo-Kan Chen|Hao Li|Tianyi Peng|Yu-Ao Chen|Cheng-Zhi Peng|Sheng-Cai Shi|Zhen Wang|Lixing You|Xiongfeng Ma|Jingyun Fan|Qiang Zhang|Jian-Wei Pan	  Quantum mechanics provides means of generating genuine randomness that is impossible with deterministic classical processes. Remarkably, the unpredictability of randomness can be certified in a self-testing manner that is independent of implementation devices. Here, we present an experimental demonstration of self-testing quantum random number generation based on an detection-loophole free Bell test with entangled photons. In the randomness analysis, without the assumption of independent identical distribution, we consider the worst case scenario that the adversary launches the most powerful attacks against quantum adversary. After considering statistical fluctuations and applying an 80 Gb $\times$ 45.6 Mb Toeplitz matrix hashing, we achieve a final random bit rate of 114 bits/s, with a failure probability less than $10^{-5}$. Such self-testing random number generators mark a critical step towards realistic applications in cryptography and fundamental physics tests. 	
1711.04691v2	http://arxiv.org/pdf/1711.04691v2	2017	Observational Signatures of Mass-Loading in Jets Launched by Rotating   Black Holes	Michael O' Riordan|Asaf Pe'er|Jonathan C. McKinney	  It is widely believed that relativistic jets in X-ray binaries and active-galactic nuclei are powered by the rotational energy of black holes. This idea is supported by general-relativistic magnetohydrodynamic (GRMHD) simulations of accreting black holes, which demonstrate efficient energy extraction via the Blandford-Znajek mechanism. However, due to uncertainties in the physics of mass-loading, and the failure of GRMHD numerical schemes in the highly-magnetized funnel region, the matter content of the jet remains poorly constrained. We investigate the observational signatures of mass-loading in the funnel by performing general-relativistic radiative transfer calculations on a range of 3D GRMHD simulations of accreting black holes. We find significant observational differences between cases in which the funnel is empty and cases where the funnel is filled with plasma, particularly in the optical and X-ray bands. In the context of Sgr A*, current spectral data constrains the jet filling only if the black hole is rapidly rotating with $a\gtrsim0.9$. In this case, the limits on the infrared flux disfavour a strong contribution from material in the funnel. We comment on the implications of our models for interpreting future Event Horizon Telescope observations. We also scale our models to stellar-mass black holes, and discuss their applicability to the low-luminosity state in X-ray binaries. 	
1712.06205v2	http://arxiv.org/pdf/1712.06205v2	2018	No axion-like particles from core-collapse supernovae?	Ignazio Bombaci|Giorgio Galanti|Marco Roncadelli	  A strong bound on the properties of axion-like particles (ALPs) has been set by assuming that ALPs are emitted by the protoneutron star just before the core-bounce in Galactic core-collapse supernovae, and that these ALPs subsequently convert to $\gamma$-ray photons which ought to be detected by a $\gamma$-ray mission. This argument has been applied to supernova 1987A to derive the bound on the ALP-photon coupling $g_{a \gamma \gamma} \lesssim 5.3 \cdot 10^{- 12} \, {\rm GeV}^{- 1}$ for an ALP mass $m_a \lesssim 4.4 \cdot 10^{- 10} \, {\rm eV}$, and can be applied to the next Galactic supernova to derive the even stronger bound $g_{a \gamma \gamma} \lesssim 2 \cdot 10^{- 13} \, {\rm GeV}^{- 1}$ for an ALP mass $m_a \lesssim 10^{- 9} \, {\rm eV}$. We carefully analyze the considered ALP production mechanism and find that it is oversimplified to an unacceptable extent. By taking into account the minimal ingredients required by a realistic analysis, we conclude that the previous results are doomed to failure. As a consequence, all papers quoting the above bound should be properly revised. Yet, since we are unable to rule out the possibility that protoneutron stars emit ALPs, in case a core-collapse supernova explodes in the Galaxy the $\gamma$-ray satellite missions active at that time should look for photons possibly coming from the supernova. 	
1801.00062v1	http://arxiv.org/pdf/1801.00062v1	2017	Dendritic error backpropagation in deep cortical microcircuits	João Sacramento|Rui Ponte Costa|Yoshua Bengio|Walter Senn	  Animal behaviour depends on learning to associate sensory stimuli with the desired motor command. Understanding how the brain orchestrates the necessary synaptic modifications across different brain areas has remained a longstanding puzzle. Here, we introduce a multi-area neuronal network model in which synaptic plasticity continuously adapts the network towards a global desired output. In this model synaptic learning is driven by a local dendritic prediction error that arises from a failure to predict the top-down input given the bottom-up activities. Such errors occur at apical dendrites of pyramidal neurons where both long-range excitatory feedback and local inhibitory predictions are integrated. When local inhibition fails to match excitatory feedback an error occurs which triggers plasticity at bottom-up synapses at basal dendrites of the same pyramidal neurons. We demonstrate the learning capabilities of the model in a number of tasks and show that it approximates the classical error backpropagation algorithm. Finally, complementing this cortical circuit with a disinhibitory mechanism enables attention-like stimulus denoising and generation. Our framework makes several experimental predictions on the function of dendritic integration and cortical microcircuits, is consistent with recent observations of cross-area learning, and suggests a biological implementation of deep learning. 	
1801.01416v1	http://arxiv.org/pdf/1801.01416v1	2018	The dynamics of a shear band	Diana Giarola|Domenico Capuani|Davide Bigoni	  A shear band of finite length, formed inside a ductile material at a certain stage of a con- tinued homogeneous strain, provides a dynamic perturbation to an incident wave field, which strongly influences the dynamics of the material and affects its path to failure. The investigation of this perturbation is presented for a ductile metal, with reference to the incremental mechanics of a material obeying the J 2-deformation theory of plasticity (a special form of prestressed, elastic, anisotropic, and incompressible solid). The treatment originates from the derivation of integral representations relating the incremental mechan- ical fields at every point of the medium to the incremental displacement jump across the shear band faces, generated by an impinging wave. The boundary integral equations (under the plane strain assumption) are numerically approached through a collocation technique, which keeps into account the singularity at the shear band tips and permits the analysis of an incident wave impinging a shear band. It is shown that the presence of the shear band induces a resonance, visible in the incremental displacement field and in the stress intensity factor at the shear band tips, which promotes shear band growth. Moreover, the waves scattered by the shear band are shown to generate a fine texture of vibrations, par- allel to the shear band line and propagating at a long distance from it, but leaving a sort of conical shadow zone, which emanates from the tips of the shear band. 	
1801.06266v1	http://arxiv.org/pdf/1801.06266v1	2018	Generalization of BCS theory to short coherence length superconductors:   A BCS--Bose-Einstein crossover scenario	Qijin Chen	  The (mean field based) BCS theory is considered one of the most successful theories in condensed matter physics. It is justified in ordinary metal superconductors the coherence length $\xi$ is large, with two important features: the order parameter (OP) and excitation gap (EG) are identical, and the pair formation and their Bose condensation take place at the same temperature Tc. It fails to explain the underdoped cuprate superconductivity: EG is finite at Tc and thus distinct from OP. Since these superconductors belong to a large class of small $\xi$ materials, this failure has the potential for widespread impact.   Here we have extended BCS theory in a natural way to short $\xi$ superconductors, based on a BCS--BEC crossover scenario, and arrived at a simple physical picture in which incoherent, finite momentum pairs become progressively more important as the pairing interaction becomes stronger, leading to the distinction between EG and OP. The superconductivity from the fermionic perspective and BEC from the bosonic perspective are just two sides of the same coin.   Our theory is capable of making verifiable quantitative predictions. We obtain a cuprate phase diagram (with one free parameter) , in (semi-)quantitative agreement with experiment. The mutually compensating contributions from fermionic quasiparticles and bosonic pair excitations provides a natural explanation for the quasi-universal behavior of the in-plane superfluid density versus T. Our bosonic pair excitations also provide an intrinsic mechanism for the long mysterious linear T terms in the specific heat. Incoherent pair contributions lead to new low T power laws, consistent with existing experiments. Finally, we demonstrated that the onset of superconducting long range order leads to sharp features in the specific heat at Tc, consistent with experiment. 	
1801.08727v1	http://arxiv.org/pdf/1801.08727v1	2018	Impact ionization and transport properties of hexagonal boron nitride in   constant-voltage measurement	Y. Hattori|T. Taniguchi|K. Watanabe|K. Nagashio	  The electrical evaluation of the crystallinity of hexagonal boron nitride (h-BN) is still limited to the measurement of dielectric breakdown strength, in spite of its importance as the substrate for 2-dimensional van der Waals heterostructure devices. In this study, physical phenomena for degradation and failure in exfoliated single-crystal h-BN films were investigated using the constant-voltage stress test. At low electrical fields, the current gradually reduced and saturated with time, while the current increased at electrical fields higher than ~8 MV/cm and finally resulted in the catastrophic dielectric breakdown. These transient behaviors may be due to carrier trapping to the defect sites in h-BN because trapped carriers lower or enhance the electrical fields in h-BN depending on their polarities. The key finding is the current enhancement with time at the high electrical field, suggesting the accumulation of electrons generated by the impact ionization process. Therefore, a theoretical model including the electron generation rate by impact ionization process was developed. The experimental data support the expected degradation mechanism of h-BN. Moreover, the impact ionization coefficient was successfully extracted, which is comparable to that of SiO2, even though the fundamental band gap for h-BN is smaller than that for SiO2. Therefore, the dominant impact ionization in h-BN could be band-to-band excitation, not defect-assisted impact ionization. 	
1802.00951v1	http://arxiv.org/pdf/1802.00951v1	2018	Scheduling and Checkpointing optimization algorithm for Byzantine fault   tolerance in Cloud Clusters	Sathya Chinnathambi|Agilan Santhanam	  Among those faults Byzantine faults offers serious challenge to fault tolerance mechanism, because it often go undetected at the initial stage and it can easily propagate to other VMs before a detection is made. Consequently some of the mission critical application such as air traffic control, online baking etc still staying away from the cloud for such reasons. However if a Byzantine faults is not detected and tolerated at initial stage then applications such as big data analytics can go completely wrong in spite of hours of computations performed by the entire cloud. Therefore in the previous work a fool-proof Byzantine fault detection has been proposed, as a continuation this work designs a scheduling algorithm (WSSS) and checkpoint optimization algorithm (TCC) to tolerate and eliminate the Byzantine faults before it makes any impact. The WSSS algorithm keeps track of server performance which is part of Virtual Clusters to help allocate best performing server to mission critical application. WSSS therefore ranks the servers based on a counter which monitors every Virtual Nodes (VN) for time and performance failures. The TCC algorithm works to generalize the possible Byzantine error prone region through monitoring delay variation to start new VNs with previous checkpointing. Moreover it can stretch the state interval for performing and error free VNs in an effect to minimize the space, time and cost overheads caused by checkpointing. The analysis is performed with plotting state transition and CloudSim based simulation. The result shows TCC reduces fault tolerance overhead exponentially and the WSSS allots virtual resources effectively 	
1802.03898v1	http://arxiv.org/pdf/1802.03898v1	2018	Scalable Downward Routing for Wireless Sensor Networks and Internet of   Things Actuation	Xiaoyang Zhong|Yao Liang	  In this paper, we study the downward routing for network control/actuation in large-scale and heterogeneous wireless sensor networks (WSNs) and Internet of Things (IoT). We propose the Opportunistic Source Routing (OSR), a scalable and reliable downward routing protocol for WSNs/IoT. OSR introduces opportunistic routing into traditional source routing based on the parent set of a node's upward routing in data collection, significantly addressing the drastic link dynamics in low-power and lossy WSNs. We devise a novel adaptive Bloom filter mechanism to effectively and efficiently encode a downward source-route in OSR, which enables a significant reduction of the length of source-route field in packet header. OSR is scalable to very large-size WSN/IoT deployments, since each resource-constrained node in the network only stores the set of its direct children. The probabilistic nature of the Bloom filter passively explores opportunistic routing. Upon a delivery failure at any hop along the downward path, OSR actively performs opportunistic routing to bypass the obsolete/bad link. We demonstrate the desirable scalability of OSR against the standard RPL downward routing. We evaluate the performance of OSR via both simulations and real-world testbed experiments, in comparison with the standard RPL (both storing mode and non-storing mode), ORPL, and the representative dissemination protocol Drip. Our results show that OSR significantly outperforms RPL and ORPL in scalability and reliability. OSR also achieves significantly better energy efficiency compared to TinyRPL and Drip which are based on the same TinyOS platform as OSR implementation. 	
1803.01671v1	http://arxiv.org/pdf/1803.01671v1	2018	Do thermodynamically stable rigid solids exist?	Parswa Nath|Saswati Ganguly|Jürgen Horbach|Peter Sollich|Smarajit Karmakar|Surajit Sengupta	  Customarily, crystalline solids are defined to be {\em rigid} since they resist changes of shape determined by their boundaries. However, rigid solids cannot exist in the thermodynamic limit where boundaries become irrelevant. Particles in the solid may rearrange to adjust to shape changes eliminating stress without destroying crystalline order. Rigidity is therefore valid only in the {\em metastable} state that emerges because these particle rearrangements in response to a deformation, or strain, are associated with slow collective processes. Here, we show that a thermodynamic collective variable may be used to quantify particle rearrangements that occur as a solid is deformed at zero strain rate. Advanced Monte Carlo simulation techniques are then employed to obtain the equilibrium free energy as a function of this variable. Our results lead to a new view on rigidity: While at zero strain a rigid crystal coexists with one that responds to infinitesimal strain by rearranging particles and expelling stress, at finite strain the rigid crystal is metastable, associated with a free energy barrier that decreases with increasing strain. The rigid phase becomes thermodynamically stable by switching on an external field, which penalises particle rearrangements. This produces a line of first-order phase transitions in the field - strain plane that intersects the origin. Failure of a solid once strained beyond its elastic limit is associated with kinetic decay processes of the metastable rigid crystal deformed with a finite strain rate. These processes can be understood in quantitative detail using our computed phase diagram as reference. 	
1401.0064v1	http://arxiv.org/pdf/1401.0064v1	2013	Linearity of quantum probability measure and Hardy's model	Kazuo Fujikawa|C. H. Oh|Chengjie Zhang	  We re-examine d=4 hidden-variables-models for a system of two spin-$1/2$ particles in view of the concrete model of Hardy, who analyzed the criterion of entanglement without referring to inequality. The basis of our analysis is the linearity of the probability measure related to the Born probability interpretation, which excludes non-contextual hidden-variables models in $d\geq 3$. To be specific, we note the inconsistency of the non-contextual hidden-variables model in $d=4$ with the linearity of the quantum mechanical probability measure in the sense $\langle\psi|{\bf a}\cdot {\bf \sigma}\otimes{\bf b}\cdot {\bf \sigma}|\psi\rangle+\langle\psi|{\bf a}\cdot {\bf \sigma}\otimes{\bf b}^{\prime}\cdot {\bf \sigma}|\psi\rangle=\langle\psi|{\bf a}\cdot {\bf \sigma}\otimes ({\bf b}+{\bf b}^{\prime})\cdot {\bf \sigma}|\psi\rangle$ for non-collinear ${\bf b}$ and ${\bf b}^{\prime}$. It is then shown that Hardy's model in $d=4$ does not lead to a unique mathematical expression in the demonstration of the discrepancy of local realism (hidden-variables model) with entanglement and thus his proof is incomplete. We identify the origin of this non-uniqueness with the non-uniqueness of translating quantum mechanical expressions into expressions in hidden-variables models, which results from the failure of the above linearity of the probability measure. In contrast, if the linearity of the probability measure is strictly imposed, which is tantamount to asking that the non-contextual hidden-variables model in $d=4$ gives the CHSH inequality $|\langle B\rangle|\leq 2$ uniquely, it is shown that the hidden-variables model can describe only separable quantum mechanical states; this conclusion is in perfect agreement with the so-called Gisin's theorem which states that $|\langle B\rangle|\leq 2$ implies separable states. 	
1603.05841v2	http://arxiv.org/pdf/1603.05841v2	2016	Competing damage mechanisms in a two-phase microstructure: how   microstructure and loading conditions determine the onset of fracture	T. W. J. de Geus|R. H. J. Peerlings|M. G. D. Geers	  This paper studies the competition of fracture initiation in the ductile soft phase and in the comparatively brittle hard phase in the microstructure of a two-phase material. A simple microstructural model is used to predict macroscopic fracture initiation. The simplicity of the model ensures highly efficient computations, enabling an comprehensive study: a large range of hard phase volume fractions and yield stress ratios, for wide range of applied stress states. Each combination of these parameters is analyzed using a large set of (random) microstructures. It is observed that only one of the phases dominates macroscopic fracture initiation: at low stress triaxiality the soft phase is dominant, but above a critical triaxiality the hard phase takes over resulting in a strong decrease in ductility. This transition is strongly dependent on microstructural parameters. If the hard phase volume fraction is small, the fracture initiation is dominated by the soft phase even at high phase contrast. At higher hard phase volume fraction, the hard phase dominates already at low phase contrast. This simple model thereby reconciles experimental observations from the literature for a specific combination of parameters, which may have triggered contradictory statements in the past. A microscopic analysis reveals that the average phase distribution around fracture initiation sites is nearly the same for the two failure mechanisms. Along the tensile direction, regions of the hard phase are found directly next to the fracture initiation site. This `band' of hard phase is intersected through the fracture initiation site by `bands' of the soft phase aligned with shear. Clearly, the local mechanical incompatibility is dominant for the initiation of fracture, regardless whether fracture initiates in the soft or in the hard phase. 	
0106558v1	http://arxiv.org/pdf/cond-mat/0106558v1	2001	Heterogeneous Interfacial Failure between Two Elastic Blocks	G. George Batrouni|Alex Hansen|Jean Schmittbuhl	  We investigate numerically the failure process when two elastic media, one hard and one soft that have been glued together thus forming a common interface, are pulled apart. We present three main results: (1) The area distribution of simultaneously failing glue (bursts) follows a power law consistent with the theoretically expected exponent 2.5, (2) the maximum load and displacement before catastrophic failure scale as L^2 and L^0 respectively, where L is the linear size of the system, and (3) the area distribution of failed glue regions (clusters) is a power law with exponent -1.6 when the system fails catstrophically. 	
0203476v1	http://arxiv.org/pdf/cond-mat/0203476v1	2002	Calculation of the incremental stress-strain relation of a polygonal   packing	F. Alonso-Marroquin|H. J. Herrmann	  The constitutive relation of the quasi-static deformation on two dimensional packed samples of polygons is calculated using molecular dynamic simulations. The stress values at which the system remains stable are bounded by a failure surface, that shows a power law dependence on the pressure. Below the failure surface, non linear elasticity and plastic deformation are obtained, which are evaluated in the framework of the incremental linear theory. The results shows that the stiffness tensor can be directly related to the micro-contact rearrangements. The plasticity obeys a non-associated flow rule, with a plastic limit surface that does not agree with the failure surface. 	
0405096v1	http://arxiv.org/pdf/cond-mat/0405096v1	2004	Universal Breakdown of Elasticity at the Onset of Material Failure	Craig Maloney|Anaël Lemaître	  We show that, in the athermal quasi-static deformation of amorphous materials, the onset of failure is accompanied by universal scalings associated with a \emph{divergence} of elastic constants. A normal mode analysis of the non-affine elastic displacement field allows us to clarify its relation to the zero-frequency mode at the onset of failure and to the crack-like pattern which results from the subsequent relaxation of energy. 	
0003056v1	http://arxiv.org/pdf/cs/0003056v1	2000	A note on the Declarative reading(s) of Logic Programming	Marc Denecker	  This paper analyses the declarative readings of logic programming. Logic programming - and negation as failure - has no unique declarative reading. One common view is that logic programming is a logic for default reasoning, a sub-formalism of default logic or autoepistemic logic. In this view, negation as failure is a modal operator. In an alternative view, a logic program is interpreted as a definition. In this view, negation as failure is classical objective negation. From a commonsense point of view, there is definitely a difference between these views. Surprisingly though, both types of declarative readings lead to grosso modo the same model semantics. This note investigates the causes for this. 	
0302029v1	http://arxiv.org/pdf/nlin/0302029v1	2003	Noise-induced failures of chaos stabilization: large fluctuations and   their control	I. A. Khovanov|N. A. Khovanova|P. V. E. McClintock	  Noise-induced failures in the stabilization of an unstable orbit in the one-dimensional logistic map are considered as large fluctuations from a stable state. The properties of the large fluctuations are examined by determination and analysis of the optimal path and the optimal fluctuational force corresponding to the stabilization failure. The problem of controlling noise-induced large fluctuations is discussed, and methods of control have been developed. 	
0310005v2	http://arxiv.org/pdf/q-bio/0310005v2	2003	Structured psychosocial stress and therapeutic failure	Rodrick Wallace|Deborah Wallace	  Generalized language-of-thought arguments appropriate to interacting cognitive modules permit exploration of how disease states interact with medical treatment. The interpenetrating feedback between treatment and response to it creates a kind of idiotypic hall-of-mirrors generating a synergistic pattern of efficacy, treatment failure, adverse reactions, and patient noncompliance which, from a Rate Distortion perspective, embodies a distorted image of externally-imposed structured psychosocial stress. For the US, accelerating spatial and social diffusion of such stress enmeshes both dominant and subordinate populations in a linked system which will express itself, not only in an increasingly unhealthy society, but in the diffusion of therapeutic failure, including, but not limited to, drug-based treatments. 	
0701090v1	http://arxiv.org/pdf/quant-ph/0701090v1	2007	Error propagation in loss- and failure-tolerant quantum computation   schemes	Peter P. Rohde|Timothy C. Ralph|William J. Munro	  Qubit loss and gate failure are significant obstacles for the implementation of scalable quantum computation. Recently there have been several proposals for overcoming these problems, including schemes based on parity and cluster states. While effective at dealing with loss and gate failure, these schemes typically lead to a blow-out in effective depolarizing noise rates. In this supplementary paper we present a detailed analysis of this problem and techniques for minimizing it. 	
0704.0345v1	http://arxiv.org/pdf/0704.0345v1	2007	A High Robustness and Low Cost Model for Cascading Failures	Bing Wang|Beom Jun Kim	  We study numerically the cascading failure problem by using artificially created scale-free networks and the real network structure of the power grid. The capacity for a vertex is assigned as a monotonically increasing function of the load (or the betweenness centrality). Through the use of a simple functional form with two free parameters, revealed is that it is indeed possible to make networks more robust while spending less cost. We suggest that our method to prevent cascade by protecting less vertices is particularly important for the design of more robust real-world networks to cascading failures. 	
0707.1622v1	http://arxiv.org/pdf/0707.1622v1	2007	Dynamic Failure in Amorphous Solids via a Cavitation Instability	Eran Bouchbinder|Ting-Shek Lo|Itamar Procaccia	  The understanding of dynamic failure in amorphous materials via the propagation of free boundaries like cracks and voids must go beyond elasticity theory, since plasticity intervenes in a crucial and poorly understood manner near the moving free boundary. In this Letter we focus on failure via a cavitation instability in a radially-symmetric stressed material, set up the free boundary dynamics taking both elasticity and visco-plasticity into account, using the recently proposed athermal Shear Transformation Zone theory. We demonstrate the existence (in amorphous systems) of fast cavitation modes accompanied by extensive plastic deformations and discuss the revealed physics. 	
0806.3558v2	http://arxiv.org/pdf/0806.3558v2	2009	Failure of Local Realism Revealed by Extremely Coarse-Grained   Measurements	H. Jeong|M. Paternostro|T. C. Ralph	  We show that failure of local realism can be revealed to observers for whom only extremely coarse-grained measurements are available. In our instances, Bell's inequality is violated even up to the maximum limit while both the local measurements and the initial local states under scrutiny approach the classical limit. Furthermore, we can observe failure of local realism when an inequality enforced by non-local realistic theories is satisfied. This suggests that locality alone may be violated while realism cannot be excluded for specific observables and states. Small-scale experimental demonstration of our examples may be possible in the foreseeable future. 	
0809.0279v1	http://arxiv.org/pdf/0809.0279v1	2008	Competing risks within shock models	Antonio Di Crescenzo|Maria Longobardi	  We consider a competing risks model, in which system failures are due to one out of two mutually exclusive causes, formulated within the framework of shock models driven by bivariate Poisson process. We obtain the failure densities and the survival functions as well as other related quantities under three different schemes. Namely, system failures are assumed to occur at the first instant in which a random constant threshold is reached by (a) the sum of received shocks, (b) the minimum of shocks, (c) the maximum of shocks. 	
0811.1693v1	http://arxiv.org/pdf/0811.1693v1	2008	Protection Schemes for Two Link Failures in Optical Networks	Salah A. Aly|Ahmed E. Kamal	  In this paper we develop network protection schemes against two link failures in optical networks. The motivation behind this work is the fact that the majority of all available links in an optical network suffer from single and double link failures. In the proposed network protection schemes, NPS2-I and NPS2-II, we deploy network coding and reduced capacity on the working paths to provide backup protection paths. In addition, we demonstrate the encoding and decoding aspects of the proposed schemes. 	
0902.3644v2	http://arxiv.org/pdf/0902.3644v2	2009	Failure of the Hasse principle for Chatelet surfaces in characteristic 2	Bianca Viray	  Given any global field k of characteristic 2, we construct a Chatelet surface over k which fails to satisfy the Hasse principle. This failure is due to a Brauer-Manin obstruction. This construction extends a result of Poonen to characteristic 2, thereby showing that the etale-Brauer obstruction is insufficient to explain all failures of the Hasse principle over a global field of any characteristic. 	
0903.0054v1	http://arxiv.org/pdf/0903.0054v1	2009	Considerations on Resource Usage in Exceptions and Failures in Workflows	Alexandra Fortis|Alexandru Cicortas|Victoria Iordan	  The paper presents a description of some point of view of different authors related to the failures and exceptions that appear in workflows, as a direct consequence of unavailability of resources involved in the workflow. Each of these interpretations is typical for a certain situation, depending on the authors' interpretation of failures and exceptions in workflows modeling real dynamical systems. 	
0903.3104v1	http://arxiv.org/pdf/0903.3104v1	2009	Piezonuclear neutrons from fracturing of inert solids	F. Cardone|A. Carpinteri|G. Lacidogna	  Neutron emission measurements by means of helium-3 neutron detectors were performed on solid test specimens during crushing failure. The materials used were marble and granite, selected in that they present a different behaviour in compression failure (i.e., a different brittleness index) and a different iron content. All the test specimens were of the same size and shape. Neutron emissions from the granite test specimens were found to be of about one order of magnitude higher than the natural background level at the time of failure. 	
1001.2077v2	http://arxiv.org/pdf/1001.2077v2	2010	On Random Linear Network Coding for Butterfly Network	Xuan Guang|Fang-Wei Fu	  Random linear network coding is a feasible encoding tool for network coding, specially for the non-coherent network, and its performance is important in theory and application. In this letter, we study the performance of random linear network coding for the well-known butterfly network by analyzing the failure probabilities. We determine the failure probabilities of random linear network coding for the well-known butterfly network and the butterfly network with channel failure probability p. 	
1004.5256v1	http://arxiv.org/pdf/1004.5256v1	2010	Construction auto-stabilisante d'arbre couvrant en dépit d'actions   malicieuses	Swan Dubois|Toshimitsu Masuzawa|Sébastien Tixeuil	  A self-stabilizing protocol provides by definition a tolerance to transient failures. Recently, a new class of self-stabilizing protocols appears. These protocols provides also a tolerance to a given number of permanent failures. In this article, we are interested in self-stabilizing protocols that deal with Byzantines failures. We prove that, for some problems which not allow strict stabilization (see [Nesterenko,Arora,2002]), there exist solutions that tolerates Byzantine faults if we define a new criteria of tolerance. 	
1007.5447v1	http://arxiv.org/pdf/1007.5447v1	2010	Politiques de Tests Partiels \& Systèmes de Sécurité	Florent Brissaud|Anne Barros|Christophe Bérenguer	  A set of general formulas is proposed for the probability of failure on demand (PFD) assessment of MooN architecture (i.e. k-out-of-n) systems subject to partial and full tests. Partial tests (e.g. visual inspections, imperfect testing) may detect only some failures, whereas owing to a full test, the system is restored to an as good as new condition. Following the proposed approach and according to an example, performance estimations of the system and test policies are presented, by using the feedback from partial and full tests. An optimization of the partial test distribution is also proposed, which allows reducing the average probability of system failure on demand (PFDavg). 	
1008.1369v1	http://arxiv.org/pdf/1008.1369v1	2010	Fully fault tolerant quantum computation with non-deterministic gates	Ying Li|Sean D. Barrett|Thomas M. Stace|Simon C. Benjamin	  In certain approaches to quantum computing the operations between qubits are non-deterministic and likely to fail. For example, a distributed quantum processor would achieve scalability by networking together many small components; operations between components should assumed to be failure prone. In the logical limit of this architecture each component contains only one qubit. Here we derive thresholds for fault tolerant quantum computation under such extreme paradigms. We find that computation is supported for remarkably high failure rates (exceeding 90%) providing that failures are heralded, meanwhile the rate of unknown errors should not exceed 2 in 10^4 operations. 	
1010.3828v2	http://arxiv.org/pdf/1010.3828v2	2010	Self-organized criticality as a precursor of fatigue: application to   shape memory alloys	C. Dunand-Chatellet|Z. Moumni	  Fatigue failure can be thought by studying the collective motions of defects inside materials instead of focusing on the growth of a pre-existing micro-crack. An experimental study of the statistical distribution of acoustic emissions avalanches along cycling is presented. The evolutions of critical exponents through cyclic driving are estimated to track changes in the dissipation modes and consequently identify fatigue failure precursors. We also use critical rupture models developed for earthquakes and stock market crashes predictions to forecast the time to failure with good reliability. 	
1106.0224v1	http://arxiv.org/pdf/1106.0224v1	2011	Reasoning about Minimal Belief and Negation as Failure	R. Rosati	  We investigate the problem of reasoning in the propositional fragment of MBNF, the logic of minimal belief and negation as failure introduced by Lifschitz, which can be considered as a unifying framework for several nonmonotonic formalisms, including default logic, autoepistemic logic, circumscription, epistemic queries, and logic programming. We characterize the complexity and provide algorithms for reasoning in propositional MBNF. In particular, we show that entailment in propositional MBNF lies at the third level of the polynomial hierarchy, hence it is harder than reasoning in all the above mentioned propositional formalisms for nonmonotonic reasoning. We also prove the exact correspondence between negation as failure in MBNF and negative introspection in Moore's autoepistemic logic. 	
1204.5724v1	http://arxiv.org/pdf/1204.5724v1	2012	Nonparametric survival analysis and vaccine efficacy using   Dempster-Shafer analysis	Paul T. Edlefsen|Arthur P. Dempster	  We introduce an extension of nonparametric DS inference for arbitrary univariate CDFs to the case in which some failure times are (right)-censored, and then apply this to the problem of assessing evidence regarding assertions about relative risks across two populations. The approach enables exploration of the sensitivity of survival analyses to assumed independence of the missing data process and the failure proces. We present an application to the partially efficacious RV144 (HIV-1) vaccine trial, and show that the strength of conclusions of vaccine efficacy depend on assumptions about the maximum failure rates of the subjects lost-to-followup. 	
1303.4051v1	http://arxiv.org/pdf/1303.4051v1	2013	The failure risk analysis of digital circuits	A. N. Pchelintsev	  To analyze the failure risk of asynchronous digital circuits the time-parameter is introduced into the Boolean algebra replacing the arithmetic operations by logical operations. There considered an example of construction of signals passing through the logical elements, using the described below mathematical apparatus. 	
1308.1968v1	http://arxiv.org/pdf/1308.1968v1	2013	Detection and Isolation of Link Failures under the Agreement Protocol	M. Amin Rahimian|Victor M. Preciado	  In this paper a property of the multi-agent consensus dynamics that relates the failure of links in the network to jump discontinuities in the derivatives of the output responses of the nodes is derived and verified analytically. At the next step, an algorithm for sensor placement is proposed, which would enable the designer to detect and isolate any link failures across the network based on the observed jump discontinuities in the derivatives of the responses of a subset of nodes. These results are explained through elaborative examples. 	
1309.5540v2	http://arxiv.org/pdf/1309.5540v2	2014	Detection and Isolation of Failures in Linear Multi-Agent Networks	M. Amin Rahimian|Victor M. Preciado	  In this paper the focus is on the relationship between the occurrence of failures in a (directed or undirected) network of linear single integrator agents and the presence of jump discontinuities in the derivatives of the network output. Based on this relationship, an algorithm for sensor placement is proposed, which enables the designer to detect and isolate any link failures across the network, based on the jump discontinuities observed by the sensor nodes. These results are explained through elaborative examples and computer experiments. 	
1405.5841v1	http://arxiv.org/pdf/1405.5841v1	2014	Parameter Estimates of General Failure Rate Model: A Bayesian Approach	Asok K. Nanda|Sudhansu S. Maiti|Chanchal Kundu|Amarjit Kundu	  The failure rate function plays an important role in studying the lifetime distributions in reliability theory and life testing models. A study of the general failure rate model $r(t)=a+bt^{\theta-1}$, under squared error loss function taking $a$ and $b$ independent exponential random variables has been analyzed in the literature. In this article, we consider $a$ and $b$ not necessarily independent. The estimates of the parameters $a$ and $b$ under squared error loss, linex loss and entropy loss functions are obtained here. 	
1509.00570v1	http://arxiv.org/pdf/1509.00570v1	2015	Nonlocal effects and counter measures in cascading failures	Dirk Witthaut|Marc Timme	  We study the propagation of cascading failures in complex supply networks with a focus on nonlocal effects occurring far away from the initial failure. It is shown that a high clustering and a small average path length of a network generally suppress nonlocal overloads. These properties are typical for many real-world networks, often called small-world networks, such that cascades propagate mostly locally in these networks. Furthermore, we analyze the spatial aspects of countermeasures based on the intentional removal of additional edges. Nonlocal actions are generally required in networks which have a low redundancy and are thus especially vulnerable to cascades. 	
1509.02746v1	http://arxiv.org/pdf/1509.02746v1	2015	Low-frequency failure of the Göppert-Mayer gauge transformation and   consequences for the Strong-Field Approximation	H. R. Reiss	  The G\"oppert-Mayer (GM) gauge transformation, of central importance in atomic, molecular, and optical physics since it connects the length gauge and the velocity gauge, becomes unphysical as the field frequency declines towards zero. This is not consequential for theories of transverse fields, but it is the underlying reason for the failure of gauge invariance in the dipole-approximation version of the Strong-Field Approximation (SFA). This failure of the GM gauge transformation explains why the length gauge is preferred in analytical approximation methods for fields that possess a constant electric field as a zero-frequency limit. 	
1509.06668v1	http://arxiv.org/pdf/1509.06668v1	2015	Efficient failure probability calculation through mesh refinement	Jing Li|Panos Stinis	  We present a novel way of accelerating hybrid surrogate methods for the calculation of failure probabilities. The main idea is to use mesh refinement in order to obtain improved local surrogates of low computation cost to simulate on. These improved surrogates can reduce significantly the required number of evaluations of the exact model (which is the usual bottleneck of failure probability calculations). Meanwhile the effort on evaluations of surrogates is dramatically reduced by utilizing low order local surrogates. Numerical results of the application of the proposed approach in several examples of increasing complexity show the robustness, versatility and gain in efficiency of the method. 	
1510.00990v1	http://arxiv.org/pdf/1510.00990v1	2015	On the Failure of BD-N and BD, and an Application to the Anti-Specker   Property	Robert Lubarsky	  We give the natural topological model for the failure of BD-N, and use it to show that the closure of spaces with the anti-Specker property under product does not imply BD-N. Also, the natural topological model for the failure of BD is presented. Finally, for some of the realizability models known indirectly to falsify BD-N, it is brought out in detail how BD-N fails. 	
1607.01540v1	http://arxiv.org/pdf/1607.01540v1	2016	Metallized Film Capacitor Lifetime Evaluation and Failure Mode Analysis	R. Gallay	  One of the main concerns for power electronic engineers regarding capacitors is to predict their remaining lifetime in order to anticipate costly failures or system unavailability. This may be achieved using a Weibull statistical law combined with acceleration factors for the temperature, the voltage, and the humidity. This paper discusses the different capacitor failure modes and their effects and consequences. 	
1610.02885v1	http://arxiv.org/pdf/1610.02885v1	2016	Hardening Cassandra Against Byzantine Failures	Roy Friedman|Roni Licher	  Cassandra is one of the most widely used distributed data stores these days. Cassandra supports flexible consistency guarantees over a wide-column data access model and provides almost linear scale-out performance. This enables application developers to tailor the performance and availability of Cassandra to their exact application's needs and required semantics. Yet, Cassandra is designed to withstand benign failures, and cannot cope with most forms of Byzantine attacks.   In this work, we present an analysis of Cassandra's vulnerabilities and propose protocols for hardening Cassandra against Byzantine failures. We examine several alternative design choices and compare between them both qualitatively and empirically by using the Yahoo! Cloud Serving Benchmark (YCSB) performance benchmark. We include incremental performance analysis for our algorithmic and cryptographic adjustments, supporting our design choices. 	
1704.05396v1	http://arxiv.org/pdf/1704.05396v1	2017	A Study of Deep Learning Robustness Against Computation Failures	Jean-Charles Vialatte|François Leduc-Primeau	  For many types of integrated circuits, accepting larger failure rates in computations can be used to improve energy efficiency. We study the performance of faulty implementations of certain deep neural networks based on pessimistic and optimistic models of the effect of hardware faults. After identifying the impact of hyperparameters such as the number of layers on robustness, we study the ability of the network to compensate for computational failures through an increase of the network size. We show that some networks can achieve equivalent performance under faulty implementations, and quantify the required increase in computational complexity. 	
0406005v3	http://arxiv.org/pdf/cs/0406005v3	2004	Microreboot -- A Technique for Cheap Recovery	George Candea|Shinichi Kawamoto|Yuichi Fujiki|Greg Friedman|Armando Fox	  A significant fraction of software failures in large-scale Internet systems are cured by rebooting, even when the exact failure causes are unknown. However, rebooting can be expensive, causing nontrivial service disruption or downtime even when clusters and failover are employed. In this work we separate process recovery from data recovery to enable microrebooting -- a fine-grain technique for surgically recovering faulty application components, without disturbing the rest of the application.   We evaluate microrebooting in an Internet auction system running on an application server. Microreboots recover most of the same failures as full reboots, but do so an order of magnitude faster and result in an order of magnitude savings in lost work. This cheap form of recovery engenders a new approach to high availability: microreboots can be employed at the slightest hint of failure, prior to node failover in multi-node clusters, even when mistakes in failure detection are likely; failure and recovery can be masked from end users through transparent call-level retries; and systems can be rejuvenated by parts, without ever being shut down. 	
0812.0972v2	http://arxiv.org/pdf/0812.0972v2	2008	Network Protection Codes: Providing Self-healing in Autonomic Networks   Using Network Coding	Salah A. Aly|Ahmed E. Kamal	  Agile recovery from link failures in autonomic communication networks is essential to increase robustness, accessibility, and reliability of data transmission. However, this must be done with the least amount of protection resources, while using simple management plane functionality. Recently, network coding has been proposed as a solution to provide agile and cost efficient network self-healing against link failures, in a manner that does not require data rerouting, packet retransmission, or failure localization, hence leading to simple control and management planes. To achieve this, separate paths have to be provisioned to carry encoded packets, hence requiring either the addition of extra links, or reserving some of the resources for this purpose.   In this paper we introduce autonomic self-healing strategies for autonomic networks in order to protect against link failures. The strategies are based on network coding and reduced capacity, which is a technique that we call network protection codes (NPC). In these strategies, an autonomic network is able to provide self-healing from various network failures affecting network operation. The techniques improve service and enhance reliability of autonomic communication.   Network protection codes are extended to provide self-healing from multiple link failures in autonomic networks. We provide implementation aspects of the proposed strategies. We present bounds and network protection code constructions. Finally, we study the construction of such codes over the binary field. The paper also develops an Integer Linear Program formulation to evaluate the cost of provisioning connections using the proposed strategies. 	
0905.2248v3	http://arxiv.org/pdf/0905.2248v3	2010	Protection against link errors and failures using network coding	Shizheng Li|Aditya Ramamoorthy	  We propose a network-coding based scheme to protect multiple bidirectional unicast connections against adversarial errors and failures in a network. The network consists of a set of bidirectional primary path connections that carry the uncoded traffic. The end nodes of the bidirectional connections are connected by a set of shared protection paths that provide the redundancy required for protection. Such protection strategies are employed in the domain of optical networks for recovery from failures. In this work we consider the problem of simultaneous protection against adversarial errors and failures.   Suppose that n_e paths are corrupted by the omniscient adversary. Under our proposed protocol, the errors can be corrected at all the end nodes with 4n_e protection paths. More generally, if there are n_e adversarial errors and n_f failures, 4n_e + 2n_f protection paths are sufficient. The number of protection paths only depends on the number of errors and failures being protected against and is independent of the number of unicast connections. 	
0907.3371v1	http://arxiv.org/pdf/0907.3371v1	2009	Robot Reliability Using Petri Nets and Fuzzy Lambda-Tau Methodology	Ajay Kumar|S. P. Sharma|Dinesh Kumar	  Robot reliability has become an increasingly important issue in the last few years due to increased application of robots in many industries (like automobile industry) under hazardous and unstructured environment. As the component failure behavior is dependent on configuration and environment, the available information about the constituent component of robots is most of the time imprecise, incomplete, vague and conflicting and so it is very difficult to analyze their behavior and to predict their failure pattern. The reliability analysis of any system provides an understanding about the likelihood of failures occurring in the system/component and the increased insight about its inherent weakness. The objective of this paper is to quantify the uncertainties that makes the decision more realistic, generic and extendable to application domain. In this paper various reliability parameters (such as mean time between failures, expected number of failures, reliability, availability etc.) are computed using Fuzzy Lambda-Tau methodology. Triangular fuzzy numbers are used to represent failure rates and repair times as they allow expert opinion, linguistic variables, operating conditions, uncertainty and imprecision in reliability information, to be incorporated into system model. Petri Nets are used because unlike the fault tree methodology, the use of Petri Nets allows efficient simultaneous generation of minimal cut and path sets. 	
1110.1842v3	http://arxiv.org/pdf/1110.1842v3	2011	Failure Detectors in Homonymous Distributed Systems (with an Application   to Consensus)	Sergio Arévalo|Antonio Fernández Anta|Damien Imbs|Ernesto Jiménez|Michel Raynal	  This paper addresses the consensus problem in homonymous distributed systems where processes are prone to crash failures and have no initial knowledge of the system membership ("homonymous" means that several processes may have the same identifier). New classes of failure detectors suited to these systems are first defined. Among them, the classes H\Omega\ and H\Sigma\ are introduced that are the homonymous counterparts of the classes \Omega\ and \Sigma, respectively. (Recall that the pair <\Omega,\Sigma> defines the weakest failure detector to solve consensus.) Then, the paper shows how H\Omega\ and H\Sigma\ can be implemented in homonymous systems without membership knowledge (under different synchrony requirements). Finally, two algorithms are presented that use these failure detectors to solve consensus in homonymous asynchronous systems where there is no initial knowledge of the membership. One algorithm solves consensus with <H\Omega,H\Sigma>, while the other uses only H\Omega, but needs a majority of correct processes.   Observe that the systems with unique identifiers and anonymous systems are extreme cases of homonymous systems from which follows that all these results also apply to these systems. Interestingly, the new failure detector class H\Omega\ can be implemented with partial synchrony, while the analogous class A\Omega\ defined for anonymous systems can not be implemented (even in synchronous systems). Hence, the paper provides us with the first proof showing that consensus can be solved in anonymous systems with only partial synchrony (and a majority of correct processes). 	
1206.0244v2	http://arxiv.org/pdf/1206.0244v2	2012	Detection Performance in Balanced Binary Relay Trees with Node and Link   Failures	Zhenliang Zhang|Edwin K. P. Chong|Ali Pezeshki|William Moran|Stephen D. Howard	  We study the distributed detection problem in the context of a balanced binary relay tree, where the leaves of the tree correspond to $N$ identical and independent sensors generating binary messages. The root of the tree is a fusion center making an overall decision. Every other node is a relay node that aggregates the messages received from its child nodes into a new message and sends it up toward the fusion center. We derive upper and lower bounds for the total error probability $P_N$ as explicit functions of $N$ in the case where nodes and links fail with certain probabilities. These characterize the asymptotic decay rate of the total error probability as $N$ goes to infinity. Naturally, this decay rate is not larger than that in the non-failure case, which is $\sqrt N$. However, we derive an explicit necessary and sufficient condition on the decay rate of the local failure probabilities $p_k$ (combination of node and link failure probabilities at each level) such that the decay rate of the total error probability in the failure case is the same as that of the non-failure case. More precisely, we show that $\log P_N^{-1}=\Theta(\sqrt N)$ if and only if $\log p_k^{-1}=\Omega(2^{k/2})$. 	
1401.4473v1	http://arxiv.org/pdf/1401.4473v1	2013	The Impact of the Topology on Cascading Failures in Electric Power Grids	Yakup Koç|Martijn Warnier|Piet Van Mieghem|Robert E. Kooij|Frances M. T. Brazier	  Cascading failures are one of the main reasons for blackouts in power transmission grids. The topology of a power grid, together with its operative state determine, for the most part, the robustness of the power grid against cascading failures. Secure electrical power supply requires, together with careful operation, a robust design of the electrical power grid topology. This paper investigates the impact of a power grid topology on its robustness against cascading failures. Currently, the impact of the topology on a grid robustness is mainly assessed by using purely topological approaches that fail to capture the essence of electric power flow. This paper proposes a metric, the effective graph resistance, that relates the topology of a power grid to its robustness against cascading failures by deliberate attacks, while also taking the fundamental characteristics of the electric power grid into account such as power flow allocation according to Kirchoff Laws. Experimental verification shows that the proposed metric anticipates the grid robustness accurately. The proposed metric is used to optimize a grid topology for a higher level of robustness. To demonstrate its applicability, the metric is applied on the IEEE 118 bus power system to improve its robustness against cascading failures. 	
1402.1780v1	http://arxiv.org/pdf/1402.1780v1	2014	Cascading Failures in Power Grids - Analysis and Algorithms	Saleh Soltan|Dorian Mazauric|Gil Zussman	  This paper focuses on cascading line failures in the transmission system of the power grid. Recent large-scale power outages demonstrated the limitations of percolation- and epid- emic-based tools in modeling cascades. Hence, we study cascades by using computational tools and a linearized power flow model. We first obtain results regarding the Moore-Penrose pseudo-inverse of the power grid admittance matrix. Based on these results, we study the impact of a single line failure on the flows on other lines. We also illustrate via simulation the impact of the distance and resistance distance on the flow increase following a failure, and discuss the difference from the epidemic models. We then study the cascade properties, considering metrics such as the distance between failures and the fraction of demand (load) satisfied after the cascade (yield). We use the pseudo-inverse of admittance matrix to develop an efficient algorithm to identify the cascading failure evolution, which can be a building block for cascade mitigation. Finally, we show that finding the set of lines whose removal has the most significant impact (under various metrics) is NP-Hard and introduce a simple heuristic for the minimum yield problem. Overall, the results demonstrate that using the resistance distance and the pseudo-inverse of admittance matrix provides important insights and can support the development of efficient algorithms. 	
1402.6809v1	http://arxiv.org/pdf/1402.6809v1	2014	Analyzing Cascading Failures in Smart Grids under Random and Targeted   Attacks	Sushmita Ruj|Arindam Pal	  We model smart grids as complex interdependent networks, and study targeted attacks on smart grids for the first time. A smart grid consists of two networks: the power network and the communication network, interconnected by edges. Occurrence of failures (attacks) in one network triggers failures in the other network, and propagates in cascades across the networks. Such cascading failures can result in disintegration of either (or both) of the networks. Earlier works considered only random failures. In practical situations, an attacker is more likely to compromise nodes selectively.   We study cascading failures in smart grids, where an attacker selectively compromises the nodes with probabilities proportional to their degrees; high degree nodes are compromised with higher probability. We mathematically analyze the sizes of the giant components of the networks under targeted attacks, and compare the results with the corresponding sizes under random attacks. We show that networks disintegrate faster for targeted attacks compared to random attacks. A targeted attack on a small fraction of high degree nodes disintegrates one or both of the networks, whereas both the networks contain giant components for random attack on the same fraction of nodes. 	
1410.3512v1	http://arxiv.org/pdf/1410.3512v1	2014	Cascading Failures in Finite-Size Random Geometric Networks	Ali Eslami|Chuan Huang|Junshan Zhang|Shuguang Cui	  The problem of cascading failures in cyber-physical systems is drawing much attention in lieu of different network models for a diverse range of applications. While many analytic results have been reported for the case of large networks, very few of them are readily applicable to finite-size networks. This paper studies cascading failures in finite-size geometric networks where the number of nodes is on the order of tens or hundreds as in many real-life networks. First, the impact of the tolerance parameter on network resiliency is investigated. We quantify the network reaction to initial disturbances of different sizes by measuring the damage imposed on the network. Lower and upper bounds on the number of failures are derived to characterize such damages. Such finite-size analysis reveals the decisiveness and criticality of taking action within the first few stages of failure propagation in preventing a cascade. By studying the trend of the bounds as the number of nodes increases, we observe a phase transition phenomenon in terms of the tolerance parameter. The critical value of the tolerance parameter, known as the threshold, is further derived. The findings of this paper, in particular, shed light on how to choose the tolerance parameter appropriately such that a cascade of failures could be avoided. 	
1411.5926v1	http://arxiv.org/pdf/1411.5926v1	2014	Stress and Failure Analysis of Rapidly Rotating Asteroid (29075) 1950 DA	Masatoshi Hirabayashi|Daniel J. Scheeres	  Rozitis et al. recently reported that near-Earth asteroid (29075) 1950 DA, whose bulk density ranges from 1.0 g/cm3 to 2.4 g/cm3, is a rubble pile and requires a cohesive strength of at least 44 Pa to 74 Pa to keep from failing due to its fast spin period. Since their technique for giving failure conditions required the averaged stress over the whole volume, it discarded information about the asteroid's failure mode and internal stress condition. This paper develops a finite element model and revisits the stress and failure analysis of 1950 DA. For the modeling, we do not consider material-hardening and softening. Under the assumption of an associated flow rule and uniform material distribution, we identify the deformation process of 1950 DA when its constant cohesion reaches the lowest value that keeps its current shape. The results show that to avoid structural failure the internal core requires a cohesive strength of at least 75 Pa - 85 Pa. It suggests that for the failure mode of this body, the internal core first fails structurally, followed by the surface region. This implies that if cohesion is constant over the whole volume, the equatorial ridge of 1950 DA results from a material flow going outward along the equatorial plane in the internal core, but not from a landslide as has been hypothesized. This has additional implications for the likely density of the interior of the body. 	
1604.06733v3	http://arxiv.org/pdf/1604.06733v3	2016	Cascading Failures in AC Electricity Grids	Martin Rohden|Daniel Jung|Samyak Tamrakar|Stefan Kettemann	  Sudden failure of a single transmission element in a power grid can induce a domino effect of cascading failures, which can lead to the isolation of a large number of consumers or even to the failure of the entire grid. Here we present results of the simulation of cascading failures in power grids, using an alternating current (AC) model. We first apply this model to a regular square grid topology. For a random placement of consumers and generators on the grid, the probability to find more than a certain number of unsupplied consumers decays as a power law and obeys a scaling law with respect to system size. Varying the transmitted power threshold above which a transmission line fails does not seem to change the power law exponent $q \approx 1.6$. Furthermore, we study the influence of the placement of generators and consumers on the number of affected consumers and demonstrate that large clusters of generators and consumers are especially vulnerable to cascading failures. As a real-world topology we consider the German high-voltage transmission grid. Applying the dynamic AC model and considering a random placement of consumers, we find that the probability to disconnect more than a certain number of consumers depends strongly on the threshold. For large thresholds the decay is clearly exponential, while for small ones the decay is slow, indicating a power law decay. 	
1607.06865v3	http://arxiv.org/pdf/1607.06865v3	2017	Connectivity Oracles for Graphs Subject to Vertex Failures	Ran Duan|Seth Pettie	  We introduce new data structures for answering connectivity queries in graphs subject to batched vertex failures. A deterministic structure processes a batch of $d\leq d_{\star}$ failed vertices in $\tilde{O}(d^3)$ time and thereafter answers connectivity queries in $O(d)$ time. It occupies space $O(d_{\star} m\log n)$. We develop a randomized Monte Carlo version of our data structure with update time $\tilde{O}(d^2)$, query time $O(d)$, and space $\tilde{O}(m)$ for any failure bound $d\le n$. This is the first connectivity oracle for general graphs that can efficiently deal with an unbounded number of vertex failures.   We also develop a more efficient Monte Carlo edge-failure connectivity oracle. Using space $O(n\log^2 n)$, $d$ edge failures are processed in $O(d\log d\log\log n)$ time and thereafter, connectivity queries are answered in $O(\log\log n)$ time, which are correct w.h.p.   Our data structures are based on a new decomposition theorem for an undirected graph $G=(V,E)$, which is of independent interest. It states that for any terminal set $U\subseteq V$ we can remove a set $B$ of $|U|/(s-2)$ vertices such that the remaining graph contains a Steiner forest for $U-B$ with maximum degree $s$. 	
1705.09822v2	http://arxiv.org/pdf/1705.09822v2	2017	The risk factors affecting to the software quality failures in Sri   Lankan Software industry	Namadawa Bashini Jeewanthi Gamage	  Software project failure and cancellation rates increase day by day due to technical failures, quality failures, lack of end client acceptance etc. and also the lack of proper management. There are a number of reasons affected by the software project failures. According to empirical evidence, inadequate testing resources are one of the major factors that contribute to the poor quality. The main objectives of this study are to study the risk factors that affect the software quality to provide some recommendation to minimize the risk of poor quality. There are three main factors affecting to software quality namely proper testing, test planning and QA team which are directly impacted to the software quality risks. To conduct this study, I employed an open-ended questionnaire for collecting qualitative data from responses analyzed them using thematic approach method. The participants with their experiences agreed only with requirement clarity and clearly defined acceptance criteria, not with adequate unit testing and finally and also with that not doing regression testing force to quality failures. As of data analysis, not having proper formal test planning, initial test planning not being realistic, not following quality risk management, non-proper process and contingency action planning also lead to the risk of poor project quality. According to the participants added that the following factors are also behind the reasons for the lack quality of software. The experienced and skilled employees move out from the company as there is not a proper QA process and team members as they do not have the risk management mentality. 	
1706.07575v1	http://arxiv.org/pdf/1706.07575v1	2017	A generic construction of quantum-oblivious-key-transfer-based private   query with ideal database security and zero failure	Chun-Yan Wei|Xiao-Qiu Cai|Bin Liu|Tian-Yin Wang|Fei Gao	  Higher security and lower failure probability have always been people's pursuits in quantum-oblivious-key-transfer-based private query (QOKT-PQ) protocols since Jacobi \emph{et al}. [Phys. Rev. A 83, 022301 (2011)] proposed the first protocol of this kind. However, higher database security generally has to be obtained at the cost of a higher failure probability, and vice versa. Recently, based on a round-robin differential-phase-shift quantum key distribution protocol, Liu \emph{et al}. [Sci. China-Phys. Mech. Astron.58, 100301 (2015)] presented a private query protocol (RRDPS-PQ protocol) utilizing ideal single-photon signal which realizes both ideal database security and zero failure probability. However, ideal single-photon source is not available today, and for large database the required pulse train is too long to implement. Here, we reexamine the security of RRDPS-PQ protocol under imperfect source and present an improved protocol using a special "low-shift and addition" (LSA) technique, which not only can be used to query from large database but also retains the features of "ideal database security" and "zero-failure" even under weak coherent source. Finally, we generalize the LSA technique and establish a generic QOKT-PQ model in which both "ideal database security" and "zero failure" are achieved via acceptable communications. 	
9812068v1	http://arxiv.org/pdf/cond-mat/9812068v1	1998	Incipient failure in sandpile models	O. Narayan|S. R. Nagel	  Elastoplastic and constitutive equation theories are two approaches based on very different assumptions for creating a continuum theory for the stress distributions in a static sandpile. Both models produce the same surprising prediction that in a two dimensional granular pile constructed at its angle of repose, the outside wedge will be on the verge of failure. We show how these predictions can be tested experimentally. 	
0209038v1	http://arxiv.org/pdf/cond-mat/0209038v1	2002	Real event detection and the treatment of congestive heart failure: an e   fficient technique to help cardiologists to make crucial decisions	P. Allegrini|R. Balocchi|S. Chillemi|P. Grigolini|P. Hamilton|R. Maestri|L. Palatella|G. Raffaelli	  Using a method of entropic analysis of time series we establish the correlation between heartbeat long-range memory and mortality risk in patients with congestive heart failure. 	
0403032v1	http://arxiv.org/pdf/cs/0403032v1	2004	Where Fail-Safe Default Logics Fail	Paolo Liberatore	  Reiter's original definition of default logic allows for the application of a default that contradicts a previously applied one. We call failure this condition. The possibility of generating failures has been in the past considered as a semantical problem, and variants have been proposed to solve it. We show that it is instead a computational feature that is needed to encode some domains into default logic. 	
0009078v1	http://arxiv.org/pdf/math/0009078v1	2000	On properties of theories which preclude the existence of universal   models	Mirna Džamonja|Saharon Shelah	  In this paper we investigate some properties of first order theories which prevent them from having universal models under certain cardinal arithmetic assumptions. Our results give a new syntactical condition, oak property, which is a sufficient condition for a theory not to have universal models in cardinality lambda when certain cardinal arithmetic assumptions implying the failure of GCH (and close to the failure of SCH) hold. 	
0711.3218v1	http://arxiv.org/pdf/0711.3218v1	2007	An Integral Measure of Aging/Rejuvenation for Repairable and   Non-repairable Systems	Mark Kaminskiy|Vasiliy Krivtsov	  This paper introduces a simple index that helps to assess the degree of aging or rejuvenation of a (non)repairable system. The index ranges from -1 to 1 and is negative for the class of decreasing failure rate distributions (or deteriorating point processes) and is positive for the increasing failure rate distributions (or improving point processes). The introduced index is distribution free. 	
0810.4904v1	http://arxiv.org/pdf/0810.4904v1	2008	On Finite Bases for Weak Semantics: Failures versus Impossible Futures	Taolue Chen|Wan Fokkink|Rob van Glabbeek	  We provide a finite basis for the (in)equational theory of the process algebra BCCS modulo the weak failures preorder and equivalence. We also give positive and negative results regarding the axiomatizability of BCCS modulo weak impossible futures semantics. 	
0903.0343v1	http://arxiv.org/pdf/0903.0343v1	2009	Exact three-dimensional wave function and the on-shell t-matrix for the   sharply cut off Coulomb potential: failure of the standard renormalization   factor	W. Glockle|J. Golak|R. Skibinski|H. Witala	  The 3-dimensional wave function for a sharply cut-off Coulomb potential is analytically derived. The asymptotic form of the related scattering amplitude reveals a failure of the standard renormalization factor which is believed to be generally valid for any type of screening. 	
1104.5161v1	http://arxiv.org/pdf/1104.5161v1	2011	Hubbard-Stratonovich Transformation: Successes, Failure, and Cure	H. Kleinert	  We recall the successes of the Hubbard-Stratonovich Transformation (HST) of many-body theory, point out its failure to cope with competing channels of collective phenomena and show how to overcome this by Variational Perturbation Theory. That yields exponentially fast converging results, thanks to the help of a variety of {\it collective classical fields}, rather than a fluctuating {\it collective quantum field} as suggested by the HST. 	
1106.1846v1	http://arxiv.org/pdf/1106.1846v1	2011	New Efficient Error-Free Multi-Valued Consensus with Byzantine Failures	Guanfeng Liang|Nitin Vaidya	  In this report, we investigate the multi-valued Byzantine consensus problem. We introduce two algorithms: the first one achieves traditional validity requirement for consensus, and the second one achieves a stronger "q-validity" requirement. Both algorithms are more efficient than the ones introduces in our recent PODC 2011 paper titled "Error-Free Multi-Valued Consensus with Byzantine Failures". 	
1110.3390v1	http://arxiv.org/pdf/1110.3390v1	2011	Bayesian Post-Processor and other Enhancements of Subset Simulation for   Estimating Failure Probabilities in High Dimensions	Konstantin M. Zuev|James L. Beck|Siu-Kui Au|Lambros S. Katafygiotis	  Estimation of small failure probabilities is one of the most important and challenging computational problems in reliability engineering. The failure probability is usually given by an integral over a high-dimensional uncertain parameter space that is difficult to evaluate numerically. This paper focuses on enhancements to Subset Simulation (SS), proposed by Au and Beck, which provides an efficient algorithm based on MCMC (Markov chain Monte Carlo) simulation for computing small failure probabilities for general high-dimensional reliability problems. First, we analyze the Modified Metropolis algorithm (MMA), an MCMC technique, which is used in SS for sampling from high-dimensional conditional distributions. We present some observations on the optimal scaling of MMA, and develop an optimal scaling strategy for this algorithm when it is employed within SS. Next, we provide a theoretical basis for the optimal value of the conditional failure probability $p_0$, an important parameter one has to choose when using SS. Finally, a Bayesian post-processor SS+ for the original SS method is developed where the uncertain failure probability that one is estimating is modeled as a stochastic variable whose possible values belong to the unit interval. Simulated samples from SS are viewed as informative data relevant to the system's reliability. Instead of a single real number as an estimate, SS+ produces the posterior PDF of the failure probability, which takes into account both prior information and the information in the sampled data. This PDF quantifies the uncertainty in the value of the failure probability and it may be further used in risk analyses to incorporate this uncertainty. The relationship between the original SS and SS+ is also discussed 	
1304.7710v1	http://arxiv.org/pdf/1304.7710v1	2013	Learning Geo-Temporal Non-Stationary Failure and Recovery of Power   Distribution	Yun Wei|Chuanyi Ji|Floyd Galvan|Stephen Couvillon|George Orellana|James Momoh	  Smart energy grid is an emerging area for new applications of machine learning in a non-stationary environment. Such a non-stationary environment emerges when large-scale failures occur at power distribution networks due to external disturbances such as hurricanes and severe storms. Power distribution networks lie at the edge of the grid, and are especially vulnerable to external disruptions. Quantifiable approaches are lacking and needed to learn non-stationary behaviors of large-scale failure and recovery of power distribution. This work studies such non-stationary behaviors in three aspects. First, a novel formulation is derived for an entire life cycle of large-scale failure and recovery of power distribution. Second, spatial-temporal models of failure and recovery of power distribution are developed as geo-location based multivariate non-stationary GI(t)/G(t)/Infinity queues. Third, the non-stationary spatial-temporal models identify a small number of parameters to be learned. Learning is applied to two real-life examples of large-scale disruptions. One is from Hurricane Ike, where data from an operational network is exact on failures and recoveries. The other is from Hurricane Sandy, where aggregated data is used for inferring failure and recovery processes at one of the impacted areas. Model parameters are learned using real data. Two findings emerge as results of learning: (a) Failure rates behave similarly at the two different provider networks for two different hurricanes but differently at the geographical regions. (b) Both rapid- and slow-recovery are present for Hurricane Ike but only slow recovery is shown for a regional distribution network from Hurricane Sandy. 	
1306.4943v1	http://arxiv.org/pdf/1306.4943v1	2013	Failure of Calibration is Typical	Gordon Belot	  Schervish (1985b) showed that every forecasting system is noncalibrated for uncountably many data sequences that it might see. This result is strengthened here: from a topological point of view, failure of calibration is typical and calibration rare. Meanwhile, Bayesian forecasters are certain that they are calibrated---this invites worries about the connection between Bayesianism and rationality. 	
1402.6881v1	http://arxiv.org/pdf/1402.6881v1	2014	Obstructions de Brauer-Manin entières sur les espaces homogènes à   stabilisateurs finis nilpotents	Cyril Demarche	  Let $k$ be a number field. We construct homogeneous spaces of $SL_{n,k}$ with finite nilpotent non-abelian stabilizers for which the Brauer-Manin obstruction does not explain the failure of strong approximation (resp. the failure of the integral Hasse principle). 	
1505.00692v1	http://arxiv.org/pdf/1505.00692v1	2015	Dual Failure Resilient BFS Structure	Merav Parter	  We study {\em breadth-first search (BFS)} spanning trees, and address the problem of designing a sparse {\em fault-tolerant} BFS structure, or {\em FT-BFS } for short, resilient to the failure of up to two edges in the given undirected unweighted graph $G$, i.e., a sparse subgraph $H$ of $G$ such that subsequent to the failure of up to two edges, the surviving part $H'$ of $H$ still contains a BFS spanning tree for (the surviving part of) $G$. FT-BFS structures, as well as the related notion of replacement paths, have been studied so far for the restricted case of a single failure. It has been noted widely that when concerning shortest-paths in a variety of contexts, there is a sharp qualitative difference between a single failure and two or more failures.   Our main results are as follows. We present an algorithm that for every $n$-vertex unweighted undirected graph $G$ and source node $s$ constructs a (two edge failure) FT-BFS structure rooted at $s$ with $O(n^{5/3})$ edges. To provide a useful theory of shortest paths avoiding 2 edges failures, we take a principled approach to classifying the arrangement these paths. We believe that the structural analysis provided in this paper may decrease the barrier for understanding the general case of $f\geq 2$ faults and pave the way to the future design of $f$-fault resilient structures for $f \geq 2$. We also provide a matching lower bound, which in fact holds for the general case of $f \geq 1$ and multiple sources $S \subseteq V$. It shows that for every $f\geq 1$, and integer $1 \leq \sigma \leq n$, there exist $n$-vertex graphs with a source set $S \subseteq V$ of cardinality $\sigma$ for which any FT-BFS structure rooted at each $s \in S$, resilient to up to $f$-edge faults has $\Omega(\sigma^{1/(f+1)} \cdot n^{2-1/(f+1)})$ edges. 	
1706.01226v1	http://arxiv.org/pdf/1706.01226v1	2017	Failure of 0-1 law for sparse random graph in strong logics	Saharon Shelah	  Let $\alpha\in(0,1)_\mathbb{R}$ be irrational and $G_n = G_{{n, 1/n}^\alpha}$ be the random graph with edge probability $1/n^\alpha$; we know that it satisfies the 0-1 law for first order logic. We deal with the failure of the 0-1 law for stronger logics: $\mathbb{L}_{ \infty, k}, k$ large enough and the LFP, least fix point logic. 	
1707.03173v2	http://arxiv.org/pdf/1707.03173v2	2018	Reliability of components of coherent systems: estimates in presence of   masked data	Agatha Sacramento Rodrigues|Carlos Alberto de Braganca Pereira|Adriano Polpo	  The reliability of a system of components depends on reliability of each component. Thus, the initial statistical work should be the estimation of the reliability of each component of the system. This is not an easy task because when the system fails, the failure time of a given component can not be observed, that is, censored data. Rodrigues et al. (2017) presented a solution for reliability estimation of components when it is avaliable the system failure time and the status of each component at the time of system failure (if it had failed before, after or it is responsible for system failure). However, there are situations it may be difficult to identify the status of components at the moment of system failure.   Such cases are systems with masked causes of failure. Since parallel and series systems are the simplest systems, innumerous alternative solutions for these two systems have been appeared in the literature. To the best of our knowledge, this seems to be the first work that considers the general case of coherent systems. The three-parameter Weibull distribution is considered as the component failure time model. Identically distributed failure times is not required restrictions. Furthermore, there is no restriction on the subjective choice of prior distributions but preference has been given to continuous prior distributions; these priors represent well the nuances of the environment that the system operates. The statistical work of obtaining quantities of the posterior distribution is supported by the Metropolis within Gibbs algorithm. With several simulations, the excellent performance of the model was evaluated. We also consider a computer hard-drives real dataset in order to present the practical relevance of the proposed model. 	
1709.03707v1	http://arxiv.org/pdf/1709.03707v1	2017	Failure of the local-global principle for isotropy of quadratic forms   over rational function fields	Asher Auel|V. Suresh	  We prove the failure of the local-global principle, with respect to all discrete valuations, for isotropy of quadratic forms over a rational function field of transcendence degree at least 2 over the complex numbers. Our construction involves the generalized Kummer varieties considered by Borcea and Cynk--Hulek. 	
0710.5082v1	http://arxiv.org/pdf/0710.5082v1	2007	Heating of blue compact dwarf galaxies: gas distribution and   photoionization by stars in I Zw 18	D. Pequignot	  Photoionization models so far are unable to account for the high electron temperature Te([O III]) implied by the line ratio [O III]4363A/[O III]5007A in low-metallicity blue compact dwarf galaxies, casting doubts on the assumption of photoionization by hot stars as the dominant source of heating of the gas in these objects. Combinations of runs of the 1-D photoionization code NEBU are used to explore alternative models for the giant H II region shell I Zw 18 NW. Acceptable models are obtained, which represent schematically an incomplete shell comprising radiation-bounded condensations embedded in a low-density matter-bounded diffuse medium. The thermal pressure contrast between gas components is about a factor 7. The diffuse phase can be in pressure balance with the hot superbubble fed by mechanical energy from the inner massive star cluster. The failure of previous modellings is ascribed to (1) the adoption of an inadequate small-scale gas density distribution, which proves critical when the collisional excitation of hydrogen contributes significantly to the cooling of the gas, and possibly (2) a too restrictive implementation of Wolf-Rayet stars in synthetic stellar cluster spectral energy distributions. A neutral gas component heated by soft X-rays, whose power is less than 1% of the star cluster luminosity and consistent with CHANDRA data, can explain the low-ionization fine-structure lines detected by SPITZER. [O/Fe] is slightly smaller in I Zw 18 NW than in Galactic Halo stars of similar metallicity and [C/O] is correlatively large. Extra heating by, e.g., dissipation of mechanical energy is not required to explain Te([O III]) in I Zw 18. Important astrophysical developments are at stakes in the 5% uncertainty attached to [O III] collision strengths. 	
0909.5552v1	http://arxiv.org/pdf/0909.5552v1	2009	On the physical origin of the second solar spectrum of the Sc II line at   4247 A	Luca Belluzzi	  The peculiar three-peak structure of the linear polarization profile shown in the second solar spectrum by the Ba II line at 4554 A has been interpreted as the result of the different contributions coming from the barium isotopes with and without hyperfine structure (HFS). In the same spectrum, a triple peak polarization signal is also observed in the Sc II line at 4247 A. Scandium has a single stable isotope (^{45}Sc), which shows HFS due to a nuclear spin I=7/2. We investigate the possibility of interpreting the linear polarization profile shown in the second solar spectrum by this Sc II line in terms of HFS. A two-level model atom with HFS is assumed. Adopting an optically thin slab model, the role of atomic polarization and of HFS is investigated, avoiding the complications caused by radiative transfer effects. The slab is assumed to be illuminated from below by the photospheric continuum, and the polarization of the radiation scattered at 90 degrees is investigated. The three-peak structure of the scattering polarization profile observed in this Sc II line cannot be fully explained in terms of HFS. Given the similarities between the Sc II line at 4247 A and the Ba II line at 4554 A, it is not clear why, within the same modeling assumptions, only the three-peak Q/I profile of the barium line can be fully interpreted in terms of HFS. The failure to interpret this Sc II polarization signal raises important questions, whose resolution might lead to significant improvements in our understanding of the second solar spectrum. In particular, if the three-peak structure of the Sc II signal is actually produced by a physical mechanism neglected within the approach considered here, it will be extremely interesting not only to identify this mechanism, but also to understand why it seems to be less important in the case of the barium line. 	
1003.1736v4	http://arxiv.org/pdf/1003.1736v4	2011	Modeling rf breakdown arcs	Zeke Insepov|Jim Norem|Thomas Proslier|Dazhang Huang|Sudhakar Mahalingam|Seth Veitzer	  We describe breakdown in 805 MHz rf accelerator cavities in terms of a number of self consistent mechanisms. We divide the breakdown process into three stages: 1) we model surface failure using molecular dynamics of fracture caused by electrostatic tensile stress, 2) we model the ionization of neutrals responsible for plasma initiation and plasma growth using a particle in cell code, and 3) we model surface damage by assuming a process similar to unipolar arcing. We find that the cold, dense plasma in contact with the surface produces very small Debye lengths and very high electric fields over a large area, consistent with unipolar arc behavior, although unipolar arcs are strictly defined with equipotential boundaries. These high fields produce strong erosion mechanisms, primarily self sputtering, compatible with the crater formation that we see. We use OOPIC modeling to estimate very high surface electric fields in the dense plasma and measure these field using electrohydrodynamic arguments to relate the dimensions of surface damage with the applied electric field. We also present a geometrical explanation of the large enhancement factors of field emitters.This is consistent with the apparent absence of whiskers on surfaces exposed to high fields. The enhancement factors we derive, when combined with the Fowler-Nordheim analysis produce a consistent picture of breakdown and field emission from surfaces at local fields of 7 - 10 GV/m. We show that the plasma growth rates we obtain from OOPIC are consistent with growth rates of the cavity shorting currents using x ray measurements. We believe the general picture presented here for rf breakdown arcs should be directly applicable to a larger class of vacuum arcs. Results from the plasma simulation are included as a guide to experimental verification of this model. 	
1406.3788v1	http://arxiv.org/pdf/1406.3788v1	2014	Empirical Tests of Pre-Main-Sequence Stellar Evolution Models with   Eclipsing Binaries	Keivan G. Stassun|Gregory A. Feiden|Guillermo Torres	  We examine the performance of standard PMS stellar evolution models against the accurately measured properties of a benchmark sample of 26 PMS stars in 13 EB systems. We provide a definitive compilation of all fundamental properties for the EBs. We also provide a definitive compilation of the various PMS model sets. In the H-R diagram, the masses inferred for the individual stars by the models are accurate to better than 10% above 1 Msun, but below 1 Msun they are discrepant by 50-100%. We find evidence that the failure of the models to match the data is linked to the triples in the EB sample; at least half of the EBs possess tertiary companions. Excluding the triples, the models reproduce the stellar masses to better than ~10% in the H-R diagram, down to 0.5 Msun, below which the current sample is fully contaminated by tertiaries. We consider several mechanisms by which a tertiary might cause changes in the EB properties and thus corrupt the agreement with stellar model predictions. We show that the energies of the tertiary orbits are comparable to that needed to potentially explain the scatter in the EB properties through injection of heat, perhaps involving tidal interaction. It seems from the evidence at hand that this mechanism, however it operates in detail, has more influence on the surface properties of the stars than on their internal structure, as the lithium abundances are broadly in good agreement with model predictions. The EBs that are members of young clusters appear individually coeval to within 20%, but collectively show an apparent age spread of ~50%, suggesting true age spreads in young clusters. However, this apparent spread in the EB ages may also be the result of scatter in the EB properties induced by tertiaries. [Abridged] 	
1502.04571v2	http://arxiv.org/pdf/1502.04571v2	2015	Air Entrainment in Dynamic Wetting: Knudsen Effects and the Influence of   Ambient Air Pressure	James E. Sprittles	  Recent experiments on coating flows and liquid drop impact both demonstrate that wetting failures caused by air entrainment can be suppressed by reducing the ambient gas pressure. Here, it is shown that non-equilibrium effects in the gas can account for this behaviour, with ambient pressure reductions increasing the gas' mean free path and hence the Knudsen number $Kn$. These effects first manifest themselves through Maxwell slip at the gas' boundaries so that for sufficiently small $Kn$ they can be incorporated into a continuum model for dynamic wetting flows. The resulting mathematical model contains flow structures on the nano-, micro- and milli-metre scales and is implemented into a computational platform developed specifically for such multiscale phenomena. The coating flow geometry is used to show that for a fixed gas-liquid-solid system (a) the increased Maxwell slip at reduced pressures can substantially delay air entrainment, i.e. increase the `maximum speed of wetting', (b) unbounded maximum speeds are obtained as the pressure is reduced only when slip at the gas-liquid interface is allowed for and (c) the observed behaviour can be rationalised by studying the dynamics of the gas film in front of the moving contact line. A direct comparison to experimental results obtained in the dip-coating process shows that the model recovers most trends but does not accurately predict some of the high viscosity data at reduced pressures. This discrepancy occurs because the gas flow enters the `transition regime', so that more complex descriptions of its non-equilibrium nature are required. Finally, by collapsing onto a master curve experimental data obtained for drop impact in a reduced pressure gas, it is shown that the same physical mechanisms are also likely to govern splash suppression phenomena. 	
1505.01376v3	http://arxiv.org/pdf/1505.01376v3	2017	Tissue fibrosis: a principal proof for the central role of Misrepair in   aging	Jicun Wang-Michelitsch|Thomas M. Michelitsch	  Tissue fibrosis is the phenomenon that a tissue has progressive deposition of collagen fibers with age. Tissue fibrosis is associated with aging of most of our organs, and it is the main pathology in arteriosclerosis, chronic bronchitis/emphysema, and benign prostatic hyperplasia. The causes and characteristics of fibrosis are analyzed with Misrepair mechanism, a mechanism proposed in Misrepair-accumulation aging theory. Tissue fibrosis is known to be a result of repairs of tissue by collagen fibers. A repair with collagen fibers is a manner of "Misrepair". The collagen fibers are used for replacing dead cells or disrupted extracellular matrixes (ECMs) including elastic fibers, myofibers, and basement membrane. The progressive tissue fibrosis with age manifests the essential role of Misrepair in aging, because it reveals three facts: A. a process of Misrepair exists; B. Misrepairs are unavoidable; and C. Misrepairs accumulate. As a result of accumulation of Misrepairs of tissue with collagen fibers, tissue fibrosis is focalized and self-accelerating, appearing as growing of spots of hyaline degeneration. Fibrosis results in stiffness or atrophy of an organ and progressive failure of the organ. In arteriosclerosis, the deposition of collagen fibers in arterial wall is for replacing disrupted elastic fibers or myofibers, however results in hardness of the wall. Wrinkle formation is part of skin fibrosis, and it may be a result of accumulation of collagen fibers of different lengths. Senile hair-loss and hair-whitening are probably consequence of dermal fibrosis. In conclusion, tissue fibrosis is a result of accumulation of Misrepairs of tissue with collagen fibers, and the phenomenon of fibrosis is a powerful proof for the central role of Misrepair in aging. 	
1508.02129v1	http://arxiv.org/pdf/1508.02129v1	2015	Fracture resistance of zigzag single walled carbon nanotubes	Qiang Lu|Baidurya Bhattacharya	  Brittle fracture is one of the important failure modes of Single-Walled Carbon Nanotube (SWNT) due to mechanical loading. In this paper, the fracture resistance of zigzag SWNTs with preexisting defects is calculated using fracture mechanics concepts based on atomistic simulations. The problem of unstable crack growth at finite temperature, presumably caused by lattice trapping effect, is circumvented by computing the strain energy release rate through a series of displacement-controlled tensile loading of SWNTs (applied through moving the outermost layer of atoms at one end at constant strain rate of 9.4x10-4/ps) with pre-existing crack-like defects of various lengths. The strain energy release rate, G, is computed for (17,0), (28,0) and (35,0) SWNTs (each with aspect ratio 4) with pre-existing cracks up to 29.5{\AA} long. The fracture resistance, Gc, is determined as a function of crack length for each tube at three different temperatures (1K, 300K and 500K). A significant dependence of Gc on crack length is observed reminiscent of the rising R curve behavior of metals at the macroscale: for the zigzag nanotubes Gc increases with crack length at small length, and tends to reach a constant value if the tube diameter is large enough. We suspect that the lattice trapping effect plays the role of crack tip plasticity at the atomic scale. For example, at 300 Kelvin, Gc for the (35,0) tube with aspect ratio 4 converges to 6 Joule/m2 as the crack length exceeds 20 Angstrom. This value is comparable with the fracture toughness of graphite and Silicon. The fracture resistance of the tubes is found to decrease significantly as the temperature increases. To study the length effects, the computations are repeated for zigzag nanotubes with the same three chiralities but with aspect ratio 8 at 1K. 	
1610.08036v1	http://arxiv.org/pdf/1610.08036v1	2016	Modeling pressure distribution and heat in the body tissue and extract   the relationship between them in order to improve treatment planning in HIFU	Saeed Reza Hajian|Ali Abbaspour Tehrani Fard|Majid Pouladian|Gholam Reza Hemmasi	  In high intensity focused ultrasound (HIFU) systems using non-ionizing methods in cancer treatment, if the device is applied to the body externally, the HIFU beam can damage nearby healthy tissues and burn skin due to lack of knowledge about the viscoelastic properties of patient tissue and failure to consider the physical properties of tissue in treatment planning. Addressing this problem by using various methods, such as MRI or ultrasound, elastography can effectively measure visco-elastic properties of tissue and fits within the pattern of stimulation and total treatment planning. In this paper, in a linear path of HIFU propagation, and by considering the smallest part of the path, including voxel with three mechanical elements of mass, spring and damper, which represents the properties of viscoelasticity of tissue, by creating waves of HIFU in the wire environment of MATLAB mechanics and stimulating these elements, pressure and heat transfer due to stimulation in the hypothetical voxel was obtained. Through the repeatability of these three-dimensional elements, tissue is created. The measurement was performed on three layers. The values of these elements for liver tissue and kidney of sheep in a practical example and outside the body are measured, and pressure and heat for three layers of liver and kidney tissue of an organism were obtained by applying ultrasound signals with a designed model. This action is repeated in three different directions, and the results are then compared with simulation software for ultrasound, as a reference to U.S. Food and Drug Administration (FDA) measures for HIFU, as well as comparisons of results with an operational method for an HIFU cell. 	
1704.07753v1	http://arxiv.org/pdf/1704.07753v1	2017	Fluid-structure interaction modelling and stabilisation of a   patient-specific arteriovenous access fistula	W. P. Guess|B. D. Reddy|A. McBride|B. Spottiswoode|J. Downs|T. Franz	  A patient-specific fluid-structure interaction (FSI) model of a phase-contrast magnetic resonance angiography (PC-MRA) imaged arteriovenous fistula is presented. The numerical model is developed and simulated using a commercial multiphysics simulation package where a semi-implicit FSI coupling scheme combines a finite volume method blood flow model and a finite element method vessel wall model. A pulsatile mass-flow boundary condition is prescribed at the artery inlet of the model, and a three-element Windkessel model at the artery and vein outlets. The FSI model is freely available for analysis and extension. This work shows the effectiveness of combining a number of stabilisation techniques to simultaneously overcome the added-mass effect and optimise the efficiency of the overall model. The PC-MRA data, fluid model, and FSI model results show almost identical flow features in the fistula; this applies in particular to a flow recirculation region in the vein that could potentially lead to fistula failure. 	
1706.02272v1	http://arxiv.org/pdf/1706.02272v1	2017	Handling Model and Implementation Uncertainties via an Adaptive Discrete   Sliding Mode Controller Design	Mohammad Reza Amini|Mahdi Shahbakhti|Selina Pan|J. Karl Hedrick	  Analog-to-digital conversion (ADC) and uncertainties in modeling the plant dynamics are the main sources of imprecisions in the design cycle of model-based controllers. These implementation and model uncertainties should be addressed in the early stages of the controller design, otherwise they could lead to failure in the controller performance and consequently increase the time and cost required for completing the controller verification and validation (V&V) with more iterative loops. In this paper, a new control approach is developed based on a nonlinear discrete sliding mode controller (DSMC) formulation to mitigate the ADC imprecisions and model uncertainties. To this end, a DSMC design is developed against implementation imprecisions by incorporating the knowledge of ADC uncertainties on control inputs via an online uncertainty prediction and propagation mechanism. Next, a generic online adaptive law will be derived to compensate for the impact of an unknown parameter in the controller equations that is assumed to represent the model uncertainty. The final proposed controller is an integrated adaptive DSMC with robustness to implementation and model uncertainties that includes (i) an online ADC uncertainty mechanism, and (ii) an online adaptation law. The proposed adaptive control approach is evaluated on a nonlinear automotive engine control problem in real-time using a processor-in-the-loop (PIL) setup with an actual electronic control unit (ECU). The results reveal that the proposed adaptive control technique removes the uncertainty in the model fast, and significantly improves the robustness of the controllers to ADC imprecisions. This provides up to 60% improvement in the performance of the controller under implementation and model uncertainties compared to a baseline DSMC, in which there are no incorporated ADC imprecisions. 	
1706.04087v1	http://arxiv.org/pdf/1706.04087v1	2017	MIMO First and Second Order Discrete Sliding Mode Controls of Uncertain   Linear Systems under Implementation Imprecisions	Mohammad Reza Amini|Mahdi Shahbakhti|Selina Pan	  The performance of a conventional model-based controller significantly depends on the accuracy of the modeled dynamics. The model of a plant's dynamics is subjected to errors in estimating the numerical values of the physical parameters, and variations over operating environment conditions and time. These errors and variations in the parameters of a model are the major sources of uncertainty within the controller structure. Digital implementation of controller software on an actual electronic control unit (ECU) introduces another layer of uncertainty at the controller inputs/outputs. The implementation uncertainties are mostly due to data sampling and quantization via the analog-to-digital conversion (ADC) unit. The failure to address the model and ADC uncertainties during the early stages of a controller design cycle results in a costly and time consuming verification and validation (V&V) process. In this paper, new formulations of the first and second order discrete sliding mode controllers (DSMC) are presented for a general class of uncertain linear systems. The knowledge of the ADC imprecisions is incorporated into the proposed DSMCs via an online ADC uncertainty prediction mechanism to improve the controller robustness characteristics. Moreover, the DSMCs are equipped with adaptation laws to remove two different types of modeling uncertainties (multiplicative and additive) from the parameters of the linear system model. The proposed adaptive DSMCs are evaluated on a DC motor speed control problem in real-time using a processor-in-the-loop (PIL) setup with an actual ECU. The results show that the proposed SISO and MIMO second order DSMCs improve the conventional SISO first order DSMC tracking performance by 69% and 84%, respectively. Moreover, the proposed adaptation mechanism is able to remove the uncertainties in the model by up to 90%. 	
9811044v1	http://arxiv.org/pdf/cond-mat/9811044v1	1998	Failure Probabilities and Tough-Brittle Crossover of Heterogeneous   Materials with Continuous Disorder	B. Q. Wu|P. L. Leath	  The failure probabilities or the strength distributions of heterogeneous 1D systems with continuous local strength distribution and local load sharing have been studied using a simple, exact, recursive method. The fracture behavior depends on the local bond-strength distribution, the system size, and the applied stress, and crossovers occur as system size or stress changes. In the brittle region, systems with continuous disorders have a failure probability of the modified-Gumbel form, similar to that for systems with percolation disorder. The modified-Gumbel form is of special significance in weak-stress situations. This new recursive method has also been generalized to calculate exactly the failure probabilities under various boundary conditions, thereby illustrating the important effect of surfaces in the fracture process. 	
0312009v1	http://arxiv.org/pdf/cs/0312009v1	2003	Failure-Free Genetic Algorithm Optimization of a System Controller Using   SAFE/LEARNING Controllers in Tandem	E. S. Sazonov|D. Del Gobbo|P. Klinkhachorn|R. L. Klein	  The paper presents a method for failure free genetic algorithm optimization of a system controller. Genetic algorithms present a powerful tool that facilitates producing near-optimal system controllers. Applied to such methods of computational intelligence as neural networks or fuzzy logic, these methods are capable of combining the non-linear mapping capabilities of the latter with learning the system behavior directly, that is, without a prior model. At the same time, genetic algorithms routinely produce solutions that lead to the failure of the controlled system. Such solutions are generally unacceptable for applications where safe operation must be guaranteed. We present here a method of design, which allows failure-free application of genetic algorithms through utilization of SAFE and LEARNING controllers in tandem, where the SAFE controller recovers the system from dangerous states while the LEARNING controller learns its behavior. The method has been validated by applying it to an inherently unstable system of inverted pendulum. 	
0506001v1	http://arxiv.org/pdf/cs/0506001v1	2005	SafeMPI - Extending MPI for Byzantine Error Detection on Parallel   Clusters	Dmitry Mogilevsky|Sean Keller	  Modern high-performance computing relies heavily on the use of commodity processors arranged together in clusters. These clusters consist of individual nodes (typically off-the-shelf single or dual processor machines) connected together with a high speed interconnect. Using cluster computation has many benefits, but also carries the liability of being failure prone due to the sheer number of components involved. Many effective solutions have been proposed to aid failure recovery in clusters, their one significant downside being the failure models they support. Most of the work in the area has focused on detecting and correcting fail-stop errors. We propose a system that will also detect more general error models, such as Byzantine errors, thus allowing existing failure recovery methods to handle them correctly. 	
0604053v1	http://arxiv.org/pdf/cs/0604053v1	2006	Survivable Routing in IP-over-WDM Networks in the Presence of Multiple   Failures	Maciej Kurant|Patrick Thiran	  Failure restoration at the IP layer in IP-over-WDM networks requires to map the IP topology on the WDM topology in such a way that a failure at the WDM layer leaves the IP topology connected. Such a mapping is called $survivable$. As finding a survivable mapping is known to be NP-complete, in practice it requires a heuristic approach. We have introduced in [1] a novel algorithm called ``SMART'', that is more effective and scalable than the heuristics known to date. Moreover, the formal analysis of SMART [2] has led to new applications: the formal verification of the existence of a survivable mapping, and a tool tracing and repairing the vulnerable areas of the network. In this paper we extend the theoretical analysis in [2] by considering $multiple failures$. 	
0605108v2	http://arxiv.org/pdf/cs/0605108v2	2006	Diagnosability of Fuzzy Discrete Event Systems	Fuchun Liu|Daowen Qiu|Hongyan Xing|Zhujun Fan	  In order to more effectively cope with the real-world problems of vagueness, {\it fuzzy discrete event systems} (FDESs) were proposed recently, and the supervisory control theory of FDESs was developed. In view of the importance of failure diagnosis, in this paper, we present an approach of the failure diagnosis in the framework of FDESs. More specifically: (1) We formalize the definition of diagnosability for FDESs, in which the observable set and failure set of events are {\it fuzzy}, that is, each event has certain degree to be observable and unobservable, and, also, each event may possess different possibility of failure occurring. (2) Through the construction of observability-based diagnosers of FDESs, we investigate its some basic properties. In particular, we present a necessary and sufficient condition for diagnosability of FDESs. (3) Some examples serving to illuminate the applications of the diagnosability of FDESs are described. To conclude, some related issues are raised for further consideration. 	
9706224v1	http://arxiv.org/pdf/math/9706224v1	1997	A polarized partition relation and failure of GCH	Saharon Shelah	  The main result is that for lambda strong limit singular failing the continuum hypothesis (i.e. 2^lambda > lambda^+), a polarized partition theorem holds. 	
0611176v1	http://arxiv.org/pdf/math/0611176v1	2006	Restricted estimation of the cumulative incidence functions   corresponding to competing risks	Hammou El Barmi|Hari Mukerjee	  In the competing risks problem, an important role is played by the cumulative incidence function (CIF), whose value at time $t$ is the probability of failure by time $t$ from a particular type of failure in the presence of other risks. In some cases there are reasons to believe that the CIFs due to various types of failure are linearly ordered. El Barmi et al. [3] studied the estimation and inference procedures under this ordering when there are only two causes of failure. In this paper we extend the results to the case of $k$ CIFs, where $k\ge3$. Although the analyses are more challenging, we show that most of the results in the 2-sample case carry over to this $k$-sample case. 	
0602040v1	http://arxiv.org/pdf/physics/0602040v1	2006	Gravitational waves from BBH-systems? A (doubly) vain quest	A. Loinger	  The theoretical reasons at the root of LIGO's experimental failure in searching gravitational waves (GW's) from binary black hole (BBH) inspirals. 	
0701047v1	http://arxiv.org/pdf/q-bio/0701047v1	2007	Many Attractors, Long Chaotic Transients, and Failure in Small-World   Networks of Excitable Neurons	Hermann Riecke|Alex Roxin|Santiago Madruga|Sara A. Solla	  We study the dynamical states that emerge in a small-world network of recurrently coupled excitable neurons through both numerical and analytical methods. These dynamics depend in large part on the fraction of long-range connections or `short-cuts' and the delay in the neuronal interactions. Persistent activity arises for a small fraction of `short-cuts', while a transition to failure occurs at a critical value of the `short-cut' density. The persistent activity consists of multi-stable periodic attractors, the number of which is at least on the order of the number of neurons in the network. For long enough delays, network activity at high `short-cut' densities is shown to exhibit exceedingly long chaotic transients whose failure-times averaged over many network configurations follow a stretched exponential. We show how this functional form arises in the ensemble-averaged activity if each network realization has a characteristic failure-time which is exponentially distributed. 	
0707.2257v1	http://arxiv.org/pdf/0707.2257v1	2007	A Bayes method for a Bathtub Failure Rate via two $\mathbf{S}$-paths	Man-Wai Ho	  A class of semi-parametric hazard/failure rates with a bathtub shape is of interest. It does not only provide a great deal of flexibility over existing parametric methods in the modeling aspect but also results in a closed and tractable Bayes estimator for the bathtub-shaped failure rate (BFR). Such an estimator is derived to be a finite sum over two $\mathbf{S}$-paths due to an explicit posterior analysis in terms of two (conditionally independent) $\mathbf{S}$-paths. These, newly discovered, explicit results can be proved to be a Rao-Blackwellization of counterpart results in terms of partitions that are readily available by a specialization of James (2005)'s work. We develop both iterative and non-iterative computational procedures based on existing efficient Monte Carlo methods for sampling one single $\mathbf{S}$-path. Nmerical simulations are given to demonstrate the practicality and the effectiveness of our methodology. Last but not least, two applications of the proposed method are discussed, of which one is about a Bayesian test for failure rates and the other is related to modeling with covariates. 	
0708.1211v1	http://arxiv.org/pdf/0708.1211v1	2007	A Deterministic Sub-linear Time Sparse Fourier Algorithm via   Non-adaptive Compressed Sensing Methods	M. A. Iwen	  We study the problem of estimating the best B term Fourier representation for a given frequency-sparse signal (i.e., vector) $\textbf{A}$ of length $N \gg B$. More explicitly, we investigate how to deterministically identify B of the largest magnitude frequencies of $\hat{\textbf{A}}$, and estimate their coefficients, in polynomial$(B,\log N)$ time. Randomized sub-linear time algorithms which have a small (controllable) probability of failure for each processed signal exist for solving this problem. However, for failure intolerant applications such as those involving mission-critical hardware designed to process many signals over a long lifetime, deterministic algorithms with no probability of failure are highly desirable. In this paper we build on the deterministic Compressed Sensing results of Cormode and Muthukrishnan (CM) \cite{CMDetCS3,CMDetCS1,CMDetCS2} in order to develop the first known deterministic sub-linear time sparse Fourier Transform algorithm suitable for failure intolerant applications. Furthermore, in the process of developing our new Fourier algorithm, we present a simplified deterministic Compressed Sensing algorithm which improves on CM's algebraic compressibility results while simultaneously maintaining their results concerning exponential decay. 	
0709.2651v1	http://arxiv.org/pdf/0709.2651v1	2007	Seismic precursory patterns before a cliff collapse and critical-point   phenomena	David Amitrano|Jean Robert Grasso|Gloria Senfaute	  We analyse the statistical pattern of seismicity before a 1-2 103 m3 chalk cliff collapse on the Normandie ocean shore, Western France. We show that a power law acceleration of seismicity rate and energy in both 40 Hz-1.5 kHz and 2 Hz-10kHz frequency range, is defined on 3 order of magnitude, within 2 hours from the collapse time. Simultaneously, the average size of the seismic events increases toward the time to failure. These in-situ results are derived from the only station located within one rupture length distance from the rock fall rupture plane. They mimic the "critical point" like behavior recovered from physical and numerical experiments before brittle failures and tertiary creep failures. Our analysis of this first seismic monitoring data of a cliff collapse suggests that the thermodynamic phase transition models for failure may apply for cliff collapse. 	
0809.1894v1	http://arxiv.org/pdf/0809.1894v1	2008	A Generalization of the Exponential-Poisson Distribution	Wagner Barreto-Souza|Francisco Cribari-Neto	  The two-parameter distribution known as exponential-Poisson (EP) distribution, which has decreasing failure rate, was introduced by Kus (2007). In this paper we generalize the EP distribution and show that the failure rate of the new distribution can be decreasing or increasing. The failure rate can also be upside-down bathtub shaped. A comprehensive mathematical treatment of the new distribution is provided. We provide closed-form expressions for the density, cumulative distribution, survival and failure rate functions; we also obtain the density of the $i$th order statistic. We derive the $r$th raw moment of the new distribution and also the moments of order statistics. Moreover, we discuss estimation by maximum likelihood and obtain an expression for Fisher's information matrix. Furthermore, expressions for the R\'enyi and Shannon entropies are given and estimation of the stress-strength parameter is discussed. Applications using two real data sets are presented. 	
0902.0534v1	http://arxiv.org/pdf/0902.0534v1	2009	Co-fibered products of algebraic curves	Fedor Bogomolov|Yuri Tschinkel	  We give examples of failure of the existence of co-fibered products in the category of algebraic curves. 	
0912.3188v2	http://arxiv.org/pdf/0912.3188v2	2010	Robust Fault Tolerant uncapacitated facility location	Shiri Chechik|David Peleg	  In the uncapacitated facility location problem, given a graph, a set of demands and opening costs, it is required to find a set of facilities R, so as to minimize the sum of the cost of opening the facilities in R and the cost of assigning all node demands to open facilities. This paper concerns the robust fault-tolerant version of the uncapacitated facility location problem (RFTFL). In this problem, one or more facilities might fail, and each demand should be supplied by the closest open facility that did not fail. It is required to find a set of facilities R, so as to minimize the sum of the cost of opening the facilities in R and the cost of assigning all node demands to open facilities that did not fail, after the failure of up to \alpha facilities. We present a polynomial time algorithm that yields a 6.5-approximation for this problem with at most one failure and a 1.5 + 7.5\alpha-approximation for the problem with at most \alpha > 1 failures. We also show that the RFTFL problem is NP-hard even on trees, and even in the case of a single failure. 	
1002.2268v4	http://arxiv.org/pdf/1002.2268v4	2010	Do topological models provide good information about vulnerability in   electric power networks?	Paul Hines|Eduardo Cotilla-Sanchez|Seth Blumsack	  In order to identify the extent to which results from topological graph models are useful for modeling vulnerability in electricity infrastructure, we measure the susceptibility of power networks to random failures and directed attacks using three measures of vulnerability: characteristic path lengths, connectivity loss and blackout sizes. The first two are purely topological metrics. The blackout size calculation results from a model of cascading failure in power networks. Testing the response of 40 areas within the Eastern US power grid and a standard IEEE test case to a variety of attack/failure vectors indicates that directed attacks result in larger failures using all three vulnerability measures, but the attack vectors that appear to cause the most damage depend on the measure chosen. While our topological and power grid model results show some trends that are similar, there is only a mild correlation between the vulnerability measures for individual simulations. We conclude that evaluating vulnerability in power networks using purely topological metrics can be misleading. 	
1004.1706v1	http://arxiv.org/pdf/1004.1706v1	2010	Enhanced Ad-Hoc on Demand Multipath Distance Vector Routing protocol	Sujata V. Mallapur|Sujata Terdal	  Due to mobility in Ad-Hoc network the topology of the network may change randomly, rapidly and unexpectedly, because of these aspects, the routes in the network often disappear and new to arise. To avoid frequent route discovery and route failure EAOMDV was proposed based on existing routing protocol AOMDV. The EAOMDV (Enhanced Ad-Hoc on Demand Multipath Distance Vector) Routing protocol was proposed to solve the "route failure" problem in AOMDV. EAOMDV protocol reduces the route failure problem by preemptively predicting the link failure by the signal power received by the receiver (pr). This proposed protocol controls overhead, increases throughput and reduces the delay. The EAOMDV protocol was implemented on NS-2 and evaluation results show that the EAOMDV outperformed AOMDV. 	
1009.4166v1	http://arxiv.org/pdf/1009.4166v1	2010	A damage model based on failure threshold weakening	Joseph D. Gran|John B. Rundle|Donald L. Turcotte|James R. Holliday|William Klein	  A variety of studies have modeled the physics of material deformation and damage as examples of generalized phase transitions, involving either critical phenomena or spinodal nucleation. Here we study a model for frictional sliding with long range interactions and recurrent damage that is parameterized by a process of damage and partial healing during sliding. We introduce a failure threshold weakening parameter into the cellular-automaton slider-block model which allows blocks to fail at a reduced failure threshold for all subsequent failures during an event. We show that a critical point is reached beyond which the probability of a system-wide event scales with this weakening parameter. We provide a mapping to the percolation transition, and show that the values of the scaling exponents approach the values for mean-field percolation (spinodal nucleation) as lattice size $L$ is increased for fixed $R$. We also examine the effect of the weakening parameter on the frequency-magnitude scaling relationship and the ergodic behavior of the model. 	
1010.1899v1	http://arxiv.org/pdf/1010.1899v1	2010	The Failure Probability at Sink Node of Random Linear Network Coding	Xuan Guang|Fang-Wei Fu	  In practice, since many communication networks are huge in scale or complicated in structure even dynamic, the predesigned network codes based on the network topology is impossible even if the topological structure is known. Therefore, random linear network coding was proposed as an acceptable coding technique. In this paper, we further study the performance of random linear network coding by analyzing the failure probabilities at sink node for different knowledge of network topology and get some tight and asymptotically tight upper bounds of the failure probabilities. In particular, the worst cases are indicated for these bounds. Furthermore, if the more information about the network topology is utilized, the better upper bounds are obtained. These bounds improve on the known ones. Finally, we also discuss the lower bound of this failure probability and show that it is also asymptotically tight. 	
1011.3550v1	http://arxiv.org/pdf/1011.3550v1	2010	Overlay Protection Against Link Failures Using Network Coding	Ahmed E. Kamal|Aditya Ramamoorthy|Long Long|Shizheng Li	  This paper introduces a network coding-based protection scheme against single and multiple link failures. The proposed strategy ensures that in a connection, each node receives two copies of the same data unit: one copy on the working circuit, and a second copy that can be extracted from linear combinations of data units transmitted on a shared protection path. This guarantees instantaneous recovery of data units upon the failure of a working circuit. The strategy can be implemented at an overlay layer, which makes its deployment simple and scalable. While the proposed strategy is similar in spirit to the work of Kamal '07 & '10, there are significant differences. In particular, it provides protection against multiple link failures. The new scheme is simpler, less expensive, and does not require the synchronization required by the original scheme. The sharing of the protection circuit by a number of connections is the key to the reduction of the cost of protection. The paper also conducts a comparison of the cost of the proposed scheme to the 1+1 and shared backup path protection (SBPP) strategies, and establishes the benefits of our strategy. 	
1011.3638v1	http://arxiv.org/pdf/1011.3638v1	2010	Backward estimation of stochastic processes with failure events as time   origins	Kwun Chuen Gary Chan|Mei-Cheng Wang	  Stochastic processes often exhibit sudden systematic changes in pattern a short time before certain failure events. Examples include increase in medical costs before death and decrease in CD4 counts before AIDS diagnosis. To study such terminal behavior of stochastic processes, a natural and direct way is to align the processes using failure events as time origins. This paper studies backward stochastic processes counting time backward from failure events, and proposes one-sample nonparametric estimation of the mean of backward processes when follow-up is subject to left truncation and right censoring. We will discuss benefits of including prevalent cohort data to enlarge the identifiable region and large sample properties of the proposed estimator with related extensions. A SEER--Medicare linked data set is used to illustrate the proposed methodologies. 	
1011.4098v1	http://arxiv.org/pdf/1011.4098v1	2010	Understanding Cascading Failures in Power Grids	Sachin Kadloor|Nandakishore Santhi	  In the past, we have observed several large blackouts, i.e. loss of power to large areas. It has been noted by several researchers that these large blackouts are a result of a cascade of failures of various components. As a power grid is made up of several thousands or even millions of components (relays, breakers, transformers, etc.), it is quite plausible that a few of these components do not perform their function as desired. Their failure/misbehavior puts additional burden on the working components causing them to misbehave, and thus leading to a cascade of failures.   The complexity of the entire power grid makes it difficult to model each and every individual component and study the stability of the entire system. For this reason, it is often the case that abstract models of the working of the power grid are constructed and then analyzed. These models need to be computationally tractable while serving as a reasonable model for the entire system. In this work, we construct one such model for the power grid, and analyze it. 	
1106.0235v1	http://arxiv.org/pdf/1106.0235v1	2011	Robust Agent Teams via Socially-Attentive Monitoring	G. A. Kaminka|M. Tambe	  Agents in dynamic multi-agent environments must monitor their peers to execute individual and group plans. A key open question is how much monitoring of other agents' states is required to be effective: The Monitoring Selectivity Problem. We investigate this question in the context of detecting failures in teams of cooperating agents, via Socially-Attentive Monitoring, which focuses on monitoring for failures in the social relationships between the agents. We empirically and analytically explore a family of socially-attentive teamwork monitoring algorithms in two dynamic, complex, multi-agent domains, under varying conditions of task distribution and uncertainty. We show that a centralized scheme using a complex algorithm trades correctness for completeness and requires monitoring all teammates. In contrast, a simple distributed teamwork monitoring algorithm results in correct and complete detection of teamwork failures, despite relying on limited, uncertain knowledge, and monitoring only key agents in a team. In addition, we report on the design of a socially-attentive monitoring system and demonstrate its generality in monitoring several coordination relationships, diagnosing detected failures, and both on-line and off-line applications. 	
1106.3579v2	http://arxiv.org/pdf/1106.3579v2	2012	Consensus vs Broadcast in Communication Networks with Arbitrary Mobile   Omission Faults	Emmanuel Godard|Joseph Peters	  We compare the solvability of the Consensus and Broadcast problems in synchronous communication networks in which the delivery of messages is not reliable. The failure model is the mobile omission faults model. During each round, some messages can be lost and the set of possible simultaneous losses is the same for each round. We investigate these problems for the first time for arbitrary sets of possible failures. Previously, these sets were defined by bounding the numbers of failures.   In this setting, we present a new necessary condition for the solvability of Consensus that unifies previous impossibility results in this area. This condition is expressed using Broadcastability properties. As a very important application, we show that when the sets of omissions that can occur are defined by bounding the numbers of failures, counted in any way (locally, globally, etc.), then the Consensus problem is actually equivalent to the Broadcast problem. 	
1108.1426v1	http://arxiv.org/pdf/1108.1426v1	2011	Efficient Algorithms to Enhance Recovery Schema in Link State Protocols	Radwan Abujassar|Mohammed Ghanbari	  With the increasing demands for real-time applications traffic in net- works such as video and voice a high convergence time for the existing routing protocols when failure occurred is required. These applications can be very sensitive to packet loss when link/node goes down. In this paper, we propose two algorithms schemas for the link state protocol to reroute the traffic in two states; first, pre-calculated an alternative and disjoint path with the primary one from the source to the destination by re-routing traffic through it, regardless of the locations of failure and the number of failed links. Second, rerouting the traffic via an alternative path from a node whose local link is down without the need to wait until the source node knows about the failure. This is achieved by creating a new backup routing table based on the original routing table which is computed by the dijkstra algorithm. The goal of these algorithms is to reduce loss of packets, end-to-end delay time, improve throughput and avoiding local loop when nodes re-converge the topology in case of failure. 	
1109.6646v1	http://arxiv.org/pdf/1109.6646v1	2011	A Non-MDS Erasure Code Scheme For Storage Applications	Abbas Kiani|Soroush Akhlaghi	  This paper investigates the use of redundancy and self repairing against node failures in distributed storage systems, using various strategies. In replication method, access to one replication node is sufficient to reconstruct a lost node, while in MDS erasure coded systems which are optimal in terms of redundancy-reliability tradeoff, a single node failure is repaired after recovering the entire stored data. Moreover, regenerating codes yield a tradeoff curve between storage capacity and repair bandwidth. The current paper aims at investigating a new storage code. Specifically, we propose a non-MDS (2k, k) code that tolerates any three node failures and more importantly, it is shown using our code a single node failure can be repaired through access to only three nodes. 	
1202.2291v2	http://arxiv.org/pdf/1202.2291v2	2012	Onset of Localization in Heterogeneous Interfacial Failure	Arne Stormo|Knut Skogstrand Gjerden|Alex Hansen	  We study numerically the failure of an interface joining two elastic materials under load using a fiber bundle model connected to an elastic half space. We find that the breakdown process follows the equal load sharing fiber bundle model without any detectable spatial correlations between the positions of the failing fibers until localization sets in. The onset of localization is an instability, not a phase transition. Depending on the elastic constant describing the elastic half space, localization sets in before or after the critical load causing the interface to fail completely, is reached. There is a crossover between failure due to localization or failure without spatial correlations when tuning the elastic constant, not a phase transition. Contrary to earlier claims based on models different from ours, we find that a finite fraction of fibers must fail before the critical load is attained, even in the extreme localization regime, i.e.\ for very small elastic constant. We furthermore find that the critical load remains finite for all values of the elastic constant in the limit of an infinitely large system. 	
1203.0029v2	http://arxiv.org/pdf/1203.0029v2	2012	Assortativity Decreases the Robustness of Interdependent Networks	Di Zhou|Gregorio D'Agostino|Antonio Scala|H. Eugene Stanley	  It was recently recognized that interdependencies among different networks can play a crucial role in triggering cascading failures and hence system-wide disasters. A recent model shows how pairs of interdependent networks can exhibit an abrupt percolation transition as failures accumulate. We report on the effects of topology on failure propagation for a model system consisting of two interdependent networks. We find that the internal node correlations in each of the two interdependent networks significantly changes the critical density of failures that triggers the total disruption of the two-network system. Specifically, we find that the assortativity (i.e. the likelihood of nodes with similar degree to be connected) within a single network decreases the robustness of the entire system. The results of this study on the influence of assortativity may provide insights into ways of improving the robustness of network architecture, and thus enhances the level of protection of critical infrastructures. 	
1204.0285v1	http://arxiv.org/pdf/1204.0285v1	2012	Semiparametric Multivariate Accelerated Failure Time Model with   Generalized Estimating Equations	Steven Chiou|Junghi Kim|Jun Yan	  The semiparametric accelerated failure time model is not as widely used as the Cox relative risk model mainly due to computational difficulties. Recent developments in least squares estimation and induced smoothing estimating equations provide promising tools to make the accelerate failure time models more attractive in practice. For semiparametric multivariate accelerated failure time models, we propose a generalized estimating equation approach to account for the multivariate dependence through working correlation structures. The marginal error distributions can be either identical as in sequential event settings or different as in parallel event settings. Some regression coefficients can be shared across margins as needed. The initial estimator is a rank-based estimator with Gehan's weight, but obtained from an induced smoothing approach with computation ease. The resulting estimator is consistent and asymptotically normal, with a variance estimated through a multiplier resampling method. In a simulation study, our estimator was up to three times as efficient as the initial estimator, especially with stronger multivariate dependence and heavier censoring percentage. Two real examples demonstrate the utility of the proposed method. 	
1210.4973v3	http://arxiv.org/pdf/1210.4973v3	2013	Cascading Failures in Bi-partite Graphs: Model for Systemic Risk   Propagation	Xuqing Huang|Irena Vodenska|Shlomo Havlin|H. Eugene Stanley	  As economic entities become increasingly interconnected, a shock in a financial network can provoke significant cascading failures throughout the system. To study the systemic risk of financial systems, we create a bi-partite banking network model composed of banks and bank assets and propose a cascading failure model to describe the risk propagation process during crises. We empirically test the model with 2007 US commercial banks balance sheet data and compare the model prediction of the failed banks with the real failed banks after 2007. We find that our model efficiently identifies a significant portion of the actual failed banks reported by Federal Deposit Insurance Corporation. The results suggest that this model could be useful for systemic risk stress testing for financial systems. The model also identifies that commercial rather than residential real estate assets are major culprits for the failure of over 350 US commercial banks during 2008-2011. 	
1211.5499v1	http://arxiv.org/pdf/1211.5499v1	2012	Experimental Investigation of Plastic Deformations Before Granular   Avalanche	Axelle Amon|Roman Bertoni|Jérôme Crassous	  We present an experimental study of the deformation inside a granular material that is progressively tilted. We investigate the deformation before the avalanche with a spatially resolved Diffusive Wave Spectroscopy setup. At the beginning of the inclination process, we first observe localized and isolated events in the bulk, with a density which decreases with the depth. As the angle of inclination increases, series of micro-failures occur periodically in the bulk, and finally a granular avalanche takes place. The micro-failures are observed only when the tilt angles are larger than a threshold angle much smaller than the granular avalanche angle. We have characterized the density of reorganizations and the localization of micro-failures. We have also explored the effect of the nature of the grains, the relative humidity conditions and the packing fraction of the sample. We discuss those observations in the framework of the plasticity of granular matter. Micro-failures may then be viewed as the result of the accumulation of numerous plastic events. 	
1212.3295v2	http://arxiv.org/pdf/1212.3295v2	2012	Measures of Fault Tolerance in Distributed Simulated Annealing	Aaditya Prakash	  In this paper, we examine the different measures of Fault Tolerance in a Distributed Simulated Annealing process. Optimization by Simulated Annealing on a distributed system is prone to various sources of failure. We analyse simulated annealing algorithm, its architecture in distributed platform and potential sources of failures. We examine the behaviour of tolerant distributed system for optimization task. We present possible methods to overcome the failures and achieve fault tolerance for the distributed simulated annealing process. We also examine the implementation of Simulated Annealing in MapReduce system and possible ways to prevent failures in reaching the global optima. This paper will be beneficial to those who are interested in implementing a large scale distributed simulated annealing optimization problem of industrial or academic interest. We recommend hybrid tolerance technique to optimize the trade-off between efficiency and availability. 	
1212.4297v1	http://arxiv.org/pdf/1212.4297v1	2012	On Local and Global Conjugacy	Song Wang	  In this note we discuss and classify LFMO-spcial representations, for which under certain functoriality, it gives the instance of failure of multiplicity one. 	
1212.5615v1	http://arxiv.org/pdf/1212.5615v1	2012	Beta-Linear Failure Rate Distribution and its Applications	Ali Akbar Jafari|Eisa Mahmoudi	  We introduce in this paper a new four-parameter generalized version of the linear failure rate (LFR) distribution which is called Beta-linear failure rate (BLFR) distribution. The new distribution is quite flexible and can be used effectively in modeling survival data and reliability problems. It can have a constant, decreasing, increasing, upside-down bathtub (unimodal) and bathtub-shaped failure rate function depending on its parameters. It includes some well-known lifetime distributions as special submodels. We provide a comprehensive account of the mathematical properties of the new distributions. In particular, A closed-form expressions for the density, cumulative distribution and hazard rate function of the BLFR is given. Also, the $r$th order moment of this distribution is derived. We discuss maximum likelihood estimation of the unknown parameters of the new model for complete sample and obtain an expression for Fishers information matrix. In the end, to show the flexibility of this distribution and illustrative purposes, an application using a real data set is presented. 	
1302.4147v2	http://arxiv.org/pdf/1302.4147v2	2013	The Failure Probability of Random Linear Network Coding for Networks	Xuan Guang|Fang-Wei Fu	  In practice, since many communication networks are huge in scale, or complicated in structure, or even dynamic, the predesigned linear network codes based on the network topology is impossible even if the topological structure is known. Therefore, random linear network coding has been proposed as an acceptable coding technique for the case that the network topology cannot be utilized completely. Motivated by the fact that different network topological information can be obtained for different practical applications, we study the performance analysis of random linear network coding by analyzing some failure probabilities depending on these different topological information of networks. We obtain some tight or asymptotically tight upper bounds on these failure probabilities and indicate the worst cases for these bounds, i.e., the networks meeting the upper bounds with equality. In addition, if the more topological information of the network is utilized, the better upper bounds are obtained. On the other hand, we also discuss the lower bounds on the failure probabilities. 	
1304.0423v1	http://arxiv.org/pdf/1304.0423v1	2013	Reliability sensitivity analysis based on probability distribution   perturbation with application to CO2 storage	Ekaterina Sergienko|Paul Lemaître|Aurélie Arnaud|Daniel Busby|Fabrice Gamboa	  The objective of reliability sensitivity analysis is to determine input variables that mostly contribute to the variability of the failure probability. In this paper, we study a recently introduced method for the reliability sensitivity analysis based on a perturbation of the original probability distribution of the input variables. The objective is to determine the most influential input variables and to analyze their impact on the failure probability. We propose a moment independent sensitivity measure that is based on a perturbation of the original probability density independently for each input variable. The variables providing the highest variation of the original failure probability are settled to be more influential. These variables will need a proper characterization in terms of uncertainty. The method is intended to work in applications involving a computationally expensive simulation code for evaluating the failure probability such as the CO2 storage risk analysis. An application of the method to a synthetic CO2 storage case study is provided together with some analytical examples 	
1307.0412v1	http://arxiv.org/pdf/1307.0412v1	2013	Characterizing and Predicting the Robustness of Power-law Networks	Sarah LaRocca|Seth Guikema	  Power-law networks such as the Internet, terrorist cells, species relationships, and cellular metabolic interactions are susceptible to node failures, yet maintaining network connectivity is essential for network functionality. Disconnection of the network leads to fragmentation and, in some cases, collapse of the underlying system. However, the influences of the topology of networks on their ability to withstand node failures are poorly understood. Based on a study of the response of 2,000 power-law networks to node failures, we find that networks with higher nodal degree and clustering coefficient, lower betweenness centrality, and lower variability in path length and clustering coefficient maintain their cohesion better during such events. We also find that network robustness, i.e., the ability to withstand node failures, can be accurately predicted a priori for power-law networks across many fields. These results provide a basis for designing new, more robust networks, improving the robustness of existing networks such as the Internet and cellular metabolic pathways, and efficiently degrading networks such as terrorist cells. 	
1308.0174v1	http://arxiv.org/pdf/1308.0174v1	2013	MATCASC: A tool to analyse cascading line outages in power grids	Yakup Koç|Trivik Verma|Nuno A. M. Araujo|Martijn Warnier	  Blackouts in power grids typically result from cascading failures. The key importance of the electric power grid to society encourages further research into sustaining power system reliability and developing new methods to manage the risks of cascading blackouts. Adequate software tools are required to better analyze, understand, and assess the consequences of the cascading failures. This paper presents MATCASC, an open source MATLAB based tool to analyse cascading failures in power grids. Cascading effects due to line overload outages are considered. The applicability of the MATCASC tool is demonstrated by assessing the robustness of IEEE test systems and real-world power grids with respect to cascading failures. 	
1404.5406v1	http://arxiv.org/pdf/1404.5406v1	2014	Degradation Analysis of Probabilistic Parallel Choice Systems	Avinash Saxena|Shrisha Rao	  Degradation analysis is used to analyze the useful lifetimes of systems, their failure rates, and various other system parameters like mean time to failure (MTTF), mean time between failures (MTBF), and the system failure rate (SFR). In many systems, certain possible parallel paths of execution that have greater chances of success are preferred over others. Thus we introduce here the concept of probabilistic parallel choice. We use binary and $n$-ary probabilistic choice operators in describing the selections of parallel paths. These binary and $n$-ary probabilistic choice operators are considered so as to represent the complete system (described as a series-parallel system) in terms of the probabilities of selection of parallel paths and their relevant parameters. Our approach allows us to derive new and generalized formulae for system parameters like MTTF, MTBF, and SFR. We use a generalized exponential distribution, allowing distinct installation times for individual components, and use this model to derive expressions for such system parameters. 	
1405.0317v1	http://arxiv.org/pdf/1405.0317v1	2014	Robustness of Cucker-Smale flocking model	Eduardo Canale|Federico Dalmao|Ernesto Mordecki|Max Souza	  Consider a system of autonomous interacting agents moving in space, adjusting each own velocity as a weighted mean of the relative velocities of the other agents. In order to test the robustness of the model, we assume that each pair of agents, at each time step, can fail to connect with certain probability, the failure rate. This is a modification of the (deterministic) Flocking model introduced by Cucker and Smale in Emergent behavior in flocks, IEEE Trans. on Autom. Control, 2007, 52 (May) pp. 852-862. We prove that, if this random failures are independent in time and space, and have linear or sub-linear distance dependent rate of decay, the characteristic behavior of flocking exhibited by the original deterministic model, also holds true under random failures, for all failure rates. 	
1405.2866v1	http://arxiv.org/pdf/1405.2866v1	2014	Mitigating Cascading Failures in Interdependent Power Grids and   Communication Networks	Marzieh Parandehgheibi|Eytan Modiano|David Hay	  In this paper, we study the interdependency between the power grid and the communication network used to control the grid. A communication node depends on the power grid in order to receive power for operation, and a power node depends on the communication network in order to receive control signals for safe operation. We demonstrate that these dependencies can lead to cascading failures, and it is essential to consider the power flow equations for studying the behavior of such interdependent networks. We propose a two-phase control policy to mitigate the cascade of failures. In the first phase, our control policy finds the non-avoidable failures that occur due to physical disconnection. In the second phase, our algorithm redistributes the power so that all the connected communication nodes have enough power for operation and no power lines overload. We perform a sensitivity analysis to evaluate the performance of our control policy, and show that our control policy achieves close to optimal yield for many scenarios. This analysis can help design robust interdependent grids and associated control policies. 	
1408.6856v1	http://arxiv.org/pdf/1408.6856v1	2014	A multilevel Monte Carlo method for computing failure probabilities	Daniel Elfverson|Fredrik Hellman|Axel Målqvist	  We propose and analyze a method for computing failure probabilities of systems modeled as numerical deterministic models (e.g., PDEs) with uncertain input data. A failure occurs when a functional of the solution to the model is below (or above) some critical value. By combining recent results on quantile estimation and the multilevel Monte Carlo method we develop a method which reduces computational cost without loss of accuracy. We show how the computational cost of the method relates to error tolerance of the failure probability. For a wide and common class of problems, the computational cost is asymptotically proportional to solving a single accurate realization of the numerical model, i.e., independent of the number of samples. Significant reductions in computational cost are also observed in numerical experiments. 	
1501.04000v1	http://arxiv.org/pdf/1501.04000v1	2015	Large deformation and post-failure simulations of segmental retaining   walls using mesh-free method (SPH)	H. H. Bui|J. A. Kodikara|R. Pathegama|A. Bouazza|A. Haque	  Numerical methods are extremely useful in gaining insights into the behaviour of reinforced soil retaining walls. However, traditional numerical approaches such as limit equilibrium or finite element methods are unable to simulate large deformation and post-failure behaviour of soils and retaining wall blocks in the reinforced soil retaining walls system. To overcome this limitation, a novel numerical approach is developed aiming to predict accurately the large deformation and post-failure behaviour of soil and segmental wall blocks. Herein, soil is modelled using an elasto-plastic constitutive model, while segmental wall blocks are assumed rigid with full degrees of freedom. A soft contact model is proposed to simulate the interaction between soil-block and block-block. A two dimensional experiment of reinforced soil retaining walls collapse was conducted to verify the numerical results. It is shown that the proposed method can simulate satisfactory post-failure behaviour of segmental wall blocks in reinforced soil retaining wall systems. The comparison showed that the proposed method can provide satisfactory agreement with experiments. 	
1502.01496v1	http://arxiv.org/pdf/1502.01496v1	2015	Assisting V2V failure recovery using Device-to-Device Communications	Emad Abd-Elrahman|Adel Mounir Said|Thouraya Toukabri|Hossam Afifi|Michel Marot	  This paper aims to propose a new solution for failure recovery (dead-ends) in Vehicle to Vehicle (V2V) communications through LTE-assisted Device-to-Device communications (D2D). Based on the enhanced networking capabilities offered by Intelligent Transportation Systems (ITS) architecture, our solution can efficiently assist V2V communications in failure recovery situations. We also derive an analytical model to evaluate generic V2V routing recovery failures. Moreover, the proposed hybrid model is simulated and compared to the generic model under different constrains of worst and best cases of D2D discovery and communication. According to our comparison and simulation results, the hybrid model decreases the delay for alarm message propagation to the destination (typically the Traffic Control Center TCC) through the Road Side Unit (RSU) 	
1504.00856v3	http://arxiv.org/pdf/1504.00856v3	2015	Robust Control of Cascading Power Grid Failures using Stochastic   Approximation	Daniel Bienstock|Guy Grebla	  Cascading failure of a power transmission system are initiated by an exogenous event that disable a set of elements (e.g., lines) followed by a sequence of interrelated failures (or more precisely, trips) of overloaded elements caused by the combination of physics of power flows in the changed system topology, and controls. Should this sequence accelerate it can lead to a large system failure with significant loss of load. In previous work we have analyzed deterministic algorithms that in an online fashion (i.e., responding to observed data) selectively shed load so as to minimize the amount of lost load at termination of the cascade. In this work we present a rigorous methodology for incorporating noise and model errors, based on the Sample Average Approximation methodology for stochastic optimization. 	
1505.02648v1	http://arxiv.org/pdf/1505.02648v1	2015	Towards Formal Fault Tree Analysis using Theorem Proving	Waqar Ahmed|Osman Hasan	  Fault Tree Analysis (FTA) is a dependability analysis technique that has been widely used to predict reliability, availability and safety of many complex engineering systems. Traditionally, these FTA-based analyses are done using paper-and-pencil proof methods or computer simulations, which cannot ascertain absolute correctness due to their inherent limitations. As a complementary approach, we propose to use the higher-order-logic theorem prover HOL4 to conduct the FTA-based analysis of safety-critical systems where accuracy of failure analysis is a dire need. In particular, the paper presents a higher-order-logic formalization of generic Fault Tree gates, i.e., AND, OR, NAND, NOR, XOR and NOT and the formal verification of their failure probability expressions. Moreover, we have formally verified the generic probabilistic inclusion-exclusion principle, which is one of the foremost requirements for conducting the FTA-based failure analysis of any given system. For illustration purposes, we conduct the FTA-based failure analysis of a solar array that is used as the main source of power for the Dong Fang Hong-3 (DFH-3) satellite. 	
1507.07155v1	http://arxiv.org/pdf/1507.07155v1	2015	Majority Logic Decoding under Data-Dependent Logic Gate Failures	Srdan Brkic|Predrag Ivanis|Bane Vasic	  A majority logic decoder made of unreliable logic gates, whose failures are transient and datadependent, is analyzed. Based on a combinatorial representation of fault configurations a closed-form expression for the average bit error rate for an one-step majority logic decoder is derived, for a regular low-density parity-check (LDPC) code ensemble and the proposed failure model. The presented analysis framework is then used to establish bounds on the one-step majority logic decoder performance under the simplified probabilistic gate-output switching model. Based on the expander property of Tanner graphs of LDPC codes, it is proven that a version of the faulty parallel bit flipping decoder can correct a fixed fraction of channel errors in the presence of data-dependent gate failures. The results are illustrated with numerical examples of finite geometry codes. 	
1508.01691v1	http://arxiv.org/pdf/1508.01691v1	2015	Your Proof Fails? Testing Helps to Find the Reason	Guillaume Petiot|Nikolai Kosmatov|Bernard Botella|Alain Giorgetti|Jacques Julliand	  Applying deductive verification to formally prove that a program respects its formal specification is a very complex and time-consuming task due in particular to the lack of feedback in case of proof failures. Along with a non-compliance between the code and its specification (due to an error in at least one of them), possible reasons of a proof failure include a missing or too weak specification for a called function or a loop, and lack of time or simply incapacity of the prover to finish a particular proof. This work proposes a new methodology where test generation helps to identify the reason of a proof failure and to exhibit a counter-example clearly illustrating the issue. We describe how to transform an annotated C program into C code suitable for testing and illustrate the benefits of the method on comprehensive examples. The method has been implemented in STADY, a plugin of the software analysis platform FRAMA-C. Initial experiments show that detecting non-compliances and contract weaknesses allows to precisely diagnose most proof failures. 	
1508.01734v1	http://arxiv.org/pdf/1508.01734v1	2015	Can complexity decrease in Congestive Heart failure?	Sayan Mukherjee|Sanjay Kumar Palit|Santo Banerjee|M. R. K. Ariffin|Lamberto Rondoni|D. K. Bhattacharya	  The complexity of a signal can be measured by the Recurrence period density entropy (RPDE) from the reconstructed phase space. We have chosen a window based RPDE method for the classification of signals, as RPDE is an average entropic measure of the whole phase space. We have observed the changes in the complexity in cardiac signals of normal healthy person (NHP) and congestive heart failure patients (CHFP). The results show that the cardiac dynamics of a healthy subject is more complex and random compare to the same for a heart failure patient, whose dynamics is more deterministic. We have constructed a general threshold to distinguish the border line between a healthy and a congestive heart failure dynamics. The results may be useful for wide range for physiological and biomedical analysis. 	
1601.02923v1	http://arxiv.org/pdf/1601.02923v1	2016	Cascading Edge Failures: A Dynamic Network Process	June Zhang|José M. F. Moura	  This paper considers the dynamics of edges in a network. The Dynamic Bond Percolation (DBP) process models, through stochastic local rules, the dependence of an edge $(a,b)$ in a network on the states of its neighboring edges. Unlike previous models, DBP does not assume statistical independence between different edges. In applications, this means for example that failures of transmission lines in a power grid are not statistically independent, or alternatively, relationships between individuals (dyads) can lead to changes in other dyads in a social network. We consider the time evolution of the probability distribution of the network state, the collective states of all the edges (bonds), and show that it converges to a stationary distribution. We use this distribution to study the emergence of global behaviors like consensus (i.e., catastrophic failure or full recovery of the entire grid) or coexistence (i.e., some failed and some operating substructures in the grid). In particular, we show that, depending on the local dynamical rule, different network substructures, such as hub or triangle subgraphs, are more prone to failure. 	
1602.01788v2	http://arxiv.org/pdf/1602.01788v2	2016	Semiparametric Regression Analysis of Interval-Censored Competing Risks   Data	Lu Mao|D. Y. Lin|Donglin Zeng	  Interval-censored competing risks data arise when each study subject may experience an event or failure from one of several causes and the failure time is not observed exactly but rather known to lie in an interval between two successive examinations. We formulate the effects of possibly time-varying covariates on the cumulative incidence or sub-distribution function (i.e., the marginal probability of failure from a particular cause) of competing risks through a broad class of semiparametric regression models that captures both proportional and non-proportional hazards structures for the sub-distribution. We allow each subject to have an arbitrary number of examinations and accommodate missing information on the cause of failure. We consider nonparametric maximum likelihood estimation and devise a fast and stable EM-type algorithm for its computation. We then establish the consistency, asymptotic normality, and semiparametric efficiency of the resulting estimators by appealing to modern empirical process theory. In addition, we show through extensive simulation studies that the proposed methods perform well in realistic situations. Finally, we provide an application to a study on HIV-1 infection with different viral subtypes. 	
1603.06461v1	http://arxiv.org/pdf/1603.06461v1	2016	Remote Antenna Unit Selection Assisted Seamless Handover for High-Speed   Railway Communications with Distributed Antennas	Yang Lu|Ke Xiong|Zhuyan Zhao|Pingyi Fan|Zhangdui Zhong	  To attain seamless handover and reduce the han- dover failure probability for high-speed railway (HSR) com- munication systems, this paper proposes a remote antenna unit (RAU) selection assisted handover scheme where two antennas are installed on high speed train (HST) and distributed antenna system (DAS) cell architecture on ground is adopted. The RAU selection is used to provide high quality received signals for trains moving in DAS cells and the two HST antennas are employed on trains to realize seamless handover. Moreover, to efficiently evaluate the system performance, a new met- ric termed as handover occurrence probability is defined for describing the relation between handover occurrence position and handover failure probability. We then analyze the received signal strength, the handover trigger probability, the handover occurrence probability, the handover failure probability and the communication interruption probability. Numerical results are provided to compare our proposed scheme with the current existing ones. It is shown that our proposed scheme achieves better performances in terms of handover failure probability and communication interruption probability. 	
1607.07286v3	http://arxiv.org/pdf/1607.07286v3	2017	Session Types for Link Failures (Technical Report)	Manuel Adameit|Kirstin Peters|Uwe Nestmann	  We strive to use session type technology to prove behavioural properties of fault-tolerant distributed algorithms. Session types are designed to abstractly capture the structure of (even multi-party) communication protocols. The goal of session types is the analysis and verification of the protocols' behavioural properties. One important such property is progress, i.e., the absence of (unintended) deadlock. Distributed algorithms often resemble (compositions of) multi-party communication protocols. In contrast to protocols that are typically studied with session types, they are often designed to cope with system failures. An essential behavioural property is (successful) termination, despite failures, but it is often elaborate to prove for distributed algorithms.   We extend multi-party session types (and multi-party session types with nested sessions) by optional blocks that cover a limited class of link and crash failures. This allows us to automatically derive termination of distributed algorithms that come within these limits. To illustrate our approach, we prove termination for an implementation of the *rotating coordinator* Consensus algorithm. 	
1607.08322v1	http://arxiv.org/pdf/1607.08322v1	2016	Cooperative Repair of Multiple Node Failures in Distributed Storage   Systems	Kenneth W. Shum|Junyu Chen	  Cooperative regenerating codes are designed for repairing multiple node failures in distributed storage systems. In contrast to the original repair model of regenerating codes, which are for the repair of single node failure, data exchange among the new nodes is enabled. It is known that further reduction in repair bandwidth is possible with cooperative repair. Currently in the literature, we have an explicit construction of exact-repair cooperative code achieving all parameters corresponding to the minimum-bandwidth point. We give a slightly generalized and more flexible version of this cooperative regenerating code in this paper. For minimum-storage regeneration with cooperation, we present an explicit code construction which can jointly repair any number of systematic storage nodes. 	
1612.04922v2	http://arxiv.org/pdf/1612.04922v2	2017	A Continuum Description of Failure Waves	Hamid A. Said|James Glimm	  Shattering of a brittle material such as glass occurs dynamically through a propagating failure wave, which however, can not be assigned to any of the classical waves of the elasto-plastic theories of materials. Such failure waves have been a topic of research for decades. In this paper, we build a thermodynamically consistent theory based on the idea that a failure wave is analogous to a deflagration wave. Our theory admits, as special cases, the classical models of Feng and Clifton. Two fundamental thermodynamic functions (the free energy and the entropy production rate) form the basis of our theory. Such a two-function approach allows for the construction of a new variational principle and a new Lagrangian formulation that produce the equations of motion. Finally, a linearization of this theory is examined to gain insight into the coupling between the diffusive and elastic wave phenomena. 	
1701.02641v1	http://arxiv.org/pdf/1701.02641v1	2017	Distributed Algorithm for Collision Avoidance at Road Intersections in   the Presence of Communication Failures	Vladimir Savic|Elad M. Schiller|Marina Papatriantafilou	  Vehicle-to-vehicle (V2V) communication is a crucial component of the future autonomous driving systems since it enables improved awareness of the surrounding environment, even without extensive processing of sensory information. However, V2V communication is prone to failures and delays, so a distributed fault-tolerant approach is required for safe and efficient transportation. In this paper, we focus on the intersection crossing (IC) problem with autonomous vehicles that cooperate via V2V communications, and propose a novel distributed IC algorithm that can handle an unknown number of communication failures. Our analysis shows that both safety and liveness requirements are satisfied in all realistic situations. We also found, based on a real data set, that the crossing delay is only slightly increased even in the presence of highly correlated failures. 	
1701.03292v1	http://arxiv.org/pdf/1701.03292v1	2017	Structural instability of large-scale functional networks	Shogo Mizutaka|Kousuke Yakubo	  We study how large functional networks can grow stably under possible cascading overload failures and evaluated the maximum stable network size above which even a small-scale failure would cause a fatal breakdown of the network. Employing a model of cascading failures induced by temporally fluctuating loads, the maximum stable size $n_{\text{max}}$ has been calculated as a function of the load reduction parameter $r$ that characterizes how quickly the total load is reduced during the cascade. If we reduce the total load sufficiently fast ($r\ge r_{\text{c}}$), the network can grow infinitely. Otherwise, $n_{\text{max}}$ is finite and increases with $r$. For a fixed $r\,(<r_{\text{c}})$, $n_{\text{max}}$ for a scale-free network is larger than that for an exponential network with the same average degree. We also discuss how one detects and avoids the crisis of a fatal breakdown of the network from the relation between the sizes of the initial network and the largest component after an ordinarily occurring cascading failure. 	
1701.05983v1	http://arxiv.org/pdf/1701.05983v1	2017	A Minimum Reconfiguration Probability Routing Algorithm for RWA in   All-Optical Networks	Mohan Kumar S|Jagadeesha SN	  In this paper, we present a detailed study of Minimum Reconfiguration Probability Routing (MRPR) algorithm, and its performance evaluation in comparison with Adaptive unconstrained routing (AUR) and Least Loaded routing (LLR) algorithms. We have minimized the effects of failures on link and router failure in the network under changing load conditions, we assess the probability of service and number of light path failures due to link or route failure on Wavelength Interchange(WI) network. The computation complexity is reduced by using Kalman Filter(KF) techniques. The minimum reconfiguration probability routing (MRPR) algorithm selects most reliable routes and assign wavelengths to connections in a manner that utilizes the light path(LP) established efficiently considering all possible requests. 	
1703.05232v1	http://arxiv.org/pdf/1703.05232v1	2017	Modeling and Identification of Worst-Case Cascading Failures in Power   Systems	Chao Zhai|Hehong Zhang|Gaoxi Xiao|Tso-Chien Pan	  Cascading failures in power systems normally occur as a result of initial disturbance or faults on electrical elements, closely followed by errors of human operators. It remains a great challenge to systematically trace the source of cascading failures in power systems. In this paper, we develop a mathematical model to describe the cascading dynamics of transmission lines in power networks. In particular, the direct current (DC) power flow equation is employed to calculate the transmission power on the branches. By regarding the disturbances on the elements as the control inputs, we formulate the problem of determining the initial disturbances causing the cascading blackout of power grids in the framework of optimal control theory, and the magnitude of disturbances or faults on the selected branch can be obtained by solving the system of algebraic equations. Moreover, an iterative search algorithm is proposed to look for the optimal solution leading to the worst case of cascading failures. Theoretical analysis guarantees the asymptotic convergence of the iterative search algorithm. Finally, numerical simulations are carried out in IEEE 9 Bus System and IEEE 14 Bus System to validate the proposed approach. 	
1705.04453v1	http://arxiv.org/pdf/1705.04453v1	2017	The Geometry of Limit State Function Graphs and Subset Simulation	Karl Breitung	  In the last fifteen the subset sampling method has often been used in reliability problems as a tool for calculating small probabilities. This method is extrapolating from an initial Monte Carlo estimate for the probability content of a failure domain found by a suitable higher level of the original limit state function. Then iteratively conditional probabilities are estimated for failures domains decreasing to the original failure domain.   But there are assumptions not immediately obvious about the structure of the failure domains which must be fulfilled that the method works properly. Here examples are studied that show that at least in some cases if these premises are not fulfilled, inaccurate results may be obtained. For the further development of the subset sampling method it is certainly desirable to find approaches where it is possible to check that these implicit assumptions are not violated. Also it would be probably important to develop further improvements of the concept to get rid of these limitations. 	
1705.09411v1	http://arxiv.org/pdf/1705.09411v1	2017	Identifying Critical Risks of Cascading Failures in Power Systems	Hehong Zhang|Chao Zhai|Gaoxi Xiao|Tso-Chien Pan	  Potential critical risks of cascading failures in power systems can be identified by exposing those critical electrical elements on which certain initial disturbances may cause maximum disruption to power transmission networks. In this work, we investigate cascading failures in power systems described by the direct current (DC) power flow equations, while initial disturbances take the form of altering admittance of elements. The disruption is quantified with the remaining transmission power at the end of cascading process. In particular, identifying the critical elements and the corresponding initial disturbances causing the worst-case cascading blackout is formulated as a dynamic optimization problem (DOP) in the framework of optimal control theory, where the entire propagation process of cascading failures is put under consideration. An Identifying Critical Risk Algorithm (ICRA) based on the maximum principle is proposed to solve the DOP. Simulation results on the IEEE 9-Bus and the IEEE 14-Bus test systems are presented to demonstrate the effectiveness of the algorithm. 	
1707.02327v1	http://arxiv.org/pdf/1707.02327v1	2017	Why Modern Open Source Projects Fail	Jailton Coelho|Marco Tulio Valente	  Open source is experiencing a renaissance period, due to the appearance of modern platforms and workflows for developing and maintaining public code. As a result, developers are creating open source software at speeds never seen before. Consequently, these projects are also facing unprecedented mortality rates. To better understand the reasons for the failure of modern open source projects, this paper describes the results of a survey with the maintainers of 104 popular GitHub systems that have been deprecated. We provide a set of nine reasons for the failure of these open source projects. We also show that some maintenance practices -- specifically the adoption of contributing guidelines and continuous integration -- have an important association with a project failure or success. Finally, we discuss and reveal the principal strategies developers have tried to overcome the failure of the studied projects. 	
1707.09516v1	http://arxiv.org/pdf/1707.09516v1	2017	Comparing Different Models for Investigating Cascading Failures in Power   Systems	Chao Zhai|Hehong Zhang|Gaoxi Xiao|Tso-Chien Pan	  This paper centers on the comparison of three different models that describe cascading failures of power systems. Specifically, these models are different in characterizing the physical properties of power networks and computing the branch power flow. Optimal control approach is applied on these models to identify the critical disturbances that result in the worst-case cascading failures of power networks. Then we compare these models by analyzing the critical disturbances and cascading processes. Significantly, comparison results on IEEE 9 bus system demonstrate that physical and electrical properties of power networks play a crucial role in the evolution of cascading failures, and it is necessary to take into account these properties appropriately while applying the model in the analysis of cascading blackout. 	
1708.06277v1	http://arxiv.org/pdf/1708.06277v1	2017	Models of Brauer-Severi surface bundles	Andrew Kresch|Yuri Tschinkel	  We study Brauer-Severi surface bundles over smooth projective varieties via root stacks, with a view towards applications to failure of stable rationality. 	
1708.08155v1	http://arxiv.org/pdf/1708.08155v1	2017	ByRDiE: Byzantine-resilient distributed coordinate descent for   decentralized learning	Zhixiong Yang|Waheed U. Bajwa	  Distributed machine learning algorithms enable processing of datasets that are distributed over a network without gathering the data at a centralized location. While efficient distributed algorithms have been developed under the assumption of faultless networks, failures that can render these algorithms nonfunctional indeed happen in the real world. This paper focuses on the problem of Byzantine failures, which are the hardest to safeguard against in distributed algorithms. While Byzantine fault tolerance has a rich history, existing work does not translate into efficient and practical algorithms for high-dimensional distributed learning tasks. In this paper, two variants of an algorithm termed Byzantine-resilient distributed coordinate descent (ByRDiE) are developed and analyzed that solve distributed learning problems in the presence of Byzantine failures. Theoretical analysis as well as numerical experiments presented in the paper highlight the usefulness of ByRDiE for high-dimensional distributed learning in the presence of Byzantine failures. 	
1709.03096v1	http://arxiv.org/pdf/1709.03096v1	2017	Survivable Probability of SDN-enabled Cloud Networking with Random   Physical Link Failure	Zhili Zhou|Tachun Lin	  Software-driven cloud networking is a new paradigm in orchestrating physical resources (CPU, network bandwidth, energy, storage) allocated to network functions, services, and applications, which is commonly modeled as a cross-layer network. This model carries a physical network representing the physical infrastructure, a logical network showing demands, and logical-to-physical node/link mappings. In such networks, a single failure in the physical network may trigger cascading failures in the logical network and disable network services and connectivity. In this paper, we propose an evaluation metric, survivable probability, to evaluate the reliability of such networks under random physical link failure(s). We propose the concept of base protecting spanning tree and prove the necessary and sufficient conditions for its existence and relation to survivability. We then develop mathematical programming formulations for reliable cross-layer network routing design with the maximal reliable probability. Computation results demonstrate the viability of our approach. 	
1709.03439v1	http://arxiv.org/pdf/1709.03439v1	2017	Why Do Deep Neural Networks Still Not Recognize These Images?: A   Qualitative Analysis on Failure Cases of ImageNet Classification	Han S. Lee|Alex A. Agarwal|Junmo Kim	  In a recent decade, ImageNet has become the most notable and powerful benchmark database in computer vision and machine learning community. As ImageNet has emerged as a representative benchmark for evaluating the performance of novel deep learning models, its evaluation tends to include only quantitative measures such as error rate, rather than qualitative analysis. Thus, there are few studies that analyze the failure cases of deep learning models in ImageNet, though there are numerous works analyzing the networks themselves and visualizing them. In this abstract, we qualitatively analyze the failure cases of ImageNet classification results from recent deep learning model, and categorize these cases according to the certain image patterns. Through this failure analysis, we believe that it can be discovered what the final challenges are in ImageNet database, which the current deep learning model is still vulnerable to. 	
1709.06934v1	http://arxiv.org/pdf/1709.06934v1	2017	REACT to Cyber Attacks on Power Grids	Saleh Soltan|Mihalis Yannakakis|Gil Zussman	  Motivated by the recent cyber attack on the Ukrainian power grid, we study cyber attacks on power grids that affect both the physical infrastructure and the data at the control center. In particular, we assume that an adversary attacks an area by: (i) remotely disconnecting some lines within the attacked area, and (ii) modifying the information received from the attacked area to mask the line failures and hide the attacked area from the control center. For the latter, we consider two types of attacks: (i) data distortion: which distorts the data by adding powerful noise to the actual data, and (ii) data replay: which replays a locally consistent old data instead of the actual data. We use the DC power flow model and prove that the problem of finding the set of line failures given the phase angles of the nodes outside of the attacked area is strongly NP-hard, even when the attacked area is known. However, we introduce the polynomial time REcurrent Attack Containment and deTection (REACT) Algorithm to approximately detect the attacked area and line failures after a cyber attack. We numerically show that it performs very well in detecting the attacked area, and detecting single, double, and triple line failures in small and large attacked areas. 	
1709.07399v1	http://arxiv.org/pdf/1709.07399v1	2017	EXPOSE the Line Failures following a Cyber-Physical Attack on the Power   Grid	Saleh Soltan|Gil Zussman	  Recent attacks on power grids demonstrated the vulnerability of the grids to cyber and physical attacks. To analyze this vulnerability, we study cyber-physical attacks that affect both the power grid physical infrastructure and its underlying Supervisory Control And Data Acquisition (SCADA) system. We assume that an adversary attacks an area by: (i) disconnecting some lines within that area, and (ii) obstructing the information (e.g., status of the lines and voltage measurements) from within the area to reach the control center. We leverage the algebraic properties of the AC power flows to introduce the efficient EXPOSE Algorithm for detecting line failures and recovering voltages inside that attacked area after such an attack. The EXPOSE Algorithm outperforms the state-of-the-art algorithm for detecting line failures using partial information under the AC power flow model in terms of scalability and accuracy. The main advantages of the EXPOSE Algorithm are that its running time is independent of the size of the grid and number of line failures, and that it provides accurate information recovery under some conditions on the attacked area. Moreover, it approximately recovers the information and provides the confidence of the solution when these conditions do not hold. 	
1710.05640v1	http://arxiv.org/pdf/1710.05640v1	2017	Survivable Probability of Network Slicing with Random Physical Link   Failure	Zhili Zhou|Tachun Lin	  The fifth generation of communication technology (5G) revolutionizes mobile networks and the associated ecosystems through the integration of cross-domain networks. Network slicing is an enabling technology for 5G as it provides dynamic, on-demand, and reliable logical network slices (i.e., network services) over a common physical network/infrastructure. Since a network slice is subject to failures originated from disruptions, namely node or link failures, in the physical infrastructure, our utmost interest is to evaluate the reliability of a network slice before assigning it to customers. In this paper, we propose an evaluation metric, \textit{survivable probability}, to quantify the reliability of a network slice under random physical link failure(s). We prove the existence of a \textit{base protecting spanning tree set} which has the same survivable probability as that of a network slice. We propose the necessary and sufficient conditions to identify a base protecting spanning tree set and develop corresponding mathematical formulations, which can be used to generate reliable network slices in the 5G environment. In addition to proving the viability of our approaches with simulation results, we also discuss how our problems and approaches are related to the Steiner tree problems and present their computational complexity and approximability. 	
1711.02580v1	http://arxiv.org/pdf/1711.02580v1	2017	Quantifying the Influence of Component Failure Probability on Cascading   Blackout Risk	Jinpeng Guo|Feng Liu|Jianhui Wang|Ming Cao|Shengwei Mei	  The risk of cascading blackouts greatly relies on failure probabilities of individual components in power grids. To quantify how component failure probabilities (CFP) influences blackout risk (BR), this paper proposes a sample-induced semi-analytic approach to characterize the relationship between CFP and BR. To this end, we first give a generic component failure probability function (CoFPF) to describe CFP with varying parameters or forms. Then the exact relationship between BR and CoFPFs is built on the abstract Markov-sequence model of cascading outages. Leveraging a set of samples generated by blackout simulations, we further establish a sample-induced semi-analytic mapping between the unbiased estimation of BR and CoFPFs. Finally, we derive an efficient algorithm that can directly calculate the unbiased estimation of BR when the CoFPFs change. Since no additional simulations are required, the algorithm is computationally scalable and efficient. Numerical experiments well confirm the theory and the algorithm. 	
1711.06855v1	http://arxiv.org/pdf/1711.06855v1	2017	A Probabilistic Characterization of Random and Malicious Communication   Failures in Multi-Hop Networked Control	Ahmet Cetinkaya|Hideaki Ishii|Tomohisa Hayakawa	  The control problem of a linear discrete-time dynamical system over a multi-hop network is explored. The network is assumed to be subject to packet drops by malicious and nonmalicious nodes as well as random and malicious data corruption issues. We utilize asymptotic tail-probability bounds of transmission failure ratios to characterize the links and paths of a network as well as the network itself. This probabilistic characterization allows us to take into account multiple failures that depend on each other, and coordinated malicious attacks on the network. We obtain a sufficient condition for the stability of the networked control system by utilizing our probabilistic approach. We then demonstrate the efficacy of our results in different scenarios concerning transmission failures on a multi-hop network. 	
1712.05465v2	http://arxiv.org/pdf/1712.05465v2	2018	Choreographies meet Communication Failures	Fabrizio Montesi|Marco Peressotti	  Choreographies are global descriptions of communication structures, inspired by the "Alice and Bob" notation of security protocols. They have been successfully employed in the design and implementation of distributed systems. However, there is still limited evidence of the applicability of choreographies in the real-world setting of distributed programming, where communication actions may fail. In this work, we propose the first choreography model that allows for communication failures and the programming of user-defined code to deal with such failures. We validate our model by implementing common strategies for handling communication failures in a robust way, which in turn can be used as a library by choreographies that assume reliable communication. We equip our model with a typing discipline that can statically verify reliability properties, in particular at-most-once and exactly-once delivery. We demonstrate the applicability of our model by defining a semantics-preserving compilation procedure towards a process calculus equipped with unreliable I/O actions. 	
1802.00104v1	http://arxiv.org/pdf/1802.00104v1	2018	On the Achievability Region of Regenerating Codes for Multiple Erasures	Marwen Zorgui|Zhiying Wang	  We study the problem of centralized exact repair of multiple failures in distributed storage. We describe constructions that achieve a new set of interior points under exact repair. The constructions build upon the layered code construction by Tian et al., designed for exact repair of single failure. We firstly improve upon the layered construction for general system parameters. Then, we extend the improved construction to support the repair of multiple failures, with varying number of helpers. In particular, we prove the optimality of one point on the functional repair tradeoff of multiple failures for some parameters. Finally, considering minimum bandwidth cooperative repair (MBCR) codes as centralized repair codes, we determine explicitly the best achievable region obtained by space-sharing among all known points, including the MBCR point. 	
1803.01719v1	http://arxiv.org/pdf/1803.01719v1	2018	How to Start Training: The Effect of Initialization and Architecture	Boris Hanin|David Rolnick	  We investigate the effects of initialization and architecture on the start of training in deep ReLU nets. We identify two common failure modes for early training in which the mean and variance of activations are poorly behaved. For each failure mode, we give a rigorous proof of when it occurs at initialization and how to avoid it. The first failure mode, exploding/vanishing mean activation length, can be avoided by initializing weights from a symmetric distribution with variance 2/fan-in. The second failure mode, exponentially large variance of activation length, can be avoided by keeping constant the sum of the reciprocals of layer widths. We demonstrate empirically the effectiveness of our theoretical results in predicting when networks are able to start training. In particular, we note that many popular initializations fail our criteria, whereas correct initialization and architecture allows much deeper networks to be trained. 	
1307.1149v2	http://arxiv.org/pdf/1307.1149v2	2013	Solar Activity and Transformer Failures in the Greek National Electric   Grid	Ioannis P. Zois	  We study both the short term and long term effects of solar activity on the large transformers (150kV and 400kV) of the Greek national electric grid. We use data analysis and various analytic and statistical methods and models. Contrary to the common belief in PPC Greece, we see that there are considerable both short term (immediate) and long term effects of solar activity onto large transformers in a mid-latitude country (latitude approx. 35 - 41 degrees North) like Greece. Our results can be summarized as follows: For the short term effects: During 1989-2010 there were 43 stormy days (namely days with for example Ap larger or equal to 100) and we had 19 failures occurring during a stormy day plus or minus 3 days and 51 failures occurring during a stormy day plus or minus 7 days. All these failures can be directly related to Geomagnetically Induced Currents (GICs). Explicit cases are presented. For the long term effects we have two main results: The annual transformer failure number for the period of study 1989-2010 follows the solar activity pattern (11 year periodicity, bell-shaped graph). Yet the maximum number of transformer failures occur 3-4 years after the maximum of solar activity. There is statistical correlation between solar activity expressed using various newly defined long term solar activity indices and the annual number of transformer failures. These new long term solar activity indices were defined using both local (from geomagnetic stations in Greece) and global (planetary averages) geomagnetic data. Applying both linear and non-linear statistical regression we compute the regression equations and the corresponding coefficients of determination. 	
1707.08018v1	http://arxiv.org/pdf/1707.08018v1	2017	Dynamically induced cascading failures in supply networks	Benjamin Schäfer|Dirk Witthaut|Marc Timme|Vito Latora	  Reliable functioning of infrastructure networks is essential for our modern society since the disruption of any communication, transport or supply network poses serious risks to our normal life. Cascading failures, namely events in which the initial and local failure of a component triggers a sequence of multiple failures of other parts of the network, are the main cause of large-scale network outages. Although cascading failures often exhibit dynamical transients, i.e., momentary variations of the state of the system, the modelling of cascades has so far mainly focused on the analysis of sequences of steady states. In this article, we focus on electrical supply networks and introduce a general dynamical framework that takes into consideration both the event-based nature of cascades and the details of the network dynamics. In this way, we account for possible losses of transmission lines and for the dynamical transition from one steady state to the next, which can significantly increase the vulnerability of a network. We find that transients in the flows of a supply network play a crucial role in the emergence of collective behaviors and may cause cascades which cannot be predicted by a steady-state analysis approach. We illustrate our results on a series of network case studies, including the real topology of the national power grids of Spain, France and Great Britain. We finally propose a forecasting method that may help to better understanding the stability conditions of a network, and also to identify its critical lines and components in advance or during an exceptional situation. Overall, our work highlights the relevance of dynamically induced failures on the synchronization dynamics of national power grids of different European countries and it provides novel methods to predict and limit cascading failures. 	
0512015v2	http://arxiv.org/pdf/cond-mat/0512015v2	2006	Crossover behavior in failure avalanches	Srutarshi Pradhan|Alex Hansen|Per C. Hemmer	  Composite materials, with statistically distributed threshold for breakdown of individual elements, are considered. During the failure process of such materials under external stress (load or voltage), avalanches consisting of simultaneous rupture of several elements occur, with a distribution $D(\Delta)$ of the magnitude $\Delta$ of such avalanches. The distribution is typically a power law $D(\Delta)\propto\Delta^{-\xi}$. For the systems we study here, a crossover behavior is seen between two power laws, with a small exponent $\xi$ in the vicinity of complete breakdown and a larger exponent $\xi$ for failures away from the breakdown point. We demonstrate this analytically for bundles of many fibers where the load is uniformly distributed among the surviving fibers. In this case $\xi=3/2$ near the breakdown point and $\xi=5/2$ away from it. The latter is known to be the generic behavior. This crossover is a signal of imminent catastrophic failure of the material. Near the breakdown point, avalanche statistics show nontrivial finite size scaling. We observe similar crossover behavior in a network of electric fuses, and find $\xi=2$ near the catastrophic failure and $\xi=3$ away from it. For this fuse model power dissipation avalanches show a similar crossover near breakdown. 	
0504031v1	http://arxiv.org/pdf/q-bio/0504031v1	2005	Parametric Resonance May Explain Virologic Failure to HIV Treatment   Interruptions	Romulus Breban|Sally Blower	  Pilot studies of structured treatment interruptions (STI) in HIV therapy have shown that patients can maintain low viral loads whilst benefiting from reduced treatment toxicity. However, a recent STI clinical trial reported a high degree of virologic failure. Here we present a novel hypothesis that could explain virologic failure to STI and provides new insights of great clinical relevance. We analyze a classic mathematical model of HIV within-host viral dynamics and find that nonlinear parametric resonance occurs when STI are added to the model; resonance is observed as virologic failure. We use the model to simulate clinical trial data and to calculate patient-specific resonant spectra. We gain two important insights. Firstly, within an STI trial, we determine that patients who begin with similar viral loads can be expected to show extremely different virologic responses as a result of resonance. Thus, high heterogeneity of patient response within a STI clinical trial is to be expected. Secondly and more importantly, we determine that virologic failure is not simply due to STI or patient characteristics; rather it is the result of a complex dynamic interaction between STI and patient viral dynamics. Hence, our analyses demonstrate that no universal regimen with periodic interruptions will be effective for all patients. On the basis of our results, we suggest that immunologic and virologic parameters should be used to design patient-specific STI regimens. 	
0809.1258v2	http://arxiv.org/pdf/0809.1258v2	2008	Network Protection Codes Against Link Failures Using Network Coding	Salah A. Aly|Ahmed E. Kamal	  Protecting against link failures in communication networks is essential to increase robustness, accessibility, and reliability of data transmission. Recently, network coding has been proposed as a solution to provide agile and cost efficient network protection against link failures, which does not require data rerouting, or packet retransmission. To achieve this, separate paths have to be provisioned to carry encoded packets, hence requiring either the addition of extra links, or reserving some of the resources for this purpose. In this paper, we propose network protection codes against a single link failure using network coding, where a separate path using reserved links is not needed. In this case portions of the link capacities are used to carry the encoded packets.   The scheme is extended to protect against multiple link failures and can be implemented at an overlay layer. Although this leads to reducing the network capacity, the network capacity reduction is asymptotically small in most cases of practical interest. We demonstrate that such network protection codes are equivalent to error correcting codes for erasure channels. Finally, we study the encoding and decoding operations of such codes over the binary field. 	
0909.1788v1	http://arxiv.org/pdf/0909.1788v1	2009	Building on Quicksand	Pat Helland|David Campbell	  Reliable systems have always been built out of unreliable components. Early on, the reliable components were small such as mirrored disks or ECC (Error Correcting Codes) in core memory. These systems were designed such that failures of these small components were transparent to the application. Later, the size of the unreliable components grew larger and semantic challenges crept into the application when failures occurred.   As the granularity of the unreliable component grows, the latency to communicate with a backup becomes unpalatable. This leads to a more relaxed model for fault tolerance. The primary system will acknowledge the work request and its actions without waiting to ensure that the backup is notified of the work. This improves the responsiveness of the system.   There are two implications of asynchronous state capture: 1) Everything promised by the primary is probabilistic. There is always a chance that an untimely failure shortly after the promise results in a backup proceeding without knowledge of the commitment. Hence, nothing is guaranteed! 2) Applications must ensure eventual consistency. Since work may be stuck in the primary after a failure and reappear later, the processing order for work cannot be guaranteed.   Platform designers are struggling to make this easier for their applications. Emerging patterns of eventual consistency and probabilistic execution may soon yield a way for applications to express requirements for a "looser" form of consistency while providing availability in the face of ever larger failures.   This paper recounts portions of the evolution of these trends, attempts to show the patterns that span these changes, and talks about future directions as we continue to "build on quicksand". 	
1009.5177v2	http://arxiv.org/pdf/1009.5177v2	2012	Sequential design of computer experiments for the estimation of a   probability of failure	Julien Bect|David Ginsbourger|Ling Li|Victor Picheny|Emmanuel Vazquez	  This paper deals with the problem of estimating the volume of the excursion set of a function $f:\mathbb{R}^d \to \mathbb{R}$ above a given threshold, under a probability measure on $\mathbb{R}^d$ that is assumed to be known. In the industrial world, this corresponds to the problem of estimating a probability of failure of a system. When only an expensive-to-simulate model of the system is available, the budget for simulations is usually severely limited and therefore classical Monte Carlo methods ought to be avoided. One of the main contributions of this article is to derive SUR (stepwise uncertainty reduction) strategies from a Bayesian-theoretic formulation of the problem of estimating a probability of failure. These sequential strategies use a Gaussian process model of $f$ and aim at performing evaluations of $f$ as efficiently as possible to infer the value of the probability of failure. We compare these strategies to other strategies also based on a Gaussian process model for estimating a probability of failure. 	
1101.2003v2	http://arxiv.org/pdf/1101.2003v2	2011	Fault-tolerant Cooperative Tasking for Multi-agent Systems	Mohammad Karimadini|Hai Lin	  A natural way for cooperative tasking in multi-agent systems is through a top-down design by decomposing a global task into sub-tasks for each individual agent such that the accomplishments of these sub-tasks will guarantee the achievement of the global task. In our previous works [1], [2] we presented necessary and sufficient conditions on the decomposability of a global task automaton between cooperative agents. As a follow-up work, this paper deals with the robustness issues of the proposed top-down design approach with respect to event failures in the multi-agent systems. The main concern under event failure is whether a previously decomposable task can still be achieved collectively by the agents, and if not, we would like to investigate that under what conditions the global task could be robustly accomplished. This is actually the fault-tolerance issue of the top-down design, and the results provide designers with hints on which events are fragile with respect to failures, and whether redundancies are needed. The main objective of this paper is to identify necessary and sufficient conditions on failed events under which a decomposable global task can still be achieved successfully. For such a purpose, a notion called passivity is introduced to characterize the type of event failures. The passivity is found to reflect the redundancy of communication links over shared events, based on which necessary and sufficient conditions for the reliability of cooperative tasking under event failures are derived, followed by illustrative examples and remarks for the derived conditions. 	
1201.2698v2	http://arxiv.org/pdf/1201.2698v2	2012	Optimal Allocation of Interconnecting Links in Cyber-Physical Systems:   Interdependence, Cascading Failures and Robustness	Osman Yagan|Dajun Qian|Junshan Zhang|Douglas Cochran	  We consider a cyber-physical system consisting of two interacting networks, i.e., a cyber-network overlaying a physical-network. It is envisioned that these systems are more vulnerable to attacks since node failures in one network may result in (due to the interdependence) failures in the other network, causing a cascade of failures that would potentially lead to the collapse of the entire infrastructure. The robustness of interdependent systems against this sort of catastrophic failure hinges heavily on the allocation of the (interconnecting) links that connect nodes in one network to nodes in the other network. In this paper, we characterize the optimum inter-link allocation strategy against random attacks in the case where the topology of each individual network is unknown. In particular, we analyze the "regular" allocation strategy that allots exactly the same number of bi-directional inter-network links to all nodes in the system. We show, both analytically and experimentally, that this strategy yields better performance (from a network resilience perspective) compared to all possible strategies, including strategies using random allocation, unidirectional inter-links, etc. 	
1202.4720v4	http://arxiv.org/pdf/1202.4720v4	2012	Non-Stationary Random Process for Large-Scale Failure and Recovery of   Power Distributions	Yun Wei|Chuanyi Ji|Floyd Galvan|Stephen Couvillon|George Orellana|James Momoh	  A key objective of the smart grid is to improve reliability of utility services to end users. This requires strengthening resilience of distribution networks that lie at the edge of the grid. However, distribution networks are exposed to external disturbances such as hurricanes and snow storms where electricity service to customers is disrupted repeatedly. External disturbances cause large-scale power failures that are neither well-understood, nor formulated rigorously, nor studied systematically. This work studies resilience of power distribution networks to large-scale disturbances in three aspects. First, a non-stationary random process is derived to characterize an entire life cycle of large-scale failure and recovery. Second, resilience is defined based on the non-stationary random process. Close form analytical expressions are derived under specific large-scale failure scenarios. Third, the non-stationary model and the resilience metric are applied to a real life example of large-scale disruptions due to Hurricane Ike. Real data on large-scale failures from an operational network is used to learn time-varying model parameters and resilience metrics. 	
1204.4159v2	http://arxiv.org/pdf/1204.4159v2	2012	Connectivity Oracles for Planar Graphs	Glencora Borradaile|Seth Pettie|Christian Wulff-Nilsen	  We consider dynamic subgraph connectivity problems for planar graphs. In this model there is a fixed underlying planar graph, where each edge and vertex is either "off" (failed) or "on" (recovered). We wish to answer connectivity queries with respect to the "on" subgraph. The model has two natural variants, one in which there are $d$ edge/vertex failures that precede all connectivity queries, and one in which failures/recoveries and queries are intermixed.   We present a $d$-failure connectivity oracle for planar graphs that processes any $d$ edge/vertex failures in $sort(d,n)$ time so that connectivity queries can be answered in $pred(d,n)$ time. (Here $sort$ and $pred$ are the time for integer sorting and integer predecessor search over a subset of $[n]$ of size $d$.) Our algorithm has two discrete parts. The first is an algorithm tailored to triconnected planar graphs. It makes use of Barnette's theorem, which states that every triconnected planar graph contains a degree-3 spanning tree. The second part is a generic reduction from general (planar) graphs to triconnected (planar) graphs. Our algorithm is, moreover, provably optimal. An implication of Patrascu and Thorup's lower bound on predecessor search is that no $d$-failure connectivity oracle (even on trees) can beat $pred(d,n)$ query time.   We extend our algorithms to the subgraph connectivity model where edge/vertex failures (but no recoveries) are intermixed with connectivity queries. In triconnected planar graphs each failure and query is handled in $O(\log n)$ time (amortized), whereas in general planar graphs both bounds become $O(\log^2 n)$. 	
1212.5620v1	http://arxiv.org/pdf/1212.5620v1	2012	Topological Analysis and Mitigation Strategies for Cascading Failures in   Power Grid Networks	Sakshi Pahwa|Caterina Scoglio|Noel Schulz	  Recently, there has been a growing concern about the overload status of the power grid networks, and the increasing possibility of cascading failures. Many researchers have studied these networks to provide design guidelines for more robust power grids. Topological analysis is one of the components of system analysis for its robustness. This paper presents a complex systems analysis of power grid networks. First, the cascading effect has been simulated on three well known networks: the IEEE 300 bus test system, the IEEE 118 bus test system, and the WSCC 179 bus equivalent model. To extend the analysis to a larger set of networks, we develop a network generator and generate multiple graphs with characteristics similar to the IEEE test networks but with different topologies. The generated graphs are then compared to the test networks to show the effect of topology in determining their robustness with respect to cascading failures. The generated graphs turn out to be more robust than the test graphs, showing the importance of topology in the robust design of power grids. The second part of this paper concerns the discussion of two novel mitigation strategies for cascading failures: Targeted Load Reduction and Islanding using Distributed Sources. These new mitigation strategies are compared with the Homogeneous Load Reduction strategy. Even though the Homogeneous Load Reduction is simpler to implement, the Targeted Load Reduction is much more effective. Additionally, an algorithm is presented for the partitioning of the network for islanding as an effort towards fault isolation to prevent cascading failures. The results for island formation are better if the sources are well distributed, else the algorithm leads to the formation of superislands. 	
1408.3164v2	http://arxiv.org/pdf/1408.3164v2	2014	Detection and Isolation of Failures in Directed Networks of LTI Systems	Mohammad Amin Rahimian|Victor M. Preciado	  We propose a methodology to detect and isolate link failures in a weighted and directed network of identical multi-input multi-output LTI systems when only the output responses of a subset of nodes are available. Our method is based on the observation of jump discontinuities in the output derivatives, which can be explicitly related to the occurrence of link failures. The order of the derivative at which the jump is observed is given by $r(d+1)$, where $r$ is the relative degree of each system's transfer matrix, and $d$ denotes the distance from the location of the failure to the observation point. We then propose detection and isolation strategies based on this relation. Furthermore, we propose an efficient algorithm for sensor placement to detect and isolate any possible link failure using a small number of sensors. Available results from the theory of sub-modular set functions provide us with performance guarantees that bound the size of the chosen sensor set within a logarithmic factor of the smallest feasible set of sensors. These results are illustrated through elaborative examples and supplemented by computer experiments. 	
1412.0366v1	http://arxiv.org/pdf/1412.0366v1	2014	Node Failure Time and Coverage Loss Time Analysis for Maximum Stability   Vs Minimum Distance Spanning Tree based Data Gathering in Mobile Sensor   Networks	Natarajan Meghanathan|Philip Mumford	  A mobile sensor network is a wireless network of sensor nodes that move arbitrarily. In this paper, we explore the use of a maximum stability spanning tree-based data gathering (Max.Stability-DG) algorithm and a minimum-distance spanning tree-based data gathering (MST-DG) algorithm for mobile sensor networks. We analyze the impact of these two algorithms on the node failure times and the resulting coverage loss due to node failures. Both the Max.Stability-DG and MST-DG algorithms are based on a greedy strategy of determining a data gathering tree when one is needed and using that tree as long as it exists. The Max.Stability-DG algorithm assumes the availability of the complete knowledge of future topology changes and determines a data gathering tree whose corresponding spanning tree would exist for the longest time since the current time instant; whereas, the MST-DG algorithm determines a data gathering tree whose corresponding spanning tree is the minimum distance tree at the current time instant. We observe the Max.Stability-DG trees to incur a longer network lifetime (time of disconnection of the network of live sensor nodes due to node failures), a larger coverage loss time for a particular fraction of loss of coverage as well as a lower fraction of coverage loss at any time. The tradeoff is that the Max.Stability-DG trees incur a lower node lifetime (the time of first node failure) due to repeated use of a data gathering tree for a longer time. 	
1502.00781v1	http://arxiv.org/pdf/1502.00781v1	2015	CHAOS: Accurate and Realtime Detection of Aging-Oriented Failure Using   Entropy	Pengfei Chen|Yong Qi|Di Hou	  Even well-designed software systems suffer from chronic performance degradation, also named "software aging", due to internal (e.g. software bugs) and external (e.g. resource exhaustion) impairments. These chronic problems often fly under the radar of software monitoring systems before causing severe impacts (e.g. system failure). Therefore it's a challenging issue how to timely detect these problems to prevent system crash. Although a large quantity of approaches have been proposed to solve this issue, the accuracy and effectiveness of these approaches are still far from satisfactory due to the insufficiency of aging indicators adopted by them. In this paper, we present a novel entropy-based aging indicator, Multidimensional Multi-scale Entropy (MMSE). MMSE employs the complexity embedded in runtime performance metrics to indicate software aging and leverages multi-scale and multi-dimension integration to tolerate system fluctuations. Via theoretical proof and experimental evaluation, we demonstrate that MMSE satisfies Stability, Monotonicity and Integration which we conjecture that an ideal aging indicator should have. Based upon MMSE, we develop three failure detection approaches encapsulated in a proof-of-concept named CHAOS. The experimental evaluations in a Video on Demand (VoD) system and in a real-world production system, AntVision, show that CHAOS can detect the failure-prone state in an extraordinarily high accuracy and a near 0 Ahead-Time-To-Failure (ATTF). Compared to previous approaches, CHAOS improves the detection accuracy by about 5 times and reduces the ATTF even by 3 orders of magnitude. In addition, CHAOS is light-weight enough to satisfy the realtime requirement. 	
1508.06913v1	http://arxiv.org/pdf/1508.06913v1	2015	Failure modes and conditions of a cohesive, spherical body due to YORP   spin-up	Masatoshi Hirabayashi	  This paper presents transition of the failure mode of a cohesive, spherical body due to YORP spin-up. On the assumption that the distribution of materials in the body is homogeneous, failed regions first appearing in the body at different spin rates are predicted by comparing the yield condition of an elastic stress in the body. It is found that as the spin rate increases, the locations of the failed regions move from the equatorial surface to the central region. To avoid such failure modes, the body should have higher cohesive strength. The results by this model are consistent with those by a plastic finite element model. Then, this model and a two-layered-cohesive model first proposed by Hirabayashi et al. are used to classify possible evolution and disruption of a spherical body. There are three possible pathways to disruption. First, because of a strong structure, failure of the central region is dominant and eventually leads to a breakup into multiple components. Second, a weak surface and a weak interior make the body oblate. Third, a strong internal core prevents the body from failing and only allows surface shedding. This implies that observed failure modes may highly depend on the internal structure of an asteroid, which could provide crucial information for giving constraints on the physical properties. 	
1509.03838v1	http://arxiv.org/pdf/1509.03838v1	2015	Failure Mitigation in Linear, Sesquilinear and Bijective Operations On   Integer Data Streams Via Numerical Entanglement	Mohammad Ashraful Anam|Yiannis Andreopoulos	  A new roll-forward technique is proposed that recovers from any single fail-stop failure in $M$ integer data streams ($M\geq3$) when undergoing linear, sesquilinear or bijective (LSB) operations, such as: scaling, additions/subtractions, inner or outer vector products and permutations. In the proposed approach, the $M$ input integer data streams are linearly superimposed to form $M$ numerically entangled integer data streams that are stored in-place of the original inputs. A series of LSB operations can then be performed directly using these entangled data streams. The output results can be extracted from any $M-1$ entangled output streams by additions and arithmetic shifts, thereby guaranteeing robustness to a fail-stop failure in any single stream computation. Importantly, unlike other methods, the number of operations required for the entanglement, extraction and recovery of the results is linearly related to the number of the inputs and does not depend on the complexity of the performed LSB operations. We have validated our proposal in an Intel processor (Haswell architecture with AVX2 support) via convolution operations. Our analysis and experiments reveal that the proposed approach incurs only $1.8\%$ to $2.8\%$ reduction in processing throughput in comparison to the failure-intolerant approach. This overhead is 9 to 14 times smaller than that of the equivalent checksum-based method. Thus, our proposal can be used in distributed systems and unreliable processor hardware, or safety-critical applications, where robustness against fail-stop failures becomes a necessity. 	
1605.05412v4	http://arxiv.org/pdf/1605.05412v4	2016	Maximally Recoverable Codes for Grid-like Topologies	Parikshit Gopalan|Guangda Hu|Swastik Kopparty|Shubhangi Saraf|Carol Wang|Sergey Yekhanin	  The explosion in the volumes of data being stored online has resulted in distributed storage systems transitioning to erasure coding based schemes. Yet, the codes being deployed in practice are fairly short. In this work, we address what we view as the main coding theoretic barrier to deploying longer codes in storage: at large lengths, failures are not independent and correlated failures are inevitable. This motivates designing codes that allow quick data recovery even after large correlated failures, and which have efficient encoding and decoding. We propose that code design for distributed storage be viewed as a two-step process. The first step is choose a topology of the code, which incorporates knowledge about the correlated failures that need to be handled, and ensures local recovery from such failures. In the second step one specifies a code with the chosen topology by choosing coefficients from a finite field. In this step, one tries to balance reliability (which is better over larger fields) with encoding and decoding efficiency (which is better over smaller fields). This work initiates an in-depth study of this reliability/efficiency tradeoff. We consider the field-size needed for achieving maximal recoverability: the strongest reliability possible with a given topology. We propose a family of topologies called grid-like topologies which unify a number of topologies considered both in theory and practice, and prove a collection of results about maximally recoverable codes in such topologies including the first super-polynomial lower bound on the field size. 	
1609.07796v2	http://arxiv.org/pdf/1609.07796v2	2017	Error Correction Coding Meets Cyber-Physical Systems: Message-Passing   Analysis of Self-Healing Interdependent Networks	Ali Behfarnia|Ali Eslami	  Coupling cyber and physical systems gives rise to numerous engineering challenges and opportunities. An important challenge is the contagion of failure from one system to another, which can lead to large-scale cascading failures. However, the self-healing ability emerges as a valuable opportunity where the overlaying cyber network can cure failures in the underlying physical network. To capture both self-healing and contagion, this paper considers a graphical model representation of an interdependent cyber-physical system, in which nodes represent various cyber or physical functionalities, and edges capture the interactions between the nodes. A message-passing algorithm is proposed for this representation to study the dynamics of failure propagation and healing. By conducting a density evolution analysis for this algorithm, network reaction to initial disruptions is investigated. It is proved that as the number of message-passing iterations increases, the network reaches a steady-state condition that would be either a complete healing or a complete collapse. Then, a sufficient condition is derived to select the network parameters to guarantee the complete healing of the system. The result of the density evolution analysis is further employed to jointly optimize the design of cyber and physical networks for maximum resiliency. This analytical framework is then extended to the cases where propagation of failures in the physical network is faster than the healing responses of the cyber network. Such scenarios are of interest in many real-life applications such as smart grid. Finally, extensive numerical results are presented to verify the analysis and investigate the impact of the network parameters on the resiliency of the network. 	
1610.04872v1	http://arxiv.org/pdf/1610.04872v1	2016	Fault Detection Engine in Intelligent Predictive Analytics Platform for   DCIM	Bodhisattwa Prasad Majumder|Ayan Sengupta|Sajal jain|Parikshit Bhaduri	  With the advancement of huge data generation and data handling capability, Machine Learning and Probabilistic modelling enables an immense opportunity to employ predictive analytics platform in high security critical industries namely data centers, electricity grids, utilities, airport etc. where downtime minimization is one of the primary objectives. This paper proposes a novel, complete architecture of an intelligent predictive analytics platform, Fault Engine, for huge device network connected with electrical/information flow. Three unique modules, here proposed, seamlessly integrate with available technology stack of data handling and connect with middleware to produce online intelligent prediction in critical failure scenarios. The Markov Failure module predicts the severity of a failure along with survival probability of a device at any given instances. The Root Cause Analysis model indicates probable devices as potential root cause employing Bayesian probability assignment and topological sort. Finally, a community detection algorithm produces correlated clusters of device in terms of failure probability which will further narrow down the search space of finding route cause. The whole Engine has been tested with different size of network with simulated failure environments and shows its potential to be scalable in real-time implementation. 	
1610.09435v2	http://arxiv.org/pdf/1610.09435v2	2016	On the Power of Weaker Pairwise Interaction: Fault-Tolerant Simulation   of Population Protocols	Giuseppe Antonio Di Luna|Paola Flocchini|Taisuke Izumi|Tomoko Izumi|Nicola Santoro|Giovanni Viglietta	  In this paper we investigate the computational power of Population Protocols (PP) under some unreliable and/or weaker interaction models. More precisely, we focus on two features related to the power of interactions: omission failures and one-way communications. An omission failure, a notion that this paper introduces for the first time in the context of PP, is the loss by one or both parties of the information transmitted in an interaction. The failure may or may not be detected by either party. On the other hand, in one-way models, communication happens only in one direction: only one of the two agents can change its state depending on both agents' states, and the other agent may or may not be aware of the interaction. These notions can be combined, obtaining one-way protocols with (possibly detectable) omission failures.   A general question is what additional power is necessary and sufficient to completely overcome the weakness of one-way protocols and enable them to simulate two-way protocols, with and without omission failures. As a basic feature, a simulator needs to implement an atomic communication of states between two agents; this task is further complicated by the anonymity of the agents, their lack of knowledge of the system, and the limited amount of memory that they may have.   We provide the first answers to these questions by presenting and analyzing several simulators, i.e., wrapper protocols converting any protocol for the standard two-way model into one running on a weaker one. 	
1708.00428v2	http://arxiv.org/pdf/1708.00428v2	2017	Cascading Failures in Interdependent Networks with Multiple   Supply-Demand Links and Functionality Thresholds	M. A. Di Muro|L. D. Valdez|H. H. A. Rêgo|S. V. Buldyrev|H. E. Stanley|L. A. Braunstein	  Various social, financial, biological and technological systems can be modeled by interdependent networks. It has been assumed that in order to remain functional, nodes in one network must receive the support from nodes belonging to different networks. So far these models have been limited to the case in which the failure propagates across networks only if the nodes lose all their supply nodes. In this paper we develop a more realistic model for two interdependent networks in which each node has its own supply threshold, i.e., they need the support of a minimum number of supply nodes to remain functional. In addition, we analyze different conditions of internal node failure due to disconnection from nodes within its own network. We show that several local internal failure conditions lead to similar nontrivial results. When there are no internal failures the model is equivalent to a bipartite system, which can be useful to model a financial market. We explore the rich behaviors of these models that include discontinuous and continuous phase transitions. Using the generating functions formalism, we analytically solve all the models in the limit of infinitely large networks and find an excellent agreement with the stochastic simulations. 	
1712.02872v1	http://arxiv.org/pdf/1712.02872v1	2017	Dynamic Fault Trees Analysis using an Integration of Theorem Proving and   Model Checking	Yassmeen Elderhalli|Osman Hasan|Waqar Ahmad|Sofiene Tahar	  Dynamic fault trees (DFTs) have emerged as an important tool for capturing the dynamic behavior of system failure. These DFTs are then analyzed qualitatively and quantitatively using stochastic or algebraic methods to judge the failure characteristics of the given system in terms of the failures of its sub-components. Model checking has been recently proposed to conduct the failure analysis of systems using DFTs with the motivation to provide a rigorous failure analysis of safety-critical systems. However, model checking has not been used for the DFT qualitative analysis and the reduction algorithms used in model checking are usually not formally verified. Moreover, the analysis time grows exponentially with the increase of the number of states. These issues limit the usefulness of model checking for analyzing complex systems used in safety-critical domains, where the accuracy and completeness of analysis matters the most. To overcome these limitations, we propose a comprehensive methodology to perform the qualitative and quantitative analysis of DFTs using an integration of theorem proving and model checking based approaches. For this purpose, we formalized all the basic dynamic fault tree gates using higher-order logic based on the algebraic approach and formally verified some of the simplification properties. This formalization allows us to formally verify the equivalence between the original and reduced DFTs using a theorem prover, and conduct the qualitative analysis. We then use model checking to perform the quantitative analysis of the formally verified reduced DFT. We applied our methodology to five benchmarks and the results show that the formally verified reduced DFT was analyzed using model checking with up to six times less states and up to 133000 times faster. 	
0512189v2	http://arxiv.org/pdf/astro-ph/0512189v2	2006	Two-Dimensional Hydrodynamic Core-Collapse Supernova Simulations with   Spectral Neutrino Transport II. Models for Different Progenitor Stars	R. Buras|H. -Th. Janka|M. Rampp|K. Kifonidis	  1D and 2D supernova simulations for stars between 11 and 25 solar masses are presented, making use of the Prometheus/Vertex neutrino-hydrodynamics code, which employs a full spectral treatment of the neutrino transport. Multi-dimensional transport aspects are treated by the ``ray-by-ray plus'' approximation described in Paper I. Our set of models includes a 2D calculation for a 15 solar mass star whose iron core is assumed to rotate rigidly with an angular frequency of 0.5 rad/s before collapse. No important differences were found depending on whether random seed perturbations for triggering convection are included already during core collapse, or whether they are imposed on a 1D collapse model shortly after bounce. Convection below the neutrinosphere sets in about 40 ms p.b. at a density above 10**12 g/cm^3 in all 2D models, and encompasses a layer of growing mass as time goes on. It leads to a more extended proto-neutron star structure with accelerated lepton number and energy loss and significantly higher muon and tau neutrino luminosities, but reduced mean energies of the radiated neutrinos, at times later than ~100 ms p.b. In case of an 11.2 solar mass star we find that low (l = 1,2) convective modes cause a probably rather weak explosion by the convectively supported neutrino-heating mechanism after ~150 ms p.b. when the 2D simulation is performed with a full 180 degree grid, whereas the same simulation with 90 degree wedge fails to explode like all other models. This sensitivity demonstrates the proximity of our 2D models to the borderline between success and failure, and stresses the need of simulations in 3D, ultimately without the axis singularity of a polar grid. (abridged) 	
0208413v2	http://arxiv.org/pdf/cond-mat/0208413v2	2003	Slider-Block Friction Model for Landslides: Application to Vaiont and La   Clapiere Landslides	A. Helmstetter|D. Sornette|J. -R. Grasso|J. V. Andersen|S. Gluzman|V. Pisarenko	  Accelerating displacements preceding some catastrophic landslides have been found empirically to follow a time-to-failure power law, corresponding to a finite-time singularity of the velocity $v \sim 1/(t_c-t)$ [{\it Voight}, 1988]. Here, we provide a physical basis for this phenomenological law based on a slider-block model using a state and velocity dependent friction law established in the laboratory and used to model earthquake friction. This physical model accounts for and generalizes Voight's observation: depending on the ratio $B/A$ of two parameters of the rate and state friction law and on the initial frictional state of the sliding surfaces characterized by a reduced parameter $x_i$, four possible regimes are found. Two regimes can account for an acceleration of the displacement. We use the slider-block friction model to analyze quantitatively the displacement and velocity data preceding two landslides, Vaiont and La Clapi\`ere. The Vaiont landslide was the catastrophic culmination of an accelerated slope velocity. La Clapi\`ere landslide was characterized by a peak of slope acceleration that followed decades of ongoing accelerating displacements, succeeded by a restabilizing phase. Our inversion of the slider-block model on these data sets shows good fits and suggest to classify the Vaiont (respectively La Clapi\`ere) landslide as belonging to the velocity weakening unstable (respectively strengthening stable) sliding regime. 	
0310374v2	http://arxiv.org/pdf/cond-mat/0310374v2	2005	Two early stage inverse power-law relaxations in the far from   equilibrium dynamics in semi-classical percolative composites	Somnath Bhattacharya|Partha Pratim Roy|Asok Kumar Sen	  In several experiments for measuring various classes of responses, performed at least some four decades ago, on driven physical systems in a far-from-equilibrium (or, from a steady-state) situation, early stage inverse-power-law relaxation dynamics had been observed. Since then, this intriguing behavior raised its head off and on until it regained its central role in the mainstream physical sciences about a decade ago with a breakdown and/or avalanche type (also called self-organized critical) behavior of the sand-pile model and a host of other similar problems. In this communication, we report on the non-equilibrium dynamics in our Random Resistor cum Tunneling-bond Network (RRTN) model. Previously, this semi-classical, or semi-quantum percolative model has been highly successful in explaining the static behavior for various random composite systems. In our dynamic studies for the last several years, we observe two initial power-laws (more than a decade each) and then an exponential relaxation for asymptotically large time scales. Efforts were made to interpret our results with various existing theoretical wisdom/s (which give, only one power-law relaxation for each such system near its breakdown or run-away type state). Obviously, our results (with two different power-laws) are richer than those particular cases. Further, a complete theory is still lacking probably due to a much deeper issue of entropy at stake. The appearance of two power-laws seems to be connected to some non-extensive information-loss / entropy (the experimental systems being mostly athermal) for such systems near their brinks (catastrophic failure not necessarily due to criticality). 	
9911375v1	http://arxiv.org/pdf/hep-ph/9911375v1	1999	The physics of exclusive reactions in QCD: Theory and phenomenology	N. G. Stefanis	  The modern formulation of exclusive reactions within Quantum Chromodynamics is reviewed, the emphasis being placed on the pivotal ideas and methods pertaining to perturbative and non-perturbative topics. Specific problems, related to scale locality, infrared safety, gluonic radiative corrections (Sudakov effects), and the role of hadronic size effects (intrinsic transverse momentum), are studied. These issues are more precisely analyzed in terms of the essential mechanisms of momentum transfer to a hadron while remaining intact. Different factorization schemes are considered and the conceptual lacunas are pointed out. The quite technical subject of renormalization-group evolution is given a detailed account. By combining analytical and numerical algorithms, the one-gluon exchange nucleon evolution equation is diagonalized and next-to-leading eigenfunctions are calculated in terms of Appell polynomials. The corresponding anomalous dimensions of trilinear quark operators are found to form a degenerate system whose envelope shows logarithmic large-order behavior. Selected applications of this framework are presented, focusing on the helicity-conserving elastic form factors of the pion and the nucleon. The theoretical constraints imposed by QCD sum rules on the moments of nucleon distribution amplitudes are used to determine a whole spectrum of optional solutions. They organize themselves along an ``orbit'' characterized by a striking scaling relation between the form-factor ratio $R=|G_{\rm M}^{\rm n}|/G_{\rm M}^{\rm p}$ and the projection coefficient $B_{4}$ on to the corresponding eigensolution. The main reasons for the failure of the present theoretical predictions to match the experimental data are discussed and workable explanations are sketched. 	
0810.5753v1	http://arxiv.org/pdf/0810.5753v1	2008	GalICS II: the [alpha/Fe]-mass relation in elliptical galaxies	A. Pipino|J. E. G. Devriendt|D. Thomas|J. Silk|S. Kaviraj	  We aim at reproducing the mass- and sigma-[alpha/Fe] relations in the stellar populations of early-type galaxies by means of a cosmologically motivated assembly history for the spheroids. We implement a detailed treatment for the chemical evolution of H, He, O and Fe in GalICS, a semi-analytical model for galaxy formation which successfully reproduces basic low- and high-redshift galaxy properties. The contribution of supernovae (both type Ia and II) as well as low- and intermediate-mass stars to chemical feedback are taken into account. We find that this chemically improved GalICS does not produce the observed mass- and sigma-[alpha/Fe] relations. The slope is too shallow and scatter too large, in particular in the low and intermediate mass range. The model shows significant improvement at the highest masses and velocity dispersions, where the predicted [alpha/Fe] ratios are now marginally consistent with observed values. We show that this result comes from the implementation of AGN (plus halo) quenching of the star formation in massive haloes. A thorough exploration of the parameter space shows that the failure of reproducing the mass- and sigma-[alpha/Fe] relations can partly be attributed to the way in which star formation and feedback are currently modelled. The merger process is responsible for a part of the scatter. We suggest that the next generation of semi-analytical model should feature feedback (either stellar of from AGN) mechanisms linked to single galaxies and not only to the halo, especially in the low and intermediate mass range. The integral star formation history of a single galaxy determines its final stellar [alpha/Fe] as it might be expected from the results of closed box chemical evolution models. (abridged) 	
0904.1426v4	http://arxiv.org/pdf/0904.1426v4	2012	What are the limits on Commercial Bank Lending?	Jacky Mallett	  Analysis of the 2007-8 credit crisis has concentrated on issues of relaxed lending standards, and the perception of irrational behaviour by speculative investors in real estate and other assets. Asset backed securities have been extensively criticised for creating a moral hazard in loan issuance and an associated increase in default risk, by removing the immediate lender's incentive to ensure that the underlying loans could be repaid. However significant monetary issues can accompany any form of increased commercial bank lending, and these appear to have been overlooked by this analysis. In this paper we propose a general explanation for credit crises based on an examination of the mechanics of the banking system, and in particular its internal controls on the supply of credit. We suggest that the current credit crisis is the result of multiple failures in the Basel regulatory framework, including the removal of central bank reserve requirements from some classes of deposit accounts within the banking system, allowing financial instruments representing debt to be used as regulatory capital, and in particular the introduction of securitized lending which effectively removed a previously implicit control over the total quantity of lending originating from the banking system. We further argue that the interaction of these problems has led to a destabilising imbalance between total money and loan supply growth, in that total lending sourced from the commercial bank sector increased at a faster rate than accompanying growth in the money supply. This not only created a multi-decade macro-economic debt spiral, but by increasing the ratio of debt to money within the monetary system acted to increase the risk of loan defaults, and consequentially reduce the overall stability of the banking system. 	
0909.2886v1	http://arxiv.org/pdf/0909.2886v1	2009	Comment on "Breakdown of the Luttinger sum rule within the Mott-Hubbard   insulator", by J. Kokalj and P. Prelovsek [Phys. Rev. B 78, 153103 (2008),   arXiv:arXiv:0803.4468]	Behnam Farid|Alexei M. Tsvelik	  On the basis of an analysis of the numerical results corresponding to the half-filled 1D t-t'-V model on some finite lattices, Kokalj and Prelovsek (KP) have in a recent paper [Phys. Rev. B 78, 153103 (2008), arXiv:arXiv:0803.4468] concluded that the Luttinger theorem (LT) does not apply for the Mott-Hubbard (MH) insulating phase of this model (i.e. for V >> t) in the thermodynamic limit; KP even suggested, incorrectly, that failure of the LT were apparent for a half-filled finite system consisting of N=26 lattice sites. By employing a simple model for the self-energy Sigma of a MH state, we show that the finite-size-scaling approach of the type utilised by KP is not reliable for the system sizes considered by KP. On the basis of the equivalence of the model under consideration (at half-filling and for t'/t << 1) and the XXZ spin-chain Hamiltonian for SU(2) spins, we further show that for V > V_c(t,t') the system under consideration has a charge-density-wave (CDW) ground state (GS) in the thermodynamic limit, corresponding to a doubling of the unit cell in comparison with that specific to the underlying lattice. Although this GS is also insulating, its spectral gap is due to the broken translational symmetry of the GS; it is not a correlation-induced MH gap. The LT is therefore a priori valid for this GS. This fact establishes that the conclusion by KP is indeed erroneous. Finally, we present a heuristic argument due to Volovik that sheds light on the mechanism underlying the robustness of the LT. In an appendix, we present the details of the calculation of the single-particle Green function of the broken-symmetry GS of the model under consideration by means of bosonization and in terms of the form factors of a class of soliton-generating fields pertaining to the quantum sine-Gordon Hamiltonian. [Shortened abstract] 	
1004.1820v2	http://arxiv.org/pdf/1004.1820v2	2010	Equivalence of Maxwell's source-free equations to the time-dependent   Schroedinger equation for a solitary particle with two polarizations and   Hamiltonian |cp|	Steven Kenneth Kauffmann	  It was pointed out in a previous paper that although neither the Klein-Gordon equation nor the Dirac Hamiltonian produces sound solitary free-particle relativistic quantum mechanics, the natural square-root relativistic Hamiltonian for a nonzero-mass free particle does achieve this. Failures of the Klein-Gordon and Dirac theories are reviewed: the solitary Dirac free particle has, inter alia, an invariant speed well in excess of c and staggering spontaneous Compton acceleration, but no pathologies whatsoever arise from the square-root relativistic Hamiltonian. Dirac's key misapprehension of the underlying four-vector character of the time-dependent, configuration-representation Schroedinger equation for a solitary particle is laid bare, as is the invalidity of the standard "proof" that the nonrelativistic limit of the Dirac equation is the Pauli equation. Lorentz boosts from the particle rest frame point uniquely to the square-root Hamiltonian, but these don't exist for a massless particle. Instead, Maxwell's equations are dissected in spatial Fourier transform to separate nondynamical longitudinal from dynamical transverse field degrees of freedom. Upon their decoupling in the absence of sources, the transverse field components are seen to obey two identical time-dependent Schroedinger equations (owing to two linear polarizations), which have the massless free-particle diagonalized square-root Hamiltonian. Those fields are readily modified to conform to the attributes of solitary-photon wave functions. The wave functions' relations to the potentials in radiation gauge are also worked out. The exercise is then repeated without the considerable benefit of the spatial Fourier transform. 	
1010.2634v1	http://arxiv.org/pdf/1010.2634v1	2010	The statistical laws of popularity: Universal properties of the box   office dynamics of motion pictures	Raj Kumar Pan|Sitabhra Sinha	  Are there general principles governing the process by which certain products or ideas become popular relative to other (often qualitatively similar) competitors? To investigate this question in detail, we have focused on the popularity of movies as measured by their box-office income. We observe that the log-normal distribution describes well the tail (corresponding to the most successful movies) of the empirical distributions for the total income, the income on the opening week, as well as, the weekly income per theater. This observation suggests that popularity may be the outcome of a linear multiplicative stochastic process. In addition, the distributions of the total income and the opening income show a bimodal form, with the majority of movies either performing very well or very poorly in theaters. We also observe that the gross income per theater for a movie at any point during its lifetime is, on average, inversely proportional to the period that has elapsed after its release. We argue that (i) the log-normal nature of the tail, (ii) the bimodal form of the overall gross income distribution, and (iii) the decay of gross income per theater with time as a power law, constitute the fundamental set of {\em stylized facts} (i.e., empirical "laws") that can be used to explain other observations about movie popularity. We show that, in conjunction with an assumption of a fixed lower cut-off for income per theater below which a movie is withdrawn from a cinema, these laws can be used to derive a Weibull distribution for the survival probability of movies which agrees with empirical data. The connection to extreme-value distributions suggests that popularity can be viewed as a process where a product becomes popular by avoiding failure (i.e., being pulled out from circulation) for many successive time periods. We suggest that these results may apply to popularity in general. 	
1101.5056v5	http://arxiv.org/pdf/1101.5056v5	2012	The Peculiar Status of the Second Law of Thermodynamics and the Quest   for its Violation	Germano D'Abramo	  Even though the second law of thermodynamics holds the supreme position among the laws of nature, as stated by many distinguished scientists, notably Eddington and Einstein, its position appears to be also quite peculiar. Given the atomic nature of matter, whose behaviour is well described by statistical physics, the second law could not hold unconditionally, but only statistically. It is not an absolute law. As a result of this, in the present paper we try to argue that we have not yet any truly cogent argument (known fundamental physical laws) to exclude its possible macroscopic violation. Even Landauer's information-theoretic principle seems to fall short of the initial expectations of being the fundamental `physical' reason of all Maxwell's demons failure. Here we propose a modified Szilard engine which operates without any steps in the process resembling the creation or destruction of information. We argue that the information-based exorcisms must be wrong, or at the very least superfluous, and that the real physical reason why such engines cannot work lies in the ubiquity of thermal fluctuations (and friction).   We see in the above peculiar features the main motivation and rationale for pursuing exploratory research to challenge the second law, which is still ongoing and probably richer than ever. A quite thorough (and critical) description of some of these challenges is also given. 	
1103.3528v1	http://arxiv.org/pdf/1103.3528v1	2011	Galaxy Evolution in Cosmological Simulations With Outflows I: Stellar   Masses and Star Formation Rates	Romeel Davé|Benjamin D. Oppenheimer|Kristian Finlator	  We examine the growth of the stellar content of galaxies from z=3-0 in cosmological hydrodynamic simulations incorporating parameterised galactic outflows. Without outflows, galaxies overproduce stellar masses (M*) and star formation rates (SFRs) compared to observations. Winds introduce a three-tier form for the galaxy stellar mass and star formation rate functions, where the middle tier depends on differential (i.e. mass-dependent) recycling of ejected wind material back into galaxies. A tight M*-SFR relation is a generic outcome of all these simulations, and its evolution is well-described as being powered by cold accretion, although current observations at z>2 suggest that star formation in small early galaxies must be highly suppressed. Roughly one-third of z=0 galaxies at masses below M^* are satellites, and star formation in satellites is not much burstier than in centrals. All models fail to suppress star formation and stellar mass growth in massive galaxies at z<2, indicating the need for an external quenching mechanism such as black hole feedback. All models also fail to produce dwarfs as young and rapidly star-forming as observed. An outflow model following scalings expected for momentum-driven winds broadly matches observed galaxy evolution around M^* from z=0-3, which is a significant success since these galaxies dominate cosmic star formation, but the failures at higher and lower masses highlight the challenges still faced by this class of models. We argue that central star-forming galaxies are well-described as living in a slowly-evolving equilibrium between inflows from gravity and recycled winds, star formation, and strong and ubiquitous outflows that regulate how much inflow forms into stars. Star-forming galaxy evolution is thus primarily governed by the continual cycling of baryons between galaxies and intergalactic gas. 	
1106.1631v1	http://arxiv.org/pdf/1106.1631v1	2011	The combined effect of connectivity and dependency links on percolation   of networks	Amir Bashan|Shlomo Havlin	  Percolation theory is extensively studied in statistical physics and mathematics with applications in diverse fields. However, the research is focused on systems with only one type of links, connectivity links. We review a recently developed mathematical framework for analyzing percolation properties of realistic scenarios of networks having links of two types, connectivity and dependency links. This formalism was applied to study Erd$\ddot{o}$s-R$\acute{e}$nyi (ER) networks that include also dependency links. For an ER network with average degree $k$ that is composed of dependency clusters of size $s$, the fraction of nodes that belong to the giant component, $P_\infty$, is given by $ P_\infty=p^{s-1}[1-\exp{(-kpP_\infty)}]^s $ where $1-p$ is the initial fraction of randomly removed nodes. Here, we apply the formalism to the study of random-regular (RR) networks and find a formula for the size of the giant component in the percolation process: $P_\infty=p^{s-1}(1-r^k)^s$ where $r$ is the solution of $r=p^s(r^{k-1}-1)(1-r^k)+1$. These general results coincide, for $s=1$, with the known equations for percolation in ER and RR networks respectively without dependency links. In contrast to $s=1$, where the percolation transition is second order, for $s>1$ it is of first order. Comparing the percolation behavior of ER and RR networks we find a remarkable difference regarding their resilience. We show, analytically and numerically, that in ER networks with low connectivity degree or large dependency clusters, removal of even a finite number (zero fraction) of the network nodes will trigger a cascade of failures that fragments the whole network. This result is in contrast to RR networks where such cascades and full fragmentation can be triggered only by removal of a finite fraction of nodes in the network. 	
1108.3080v3	http://arxiv.org/pdf/1108.3080v3	2012	How unitary cosmology generalizes thermodynamics and solves the   inflationary entropy problem	Max Tegmark	  We analyze cosmology assuming unitary quantum mechanics, using a tripartite partition into system, observer and environment degrees of freedom. This generalizes the second law of thermodynamics to "The system's entropy can't decrease unless it interacts with the observer, and it can't increase unless it interacts with the environment." The former follows from the quantum Bayes Theorem we derive. We show that because of the long-range entanglement created by cosmological inflation, the cosmic entropy decreases exponentially rather than linearly with the number of bits of information observed, so that a given observer can reduce entropy by much more than the amount of information her brain can store. Indeed, we argue that as long as inflation has occurred in a non-negligible fraction of the volume, almost all sentient observers will find themselves in a post-inflationary low-entropy Hubble volume, and we humans have no reason to be surprised that we do so as well, which solves the so-called inflationary entropy problem. An arguably worse problem for unitary cosmology involves gamma-ray-burst constraints on the "Big Snap", a fourth cosmic doomsday scenario alongside the "Big Crunch", "Big Chill" and "Big Rip", where an increasingly granular nature of expanding space modifies our life-supporting laws of physics.   Our tripartite framework also clarifies when it is valid to make the popular quantum gravity approximation that the Einstein tensor equals the quantum expectation value of the stress-energy tensor, and how problems with recent attempts to explain dark energy as gravitational backreaction from super-horizon scale fluctuations can be understood as a failure of this approximation. 	
1111.2619v1	http://arxiv.org/pdf/1111.2619v1	2011	A Security Architecture for Data Aggregation and Access Control in Smart   Grids	Sushmita Ruj|Amiya Nayak|Ivan Stojmenovic	  We propose an integrated architecture for smart grids, that supports data aggregation and access control. Data can be aggregated by home area network, building area network and neighboring area network in such a way that the privacy of customers is protected. We use homomorphic encryption technique to achieve this. The consumer data that is collected is sent to the substations where it is monitored by remote terminal units (RTU). The proposed access control mechanism gives selective access to consumer data stored in data repositories and used by different smart grid users. Users can be maintenance units, utility centers, pricing estimator units or analyzing and prediction groups. We solve this problem of access control using cryptographic technique of attribute-based encryption. RTUs and users have attributes and cryptographic keys distributed by several key distribution centers (KDC). RTUs send data encrypted under a set of attributes. Users can decrypt information provided they have valid attributes. The access control scheme is distributed in nature and does not rely on a single KDC to distribute keys. Bobba \emph{et al.} \cite{BKAA09} proposed an access control scheme, which relies on a centralized KDC and is thus prone to single-point failure. The other requirement is that the KDC has to be online, during data transfer which is not required in our scheme. Our access control scheme is collusion resistant, meaning that users cannot collude and gain access to data, when they are not authorized to access. We theoretically analyze our schemes and show that the computation overheads are low enough to be carried out in smart grids. To the best of our knowledge, ours is the first work on smart grids, which integrates these two important security components (privacy preserving data aggregation and access control) and presents an overall security architecture in smart grids. 	
1112.0944v1	http://arxiv.org/pdf/1112.0944v1	2011	Predictions for mass-loss rates and terminal wind velocities of massive   O-type stars	L. E. Muijres|Jorick S. Vink|A. de Koter|P. E. Mueller|N. Langer	  Mass loss forms an important aspect of the evolution of massive stars, as well as for the enrichment of the surrounding ISM. Our goal is to predict accurate mass-loss rates and terminal wind velocities. These quantities can be compared to empirical values, thereby testing radiation-driven wind models. One specific issue is that of the "weak-wind problem", where empirically derived mass-loss rates fall orders of magnitude short of predicted values. We employ an established Monte Carlo model and a recently suggested new line acceleration formalism to solve the wind dynamics consistently. We provide a new grid of mass-loss rates and terminal wind velocities of O stars, and compare the values to empirical results. Our models fail to provide mass-loss rates for main-sequence stars below a luminosity of log(L/Lsun) = 5.2, where we run into a fundamental limit. At luminosities below this critical value there is insufficient momentum transferred in the region below the sonic point to kick-start the acceleration. This problem occurs at the location of the onset of the weak-wind problem. For O dwarfs, the boundary between being able to start a wind, and failing to do so, is at spectral type O6/O6.5. The direct cause of this failure is a combination of the lower luminosity and a lack of Fe V lines at the wind base. This might indicate that another mechanism is required to provide the necessary driving to initiate the wind. For stars more luminous than log(L/Lsun) = 5.2, our new mass-loss rates are in excellent agreement with the mass-loss prescription by Vink et al. 2000. This implies that the main assumption entering the method of the Vink et al. prescriptions - i.e. that the momentum equation is not explicitly solved for - does not compromise the reliability of the Vink et al. results for this part of parameter space (Abridged). 	
1204.3437v2	http://arxiv.org/pdf/1204.3437v2	2012	Does CHSH inequality test the model of local hidden variables?	Kazuo Fujikawa	  It is pointed out that the local hidden variables model of Bell and Clauser-Horne-Shimony-Holt (CHSH) gives $|<B>|\leq 2\sqrt{2}$ or $|<B>|\leq 2$ for the quantum CHSH operator $B={\bf a}\cdot {\bf \sigma}\otimes ({\bf b}+{\bf b}^{\prime})\cdot {\bf \sigma} +{\bf a}^{\prime}\cdot{\bf \sigma}\otimes ({\bf b}-{\bf b}^{\prime})\cdot{\bf \sigma} $ depending on two different ways of evaluation, when it is applied to a $d=4$ system of two spin-1/2 particles. This is due to the failure of linearity, and it shows that the conventional CHSH inequality $|<B>|\leq 2$ does not provide a reliable test of the $d=4$ local non-contextual hidden variables model. To achieve $|<B>|\leq 2$ uniquely, one needs to impose a linearity requirement on the hidden variables model, which in turn adds a von Neumann-type stricture. It is then shown that the local model is converted to a factored product of two non-contextual $d=2$ hidden variables models. This factored product implies pure separable quantum states and satisfies $|<B>|\leq 2$, but no more a proper hidden variables model in $d=4$. The conventional CHSH inequality $|<B>|\leq 2$ thus characterizes the pure separable quantum mechanical states but does not test the model of local hidden variables in $d=4$, to be consistent with Gleason's theorem which excludes non-contextual models in $d=4$. This observation is also consistent with an application of the CHSH inequality to quantum cryptography by Ekert, which is based on mixed separable states without referring to hidden variables. 	
1207.4544v1	http://arxiv.org/pdf/1207.4544v1	2012	The effect of size and distribution of rod-shaped β' precipitates on   the strength and ductility of a Mg-Zn alloy	Julian M. Rosalie|Hidetoshi Somekawa|Alok Singh|Toshiji Mukai	  We report on a quantitative investigation into the effect of size and distribution of rod-shaped \beta' precipitates on strength and ductility of a Mg-Zn alloy. Despite precipitation strengthening being crucial for the practical application of magnesium alloys this study represents the first systematic examination of the effect of controlled deformation on the precipitate size distribution and the resulting strength and ductility of a magnesium alloy. Pre-ageing deformation was used to obtain various distributions of rod-shaped \beta' precipitates through heterogeneous nucleation. Alloys were extruded to obtain a texture so as to avoid formation of twins and thus to ensure that dislocations were the primary nucleation site. Pre-ageing strain refined precipitate length and diameter, with average length reduced from 440 nm to 60 nm and diameter from 14 nm to 9 nm. Interparticle spacings were measured from micrographs and indicated some inhomogeneity in the precipitate distribution. The yield stress of the alloy increased from 273 MPa to 309 MPa. The yield stress increased linearly as a function of reciprocal interparticle spacing, but at a lower rate than predicted for Orowan strengthening. Pre-ageing deformation also resulted in a significant loss of ductility (from 17% to 6% elongation). Both true strain at failure and uniform elongation showed a linear relationship with particle spacing, in agreement with models for the accumulation of dislocations around non-deforming obstacles. Samples subjected to 3% pre-ageing deformation showed a substantially increased ageing response compared to non-deformed material; however, additional deformation (to 5% strain) resulted in only modest changes in precipitate distribution and mechanical properties. 	
1208.5069v2	http://arxiv.org/pdf/1208.5069v2	2012	Failed-Detonation Supernovae: Sub-Luminous Low-Velocity Ia Supernovae   and Their Kicked Remnant White Dwarfs with Iron-Rich Cores	George C. Jordan IV|Hagai B. Perets|Robert T. Fisher|Daniel R. van Rossum	  Type Ia supernovae (SNe Ia) originate from the thermonuclear explosions of carbon-oxygen (C-O) white dwarfs (WDs). The single-degenerate scenario is a well-explored model of SNe Ia where unstable thermonuclear burning initiates in an accreting, Chandrasekhar-mass WD and forms an advancing flame. By several proposed physical processes the rising, burning material triggers a detonation, which subsequently consumes and unbinds the WD. However, if a detonation is not triggered and the deflagration is too weak to unbind the star, a completely different scenario unfolds. We explore the failure of the Gravitationally-Confined Detonation (GCD) mechanism of SNe Ia, and demonstrate through 2D and 3D simulations the properties of failed-detonation SNe. We show that failed-detonation SNe expel a few 0.1 solar masses of burned and partially-burned material and that a fraction of the material falls back onto the WD, polluting the remnant WD with intermediate-mass and iron-group elements, that likely segregate to the core forming an WD whose core is iron rich. The remaining material is asymmetrically ejected at velocities comparable to the escape velocity from the WD, and in response, the WD is kicked to velocities of a few hundred km/s. These kicks may unbind the binary and eject a runaway/hyper-velocity WD. Although the energy and ejected mass of the failed-detonation SN are a fraction of typical thermonuclear SNe, they are likely to appear as sub-luminous low-velocity SNe Ia. Such failed detonations might therefore explain or are related to the observed branch of peculiar SNe Ia, such as the family of low-velocity sub-luminous SNe (SN 2002cx/SN 2008ha-like SNe). 	
1212.0944v4	http://arxiv.org/pdf/1212.0944v4	2013	The quest for a universal density functional: The accuracy of density   functionals across a broad spectrum of databases in chemistry and physics	Roberto Peverati|Donald G. Truhlar	  Kohn-Sham density functional theory is in principle an exact formulation of quantum mechanical electronic structure theory, but in practice we have to rely on approximate exchange-correlation (xc) functionals. The objective of our work has been to design an xc functional with broad accuracy across as wide an expanse of chemistry and physics as possible, leading-as a long-range goal-to a functional with good accuracy for all problems, i.e., a universal functional. To guide our path toward that goal and to measure our progress, we have developed-building on earlier work in our group-a set of databases of reference data for a variety of energetic and structural properties in chemistry and physics. These databases include energies of molecular processes such as atomization, complexation, proton addition, and ionization; they also include molecular geometries and solid-state lattice constants, chemical reaction barrier heights, and cohesive energies and band gaps of solids. For the present paper we gather many of these databases into four comprehensive databases, two with 384 energetic data for chemistry and solid-state physics and another two with 68 structural data for chemistry and solid-state physics, and we test 2 wave function methods and 77 density functionals (12 Minnesota meta functionals and 65 others) in a consistent way across this same broad set of data. We especially highlight the Minnesota density functionals, but the results have broader implications in that one may see the successes and failures of many kinds of density functionals when they are all applied to the same data. Therefore the results provide a status report on the quest for a universal functional. 	
1303.1751v1	http://arxiv.org/pdf/1303.1751v1	2013	Three Layer Hierarchical Model for Chord	Waqas Ahmed Imtiaz|Shimul Shil|A. K. M Mahfuzur Rahamn	  Increasing popularity of decentralized P2P architecture emphasizes on the need to come across an overlay structure that can provide efficient content discovery mechanism, accommodate high churn rate and adapt to failures. Traditional p2p systems are not able to solve the problems relating scalability and high churn rates. Hierarchical model were introduced to provide better fault isolation, effective bandwidth utilization, a superior adaptation to the underlying physical network and a reduction of the lookup path length as additional advantages. It is more efficient and easier to manage than traditional p2p networks. This paper discusses a further step in p2p hierarchy via 3-layers hierarchical model with distributed database architecture in different layer, each of which is connected through its root. The peers are divided into three categories according to their physical stability and strength. They are Ultra Super-peer, Super-peer and Ordinary Peer and we assign these peers to first, second and third level of hierarchy respectively. Peers in a group in lower layer have their own local database which hold as associated super-peer in middle layer and access the database among the peers through user queries. In our 3-layer hierarchical model for DHT algorithms, we used an advanced Chord algorithm with optimized finger table which can remove the redundant entry in the finger table in upper layer that influences the system to reduce the lookup latency. Our research work finally resulted that our model really provides faster search since the network lookup latency is decreased by reducing the number of hops. The peers in such network then can contribute with improve functionality and can perform well in P2P networks. 	
1305.2089v1	http://arxiv.org/pdf/1305.2089v1	2013	Some rigorous results concerning the uniform metallic ground states of   single-band Hamiltonians in arbitrary dimensions	Behnam Farid	  We reproduce and review some of the main results of three of our earlier papers, utilizing in doing so a considerably more transparent formalism than originally utilized. The most fundamental result to which we pay especial attention in this paper, is that the exact Fermi surface (FS) of the uniform metallic ground state (GS) of any single-band Hamiltonian, describing fermions, is a subset of the FS within the framework of the exact Hartree-Fock theory. We also review some of the physical implications of the latter result. Our considerations reveal that the interacting FS of a uniform metallic GS cannot be calculated exactly to order \nu (\nu \ge 2) in the coupling constant \lambda of the interaction potential in terms of the self-energy calculated to order \nu in a non-self-consistent fashion. We show this to be interlinked with the failure of the Luttinger-Ward identity, and thus of the Luttinger theorem, for a self-energy that is not appropriately related to the single-particle Green function from which the FS is deduced. We further show that the same mechanism that embodies the Luttinger theorem within the framework of the exact theory, accounts for a non-trivial dependence of the exact self-energy on \lambda that cannot be captured within a non-self-consistent framework. We thus establish that the extant calculations that purportedly prove deformation of the interacting FS of the metallic GS of the single-band Hubbard Hamiltonian with respect to its Hartree-Fock counterpart at the second order in the on-site interaction energy U, are fundamentally deficient. In an appendix we show that the number-density distribution function, to be distinguished from the site-occupation distribution function, corresponding to the GS of the Hubbard Hamiltonian is not non-interacting v-representable, a fact established earlier numerically. [Abridged Abstract] 	
1305.5876v1	http://arxiv.org/pdf/1305.5876v1	2013	Thermodynamic scaling of dynamics in polymer melts: Predictions from the   generalized entropy theory	Wen-Sheng Xu|Karl F. Freed	  Many glass-forming fluids exhibit a remarkable thermodynamic scaling in which dynamic properties, such as the viscosity, the relaxation time, and the diffusion constant, can be described under different thermodynamic conditions in terms of a unique scaling function of the ratio rho^gamma/T, where rho is the density, T is the temperature, and gamma is a material dependent constant. Given the successes of the generalized entropy theory in elucidating the influence of molecular details on the universal properties of glass-forming polymers, this theory is extended here to investigate the thermodynamic scaling in polymer melts. The predictions of theory are in accord with the appearance of thermodynamic scaling for pressures not in excess of about 50 MPa. (The failure at higher pressures arises due to inherent limitations of a lattice model.) In line with arguments relating the magnitude of gamma to the steepness of the repulsive part of the intermolecular potential, the abrupt, square-well nature of the lattice model interactions lead, as expected, to much larger values of the scaling exponent. Nevertheless, the theory is employed to study how individual molecular parameters affect the scaling exponent in order to extract a molecular understanding of the information content contained in the exponent. The chain rigidity, cohesive energy, chain length, and the side group length are all found to significantly affect the magnitude of the scaling exponent, and the computed trends agree well with available experiments. The variations of gamma with these molecular parameters are explained by establishing a correlation between the computed molecular dependence of the scaling exponent and the fragility. Thus, the efficiency of packing the polymers is established as the universal physical mechanism determining both the fragility and the scaling exponent gamma. 	
1306.6428v1	http://arxiv.org/pdf/1306.6428v1	2013	Internet Control Plane Event Identification using Model Based Change   Point Detection Techniques	S. P. Meenakshi|S. V. Raghavan	  In the raise of many global organizations deploying their data centers and content services in India, the prefix reachability performance study from global destinations garners our attention. The events such as failures and attacks occurring in the Internet topology have impact on Autonomous System (AS) paths announced in the control plane and reachability of prefixes from spatially distributed ASes. As a consequence the customer reachability to the services in terms of increased latency and outages for a short or long time are experienced. The challenge in control plane event detection is when the data plane traffic is able to reach the intended destinations correctly. However detection of such events are crucial for the operations of content and data center industries. By monitoring the spatially distributed routing table features like AS path length distributions, spatial prefix reachability distribution and covering to overlap route ratio, we can detect the control plane events. In our work, we study prefix AS paths from the publicly available route-view data and analyze the global reachability as well as reachability to Indian AS topology. To capture the spatial events in a single temporal pattern, we propose a counting based measure using prefixes announced by x % of spatial peers. Employing statistical characteristics change point detection and temporal aberration algorithm on the time series of the proposed measure, we identify the occurrence of long and stochastic control plane events. The impact and duration of the events are also quantified. We validate the mechanisms over the proposed measure using the SEA-Me-We4 cable cut event manifestations in the control plane of Indian AS topology. The cable cut events occurred on 6th June 2012 (long term event) and 17th April 2012 (stochastic event) are considered for validation. 	
1309.6215v2	http://arxiv.org/pdf/1309.6215v2	2013	Design of a horizontal neutron reflectometer for the European Spallation   Source	D. Nekrassov|M. Trapp|K. Lieutenant|J. -F. Moulin|M. Strobl|R. Steitz	  A design study of a horizontal neutron reflectometer adapted to the general baseline of the long pulse European Spallation Source (ESS) is presented. The instrument layout comprises solutions for the neutron guide, high-resolution pulse shaping and beam bending onto a sample surface being so far unique in the field of reflectometry. The length of this instrument is roughly 55 m, enabling $\delta \lambda / \lambda$ resolutions from 0.5% to 10%. The incident beam is focussed in horizontal plane to boost measurements of sample sizes of 1*1 cm{^2} and smaller with potential beam deflection in both downward and upward direction. The range of neutron wavelengths untilized by the instrument is 2 to 7.1 (12.2, ...) {\AA}, if every (second, ...) neutron source ulse is used. Angles of incidence can be set between 0{\deg} and 9{\deg} with a total accessible q-range from 4*10^{-3} {\AA}^{-1} up to 1 {\AA}^{-1}. The instrument operates both in {\theta}/{\theta} (free liquid surfaces) and {\theta}/2{\theta} (solid/liquid, air/solid interfaces) geometry. The experimental setup will in particular enable direct studies on ultrathin films (d ~ 10 {\AA}) and buried monolayers to multilayered structures of up to 3000 {\AA} total thickness. The horizontal reflectometer will further foster investigations of hierarchical systems from nanometer to micrometer length scale, as well as their kinetics and dynamical properties, in particular under load (shear, pressure, external fields). Polarization and polarization analysis as well as the GISANS option are designed as potential modules to be implemented separately in the generic instrument layout. The instrument is highly flexible and offers a variety of different measurement modes. With respect to its mechanical components the instrument is exclusively based on current technology. Risks of failure for the chosen setup are minimum. 	
1310.3264v2	http://arxiv.org/pdf/1310.3264v2	2014	Neutron spectroscopic study of crystal field excitations in Tb2Ti2O7 and   Tb2Sn2O7	J. Zhang|K. Fritsch|Z. Hao|B. V. Bagheri|M. J. P. Gingras|G. E. Granroth|P. Jiramongkolchai|R. J. Cava|B. D. Gaulin	  We present time-of-flight inelastic neutron scattering measurements at low temperature on powder samples of the magnetic pyrochlore oxides Tb2Ti2O7 and Tb2Sn2O7. These two materials possess related, but different ground states, with Tb2Sn2O7 displaying "soft" spin ice order below Tn~0.87 K, while Tb2Ti2O7 enters a hybrid, glassy spin ice state below Tg~0.2 K. Our neutron measurements, performed at T=1.5 K and 30 K, probe the crystal field states associated with the J=6 states of Tb3+ within the appropriate Fd\bar{3}m pyrochlore environment. These crystal field states determine the size and anisotropy of the Tb3+ magnetic moment in each material's ground state, information that is an essential starting point for any description of the low-temperature phase behavior and spin dynamics in Tb2Ti2O7 and Tb2Sn2O7. While these two materials have much in common, the cubic stanate lattice is expanded compared to the cubic titanate lattice. As our measurements show, this translates into a factor of ~2 increase in the crystal field bandwidth of the 2J+1=13 states in Tb2Ti2O7 compared with Tb2Sn2O7. Our results are consistent with previous measurements on crystal field states in Tb2Sn2O7, wherein the ground state doublet corresponds primarily to m_J=|\pm 5> and the first excited state doublet to mJ=|\pm 4>. In contrast, our results on Tb2Ti2O7 differ markedly from earlier studies, showing that the ground state doublet corresponds to a significant mixture of mJ=|\pm 5>, |\mp 4>, and |\pm 2>, while the first excited state doublet corresponds to a mixture of mJ=|\pm 4>, |\mp 5>, and |\pm 1>. We discuss these results in the context of proposed mechanisms for the failure of Tb2Ti2O7 to develop conventional long-range order down to 50 mK. 	
1312.7305v3	http://arxiv.org/pdf/1312.7305v3	2015	Probabilistic Computability and Choice	Vasco Brattka|Guido Gherardi|Rupert Hölzl	  We study the computational power of randomized computations on infinite objects, such as real numbers. In particular, we introduce the concept of a Las Vegas computable multi-valued function, which is a function that can be computed on a probabilistic Turing machine that receives a random binary sequence as auxiliary input. The machine can take advantage of this random sequence, but it always has to produce a correct result or to stop the computation after finite time if the random advice is not successful. With positive probability the random advice has to be successful. We characterize the class of Las Vegas computable functions in the Weihrauch lattice with the help of probabilistic choice principles and Weak Weak K\H{o}nig's Lemma. Among other things we prove an Independent Choice Theorem that implies that Las Vegas computable functions are closed under composition. In a case study we show that Nash equilibria are Las Vegas computable, while zeros of continuous functions with sign changes cannot be computed on Las Vegas machines. However, we show that the latter problem admits randomized algorithms with weaker failure recognition mechanisms. The last mentioned results can be interpreted such that the Intermediate Value Theorem is reducible to the jump of Weak Weak K\H{o}nig's Lemma, but not to Weak Weak K\H{o}nig's Lemma itself. These examples also demonstrate that Las Vegas computable functions form a proper superclass of the class of computable functions and a proper subclass of the class of non-deterministically computable functions. We also study the impact of specific lower bounds on the success probabilities, which leads to a strict hierarchy of classes. In particular, the classical technique of probability amplification fails for computations on infinite objects. We also investigate the dependency on the underlying probability space. 	
1402.1874v3	http://arxiv.org/pdf/1402.1874v3	2014	A dynamo model of magnetic activity in solar-like stars with different   rotational velocities	Bidya Binay Karak|Leonid L. Kitchatinov|Arnab Rai Choudhuri	  We attempt to provide a quantitative theoretical explanation for the observations that Ca II H/K emission and X-ray emission from solar-like stars increase with decreasing Rossby number (i.e., with faster rotation). Assuming that these emissions are caused by magnetic cycles similar to the sunspot cycle, we construct flux transport dynamo models of $1M_{\odot}$ stars rotating with different rotation periods. We first compute the differential rotation and the meridional circulation inside these stars from a mean-field hydrodynamics model. Then these are substituted in our dynamo code to produce periodic solutions. We find that the dimensionless amplitude $f_m$ of the toroidal flux through the star increases with decreasing rotation period. The observational data can be matched if we assume the emissions to go as the power 3-4 of $f_m$. Assuming that the Babcock-Leighton mechanism saturates with increasing rotation, we can provide an explanation for the observed saturation of emission at low Rossby numbers. The main failure of our model is that it predicts an increase of magnetic cycle period with increasing rotation rate, which is the opposite of what is found observationally. Much of our calculations are based on the assumption that the magnetic buoyancy makes the magnetic flux tubes to rise radially from the bottom of the convection zone. On taking account of the fact that the Coriolis force diverts the magnetic flux tubes to rise parallel to the rotation axis in rapidly rotating stars, the results do not change qualitatively. 	
1407.6167v5	http://arxiv.org/pdf/1407.6167v5	2016	Memory of jamming - multiscale models for soft and granular matter	Nishant Kumar|Stefan Luding	  Soft, disordered, micro-structured materials are ubiquitous in nature and industry, and are different from ordinary fluids or solids, with unusual, interesting static and flow properties. The transition from fluid to solid -at the so-called jamming density- features a multitude of complex mechanisms, but there is no unified theoretical framework that explains them all. In this study, a simple yet quantitative and predictive model is presented, which allows for a variable, changing jamming density, encompassing the memory of the deformation history and explaining a multitude of phenomena at and around jamming. The jamming density, now introduced as a new state-variable, changes due to the deformation history and relates the system's macroscopic response to its microstructure. The packing efficiency can increase logarithmically slow under gentle repeated (isotropic) compression, leading to an increase of the jamming density. In contrast, shear deformations cause anisotropy, changing the packing efficiency exponentially fast with either dilatancy or compactancy. The memory of the system near jamming can be explained by a microstatistical model that involves a multiscale, fractal energy landscape and links the microscopic particle picture to the macroscopic continuum description, providing a unified explanation for the qualitatively different flow-behavior for different deformation modes. To complement our work, a recipe to extract the history-dependent jamming density from experimentally accessible data is proposed, and alternative state-variables are compared. The proposed simple macroscopic constitutive model is calibrated with the memory of microstructure. Such approach can help understanding predicting and mitigating failure of structures or geophysical hazards, and will bring forward industrial process design/optimization, and help solving scientific challenges in fundamental research. 	
1505.01378v2	http://arxiv.org/pdf/1505.01378v2	2016	Efficient exploration of multiplex networks	Federico Battiston|Vincenzo Nicosia|Vito Latora	  Efficient techniques to navigate networks with local information are fundamental to sample large-scale online social systems and to retrieve resources in peer-to-peer systems. Biased random walks, i.e. walks whose motion is biased on properties of neighbouring nodes, have been largely exploited to design smart local strategies to explore a network, for instance by constructing maximally mixing trajectories or by allowing an almost uniform sampling of the nodes. Here we introduce and study biased random walks on multiplex networks, graphs where the nodes are related through different types of links organised in distinct and interacting layers, and we provide analytical solutions for their long-time properties, including the stationary occupation probability distribution and the entropy rate. We focus on degree-biased random walks and distinguish between two classes of walks, namely those whose transition probability depends on a number of parameters which is extensive in the number of layers, and those whose motion depends on intrinsically multiplex properties of the neighbouring nodes. We analyse the effect of the structure of the multiplex network on the steady-state behaviour of the walkers, and we find that heterogeneous degree distributions as well as the presence of inter-layer degree correlations and edge overlap determine the extent to which a multiplex can be efficiently explored by a biased walk. Finally we show that, in real-world multiplex transportation networks, the trade-off between efficient navigation and resilience to link failure has resulted into systems whose diffusion properties are qualitatively different from those of appropriately randomised multiplex graphs. This fact suggests that multiplexity is an important ingredient to include in the modelling of real-world systems. 	
1505.04726v2	http://arxiv.org/pdf/1505.04726v2	2015	Angelic Processes	Pedro Ribeiro	  In the formal modelling of systems, demonic and angelic nondeterminism play fundamental roles as abstraction mechanisms. The angelic nature of a choice pertains to the property of avoiding failure whenever possible. As a concept, angelic choice first appeared in automata theory and Turing machines, where it can be implemented via backtracking. It has traditionally been studied in the refinement calculus, and has proved to be useful in a variety of applications and refinement techniques. Recently it has been studied within relational, multirelational and higher-order models. It has been employed for modelling user interactions, game-like scenarios, theorem proving tactics, constraint satisfaction problems and control systems.   When the formal modelling of state-rich reactive systems is considered, it only seems natural that both types of nondeterministic choice should be considered. However, despite several treatments of angelic nondeterminism in the context of process algebras, namely Communicating Sequential Processes, the counterpart to the angelic choice of the refinement calculus has been elusive.   In this thesis, we develop a semantics in the relational setting of Hoare and He's Unifying Theories of Programming that enables the characterisation of angelic nondeterminism in CSP. Since CSP processes are given semantics in the UTP via designs, that is, pre and postcondition pairs, we first introduce a theory of angelic designs, and an isomorphic multirelational model, that is suitable for characterising processes. We then develop a theory of reactive angelic designs by enforcing the healthiness conditions of CSP. Finally, by introducing a notion of divergence that can undo the history of events, we obtain a model where angelic choice avoids divergence. 	
1506.05693v1	http://arxiv.org/pdf/1506.05693v1	2015	A multipath energy-conserving routing protocol for wireless ad hoc   networks lifetime improvement	Omar Smail|Bernard Cousin|Zoulikha Mekkakia|Rachida Mekki	  Ad hoc networks are wireless mobile networks that can operate without infrastructure and without centralized network management. Traditional techniques of routing are not well adapted. Indeed, their lack of reactivity with respect to the variability of network changes makes them difficult to use. Moreover, conserving energy is a critical concern in the design of routing protocols for ad hoc networks, because most mobile nodes operate with limited battery capacity, and the energy depletion of a node affects not only the node itself but also the overall network lifetime. In all proposed single-path routing schemes a new path-discovery process is required once a path failure is detected, and this process causes delay and wastage of node resources. A multipath routing scheme is an alternative to maximize the network lifetime. In this paper, we propose an energy-efficient multipath routing protocol, called AOMR-LM (Ad hoc On-demand Multipath Routing with Lifetime Maximization), which preserves the residual energy of nodes and balances the consumed energy to increase the network lifetime. To achieve this goal, we used the residual energy of nodes for calculating the node energy level. The multipath selection mechanism uses this energy level to classify the paths. Two parameters are analyzed: the energy threshold beta and the coefficient alpha. These parameters are required to classify the nodes and to ensure the preservation of node energy. Our protocol improves the performance of mobile ad hoc networks by prolonging the lifetime of the network. This novel protocol has been compared with other protocols: AOMDV and ZD-AOMDV. The protocol performance has been evaluated in terms of network lifetime, energy consumption, and end-to-end delay. 	
1506.06856v3	http://arxiv.org/pdf/1506.06856v3	2016	Semi-phenomenological analysis of neutron scattering results for   quasi-two dimensional quantum anti-ferromagnet	Subhajit Sarkar|Ranjan Chaudhury|Samir K. Paul	  The available results from the inelastic neutron scattering experiment performed on the quasi-two dimensional spin $\frac{1}{2}$ anti-ferromagnetic material $La_2 Cu O_4$ have been analysed theoretically. The formalism of ours is based on a semi-classical like treatment involving a model of an ideal gas of mobile vortices and anti-vortices built on the background of the N$\acute{e}$el state, using the bipartite classical spin configuration corresponding to an XY- anisotropic Heisenberg anti-ferromagnet on a square lattice. The results for the integrated intensities for our spin $\frac{1}{2}$ model corresponding to different temperatures, show occurrence of vigorous unphysical oscillations, when convoluted with a realistic spectral window function. These results indicate failure of the conventional semi-classical theoretical model of ideal vortex/anti-vortex gas arising in the Berezinskii-Kosterlitz-Thouless theory for the low spin magnetic systems. A full fledged quantum mechanical formalism and calculations seem crucial for the understanding of topological excitations in such low spin systems. Furthermore, a severe disagreement is found to occur at finite values of energy transfer between the integrated intensities obtained theoretically from the conventional formalism and those obtained experimentally. This further suggests strongly that the full quantum treatment should also incorporate the interaction between the fragile-magnons and the topological excitations. This is quite plausible in view of the recent work establishing such a process in XXZ quantum ferromagnet on 2D lattice. The high spin XXZ quasi-two dimensional antiferromagnet like $MnPS_3$ however follows the conventional theory quite well 	
1507.07915v4	http://arxiv.org/pdf/1507.07915v4	2016	Memory-preserving equilibration after a quantum quench in a 1d critical   model	Spyros Sotiriadis	  One of the fundamental principles of statistical physics is that only partial information about a system's state is required for its macroscopic description. This is not only true for thermal ensembles, but also for the unconventional ensemble, known as Generalized Gibbs Ensemble (GGE), that is expected to describe the relaxation of integrable systems after a quantum quench. By analytically studying the quench dynamics in a prototypical one-dimensional critical model, the massless free bosonic field theory, we find evidence of a novel type of equilibration characterized by the preservation of an enormous amount of memory of the initial state that is accessible by local measurements. In particular, we show that the equilibration retains memory of non-Gaussian initial correlations, in contrast to the case of massive free evolution which erases all such memory. The GGE in its standard form, being a Gaussian ensemble, fails to predict correctly the equilibrium values of local observables, unless the initial state is Gaussian itself. Our findings show that the equilibration of a broad class of quenches whose evolution is described by Luttinger liquid theory with an initial state that is non-Gaussian in terms of the bosonic field, is not correctly captured by the corresponding bosonic GGE, raising doubts about the validity of the latter in general one-dimensional gapless integrable systems such as the Lieb-Liniger model. We also propose that the same experiment by which the GGE was recently observed [Langen et al., Science 348 (2015) 207-211] can also be used to observe its failure, simply by starting from a non-Gaussian initial state. 	
1606.02714v2	http://arxiv.org/pdf/1606.02714v2	2017	Simulating the dust content of galaxies: successes and failures	Ryan McKinnon|Paul Torrey|Mark Vogelsberger|Christopher C. Hayward|Federico Marinacci	  We present full volume cosmological simulations using the moving-mesh code AREPO to study the coevolution of dust and galaxies. We extend the dust model in AREPO to include thermal sputtering of grains and investigate the evolution of the dust mass function, the cosmic distribution of dust beyond the interstellar medium, and the dependence of dust-to-stellar mass ratio on galactic properties. The simulated dust mass function is well-described by a Schechter fit and lies closest to observations at $z = 0$. The radial scaling of projected dust surface density out to distances of $10 \, \text{Mpc}$ around galaxies with magnitudes $17 < i < 21$ is similar to that seen in Sloan Digital Sky Survey data, albeit with a lower normalisation. At $z = 0$, the predicted dust density of $\Omega_\text{dust} \approx 1.3 \times 10^{-6}$ lies in the range of $\Omega_\text{dust}$ values seen in low-redshift observations. We find that dust-to-stellar mass ratio anti-correlates with stellar mass for galaxies living along the star formation main sequence. Moreover, we estimate the $850 \, \mu\text{m}$ number density functions for simulated galaxies and analyse the relation between dust-to-stellar flux and mass ratios at $z = 0$. At high redshift, our model fails to produce enough dust-rich galaxies, and this tension is not alleviated by adopting a top-heavy initial mass function. We do not capture a decline in $\Omega_\text{dust}$ from $z = 2$ to $z = 0$, which suggests that dust production mechanisms more strongly dependent on star formation may help to produce the observed number of dusty galaxies near the peak of cosmic star formation. 	
1608.06342v1	http://arxiv.org/pdf/1608.06342v1	2016	The Chemical Abundance Structure of the Inner Milky Way: A Signature of   "Upside-Down" Disk Formation?	Jenna K. C. Freudenburg|David H. Weinberg|Michael R. Hayden|Jon A. Holtzman	  We present a model for the [alpha/Fe]-[Fe/H] distribution of stars in the inner Galaxy, R=3-5 kpc, measured as a function of vertical distance |z| from the midplane by Hayden et al. (2015, H15). Motivated by an "upside-down" scenario for thick disk formation, in which the thickness of the star-forming gas layer contracts as the stellar mass of the disk grows, we combine one-zone chemical evolution with a simple prescription in which the scale-height of the stellar distribution drops linearly from z_h=0.8 kpc to z_h=0.2 kpc over a timescale t_c, remaining constant thereafter. We assume a linear-exponential star-formation history, SFR ~ te^{-t/t_sf}. With a star-formation efficiency timescale of 2 Gyr, an outflow mass-loading factor of 1.5, t_sf=3 Gyr, and t_c=2.5 Gyr, the model reproduces the observed locus of inner disk stars in [alpha/Fe]-[Fe/H] and the metallicity distribution functions (MDFs) measured by H15 at |z|=0-0.5 kpc, 0.5-1 kpc, and 1-2 kpc. Substantial changes to model parameters lead to disagreement with the H15 data; for example, models with t_c=1 Gyr or t_sf=1 Gyr fail to match the observed MDF at high-|z| and low-|z|, respectively. The inferred scale-height evolution, with z_h(t) dropping on a timescale t_c ~ t_sf at large lookback times, favors upside-down formation over dynamical heating of an initially thin stellar population as the primary mechanism regulating disk thickness. The failure of our short-t_c models suggests that any model in which thick disk formation is a discrete event will not reproduce the continuous dependence of the MDF on |z| found by H15. Our scenario for the evolution of the inner disk can be tested by future measurements of the |z|-distribution and the age-metallicity distribution at R=3-5 kpc. 	
1609.04971v2	http://arxiv.org/pdf/1609.04971v2	2016	Adhesion and non-linear rheology of adhesives with supramolecular   crosslinking points	X Callies|C Fonteneau|S Pensec|L Bouteiller|G Ducouret|C Creton	  Soft supramolecular materials are promising for the design of innovative and highly tunable adhesives. These materials are composed of polymer chains functionalized by strongly interacting moieties, sometimes called "stickers". In order to systematically investigate the effect of the presence of associative groups on the debonding properties of a supramolecular adhesive, a series of supramolecular model systems has been characterized by probe-tack tests. These model materials, composed of linear and low dispersity poly(butylacrylate) chains functionalized in the middle by a single tri-urea sticker, are able to self-associate by six hydrogen bonds and range in molecular weight (M n) between 5 and 85 kg/mol. The linear rheology and the nanostructure of the same materials (called "PnBA3U") was the object of a previous study 1,2. At room temperature, the association of polymers via hydrogen bonds induces the formation of rod-like aggregates structured into bundles for M n \textless{} 40kg/mol and the behavior of a soft elastic material was observed (G'\textgreater{}\textgreater{}G "and G'~$\omega$ 0). For higher M n , the filaments were randomly oriented and polymers displayed a crossover towards viscous behavior although terminal relaxation was not reached in the experimental frequency window. All these materials show however similar adhesive properties characterized by a cohesive mode of failure and low debonding energies (W adh \textless{}40J/m 2 for a debonding speed of 100$\mu$m/s). The debonding mechanisms observed during the adhesion tests have been investigated in detail with an Image tools analysis developed by our group 3. The measure of the projected area covered by cavities growing in the adhesive layer during debonding can be used to estimate the true stress in the walls of the cavities and thus, to characterize the in-situ large strain deformation of the thin layer during the adhesion test itself. This analysis revealed in particular that the PnBA3U materials with M n \textless{} 40 kg/mol soften very markedly at large deformation like yield stress fluids, explaining the low adhesion energies measured for these viscoelastic gels. 2 	
1609.07246v1	http://arxiv.org/pdf/1609.07246v1	2016	Ageing underdamped scaled Brownian motion: ensemble and time averaged   particle displacements, non-ergodicity, and the failure of the overdamping   approximation	H. Safdari|A. G. Cherstvy|A. V. Chechkin|A. Bodrova|R. Metzler	  We investigate both analytically and by computer simulations the ensemble averaged, time averaged, non-ergodic, and ageing properties of massive particles diffusing in a medium with a time dependent diffusivity. We call this stochastic diffusion process the (ageing) underdamped scaled Brownian motion (UDSBM). We demonstrate how the mean squared displacement (MSD) and the time averaged MSD of UDSBM are affected by the inertial term in the Langevin equation, both at short, intermediate, and even long diffusion times. In particular, we quantify the ballistic regime for the MSD and the time averaged MSD as well as the spread of individual time averaged MSD trajectories. One of the main effects we observe is that---both for the MSD and the time averaged MSD---for superdiffusive UDSBM the ballistic regime is much shorter than for ordinary Brownian motion. In contrast, for subdiffusive UDSBM the ballistic region extends to much longer diffusion times. Therefore, particular care needs to be taken when the overdamped limit indeed provides a correct description, even in the long time limit. We also analyze to what extent ergodicity in the Boltzmann-Khinchin sense in this non-stationary system is broken, both for subdiffusive and superdiffusive UDSBM. Finally, the limiting case of ultraslow UDSBM is considered, with a mixed logarithmic and power law dependence of the ensemble and time averaged MSDs of the particles. In the limit of strong ageing, remarkably, the ordinary UDSBM and the ultraslow UDSBM behave similarly in the short time ballistic limit. The approaches developed here open new ways for considering other stochastic processes under physically important conditions when a finite particle mass and ageing in the system cannot be neglected. 	
1611.01751v1	http://arxiv.org/pdf/1611.01751v1	2016	Deep Convolutional Neural Network Features and the Original Image	Connor J. Parde|Carlos Castillo|Matthew Q. Hill|Y. Ivette Colon|Swami Sankaranarayanan|Jun-Cheng Chen|Alice J. O'Toole	  Face recognition algorithms based on deep convolutional neural networks (DCNNs) have made progress on the task of recognizing faces in unconstrained viewing conditions. These networks operate with compact feature-based face representations derived from learning a very large number of face images. While the learned features produced by DCNNs can be highly robust to changes in viewpoint, illumination, and appearance, little is known about the nature of the face code that emerges at the top level of such networks. We analyzed the DCNN features produced by two face recognition algorithms. In the first set of experiments we used the top-level features from the DCNNs as input into linear classifiers aimed at predicting metadata about the images. The results show that the DCNN features contain surprisingly accurate information about the yaw and pitch of a face, and about whether the face came from a still image or a video frame. In the second set of experiments, we measured the extent to which individual DCNN features operated in a view-dependent or view-invariant manner. We found that view-dependent coding was a characteristic of the identities rather than the DCNN features - with some identities coded consistently in a view-dependent way and others in a view-independent way. In our third analysis, we visualized the DCNN feature space for over 24,000 images of 500 identities. Images in the center of the space were uniformly of low quality (e.g., extreme views, face occlusion, low resolution). Image quality increased monotonically as a function of distance from the origin. This result suggests that image quality information is available in the DCNN features, such that consistently average feature values reflect coding failures that reliably indicate poor or unusable images. Combined, the results offer insight into the coding mechanisms that support robust representation of faces in DCNNs. 	
1701.03018v2	http://arxiv.org/pdf/1701.03018v2	2017	Spontaneous flux concentrations from the negative effective magnetic   pressure instability beneath a radiative stellar surface	Barbara Perri|Axel Brandenburg	  The formation of sunspots requires the concentration of magnetic flux near the surface. The negative magnetic pressure instability (NEMPI) might be a possible mechanism for accomplishing this, but it has mainly been studied in simple systems using an isothermal equation of state without a natural free surface. We study NEMPI in a stratified Cartesian mean-field model where turbulence effects are parameterized. We use an ideal equation of state and include radiation transport, which establishes selfconsistently a free surface. We use a Kramers-type opacity with adjustable exponents chosen such that the deeper layers are approximately isentropic. No convection is therefore possible in this model, allowing us to study NEMPI with radiation in isolation. We restrict ourselves to two-dimensional models. We use artificially enhanced mean-field coefficients to allow NEMPI to develop, making it therefore possible to study the reason why it is much harder to excite in the presence of radiation. NEMPI yields moderately strong magnetic flux concentrations a certain distance beneath the surface where optical depth is unity. The instability is oscillatory and in the form of upward travelling waves. This seems to be a new effect that has not been found in earlier models without radiative transport. The horizontal wavelength is about ten times smaller than what has been found previously in more idealized isothermal models. In our models, NEMPI saturates at field strengths too low to explain sunspots. Furthermore, the structures appear too narrow and too far beneath the surface to cause significant brightness variations at the radiative surface. We speculate that the failure to reproduce effects resembling sunspots may be related to the neglect of convection. 	
1701.05125v2	http://arxiv.org/pdf/1701.05125v2	2017	Caching Meets Millimeter Wave Communications for Enhanced Mobility   Management in 5G Networks	Omid Semiari|Walid Saad|Mehdi Bennis|Behrouz Maham	  One of the most promising approaches to overcome the uncertainty and dynamic channel variations of millimeter wave (mmW) communications is to deploy dual-mode base stations that integrate both mmW and microwave ($\mu$W) frequencies. If properly designed, such dual-mode base stations can enhance mobility and handover in highly mobile wireless environments. In this paper, a novel approach for analyzing and managing mobility in joint $\mu$W-mmW networks is proposed. The proposed approach leverages device-level caching along with the capabilities of dual-mode base stations to minimize handover failures, reduce inter-frequency measurement energy consumption, and provide seamless mobility in emerging dense heterogeneous networks. First, fundamental results on the caching capabilities, including caching probability and cache duration are derived for the proposed dual-mode network scenario. Second, the average achievable rate of caching is derived for mobile users. Third, the proposed cache-enabled mobility management problem is formulated as a dynamic matching game between mobile user equipments (MUEs) and small base stations (SBSs). The goal of this game is to find a distributed handover mechanism that subject to the network constraints on HOFs and limited cache sizes, allows each MUE to choose between executing an HO to a target SBS, being connected to the macrocell base station (MBS), or perform a transparent HO by using the cached content. The formulated matching game allows capturing the dynamics of the mobility management problem caused by HOFs. To solve this dynamic matching problem, a novel algorithm is proposed and its convergence to a two-sided dynamically stable HO policy is proved. Numerical results corroborate the analytical derivations and show that the proposed solution will provides significant reductions in both the HOF and energy consumption by MUEs. 	
1708.03402v2	http://arxiv.org/pdf/1708.03402v2	2017	Product Matrix MSR Codes with Bandwidth Adaptive Exact Repair	Kaveh Mahdaviani|Soheil Mohajer|Ashish Khisti	  In a distributed storage systems (DSS) with $k$ systematic nodes, robustness against node failure is commonly provided by storing redundancy in a number of other nodes and performing repair mechanism to reproduce the content of the failed nodes. Efficiency is then achieved by minimizing the storage overhead and the amount of data transmission required for data reconstruction and repair, provided by coding solutions such as regenerating codes [1]. Common explicit regenerating code constructions enable efficient repair through accessing a predefined number, $d$, of arbitrary chosen available nodes, namely helpers. In practice, however, the state of the system dynamically changes based on the request load, the link traffic, etc., and the parameters which optimize system's performance vary accordingly. It is then desirable to have coding schemes which are able to operate optimally under a range of different parameters simultaneously. Specifically, adaptivity in the number of helper nodes for repair is of interest. While robustness requires capability of performing repair with small number of helpers, it is desirable to use as many helpers as available to reduce the transmission delay and total repair traffic.   In this work we focus on the minimum storage regenerating (MSR) codes, where each node is supposed to store $\alpha$ information units, and the source data of size $k\alpha$ could be recovered from any arbitrary set of $k$ nodes. We introduce a class MSR codes that realize optimal repair bandwidth simultaneously with a set of different choices for the number of helpers, namely $D=\{d_{1}, \cdots, d_{\delta}\}$. Our coding scheme follows the Product Matrix (PM) framework introduced in [2], and could be considered as a generalization of the PM MSR code presented in [2], such that any $d_{i} = (i+1)(k-1)$ helpers can perform an optimal repair. ... 	
1710.08676v1	http://arxiv.org/pdf/1710.08676v1	2017	Normal behaviour models for wind turbine vibrations: An alternative   approach	Pedro G. Lind|Luis Vera-Tudela|Matthias Wächter|Martin Kühn|Joachim Peinke	  The identification of abnormal behaviour in mechanical systems is key to anticipate and avoid their potential failure. Thus wind turbine health is commonly assessed monitoring series of $10$-minute SCADA and high frequency data from sensors. To monitor wind turbine vibrations, normal behaviour models are built to predict tower top accelerations and drive-train vibrations. Signal deviations from model prediction are labelled as anomalies and are further investigated. More efficient models are expected to help enhancing the identification of abnormal behaviour. In this paper we assess a stochastic approach to reconstruct the $1$ Hz tower top acceleration signal, which was measured in a wind turbine located at the wind farm Alpha Ventus in the German North Sea. We compare the resulting data reconstruction with that of a model based on a neural network, which has been previously reported as a data-mining algorithm suitable for reconstructing this signal. In order to focus the discussion on the similarities and differences of both approaches, we limit our evaluation to a single input-output system; in doing so, we avoid other differences, i.e. loading type, pre-processing or model complexity. Our results present evidence that the stochastic approach outperforms the neural network in the high frequency domain ($1$ Hz). Although neural network retrieve accurate step-forward predictions, with low mean square errors, the stochastic approach predictions better preserve the statistics and the frequency components of the original signal, remaining high accuracy levels. The implementation of our stochastic approach is available as open source code and can easily be adapted for other situations involving stochastic data reconstruction. Based on our findings we argue that such an approach could be implemented in signal reconstruction for monitoring purposes or for abnormal behaviour detection. 	
1711.00111v1	http://arxiv.org/pdf/1711.00111v1	2017	Multi-Task Learning by Deep Collaboration and Application in Facial   Landmark Detection	Ludovic Trottier|Philippe Giguère|Brahim Chaib-draa	  Convolutional neural networks (CNN) have become the most successful and popular approach in many vision-related domains. While CNNs are particularly well-suited for capturing a proper hierarchy of concepts from real-world images, they are limited to domains where data is abundant. Recent attempts have looked into mitigating this data scarcity problem by casting their original single-task problem into a new multi-task learning (MTL) problem. The main goal of this inductive transfer mechanism is to leverage domain-specific information from related tasks, in order to improve generalization on the main task. While recent results in the deep learning (DL) community have shown the promising potential of training task-specific CNNs in a soft parameter sharing framework, integrating the recent DL advances for improving knowledge sharing is still an open problem. In this paper, we propose the Deep Collaboration Network (DCNet), a novel approach for connecting task-specific CNNs in a MTL framework. We define connectivity in terms of two distinct non-linear transformations. One aggregates task-specific features into global features, while the other merges back the global features with each task-specific network. Based on the observation that task relevance depends on depth, our transformations use skip connections as suggested by residual networks, to more easily deactivate unrelated task-dependent features. To validate our approach, we employ facial landmark detection (FLD) datasets as they are readily amenable to MTL, given the number of tasks they include. Experimental results show that we can achieve up to 24.31% relative improvement in failure rate over other state-of-the-art MTL approaches. We finally perform an ablation study showing that our approach effectively allows knowledge sharing, by leveraging domain-specific features at particular depths from tasks that we know are related. 	
1301.0533v1	http://arxiv.org/pdf/1301.0533v1	2013	A robust Bayesian approach to modelling epistemic uncertainty in   common-cause failure models	Matthias C. M. Troffaes|Gero Walter|Dana Kelly	  In a standard Bayesian approach to the alpha-factor model for common-cause failure, a precise Dirichlet prior distribution models epistemic uncertainty in the alpha-factors. This Dirichlet prior is then updated with observed data to obtain a posterior distribution, which forms the basis for further inferences.   In this paper, we adapt the imprecise Dirichlet model of Walley to represent epistemic uncertainty in the alpha-factors. In this approach, epistemic uncertainty is expressed more cautiously via lower and upper expectations for each alpha-factor, along with a learning parameter which determines how quickly the model learns from observed data. For this application, we focus on elicitation of the learning parameter, and find that values in the range of 1 to 10 seem reasonable. The approach is compared with Kelly and Atwood's minimally informative Dirichlet prior for the alpha-factor model, which incorporated precise mean values for the alpha-factors, but which was otherwise quite diffuse.   Next, we explore the use of a set of Gamma priors to model epistemic uncertainty in the marginal failure rate, expressed via a lower and upper expectation for this rate, again along with a learning parameter. As zero counts are generally less of an issue here, we find that the choice of this learning parameter is less crucial.   Finally, we demonstrate how both epistemic uncertainty models can be combined to arrive at lower and upper expectations for all common-cause failure rates. Thereby, we effectively provide a full sensitivity analysis of common-cause failure rates, properly reflecting epistemic uncertainty of the analyst on all levels of the common-cause failure model. 	
1604.01666v1	http://arxiv.org/pdf/1604.01666v1	2016	Record breaking bursts during the compressive failure of porous   materials	Gergo Pal|Frank Raischel|Sabine Lennartz-Sassinek|Ferenc Kun|Ian G. Main	  An accurate understanding of the interplay between random and deterministic processes in generating extreme events is of critical importance in many fields, from forecasting extreme meteorological events to the catastrophic failure of materials and in the Earth. Here we investigate the statistics of record-breaking events in the time series of crackling noise generated by local rupture events during the compressive failure of porous materials. The events are generated by computer simulations of the uni-axial compression of cylindrical samples in a discrete element model of sedimentary rocks that closely resemble those of real experiments. The number of records grows initially as a decelerating power law of the number of events, followed by an acceleration immediately prior to failure. We demonstrate the existence of a characteristic record rank k^* which separates the two regimes of the time evolution. Up to this rank deceleration occurs due to the effect of random disorder. Record breaking then accelerates towards macroscopic failure, when physical interactions leading to spatial and temporal correlations dominate the location and timing of local ruptures. Sub-sequences of bursts between consecutive records are characterized by a power law size distribution with an exponent which decreases as failure is approached. High rank records are preceded by bursts of increasing size and waiting time between consecutive events and they are followed by a relaxation process. As a reference, surrogate time series are generated by reshuffling the crackling bursts. The record statistics of the uncorrelated surrogates agrees very well with the corresponding predictions of independent identically distributed random variables, which confirms that the temporal and spatial correlation of cracking bursts are responsible for the observed unique behaviour. 	
1605.04989v2	http://arxiv.org/pdf/1605.04989v2	2017	Architecture-aware Coding for Distributed Storage: Repairable Block   Failure Resilient Codes	Gokhan Calis|O. Ozan Koyluoglu	  In large scale distributed storage systems (DSS) deployed in cloud computing, correlated failures resulting in simultaneous failure (or, unavailability) of blocks of nodes are common. In such scenarios, the stored data or a content of a failed node can only be reconstructed from the available live nodes belonging to the available blocks. To analyze the resilience of the system against such block failures, this work introduces the framework of Block Failure Resilient (BFR) codes, wherein the data (e.g., a file in DSS) can be decoded by reading out from a same number of codeword symbols (nodes) from a subset of available blocks of the underlying codeword. Further, repairable BFR codes are introduced, wherein any codeword symbol in a failed block can be repaired by contacting a subset of remaining blocks in the system. File size bounds for repairable BFR codes are derived, and the trade-off between per node storage and repair bandwidth is analyzed, and the corresponding minimum storage regenerating (BFR-MSR) and minimum bandwidth regenerating (BFR-MBR) points are derived. Explicit codes achieving the two operating points for a special case of parameters are constructed, wherein the underlying regenerating codewords are distributed to BFR codeword symbols according to combinatorial designs. Finally, BFR locally repairable codes (BFR-LRC) are introduced, an upper bound on the resilience is derived and optimal code construction are provided by a concatenation of Gabidulin and MDS codes. Repair efficiency of BFR-LRC is further studied via the use of BFR-MSR/MBR codes as local codes. Code constructions achieving optimal resilience for BFR-MSR/MBR-LRCs are provided for certain parameter regimes. Overall, this work introduces the framework of block failures along with optimal code constructions, and the study of architecture-aware coding for distributed storage systems. 	
1607.01159v1	http://arxiv.org/pdf/1607.01159v1	2016	Towards Network-Failure-Tolerant Content Delivery for Web Content	Wen Hu|Zhi Wang|Lifeng Sun	  Popularly used to distribute a variety of multimedia content items in today Internet, HTTP-based web content delivery still suffers from various content delivery failures. Hindered by the expensive deployment cost, the conventional CDN can not deploy as many edge servers as possible to successfully deliver content items to all users under these delivery failures. In this paper, we propose a joint CDN and peer-assisted web content delivery framework to address the delivery failure problem. Different from conventional peer-assisted approaches for web content delivery, which mainly focus on alleviating the CDN servers bandwidth load, we study how to use a browser-based peer-assisted scheme, namely WebRTC, to resolve content delivery failures. To this end, we carry out large-scale measurement studies on how users access and view webpages. Our measurement results demonstrate the challenges (e.g., peers stay on a webpage extremely short) that can not be directly solved by conventional P2P strategies, and some important webpage viewing patterns. Due to these unique characteristics, WebRTC peers open up new possibilities for helping the web content delivery, coming with the problem of how to utilize the dynamic resources efficiently. We formulate the peer selection that is the critical strategy in our framework, as an optimization problem, and design a heuristic algorithm based on the measurement insights to solve it. Our simulation experiments driven by the traces from Tencent QZone demonstrate the effectiveness of our design: compared with non-peer-assisted strategy and random peer selection strategy, our design significantly improves the successful relay ratio of web content items under network failures, e.g., our design improves the content download ratio up to 60% even when users located in a particular region (e.g., city) where none can connect to the regional CDN server. 	
9703035v1	http://arxiv.org/pdf/alg-geom/9703035v1	1997	The Ideal Generation Problem for Fat Points	Brian Harbourne	  This paper is concerned with determining the number of generators in each degree for minimal sets of homogeneous generators for saturated ideals defining fat point subschemes $Z=m_1p_1+ ... +m_rp_r$ for general sets of points $p_i$ of $P^2$. For thin points (i.e., m_i=1 for all i), a solution is known, in terms of a maximal rank property. Although this property in general fails for fat points, we show it holds in an appropriate asymptotic sense. In the uniform (i.e., $m_1= ... =m_r$) case, we determine all failures of this maximal rank property for $r\le 9$, and we develop evidence for the conjecture that no other failures occur for $r > 9$. 	
9806022v1	http://arxiv.org/pdf/chao-dyn/9806022v1	1998	ROC Analysis and a Realistic Model of Heart Rate Variability	Stefan Thurner|Markus C. Feurstein|Malvin C. Teich	  We have carried out a pilot study on a standard collection of electrocardiograms from patients who suffer from congestive heart failure, and subjects without cardiac pathology, using receiver-operating-characteristic (ROC) analysis. The scale-dependent wavelet-coefficient standard deviation \sigma_{wav}(m), a multiresolution-based analysis measure, is found to be superior to two commonly used measures of cardiac dysfunction when the two classes of patients cannot be completely separated. A jittered integrate-and-fire model with a fractal Gaussian-noise kernel provides a realistic simulation of heartbeat sequences for both heart-failure patients and normal subjects. 	
9407025v1	http://arxiv.org/pdf/cmp-lg/9407025v1	1994	Recovering From Parser Failures: A Hybrid Statistical/Symbolic Approach	Carolyn Penstein Rose'|Alex Waibel	  We describe an implementation of a hybrid statistical/symbolic approach to repairing parser failures in a speech-to-speech translation system. We describe a module which takes as input a fragmented parse and returns a repaired meaning representation. It negotiates with the speaker about what the complete meaning of the utterance is by generating hypotheses about how to fit the fragments of the partial parse together into a coherent meaning representation. By drawing upon both statistical and symbolic information, it constrains its repair hypotheses to those which are both likely and meaningful. Because it updates its statistical model during use, it improves its performance over time. 	
9606158v1	http://arxiv.org/pdf/cond-mat/9606158v1	1996	Statistics and Microphysics of the Fracture of Glass	J. I. Katz	  The tensile strength of fused silica fibers is believed to approach its intrinsic value at low temperature, and modern experiments indicate very small, perhaps unmeasured, intrinsic dispersion in this strength. I consider the application of classical ``weakest link'' models to this problem in an attempt to determine the number and therefore the nature of the failure sites. If the skewness as well as the dispersion (Weibull modulus) of failure strengths are measured it may be possible to determine both the number of sites and the distribution of their strengths. Extant data are not sufficient, but I present calculated skewnesses for comparison with future data. 	
0004444v1	http://arxiv.org/pdf/cond-mat/0004444v1	2000	The Haldane bosonisation scheme and metallic states of interacting   fermions in d spatial dimensions	Behnam Farid	  We consider the Haldane bosonisation scheme in d spatial dimensions as applied to a realistic model of interacting fermions in d=2 and unequivocally demonstrate failure of this scheme in d > 1, specifically in d=2. In addition to tracing back this failure to its origin, we show that {\sl nothing} as regards the true metallic state of the model under consideration is known with any degree of certainty. 	
0011241v1	http://arxiv.org/pdf/cond-mat/0011241v1	2000	Nature of traps responsible for failure of MOS devices	V. A. Gritsenko|P. M. Lenahan|Yu. N. Morokov|Yu. N. Novikov	  A failure of chips in a huge amount of modern electronic devices is connected as a rule with the undesirable capturing of charge (electrons and holes) by traps in a thin insulating film of silicon oxide in transistors. It leads to a breakdown of transistors or to a destructive change of their characteristics. It is suggested that silicon oxide will be replaced in the next generation of nanoscale devices by silicon oxynitride. Therefore, it is very important to understand the nature of traps in this material. We discuss this nature using the quantum-chemical simulation. 	
0302258v1	http://arxiv.org/pdf/cond-mat/0302258v1	2003	Self-Similar Extrapolation for the Law of Acoustic Emission Before   Failure of Heterogeneous Materials	A. Moura|V. I. Yukalov	  Acoustic emission before the failure of heterogeneous materials is studied as a function of applied hydrostatic pressure. A formula for the energy release is suggested, which is valid in the whole diapason of pressures, from zero to the critical pressure of rupture. This formula is obtained by employing the extrapolation technique of the self-similar approximation theory. The result is fitted to experiment in order to demonstrate the correct general behaviour of the obtained expression for the energy release. 	
0309141v2	http://arxiv.org/pdf/cond-mat/0309141v2	2004	A model for cascading failures in complex networks	Paolo Crucitti|Vito Latora|Massimo Marchiori	  Large but rare cascades triggered by small initial shocks are present in most of the infrastructure networks. Here we present a simple model for cascading failures based on the dynamical redistribution of the flow on the network. We show that the breakdown of a single node is sufficient to collapse the efficiency of the entire system if the node is among the ones with largest load. This is particularly important for real-world networks with an highly hetereogeneous distribution of loads as the Internet and electrical power grids. 	
0404347v1	http://arxiv.org/pdf/cond-mat/0404347v1	2004	Creep failures in heterogeneous materials	H. Nechad|A. Helmstetter|R. El Guerjouma|D. Sornette	  We present creep experiments on fiber composite materials with controlled heterogeneity. Recorded strain rates and acoustic emission rates exhibit a power law relaxation in the primary creep regime (Andrade law) followed by a power law acceleration up to rupture over up to four decades in time. We discover that the failure time is proportional to the duration of the primary creep regime, showing the interplay between the two regimes and offering a method of rupture prediction. These experimental results are rationalized by a mean-field model of representative elements with nonlinear visco-elastic rheology and with a large heterogeneity of strengths. 	
0405102v1	http://arxiv.org/pdf/cs/0405102v1	2004	A Proof Theoretic Approach to Failure in Functional Logic Programming	Francisco Javier Lopez-Fraguas|Jaime Sanchez-Hernandez	  How to extract negative information from programs is an important issue in logic programming. Here we address the problem for functional logic programs, from a proof-theoretic perspective. The starting point of our work is CRWL (Constructor based ReWriting Logic), a well established theoretical framework for functional logic programming, whose fundamental notion is that of non-strict non-deterministic function. We present a proof calculus, CRWLF, which is able to deduce negative information from CRWL-programs. In particular, CRWLF is able to prove finite failure of reduction within CRWL. 	
9606045v1	http://arxiv.org/pdf/gr-qc/9606045v1	1996	Comment on "Failure of standard conservation laws at a classical change   of signature"	Sean A. Hayward	  Hellaby & Dray (gr-qc/9404001) have recently claimed that matter conservation fails under a change of signature, compounding earlier claims that the standard junction conditions for signature change are unnecessary. In fact, if the field equations are satisfied, then the junction conditions and the conservation equations are satisfied. The failure is rather that the authors did not make sense of the field equations and conservation equations, which are singular at a change of signature. 	
0501065v2	http://arxiv.org/pdf/gr-qc/0501065v2	2005	Hamilton-Jacobi equation and the breaking of the WKB approximation	Fabrizio Canfora	  A simple method to deal with four dimensional Hamilton-Jacobi equation for null hypersurfaces is introduced. This method allows to find simple geometrical conditions which give rise to the failure of the WKB approximation on curved spacetimes. The relation between such failure, extreme blackholes and the Cosmic Censor hypothesis is briefly discussed. 	
9510364v1	http://arxiv.org/pdf/hep-ph/9510364v1	1995	Some Speculations on the Ultimate Planck Energy	A. Casher|S. Nussinov	  The inability to achieve in the present universe, via electromagnetic or gravitational acceleration, Planck energies for elementary particles is suggested on the basis of several, some relatively sophisticated, failed attempts. This failure is essential for schemes were the superplanckian regime for the energies of elementary particles is ``Unphysical''. The basic observation is that this failure to achieve superplanckian energies naturally occurs in our universe of finite age and horizon. It does tie up in a mysterious fashion these cosmological quantities and elementary physics parameters such as the masses of the lightest charged fermions. 	
0005250v2	http://arxiv.org/pdf/hep-th/0005250v2	2000	Holographic Stress Tensor for Kerr-AdS Black Holes and Local Failure on   IR-UV Connection	Jeongwon Ho	  We show that in general holographic stress tensor may contain a new term of divergence of a spacelike unit normal acceleration. Then, it is shown that in contrast to previous descriptions, a new stress tensor for Kerr-AdS solutions can be a traceless one. Interestingly, this prescription entails a local failure on the IR-UV connection. 	
0508057v2	http://arxiv.org/pdf/hep-th/0508057v2	2005	Failure of microcausality in quantum field theory on noncommutative   spacetime	O. W. Greenberg	  The star commutator of $:\phi(x) \star \phi(x):$ with $:\phi(y) \star \phi(y):$ fails to vanish at equal times and thus also fails to obey microcausality at spacelike separation even for the case in which $\theta^{0i}=0$. The failure to obey microcausality for this sample observable implies that this form of noncommutative field theory fails to obey microcausality in general. This result also holds for general fields and observables. We discuss possible responses to this problem. 	
0305234v1	http://arxiv.org/pdf/math/0305234v1	2003	Efficient estimation in the accelerated failure time model under cross   sectional sampling	Chris A. J. Klaassen|Philip J. Mokveld|Bert van Es	  Consider estimation of the regression parameter in the accelerated failure time model, when data are obtained by cross sectional sampling. It is shown that it is possible under regularity of the model to construct an efficient estimator of the unknown Euclidean regression parameter if the distribution of the covariate vector is known and also if it is unknown with vanishing mean. 	
0411276v1	http://arxiv.org/pdf/math/0411276v1	2004	Limiting Behaviour of the Mean Residual Life	David M. Bradley|Ramesh C. Gupta	  In survival or reliability studies, the mean residual life or life expectancy is an important characteristic of the model. Here, we study the limiting behaviour of the mean residual life, and derive an asymptotic expansion which can be used to obtain a good approximation for large values of the time variable. The asymptotic expansion is valid for a quite general class of failure rate distributions--perhaps the largest class that can be expected given that the terms depend only on the failure rate and its derivatives. 	
0611113v2	http://arxiv.org/pdf/math/0611113v2	2008	Morse Theory for the Space of Higgs Bundles	Graeme Wilkin	  Here we prove the necessary analytic results to construct a Morse theory for the Yang-Mills-Higgs functional on the space of Higgs bundles over a compact Riemann surface. The main result is that the gradient flow with initial conditions $(A'', \phi)$ converges to a critical point of this functional, the isomorphism class of which is given by the graded object associated to the Harder-Narasimhan-Seshadri filtration of $(A'', \phi)$. In particular, the results of this paper show that the failure of hyperk\"ahler Kirwan surjectivity for rank 2 fixed determinant Higgs bundles does not occur because of a failure of the existence of a Morse theory. 	
0408026v1	http://arxiv.org/pdf/nlin/0408026v1	2004	On propagation failure in 1 and 2 dimensional excitable media	Georg A. Gottwald|Lorenz Kramer	  We present a non-perturbative technique to study pulse dynamics in excitable media. The method is used to study propagation failure in one-dimensional and two-dimensional excitable media. In one-dimensional media we describe the behaviour of pulses and wave trains near the saddle node bifurcation, where propagation fails. The generalization of our method to two dimensions captures the point where a broken front (or finger) starts to retract. We obtain approximate expressions for the pulse shape, pulse velocity and scaling behavior. The results are compared with numerical simulations and show good agreement. 	
0005072v2	http://arxiv.org/pdf/nucl-th/0005072v2	2000	Quantum Hadrodynamics: Evolution and Revolution	R. J. Furnstahl|Brian D. Serot	  The underlying philosophy and motivation for quantum hadrodynamics (QHD), namely, relativistic field theories of nuclear phenomena featuring manifest covariance, have evolved over the last quarter century in response to successes, failures, and sharp criticisms. A recent revolution in QHD, based on modern effective field theory and density functional theory perspectives, explains the successes, provides antidotes to the failures, rebuts the criticisms, and focuses the arguments in favor of a covariant representation. 	
9711003v1	http://arxiv.org/pdf/patt-sol/9711003v1	1997	Propagation failure of traveling waves in a discrete bistable medium	Gabor Fath	  Propagation failure (pinning) of traveling waves is studied in a discrete scalar reaction-diffusion equation with a piecewise linear, bistable reaction function. The critical points of the pinning transition, and the wavefront profile at the onset of propagation are calculated exactly. The scaling of the wave speed near the transition, and the leading corrections to the front shape are also determined. We find that the speed vanishes logarithmically close to the critical point, thus the model belongs to a different universality class than the standard Nagumo model, defined with a smooth, polynomial reaction function. 	
0405112v1	http://arxiv.org/pdf/quant-ph/0405112v1	2004	Erasure Thresholds for Efficient Linear Optics Quantum Computation	Marcus Silva	  Using an error models motivated by the Knill, Laflamme, Milburn proposal for efficient linear optics quantum computing [Nature 409,46--52, 2001], error rate thresholds for erasure errors caused by imperfect photon detectors using a 7 qubit code are derived and verified through simulation. A novel method -- based on a Markov chain description of the erasure correction procedure -- is developed and used to calculate the recursion relation describing the error rate at different encoding levels from which the threshold is derived, matching threshold predictions by Knill, Laflamme and Milburn [quant-ph/0006120, 2000]. In particular, the erasure threshold for gate failure rate in the same order as the measurement failure rate is found to be above 1.78%. 	
0406063v3	http://arxiv.org/pdf/quant-ph/0406063v3	2006	Probabilities of failure for quantum error correction	A. J. Scott	  We investigate the performance of a quantum error-correcting code when pushed beyond its intended capacity to protect information against errors, presenting formulae for the probability of failure when the errors affect more qudits than that specified by the code's minimum distance. Such formulae provide a means to rank different codes of the same minimum distance. We consider both error detection and error correction, treating explicit examples in the case of stabilizer codes constructed from qubits and encoding a single qubit. 	
0705.0044v1	http://arxiv.org/pdf/0705.0044v1	2007	Reliable Memories Built from Unreliable Components Based on Expander   Graphs	Shashi Kiran Chilappagari|Bane Vasic	  In this paper, memories built from components subject to transient faults are considered. A fault-tolerant memory architecture based on low-density parity-check codes is proposed and the existence of reliable memories for the adversarial failure model is proved. The proof relies on the expansion property of the underlying Tanner graph of the code. An equivalence between the Taylor-Kuznetsov (TK) scheme and Gallager B algorithm is established and the results are extended to the independent failure model. It is also shown that the proposed memory architecture has lower redundancy compared to the TK scheme. The results are illustrated with specific numerical examples. 	
0712.3432v1	http://arxiv.org/pdf/0712.3432v1	2007	Statistical analysis of redundant systems with "warm" stand-by units	V. Bagdonavičius|I. Masiulaitytė|M. Nikulin	  Mathematical formulation of fluent switching from "warm" to "hot" conditions of standby units is given using the well known Sedyakin's and accelerated failure time (AFT) models. Non-parametric estimators of cumulative distribution function and mean failure time of a redundant system with several stand-by units are proposed. Goodness-of-fit tests for two given models are given. 	
0902.1610v1	http://arxiv.org/pdf/0902.1610v1	2009	Package upgrades in FOSS distributions: details and challenges	Roberto Di Cosmo|Stefano Zacchiroli|Paulo Trezentos	  The upgrade problems faced by Free and Open Source Software distributions have characteristics not easily found elsewhere. We describe the structure of packages and their role in the upgrade process. We show that state of the art package managers have shortcomings inhibiting their ability to cope with frequent upgrade failures. We survey current countermeasures to such failures, argue that they are not satisfactory, and sketch alternative solutions. 	
0903.4930v1	http://arxiv.org/pdf/0903.4930v1	2009	Time manipulation technique for speeding up reinforcement learning in   simulations	Petar Kormushev|Kohei Nomoto|Fangyan Dong|Kaoru Hirota	  A technique for speeding up reinforcement learning algorithms by using time manipulation is proposed. It is applicable to failure-avoidance control problems running in a computer simulation. Turning the time of the simulation backwards on failure events is shown to speed up the learning by 260% and improve the state space exploration by 12% on the cart-pole balancing task, compared to the conventional Q-learning and Actor-Critic algorithms. 	
0907.1516v2	http://arxiv.org/pdf/0907.1516v2	2010	Les Probabilités Défaillance comme Indicateurs de Performance des   Barrières Techniques de Sécurité - Approche Analytique	Florent Brissaud|Brice Lanternier	  French environmental laws require industrialists to include probability criteria in risk assessments, especially to define confidence levels for risk management measures. This paper presents the failure probabilities as efficient indicators for technical safety barrier performances. Generic formulas are proposed to evaluate these probabilities, including failure rate, barrier architecture, full and partial proof tests. In many cases, these results can be directly used to assess safety barrier confidence levels. 	
1007.0238v1	http://arxiv.org/pdf/1007.0238v1	2010	A new lifetime model with decreasing failure rate	Wagner Barreto-Souza|Hassan S. Bakouch	  In this paper we introduce a new lifetime distribution by compounding exponential and Poisson-Lindley distributions, named exponential Poisson-Lindley distribution. Several properties are derived, such as density, failure rate, mean lifetime, moments, order statistics and R\'enyi entropy. Furthermore, estimation by maximum likelihood and inference for large sample are discussed. The paper is motivated by two applications to real data sets and we hope that this model be able to attract wider applicability in survival and reliability. 	
1008.0373v1	http://arxiv.org/pdf/1008.0373v1	2010	Little Boxes: The Simplest Demonstration of the Failure of Einstein's   Attempt to Show the Incompleteness of Quantum Theory	John D. Norton	  The failure of Einstein's co-authored "EPR" attempt to show the incompleteness of quantum theory is demonstrated directly for spatial degrees of freedom using only elementary notions. A GHZ construction is realized in the position properties of three particles whose quantum waves are distributed over three two-chambered boxes. The same system is modeled more realistically using three spatially separated, singly ionized hydrogen molecules. 	
1101.5449v1	http://arxiv.org/pdf/1101.5449v1	2011	Failure of A Mix Network	Kun Peng	  A mix network by Wikstrom fails in correctness, provable privacy and soundness. Its claimed advantages in security and efficiency are compromised. The analysis in this paper illustrates that although the first two failures may be fixed by modifying the shuffling protocol, the last one is too serious to fix at a tolerable cost. Especially, an attack is proposed to show how easily soundness of the shuffling scheme can be compromised. Moreover, the most surprising discovery in this paper is that it is formally illustrated that in practice it is impossible to fix soundness of the shuffling scheme by Wikstrom. 	
1106.5570v1	http://arxiv.org/pdf/1106.5570v1	2011	A distributed service for on demand end to end optical circuits	Ramiro Voicu|Iosif Legrand|Harvey Newman|Nicolae Tapus|Ciprian Dobre	  In this paper we present a system for monitoring and controlling dynamic network circuits inside the USLHCNet network. This distributed service system provides in near real-time complete topological information for all the circuits, resource allocation and usage, accounting, detects automatically failures in the links and network equipment, generate alarms and has the functionality to take automatic actions. The system is developed based on the MonALISA framework, which provides a robust monitoring and controlling service oriented architecture, with no single points of failure. 	
1108.4548v1	http://arxiv.org/pdf/1108.4548v1	2011	Ant Colony Optimization of Rough Set for HV Bushings Fault Detection	J. L. Mpanza|T. Marwala	  Most transformer failures are attributed to bushings failures. Hence it is necessary to monitor the condition of bushings. In this paper three methods are developed to monitor the condition of oil filled bushing. Multi-layer perceptron (MLP), Radial basis function (RBF) and Rough Set (RS) models are developed and combined through majority voting to form a committee. The MLP performs better that the RBF and the RS is terms of classification accuracy. The RBF is the fasted to train. The committee performs better than the individual models. The diversity of models is measured to evaluate their similarity when used in the committee. 	
1108.4618v1	http://arxiv.org/pdf/1108.4618v1	2011	Artificial Neural Network and Rough Set for HV Bushings Condition   Monitoring	LJ Mpanza|T. Marwala	  Most transformer failures are attributed to bushings failures. Hence it is necessary to monitor the condition of bushings. In this paper three methods are developed to monitor the condition of oil filled bushing. Multi-layer perceptron (MLP), Radial basis function (RBF) and Rough Set (RS) models are developed and combined through majority voting to form a committee. The MLP performs better that the RBF and the RS is terms of classification accuracy. The RBF is the fasted to train. The committee performs better than the individual models. The diversity of models is measured to evaluate their similarity when used in the committee. 	
1111.3166v1	http://arxiv.org/pdf/1111.3166v1	2011	On the Concatenation of Non-Binary Random Linear Fountain Codes with   Maximum Distance Separable Codes	Francisco Lazaro Blasco|Gianluigi Liva	  A novel fountain coding scheme has been introduced. The scheme consists of a parallel concatenation of a MDS block code with a LRFC code, both constructed over the same field, $F_q$. The performance of the concatenated fountain coding scheme has been analyzed through derivation of tight bounds on the probability of decoding failure as a function of the overhead. It has been shown how the concatenated scheme performs as well as LRFC codes in channels characterized by high erasure probabilities, whereas they provide failure probabilities lower by several orders of magnitude at moderate/low erasure probabilities. 	
1201.1199v1	http://arxiv.org/pdf/1201.1199v1	2012	Passage times of perturbed subordinators with application to reliability	Christian Paroissin|Landy Rabehasaina	  We consider a wide class of increasing L\'evy processes perturbed by an independent Brownian motion as a degradation model. Such family contains almost all classical degradation models considered in the literature. Classically failure time associated to such model is defined as the hitting time or the first-passage time of a fixed level. Since sample paths are not in general increasing, we consider also the last-passage time as the failure time following a recent work by Barker and Newby. We address here the problem of determining the distribution of the first-passage time and of the last-passage time. In the last section we consider a maintenance policy for such models. 	
1205.4809v1	http://arxiv.org/pdf/1205.4809v1	2012	Iterative Approximate Byzantine Consensus under a Generalized Fault   Model	Lewis Tseng|Nitin Vaidya	  In this work, we consider a generalized fault model that can be used to represent a wide range of failure scenarios, including correlated failures and non-uniform node reliabilities. This fault model is general in the sense that fault models studied in prior related work, such as f -total and f -local models, are special cases of the generalized fault model. Under the generalized fault model, we explore iterative approximate Byzantine consensus (IABC) algorithms in arbitrary directed networks. We prove a necessary and sufficient condition for the existence of IABC algorithms. The use of the generalized fault model helps to gain a better understanding of IABC algorithms. 	
1207.0854v1	http://arxiv.org/pdf/1207.0854v1	2012	CROSS-MBCR: Exact Minimum Bandwith Coordinated Regenerating Codes	Steve Jiekak|Nicolas Le Scouarnec	  We study the exact and optimal repair of multiple failures in codes for distributed storage. More particularly, we provide an explicit construction of exact minimum bandwidth coordinated regenerating codes (MBCR) for n=d+t,k,d >= k,t >= 1. Our construction differs from existing constructions by allowing both t>1 (i.e., repair of multiple failures) and d>k (i.e., contacting more than k devices during repair). 	
1207.6281v1	http://arxiv.org/pdf/1207.6281v1	2012	A note on asymptotic exponential arbitrage with exponentially decaying   failure probability	Kai Du|Ariel David Neufeld	  The goal of this paper is to prove a result conjectured in F\"ollmer and Schachermayer [FS07], even in slightly more general form. Suppose that S is a continuous semimartingale and satisfies a large deviations estimate; this is a particular growth condition on the mean-variance tradeoff process of S. We show that S then allows asymptotic exponential arbitrage with exponentially decaying failure probability, which is a strong and quantitative form of long-term arbitrage. In contrast to F\"ollmer and Schachermayer [FS07], our result does not assume that S is a diffusion, nor does it need any ergodicity assumption. 	
1209.1256v1	http://arxiv.org/pdf/1209.1256v1	2012	On the Decreasing Failure Rate property for general counting process.   Results based on conditional interarrival times	F. G. Badía|C. Sangüesa	  In the present paper we consider general counting processes stopped at a random time $T$, independent of the process. Provided that $T$ has the decreasing failure rate (DFR) property, we give sufficient conditions on the arrival times so that the number of events occurring before $T$ preserves the DFR property of $T$. These conditions involve the study of the conditional interarrival times. As a main application, we prove the DFR property in a context of maintenance models in reliability, by the consideration of Kijima type I virtual age models under quite general assumptions. 	
1209.4031v1	http://arxiv.org/pdf/1209.4031v1	2012	Long range failure-tolerant entanglement distribution	Ying Li|Sean D. Barrett|Thomas M. Stace|Simon C. Benjamin	  We introduce a protocol to distribute entanglement between remote parties. Our protocol is based on a chain of repeater stations, and exploits topological encoding to tolerate very high levels of defects and errors. The repeater stations may employ probabilistic entanglement operations which usually fail; ours is the first protocol to explicitly allow for technologies of this kind. Given an error rate between stations in excess of 10%, arbitrarily long range high fidelity entanglement distribution is possible even if the heralded failure rate within the stations is as high as 99%, providing that unheralded errors are low (order 0.01%). 	
1209.6152v2	http://arxiv.org/pdf/1209.6152v2	2013	Parity Declustering for Fault-Tolerant Storage Systems via $t$-designs	Son Hoang Dau|Yan Jia|Chao Jin|Weiya Xi|Kheong Sann Chan	  Parity declustering allows faster reconstruction of a disk array when some disk fails. Moreover, it guarantees uniform reconstruction workload on all surviving disks. It has been shown that parity declustering for one-failure tolerant array codes can be obtained via Balanced Incomplete Block Designs. We extend this technique for array codes that can tolerate an arbitrary number of disk failures via $t$-designs. 	
1210.4249v1	http://arxiv.org/pdf/1210.4249v1	2012	Checking the error correction strength of arbitrary surface code logical   gates	Thomas J. Milburn|Austin G. Fowler	  Topologically quantum error corrected logical gates are complex. Chains of errors can form in space and time and diagonally in spacetime. It is highly nontrivial to determine whether a given logical gate is free of low weight combinations of errors leading to failure. We report a new tool Nestcheck capable of analyzing an arbitrary topological computation and determining the minimum number of errors required to cause failure. 	
1211.0761v1	http://arxiv.org/pdf/1211.0761v1	2012	Non-tame mice from tame failures of the unique branch hypothesis	Grigor Sargsyan|Nam TRang	  In this paper, we show that the failure of the unique branch hypothesis (UBH) for tame trees (see \rdef{tame iteration tree}) implies that in some homogenous generic extension of $V$ there is a transitive model $M$ containing $Ord \cup \mathbb{R}$ such that $M\vDash AD^+ + \Theta > \theta_0$. In particular, this implies the existence (in $V$) of a non-tame mouse. The results of this paper significantly extend Steel's earlier results from \cite{steel2002core} for tame trees. 	
1211.6370v1	http://arxiv.org/pdf/1211.6370v1	2012	Increasing the failure recovery probability of atomic replacement   approaches	Hadi Saboohi|Sameem Abdul Kareem	  Web processes are made up of services as their units of functionality. The services are represented as a graph and compose a synergy of service. The composite service is prone to failure due to various causes. However, the end-user should receive a smooth and non-interrupted execution. Atomic replacement of a failed Web service to recover the system is a straightforward approach. Nevertheless, finding a similar service is not reliable. In order to increase the probability of the recovery of a failed composite service, a set of services is replaced with another similar set. 	
1303.6971v1	http://arxiv.org/pdf/1303.6971v1	2013	Composite Toffoli gate with two-round error detection	Cody Jones	  We introduce a fault-tolerant construction to implement a composite quantum operation of four overlapping Toffoli gates. The same construction can produce two independent Toffoli gates. This result lowers resource overheads in designs for quantum computers by more than an order of magnitude. The procedure uses Clifford operations and 64 copies of the non-Clifford gate $T = \exp[i \pi (I - \sigma^z) /8]$. Quantum codes detect errors in the circuit. When the dominant source of error is $T$-gate failure with probability $p$, then the composite Toffoli circuit has postselected failure rate of $3072p^4$ to lowest order. 	
1304.1053v1	http://arxiv.org/pdf/1304.1053v1	2013	Integral Brauer-Manin obstructions for sums of two squares and a power	Fabian Gundlach	  We use Brauer-Manin obstructions to explain failures of the integral Hasse principle and strong approximation away from infinity for the equation x^2+y^2+z^k=m with fixed integers k>=3 and m. Under Schinzel's hypothesis (H), we prove that Brauer-Manin obstructions corresponding to specific Azumaya algebras explain all failures of strong approximation away from infinity at the variable z. Finally, we present an algorithm that, again under Schinzel's hypothesis (H), finds out whether the equation has any integral solutions. 	
1306.3668v1	http://arxiv.org/pdf/1306.3668v1	2013	Containment Counterexamples for ideals of various configurations of   points in ${\bf P}^N$	Brian Harbourne|Alexandra Seceleanu	  When $I$ is the radical homogeneous ideal of a finite set of points in projective $N$-space, ${\bf P}^N$, over a field $K$, it has been conjectured that $I^{(rN-N+1)}$ should be contained in $I^r$ for all $r\geq 1$. Recent counterexamples show that this can fail when N=r=2. We study properties of the resulting ideals. We also show that failures occur for infinitely many $r$ in every characteristic $p>2$ when N=2, and we find additional positive characteristic failures when $N>2$. 	
1307.3086v1	http://arxiv.org/pdf/1307.3086v1	2013	Towards a Good ABS Design for more Reliable Vehicles on the Roads	Afifa Ghenai|Mohamed Youcef Badaoui|Mohamed Benmohammed	  Nowadays, better driving also means better braking. To this end, vehicle designers must find all failures during the design phase of antilock braking systems which play an important role in automobiles safety. However, mechatronic systems are so complex and failures can be badly identified. So it is necessary to propose a design approach of an antilock braking system which will be able to avoid wheels locking during braking and maintain vehicle stability. This paper describes this approach, in which we model the functional and the dysfunctional behavior of an antilock braking system using stopwatch Petri nets. 	
1307.5038v2	http://arxiv.org/pdf/1307.5038v2	2015	Inverse continuity on the boundary of the numerical range	Timothy Leake|Brian Lins|Ilya Spitkovsky	  Let $A \in M_n(\C)$. We consider the mapping $f_A(x)=x^*Ax$, defined on the unit sphere in $\C^n$. The map has a multi-valued inverse $f_A^{-1}$, and the continuity properties of $f_A^{-1}$ are considered in terms of the structure of the set of pre-images for points in the numerical range. It is shown that there may be only finitely many failures of continuity of $f_A^{-1}$, and conditions for where these failure occur are given. Additionally, we give a necessary and sufficient condition for weak inverse continuity to hold for $n=4$ and a sufficient condition for $n>4$. 	
1308.1358v1	http://arxiv.org/pdf/1308.1358v1	2013	The Performance of Paxos and Fast Paxos	Gustavo M. D. Vieira|Luiz E. Buzato	  Paxos and Fast Paxos are optimal consensus algorithms that are simple and elegant, while suitable for efficient implementation. In this paper, we compare the performance of both algorithms in failure-free and failure-prone runs using Treplica, a general replication toolkit that implements these algorithms in a modular and efficient manner. We have found that Paxos outperforms Fast Paxos for small number of replicas and that collisions are not the cause of this performance difference. 	
1309.5897v1	http://arxiv.org/pdf/1309.5897v1	2013	Comment on "Discussion on `Novel attractive force between ions in   quantum plasmas -- failure of simulations based on a density functional   approach"	M. Bonitz|E. Pehlke|T. Schoof	  In a recent article [P.K. Shukla, B. Eliasson and M. Akbari-Moghanjoughi, Physica Scripta {\bf 87}, 018202 (2013)] the authors criticized our analysis of the screened proton potential in dense hydrogen that was based on {\em ab initio} density functional theory (DFT) simulations [M. Bonitz, E. Pehlke, and T. Schoof, Phys. Rev. E {\bf 87}, 037102 (2013)]. In particular, they attributed the absence of the Shukla-Eliasson attractive force between protons in the DFT simulations to a failure of DFT. Here we discuss in detail their arguments and show that their conclusions are incorrect. 	
1310.5720v1	http://arxiv.org/pdf/1310.5720v1	2013	Cascading Failures in Networks with Proximate Dependent Nodes	Yosef Kornbluth|Steven Lowinger|Gabriel Cwilich|Sergey V. Buldyrev	  We study the mutual percolation of a system composed of two interdependent random regular networks. We introduce a notion of distance to explore the effects of the proximity of interdependent nodes on the cascade of failures after an initial attack. We find a non-trivial relation between the nature of the transition through which the networks disintegrate and the parameters of the system, which are the degree of the nodes and the maximum distance between interdependent nodes. We explain this relation by solving the problem analytically for the relevant set of cases. 	
1310.6357v2	http://arxiv.org/pdf/1310.6357v2	2014	Generalized geometry of two-dimensional vacua	Dario Rosa	  We derive the conditions for unbroken supersymmetry for a Mink_2, (2,0) vacuum, arising from Type II supergravity on a compact eight-dimensional manifold M_8. When specialized to internal manifolds enjoying SU(4)xSU(4) structure the resulting system is elegantly rewritten in terms of generalized complex geometry. This particular class of vacua violates the correspondence between supersymmetry conditions and calibrations conditions of D branes (supersymmetry-calibrations correspondence). Our analysis includes and extends previous results about the failure of the supersymmetry-calibrations correspondence, and confirms the existence of a precise relation between such a failure and a subset of the supersymmetry conditions. 	
1311.1940v2	http://arxiv.org/pdf/1311.1940v2	2014	Power Decoding of Reed-Solomon Codes Revisited	Johan S. R. Nielsen	  Power decoding, or "decoding by virtual interleaving", of Reed--Solomon codes is a method for unique decoding beyond half the minimum distance. We give a new variant of the Power decoding scheme, building upon the key equation of Gao. We show various interesting properties such as behavioural equivalence to the classical scheme using syndromes, as well as a new bound on the failure probability when the powering degree is 3. 	
1312.0859v1	http://arxiv.org/pdf/1312.0859v1	2013	Accelerated Failure Time Models for Competing Risks in a Cluster   Weighted Modelling Framework	Utkarsh J. Dang|Paul D. McNicholas	  A novel approach for dealing with censored competing risks regression data is proposed. This is implemented by a mixture of accelerated failure time (AFT) models for a competing risks scenario within a cluster-weighted modelling (CWM) framework. Specifically, we make use of the log-normal AFT model here but any commonly used AFT model can be utilized. The alternating expectation conditional maximization algorithm (AECM) is used for parameter estimation and bootstrapping for standard error estimation. Finally, we present our results on some simulated and real competing risks data. 	
1312.2336v1	http://arxiv.org/pdf/1312.2336v1	2013	Hierarchical scale-free network is fragile against random failure	Takehisa Hasegawa|Koji Nemoto	  We investigate site percolation in a hierarchical scale-free network known as the Dorogovtsev- Goltsev-Mendes network. We use the generating function method to show that the percolation threshold is 1, i.e., the system is not in the percolating phase when the occupation probability is less than 1. The present result is contrasted to bond percolation in the same network of which the percolation threshold is zero. We also show that the percolation threshold of intentional attacks is 1. Our results suggest that this hierarchical scale-free network is very fragile against both random failure and intentional attacks. Such a structural defect is common in many hierarchical network models. 	
1404.1525v1	http://arxiv.org/pdf/1404.1525v1	2014	Type-amalgamation properties and polygroupoids in stable theories	John Goodrick|Byunghan Kim|Alexei Kolesnikov	  We show that in a stable first-order theory, the failure of higher-dimensional type amalgamation can always be witnessed by algebraic structures which we call n-ary polygroupoids. This generalizes a result of Hrushovski that failures of 4-amalgamation in stable theories are witnessed by definable groupoids (which are 2-ary polygroupoids in our terminology). The n-ary polygroupoids are definable in a mild expansion of the language (adding a unary predicate for an infinite Morley sequence). 	
1404.5689v1	http://arxiv.org/pdf/1404.5689v1	2014	Measurement and Internalization of Systemic Risk in a Global Banking   Network	Xiaobing Feng|Haibo Hu	  The negative externalities from an individual bank failure to the whole system can be huge. One of the key purposes of bank regulation is to internalize the social costs of potential bank failures via capital charges. This study proposes a method to evaluate and allocate the systemic risk to different countries/regions using a SIR type of epidemic spreading model and the Shapley value in game theory. The paper also explores features of a constructed bank network using real globe-wide banking data. 	
1410.0724v2	http://arxiv.org/pdf/1410.0724v2	2014	Post-Processing Free Spatio-Temporal Optical Random Number Generator   Resilient to Hardware Failure and Signal Injection Attacks	Mario Stipčević|John Bowers	  We present a random number generator based on quantum effects in photonic emission and detection. It is unique in simultaneous use of both spatial and temporal quantum information contained in the system which makes it resilient to hardware failure and signal injection attacks. We show that its deviation from randomness cam be estimated based on simple measurements. Generated numbers pass NIST Statistical test suite without post-processing. 	
1502.02814v1	http://arxiv.org/pdf/1502.02814v1	2015	Preventing Buckling of Slender Cylindrical Structures by Internal   Viscous Flows	Max Linshits|Amir D. Gat	  Viscous flows within an elastic structure apply stress on the solid-liquid interface. The stress-field created by the viscous flow can be utilized to counter stress created by external forces and thus may be applied as a tool for delaying the onset of structural failure. To illustrate this concept we study viscous flow within an elastic cylinder under compressive axial force. We obtain a closed-form expression showing an approximately linear relation between the critical buckling load and the liquid inlet pressure. Our results are validated by numerical computations. We discuss future research directions of fluid-solid composite materials which create flow under external stress, yielding enhanced resistance to structural failure. 	
1503.03003v2	http://arxiv.org/pdf/1503.03003v2	2015	Emission of Gravitational Waves from a Magnetohydrodynamic Dynamo	Friedwardt Winterberg	  The failure of the laser-interferometer gravitational wave antennas to measure the tiny changes of lengths many orders of magnitude smaller than the diameter of a proton raises the question of whether the reason for this failure is a large gravitational wave background noise, and if so, where this background noise is coming from. It is conjectured that it comes from gravitational waves emitted from a magnetohydrodynamic dynamo in the center of the sun, with the large magnetic field from this dynamo shielded by thermomagnetic currents in the tachocline. Using the moon as a large Weber bar, these gravitational waves could possibly be detected by the Poisson diffraction into the center of the lunar shadow during a total solar eclipse. 	
1504.01530v1	http://arxiv.org/pdf/1504.01530v1	2015	Modeling a fluid to solid phase transition in snow weak-layers.   Application to slab avalanche release	Francois Louchet	  Snow slab avalanche release usually results from failure of weak layers made of loose ice crystals. In previous field experiments, we evidenced for the first time an interesting stress-driven transition in the weak layer between a granular fluid and a solid phase. We propose here an original model involving the kinetics of ice grains bonds failure and reconstruction. The model evidences a sudden transition between two drastically different types of weak layer behaviors. It accounts for the characteristics of both the studied fluid-solid transition and for slab avalanche release observations. It may possibly apply to a number of other granular materials. 	
1504.06498v2	http://arxiv.org/pdf/1504.06498v2	2015	Compound geometric approximation under a failure rate constraint	Fraser Daly	  We consider compound geometric approximation for a nonnegative, integer-valued random variable $W$. The bound we give is straightforward but relies on having a lower bound on the failure rate of $W$. Applications are presented to M/G/1 queuing systems, for which we state explicit bounds in approximations for the number of customers in the system and the number of customers served during a busy period. Other applications are given to birth-death processes and Poisson processes. 	
1504.07731v1	http://arxiv.org/pdf/1504.07731v1	2015	Non-commutative groupoids obtained from the failure of $3$-uniqueness in   stable theories	Byunghan Kim|SunYoung Kim|Junguk Lee	  We construct a possibly non-commutative groupoid from the failure of $3$-uniqueness of a strong type. The commutative groupoid constructed by John Goodrick and Alexei Kolesnikov in \cite{GK} lives in the center of the groupoid.   A certain automorphism group approximated by the vertex groups of the non-commutative groupoids is suggested as a "fundamental group" of the strong type. 	
1505.05466v1	http://arxiv.org/pdf/1505.05466v1	2015	Bayesian Estimation of the Kumaraswamy Inverse Weibull Distribution	Felipe R. S. de Gusmão|Vera L. D. Tomazella|Ricardo S. Ehlers	  The Kumaraswamy Inverse Weibull distribution has the ability to model failure rates that have unimodal shapes and are quite common in reliability and biological studies. The three-parameter Kumaraswamy Inverse Weibull distribution with decreasing and unimodal failure rate is introduced. We provide a comprehensive treatment of the mathematical properties of the Kumaraswany Inverse Weibull distribution and derive expressions for its moment generating function and the $r$-th generalized moment. Some properties of the model with some graphs of density and hazard function are discussed. We also discuss a Bayesian approach for this distribution and an application was made for a real data set. 	
1507.06119v2	http://arxiv.org/pdf/1507.06119v2	2016	Reliability study of a coherent system with single general standby   component	Pradip Kundu|Nil Kamal Hazra|Asok K. Nanda	  In this paper, the properties of a coherent system equipped with a single general standby component is investigated. Here, the standby component may initially be put into cold state and is switched over to warm state after a certain time period, up to which the system certainly does not fail. Then the standby component in warm state starts to work in active state at the time of failure of a component which may cause the system failure. Here three different switch over cases regarding the state changes of the standby component are considered. Numerical examples are also provided. 	
1508.03262v1	http://arxiv.org/pdf/1508.03262v1	2015	The Plateau Problem in the Heteroskedastic Probit Model	Eric Freeman|Luke Keele|David Park|Julia Salzman|Brendan Weickert	  In parameter determination for the heteroskedastic probit model, both in simulated data and in actual data, we observe a failure of traditional local search methods to converge consistently to a single parameter vector, in contrast to the typical situation for the regular probit model. We identify features of the heteroskedastic probit log likelihood function that we argue tend to lead to this failure, and suggest ways to amend the local search methods to remedy the problem. 	
1510.06395v1	http://arxiv.org/pdf/1510.06395v1	2015	The Odd Generalized Exponential Linear Failure Rate Distribution	M. A. El-Damcese|Abdelfattah Mustafa|B. S. El-Desouky|M. E. Mustafa	  In this paper we propose a new lifetime model, called the odd generalized exponential linear failure rate distribution. Some statistical properties of the proposed distribution such as the moments, the quantiles, the median, and the mode are investigated. The method of maximum likelihood is used for estimating the model parameters. An applications to real data is carried out to illustrate that the new distribution is more flexible and effective than other popular distributions in modeling lifetime data. 	
1512.03500v1	http://arxiv.org/pdf/1512.03500v1	2015	Multi-threshold Accelerate Failure Time Model	Baisuo Jin|Jialiang Li	  A two-stage procedure for simultaneously detecting multiple thresholds and achieving model selection in the segmented accelerate failure time (AFT) model is developed in this paper. In the first stage, we formulate the threshold problem as a group model selection problem so that a concave 2-norm group selection method can be applied. In the second stage, the thresholds are finalized via a refining method. We establish the strong consistency of the threshold estimates and regression coefficient estimates under some mild technical conditions. The proposed procedure performs satisfactorily in our extensive simulation studies. Its real world applicability is demonstrated via analyzing a follicular lymphoma data. 	
1512.07423v1	http://arxiv.org/pdf/1512.07423v1	2015	NPEFix: Automatic Runtime Repair of Null Pointer Exceptions in Java	Benoit Cornu|Thomas Durieux|Lionel Seinturier|Martin Monperrus	  Null pointer exceptions, also known as null dereferences are the number one exceptions in the field. In this paper, we propose 9 alternative execution semantics when a null pointer exception is about to happen. We implement those alternative execution strategies using code transformation in a tool called NPEfix. We evaluate our prototype implementation on 11 field null dereference bugs and 519 seeded failures and show that NPEfix is able to repair at runtime 10/11 and 318/519 failures. 	
1601.01045v1	http://arxiv.org/pdf/1601.01045v1	2016	A new 3-parameter extension of generalized lindley distribution	Deepesh Bhati|Mohd. Aamir Malik|K. K. Jose	  Here, we introduce a new class of Lindley generated distributions which results in more flexible model with increasing failure rate (IFR), decreasing failure rate(DFR) and up-side down hazard functions for different choices of parametric values. We explore, various distributional properties including limiting distribution of extreme order statistics explored. Maximum likelihood estimators and the confidence intervals of the parameters are obtained. The applicability of the proposed distribution is shown through modelling two sets of real data on bladder cancer patients and waiting time in a queue. Further, we carry out stress-strength analysis for applying the model in system reliability studies. 	
1601.07189v1	http://arxiv.org/pdf/1601.07189v1	2016	Rare-Event Estimation for Dynamic Fault Trees	Sergey Porotsky	  Article describes the results of the development and using of Rare-Event Monte-Carlo Simulation Algorithms for Dynamic Fault Trees Estimation. For Fault Trees estimation usually analytical methods are used (Minimal Cut sets, Markov Chains, etc.), but for complex models with Dynamic Gates it is necessary to use Monte-Carlo simulation with combination of Importance Sampling method. Proposed article describes approach for this problem solution according for specific features of Dynamic Fault Trees. There are assumed, that failures are non-repairable with general distribution functions of times to failures (there may be Exponential distribution, Weibull, Normal and Log-Normal, etc.). Expessions for Importance Sampling Re-Calculations are proposed and some numerical results are considered 	
1602.05656v1	http://arxiv.org/pdf/1602.05656v1	2016	An Estimation Method Using Periodic Inspection of Indicators	Zheng Wang	  This paper proposes a new approach for estimating the failure time distribution using the indicator data. The indicators, which are checked by periodic inspection of a standby redundant system, only convey whether at least one failure occurs per interval. The estimation procedure first obtains the estimation of the forward recurrence time using the indicator data. Then the mean is estimated based on its relationship with the forward recurrence time. And the estimation of the sampled Cdf is thus derived based on its relationship with the forward recurrence time and the mean. Finally, the Cdf function is estimated using interpolation method. The simulation results showed that the estimation method performed well for the four Weibull distributions. 	
1605.01097v1	http://arxiv.org/pdf/1605.01097v1	2016	Reliability Testing Strategy - Reliability in Software Engineering	Kevin Taylor-Sakyi	  This paper presents the core principles of reliability in software engineering - outlining why reliability testing is critical and specifying the process of measuring reliability. The paper provides insight for both novice and experts in the software engineering field for assessing failure intensity as well as predicting failure of software systems. Measurements are conducted by utilizing information from an operational profile to further enhance a test plan and test cases, all of which this paper demonstrates how to implement. 	
1608.06367v1	http://arxiv.org/pdf/1608.06367v1	2016	Generalized Shock Model Based On The Frequency of Shocks: A Simple   Approach	Viswanathan Arunachalam	  In a $\delta-$shock model, a system subject to randomly occurring shocks, the system fails when the time between two successive shocks lies below a threshold $\delta$. In this note, we study the generalization of this model where such $\delta-$shocks are accumulated and the system fails on the occurrence of $k^{th}$ such a $\delta-$shock. The probability distribution of the system failure time and the statistical characteristics are explicitly obtained. Normal approximation to the failure time distribution is proposed. 	
1611.02338v1	http://arxiv.org/pdf/1611.02338v1	2016	Line failure probability bounds for power grids	Tommaso Nesti|Alessandro Zocca|Bert Zwart	  We develop upper bounds for line failure probabilities in power grids, under the DC approximation and assuming Gaussian noise for the power injections. Our upper bounds are explicit, and lead to characterization of safe operational capacity regions that are convex and polyhedral, making our tools compatible with existing planning methods. Our probabilistic bounds are derived through the use of powerful concentration inequalities. 	
1611.09968v1	http://arxiv.org/pdf/1611.09968v1	2016	Cauchy MDS Array Codes With Efficient Decoding Method	Hanxu Hou|Yunghsiang S. Han	  Array codes have been widely used in communication and storage systems. To reduce computational complexity, one important property of the array codes is that only XOR operation is used in the encoding and decoding process. In this work, we present a novel family of maximal-distance separable (MDS) array codes based on Cauchy matrix, which can correct up to any number of failures. We also propose an efficient decoding method for the new codes to recover the failures. We show that the encoding/decoding complexities of the proposed approach are lower than those of existing Cauchy MDS array codes, such as Rabin-Like codes and CRS codes. Thus, the proposed MDS array codes are attractive for distributed storage systems. 	
1612.05740v1	http://arxiv.org/pdf/1612.05740v1	2016	Machine Learning, Linear and Bayesian Models for Logistic Regression in   Failure Detection Problems	B. Pavlyshenko	  In this work, we study the use of logistic regression in manufacturing failures detection. As a data set for the analysis, we used the data from Kaggle competition Bosch Production Line Performance. We considered the use of machine learning, linear and Bayesian models. For machine learning approach, we analyzed XGBoost tree based classifier to obtain high scored classification. Using the generalized linear model for logistic regression makes it possible to analyze the influence of the factors under study. The Bayesian approach for logistic regression gives the statistical distribution for the parameters of the model. It can be useful in the probabilistic analysis, e.g. risk assessment. 	
1701.00578v1	http://arxiv.org/pdf/1701.00578v1	2017	The Unfolding and Control of Network Cascades	Adilson E. Motter|Yang Yang	  A characteristic property of networks is their ability to propagate influences, such as infectious diseases, behavioral changes, and failures. An especially important class of such contagious dynamics is that of cascading processes. These processes include, for example, cascading failures in infrastructure systems, extinctions cascades in ecological networks, and information cascades in social systems. In this review, we discuss recent progress and challenges associated with the modeling, prediction, detection, and control of cascades in networks. 	
1703.07950v2	http://arxiv.org/pdf/1703.07950v2	2017	Failures of Gradient-Based Deep Learning	Shai Shalev-Shwartz|Ohad Shamir|Shaked Shammah	  In recent years, Deep Learning has become the go-to solution for a broad range of applications, often outperforming state-of-the-art. However, it is important, for both theoreticians and practitioners, to gain a deeper understanding of the difficulties and limitations associated with common approaches and algorithms. We describe four types of simple problems, for which the gradient-based algorithms commonly used in deep learning either fail or suffer from significant difficulties. We illustrate the failures through practical experiments, and provide theoretical insights explaining their source, and how they might be remedied. 	
1704.04162v1	http://arxiv.org/pdf/1704.04162v1	2017	Spatial uniformity in power-grid system	Mi Jin Lee|Beom Jun Kim	  Robust synchronization is indispensable for stable operation of a power grid. Recently, it has been reported that a large number of decentralized generators, rather than a small number of large power plants, provide enhanced synchronization together with greater robustness against structural failures. In this paper, we systematically control the spatial uniformity of the geographical distribution of generators, and conclude that the more uniformly generators are distributed, the more enhanced synchronization occurs. In the presence of temporal failures of power sources, we observe that spatial uniformity helps the power grid to recover stationarity in a shorter time. We also discuss practical implications of our results in designing the structure of a power-grid network. 	
1705.01257v1	http://arxiv.org/pdf/1705.01257v1	2017	Failure localization in time critical market applications	Mikhail Davidson|Gleb Labutin	  In time critical market applications such as for example scheduling and price computation for the balancing market, failure of the algorithm in finding a solution would result in cancelation of the session and respective financial consequences for the market participants. In the paper we propose a regularization procedure that could help an operator to spot the problematic location and find out the reasons that caused the algorithm to breakdown and make necessary corrections. The approach has proved to be useful in the operation of the Balancing Market in the Russian Federation. 	
1706.05058v1	http://arxiv.org/pdf/1706.05058v1	2017	The Weak Lefschetz property for quotients by Quadratic Monomials	Juan Migliore|Uwe Nagel|Hal Schenck	  In [MMR], Micha\l{}ek--Mir\'o-Roig give a beautiful geometric characterization of Artinian quotients by ideals generated by quadratic or cubic monomials, such that the multiplication map by a general linear form fails to be injective in the first nontrivial degree. Their work was motivated by conjectures of Ilardi and Mezzetti-Mir\'o-Roig-Ottaviani, connecting the failure to Laplace equations and classical results of Togliatti on osculating planes. We study quotients by quadratic monomial ideals, explaining failure of the Weak Lefschetz Property for some cases not covered by [MMR]. 	
1707.01952v1	http://arxiv.org/pdf/1707.01952v1	2017	Significance of Disk Failure Prediction in Datacenters	Jayanta Basak|Randy H. Katz	  Modern datacenters assemble a very large number of disk drives under a single roof. Even if economic and technical factors where to make individual drives more reliable (which is not at all clear, given the commoditization of the technology), their sheer numbers combined with their ever increasing utilization in a well-balanced design makes achieving storage reliability a major challenge. In this paper, we assess the challenge of storage system reliability in the modern datacenter, and demonstrate how good disk failure prediction models can significantly improve the reliability of such systems. 	
1707.02241v2	http://arxiv.org/pdf/1707.02241v2	2017	Repairing Multiple Failures for Scalar MDS Codes	Burak Bartan|Mary Wootters	  In distributed storage, erasure codes -- like Reed-Solomon Codes -- are often employed to provide reliability. In this setting, it is desirable to be able to repair one or more failed nodes while minimizing the repair bandwidth. In this work, motivated by Reed-Solomon codes, we study the problem of repairing multiple failed nodes in a scalar MDS code. We extend the framework of (Guruswami and Wootters, 2017) to give a framework for constructing repair schemes for multiple failures in general scalar MDS codes, in the centralized repair model. We then specialize our framework to Reed-Solomon codes, and extend and improve upon recent results of (Dau et al., 2017). 	
1708.02906v1	http://arxiv.org/pdf/1708.02906v1	2017	Implementing $\Diamond P$ with Bounded Messages on a Network of ADD   Channels	Saptaparni Kumar|Jennifer Welch	  We present an implementation of the eventually perfect failure detector ($\Diamond P$) from the original hierarchy of the Chandra-Toueg oracles on an arbitrary partitionable network composed of unreliable channels that can lose and reorder messages. Prior implementations of $\Diamond P$ have assumed different partially synchronous models ranging from bounded point-to-point message delay and reliable communication to unbounded message size and known network topologies. We implement $\Diamond P$ under very weak assumptions on an arbitrary, partitionable network composed of Average Delayed/Dropped (ADD) channels to model unreliable communication. Unlike older implementations, our failure detection algorithm uses bounded-sized messages to eventually detect all nodes that are unreachable (crashed or disconnected) from it. 	
1709.02919v1	http://arxiv.org/pdf/1709.02919v1	2017	On Low-Risk Heavy Hitters and Sparse Recovery Schemes	Yi Li|Vasileios Nakos|David Woodruff	  We study the heavy hitters and related sparse recovery problems in the low-failure probability regime. This regime is not well-understood, and has only been studied for non-adaptive schemes. The main previous work is one on sparse recovery by Gilbert et al.(ICALP'13). We recognize an error in their analysis, improve their results, and contribute new non-adaptive and adaptive sparse recovery algorithms, as well as provide upper and lower bounds for the heavy hitters problem with low failure probability. 	
1711.06213v1	http://arxiv.org/pdf/1711.06213v1	2017	Weak square and stationary reflection	Gunter Fuchs|Assaf Rinot	  It is well-known that the square principle $\square_\lambda$ entails the existence of a non-reflecting stationary subset of $\lambda^+$, whereas the weak square principle $\square^*_\lambda$ does not. Here we show that if $\mu^{\mathrm{cf}(\lambda)} < \lambda$ for all $\mu < \lambda$, then $\square^*_\lambda$ entails the existence of a non-reflecting stationary subset of $E^{\lambda^+}_{\mathrm{cf}(\lambda)}$ in the forcing extension for adding a single Cohen subset of $\lambda^+$. It follows that indestructible forms of simultaneous stationary reflection entail the failure of weak square. We demonstrate this by settling a question concerning the subcomplete forcing axiom (SCFA), proving that SCFA entails the failure of $\square^*_\lambda$ for every singular cardinal $\lambda$ of countable cofinality. 	
1801.08281v1	http://arxiv.org/pdf/1801.08281v1	2018	Modeling and Simulation of Electromigration Behavior for Via Array   Structure	Karthik Airani|Rohit Guttal	  In this paper, we develop a analytical model and algorithm for calculating uneven current distribution in via array structures. We propose a stress time translation formula and cumulative failure distribution equation to model the memory effect of electro migration stress or damage on a via array structure. We develop a method to project via array electromigration (EM) lifetime based on an arbitrary via failure sequence, and demonstrate that the proposed via array EM lifetime distribution trend correlates well with experimental results. 	
1801.09976v1	http://arxiv.org/pdf/1801.09976v1	2018	On the frequency of algebraic 2-torsion Brauer classes on an open degree   four del Pezzo surface	Jörg Jahnel|Damaris Schindler	  Given systems of two (inhomogeneous) quadratic equations in four variables, it is known that the Hasse principle for integral points may fail. Sometimes this failure can be explained by some integral Brauer-Manin obstruction. We study the existence of a non-trivial algebraic part of the Brauer group for a family of such systems and show that the failure of the integral Hasse principle due to an algebraic Brauer-Manin obstruction is rare, as for a generic choice of a system the algebraic part of the Brauer-group is trivial. We use resolvent constructions to give quantitative upper bounds on the number of exceptions. 	
1802.06114v1	http://arxiv.org/pdf/1802.06114v1	2018	Restricting the bi-equivariant spectral triple on quantum SU(2) to the   Podles spheres	Elmar Wagner	  It is shown that the isospectral bi-equivariant spectral triple on quantum SU(2) and the isospectral equivariant spectral triples on the Podles spheres are related by restriction. In this approach, the equatorial Podles sphere is distinguished because only in this case the restricted spectral triple admits an equivariant grading operator together with a real structure (up to infinitesimals of arbitrary high order). The real structure is expressed by the Tomita operator on quantum SU(2) and it is shown that the failure of the real structure to satisfy the commutant property is related to the failure of the universal R-matrix operator to be unitary. 	
1803.03155v1	http://arxiv.org/pdf/1803.03155v1	2018	Learning with Rules	Deborah Cohen|Amit Daniely|Amir Globerson|Gal Elidan	  Complex classifiers may exhibit "embarassing" failures in cases that would be easily classified and justified by a human. Avoiding such failures is obviously paramount, particularly in domains where we cannot accept this unexplained behavior. In this work, we focus on one such setting, where a label is perfectly predictable if the input contains certain features, and otherwise, it is predictable by a linear classifier. We define a related hypothesis class and determine its sample complexity. We also give evidence that efficient algorithms cannot, unfortunately, enjoy this sample complexity. We then derive a simple and efficient algorithm, and also give evidence that its sample complexity is optimal, among efficient algorithms. Experiments on sentiment analysis demonstrate the efficacy of the method, both in terms of accuracy and interpretability. 	
9302002v1	http://arxiv.org/pdf/alg-geom/9302002v1	1993	On the surjectivity of Wahl maps on a general curve	Roberto Paoletti	  This paper explores the geometric meaning of the failure of certain kinds of Wahl maps to surject on a general curve. Sufficient conditions for surjectivity are given. An approach used by Voisin to study canonical Wahl maps is applied in this direction. 	
0611059v1	http://arxiv.org/pdf/astro-ph/0611059v1	2006	Comments on ``An Exact, Three-Dimensional, Time-Dependent Wave Solution   in Local Keplerian Flow'' by Balbus and Hawley (astro-ph/0608429)	G. D. Chagelishvili|A. G. Tevzadze	  We analyze the scepticism on the hydrodynamic turbulence in Keplerian astrophysical disks expressed in Balbus and Hawley 2006 and show the failure of arguments of the paper. 	
9711102v1	http://arxiv.org/pdf/cs/9711102v1	1997	Storing and Indexing Plan Derivations through Explanation-based Analysis   of Retrieval Failures	L. H. Ihrig|S. Kambhampati	  Case-Based Planning (CBP) provides a way of scaling up domain-independent planning to solve large problems in complex domains. It replaces the detailed and lengthy search for a solution with the retrieval and adaptation of previous planning experiences. In general, CBP has been demonstrated to improve performance over generative (from-scratch) planning. However, the performance improvements it provides are dependent on adequate judgements as to problem similarity. In particular, although CBP may substantially reduce planning effort overall, it is subject to a mis-retrieval problem. The success of CBP depends on these retrieval errors being relatively rare. This paper describes the design and implementation of a replay framework for the case-based planner DERSNLP+EBL. DERSNLP+EBL extends current CBP methodology by incorporating explanation-based learning techniques that allow it to explain and learn from the retrieval failures it encounters. These techniques are used to refine judgements about case similarity in response to feedback when a wrong decision has been made. The same failure analysis is used in building the case library, through the addition of repairing cases. Large problems are split and stored as single goal subproblems. Multi-goal problems are stored only when these smaller cases fail to be merged into a full solution. An empirical evaluation of this approach demonstrates the advantage of learning from experienced retrieval failure. 	
0109069v1	http://arxiv.org/pdf/cs/0109069v1	2001	United States v. Microsoft: A Failure of Antitrust in the New Economy	Nicholas Economides	  This paper analyzes the law and economics of United States v. Microsoft, a landmark case of antitrust intervention in network industries. [abridged] 	
0406079v1	http://arxiv.org/pdf/math/0406079v1	2004	Failure of the Raikov theorem for free random variables	Florent Benaych-Georges	  We show that the sum of two free random variables can have a free Poisson law without any of them having a free Poisson law. 	
0606729v2	http://arxiv.org/pdf/math/0606729v2	2006	Failure of tameness for local cohomology	Steven Dale Cutkosky|Juergen Herzog	  We give an example that shows that not all local cohomology modules are tame in the sense of Brodmann and Hellus. 	
9711024v1	http://arxiv.org/pdf/physics/9711024v1	1997	Failure of Lorentz-Dirac Approach to Radiation Reaction for Extremely   Large Velocities ?	Alexander A. Vlasov	  On the model of moving rigid charged body is shown that the Lorentz-Dirac approach to radiation reaction in classical electrodynamics does not work for trajectories of body close to the light cone. 	
0604124v2	http://arxiv.org/pdf/quant-ph/0604124v2	2006	What's wrong with this rebuttal?	A. F. Kracklauer	  A recent rebuttal to criticism of Bell's analysis is shown to be defective by fault of failure to consider all hypothetical conditions input into the derivation of Bell Inequalitites. 	
0704.0102v1	http://arxiv.org/pdf/0704.0102v1	2007	Duality and Tameness	Marc Chardin|Steven Dale Cutkosky|Juergen Herzog|Hema Srinivasan	  We prove a duality theorem for certain graded algebras and show by various examples different kinds of failure of tameness of local cohomology. 	
0704.0954v1	http://arxiv.org/pdf/0704.0954v1	2007	Sensor Networks with Random Links: Topology Design for Distributed   Consensus	Soummya Kar|Jose M. F. Moura	  In a sensor network, in practice, the communication among sensors is subject to:(1) errors or failures at random times; (3) costs; and(2) constraints since sensors and networks operate under scarce resources, such as power, data rate, or communication. The signal-to-noise ratio (SNR) is usually a main factor in determining the probability of error (or of communication failure) in a link. These probabilities are then a proxy for the SNR under which the links operate. The paper studies the problem of designing the topology, i.e., assigning the probabilities of reliable communication among sensors (or of link failures) to maximize the rate of convergence of average consensus, when the link communication costs are taken into account, and there is an overall communication budget constraint. To consider this problem, we address a number of preliminary issues: (1) model the network as a random topology; (2) establish necessary and sufficient conditions for mean square sense (mss) and almost sure (a.s.) convergence of average consensus when network links fail; and, in particular, (3) show that a necessary and sufficient condition for both mss and a.s. convergence is for the algebraic connectivity of the mean graph describing the network topology to be strictly positive. With these results, we formulate topology design, subject to random link failures and to a communication cost constraint, as a constrained convex optimization problem to which we apply semidefinite programming techniques. We show by an extensive numerical study that the optimal design improves significantly the convergence speed of the consensus algorithm and can achieve the asymptotic performance of a non-random network at a fraction of the communication cost. 	
0709.0489v1	http://arxiv.org/pdf/0709.0489v1	2007	Corotating Interaction Regions and clumping	R. Blomme	  We present hydrodynamical models for Corotating Interaction Regions, which were used by Lobel (2007) to model the Discrete Absorption Components in HD 64760. We also discuss our failure to model the rotational modulations seen in the same star. 	
0902.1120v1	http://arxiv.org/pdf/0902.1120v1	2009	Tight closure's failure to localize - a self-contained exposition	Paul Monsky	  We give a treatment of the Brenner-Monsky example based on polynomial algebra and linear algebra. No prior knowledge of tight closure theory, Hilbert-Kunz theory, algebraic geometry or local cohomology is assumed. 	
0905.0736v1	http://arxiv.org/pdf/0905.0736v1	2009	Tensile Deformation and Failure of Thin Films of Aging Laponite   Suspension	Asima Shaukat|Yogesh M. Joshi|Ashutosh Sharma	  In this paper we study deformation, failure and breakage of visco-elastic thin films of aging laponite suspension under tensile deformation field. Aqueous suspension of laponite is known to undergo waiting time dependent evolution of its micro-structure, also known as aging, which is accompanied by an increase in the elastic modulus and relaxation time. In the velocity controlled tensile deformation experiments, we observed that the dependence of force and dissipated energy on velocity and initial thickness of the film is intermediate to a Newtonian fluid and a yield stress fluid. For a fixed waiting time, strain at break and dissipated energy increased with velocity, but decreased with initial thickness. With increase in age, strain at break and dissipated energy showed a decrease suggesting enhanced brittle behavior with increase in waiting time, which may be caused by restricted relaxation modes due to aging. In a force controlled mode, decrease in strain at failure at higher age also suggested enhanced brittleness with increase in waiting time. Remarkably, the constant force tensile deformation data up to the point of failure showed experimental time- aging time superposition that gave an independent estimation of relaxation time and elastic modulus dependence on age. 	
0905.3754v1	http://arxiv.org/pdf/0905.3754v1	2009	Club-guessing, stationary reflection, and coloring theorems	Todd Eisworth	  We obtain strong coloring theorems at successors of singular cardinals from failures of certain instances of simultaneous reflection of stationary sets. Along the way, we establish new results in club-guessing and in the general theory of ideals. 	
0906.0007v1	http://arxiv.org/pdf/0906.0007v1	2009	Invariant CR Mappings	John P. D'Angelo	  We summarize some work on CR mappings invariant under a subgroup of U(n) and prove a result on the failure of rigidity. 	
1007.0409v1	http://arxiv.org/pdf/1007.0409v1	2010	A Performance Comparison of Stability, Load-Balancing and Power-Aware   Routing Protocols for Mobile Ad Hoc Networks	Natarajan Meghanathan|Leslie Milton	  The high-level contribution of this paper is a simulation-based detailed performance comparison of three different classes of routing protocols for mobile ad hoc networks: stability-based routing, power-aware routing and load-balanced routing. We choose the Flow-Oriented Routing protocol (FORP), the traffic interference based Load Balancing Routing (LBR) protocol and Min-Max Battery Cost Routing (MMBCR) as representatives of the stability-based routing, load-balancing and power-aware routing protocols respectively. Among the three routing protocols, FORP incurs the least number of route transitions; while LBR incurs the smallest hop count and lowest end-to-end delay per data packet. Energy consumed per node is the least for MMBCR, closely followed by LBR. MMBCR is the most fair in terms of node usage and hence it incurs the largest time for first node failure. FORP tends to repeatedly use nodes lying on the stable path and hence is the most unfair of the three routing protocols and it incurs the smallest value for the time of first node failure. As we measure the failure times of up to the first five nodes in the network, we observe that LBR incurs the maximum improvement in the lifetime of the nodes and MMBCR incurs the least improvement beyond the time of first node failure. 	
1007.5448v1	http://arxiv.org/pdf/1007.5448v1	2010	Probability of Failure of Safety-Critical Systems Subject to Partial   Tests	Florent Brissaud|Anne Barros|Christophe Bérenguer	  A set of general formulas is proposed for the probability of failure on demand (PFD) assessment of MooN architecture (i.e. k-out-of-n) systems subject to proof tests. The proof tests can be partial or full. The partial tests (e.g. visual inspections, partial stroke testing) are able to detect only some system failures and leave the others latent, whereas the full tests refer to overhauls which restore the system to an as good as new condition. Partial tests may occur at different time instants (periodic or not), up to the full test. The system performances which are investigated are the system availability according to time, the PFD average in each partial test time interval, and the total PFD average calculated on the full test time interval. Following the given expressions, parameter estimations are proposed to assess the system failure rates and the partial test effectiveness according to feedback data from previous test policies. Subsequently, an optimization of the partial test strategy is presented. In the 2oo6 system given as example, an improvement of about 10% of the total PFD average has been obtained, just by a better (non-periodic) distribution of the same number of partial tests, in the full test time interval. 	
1010.4498v1	http://arxiv.org/pdf/1010.4498v1	2010	The critical effect of dependency groups on the function of networks	Roni Parshani|Sergey V. Buldyrev|Shlomo Havlin	  Current network models assume one type of links to define the relations between the network entities. However, many real networks can only be correctly described using two different types of relations. Connectivity links that enable the nodes to function cooperatively as a network and dependency links that bind the failure of one network element to the failure of other network elements. Here we present for the first time an analytical framework for studying the robustness of networks that include both connectivity and dependency links. We show that the synergy between the two types of failures leads to an iterative process of cascading failures that has a devastating effect on the network stability and completely alters the known assumptions regarding the robustness of networks. We present exact analytical results for the dramatic change in the network behavior when introducing dependency links. For a high density of dependency links the network disintegrates in a form of a first order phase transition while for a low density of dependency links the network disintegrates in a second order transition. Moreover, opposed to networks containing only connectivity links where a broader degree distribution results in a more robust network, when both types of links are present a broad degree distribution leads to higher vulnerability. 	
1112.6124v1	http://arxiv.org/pdf/1112.6124v1	2011	Kinds of concepts	Nik Weaver	  The central focus is on clarifying the distinction between sets and proper classes. To this end we identify several categories of concepts (surveyable, definite, indefinite), and we attribute the classical set theoretic paradoxes to a failure to appreciate the distinction between surveyability and definiteness. 	
1204.5788v2	http://arxiv.org/pdf/1204.5788v2	2012	Intuitionistic predicate logic of constant domains does not have Beth   property	Grigory K. Olkhovikov	  Drawing on the previous work on interpolation failure, we show that Beth's definability theorem does not hold for intuitionistic predicate logic of constant domains without identity. 	
1208.6406v1	http://arxiv.org/pdf/1208.6406v1	2012	Building Resilient Cloud Over Unreliable Commodity Infrastructure	Piyus Kedia|Sorav Bansal|Deepak Deshpande|Sreekanth Iyer	  Cloud Computing has emerged as a successful computing paradigm for efficiently utilizing managed compute infrastructure such as high speed rack-mounted servers, connected with high speed networking, and reliable storage. Usually such infrastructure is dedicated, physically secured and has reliable power and networking infrastructure. However, much of our idle compute capacity is present in unmanaged infrastructure like idle desktops, lab machines, physically distant server machines, and laptops. We present a scheme to utilize this idle compute capacity on a best-effort basis and provide high availability even in face of failure of individual components or facilities.   We run virtual machines on the commodity infrastructure and present a cloud interface to our end users. The primary challenge is to maintain availability in the presence of node failures, network failures, and power failures. We run multiple copies of a Virtual Machine (VM) redundantly on geographically dispersed physical machines to achieve availability. If one of the running copies of a VM fails, we seamlessly switchover to another running copy. We use Virtual Machine Record/Replay capability to implement this redundancy and switchover. In current progress, we have implemented VM Record/Replay for uniprocessor machines over Linux/KVM and are currently working on VM Record/Replay on shared-memory multiprocessor machines. We report initial experimental results based on our implementation. 	
1209.2058v2	http://arxiv.org/pdf/1209.2058v2	2012	Safe and Stabilizing Distributed Multi-Path Cellular Flows	Taylor T. Johnson|Sayan Mitra	  We study the problem of distributed traffic control in the partitioned plane, where the movement of all entities (robots, vehicles, etc.) within each partition (cell) is coupled. Establishing liveness in such systems is challenging, but such analysis will be necessary to apply such distributed traffic control algorithms in applications like coordinating robot swarms and the intelligent highway system. We present a formal model of a distributed traffic control protocol that guarantees minimum separation between entities, even as some cells fail. Once new failures cease occurring, in the case of a single target, the protocol is guaranteed to self-stabilize and the entities with feasible paths to the target cell make progress towards it. For multiple targets, failures may cause deadlocks in the system, so we identify a class of non-deadlocking failures where all entities are able to make progress to their respective targets. The algorithm relies on two general principles: temporary blocking for maintenance of safety and local geographical routing for guaranteeing progress. Our assertional proofs may serve as a template for the analysis of other distributed traffic control protocols. We present simulation results that provide estimates of throughput as a function of entity velocity, safety separation, single-target path complexity, failure-recovery rates, and multi-target path complexity. 	
1210.6382v2	http://arxiv.org/pdf/1210.6382v2	2013	Data Survivability in Networks of Mobile Robots in Urban Disaster   Environments	Nicolas Kourtellis|Adriana Iamnitchi|Cristian Borcea|Robin Murphy	  Mobile multi-robot teams deployed for monitoring or search-and-rescue missions in urban disaster areas can greatly improve the quality of vital data collected on-site. Analysis of such data can identify hazards and save lives. Unfortunately, such real deployments at scale are cost prohibitive and robot failures lead to data loss. Moreover, scaled-down deployments do not capture significant levels of interaction and communication complexity. To tackle this problem, we propose novel mobility and failure generation frameworks that allow realistic simulations of mobile robot networks for large scale disaster scenarios. Furthermore, since data replication techniques can improve the survivability of data collected during the operation, we propose an adaptive, scalable data replication technique that achieves high data survivability with low overhead. Our technique considers the anticipated robot failures and robot heterogeneity to decide how aggressively to replicate data. In addition, it considers survivability priorities, with some data requiring more effort to be saved than others. Using our novel simulation generation frameworks, we compare our adaptive technique with flooding and broadcast-based replication techniques and show that for failure rates of up to 60% it ensures better data survivability with lower communication costs. 	
1212.0131v1	http://arxiv.org/pdf/1212.0131v1	2012	Reflexivity and connectedness	Sean Sather-Wagstaff	  Given a finitely generated module over a commutative noetherian ring that satisfies certain reflexivity conditions, we show how failure of the semidualizing property for the module manifests in a disconnection of the prime spectrum of the ring. 	
1304.3741v1	http://arxiv.org/pdf/1304.3741v1	2013	Cascade sizes in a branching process with Gamma distributed generations	James Burridge	  We derive an exact expression for the probability density function of the cascade size (total progeny) in a continuous state branching process when the generations are Gamma distributed. The distribution has application in the modelling of cascade processes such as landslides and electrical network failures. 	
1304.3839v4	http://arxiv.org/pdf/1304.3839v4	2015	Parameter estimation in Cox models with missing failure indicators and   the OPPERA study	Naomi Brownstein|Jianwen Cai|Gary Slade|Eric Bair	  In a prospective cohort study, examining all participants for incidence of the condition of interest may be prohibitively expensive. For example, the "gold standard" for diagnosing temporomandibular disorder (TMD) is a physical examination by a trained clinician. In large studies, examining all participants in this manner is infeasible. Instead, it is common to use questionnaires to screen for incidence of TMD and perform the "gold standard" examination only on participants who screen positively. Unfortunately, some participants may leave the study before receiving the "gold standard" examination. Within the framework of survival analysis, this results in missing failure indicators. Motivated by the Orofacial Pain: Prospective Evaluation and Risk Assessment (OPPERA) study, a large cohort study of TMD, we propose a method for parameter estimation in survival models with missing failure indicators. We estimate the probability of being an incident case for those lacking a "gold standard" examination using logistic regression. These estimated probabilities are used to generate multiple imputations of case status for each missing examination that are combined with observed data in appropriate regression models. The variance introduced by the procedure is estimated using multiple imputation. The method can be used to estimate both regression coefficients in Cox proportional hazard models as well as incidence rates using Poisson regression. We simulate data with missing failure indicators and show that our method performs as well as or better than competing methods. Finally, we apply the proposed method to data from the OPPERA study. 	
1310.2381v3	http://arxiv.org/pdf/1310.2381v3	2014	MDR Codes: A New Class of RAID-6 Codes with Optimal Rebuilding and   Encoding	Yan Wang|Xunrui Yin|Xin Wang	  As storage systems grow in size, device failures happen more frequently than ever before. Given the commodity nature of hard drives employed, a storage system needs to tolerate a certain number of disk failures while maintaining data integrity, and to recover lost data with minimal interference to normal disk I/O operations. RAID-6, which can tolerate up to two disk failures with the minimum redundancy, is becoming widespread. However, traditional RAID-6 codes suffer from high disk I/O overhead during recovery. In this paper, we propose a new family of RAID-6 codes, the Minimum Disk I/O Repairable (MDR) codes, which achieve the optimal disk I/O overhead for single failure recoveries. Moreover, we show that MDR codes can be encoded with the minimum number of bit-wise XOR operations. Simulation results show that MDR codes help to save about half of disk read operations than traditional RAID-6 codes, and thus can reduce the recovery time by up to 40%. 	
1310.6345v3	http://arxiv.org/pdf/1310.6345v3	2014	Triple point induced by targeted autonomization on interdependent scale   free networks	L. D. Valdez|P. A. Macri|L. A. Braunstein	  Many man-made networks support each other to provide efficient services and resources to the customers, despite that this support produces a strong interdependency between the individual networks. Thus an initial failure of a fraction $1-p$ of nodes in one network, exposes the system to cascade of failures and, as a consequence, to a full collapse of the overall system. Therefore it is important to develop efficient strategies to avoid the collapse by increasing the robustness of the individual networks against failures. Here, we provide an exact theoretical approach to study the evolution of the cascade of failures on interdependent networks when a fraction $\alpha$ of the nodes with higher connectivity in each individual network are autonomous. With this pattern of interdependency we found, for pair of heterogeneous networks, two critical percolation thresholds that depend on $\alpha$, separating three regimes with very different network's final sizes that converge into a triple point in the plane $p-\alpha$. Our findings suggest that the heterogeneity of the networks represented by high degree nodes is the responsible of the rich phase diagrams found in this and other investigations. 	
1311.6949v1	http://arxiv.org/pdf/1311.6949v1	2013	Distributed Power Loss Minimization in Residential Micro Grids: a   Communications Perspective	Riccardo Bonetto|Stefano Tomasin|Michele Rossi	  The constantly increasing number of power generation devices based on renewables is calling for a transition from the centralized control of electrical distribution grids to a distributed control scenario. In this context, distributed generators (DGs) are exploited to achieve other objectives beyond supporting loads, such as the minimization of the power losses along the distribution lines. The aim of this work is that of designing a full-fledged system that extends existing state of the art algorithms for the distributed minimization of power losses. We take into account practical aspects such as the design of a communication and coordination protocol that is resilient to link failures and manages channel access, message delivery and DG coordination. Thus, we analyze the performance of the resulting optimization and communication scheme in terms of power loss reduction, reduction of aggregate power demand, convergence rate and resilience to communication link failures. After that, we discuss the results of a thorough simulation campaign, obtained using topologies generated through a statistical approach that has been validated in previous research, by also assessing the performance deviation with respect to localized schemes, where the DGs are operated independently. Our results reveal that the convergence and stability performance of the selected algorithms vary greatly. However, configurations exist for which convergence is possible within five to ten communication steps and, when just 30% of the nodes are DGs, the aggregate power demand is roughly halved. Also, some of the considered approaches are quite robust against link failures as they still provide gains with respect to the localized solutions for failure rates as high as 50%. 	
1401.3936v2	http://arxiv.org/pdf/1401.3936v2	2014	Cyber Security of Smart Grid Infrastructure	Adnan Anwar|Abdun Naser Mahmood	  Smart grid security is crucial to maintain stable and reliable power system operation during the contingency situation due to the failure of any critical power system component. Ensuring a secured smart grid involves with a less possibility of power grid collapse or equipment malfunction. Due to lack of the proper security measures, a major blackout may occur which can even lead to a cascading failure. Therefore, to protect this critical power system infrastructure and to ensure a reliable and an uninterrupted power supply to the end users, smart grid security issues must be addressed with high priority. In a smart grid environment, electric power infrastructure is modernized by incorporating the current and future requirements and advanced functionalities to its consumers. To make the smart grid happen, cyber system is integrated with the physical power system. Although adoption of cyber system has made the grid more energy efficient and modernized, it has introduced cyber-attack issues which are critical for national infrastructure security and customer satisfaction. Due to the cyber-attack, power grid may face operational failures and loss of synchronization. This operational failure may damage critical power system components which may interrupt the power supply and make the system unstable resulting high financial penalties. In this chapter, some recent cyber attack related incidents into a smart grid environment are discussed. The requirements and the state of the art of cyber security issues of a critical power system infrastructure are illustrated elaborately. 	
1404.2403v2	http://arxiv.org/pdf/1404.2403v2	2014	Robustness surfaces of complex networks	Marc Manzano|Faryad Sahneh|Caterina Scoglio|Eusebi Calle|Jose Luis Marzo	  Despite the robustness of complex networks has been extensively studied in the last decade, there still lacks a unifying framework able to embrace all the proposed metrics. In the literature there are two open issues related to this gap: (a) how to dimension several metrics to allow their summation and (b) how to weight each of the metrics. In this work we propose a solution for the two aforementioned problems by defining the $R^*$-value and introducing the concept of \emph{robustness surface} ($\Omega$). The rationale of our proposal is to make use of Principal Component Analysis (PCA). We firstly adjust to 1 the initial robustness of a network. Secondly, we find the most informative robustness metric under a specific failure scenario. Then, we repeat the process for several percentage of failures and different realizations of the failure process. Lastly, we join these values to form the robustness surface, which allows the visual assessment of network robustness variability. Results show that a network presents different robustness surfaces (i.e., dissimilar shapes) depending on the failure scenario and the set of metrics. In addition, the robustness surface allows the robustness of different networks to be compared. 	
1404.6844v1	http://arxiv.org/pdf/1404.6844v1	2014	Evaluating the Assessment of Software Fault-Freeness	John Rushby|Bev Littlewood|Lorenzo Strigini	  We propose to validate experimentally a theory of software certification that proceeds from assessment of confidence in fault-freeness (due to standards) to conservative prediction of failure-free operation. 	
1405.0786v1	http://arxiv.org/pdf/1405.0786v1	2014	Fault Localization in a Software Project using Back-Tracking Principles   of Matrix Dependency	Vishal Anand|Ramani S	  Fault identification and testing has always been the most specific concern in the field of software development. To identify and testify the bug we should be aware of the source of the failure or any unwanted issue. In this paper, we are trying to extract the location of failure and trying to cope up with the bug. Using directed graph, we tried to obtain the dependency of multiple activities in live environment to trace the origin of fault. Software development comes up with series of activities and we tried to show the dependency of multiple activities on each other. Critical activities are considered as they cause abnormal functioning of the whole system. The paper discuss about the priorities of activities of dependency of software failure on the critical activities. Matrix representation of activities as part of the software is chosen to determine root of the failure using concept of dependency. It can vary with the topography of network and software environment. When faults occur, the possible symptoms will be reflected in the dependency matrix with high probability in fault itself. Thus, independent faults are located in the main diagonal of dependency matrix. 	
1405.4213v2	http://arxiv.org/pdf/1405.4213v2	2014	Estimating Cascading Failure Risk with Random Chemistry	Pooya Rezaei|Paul D. H. Hines|Margaret J. Eppstein	  The potential for cascading failure in power systems adds substantially to overall reliability risk. Monte Carlo sampling can be used with a power system model to estimate this impact, but doing so is computationally expensive. This paper presents a new approach to estimating the risk of large cascading blackouts triggered by multiple contingencies. The method uses a search algorithm (Random Chemistry) to identify blackout-causing contingencies, and then combines the results with outage probabilities to estimate overall risk. Comparing this approach with Monte Carlo sampling for two test cases (the IEEE RTS-96 and a 2383 bus model of the Polish system) illustrates that the new approach is at least two orders of magnitude faster than Monte Carlo, without introducing measurable bias. Moreover, the approach enables one to compute the sensitivity of overall blackout risk to individual component-failure probabilities in the initiating contingency, allowing one to quickly identify low-cost strategies for reducing risk. By computing the sensitivity of risk to individual initial outage probabilities for the Polish system, we found that reducing three line-outage probabilities by 50% would reduce cascading failure risk by 33%. Finally, we used the method to estimate changes in risk as a function of load. Surprisingly, this calculation illustrates that risk can sometimes decrease as load increases. 	
1408.4262v1	http://arxiv.org/pdf/1408.4262v1	2014	Failures of the Silver Dichotomy in the Generalised Baire Space	Sy-David Friedman|Vadim Kulikov	  We prove results that falsify Silver's dichotomy for Borel equivalence relations on the generalised Baire space under the assumption V=L. 	
1408.4780v1	http://arxiv.org/pdf/1408.4780v1	2014	Simple explanation on why QKD keys have not been proved secure	Horace Yuen	  A simple counter-example is given on the prevalent interpretation of the trace distance criterion as failure probability in quantum key distribution protocols. A summary of its ramifications is listed. 	
1408.7035v2	http://arxiv.org/pdf/1408.7035v2	2015	Cooperation with Disagreement Correction in the Presence of   Communication Failures	Oscar Morales-Ponce|Elad M. Schiller|Paolo Falcone	  Vehicle-to-vehicle communication is a fundamental requirement in cooperative vehicular systems to achieve high performance while keeping high safety standards. Vehicles periodically exchange critical information with nearby vehicles to determine their maneuvers according to the information quality and established strategies. However, wireless communication is prone to failures. Thus, participants can be unaware that other participants have not received the information on time resulting in conflicting trajectories that may not be safe. We present a deterministic solution that allows all participants to use a default strategy when other participants have not received on time the complete information. We base our solution on a timed distributed protocol that adapts its output according to the effect of message omission failures so that the disagreement period occurs for no longer than a constant time (of the order of milliseconds) that only depends on the message delay. We formally show the correctness and perform experiments to corroborate its efficiency. We explain how the proposed solution can be used on vehicular platooning to attain high performance and still guarantee high safety standards despite communication failures. We believe that this work can facilitate the implementation of cooperative driving systems that have to deal with inherent (communication) uncertainties. 	
1411.7775v1	http://arxiv.org/pdf/1411.7775v1	2014	The proportion of failures of the Hasse norm principle	Tim Browning|Rachel Newton	  For any number field we calculate the exact proportion of rational numbers which are everywhere locally a norm but not globally a norm from the number field. 	
1501.01093v1	http://arxiv.org/pdf/1501.01093v1	2015	Failure of flat descent of model structures on module categories	Andrew Salch	  We prove that the collection of model structures on (quasicoherent) module categories does not obey flat descent. In particular, it fails to be a separated presheaf, in the fppf topology, on Artin stacks. 	
1503.03211v1	http://arxiv.org/pdf/1503.03211v1	2015	A Multi-Gene Genetic Programming Application for Predicting Students   Failure at School	J. O. Orove|N. E. Osegi|B. O. Eke	  Several efforts to predict student failure rate (SFR) at school accurately still remains a core problem area faced by many in the educational sector. The procedure for forecasting SFR are rigid and most often times require data scaling or conversion into binary form such as is the case of the logistic model which may lead to lose of information and effect size attenuation. Also, the high number of factors, incomplete and unbalanced dataset, and black boxing issues as in Artificial Neural Networks and Fuzzy logic systems exposes the need for more efficient tools. Currently the application of Genetic Programming (GP) holds great promises and has produced tremendous positive results in different sectors. In this regard, this study developed GPSFARPS, a software application to provide a robust solution to the prediction of SFR using an evolutionary algorithm known as multi-gene genetic programming. The approach is validated by feeding a testing data set to the evolved GP models. Result obtained from GPSFARPS simulations show its unique ability to evolve a suitable failure rate expression with a fast convergence at 30 generations from a maximum specified generation of 500. The multi-gene system was also able to minimize the evolved model expression and accurately predict student failure rate using a subset of the original expression 	
1505.07661v1	http://arxiv.org/pdf/1505.07661v1	2015	Reactive point processes: A new approach to predicting power failures in   underground electrical systems	Şeyda Ertekin|Cynthia Rudin|Tyler H. McCormick	  Reactive point processes (RPPs) are a new statistical model designed for predicting discrete events in time based on past history. RPPs were developed to handle an important problem within the domain of electrical grid reliability: short-term prediction of electrical grid failures ("manhole events"), including outages, fires, explosions and smoking manholes, which can cause threats to public safety and reliability of electrical service in cities. RPPs incorporate self-exciting, self-regulating and saturating components. The self-excitement occurs as a result of a past event, which causes a temporary rise in vulner ability to future events. The self-regulation occurs as a result of an external inspection which temporarily lowers vulnerability to future events. RPPs can saturate when too many events or inspections occur close together, which ensures that the probability of an event stays within a realistic range. Two of the operational challenges for power companies are (i) making continuous-time failure predictions, and (ii) cost/benefit analysis for decision making and proactive maintenance. RPPs are naturally suited for handling both of these challenges. We use the model to predict power-grid failures in Manhattan over a short-term horizon, and to provide a cost/benefit analysis of different proactive maintenance programs. 	
1509.00509v1	http://arxiv.org/pdf/1509.00509v1	2015	Disaster-Resilient Control Plane Design and Mapping in Software-Defined   Networks	S. Sedef Savas|Massimo Tornatore|M. Farhan Habib|Pulak Chowdhury|Biswanath Mukherjee	  Communication networks, such as core optical networks, heavily depend on their physical infrastructure, and hence they are vulnerable to man-made disasters, such as Electromagnetic Pulse (EMP) or Weapons of Mass Destruction (WMD) attacks, as well as to natural disasters. Large-scale disasters may cause huge data loss and connectivity disruption in these networks. As our dependence on network services increases, the need for novel survivability methods to mitigate the effects of disasters on communication networks becomes a major concern. Software-Defined Networking (SDN), by centralizing control logic and separating it from physical equipment, facilitates network programmability and opens up new ways to design disaster-resilient networks. On the other hand, to fully exploit the potential of SDN, along with data-plane survivability, we also need to design the control plane to be resilient enough to survive network failures caused by disasters. Several distributed SDN controller architectures have been proposed to mitigate the risks of overload and failure, but they are optimized for limited faults without addressing the extent of large-scale disaster failures. For disaster resiliency of the control plane, we propose to design it as a virtual network, which can be solved using Virtual Network Mapping techniques. We select appropriate mapping of the controllers over the physical network such that the connectivity among the controllers (controller-to-controller) and between the switches to the controllers (switch-to-controllers) is not compromised by physical infrastructure failures caused by disasters. We formally model this disaster-aware control-plane design and mapping problem, and demonstrate a significant reduction in the disruption of controller-to-controller and switch-to-controller communication channels using our approach. 	
1509.02416v1	http://arxiv.org/pdf/1509.02416v1	2015	Coupling a reactive potential with a harmonic approximation for   atomistic simulations of material failure	Ignacio Tejada|Laurent Brochard|Tony Lelievre|Gabriel Stoltz|Frederic Legoll|Eric Cances	  Molecular dynamics (MD) simulations involving reactive potentials can be used to model material failure. The empirical potentials which are used in such simulations are able to adapt to the atomic environment, at the expense of a significantly higher computational cost than non-reactive potentials. However, during a simulation of failure, the reactive ability is needed only in some limited parts of the system, where bonds break or form and the atomic environment changes. Therefore, simpler non-reactive potentials can be used in the remainder of the system, provided that such potentials reproduce correctly the behavior of the reactive potentials in this region, and that seamless coupling is ensured at the interface between the reactive and non-reactive regions. In this article, we propose a methodology to combine a reactive potential with a non-reactive approximation thereof, made of a set of harmonic pair and angle interactions and whose parameters are adjusted to predict the same energy, geometry and Hessian in the ground state of the potential. We present a methodology to construct the non-reactive approximation of the reactive potential, and a way to couple these two potentials. We also propose a criterion for on-the-fly substitution of the reactive potential by its non-reactive approximation during a simulation. We illustrate the correctness of this hybrid technique for the case of MD simulation of failure in two-dimensional graphene originally modeled with REBO potential. 	
1510.02935v1	http://arxiv.org/pdf/1510.02935v1	2015	Shelah's strong covering property and CH in V[r]	Esfandiar Eslami|Mohammad Golshani	  In this paper we review Shelah's strong covering property and its applications. We also extend some of the results of Shelah and Woodin on the failure of $CH$ by adding a real. 	
1601.01739v1	http://arxiv.org/pdf/1601.01739v1	2016	Photometric Redshift Estimation for Quasars by Integration of KNN and   SVM	Bo Han|Hongpeng Ding|Yanxia Zhang|Yongheng Zhao	  The massive photometric data collected from multiple large-scale sky surveys offer significant opportunities for measuring distances of celestial objects by photometric redshifts. However, catastrophic failure is still an unsolved problem for a long time and exists in the current photometric redshift estimation approaches (such as $k$-nearest-neighbor). In this paper, we propose a novel two-stage approach by integration of $k$-nearest-neighbor (KNN) and support vector machine (SVM) methods together. In the first stage, we apply KNN algorithm on photometric data and estimate their corresponding z$_{\rm phot}$. By analysis, we find two dense regions with catastrophic failure, one in the range of z$_{\rm phot}\in[0.3,1.2]$, the other in the range of z$_{\rm phot}\in [1.2,2.1]$. In the second stage, we map the photometric input pattern of points falling into the two ranges from original attribute space into a high dimensional feature space by Gaussian kernel function in SVM. In the high dimensional feature space, many outlier points resulting from catastrophic failure by simple Euclidean distance computation in KNN can be identified by a classification hyperplane of SVM and further be corrected. Experimental results based on the SDSS (the Sloan Digital Sky Survey) quasar data show that the two-stage fusion approach can significantly mitigate catastrophic failure and improve the estimation accuracy of photometric redshifts of quasars. The percents in different |$\Delta$z| ranges and rms (root mean square) error by the integrated method are $83.47\%$, $89.83\%$, $90.90\%$ and 0.192, respectively, compared to the results by KNN ($71.96\%$, $83.78\%$, $89.73\%$ and 0.204). 	
1605.06927v1	http://arxiv.org/pdf/1605.06927v1	2016	MDS Codes with Progressive Engagement Property for Cloud Storage Systems	Mahdi Hajiaghayi|Hamid Jafarkhani	  Fast and efficient failure recovery is a new challenge for cloud storage systems with a large number of storage nodes. A pivotal recovery metric upon the failure of a storage node is repair bandwidth cost which refers to the amount of data that must be downloaded for regenerating the lost data. Since all the surviving nodes are not always accessible, we intend to introduce a class of maximum distance separable (MDS) codes that can be re-used when the number of selected nodes varies yet yields close to optimal repair bandwidth. Such codes provide flexibility in engaging more surviving nodes in favor of reducing the repair bandwidth without redesigning the code structure and changing the content of the existing nodes. We call this property of MDS codes progressive engagement. This name comes from the fact that if a failure occurs, it is shown that the best strategy is to incrementally engage the surviving nodes according to their accessing cost (delay, number of hops, traffic load or availability in general) until the repair-bandwidth or accessing cost constraints are met. We argue that the existing MDS codes fail to satisfy the progressive engagement property. We subsequently present a search algorithm to find a new set of codes named rotation codes that has both progressive engagement and MDS properties. Furthermore, we illustrate how the existing permutation codes can provide progressive engagement by modifying the original recovery scheme. Simulation results are presented to compare the repair bandwidth performance of such codes when the number of participating nodes varies as well as their speed of single failure recovery. 	
1606.03494v3	http://arxiv.org/pdf/1606.03494v3	2016	Failure-recovery model with competition between failures in complex   networks: a dynamical approach	L. D. Valdez|M. A. Di Muro|L. A. Braunstein	  Real systems are usually composed by units or nodes whose activity can be interrupted and restored intermittently due to complex interactions not only with the environment, but also with the same system. Majdand\v{z}i\'c $et\;al.$ [Nature Physics 10, 34 (2014)] proposed a model to study systems in which active nodes fail and recover spontaneously in a complex network and found that in the steady state the density of active nodes can exhibit an abrupt transition and hysteresis depending on the values of the parameters. Here we investigate a model of recovery-failure from a dynamical point of view. Using an effective degree approach we find that the systems can exhibit a temporal sharp decrease in the fraction of active nodes. Moreover we show that, depending on the values of the parameters, the fraction of active nodes has an oscillatory regime which we explain as a competition between different failure processes. We also find that in the non-oscillatory regime, the critical fraction of active nodes presents a discontinuous drop which can be related to a "targeted" k-core percolation process. Finally, using mean field equations we analyze the space of parameters at which hysteresis and oscillatory regimes can be found. 	
1606.05735v2	http://arxiv.org/pdf/1606.05735v2	2016	A Comparative Analysis of classification data mining techniques :   Deriving key factors useful for predicting students performance	Muhammed Salman Shamsi|Jhansi Lakshmi	  Students opting for Engineering as their discipline is increasing rapidly. But due to various factors and inappropriate primary education in India, failure rates are high. Students are unable to excel in core engineering because of complex and mathematical subjects. Hence, they fail in such subjects. With the help of data mining techniques, we can predict the performance of students in terms of grades and failure in subjects. This paper performs a comparative analysis of various classification techniques, such as Na\"ive Bayes, LibSVM, J48, Random Forest, and JRip and tries to choose best among these. Based on the results obtained, we found that Na\"ive Bayes is the most accurate method in terms of students failure prediction and JRip is most accurate in terms of students grade prediction. We also found that JRip marginally differs from Na\"ive Bayes in terms of accuracy for students failure prediction and gives us a set of rules from which we derive the key factors influencing students performance. Finally, we suggest various ways to mitigate these factors. This study is limited to Indian Education system scenarios. However, the factors found can be helpful in other scenarios as well. 	
1607.00751v2	http://arxiv.org/pdf/1607.00751v2	2016	(Weak) diamond can fail at the least inaccessible cardinal	Mohammad Golshani	  Starting from suitable large cardinals, we force the failure of (weak) diamond at the least inaccessible cardinal. The result improves an unpublished theorem of Woodin and a recent result of Ben-Neria, Garti and Hayut. 	
1608.04155v1	http://arxiv.org/pdf/1608.04155v1	2016	Resilience of Locally Routed Network Flows: More Capacity is Not Always   Better	A. Yasin Yazicioglu|Mardavij Roozbehani|Munther A. Dahleh	  In this paper, we are concerned with the resilience of locally routed network flows with finite link capacities. In this setting, an external inflow is injected to the so-called origin nodes. The total inflow arriving at each node is routed locally such that none of the outgoing links are overloaded unless the node receives an inflow greater than its total outgoing capacity. A link irreversibly fails if it is overloaded or if there is no operational link in its immediate downstream to carry its flow. For such systems, resilience is defined as the minimum amount of reduction in the link capacities that would result in the failure of all the outgoing links of an origin node. We show that such networks do not necessarily become more resilient as additional capacity is built in the network. Moreover, when the external inflow does not exceed the network capacity, selective reductions of capacity at certain links can actually help averting the cascading failures, without requiring any change in the local routing policies. This is an attractive feature as it is often easier in practice to reduce the available capacity of some critical links than to add physical capacity or to alter routing policies, e.g., when such policies are determined by social behavior, as in the case of road traffic networks. The results can thus be used for real-time monitoring of distance-to-failure in such networks and devising a feasible course of actions to avert systemic failures. 	
1609.05000v1	http://arxiv.org/pdf/1609.05000v1	2016	Sturm's operator for scalar weight in arbitrary genus	Kathrin Maurischat	  In contrast to the wellknown cases of large weights, Sturm's operator does not realize the holomorphic projection operator for lower weights. We prove its failure for arbitrary Siegel genus $m\geq 2$ and scalar weight $\kappa=m+1$. This generalizes a result for genus two in [4]. 	
1609.06700v1	http://arxiv.org/pdf/1609.06700v1	2016	Resilient Operation of Transportation Networks via Variable Speed Limits	A. Yasin Yazicioglu|Mardavij Roozbehani|Munther A. Dahleh	  In this paper, we investigate the use of variable speed limits for resilient operation of transportation networks, which are modeled as dynamical flow networks under local routing decisions. In such systems, some external inflow is injected to the so-called origin nodes of the network. The total inflow arriving at each node is routed to its operational outgoing links based on their current particle densities. The density on each link has first order dynamics driven by the difference of its incoming and outgoing flows. A link irreversibly fails if it reaches its jam density. Such failures may propagate in the network and cause a systemic failure. We show that larger link capacities do not necessarily help in preventing systemic failures under local routing. Accordingly, we propose the use of variable speed limits to operate the links below their capacities, when necessary, to compensate for the lack of global information and coordination in routing decisions. Our main result shows that systemic failures under feasible external inflows can always be averted through a proper selection of speed limits if the routing decisions are sufficiently responsive to local congestion and the network is initially uncongested. This is an attractive feature as it is much easier in practice to adjust the speed limits than to build more physical capacity or to alter routing decisions that are determined by social behavior. 	
1611.05651v1	http://arxiv.org/pdf/1611.05651v1	2016	A Theory of Available-by-Design Communicating Systems	Hugo A. López|Flemming Nielson|Hanne Riis Nielson	  Choreographic programming is a programming-language design approach that drives error-safe protocol development in distributed systems. Starting from a global specification (choreography) one can generate distributed implementations. The advantages of this top-down approach lie in the correctness-by-design principle, where implementations (endpoints) generated from a choreography behave according to the strict control flow described in the choreography, and do not deadlock. Motivated by challenging scenarios in Cyber-Physical Systems (CPS), we study how choreographic programming can cater for dynamic infrastructures where not all endpoints are always available. We introduce the Global Quality Calculus ($GC_q$), a variant of choreographic programming for the description of communication systems where some of the components involved in a communication might fail. GCq features novel operators for multiparty, partial and collective communications. This paper studies the nature of failure-aware communication: First, we introduce $GC_q$ syntax, semantics and examples of its use. The interplay between failures and collective communications in a choreography can lead to choreographies that cannot progress due to absence of resources. In our second contribution, we provide a type system that ensures that choreographies can be realized despite changing availability conditions. A specification in $GC_q$ guides the implementation of distributed endpoints when paired with global (session) types. Our third contribution provides an endpoint-projection based methodology for the generation of failure-aware distributed processes. We show the correctness of the projection, and that well-typed choreographies with availability considerations enjoy progress. 	
1612.02830v2	http://arxiv.org/pdf/1612.02830v2	2017	Hard decoding algorithm for optimizing thresholds under general   Markovian noise	Christopher Chamberland|Joel J. Wallman|Stefanie Beale|Raymond Laflamme	  Quantum error correction is instrumental in protecting quantum systems from noise in quantum computing and communication settings. Pauli channels can be efficiently simulated and threshold values for Pauli error rates under a variety of error-correcting codes have been obtained. However, realistic quantum systems can undergo noise processes that differ significantly from Pauli noise. In this paper, we present an efficient hard decoding algorithm for optimizing thresholds and lowering failure rates of an error-correcting code under general completely positive and trace-preserving (i.e., Markovian) noise. We use our hard decoding algorithm to study the performance of several error-correcting codes under various non-Pauli noise models by computing threshold values and failure rates for these codes. We compare the performance of our hard decoding algorithm to decoders optimized for depolarizing noise and show improvements in thresholds and reductions in failure rates by several orders of magnitude. Our hard decoding algorithm can also be adapted to take advantage of a code's non-Pauli transversal gates to further suppress noise. For example, we show that using the transversal gates of the 5-qubit code allows arbitrary rotations around certain axes to be perfectly corrected. Furthermore, we show that Pauli twirling can increase or decrease the threshold depending upon the code properties. Lastly, we show that even if the physical noise model differs slightly from the hypothesized noise model used to determine an optimized decoder, failure rates can still be reduced by applying our hard decoding algorithm. 	
1702.05062v1	http://arxiv.org/pdf/1702.05062v1	2017	Successive failures of approachability	Spencer Unger	  Motivated by showing that in ZFC we cannot construct a special Aronszajn tree on some cardinal greater than $\aleph_1$, we produce a model in which the approachability property fails (hence there are no special Aronszajn trees) at all regular cardinals in the interval $[\aleph_2, \aleph_{\omega^2+3}]$ and $\aleph_{\omega^2}$ is strong limit. 	
1703.02358v2	http://arxiv.org/pdf/1703.02358v2	2017	Survivability Improvement Against Earthquakes in Backbone Optical   Networks Using Actual Seismic Zone Information	Anuj Agrawal|Purva Sharma|Vimal Bhatia|Shashi Prakash	  Optical backbone networks carry a huge amount of bandwidth and serve as a key enabling technology to provide telecommunication connectivity across the world. Hence, in events of network component (node/link) failures, communication networks may suffer from huge amount of bandwidth loss and service disruptions. Natural disasters such as earthquakes, hurricanes, tornadoes, etc., occur at different places around the world, causing severe communication service disruptions due to network component failures. Most of the previous works on optical network survivability assume that the failures are going to occur in future, and the network is made survivable to ensure connectivity in events of failures. With the advancements in seismology, the predictions of earthquakes are becoming more accurate. Earthquakes have been a major cause of telecommunication service disruption in the past. Hence, the information provided by the meteorological departments and other similar agencies of different countries may be helpful in designing networks that are more robust against earthquakes. In this work, we consider the actual information provided by the Indian meteorological department (IMD) on seismic zones, and earthquakes occurred in the past in India, and propose a scheme to improve the survivability of the existing Indian optical network through minute changes in network topology. Simulations show significant improvement in the network survivability can be achieved using the proposed scheme in events of earthquakes. 	
1704.05391v7	http://arxiv.org/pdf/1704.05391v7	2018	Probabilistic $N$-$k$ Failure-Identification for Power Systems	Kaarthik Sundar|Carleton Coffrin|Harsha Nagarajan|Russell Bent	  This paper considers a probabilistic generalization of the $N$-$k$ failure-identification problem in power transmission networks, where the probability of failure of each component in the network is known a priori and the goal of the problem is to find a set of $k$ components that maximizes disruption to the system loads weighted by the probability of simultaneous failure of the $k$ components. The resulting problem is formulated as a bilevel mixed-integer nonlinear program. Convex relaxations, linear approximations, and heuristics are developed to obtain feasible solutions that are close to the optimum. A general cutting-plane algorithm is proposed to solve the convex relaxation and linear approximations of the $N$-$k$ problem. Extensive numerical results corroborate the effectiveness of the proposed algorithms on small-, medium-, and large-scale test instances; the test instances include the IEEE 14-bus system, the IEEE single-area and three-area RTS96 systems, the IEEE 118-bus system, the WECC 240-bus test system, the 1354-bus PEGASE system, and the 2383-bus Polish winter-peak test system. 	
1705.01611v2	http://arxiv.org/pdf/1705.01611v2	2017	Diamonds, Compactness, and Measure Sequences	Omer Ben-Neria	  We establish the consistency of the failure of the diamond principle on a cardinal $\kappa$ which satisfies a strong simultaneous reflection property. The result is based on an analysis of Radin forcing, and further leads to a characterization of weak compactness of $\kappa$ in a Radin generic extension. 	
1705.02639v2	http://arxiv.org/pdf/1705.02639v2	2017	Codes for Graph Erasures	Lev Yohananov|Eitan Yaakobi	  Motivated by systems where the information is represented by a graph, such as neural networks, associative memories, and distributed systems, we present in this work a new class of codes, called codes over graphs. Under this paradigm, the information is stored on the edges of an undirected graph, and a code over graphs is a set of graphs. A node failure is the event where all edges in the neighborhood of the failed node have been erased. We say that a code over graphs can tolerate $\rho$ node failures if it can correct the erased edges of any $\rho$ failed nodes in the graph. While the construction of such codes can be easily accomplished by MDS codes, their field size has to be at least $O(n^2)$, when $n$ is the number of nodes in the graph. In this work we present several constructions of codes over graphs with smaller field size. In particular, we present optimal codes over graphs correcting two node failures over the binary field, when the number of nodes in the graph is a prime number. We also present a construction of codes over graphs correcting $\rho$ node failures for all $\rho$ over a field of size at least $(n+1)/2-1$, and show how to improve this construction for optimal codes when $\rho=2,3$. 	
1707.04177v1	http://arxiv.org/pdf/1707.04177v1	2017	Failure of strong approximation on an affine cone	Martin Bright|Ivo Kok	  We use the Brauer-Manin obstruction to strong approximation on a punctured affine cone to explain a curious property of coprime integer solutions to a homogeneous Diophantine equation. 	
1710.07380v3	http://arxiv.org/pdf/1710.07380v3	2017	Fault-tolerant parallel scheduling of arbitrary length jobs on a shared   channel	Marek Klonowski|Dariusz R. Kowalski|Jarosław Mirek|Prudence W. H. Wong	  We study the problem of scheduling jobs on fault-prone machines communicating via a shared channel, also known as multiple-access channel. We have $n$ arbitrary length jobs to be scheduled on $m$ identical machines, $f$ of which are prone to crashes by an adversary. A machine can inform other machines when a job is completed via the channel without collision detection. Performance is measured by the total number of available machine steps during the whole execution. Our goal is to study the impact of preemption (i.e., interrupting the execution of a job and resuming later in the same or different machine) and failures on the work performance of job processing. The novelty is the ability to identify the features that determine the complexity (difficulty) of the problem. We show that the problem becomes difficult when preemption is not allowed, by showing corresponding lower and upper bounds, the latter with algorithms reaching them. We also prove that randomization helps even more, but only against a non-adaptive adversary; in the presence of more severe adaptive adversary, randomization does not help in any setting. Our work has extended from previous work that focused on settings including: scheduling on multiple-access channel without machine failures, complete information about failures, or incomplete information about failures (like in this work) but with unit length jobs and, hence, without considering preemption. 	
1802.03901v1	http://arxiv.org/pdf/1802.03901v1	2018	Network Overload due to Massive Attacks	Yosef Kornbluth|Gilad Barach|Mark Tuchman|Benjamin Kadish|Gabriel Cwilich|Sergey V. Buldyrev	  We study the cascading failure of networks due to overload, using the betweenness centrality of a node as the measure of its load following the Motter and Lai model. We study the fraction of survived nodes at the end of the cascade $p_f$ as function of the strength of the initial attack, measured by the fraction of nodes $p$, which survive the initial attack for different values of tolerance $\alpha$ in random regular and Erd\"os-Renyi graphs. We find the existence of first order phase transition line $p_t(\alpha)$ on a $p-\alpha$ plane, such that if $p <p_t$ the cascade of failures lead to a very small fraction of survived nodes $p_f$ and the giant component of the network disappears, while for $p>p_t$, $p_f$ is large and the giant component of the network is still present. Exactly at $p_t$ the function $p_f(p)$ undergoes a first order discontinuity. We find that the line $p_t(\alpha)$ ends at critical point $(p_c,\alpha_c)$ ,in which the cascading failures are replaced by a second order percolation transition. We analytically find the average betweenness of nodes with different degrees before and after the initial attack, investigate their roles in the cascading failures, and find a lower bound for $p_t(\alpha)$. We also study the difference between a localized and random attacks. 	
1802.08634v1	http://arxiv.org/pdf/1802.08634v1	2018	Fully Asynchronous Push-Sum With Growing Intercommunication Intervals	Alex Olshevsky|Ioannis Ch. Paschalidis|Artin Spiridonoff	  We propose an algorithm for average consensus over a directed graph which is both fully asynchronous and robust to unreliable communications. We show its convergence to the average, while allowing for slowly growing but potentially unbounded communication failures. 	
1803.00117v1	http://arxiv.org/pdf/1803.00117v1	2018	Redundancy allocation in finite-length nested codes for nonvolatile   memories	Yongjune Kim|B. V. K. Vijaya Kumar	  In this paper, we investigate the optimum way to allocate redundancy of finite-length nested codes for modern nonvolatile memories suffering from both permanent defects and transient errors (erasures or random errors). A nested coding approach such as partitioned codes can handle both permanent defects and transient errors by using two parts of redundancy: 1) redundancy to deal with permanent defects and 2) redundancy for transient errors. We consider two different channel models of the binary defect and erasure channel (BDEC) and the binary defect and symmetric channel (BDSC). The transient errors of the BDEC are erasures and the BDSC's transient errors are modeled by the binary symmetric channel, respectively. Asymptotically, the probability of recovery failure can converge to zero if the capacity region conditions of nested codes are satisfied. However, the probability of recovery failure of finite-length nested codes can be significantly variable for different redundancy allocations even though they all satisfy the capacity region conditions. Hence, we formulate the redundancy allocation problem of finite-length nested codes to minimize the recovery failure probability. We derive the upper bounds on the probability of recovery failure and use them to estimate the optimal redundancy allocation. Numerical results show that our estimated redundancy allocation matches well the optimal redundancy allocation. 	
9711003v1	http://arxiv.org/pdf/adap-org/9711003v1	1997	Multiresolution wavelet analysis of heartbeat intervals discriminates   healthy patients from those with cardiac pathology	Stefan Thurner|Markus C. Feurstein|Malvin C. Teich	  We applied multiresolution wavelet analysis to the sequence of times between human heartbeats (R-R intervals) and have found a scale window, between 16 and 32 heartbeats, over which the widths of the R-R wavelet coefficients fall into disjoint sets for normal and heart-failure patients. This has enabled us to correctly classify every patient in a standard data set as either belonging to the heart-failure or normal group with 100% accuracy, thereby providing a clinically significant measure of the presence of heart-failure from the R-R intervals alone.   Comparison is made with previous approaches, which have provided only statistically significant measures. 	
0410134v1	http://arxiv.org/pdf/cond-mat/0410134v1	2004	A Two-Threshold Model for Scaling Laws of Non-Interacting Snow   Avalanches	Jerome Faillettaz|Francois Louchet|Jean-Robert Grasso	  The sizes of snow slab failure that trigger snow avalanches are power-law distributed. Such a power-law probability distribution function has also been proposed to characterize different landslide types. In order to understand this scaling for gravity driven systems, we introduce a two-threshold 2-d cellular automaton, in which failure occurs irreversibly. Taking snow slab avalanches as a model system, we find that the sizes of the largest avalanches just preceeding the lattice system breakdown are power law distributed. By tuning the maximum value of the ratio of the two failure thresholds our model reproduces the range of power law exponents observed for land-, rock- or snow avalanches. We suggest this control parameter represents the material cohesion anisotropy. 	
9908011v1	http://arxiv.org/pdf/cs/9908011v1	1999	The Load and Availability of Byzantine Quorum Systems	Dahlia Malkhi|Michael Reiter|Avishai Wool	  Replicated services accessed via {\em quorums} enable each access to be performed at only a subset (quorum) of the servers, and achieve consistency across accesses by requiring any two quorums to intersect. Recently, $b$-masking quorum systems, whose intersections contain at least $2b+1$ servers, have been proposed to construct replicated services tolerant of $b$ arbitrary (Byzantine) server failures. In this paper we consider a hybrid fault model allowing benign failures in addition to the Byzantine ones. We present four novel constructions for $b$-masking quorum systems in this model, each of which has optimal {\em load} (the probability of access of the busiest server) or optimal availability (probability of some quorum surviving failures). To show optimality we also prove lower bounds on the load and availability of any $b$-masking quorum system in this model. 	
0003069v1	http://arxiv.org/pdf/cs/0003069v1	2000	Proving Failure of Queries for Definite Logic Programs Using XSB-Prolog	Nikolay Pelov|Maurice Bruynooghe	  Proving failure of queries for definite logic programs can be done by constructing a finite model of the program in which the query is false. A general purpose model generator for first order logic can be used for this. A recent paper presented at PLILP98 shows how the peculiarities of definite programs can be exploited to obtain a better solution. There a procedure is described which combines abduction with tabulation and uses a meta-interpreter for heuristic control of the search. The current paper shows how similar results can be obtained by direct execution under the standard tabulation of the XSB-Prolog system. The loss of control is compensated for by better intelligent backtracking and more accurate failure analysis. 	
0306074v1	http://arxiv.org/pdf/cs/0306074v1	2003	Understanding and Coping with Hardware and Software Failures in a Very   Large Trigger Farm	Jim Kowalkowski	  When thousands of processors are involved in performing event filtering on a trigger farm, there is likely to be a large number of failures within the software and hardware systems. BTeV, a proton/antiproton collider experiment at Fermi National Accelerator Laboratory, has designed a trigger, which includes several thousand processors. If fault conditions are not given proper treatment, it is conceivable that this trigger system will experience failures at a high enough rate to have a negative impact on its effectiveness. The RTES (Real Time Embedded Systems) collaboration is a group of physicists, engineers, and computer scientists working to address the problem of reliability in large-scale clusters with real-time constraints such as this. Resulting infrastructure must be highly scalable, verifiable, extensible by users, and dynamically changeable. 	
0402012v1	http://arxiv.org/pdf/cs/0402012v1	2004	A Knowledge-Theoretic Analysis of Uniform Distributed Coordination and   Failure Detectors	Joseph Y. Halpern|Aleta Ricciardi	  It is shown that, in a precise sense, if there is no bound on the number of faulty processes in a system with unreliable but fair communication, Uniform Distributed Coordination (UDC) can be attained if and only if a system has perfect failure detectors. This result is generalized to the case where there is a bound t on the number of faulty processes. It is shown that a certain type of generalized failure detector is necessary and sufficient for achieving UDC in a context with at most t faulty processes. Reasoning about processes' knowledge as to which other processes are faulty plays a key role in the analysis. 	
0406046v1	http://arxiv.org/pdf/cs/0406046v1	2004	Cheap Recovery: A Key to Self-Managing State	Andrew C. Huang|Armando Fox	  Cluster hash tables (CHTs) are a key persistent-storage component of many large-scale Internet services due to their high performance and scalability. We show that a correctly-designed CHT can also be as easy to manage as a farm of stateless servers. Specifically, we trade away some consistency to obtain reboot-based recovery that is simple, maintains full data availability, and only has modest impact on performance. This simplifies management in two ways. First, it simplifies failure detection by lowering the cost of acting on false positives, allowing us to use simple but aggressive statistical techniques to quickly detect potential failures and node degradations; even when a false alarm is raised or when rebooting will not fix the problem, attempting recovery by rebooting is relatively non-intrusive to system availability and performance. Second, it allows us to re-cast online repartitioning as failure plus recovery, simplifying dynamic scaling and capacity planning. These properties make it possible for the system to be continuously self-adjusting, a key property of self-managing, autonomic systems. 	
0603112v1	http://arxiv.org/pdf/cs/0603112v1	2006	A General Framework for Scalability and Performance Analysis of DHT   Routing Systems	Joseph S. Kong|Jesse S. A. Bridgewater|Vwani P. Roychowdhury	  In recent years, many DHT-based P2P systems have been proposed, analyzed, and certain deployments have reached a global scale with nearly one million nodes. One is thus faced with the question of which particular DHT system to choose, and whether some are inherently more robust and scalable.   Toward developing such a comparative framework, we present the reachable component method (RCM) for analyzing the performance of different DHT routing systems subject to random failures. We apply RCM to five DHT systems and obtain analytical expressions that characterize their routability as a continuous function of system size and node failure probability. An important consequence is that in the large-network limit, the routability of certain DHT systems go to zero for any non-zero probability of node failure. These DHT routing algorithms are therefore unscalable, while some others, including Kademlia, which powers the popular eDonkey P2P system, are found to be scalable. 	
0607077v1	http://arxiv.org/pdf/cs/0607077v1	2006	Fault-Tolerant Real-Time Streaming with FEC thanks to Capillary   Multi-Path Routing	Emin Gabrielyan	  Erasure resilient FEC codes in off-line packetized streaming rely on time diversity. This requires unrestricted buffering time at the receiver. In real-time streaming the playback buffering time must be very short. Path diversity is an orthogonal strategy. However, the large number of long paths increases the number of underlying links and consecutively the overall link failure rate. This may increase the overall requirement in redundant FEC packets for combating the link failures. We introduce the Redundancy Overall Requirement (ROR) metric, a routing coefficient specifying the total number of FEC packets required for compensation of all underlying link failures. We present a capillary routing algorithm for constructing layer by layer steadily diversifying multi-path routing patterns. By measuring the ROR coefficients of a dozen of routing layers on hundreds of network samples, we show that the number of required FEC packets decreases substantially when the path diversity is increased by the capillary routing construction algorithm. 	
9709462v1	http://arxiv.org/pdf/hep-ph/9709462v1	1997	Magnetic Defects Signal Failure of Abelian Projection Gauges in QCD	Harald W. Griesshammer	  Magnetically charged Abelian defects are shown to arise on most compact base manifolds and in most Abelian projection gauges. They obey the Dirac quantisation condition and give rise to homogeneous magnetic background fields. The reasons for their occurrence are global failures of the procedure with which gauge covariant operators are diagonalised or their eigenphases extracted. Defects related to the former case are string-like; for the latter case they resemble domain walls. Either configuration forms the generic case and indicates a failure of gauge fixing as continuity and periodicity properties of the functional space are changed. These results are first obtained in canonically quantised QCD_{3+1} and path integral QCD_{2+1} on the torus for the modified axial gauge which keeps only the eigenphases as dynamical variables of the Wilson line in the x_3-direction. In the end, they are extended to other gauges, dimensions and standard manifolds. 	
0409165v1	http://arxiv.org/pdf/math/0409165v1	2004	Estimating the causal effect of a time-varying treatment on   time-to-event using structural nested failure time models	J. J. Lok|R. D. Gill|A. W. van der Vaart|J. M. Robins	  In this paper we review an approach to estimating the causal effect of a time-varying treatment on time to some event of interest. This approach is designed for the situation where the treatment may have been repeatedly adapted to patient characteristics, which themselves may also be time-dependent. In this situation the effect of the treatment cannot simply be estimated by conditioning on the patient characteristics, as these may themselves be indicators of the treatment effect. This so-called time-dependent confounding is typical in observational studies. We discuss a new class of failure time models, structural nested failure time models, which can be used to estimate the causal effect of a time-varying treatment, and present methods for estimating and testing the parameters of these models. 	
0703300v1	http://arxiv.org/pdf/math/0703300v1	2007	Case-Control Survival Analysis with a General Semiparametric Shared   Frailty Model - a Pseudo Full Likelihood Approach	Malka Gorfine|David M. Zucker|Li Hsu	  In this work we deal with correlated failure time (age at onset) data arising from population-based case-control studies, where case and control probands are selected by population-based sampling and an array of risk factor measures is collected for both cases and controls and their relatives. Parameters of interest are effects of risk factors on the hazard function of failure times and within-family dependencies of failure times after adjusting for the risk factors. Due to the retrospective nature of sampling, a large sample theory for existing methods has not been established. We develop a novel estimation techniques for estimating these parameters under a general semiparametric shared frailty model. We also present a simple, easily computed, and non-iterative nonparametric estimator for the cumulative baseline hazard function. A rigorous large sample theory for the proposed estimators of these parameters is given along with simulations and a real data example illustrate the utility of the proposed method. 	
0309067v1	http://arxiv.org/pdf/nlin/0309067v1	2003	Self-sustained activity in a small-world network of excitable neurons	Alex Roxin|Hermann Riecke|Sara A. Solla	  We study the dynamics of excitable integrate-and-fire neurons in a small-world network. At low densities $p$ of directed random connections, a localized transient stimulus results in either self-sustained persistent activity or in a brief transient followed by failure. Averages over the quenched ensemble reveal that the probability of failure changes from 0 to 1 over a narrow range in $p$; this failure transition can be described analytically through an extension of an existing mean-field result. Exceedingly long transients emerge at higher densities $p$; their activity patterns are disordered, in contrast to the mostly periodic persistent patterns observed at low $p$. The times at which such patterns die out are consistent with a stretched-exponential distribution, which depends sensitively on the propagation velocity of the excitation. 	
0210179v1	http://arxiv.org/pdf/quant-ph/0210179v1	2002	Distinguishing two-qubit states using local measurements and restricted   classical communication	Mark Hillery|Jihane Mimih	  The problem of unambiguous state discrimination consists of determining which of a set of known quantum states a particular system is in. One is allowed to fail, but not to make a mistake. The optimal procedure is the one with the lowest failure probability. This procedure has been extended to bipartite states where the two parties, Alice and Bob, are allowed to manipulate their particles locally and communicate classically in order to determine which of two possible two-particle states they have been given. The failure probability of this local procedure has been shown to be the same as if the particles were together in the same location. Here we examine the effect of restricting the classical communication between the parties, either allowing none or eliminating the possibility that one party's measurement depends on the result of the other party's. These issues are studied for two-qubit states, and optimal procedures are found. In some cases the restrictions cause increases in the failure probability, but in other cases they do not. Applications of these procedures, in particular to secret sharing, are discussed. 	
0501174v1	http://arxiv.org/pdf/quant-ph/0501174v1	2005	Unambiguous discrimination of special sets of multipartite states using   local measurements and classical communication	Jihane Mimih|Mark Hillery	  We initially consider a quantum system consisting of two qubits, which can be in one of two nonorthogonal states, \Psi_0 or \Psi_1. We distribute the qubits to two parties, Alice and Bob. They each measure their qubit and then compare their measurement results to determine which state they were sent. This procedure is error-free, which implies that it must sometimes fail. In addition, no quantum memory is required; it is not necessary to store one of the qubits until the result of the measurement on the other is known. We consider the cases in which, should failure occur, both parties receive a failure signal or only one does. In the latter case, if the states share the same Schmidt basis, the states can be discriminated with the same failure probability as would be obtained if the two qubits were measured together. This scheme is sufficiently simple that it can be generalized to multipartite qubit and qudit states. Applications to quantum secret sharing are discussed. Finally, we present an optical scheme to experimenatlly realize the protocol in the case of two qubits. 	
0603130v3	http://arxiv.org/pdf/quant-ph/0603130v3	2007	Error tolerance and tradeoffs in loss- and failure-tolerant quantum   computing schemes	Peter P. Rohde|Timothy C. Ralph|William J. Munro	  Qubit loss and gate failure are significant problems for the development of scalable quantum computing. Recently various schemes have been proposed for tolerating qubit loss and gate failure. These include schemes based on cluster and parity states. We show that by designing such schemes specifically to tolerate these error types we cause an exponential blow-out in depolarizing noise. We discuss several examples and propose techniques for minimizing this problem. In general this introduces a tradeoff with other undesirable effects. In some cases this is physical resource requirements, while in others it is noise rates. 	
0706.3889v1	http://arxiv.org/pdf/0706.3889v1	2007	Statistical properties of microcracking in polyurethane foams under   tensile test, influence of temperature and density	Stéphanie Deschanel|Loïc Vanel|Gérard Vigier|Nathalie Godin|Sergio Ciliberto	  We report tensile failure experiments on polyurethane (PU) foams. Experiments have been performed by imposing a constant strain rate. We work on heterogeneous materials for whom the failure does not occur suddenly and can develop as a multistep process through a succession of microcracks that end at pores. The acoustic energy and the waiting times between acoustic events follow power-law distributions. This remains true while the foam density is varied. However, experiments at low temperatures (PU foams more brittle) have not yielded power-laws for the waiting times. The cumulative acoustic energy has no power law divergence at the proximity of the failure point which is qualitatively in agreement with other experiments done at imposed strain. We notice a plateau in cumulative acoustic energy that seems to occur when a single crack starts to propagate. 	
0708.0281v1	http://arxiv.org/pdf/0708.0281v1	2007	Stochastic Programming with Probability	Laetitia Andrieu|Guy Cohen|Felisa Vázquez-Abad	  In this work we study optimization problems subject to a failure constraint. This constraint is expressed in terms of a condition that causes failure, representing a physical or technical breakdown. We formulate the problem in terms of a probability constraint, where the level of "confidence" is a modelling parameter and has the interpretation that the probability of failure should not exceed that level. Application of the stochastic Arrow-Hurwicz algorithm poses two difficulties: one is structural and arises from the lack of convexity of the probability constraint, and the other is the estimation of the gradient of the probability constraint. We develop two gradient estimators with decreasing bias via a convolution method and a finite difference technique, respectively, and we provide a full analysis of convergence of the algorithms. Convergence results are used to tune the parameters of the numerical algorithms in order to achieve best convergence rates, and numerical results are included via an example of application in finance. 	
0708.0362v1	http://arxiv.org/pdf/0708.0362v1	2007	On the Statistical Modeling and Analysis of Repairable Systems	Bo Henry Lindqvist	  We review basic modeling approaches for failure and maintenance data from repairable systems. In particular we consider imperfect repair models, defined in terms of virtual age processes, and the trend-renewal process which extends the nonhomogeneous Poisson process and the renewal process. In the case where several systems of the same kind are observed, we show how observed covariates and unobserved heterogeneity can be included in the models. We also consider various approaches to trend testing. Modern reliability data bases usually contain information on the type of failure, the type of maintenance and so forth in addition to the failure times themselves. Basing our work on recent literature we present a framework where the observed events are modeled as marked point processes, with marks labeling the types of events. Throughout the paper the emphasis is more on modeling than on statistical inference. 	
0708.1058v1	http://arxiv.org/pdf/0708.1058v1	2007	Non- and semi-parametric analysis of failure time data with missing   failure indicators	Irene Gijbels|Danyu Lin|Zhiliang Ying	  A class of estimating functions is introduced for the regression parameter of the Cox proportional hazards model to allow unknown failure statuses on some study subjects. The consistency and asymptotic normality of the resulting estimators are established under mild conditions. An adaptive estimator which achieves the minimum variance-covariance bound of the class is constructed. Numerical studies demonstrate that the asymptotic approximations are adequate for practical use and that the efficiency gain of the adaptive estimator over the complete-case analysis can be quite substantial. Similar methods are also developed for the nonparametric estimation of the survival function of a homogeneous population and for the estimation of the cumulative baseline hazard function under the Cox model. 	
0803.2717v3	http://arxiv.org/pdf/0803.2717v3	2009	Distributed authentication for randomly compromised networks	Travis R. Beals|Kevin P. Hynes|Barry C. Sanders	  We introduce a simple, practical approach with probabilistic information-theoretic security to solve one of quantum key distribution's major security weaknesses: the requirement of an authenticated classical channel to prevent man-in-the-middle attacks. Our scheme employs classical secret sharing and partially trusted intermediaries to provide arbitrarily high confidence in the security of the protocol. Although certain failures elude detection, we discuss preemptive strategies to reduce the probability of failure to an arbitrarily small level: probability of such failures is exponentially suppressed with increases in connectivity (i.e., connections per node). 	
0807.0626v1	http://arxiv.org/pdf/0807.0626v1	2008	Asymptotic Mean Time To Failure and Higher Moments for Large, Recursive   Networks	Christian Tanguy	  This paper deals with asymptotic expressions of the Mean Time To Failure (MTTF) and higher moments for large, recursive, and non-repairable systems in the context of two-terminal reliability. Our aim is to extend the well-known results of the series and parallel cases. We first consider several exactly solvable configurations of identical components with exponential failure-time distribution functions to illustrate different (logarithmic or power-law) behaviors as the size of the system, indexed by an integer n, increases. The general case is then addressed: it provides a simple interpretation of the origin of the power-law exponent and an efficient asymptotic expression for the total reliability of large, recursive systems. Finally, we assess the influence of the non-exponential character of the component reliability on the n-dependence of the MTTF. 	
0810.4059v1	http://arxiv.org/pdf/0810.4059v1	2008	Network Coding-based Protection Strategies Against a Single Link Failure   in Optical Networks	Salah A. Aly|Ahmed E. Kamal	  In this paper we develop network protection strategies against a single link failure in optical networks. The motivation behind this work is the fact that $%70$ of all available links in an optical network suffers from a single link failure. In the proposed protection strategies, denoted NPS-I and NPS-II, we deploy network coding and reduced capacity on the working paths to provide a backup protection path that will carry encoded data from all sources. In addition, we provide implementation aspects and how to deploy the proposed strategies in case of an optical network with $n$ disjoint working paths. 	
0906.1109v1	http://arxiv.org/pdf/0906.1109v1	2009	Analysis of major failures in Europe's power grid	Martí Rosas-Casals|Ricard Solé	  Power grids are prone to failure. Time series of reliability measures such as total power loss or energy not supplied can give significant account of the underlying dynamical behavior of these systems, specially when the resulting probability distributions present remarkable features such as an algebraic tail, for example. In this paper, seven years (from 2002 to 2008) of Europe's transport of electricity network failure events have been analyzed and the best fit for this empirical data probability distribution is presented. With the actual span of available data and although there exists a moderate support for the power law model, the relatively small amount of events contained in the function's tail suggests that other causal factors might be significantly ruling the system's dynamics. 	
1002.2300v1	http://arxiv.org/pdf/1002.2300v1	2010	Public Transport Networks under Random Failure and Directed Attack	Bertrand Berche|Christian von Ferber|Taras Holovatch|Yurij Holovatch	  The behaviour of complex networks under failure or attack depends strongly on the specific scenario. Of special interest are scale-free networks, which are usually seen as robust under random failure but appear to be especially vulnerable to targeted attacks. In a recent study of public transport networks of 14 major cities of the world we have shown that these systems when represented by appropriate graphs may exhibit scale-free behaviour. In this paper we briefly review some of the recent results about the effects that defunct or removed nodes have on the properties of public transport networks. Simulating different directed attack strategies, we derive vulnerability criteria that result in minimal strategies with high impact on these systems. 	
1005.5367v1	http://arxiv.org/pdf/1005.5367v1	2010	Designing and Embedding Reliable Virtual Infrastructures	Wai-Leong Yeow|Cédric Westphal|Ulaş C. Kozat	  In a virtualized infrastructure where physical resources are shared, a single physical server failure will terminate several virtual servers and crippling the virtual infrastructures which contained those virtual servers. In the worst case, more failures may cascade from overloading the remaining servers. To guarantee some level of reliability, each virtual infrastructure, at instantiation, should be augmented with backup virtual nodes and links that have sufficient capacities. This ensures that, when physical failures occur, sufficient computing resources are available and the virtual network topology is preserved. However, in doing so, the utilization of the physical infrastructure may be greatly reduced. This can be circumvented if backup resources are pooled and shared across multiple virtual infrastructures, and intelligently embedded in the physical infrastructure. These techniques can reduce the physical footprint of virtual backups while guaranteeing reliability. 	
1006.5749v1	http://arxiv.org/pdf/1006.5749v1	2010	Critical Success factors for Enterprise Resource Planning implementation   in Indian Retail Industry: An Exploratory study	Poonam Garg	  Enterprise resource Planning (ERP) has become a key business driver in today's world. Retailers are also trying to reap in the benefits of the ERP. In most large Indian Retail Industry ERP systems have replaced nonintegrated information systems with integrated and maintainable software. Retail ERP solution integrates demand and supply effectively to help improve bottom line. The implementation of ERP systems in such firms is a difficult task. So far, ERP implementations have yielded more failures than successes. Very few implementation failures are recorded in the literature because few companies wish to publicize their implementation failure. This paper explores and validates the existing literature empirically to find out the critical success factors that lead to the success of ERP in context to Indian retail industry. The findings of the results provide valuable insights for the researchers and practitioners who are interested in implementing Enterprise Resource Planning systems in retail industry, how best they can utilize their limited resources and to pay adequate attention to those factors that are most likely to have an impact upon the implementation of the ERP system. 	
1008.0440v1	http://arxiv.org/pdf/1008.0440v1	2010	Preserving HTTP Sessions in Vehicular Environments	Yibei Ling|Wai Chen|Russell Hsing|Onur Altintas	  Wireless Internet in the in-vehicle environment is an evolving reality that reflects the gradual maturity of wireless technologies. Its complexity is reflected in the diversity of wireless technologies and dynamically changing network environments. The ability to adapt to the dynamics of such environments and to survive transient failures due to network handoffs are fundamentally important in failure-prone vehicular environments. In this paper we identify several new issues arising from network heterogeneity in vehicular environments and concentrate on designing and implementing a network-aware prototype system that supports HTTP session continuity in the presence of network volatility, with the emphasis on the following specifically tailored features: (1) automatic and transparent HTTP failure recovery, (2) network awareness and adaptation, (3) application-layer preemptive network handoff. Experimental results gathered from real application environments based on CDMA {\it 1xRTT} and IEEE 802 networks are presented and analyzed. 	
1008.0659v2	http://arxiv.org/pdf/1008.0659v2	2010	Evaluating and Improving Modern Variable and Revision Ordering   Strategies in CSPs	Thanasis Balafoutis|Kostas Stergiou	  A key factor that can dramatically reduce the search space during constraint solving is the criterion under which the variable to be instantiated next is selected. For this purpose numerous heuristics have been proposed. Some of the best of such heuristics exploit information about failures gathered throughout search and recorded in the form of constraint weights, while others measure the importance of variable assignments in reducing the search space. In this work we experimentally evaluate the most recent and powerful variable ordering heuristics, and new variants of them, over a wide range of benchmarks. Results demonstrate that heuristics based on failures are in general more efficient. Based on this, we then derive new revision ordering heuristics that exploit recorded failures to efficiently order the propagation list when arc consistency is maintained during search. Interestingly, in addition to reducing the number of constraint checks and list operations, these heuristics are also able to cut down the size of the explored search tree. 	
1008.4264v1	http://arxiv.org/pdf/1008.4264v1	2010	Network Protection Design Using Network Coding	Salah A. Aly|Ahmed E. Kamal|Anwar I. Walid	  Link and node failures are two common fundamental problems that affect operational networks. Protection of communication networks against such failures is essential for maintaining network reliability and performance. Network protection codes (NPC) are proposed to protect operational networks against link and node failures. Furthermore, encoding and decoding operations of such codes are well developed over binary and finite fields. Finding network topologies, practical scenarios, and limits on graphs applicable for NPC are of interest. In this paper, we establish limits on network protection design. We investigate several network graphs where NPC can be deployed using network coding. Furthermore, we construct graphs with minimum number of edges suitable for network protection codes deployment. 	
1010.2160v3	http://arxiv.org/pdf/1010.2160v3	2010	Robustness of interdependent networks under targeted attack	Xuqing Huang|Jianxi Gao|Sergey V. Buldyrev|Shlomo Havlin|H. Eugene Stanley	  When an initial failure of nodes occurs in interdependent networks, a cascade of failure between the networks occurs. Earlier studies focused on random initial failures. Here we study the robustness of interdependent networks under targeted attack on high or low degree nodes. We introduce a general technique and show that the {\it targeted-attack} problem in interdependent networks can be mapped to the {\it random-attack} problem in a transformed pair of interdependent networks. We find that when the highly connected nodes are protected and have lower probability to fail, in contrast to single scale free (SF) networks where the percolation threshold $p_c=0$, coupled SF networks are significantly more vulnerable with $p_c$ significantly larger than zero. The result implies that interdependent networks are difficult to defend by strategies such as protecting the high degree nodes that have been found useful to significantly improve robustness of single networks. 	
1010.3586v1	http://arxiv.org/pdf/1010.3586v1	2010	A nonparametric urn-based approach to interacting failing systems with   an application to credit risk modeling	Pasquale Cirillo|Jürg Hüsler|Pietro Muliere	  In this paper we propose a new nonparametric approach to interacting failing systems (FS), that is systems whose probability of failure is not negligible in a fixed time horizon, a typical example being firms and financial bonds. The main purpose when studying a FS is to calculate the probability of default and the distribution of the number of failures that may occur during the observation period. A model used to study a failing system is defined default model. In particular, we present a general recursive model constructed by the means of inter- acting urns. After introducing the theoretical model and its properties we show a first application to credit risk modeling, showing how to assess the idiosyncratic probability of default of an obligor and the joint probability of failure of a set of obligors in a portfolio of risks, that are divided into reliability classes. 	
1011.6180v1	http://arxiv.org/pdf/1011.6180v1	2010	Adapting MAC 802.11 Adapting MAC 802.11 for Performance Optimization of   MANET using Cross Layer Interaction	Gaurav Bhatia|Vivek Kumar	  In this research, we study the optimization challenges of MANET and cross-layer technique to improve its performance. We propose an adaptive retransmission limits algorithm for IEEE 802.11 MAC to reduce the false link failures and predict the node mobility. We implemented cross layer interaction between physical and MAC layers. The MAC layer utilizes the physical layer information for differentiating false link failure from true link failure. The MAC layer adaptively selects a retransmission limit (short and long) based on the neighbour signal strength and sender node speed information from the physical layer. The proposed approach tracks the signal strength of each node in network and, while transmitting to a neighbour node, if it's received signal strength is high and is received recently then Adaptive MAC persists in its retransmission attempts. As there is high probability that neighbour node is still in transmission range and may be not responding due to some problems other then mobility. In this paper, we evaluate the performance of MANET and show that how our Adaptive MAC greatly improves it. The simulation is done using Network Simulator NS-2. 	
1101.1365v1	http://arxiv.org/pdf/1101.1365v1	2011	An imputation-based approach for parameter estimation in the presence of   ambiguous censoring with application in industrial supply chain	Samiran Ghosh	  This paper describes a novel approach based on "proportional imputation" when identical units produced in a batch have random but independent installation and failure times. The current problem is motivated by a real life industrial production-delivery supply chain where identical units are shipped after production to a third party warehouse and then sold at a future date for possible installation. Due to practical limitations, at any given time point, the exact installation as well as the failure times are known for only those units which have failed within that time frame after the installation. Hence, in-house reliability engineers are presented with a very limited, as well as partial, data to estimate different model parameters related to installation and failure distributions. In reality, other units in the batch are generally not utilized due to lack of proper statistical methodology, leading to gross misspecification. In this paper we have introduced a likelihood based parametric and computationally efficient solution to overcome this problem. 	
1104.0183v2	http://arxiv.org/pdf/1104.0183v2	2011	Exact and Efficient Algorithm to Discover Extreme Stochastic Events in   Wind Generation over Transmission Power Grids	Michael Chertkov|Mikhail Stepanov|Feng Pan|Ross Baldick	  In this manuscript we continue the thread of [M. Chertkov, F. Pan, M. Stepanov, Predicting Failures in Power Grids: The Case of Static Overloads, IEEE Smart Grid 2011] and suggest a new algorithm discovering most probable extreme stochastic events in static power grids associated with intermittent generation of wind turbines. The algorithm becomes EXACT and EFFICIENT (polynomial) in the case of the proportional (or other low parametric) control of standard generation, and log-concave probability distribution of the renewable generation, assumed known from the wind forecast. We illustrate the algorithm's ability to discover problematic extreme events on the example of the IEEE RTS-96 model of transmission with additions of 10%, 20% and 30% of renewable generation. We observe that the probability of failure may grow but it may also decrease with increase in renewable penetration, if the latter is sufficiently diversified and distributed. 	
1106.1634v1	http://arxiv.org/pdf/1106.1634v1	2011	Repair Optimal Erasure Codes through Hadamard Designs	Dimitris S. Papailiopoulos|Alexandros G. Dimakis|Viveck R. Cadambe	  In distributed storage systems that employ erasure coding, the issue of minimizing the total {\it communication} required to exactly rebuild a storage node after a failure arises. This repair bandwidth depends on the structure of the storage code and the repair strategies used to restore the lost data. Designing high-rate maximum-distance separable (MDS) codes that achieve the optimum repair communication has been a well-known open problem. In this work, we use Hadamard matrices to construct the first explicit 2-parity MDS storage code with optimal repair properties for all single node failures, including the parities. Our construction relies on a novel method of achieving perfect interference alignment over finite fields with a finite file size, or number of extensions. We generalize this construction to design $m$-parity MDS codes that achieve the optimum repair communication for single systematic node failures and show that there is an interesting connection between our $m$-parity codes and the systematic-repair optimal permutation-matrix based codes of Tamo {\it et al.} \cite{Tamo} and Cadambe {\it et al.} \cite{PermCodes_ISIT, PermCodes}. 	
1106.5457v1	http://arxiv.org/pdf/1106.5457v1	2011	Modelling Resilience in Cloud-Scale Data Centres	John Cartlidge|Ilango Sriram	  The trend for cloud computing has initiated a race towards data centres (DC) of an ever-increasing size. The largest DCs now contain many hundreds of thousands of virtual machine (VM) services. Given the finite lifespan of hardware, such large DCs are subject to frequent hardware failure events that can lead to disruption of service. To counter this, multiple redundant copies of task threads may be distributed around a DC to ensure that individual hardware failures do not cause entire jobs to fail. Here, we present results demonstrating the resilience of different job scheduling algorithms in a simulated DC with hardware failure. We use a simple model of jobs distributed across a hardware network to demonstrate the relationship between resilience and additional communication costs of different scheduling methods. 	
1107.0614v4	http://arxiv.org/pdf/1107.0614v4	2015	Estimating failure probabilities	Holger Drees|Laurens de Haan	  In risk management, often the probability must be estimated that a random vector falls into an extreme failure set. In the framework of bivariate extreme value theory, we construct an estimator for such failure probabilities and analyze its asymptotic properties under natural conditions. It turns out that the estimation error is mainly determined by the accuracy of the statistical analysis of the marginal distributions if the extreme value approximation to the dependence structure is at least as accurate as the generalized Pareto approximation to the marginal distributions. Moreover, we establish confidence intervals and briefly discuss generalizations to higher dimensions and issues arising in practical applications as well. 	
1107.1409v2	http://arxiv.org/pdf/1107.1409v2	2012	Fluctuations of spiked random matrix models and failure diagnosis in   sensor networks	Romain Couillet|Walid Hachem	  In this article, the joint fluctuations of the extreme eigenvalues and eigenvectors of a large dimensional sample covariance matrix are analyzed when the associated population covariance matrix is a finite-rank perturbation of the identity matrix, corresponding to the so-called spiked model in random matrix theory. The asymptotic fluctuations, as the matrix size grows large, are shown to be intimately linked with matrices from the Gaussian unitary ensemble (GUE). When the spiked population eigenvalues have unit multiplicity, the fluctuations follow a central limit theorem. This result is used to develop an original framework for the detection and diagnosis of local failures in large sensor networks, for known or unknown failure magnitude. 	
1109.1213v1	http://arxiv.org/pdf/1109.1213v1	2011	Heterogeneity, correlations and financial contagion	Fabio Caccioli|Thomas A. Catanach|J. Doyne Farmer	  We consider a model of contagion in financial networks recently introduced in the literature, and we characterize the effect of a few features empirically observed in real networks on the stability of the system. Notably, we consider the effect of heterogeneous degree distributions, heterogeneous balance sheet size and degree correlations between banks. We study the probability of contagion conditional on the failure of a random bank, the most connected bank and the biggest bank, and we consider the effect of targeted policies aimed at increasing the capital requirements of a few banks with high connectivity or big balance sheets. Networks with heterogeneous degree distributions are shown to be more resilient to contagion triggered by the failure of a random bank, but more fragile with respect to contagion triggered by the failure of highly connected nodes. A power law distribution of balance sheet size is shown to induce an inefficient diversification that makes the system more prone to contagion events. A targeted policy aimed at reinforcing the stability of the biggest banks is shown to improve the stability of the system in the regime of high average degree. Finally, disassortative mixing, such as that observed in real banking networks, is shown to enhance the stability of the system. 	
1110.3617v1	http://arxiv.org/pdf/1110.3617v1	2011	Failure of the Maxwell relation for the quantification of caloric   effects in ferroic materials	Robert Niemann|Oleg Heczko|Ludwig Schultz|Sebastian Fähler	  Giant caloric effects were reported in elasto-, electro- and magnetocaloric materials near phase transformations. Commonly, their entropy change is indirectly evaluated by a Maxwell relation. We report the fundamental failure of this approach. We analyze exemplarily the Ni-Mn-Ga magnetic shape memory alloy. An applied field results in magnetically induced reorientation of martensitic variants, which form during the phase transformation. This results in a spurious magnetocaloric effect, which only disappears when repeating the measurement a second time. This failure is universal as the vector character of the applied field is not considered in the common scalar evaluation of a Maxwell relation. 	
1201.1661v1	http://arxiv.org/pdf/1201.1661v1	2012	Slick Packets	Giang T. K. Nguyen|Rachit Agarwal|Junda Liu|Matthew Caesar|P. Brighten Godfrey|Scott Shenker	  Source-controlled routing has been proposed as a way to improve flexibility of future network architectures, as well as simplifying the data plane. However, if a packet specifies its path, this precludes fast local re-routing within the network. We propose SlickPackets, a novel solution that allows packets to slip around failures by specifying alternate paths in their headers, in the form of compactly-encoded directed acyclic graphs. We show that this can be accomplished with reasonably small packet headers for real network topologies, and results in responsiveness to failures that is competitive with past approaches that require much more state within the network. Our approach thus enables fast failure response while preserving the benefits of source-controlled routing. 	
1201.6053v1	http://arxiv.org/pdf/1201.6053v1	2012	A Comparison Between Data Mining Prediction Algorithms for Fault   Detection(Case study: Ahanpishegan co.)	Golriz Amooee|Behrouz Minaei-Bidgoli|Malihe Bagheri-Dehnavi	  In the current competitive world, industrial companies seek to manufacture products of higher quality which can be achieved by increasing reliability, maintainability and thus the availability of products. On the other hand, improvement in products lifecycle is necessary for achieving high reliability. Typically, maintenance activities are aimed to reduce failures of industrial machinery and minimize the consequences of such failures. So the industrial companies try to improve their efficiency by using different fault detection techniques. One strategy is to process and analyze previous generated data to predict future failures. The purpose of this paper is to detect wasted parts using different data mining algorithms and compare the accuracy of these algorithms. A combination of thermal and physical characteristics has been used and the algorithms were implemented on Ahanpishegan's current data to estimate the availability of its produced parts.   Keywords: Data Mining, Fault Detection, Availability, Prediction Algorithms. 	 data mining, fault detection, availability, prediction algorithms 

1202.1733v1	http://arxiv.org/pdf/1202.1733v1	2012	Handover Necessity Estimation for 4G Heterogeneous Networks	Abdoul Aziz Issaka Hassane|Li Renfa|Zeng Fanzi	  One of the most challenges of 4G network is to have a unified network of heterogeneous wireless networks. To achieve seamless mobility in such a diverse environment, vertical hand off is still a challenging problem. In many situations handover failures and unnecessary handoffs are triggered causing degradation of services, reduction in throughput and increase the blocking probability and packet loss. In this paper a new vertical handoff decision algorithm handover necessity estimation (HNE), is proposed to minimize the number of handover failure and unnecessary handover in heterogeneous wireless networks. we have proposed a multi criteria vertical handoff decision algorithm based on two parts: traveling time estimation and time threshold calculation. Our proposed methods are compared against two other methods: (a) the fixed RSS threshold based method, in which handovers between the cellular network and the WLAN are initiated when the RSS from the WLAN reaches a fixed threshold, and (b) the hysteresis based method, in which a hysteresis is introduced to prevent the ping-pong effect. Simulation results show that, this method reduced the number of handover failures and unnecessary handovers up to 80% and 70%, respectively. 	
1203.1590v1	http://arxiv.org/pdf/1203.1590v1	2012	Study of effects of failure of beamline elements and their compensation   in CW superconducting linac	A. Saini|K. Ranjan|N. Solyak|S. Mishra|V. Yakovlev	  Project-X is the proposed high intensity proton facility to be built at Fermilab, US. First stage of the Project-X consists of superconducting linac which will be operated in continuous wave (CW) mode to accelerate the beam from 2.5 MeV to 3 GeV. The operation at CW mode puts high tolerances on the beam line components, particularly on radiofrequency (RF) cavity. The failure of beam line elements at low energy is very critical as it results in mis-match of the beam with the following sections due to different beam parameters than designed parameter. It makes the beam unstable which causes emittance dilution, and ultimately results in beam losses. In worst case, it could affect the reliability of the machine and may lead to the shutdown of the Linac to replace the failed elements. Thus, it is important to study these effects and their compensation to get smooth beam propagation in linac. This paper describes the results of study performed for the failure of RF cavity & solenoid in SSR0 section. 	
1204.2490v1	http://arxiv.org/pdf/1204.2490v1	2012	Lateral Stability Analysis of Hypersonic Vehicle under Pressure   Fluctuation by Solving Mathieu Differential Equation	Qingkai Wei|Xun Huang	  Two recent test failures of Hypersonic Technology Vehicle 2 impose a strike to the increasingly growing enthusiasm, not only on the United States side. It is important to find out the exact failure reason, otherwise a solution is impossible. In this Note, we propose a potential failure reason from the perspective of lateral stability analysis. We argue that the time variant pressure fluctuations, which are normally omitted in classical aircraft dynamics analysis, could not be neglected in dynamic analysis of hypersonic vehicles. To demonstrate the idea, a hypersonic model is imagined in this work and its aerodynamic parameters are estimated using fundamental fluid principles. Pressure fluctuations are thereafter estimated by an empirical formula. A lateral dynamic equation is set up, taking those time variant fluctuations into account. The resulted equation is a Mathieu differential equation. Numerical solutions of this equation show that the inclusion of fluctuation terms generates more complicated dynamics and should be considered in flight controller design. 	
1206.0190v1	http://arxiv.org/pdf/1206.0190v1	2012	Fast Accelerated Failure Time Modeling for Case-Cohort Data	Steven Chiou|Sangwook Kang|Jun Yan	  Semiparametric accelerated failure time (AFT) models directly relate the predicted failure times to covariates and are a useful alternative to models that work on the hazard function or the survival function. For case-cohort data, much less development has been done with AFT models. In addition to the missing covariates outside of the sub-cohort in controls, challenges from AFT model inferences with full cohort are retained. The regression parameter estimator is hard to compute because the most widely used rank-based estimating equations are not smooth. Further, its variance depends on the unspecified error distribution, and most methods rely on computationally intensive bootstrap to estimate it. We propose fast rank-based inference procedures for AFT models, applying recent methodological advances to the context of case-cohort data. Parameters are estimated with an induced smoothing approach that smooths the estimating functions and facilitates the numerical solution. Variance estimators are obtained through efficient resampling methods for nonsmooth estimating functions that avoids full blown bootstrap. Simulation studies suggest that the recommended procedure provides fast and valid inferences among several competing procedures. Application to a tumor study demonstrates the utility of the proposed method in routine data analysis. 	
1209.1358v1	http://arxiv.org/pdf/1209.1358v1	2012	On Byzantine Broadcast in Loosely Connected Networks	Alexandre Maurer|Sébastien Tixeuil	  We consider the problem of reliably broadcasting information in a multihop asynchronous network that is subject to Byzantine failures. Most existing approaches give conditions for perfect reliable broadcast (all correct nodes deliver the authentic message and nothing else), but they require a highly connected network. An approach giving only probabilistic guarantees (correct nodes deliver the authentic message with high probability) was recently proposed for loosely connected networks, such as grids and tori. Yet, the proposed solution requires a specific initialization (that includes global knowledge) of each node, which may be difficult or impossible to guarantee in self-organizing networks - for instance, a wireless sensor network, especially if they are prone to Byzantine failures. In this paper, we propose a new protocol offering guarantees for loosely connected networks that does not require such global knowledge dependent initialization. In more details, we give a methodology to determine whether a set of nodes will always deliver the authentic message, in any execution. Then, we give conditions for perfect reliable broadcast in a torus network. Finally, we provide experimental evaluation for our solution, and determine the number of randomly distributed Byzantine failures than can be tolerated, for a given correct broadcast probability. 	
1209.3733v2	http://arxiv.org/pdf/1209.3733v2	2013	Cascade Failures from Distributed Generation in Power Grids	Antonio Scala|Sakshi Pahwa|Caterina Scoglio	  Power grids are nowadays experiencing a transformation due to the introduction of Distributed Generation based on Renewable Sources. At difference with classical Distributed Generation, where local power sources mitigate anomalous user consumption peaks, Renewable Sources introduce in the grid intrinsically erratic power inputs. By introducing a simple schematic (but realistic) model for power grids with stochastic distributed generation, we study the effects of erratic sources on the robustness of several IEEE power grid test networks with up to 2000 buses. We find that increasing the penetration of erratic sources causes the grid to fail with a sharp transition. We compare such results with the case of failures caused by the natural increasing power demand. 	
1209.6580v2	http://arxiv.org/pdf/1209.6580v2	2013	Testing MapReduce-Based Systems	João Eugenio Marynowski|Michel Albonico|Eduardo Cunha de Almeida|Gerson Sunyé	  MapReduce (MR) is the most popular solution to build applications for large-scale data processing. These applications are often deployed on large clusters of commodity machines, where failures happen constantly due to bugs, hardware problems, and outages. Testing MR-based systems is hard, since it is needed a great effort of test harness to execute distributed test cases upon failures. In this paper, we present a novel testing solution to tackle this issue called HadoopTest. This solution is based on a scalable harness approach, where distributed tester components are hung around each map and reduce worker (i.e., node). Testers are allowed to stimulate each worker to inject failures on them, monitor their behavior, and validate testing results. HadoopTest was used to test two applications bundled into Hadoop, the Apache open source MapReduce implementation. Our initial implementation demonstrates promising results, with HadoopTest coordinating test cases across distributed MapReduce workers, and finding bugs. 	
1210.1074v3	http://arxiv.org/pdf/1210.1074v3	2014	Density modification based reliability sensitivity analysis	Paul Lemaître|Ekatarina Sergienko|Aurélie Arnaud|Nicolas Bousquet|Fabrice Gamboa|Bertrand Iooss	  Sensitivity analysis of a numerical model, for instance simulating physical phenomena, is useful to quantify the influence of the inputs on the model responses. This paper proposes a new sensitivity index, based upon the modification of the probability density function (pdf) of the random inputs, when the quantity of interest is a failure probability (probability that a model output exceeds a given threshold). An input is considered influential if the input pdf modification leads to a broad change in the failure probability. These sensitivity indices can be computed using the sole set of simulations that has already been used to estimate the failure probability, thus limiting the number of calls to the numerical model. In the case of a Monte Carlo sample, asymptotical properties of the indices are derived. Based on Kullback-Leibler divergence, several types of input perturbations are introduced. The relevance of this new sensitivity analysis method is analysed through three case studies. 	
1301.5993v1	http://arxiv.org/pdf/1301.5993v1	2013	A Probabilistic Approach to Analysis of Reliability in n-D Meshes with   Interconnect Router Failures	Farshad Safaei|Majed ValadBeigi	  The routing algorithms for parallel computers, on-chip networks, multi-core processors, and multiprocessors system-on-chip (MP-SoCs) exhibit router failures must be able to handle interconnect router failures that render a symmetrical mesh non-symmetrically. When developing a routing methodology, the time complexity of calculation should be minimal, and thus complicated routing strategies to introduce profitable paths may not be appropriate. Several reports have been released in the literature on using the concept of fault rings to provide detour paths to messages blocked by faults and to route messages around the fault regions. In order to analyze the performance of such algorithms, it is required to investigate the characteristics of fault rings. In this paper, we introduce a novel performance index of network reliability presenting the probability of message facing fault rings, and evaluating the performance-related reliability of adaptive routing schemes in n-D mesh-based interconnection networks with a variety of common cause fault patterns. Sufficient simulation results of Monte-Carlo method are conducted to demonstrate the correctness of the proposed analytical model. 	
1301.7503v1	http://arxiv.org/pdf/1301.7503v1	2013	Finite Length Analysis on Listing Failure Probability of Invertible   Bloom Lookup Tables	Daichi Yugawa|Tadashi Wadayama	  The Invertible Bloom Lookup Tables (IBLT) is a data structure which supports insertion, deletion, retrieval and listing operations of the key-value pair. The IBLT can be used to realize efficient set reconciliation for database synchronization. The most notable feature of the IBLT is the complete listing operation of the key-value pairs based on the algorithm similar to the peeling algorithm for low-density generator-matrix (LDGM) codes. In this paper, we will present a stopping set (SS) analysis for the IBLT which reveals finite length behaviors of the listing failure probability. The key of the analysis is enumeration of the number of stopping matrices of given size. We derived a novel recursive formula useful for computationally efficient enumeration. An upper bound on the listing failure probability based on the union bound accurately captures the error floor behaviors. It will be shown that, in the error floor region, the dominant SS have size 2. We propose a simple modification on hash functions, which are called SS avoiding hash functions, for preventing occurrences of the SS of size 2. 	
1303.0393v1	http://arxiv.org/pdf/1303.0393v1	2013	The effect of shock-wave profile on dynamic brittle failure	J. Pablo Escobedo|Eric N. Brown|Carl P. Trujillo|Ellen K. Cerreta|George T. Gray III	  The influence of shock-wave-loading profile on the failure processes in a brittle material has been investigated. Tungsten heavy alloy (WHA) specimens have been subjected to two shock-wave loading profiles with a similar peak stress of 15.4 GPa but different pulse durations. Contrary to the strong dependence of strength on wave profile observed in ductile metals, for WHA, specimens subjected to different loading profiles exhibited similar spall strength and damage evolution morphology. Post-mortem examination of recovered samples revealed that dynamic failure for both loading profiles is dominated by brittle cleavage fracture, with additional energy dissipation through crack branching in the more brittle tungsten particles. Overall, in this brittle material all relevant damage kinetics and the spall strength are shown to be dominated by the shock peak stress, independent of pulse duration. 	
1305.0538v3	http://arxiv.org/pdf/1305.0538v3	2013	A Companion of "Relating Strong Behavioral Equivalences for Processes   with Nondeterminism and Probabilities"	Marco Bernardo|Rocco De Nicola|Michele Loreti	  In the paper "Relating Strong Behavioral Equivalences for Processes with Nondeterminism and Probabilities" to appear in TCS, we present a comparison of behavioral equivalences for nondeterministic and probabilistic processes. In particular, we consider strong trace, failure, testing, and bisimulation equivalences. For each of these groups of equivalences, we examine the discriminating power of three variants stemming from three approaches that differ for the way probabilities of events are compared when nondeterministic choices are resolved via deterministic schedulers. The established relationships are summarized in a so-called spectrum. However, the equivalences we consider in that paper are only a small subset of those considered in the original spectrum of equivalences for nondeterministic systems introduced by Rob van Glabbeek. In this companion paper we we enlarge the spectrum by considering variants of trace equivalences (completed-trace equivalences), additional decorated-trace equivalences (failure-trace, readiness, and ready-trace equivalences), and variants of bisimulation equivalences (kernels of simulation, completed-simulation, failure-simulation, and ready-simulation preorders). Moreover, we study how the spectrum changes when randomized schedulers are used instead of deterministic ones. 	
1306.1068v1	http://arxiv.org/pdf/1306.1068v1	2013	Software Process Models and Analysis on Failure of Software Development   Projects	Rupinder Kaur|Jyotsna Sengupta	  The software process model consists of a set of activities undertaken to design, develop and maintain software systems. A variety of software process models have been designed to structure, describe and prescribe the software development process. The software process models play a very important role in software development, so it forms the core of the software product. Software project failure is often devastating to an organization. Schedule slips, buggy releases and missing features can mean the end of the project or even financial ruin for a company. Oddly, there is disagreement over what it means for a project to fail. In this paper, discussion is done on current process models and analysis on failure of software development, which shows the need of new research. 	
1308.3320v1	http://arxiv.org/pdf/1308.3320v1	2013	Improving the Testability of Object-oriented Software during Testing and   Debugging Processes	Sujata Khatri|R. S. Chhillar|V. B. Singh	  Testability is the probability whether tests will detect a fault, given that a fault in the program exists. How efficiently the faults will be uncovered depends upon the testability of the software. Various researchers have proposed qualitative and quantitative techniques to improve and measure the testability of software. In literature, a plethora of reliability growth models have been used to assess and measure the quantitative quality assessment of software during testing and operational phase. The knowledge about failure distribution and their complexity can improve the testability of software. Testing effort allocation can be made easy by knowing the failure distribution and complexity of faults, and this will ease the process of revealing faults from the software. As a result, the testability of the software will be improved. The parameters of the model along with the proportion of faults of different complexity to be removed from the software have been presented in the paper .We have used failure data of two object oriented software developed under open source environment namely MySQL for python and Squirrel SQL Client for estimation purpose 	
1310.1761v3	http://arxiv.org/pdf/1310.1761v3	2014	Simple CHT: A New Derivation of the Weakest Failure Detector for   Consensus	Eli Gafni|Petr Kuznetsov	  The paper proposes an alternative proof that Omega, an oracle that outputs a process identifier and guarantees that eventually the same correct process identifier is output at all correct processes, provides minimal information about failures for solving consensus in read-write shared-memory systems: every oracle that gives enough failure information to solve consensus can be used to implement Omega.   Unlike the original proof by Chandra, Hadzilacos and Toueg (CHT), the proof presented in this paper builds upon the very fact that 2-process wait-free consensus is impossible. Also, since the oracle that is used to implement can solve consensus, the implementation is allowed to directly access consensus objects. As a result, the proposed proof is shorter and conceptually simpler than the original one. 	
1312.1710v1	http://arxiv.org/pdf/1312.1710v1	2013	The failure of DFT-based computations for a stepped-substrate-supported   correlated Co wire	Nader Zaki|Hyowon Park|Richard M. Osgood|Andrew J. Millis|Chris A. Marianetti	  Density functional theory (DFT) has been immensely successful in its ability to predict physical properties, and, in particular, structures of condensed matter systems. Here, however, we show that DFT qualitatively fails to predict the dimerized structural phase for a monatomic Co wire that is self-assembled on a vicinal, i.e. stepped, Cu(111) substrate. To elucidate the nature of this failure, we compute the energetics of a Co chain on a Cu surface, step, notch, and embedded in bulk. The results demonstrate that increasing Co coordination extinguishes the dimerization, indicating that the failure of DFT for Co on the Cu step arises from excessive hybridization, which both weakens the ferromagnetic correlations that drive the dimerization and increases the bonding that opposes dimerization. Additionally, we show that including local interactions via DFT+U or DFT+DMFT does not restore the dimerization for the step-substrate supported wire, though the Co wire does dimerize in DFT+DMFT for the isolated vacuum case. This system can serve as a benchmark for future electronic structure methods. 	
1401.6615v1	http://arxiv.org/pdf/1401.6615v1	2014	Iterative Approximate Consensus in the presence of Byzantine Link   Failures	Lewis Tseng|Nitin Vaidya	  This paper explores the problem of reaching approximate consensus in synchronous point-to-point networks, where each directed link of the underlying communication graph represents a communication channel between a pair of nodes. We adopt the transient Byzantine link failure model [15, 16], where an omniscient adversary controls a subset of the directed communication links, but the nodes are assumed to be fault-free.   Recent work has addressed the problem of reaching approximate consen- sus in incomplete graphs with Byzantine nodes using a restricted class of iterative algorithms that maintain only a small amount of memory across iterations [22, 21, 23, 12]. However, to the best of our knowledge, we are the first to consider approximate consensus in the presence of Byzan- tine links. We extend our past work that provided exact characterization of graphs in which the iterative approximate consensus problem in the presence of Byzantine node failures is solvable [22, 21]. In particular, we prove a tight necessary and sufficient condition on the underlying com- munication graph for the existence of iterative approximate consensus algorithms under transient Byzantine link model. The condition answers (part of) the open problem stated in [16]. 	
1401.7921v2	http://arxiv.org/pdf/1401.7921v2	2014	Catastrophic shifts and lethal thresholds in a propagating front model   of unstable tumour progression	Daniel R. Amor|Ricard V. Solé	  Unstable dynamics characterizes the evolution of most solid tumors. Because of an increased failure of maintaining genome integrity, a cumulative increase in the levels of gene mutation and loss is observed. Previous work suggests that instability thresholds to cancer progression exist, defining phase transition phenomena separating tumor-winning scenarios from tumor extinction or coexistence phases. Here we present an integral equation approach to the quasispecies dynamics of unstable cancer. The model exhibits two main phases, characterized by either the success or failure of cancer tissue. Moreover, the model predicts that tumor failure can be due to either a reduced selective advantage over healthy cells or excessive instability. We also derive an approximate, analytical solution that predicts the front speed of aggressive tumor populations on the instability space. 	
1402.4783v2	http://arxiv.org/pdf/1402.4783v2	2014	Mapping systemic risk: critical degree and failures distribution in   financial networks	Matteo Smerlak|Brady Stoll|Agam Gupta|James S. Magdanz	  The 2008 financial crisis illustrated the need for a thorough, functional understanding of systemic risk in strongly interconnected financial structures. Dynamic processes on complex networks being intrinsically difficult, most recent studies of this problem have relied on numerical simulations. Here we report analytical results in a network model of interbank lending based on directly relevant financial parameters, such as interest rates and leverage ratios. Using a mean-field approach, we obtain a closed-form formula for the "critical degree", viz. the number of creditors per bank below which an individual shock can propagate throughout the network. We relate the failures distribution (probability that a single shock induces $F$ failures) to the degree distribution (probability that a bank has $k$ creditors), showing in particular that the former is fat-tailed whenever the latter is. Our criterion for the onset of contagion turns out to be isomorphic to the condition for cooperation to evolve on graphs and social networks, as recently formulated in evolutionary game theory. This remarkable connection supports recent calls for a methodological rapprochement between finance and ecology. 	
1402.7304v2	http://arxiv.org/pdf/1402.7304v2	2014	Emergence of cooperativity in plasticity of soft glassy materials	Le Bouil Antoine|Amon Axelle|McNamara Sean|Crassous Jérôme	  The elastic coupling between plastic events is generally invoked to interpret plastic properties and failure of amorphous soft glassy materials. We report an experiment where the emergence of a self-organized plastic flow is observed well before the failure. For this we impose an homogeneous stress on a granular material, and measure local deformations for very small strain increments using a light scattering setup. We observe a non-homogeneous strain that appears as transient bands of mesoscopic size and well defined orientation, different from the angle of the macroscopic frictional shear band that appears at failure. The presence and the orientation of those micro-bands may be understood by considering how localized plastic reorganizations redistribute stresses in a surrounding continuous elastic medium. We characterize the lengthscale and persistence of the structure. The presence of plastic events and the mesostructure of the plastic flow are compared to numerical simulations. 	
1403.0458v1	http://arxiv.org/pdf/1403.0458v1	2014	Persistence and failure of mean-field approximations adapted to a class   of systems of delay-coupled excitable units	I. Franovic|K. Todorovic|N. Vasovic|N. Buric	  We consider the approximations behind the typical mean-field model derived for a class of systems made up of type II excitable units influenced by noise and coupling delays. The formulation of the two approximations, referred to as the Gaussian and the quasi-independence approximation, as well as the fashion in which their validity is verified, are adapted to reflect the essential properties of the underlying system. It is demonstrated that the failure of the mean-field model associated with the breakdown of the quasi-independence approximation can be predicted by the noise-induced bistability in the dynamics of the mean-field system. As for the Gaussian approximation, its violation is related to the increase of noise intensity, but the actual condition for failure can be cast in qualitative, rather than quantitative terms. We also discuss how the fulfilment of the mean-field approximations affects the statistics of the first return times for the local and global variables, further exploring the link between the fulfilment of the quasi-independence approximation and certain forms of synchronization between the individual units. 	
1405.2986v1	http://arxiv.org/pdf/1405.2986v1	2014	Semantic Support for Log Analysis of Safety-Critical Embedded Systems	Alessio Venticinque|Nicola Mazzocca|Salvatore Venticinque|Massimo Ficco	  Testing is a relevant activity for the development life-cycle of Safety Critical Embedded systems. In particular, much effort is spent for analysis and classification of test logs from SCADA subsystems, especially when failures occur. The human expertise is needful to understand the reasons of failures, for tracing back the errors, as well as to understand which requirements are affected by errors and which ones will be affected by eventual changes in the system design. Semantic techniques and full text search are used to support human experts for the analysis and classification of test logs, in order to speedup and improve the diagnosis phase. Moreover, retrieval of tests and requirements, which can be related to the current failure, is supported in order to allow the discovery of available alternatives and solutions for a better and faster investigation of the problem. 	
1405.3009v1	http://arxiv.org/pdf/1405.3009v1	2014	Dynamic Behavior of Interacting between Epidemics and Cascades on   Heterogeneous Networks	Lurong Jiang|Xinyu Jin|Yongxiang Xia|Bo Ouyang|Duanpo Wu	  Epidemic spreading and cascading failure are two important dynamical processes over complex networks. They have been investigated separately for a long history. But in the real world, these two dynamics sometimes may interact with each other. In this paper, we explore a model combined with SIR epidemic spreading model and local loads sharing cascading failure model. There exists a critical value of tolerance parameter that whether the epidemic with high infection probability can spread out and infect a fraction of the network in this model. When the tolerance parameter is smaller than the critical value, cascading failure cuts off abundant of paths and blocks the spreading of epidemic locally. While the tolerance parameter is larger than the critical value, epidemic spreads out and infects a fraction of the network. A method for estimating the critical value is proposed. In simulation, we verify the effectiveness of this method in Barab\'asi-Albert (BA) networks. 	
1405.4845v1	http://arxiv.org/pdf/1405.4845v1	2014	Elasto-plastic response of reversibly crosslinked biopolymer bundles	Poulomi Sadhukhan|Ole Schuman|Claus Heussinger	  We study the response of F-actin bundles to driving forces through a simple analytical model. We consider two filaments connected by reversibly bound crosslinks and driven by an external force. Two failure modes under load can be defined. \textit{Brittle failure} is observed when crosslinks suddenly and collectively unbind, leading to catastrophic loss of bundle integrity. During \textit{ductile failure}, on the other hand, bundle integrity is maintained, however at the cost of crosslink reorganization and defect formation. We present phase diagrams for the onset of failure, highlighting the importance of the crosslink stiffness for these processes. Crossing the phase boundaries, force-deflection curves display (frequency-dependent) hysteresis loops, reflecting the first-order character of the failure processes. We evidence how the introduction of defects can lead to complex elasto-plastic relaxation processes, once the force is switched off. Depending on, both, the time-scale for defect motion as well as the crosslink stiffness, bundles can remain in a quasi-permanent plastically deformed state for a very long time. 	
1405.6646v3	http://arxiv.org/pdf/1405.6646v3	2016	Error Reporting in Parsing Expression Grammars	André Murbach Maidl|Sérgio Medeiros|Fabio Mascarenhas|Roberto Ierusalimschy	  Parsing Expression Grammars (PEGs) describe top-down parsers. Unfortunately, the error-reporting techniques used in conventional top-down parsers do not directly apply to parsers based on Parsing Expression Grammars (PEGs), so they have to be somehow simulated. While the PEG formalism has no account of semantic actions, actual PEG implementations add them, and we show how to simulate an error-reporting heuristic through these semantic actions.   We also propose a complementary error reporting strategy that may lead to better error messages: labeled failures. This approach is inspired by exception handling of programming languages, and lets a PEG define different kinds of failure, with each ordered choice operator specifying which kinds it catches. Labeled failures give a way to annotate grammars for better error reporting, to express some of the error reporting strategies used by deterministic parser combinators, and to encode predictive top-down parsing in a PEG. 	
1406.2106v4	http://arxiv.org/pdf/1406.2106v4	2016	Percolation on Networks with Antagonistic and Dependent Interactions	Bhushan Kotnis|Joy Kuri	  Drawing inspiration from real world interacting systems we study a system consisting of two networks that exhibit antagonistic and dependent interactions. By antagonistic and dependent interactions, we mean, that a proportion of functional nodes in a network cause failure of nodes in the other, while failure of nodes in the other results in failure of links in the first. As opposed to interdependent networks, which can exhibit first order phase transitions, we find that the phase transitions in such networks are continuous. Our analysis shows that, compared to an isolated network, the system is more robust against random attacks. Surprisingly, we observe a region in the parameter space where the giant connected components of both networks start oscillating. Furthermore, we find that for Erdos-Renyi and scale free networks the system oscillates only when the dependency and antagonism between the two networks is very high. We believe that this study can further our understanding of real world interacting systems. 	
1406.5923v1	http://arxiv.org/pdf/1406.5923v1	2014	Impact of Equipment Failures and Wind Correlation on Generation   Expansion Planning	Salvador Pineda|Juan M. Morales|Yi Ding|Jacob Oestegaard	  Generation expansion planning has become a complex problem within a deregulated electricity market environment due to all the uncertainties affecting the profitability of a given investment. Current expansion models usually overlook some of these uncertainties in order to reduce the computational burden. In this paper, we raise a flag on the importance of both equipment failures (units and lines) and wind power correlation on generation expansion decisions. For this purpose, we use a bilevel stochastic optimization problem, which models the sequential and noncooperative game between the generating company (GENCO) and the system operator. The upper-level problem maximizes the GENCO's expected profit, while the lower-level problem simulates an hourly market-clearing procedure, through which LMPs are determined. The uncertainty pertaining to failures and wind power correlation are characterized by a scenario set, and their impact on generation expansion decisions are quantified and discussed for a 24-bus power system. 	
1407.0927v1	http://arxiv.org/pdf/1407.0927v1	2014	Modelling an Aircraft Landing System in Event-B (Full Report)	Dominique Méry|Neeraj Kumar Singh	  The failure of hardware or software in a critical system can lead to loss of lives. The design errors can be main source of the failures that can be introduced during system development process. Formal techniques are an alternative approach to verify the correctness of critical systems, overcoming limitations of the traditional validation techniques such as simulation and testing. The increasing complexity and failure rate brings new challenges in the area of verification and validation of avionic systems. Since the reliability of the software cannot be quantified, the \textit{correct by construction} approach can implement a reliable system. Refinement plays a major role to build a large system incrementally from an abstract specification to a concrete system. This paper contributes as a stepwise formal development of the landing system of an aircraft. The formal models include the complex behaviour, temporal behaviour and sequence of operations of the landing gear system. The models are formalized in Event-B modelling language, which supports stepwise refinement. This case study is considered as a benchmark for techniques and tools dedicated to the verification of behavioural properties of systems. The report is the full version of a paper published for the ABZ 2014 Case Study. is 	
1407.0952v2	http://arxiv.org/pdf/1407.0952v2	2014	Predicting Lifetime of Dynamical Networks Experiencing Persistent Random   Attacks	B. Podobnik|T. Lipic|D. Horvatic|A. Majdandzic|S. Bishop|H. E. Stanley	  Empirical estimation of critical points at which complex systems abruptly flip from one state to another is among the remaining challenges in network science. However, due to the stochastic nature of critical transitions it is widely believed that critical points are difficult to estimate, and it is even more difficult, if not impossible, to predict the time such transitions occur [1-4]. We analyze a class of decaying dynamical networks experiencing persistent attacks in which the magnitude of the attack is quantified by the probability of an internal failure, and there is some chance that an internal failure will be permanent. When the fraction of active neighbors declines to a critical threshold, cascading failures trigger a network breakdown. For this class of network we find both numerically and analytically that the time to the network breakdown, equivalent to the network lifetime, is inversely dependent upon the magnitude of the attack and logarithmically dependent on the threshold. We analyze how permanent attacks affect dynamical network robustness and use the network lifetime as a measure of dynamical network robustness offering new methodological insight into system dynamics. 	
1407.2300v1	http://arxiv.org/pdf/1407.2300v1	2014	Co- Versus Contravariant Finiteness of Categories of Representations	B. Huisgen-Zimmermann|S. O. Smalø	  This article supplements recent work of the authors. (1) A criterion for failure of covariant finiteness of a full subcategory of $\Lambda\text{-mod}$ is given, where $\Lambda$ is a finite dimensional algebra. The criterion is applied to the category ${\cal P}^{\infty}(\Lambda\rm{-mod})$ of all finitely generated $\Lambda$-modules of finite projective dimension, yielding a negative answer to the question whether ${\cal P}^{\infty}(\Lambda\rm{-mod})$ is always covariantly finite in $\Lambda\text{-mod}$. Part (2) concerns contravariant finiteness of ${\cal P}^{\infty}(\Lambda\rm{-mod})$. An example is given where this condition fails, the failure being, however, curable via a sequence of one-point extensions. In particular, this example demonstrates that curing failure of contravariant finiteness of ${\cal P}^{\infty}(\Lambda\rm{-mod})$ usually involves a tradeoff with respect to other desirable qualities of the algebra. 	
1408.6760v1	http://arxiv.org/pdf/1408.6760v1	2014	Scalability and Resilience of Software-Defined Networking: An Overview	Benjamin J. van Asten|Niels L. M. van Adrichem|Fernando A. Kuipers	  Software-Defined Networking (SDN) allows to control the available network resources by an intelligent and centralized authority in order to optimize traffic flows in a flexible manner. However, centralized control may face scalability issues when the network size or the number of traffic flows increases. Also, a centralized controller may form a single point of failure, thereby affecting the network resilience.   This article provides an overview of SDN that focuses on (1) scalability concerning the increased control overhead faced by a central controller, and (2) resiliency in terms of protection against controller failure, network topology failure and security in terms of malicious attacks. 	
1409.4991v4	http://arxiv.org/pdf/1409.4991v4	2015	RoBuSt: A Crash-Failure-Resistant Distributed Storage System	Martina Eikel|Christian Scheideler|Alexander Setzer	  In this work we present the first distributed storage system that is provably robust against crash failures issued by an adaptive adversary, i.e., for each batch of requests the adversary can decide based on the entire system state which servers will be unavailable for that batch of requests. Despite up to $\gamma n^{1/\log\log n}$ crashed servers, with $\gamma>0$ constant and $n$ denoting the number of servers, our system can correctly process any batch of lookup and write requests (with at most a polylogarithmic number of requests issued at each non-crashed server) in at most a polylogarithmic number of communication rounds, with at most polylogarithmic time and work at each server and only a logarithmic storage overhead.   Our system is based on previous work by Eikel and Scheideler (SPAA 2013), who presented IRIS, a distributed information system that is provably robust against the same kind of crash failures. However, IRIS is only able to serve lookup requests. Handling both lookup and write requests has turned out to require major changes in the design of IRIS. 	
1411.1151v1	http://arxiv.org/pdf/1411.1151v1	2014	Guaranteed Monte Carlo Methods for Bernoulli Random Variables	Lan Jiang|Fred J. Hickernell	  Simple Monte Carlo is a versatile computational method with a convergence rate of $O(n^{-1/2})$. It can be used to estimate the means of random variables whose distributions are unknown. Bernoulli random variables, $Y$, are widely used to model success (failure) of complex systems. Here $Y=1$ denotes a success (failure), and $p=\mathbb{E}(Y)$ denotes the probability of that success (failure). Another application of Bernoulli random variables is $Y=\mathbb{1}_{R}(\boldsymbol{X})$, where then $p$ is the probability of $\boldsymbol{X}$ lying in the region $R$. This article explores how estimate $p$ to a prescribed absolute error tolerance, $\varepsilon$, with a high level of confidence, $1-\alpha$. The proposed algorithm automatically determines the number of samples of $Y$ needed to reach the prescribed error tolerance with the specified confidence level by using Hoeffding's inequality. The algorithm described here has been implemented in MATLAB and is part of the Guaranteed Automatic Integration Library (GAIL). 	
1411.1217v1	http://arxiv.org/pdf/1411.1217v1	2014	Kalman Filtering over Gilbert-Elliott Channels: Stability Conditions and   the Critical Curve	Junfeng Wu|Guodong Shi|Brian D. O. Anderson|Karl Henrik Johansson	  This paper investigates the stability of Kalman filtering over Gilbert-Elliott channels where random packet drop follows a time-homogeneous two-state Markov chain whose state transition is determined by a pair of failure and recovery rates. First of all, we establish a relaxed condition guaranteeing peak-covariance stability described by an inequality in terms of the spectral radius of the system matrix and transition probabilities of the Markov chain. We further show that that condition can be interpreted using a linear matrix inequality feasibility problem. Next, we prove that the peak-covariance stability implies mean-square stability, if the system matrix has no defective eigenvalues on the unit circle. This connection between the two stability notions holds for any random packet drop process. We prove that there exists a critical curve in the failure-recovery rate plane, below which the Kalman filter is mean-square stable and no longer mean-square stable above, via a coupling method in stochastic processes. Finally, a lower bound for this critical failure rate is obtained making use of the relationship we establish between the two stability criteria, based on an approximate relaxation of the system matrix. 	
1501.06487v1	http://arxiv.org/pdf/1501.06487v1	2015	Average probability of a dangerous failure on demand: Different   modelling methods, similar results	Florent Brissaud|Fernando Luiz	  According to the IEC 61508 functional safety standard, it is required to estimate the achieved safety integrity of the system due to random hardware failures. For a safety function operating in a low demand mode, this measure is the average probability of a dangerous failure on demand (PFDavg). In the present paper, four techniques have been applied to various configurations of a case study: fault tree analyses supported by GRIF/Tree, multi-phase Markov models supported by GRIF/Markov, stochastic Petri nets with predicates supported by GRIF/Petri, and approximate equations (developed by DNV and different from those given in IEC 61508) supported by OrbitSIL. It is shown that all these methods yield very similar results for PFDavg, taking the characteristics required by the standard into account. The choice of a method should therefore not be determined by dogmatic assumptions, but should result of a balance between modelling effort and objectives, given the system properties. For this task, a discussion about pros and cons of each method is proposed. 	
1503.07713v1	http://arxiv.org/pdf/1503.07713v1	2015	A method for business process reengineering based on enterprise ontology	Pedram Bahramnejad|Sayed Mehran Sharafi|Akbar Nabiollahi	  Business Process Reengineering increases enterprise's chance to survive in competition among organizations , but failure rate among reengineering efforts is high, so new methods that decrease failure, are needed, in this paper a business process reengineering method is presented that uses Enterprise Ontology for modelling the current system and its goal is to improve analysing current system and decreasing failure rate of BPR, and cost and time of performing processes, In this method instead of just modelling processes, processes with their : interactions and relations, environment, staffs and customers will be modelled in enterprise ontology. Also in choosing processes for reengineering step, after choosing them, processes which, according to the enterprise ontology, has the most connection with the chosen ones, will also be chosen to reengineer, finally this method is implemented on a company and As-Is and To-Be processes are simulated and compared by ARIS tools, Report and Simulation Experiment 	
1504.03449v1	http://arxiv.org/pdf/1504.03449v1	2015	Design Tool To Express Failure Detection Protocols	Vincenzo De Florio|Chris Blondia	  Failure detection protocols---a fundamental building block for crafting fault-tolerant distributed systems---are in many cases described by their authors making use of informal pseudo-codes of their conception. Often these pseudo-codes use syntactical constructs that are not available in COTS programming languages such as C or C++. This translates into informal descriptions that call for ad hoc interpretations and implementations. Being informal, these descriptions cannot be tested by their authors, which may translate into insufficiently detailed or even faulty specifications. This paper tackles this problem introducing a formal syntax for those constructs and a C library that implements them---a tool-set to express and reason about failure detection protocols. The resulting specifications are longer but non ambiguous, and eligible for becoming a standard form. 	
1504.07513v2	http://arxiv.org/pdf/1504.07513v2	2015	The xSAP Safety Analysis Platform	Benjamin Bittner|Marco Bozzano|Roberto Cavada|Alessandro Cimatti|Marco Gario|Alberto Griggio|Cristian Mattarei|Andrea Micheli|Gianni Zampedri	  This paper describes the xSAP safety analysis platform. xSAP provides several model-based safety analysis features for finite- and infinite-state synchronous transition systems. In particular, it supports library-based definition of fault modes, an automatic model extension facility, generation of safety analysis artifacts such as Dynamic Fault Trees (DFTs) and Failure Mode and Effects Analysis (FMEA) tables. Moreover, it supports probabilistic evaluation of Fault Trees, failure propagation analysis using Timed Failure Propagation Graphs (TFPGs), and Common Cause Analysis (CCA). xSAP has been used in several industrial projects as verification back-end, and is currently being evaluated in a joint R&D Project involving FBK and The Boeing Company. 	
1504.08309v1	http://arxiv.org/pdf/1504.08309v1	2015	Refining Existential Properties in Separation Logic Analyses	Matko Botinčan|Mike Dodds|Stephen Magill	  In separation logic program analyses, tractability is generally achieved by restricting invariants to a finite abstract domain. As this domain cannot vary, loss of information can cause failure even when verification is possible in the underlying logic. In this paper, we propose a CEGAR-like method for detecting spurious failures and avoiding them by refining the abstract domain. Our approach is geared towards discovering existential properties, e.g. "list contains value x". To diagnose failures, we use abduction, a technique for inferring command preconditions. Our method works backwards from an error, identifying necessary information lost by abstraction, and refining the forward analysis to avoid the error. We define domains for several classes of existential properties, and show their effectiveness on case studies adapted from Redis, Azureus and FreeRTOS. 	
1505.01457v1	http://arxiv.org/pdf/1505.01457v1	2015	Modeling the Impact of Communication Loss on the Power Grid under   Emergency Control	Marzieh Parandehgheibi|Konstantin Turitsyn|Eytan Modiano	  We study the interaction between the power grid and the communication network used for its control. We design a centralized emergency control scheme under both full and partial communication support, to improve the performance of the power grid. We use our emergency control scheme to model the impact of communication loss on the grid. We show that unlike previous models used in the literature, the loss of communication does not necessarily lead to the failure of the correspondent power nodes; i.e. the "point-wise" failure model is not appropriate. In addition, we show that the impact of communication loss is a function of several parameters such as the size and structure of the power and communication failure, as well as the operating mode of power nodes disconnected from the communication network. Our model can be used to design the dependency between the power grid and the communication network used for its control, so as to maximize the benefit in terms of intelligent control, while minimizing the risks due to loss of communication. 	
1505.07747v1	http://arxiv.org/pdf/1505.07747v1	2015	Seepage flow-stability analysis of the riverbank of Saigon river due to   river water level fluctuation	A. Oya|H. H. Bui|N. Hiraoka|M. Fujimoto|R. Fukagawa	  The Saigon River, which flows through the center of Ho Chi Minh City, is of critical importance for the development of the city as forms as the main water supply and drainage channel for the city. In recent years, riverbank erosion and failures have become more frequent along the Saigon River, causing flooding and damage to infrastructures near the river. A field investigation and numerical study has been undertaken by our research group to identify factors affecting the riverbank failure. In this paper, field investigation results obtained from multiple investigation points on the Saigon River are presented, followed by a comprehensive coupled finite element analysis of riverbank stability when subjected to river water level fluctuations. The river water level fluctuation has been identified as one of the main factors affecting the riverbank failure, i.e. removal of the balancing hydraulic forces acting on the riverbank during water drawdown. 	
1507.01181v1	http://arxiv.org/pdf/1507.01181v1	2015	On Fast and Robust Information Spreading in the Vertex-Congest Model	Keren Censor-Hillel|Tariq Toukan	  This paper initiates the study of the impact of failures on the fundamental problem of \emph{information spreading} in the Vertex-Congest model, in which in every round, each of the $n$ nodes sends the same $O(\log{n})$-bit message to all of its neighbors.   Our contribution to coping with failures is twofold. First, we prove that the randomized algorithm which chooses uniformly at random the next message to forward is slow, requiring $\Omega(n/\sqrt{k})$ rounds on some graphs, which we denote by $G_{n,k}$, where $k$ is the vertex-connectivity.   Second, we design a randomized algorithm that makes dynamic message choices, with probabilities that change over the execution. We prove that for $G_{n,k}$ it requires only a near-optimal number of $O(n\log^3{n}/k)$ rounds, despite a rate of $q=O(k/n\log^3{n})$ failures per round. Our technique of choosing probabilities that change according to the execution is of independent interest. 	
1507.05541v1	http://arxiv.org/pdf/1507.05541v1	2015	Maximizing electrical power supply using FACTS devices	Karsten Lehmann|Russell Bent|Feng Pan	  Modern society critically depends on the services electric power provides. Power systems rely on a network of power lines and transformers to deliver power from sources of power (generators) to the consumers (loads). However, when power lines fail (for example, through lightning or natural disasters) or when the system is heavily used, the network is often unable to fulfill all of the demand for power. While systems are vulnerable to these failures, increasingly, sophisticated control devices are being deployed to improve the efficiency of power systems. Such devices can also be used to improve the resiliency of power systems to failures. In this paper, we focus on using FACTS devices in this context. A FACTS device allows power grid operators to adjust the impedance parameters of power lines, thereby redistributing flow in the network and potentially increasing the amount of power that is supplied. Here we develop new approaches for determining the optimal parameter settings for FACTS devices in order to supply the maximal amount of power when networks are stressed, e.g. power line failures and heavy utilization. 	
1508.02200v1	http://arxiv.org/pdf/1508.02200v1	2015	Use of biogenic nanomaterials to improve the peritoneal dialysis   technique: A Translational Research Perspective	Dinesh Kumar	  Intraperitoneal and catheter exit site infections are the most common complications associated with prolonged peritoneal dialysis (PD) therapy used for treating the patients with end stage renal failure (ESRF). Recurrent and persistent infections often cause inflammation of the peritoneum, a condition known as infectious peritonitis and to resolve the condition, patients require antibiotic treatment. However, if the treatment is delayed or if it fails due to antibiotic resistance, the peritonitis may lead to permanent malfunctioning of peritoneal membrane causing technique failure and transferring the patients to haemodialysis. Severe and prolonged peritonitis is not only the major cause of technique failure, it is also the leading cause of mortality and morbidity in PD patients. Therefore, there is an urgent need to improve the existing PD technique so that the frequency of PD associated infections could be reduced and infectious peritonitis episodes thereof during prolonged peritoneal dialysis. In this perspective, I highlight the possibility to improve the PD technique through the use of antimicrobial nanoparticles synthesized biologically. 	
1508.07724v1	http://arxiv.org/pdf/1508.07724v1	2015	SDL based validation of a node monitoring protocol	Anandi Giridharan|Pallapa Venkataram	  Mobile ad hoc network is a wireless, self-configured, infrastructureless network of mobile nodes. The nodes are highly mobile, which makes the application running on them face network related problems like node failure, link failure, network level disconnection, scarcity of resources, buffer degradation, and intermittent disconnection etc. Node failure and Network fault are need to be monitored continuously by supervising the network status. Node monitoring protocol is crucial, so it is required to test the protocol exhaustively to verify and validate the functionality and accuracy of the designed protocol. This paper presents a validation model for Node Monitoring Protocol using Specification and Description Llanguage (SDL) using both Static Agent (SA) and Mobile Agent (MA). We have verified properties of the Node Monitoring Protocol (NMP) based on the global states with no exits, deadlock states or proper termination states using reachability graph. Message Sequence Chart (MSC) gives an intuitive understanding of the described system behavior with varying node density and complex behavior etc. 	
1509.01275v1	http://arxiv.org/pdf/1509.01275v1	2015	Modeling Long-term Outcomes and Treatment Effects After Androgen   Deprivation Therapy for Prostate Cancer	Yolanda Hagar|James J. Dignam|Vanja Dukic	  Analyzing outcomes in long-term cancer survivor studies can be complex. The effects of predictors on the failure process may be difficult to assess over longer periods of time, as the commonly used assumption of proportionality of hazards holding over an extended period is often questionable. In this manuscript, we compare seven different survival models that estimate the hazard rate and the effects of proportional and non-proportional covariates. In particular, we focus on an extension of the the multi-resolution hazard (MRH) estimator, combining a non-proportional hierarchical MRH approach with a data-driven pruning algorithm that allows for computational efficiency and produces robust estimates even in times of few observed failures. Using data from a large-scale randomized prostate cancer clinical trial, we examine patterns of biochemical failure and estimate the time-varying effects of androgen deprivation therapy treatment and other covariates. We compare the impact of different modeling strategies and smoothness assumptions on the estimated treatment effect. Our results show that the benefits of treatment diminish over time, possibly with implications for future treatment protocols. 	
1510.01062v1	http://arxiv.org/pdf/1510.01062v1	2015	Full characterization of modular values for two-dimensional systems	Le Bin Ho|Nobuyuki Imoto	  Vaidman pointed out the importance of modular values, and related the modular value of a Pauli spin operator to its weak value for specific coupling strengths [Phys. Rev. Lett. 105, 230401 (2010)]. It would be useful if this relationship is generalized since a modular value, which assumes a finite strength of the measurement interaction, is sometimes more practical than a weak value, which assumes an infinitesimally small interaction. In this paper, we give a general expression that relates the weak value and the modular value of an arbitrary observable in the 2-dimensional Hilbert space for an arbitrary coupling strength. Using this expression, we show the "failure of sum rule" for modular values, which has a resemblance to the "failure of product rule" for weak values. We give examples of "failure of sum rule" for some interesting cases, i.e., paradoxes based on nonlocality, which include EPR paradox, Hardy's paradox, and Cheshire cat experiment. 	
1510.04427v1	http://arxiv.org/pdf/1510.04427v1	2015	Types of signature analysis in reliability based on Hilbert series	Fatemeh Mohammadi|Eduardo Sáenz-de-Cabezón|Henry P. Wynn	  The present paper studies multiple failure and signature analysis of coherent systems using the theory of monomial ideals. While system reliability has been studied using Hilbert series of monomial ideals, this is not enough to understand in a deeper sense the ideal structure features that reflect the behavior of the system under multiple simultaneous failures and signature. Therefore, we introduce the lcm-filtration of a monomial ideal, and we study the Hilbert series and resolutions of the corresponding ideals. Given a monomial ideal, we explicitly compute the resolutions for all ideals in the associated lcm-filtration, and we apply this to study coherent systems. Some computational results are shown in examples to demonstrate the usefulness of this approach and the computational issues that arise. We also study the failure distribution from a statistical point of view by means of the algebraic tools described. 	
1510.09140v3	http://arxiv.org/pdf/1510.09140v3	2017	Strategic Investment in Protection in Networked Systems	Matt V. Leduc|Ruslan Momot	  We study the incentives that agents have to invest in costly protection against cascading failures in networked systems. Applications include vaccination, computer security and airport security. Agents are connected through a network and can fail either intrinsically or as a result of the failure of a subset of their neighbors. We characterize the equilibrium based on an agent's failure probability and derive conditions under which equilibrium strategies are monotone in degree (i.e. in how connected an agent is on the network). We show that different kinds of applications (e.g. vaccination, malware, airport/EU security) lead to very different equilibrium patterns of investments in protection, with important welfare and risk implications. Our equilibrium concept is flexible enough to allow for comparative statics in terms of network properties and we show that it is also robust to the introduction of global externalities (e.g. price feedback, congestion). 	
1511.05436v2	http://arxiv.org/pdf/1511.05436v2	2016	Polyurethane spray coating of aluminum wire bonds to prevent corrosion   and suppress resonant oscillations	Joseph M. Izen|Matthew Kurth|Rusty Boyd	  Unencapsulated aluminum wedge wire bonds are common in particle physics pixel and strip detectors. Industry-favored bulk encapsulation is eschewed due to the range of operating temperatures and radiation. Wire bond failures are a persistent source of tracking-detector failure. Unencapsulated bonds are vulnerable to condensation-induced corrosion, particularly when halides are present. Oscillations from periodic Lorentz forces are documented as another source of wire bond failure. Spray application of polyurethane coatings, performance of polyurethane-coated wire bonds after climate chamber exposure, and resonant properties of polyurethane-coated wire bonds and their resistance to periodic Lorentz forces are under study for use in a future High Luminosity Large Hadron Collider detector such as the ATLAS Inner Tracker upgrade. 	
1511.05719v1	http://arxiv.org/pdf/1511.05719v1	2015	Using Abduction in Markov Logic Networks for Root Cause Analysis	Joerg Schoenfisch|Janno von Stulpnagel|Jens Ortmann|Christian Meilicke|Heiner Stuckenschmidt	  IT infrastructure is a crucial part in most of today's business operations. High availability and reliability, and short response times to outages are essential. Thus a high amount of tool support and automation in risk management is desirable to decrease outages. We propose a new approach for calculating the root cause for an observed failure in an IT infrastructure. Our approach is based on Abduction in Markov Logic Networks. Abduction aims to find an explanation for a given observation in the light of some background knowledge. In failure diagnosis, the explanation corresponds to the root cause, the observation to the failure of a component, and the background knowledge to the dependency graph extended by potential risks. We apply a method to extend a Markov Logic Network in order to conduct abductive reasoning, which is not naturally supported in this formalism. Our approach exhibits a high amount of reusability and enables users without specific knowledge of a concrete infrastructure to gain viable insights in the case of an incident. We implemented the method in a tool and illustrate its suitability for root cause analysis by applying it to a sample scenario. 	
1511.07078v2	http://arxiv.org/pdf/1511.07078v2	2016	Site and bond percolation thresholds in $K_{n,n}$-based lattices:   Vulnerability of quantum annealers to random qubit and coupler failures on   chimera topologies	O. Melchert|Helmut G. Katzgraber|M. A. Novotny	  We estimate the critical thresholds of bond and site percolation on nonplanar, effectively two-dimensional graphs with chimera like topology. The building blocks of these graphs are complete and symmetric bipartite subgraphs of size $2n$, referred to as $K_{n,n}$ graphs. For the numerical simulations we use an efficient union-find based algorithm and employ a finite-size scaling analysis to obtain the critical properties for both bond and site percolation. We report the respective percolation thresholds for different sizes of the bipartite subgraph and verify that the associated universality class is that of standard two-dimensional percolation. For the canonical chimera graph used in the D-Wave Systems Inc.~quantum annealer ($n = 4$), we discuss device failure in terms of network vulnerability, i.e., we determine the critical fraction of qubits and couplers that can be absent due to random failures prior to losing large-scale connectivity throughout the device. 	
1511.08797v1	http://arxiv.org/pdf/1511.08797v1	2015	Full quantum theory of control-not gate in ion-trap quantum computation	Biyao Yang|Li Yang	  We investigate the exact effect on ion trap quantum computation after field quantization. First an exact expression of failure probability from field quantization after many CNOT operations in Cirac-Zoller scheme is given. It is proportional to operation number and the amplitude of $|1\rangle_x |0\rangle_y$ or $|1\rangle_x |1\rangle_y$ in initial state, and inverse proportional to mean number of photons and amplitude of $|0\rangle_x |0\rangle_y$ or $|0\rangle_x |1\rangle_y$ in initial state. Then we calculate the failure probability when the limitation to mean number of photons in sideband transition is considered. When the initial state is $|1\rangle_x |0\rangle_y$ or $|1\rangle_x |1\rangle_y$, after about $10^2$ times of CNOT operations, failure probability is no less than $10^{-2}$, while $10^{-2}$ is the known maximum threshold in fault-tolerant quantum computation. Then when the initial state is $|1\rangle_x |0\rangle_y$ or $|1\rangle_x |1\rangle_y$, the number of CNOT gates on the same pair of physical qubits should be no more than $10^2$ in one error-correction period, or else the computation cannot be implemented reliably. This conclusion can help to determine the number of CNOT operations between coding and decoding in one error-correction period in fault-tolerant quantum computation. 	
1512.02339v1	http://arxiv.org/pdf/1512.02339v1	2015	Understanding the problem with logarithmic singularities in the complex   Langevin method	Jun Nishimura|Shinji Shimasaki	  In recent years, there has been remarkable progress in theoretical justification of the complex Langevin method, which is a promising method for evading the sign problem in the path integral with a complex weight. There still remains, however, an issue concerning occasional failure of this method in the case where the action involves logarithmic singularities such as the one appearing from the fermion determinant in finite density QCD. In this talk, we point out that this failure is due to the breakdown of the relation between the complex weight which satisfies the Fokker-Planck equation and the probability distribution generated by the stochastic process. In fact, this kind of failure can occur in general when the stochastic process involves a singular drift term. We show, however, in simple examples, that there exists a parameter region in which the method works although the standard reweighting method is hardly applicable. 	
1601.02000v2	http://arxiv.org/pdf/1601.02000v2	2016	Ill-posedness of the cubic nonlinear half-wave equation and other   fractional NLS on the real line	Antoine Choffrut|Oana Pocovnicu	  In this paper, we study ill-posedness of cubic fractional nonlinear Schr\"odinger equations. First, we consider the cubic nonlinear half-wave equation (NHW) on $\mathbb R$. In particular, we prove the following ill-posedness results: (i) failure of local uniform continuity of the solution map in $H^s(\mathbb R)$ for $s\in (0,\frac 12)$, and also for $s=0$ in the focusing case; (ii) failure of $C^3$-smoothness of the solution map in $L^2(\mathbb R)$; (iii) norm inflation and, in particular, failure of continuity of the solution map in $H^s(\mathbb R)$, $s<0$. By a similar argument, we also prove norm inflation in negative Sobolev spaces for the cubic fractional NLS. Surprisingly, we obtain norm inflation above the scaling critical regularity in the case of dispersion $|D|^\beta$ with $\beta>2$. 	
1601.05387v1	http://arxiv.org/pdf/1601.05387v1	2016	On wind Turbine failure detection from measurements of phase currents: a   permutation entropy approach	Sumit Kumar Ram|Geir Kulia|Marta Molinas	  This article presents the applicability of Permutation Entropy based complexity measure of a time series for detection of fault in wind turbines. A set of electrical data from one faulty and one healthy wind turbine were analysed using traditional FastFourier analysis in addition to Permutation Entropy analysis to compare the complexity index of phase currents of the two turbines over time. The 4 seconds length data set did not reveal any low frequency in the spectra of currents, neither did they show any meaningful differences of spectrum between the two turbine currents. Permutation Entropy analysis of the current waveforms of same phases for the two turbines are found to have different complexity values over time, one of them being clearly higher than the other. The work of Yan et. al. in has found that higher entropy values related to thepresence of failure in rotary machines in his study. Following this track, further efforts will be put into relating the entropy difference found in our study to possible presence of failure in one of the wind energy conversion systems. 	
1602.00761v1	http://arxiv.org/pdf/1602.00761v1	2016	Optimality and Rate-Compatibility for Erasure-Coded Packet Transmissions   when Fading Channel Diversity Increases with Packet Length	Sudarsan V. S. Ranganathan|Tong Mu|Richard D. Wesel	  A message composed of packets is transmitted using erasure and channel coding over a fading channel with no feedback. For this scenario, the paper explores the trade-off between the redundancies allocated to the packet-level erasure code and the channel code, along with an objective of a low probability of failure to recover the message.   To this end, we consider a fading model that we term proportional-diversity block fading (PD block fading). For a fixed overall code rate and transmit power, we formulate an optimization problem to numerically find the optimal channel-coding rate (and thus the optimal erasure-coding rate) that minimizes the probability of failure for various approximations of the problem.   Furthermore, an interpretation of the results from an incremental redundancy point of view shows how rate-compatibility affects the possible trajectories of the failure probability as a function of the overall code rate. Our numerical results suggest that an optimal, rateless, hybrid coding scheme for a single-user wireless system over the PD block-fading channel should have the rate of the erasure code approach one. 	
1603.03001v1	http://arxiv.org/pdf/1603.03001v1	2016	An Extension of the Generalized Linear Failure Rate Distribution	Mohammad Reza Kazemi|Ali Akbar Jafari|Saeid Tahmasebi	  In this paper, we introduce a new extension of the generalized linear failure rate distributions. It includes some well-known lifetime distributions such as extension of generalized exponential and generalized linear failure rate distributions as special sub-models. In addition, it can have a constant, decreasing, increasing, upside-down bathtub (unimodal), and bathtub-shaped hazard rate function depending on its parameters. We provide some of its statistical properties such as moments, quantiles, skewness, kurtosis, hazard rate function, and reversible hazard rate function. The maximum likelihood estimation of the parameters is also discussed. At the end, a real data set is given to illustrate the usefulness of this new distribution in analyzing lifetime data. 	
1603.05181v2	http://arxiv.org/pdf/1603.05181v2	2016	Strength of weak layers in cascading failures on multiplex networks:   case of the international trade network	Kyu-Min Lee|Kwang-Il Goh	  Many real-world complex systems across natural, social, and economical domains consist of manifold layers to form multiplex networks. The multiple network layers give rise to nonlinear effect for the emergent dynamics of systems. Especially, weak layers that can potentially play significant role in amplifying the vulnerability of multiplex networks might be shadowed in the aggregated single-layer network framework which indiscriminately accumulates all layers. Here we present a simple model of cascading failure on multiplex networks of weight-heterogeneous layers. By simulating the model on the multiplex network of international trades, we found that the multiplex model produces more catastrophic cascading failures which are the result of emergent collective effect of coupling layers, rather than the simple sum thereof. Therefore risks can be systematically underestimated in single-layer network analyses because the impact of weak layers can be overlooked. We anticipate that our simple theoretical study can contribute to further investigation and design of optimal risk-averse real-world complex systems. 	
1604.03558v2	http://arxiv.org/pdf/1604.03558v2	2018	Error Propagation Through a Network With Non-Uniform Failure	Sandra König	  A central concern of network operators is to estimate the probability of an incident that affects a significant part and thus may yield to a breakdown. We answer this question by modeling how a failure of either a node or an edge will affect the rest of the network using percolation theory. Our model is general in the sense that it only needs two inputs: the topology of the network and the chances of failure of its components. These chances may vary to represent different types of edges having different tendencies to fail. We illustrate the approach by an example, for which we can even obtain closed form expressions for the likelihood of an outbreak remaining bounded or spreading unlimitedly. 	
1604.05534v1	http://arxiv.org/pdf/1604.05534v1	2016	Link Capacity Planning for Fault Tolerant Operation in Hybrid SDN/OSPF   Networks	Marcel Caria|Admela Jukan	  Link capacity dimensioning is the periodic task where ISPs have to make provisions for sudden traffic bursts and network failures to assure uninterrupted operations. This provision comes in the form of link working capacities with noticeable amounts of headroom, i.e., spare capacities that are used in case of congestions or network failures. Distributed routing protocols like OSPF provide convergence after network failures and have proven their reliable operation over decades, but require overprovisioning and headroom of over 50%. However, SDN has recently been proposed to either replace or work together with OSPF in routing Internet traffic. This paper addresses the question of how to robustly dimension the link capacities in emerging hybrid SDN/OSPF networks. We analyze the networks with various implementations of hybrid SDN/OSPF control planes, and show that our idea of SDN Partitioning requires less amounts of spare capacity compared to legacy or other hybrid SDN/OSPF schemes, outperformed only by a full SDN deployment. 	
1605.00947v1	http://arxiv.org/pdf/1605.00947v1	2016	Distributed Frequency Control in Power Grids Under Limited Communication	Marzieh Parandehgheibi|Konstantin Turitsyn|Eytan Modiano	  In this paper, we analyze the impact of communication failures on the performance of optimal distributed frequency control. We consider a consensus-based control scheme, and show that it does not converge to the optimal solution when the communication network is disconnected. We propose a new control scheme that uses the dynamics of power grid to replicate the information not received from the communication network, and prove that it achieves the optimal solution under any single communication link failure. In addition, we show that this control improves cost under multiple communication link failures. Next, we analyze the impact of discrete-time communication on the performance of distributed frequency control. In particular, we will show that the convergence time increases as the time interval between two messages increases. We propose a new algorithm that uses the dynamics of the power grid, and show through simulation that it improves the convergence time of the control scheme significantly. 	
1606.00195v2	http://arxiv.org/pdf/1606.00195v2	2016	Self-stabilizing Reconfiguration	Shlomi Dolev|Chryssis Georgiou|Ioannis Marcoullis|Elad M. Schiller	  Current reconfiguration techniques are based on starting the system in a consistent configuration, in which all participating entities are in their initial state. Starting from that state, the system must preserve consistency as long as a predefined churn rate of processors joins and leaves is not violated, and unbounded storage is available. Many working systems cannot control this churn rate and do not have access to unbounded storage. System designers that neglect the outcome of violating the above assumptions may doom the system to exhibit illegal behaviors. We present the first automatically recovering reconfiguration scheme that recovers from transient faults, such as temporal violations of the above assumptions. Our self-stabilizing solutions regain safety automatically by assuming temporal access to reliable failure detectors. Once safety is re-established, the failure detector reliability is no longer needed. Still, liveness is conditioned by the failure detector's unreliable signals. We show that our self-stabilizing reconfiguration techniques can serve as the basis for the implementation of several dynamic services over message passing systems. Examples include self-stabilizing reconfigurable virtual synchrony, which, in turn, can be used for implementing a self-stabilizing reconfigurable state-machine replication and self-stabilizing reconfigurable emulation of shared memory. 	
1606.01314v1	http://arxiv.org/pdf/1606.01314v1	2016	Optimal Storage Allocation for Wireless Cloud Caching Systems with a   Limited Sum Storage Capacity	Bi Hong|Wan Choi	  In wireless cloud storage systems, the recovery failure probability depends on not only wireless channel conditions but also storage size of each distributed storage node. For an efficient utilization of limited storage capacity and the performance characterization of allocation strategies, we asymptotically analyze the recovery failure probability of a wireless cloud storage system with a sum storage capacity constraint for both high SNR regime and low SNR regime. Then, we find the optimal storage allocation strategy across distributed storage nodes in terms of the asymptotic recovery failure probability. Our analysis reveals that the maximal symmetric allocation is optimal for high SNR regime and the minimal allocation (with $\lfloor T\rfloor$ complete storage nodes and an incomplete storage node) is optimal for low SNR regime, where $T$ is the sum storage capacity. Based on the numerical investigation, we also show that in intermediate SNR regime, a balance allocation between the minimal allocation and the maximal symmetric allocation would not be required if we select one between them according to SNR. 	
1607.01573v1	http://arxiv.org/pdf/1607.01573v1	2016	Radiation Risks and Mitigation in Electronic Systems	B. Todd|S. Uznanski	  Electrical and electronic systems can be disturbed by radiation-induced effects. In some cases, radiation-induced effects are of a low probability and can be ignored; however, radiation effects must be considered when designing systems that have a high mean time to failure requirement, an impact on protection, and/or higher exposure to radiation. High-energy physics power systems suffer from a combination of these effects: a high mean time to failure is required, failure can impact on protection, and the proximity of systems to accelerators increases the likelihood of radiation-induced events. This paper presents the principal radiation-induced effects, and radiation environments typical to high-energy physics. It outlines a procedure for designing and validating radiation-tolerant systems using commercial off-the-shelf components. The paper ends with a worked example of radiation-tolerant power converter controls that are being developed for the Large Hadron Collider and High Luminosity-Large Hadron Collider at CERN. 	
1608.06392v1	http://arxiv.org/pdf/1608.06392v1	2016	Formalization of Fault Trees in Higher-order Logic: A Deep Embedding   Approach	Waqar Ahmed|Osman Hasan	  Fault Tree (FT) is a standard failure modeling technique that has been extensively used to predict reliability, availability and safety of many complex engineering systems. In order to facilitate the formal analysis of FT based analyses, a higher-order-logic formalization of FTs has been recently proposed. However, this formalization is quite limited in terms of handling large systems and transformation of FT models into their corresponding Reliability Block Diagram (RBD) structures, i.e., a frequently used transformation in reliability and availability analyses. In order to overcome these limitations, we present a deep embedding based formalization of FTs. In particular, the paper presents a formalization of AND, OR and NOT FT gates, which are in turn used to formalize other commonly used FT gates, i.e., NAND, NOR, XOR, Inhibit, Comparator and majority Voting, and the formal verification of their failure probability expressions. For illustration purposes, we present a formal failure analysis of a communication gateway software for the next generation air traffic management system. 	
1610.08879v1	http://arxiv.org/pdf/1610.08879v1	2016	Coupled mass-momenta balance for modeling material failure	Konstantin Volokh	  Cracks are created by massive breakage of molecular or atomic bonds. The latter, in its turn, leads to the highly localized loss of material, which is the reason why even closed cracks are visible by a naked eye. Thus, fracture can be interpreted as the local material sink. Mass conservation is violated locally in the area of material failure. We consider a theoretical formulation of the coupled mass and momenta balance equations for a description of fracture. Our focus is on brittle fracture and we propose a finite strain hyperelastic thermodynamic framework for the coupled mass-flow-elastic boundary value problem. The attractiveness of the proposed framework as compared to the traditional continuum damage theories is that no internal parameters (like damage variables, phase fields etc.) are used while the regularization of the failure localization is provided by the physically sound law of mass balance. 	
1611.02392v1	http://arxiv.org/pdf/1611.02392v1	2016	Sums of Uncertainty: Refinements Go Gradual	Khurram A. Jafery|Joshua Dunfield	  A long-standing shortcoming of statically typed functional languages is that type checking does not rule out pattern-matching failures (run-time match exceptions). Refinement types distinguish different values of datatypes; if a program annotated with refinements passes type checking, pattern-matching failures become impossible. Unfortunately, refinement is a monolithic property of a type, exacerbating the difficulty of adding refinement types to nontrivial programs.   Gradual typing has explored how to incrementally move between static typing and dynamic typing. We develop a type system of gradual sums that combines refinement with imprecision. Then, we develop a bidirectional version of the type system, which rules out excessive imprecision, and give a type-directed translation to a target language with explicit casts. We prove that the static sublanguage cannot have match failures, that a well-typed program remains well-typed if its type annotations are made less precise, and that making annotations less precise causes target programs to fail later. Several of these results correspond to criteria for gradual typing given by Siek et al. (2015). 	
1611.06914v1	http://arxiv.org/pdf/1611.06914v1	2016	Resilience of Energy Infrastructure and Services: Modeling, Data   Analytics and Metrics	Chuanyi Ji|Yun Wei|H. Vincent Poor	  Large scale power failures induced by severe weather have become frequent and damaging in recent years, causing millions of people to be without electricity service for days. Although the power industry has been battling weather-induced failures for years, it is largely unknown how resilient the energy infrastructure and services really are to severe weather disruptions. What fundamental issues govern the resilience? Can advanced approaches such as modeling and data analytics help industry to go beyond empirical methods? This paper discusses the research to date and open issues related to these questions. The focus is on identifying fundamental challenges and advanced approaches for quantifying resilience. In particular, a first aspect of this problem is how to model large-scale failures, recoveries and impacts, involving the infrastructure, service providers, customers, and weather. A second aspect is how to identify generic vulnerability (i.e., non-resilience) in the infrastructure and services through large-scale data analytics. And, a third is to understand what resilience metrics are needed and how to develop them. 	
1611.08309v1	http://arxiv.org/pdf/1611.08309v1	2016	On Human Intellect and Machine Failures: Troubleshooting Integrative   Machine Learning Systems	Besmira Nushi|Ece Kamar|Eric Horvitz|Donald Kossmann	  We study the problem of troubleshooting machine learning systems that rely on analytical pipelines of distinct components. Understanding and fixing errors that arise in such integrative systems is difficult as failures can occur at multiple points in the execution workflow. Moreover, errors can propagate, become amplified or be suppressed, making blame assignment difficult. We propose a human-in-the-loop methodology which leverages human intellect for troubleshooting system failures. The approach simulates potential component fixes through human computation tasks and measures the expected improvements in the holistic behavior of the system. The method provides guidance to designers about how they can best improve the system. We demonstrate the effectiveness of the approach on an automated image captioning system that has been pressed into real-world use. 	
1612.03799v1	http://arxiv.org/pdf/1612.03799v1	2016	A Software Reliability Model Based on a Geometric Sequence of Failure   Rates	Stefan Wagner|Helmut Fischer	  Software reliability models are an important tool in quality management and release planning. There is a large number of different models that often exhibit strengths in different areas. This paper proposes a model that is based on a geometric sequence (or progression) of the failure rates of faults. This property of the failure process was observed in practice at Siemens among others and led to the development of the proposed model. It is described in detail and evaluated using standard criteria. Most importantly, the model performs constantly well over several projects in terms of its predictive validity. 	
1701.02648v1	http://arxiv.org/pdf/1701.02648v1	2017	Why Can't You Behave? Non-termination Analysis of Direct Recursive Rules   with Constraints	Thom Fruehwirth	  This paper is concerned with rule-based programs that go wrong. The unwanted behavior of rule applications is non-termination or failure of a computation. We propose a static program analysis of the non-termination problem for recursion in the Constraint Handling Rules (CHR) language.   CHR is an advanced concurrent declarative language involving constraint reasoning. It has been closely related to many other rule-based approaches, so the results are of a more general interest. In such languages, non-termination is due to infinite applications of recursive rules. Failure is due to accumulation of contradicting constraints during the computation.   We give theorems with so-called misbehavior conditions for potential non-termination and failure (as well as definite termination) of linear direct recursive simplification rules. Logical relationships between the constraints in a recursive rule play a crucial role in this kind of program analysis. We think that our approach can be extended to other types of recursion and to a more general class of rules. Therefore this paper can serve as a basic reference and a starting point for further research. 	
1701.07695v1	http://arxiv.org/pdf/1701.07695v1	2017	Exponential Source/Channel Duality	Sergey Tridenski|Ram Zamir	  We propose a source/channel duality in the exponential regime, where success/failure in source coding parallels error/correctness in channel coding, and a distortion constraint becomes a log-likelihood ratio (LLR) threshold. We establish this duality by first deriving exact exponents for lossy coding of a memoryless source P, at distortion D, for a general i.i.d. codebook distribution Q, for both encoding success (R < R(P,Q,D)) and failure (R > R(P,Q,D)). We then turn to maximum likelihood (ML) decoding over a memoryless channel P with an i.i.d. input Q, and show that if we substitute P=QP, Q=Q, and D=0 under the LLR distortion measure, then the exact exponents for decoding-error (R < I(Q, P)) and strict correct-decoding (R > I(Q, P)) follow as special cases of the exponents for source encoding success/failure, respectively. Moreover, by letting the threshold D take general values, the exact random-coding exponents for erasure (D > 0) and list decoding (D < 0) under the simplified Forney decoder are obtained. Finally, we derive the exact random-coding exponent for Forney's optimum tradeoff erasure/list decoder, and show that at the erasure regime it coincides with Forney's lower bound and with the simplified decoder exponent. 	
1701.08182v1	http://arxiv.org/pdf/1701.08182v1	2017	Fast failover of multicast sessions in software-defined networks	Jorik Oostenbrink|Niels L. M. van Adrichem|Fernando A. Kuipers	  With the rapid growth of services that stream to groups of users comes an increased importance of and demand for reliable multicast. In this paper, we turn to software-defined networking and develop a novel general-purpose multi-failure protection algorithm to provide quick failure recovery, via Fast Failover (FF) groups, for dynamic multicast groups. This extends previous research, which either could not realize fast failover, worked only for single link failures, or was only applicable to static multicast groups. However, while FF is know to be fast, it requires pre-installing back-up rules. These additional memory requirements, which in a multicast setting are even more pronounced than for unicast, are often mentioned as a big disadvantage of using FF.   We develop an OpenFlow application for resilient multicast, with which we study FF resource usage, in an attempt to better understand the trade-off between recovery time and resource usage. Our tests on a realistic network suggest that using FF groups can reduce the recovery time of the network significantly compared to other methods, especially when the latency between the controller and the switches is relatively large. 	
1702.02628v2	http://arxiv.org/pdf/1702.02628v2	2017	Optimal Detection of Faulty Traffic Sensors Used in Route Planning	Amin Ghafouri|Aron Laszka|Abhishek Dubey|Xenofon Koutsoukos	  In a smart city, real-time traffic sensors may be deployed for various applications, such as route planning. Unfortunately, sensors are prone to failures, which result in erroneous traffic data. Erroneous data can adversely affect applications such as route planning, and can cause increased travel time. To minimize the impact of sensor failures, we must detect them promptly and accurately. However, typical detection algorithms may lead to a large number of false positives (i.e., false alarms) and false negatives (i.e., missed detections), which can result in suboptimal route planning. In this paper, we devise an effective detector for identifying faulty traffic sensors using a prediction model based on Gaussian Processes. Further, we present an approach for computing the optimal parameters of the detector which minimize losses due to false-positive and false-negative errors. We also characterize critical sensors, whose failure can have high impact on the route planning application. Finally, we implement our method and evaluate it numerically using a real-world dataset and the route planning platform OpenTripPlanner. 	
1702.03466v1	http://arxiv.org/pdf/1702.03466v1	2017	Safe Open-Loop Strategies for Handling Intermittent Communications in   Multi-Robot Systems	Siddharth Mayya|Magnus Egerstedt	  In multi-robot systems where a central decision maker is specifying the movement of each individual robot, a communication failure can severely impair the performance of the system. This paper develops a motion strategy that allows robots to safely handle critical communication failures for such multi-robot architectures. For each robot, the proposed algorithm computes a time horizon over which collisions with other robots are guaranteed not to occur. These safe time horizons are included in the commands being transmitted to the individual robots. In the event of a communication failure, the robots execute the last received velocity commands for the corresponding safe time horizons leading to a provably safe open-loop motion strategy. The resulting algorithm is computationally effective and is agnostic to the task that the robots are performing. The efficacy of the strategy is verified in simulation as well as on a team of differential-drive mobile robots. 	
1702.04615v1	http://arxiv.org/pdf/1702.04615v1	2017	Automated Identification of Drug-Drug Interactions in Pediatric   Congestive Heart Failure Patients	Daniel Miller	  Congestive Heart Failure, or CHF, is a serious medical condition that can result in fluid buildup in the body as a result of a weak heart. When the heart can't pump enough blood to efficiently deliver nutrients and oxygen to the body, kidney function may be impaired, resulting in fluid retention. CHF patients require a broad drug regimen to maintain the delicate system balance, particularly between their heart and kidneys. These drugs include ACE inhibitors and Beta Blockers to control blood pressure, anticoagulants to prevent blood clots, and diuretics to reduce fluid overload. Many of these drugs may interact, and potential effects of these interactions must be weighed against their benefits. For this project, we consider a set of 44 drugs identified as specifically relevant for treating CHF by pediatric cardiologists at Lucile Packard Children's Hospital. This list was generated as part of our current work at the LPCH Heart Center. The goal of this project is to identify and evaluate potentially harmful drug-drug interactions (DDIs) within pediatric patients with Congestive Heart Failure. This identification will be done autonomously, so that it may continuously update by evaluating newly published literature. 	
1702.05774v1	http://arxiv.org/pdf/1702.05774v1	2017	Machine Learning Predicts Laboratory Earthquakes	Bertrand Rouet-Leduc|Claudia Hulbert|Nicholas Lubbers|Kipton Barros|Colin Humphreys|Paul A. Johnson	  Forecasting fault failure is a fundamental but elusive goal in earthquake science. Here we show that by listening to the acoustic signal emitted by a laboratory fault, machine learning can predict the time remaining before it fails with great accuracy. These predictions are based solely on the instantaneous physical characteristics of the acoustical signal, and do not make use of its history. Surprisingly, machine learning identifies a signal emitted from the fault zone previously thought to be low-amplitude noise that enables failure forecasting throughout the laboratory quake cycle. We hypothesize that applying this approach to continuous seismic data may lead to significant advances in identifying currently unknown signals, in providing new insights into fault physics, and in placing bounds on fault failure times. 	
1702.08042v1	http://arxiv.org/pdf/1702.08042v1	2017	Instant restore after a media failure	Caetano Sauer|Goetz Graefe|Theo Härder	  Media failures usually leave database systems unavailable for several hours until recovery is complete, especially in applications with large devices and high transaction volume. Previous work introduced a technique called single-pass restore, which increases restore bandwidth and thus substantially decreases time to repair. Instant restore goes further as it permits read/write access to any data on a device undergoing restore--even data not yet restored--by restoring individual data segments on demand. Thus, the restore process is guided primarily by the needs of applications, and the observed mean time to repair is effectively reduced from several hours to a few seconds.   This paper presents an implementation and evaluation of instant restore. The technique is incrementally implemented on a system starting with the traditional ARIES design for logging and recovery. Experiments show that the transaction latency perceived after a media failure can be cut down to less than a second and that the overhead imposed by the technique on normal processing is minimal. The net effect is that a few "nines" of availability are added to the system using simple and low-overhead software techniques. 	
1703.00396v1	http://arxiv.org/pdf/1703.00396v1	2017	Patient Specific Congestive Heart Failure Detection From Raw ECG signal	Yakup Kutlu|Apdullah Yayık|Esen Yıldırım|Mustafa Yeniad|Serdar Yıldırım	  In this study; in order to diagnose congestive heart failure (CHF) patients, non-linear second-order difference plot (SODP) obtained from raw 256 Hz sampled frequency and windowed record with different time of ECG records are used. All of the data rows are labelled with their belongings to classify much more realistically. SODPs are divided into different radius of quadrant regions and numbers of the points fall in the quadrants are computed in order to extract feature vectors. Fisher's linear discriminant, Naive Bayes, Radial basis function, and artificial neural network are used as classifier. The results are considered in two step validation methods as general k-fold cross-validation and patient based cross-validation. As a result, it is shown that using neural network classifier with features obtained from SODP, the constructed system could distinguish normal and CHF patients with 100% accuracy rate. Keywords 	
1703.02757v1	http://arxiv.org/pdf/1703.02757v1	2017	Byzantine-Tolerant Machine Learning	Peva Blanchard|El Mahdi El Mhamdi|Rachid Guerraoui|Julien Stainer	  The growth of data, the need for scalability and the complexity of models used in modern machine learning calls for distributed implementations. Yet, as of today, distributed machine learning frameworks have largely ignored the possibility of arbitrary (i.e., Byzantine) failures. In this paper, we study the robustness to Byzantine failures at the fundamental level of stochastic gradient descent (SGD), the heart of most machine learning algorithms. Assuming a set of $n$ workers, up to $f$ of them being Byzantine, we ask how robust can SGD be, without limiting the dimension, nor the size of the parameter space.   We first show that no gradient descent update rule based on a linear combination of the vectors proposed by the workers (i.e, current approaches) tolerates a single Byzantine failure. We then formulate a resilience property of the update rule capturing the basic requirements to guarantee convergence despite $f$ Byzantine workers. We finally propose Krum, an update rule that satisfies the resilience property aforementioned. For a $d$-dimensional learning problem, the time complexity of Krum is $O(n^2 \cdot (d + \log n))$. 	
1704.00801v1	http://arxiv.org/pdf/1704.00801v1	2017	Need for a Soft Dimension	Pradeep Waychal|Luiz Fernando Capretz	  It is impossible to separate the human factors from software engineering expertise during software development, because software is developed by people and for people. The intangible nature of software has made it a difficult product to successfully create, and an examination of the many reasons for major software system failures show that the reasons for failures eventually come down to human issues. Software developers, immersed as they are in the technological aspect of the product, can quickly learn lessons from technological failures and readily come up with solutions to avoid them in the future, yet they do not learn lessons from human aspects in software engineering. Dealing with human errors is much more difficult for developers and often this aspect is overlooked in the evaluation process as developers move on to issues that they are more comfortable solving. A major reason for this oversight is that software psychology (the softer side) has not developed as extensively. 	
1704.03961v1	http://arxiv.org/pdf/1704.03961v1	2017	Quantum error correction failure distributions: comparison of coherent   and stochastic error models	Jeff P. Barnes|Colin J. Trout|Dennis G. Lucarelli|B. D. Clader	  We compare failure distributions of quantum error correction circuits for stochastic errors and coherent errors. We utilize a fully coherent simulation of a fault tolerant quantum error correcting circuit for a $d=3$ Steane and surface code. We find that the output distributions are markedly different for the two error models, showing that no simple mapping between the two error models exists. Coherent errors create very broad and heavy-tailed failure distributions. This suggests that they are susceptible to outlier events and that mean statistics, such as pseudo-threshold estimates, may not provide the key figure of merit. This provides further statistical insight into why coherent errors can be so harmful for quantum error correction. These output probability distributions may also provide a useful metric that can be utilized when optimizing quantum error correcting codes and decoding procedures for purely coherent errors. 	
1705.02515v3	http://arxiv.org/pdf/1705.02515v3	2017	Epistemic Model Checking of Atomic Commitment Protocols with Byzantine   Failures	Omar Al-Bataineh	  The notion of knowledge-based program introduced by Halpern and Fagin provides a useful formalism for designing, analysing, and optimising distributed systems. This paper formulates the two phase commit protocol as a knowledge-based program and then an iterative process of model checking and counter-example guided refinement is followed to find concrete implementations of the program for the case of perfect recall semantic in the Byzantine failures context with synchronous reliable communication. We model several different kinds of Byzantine failures and verify different strategies to fight and mitigate them. We address a number of questions that have not been considered in the prior literature, viz., under what circumstances a sender can know that its transmission has been successful, and under what circumstances an agent can know that the coordinator is cheating, and find concrete answers to these questions. The paper describes also a methodology based on temporal-epistemic model checking technology that can be followed to verify the shortest and longest execution time of a distributed protocol and the scenarios that lead to them. 	
1705.10422v2	http://arxiv.org/pdf/1705.10422v2	2017	Learning End-to-end Multimodal Sensor Policies for Autonomous Navigation	Guan-Horng Liu|Avinash Siravuru|Sai Prabhakar|Manuela Veloso|George Kantor	  Multisensory polices are known to enhance both state estimation and target tracking. However, in the space of end-to-end sensorimotor control, this multi-sensor outlook has received limited attention. Moreover, systematic ways to make policies robust to partial sensor failure are not well explored. In this work, we propose a specific customization of Dropout, called \textit{Sensor Dropout}, to improve multisensory policy robustness and handle partial failure in the sensor-set. We also introduce an additional auxiliary loss on the policy network in order to reduce variance in the band of potential multi- and uni-sensory policies to reduce jerks during policy switching triggered by an abrupt sensor failure or deactivation/activation. Finally, through the visualization of gradients, we show that the learned policies are conditioned on the same latent states representation despite having diverse observations spaces - a hallmark of true sensor-fusion. Simulation results of the multisensory policy, as visualized in TORCS racing game, can be seen here: https://youtu.be/QAK2lcXjNZc. 	
1706.03539v1	http://arxiv.org/pdf/1706.03539v1	2017	Resilient Work Stealing	Pascal Costanza|Charlotte Herzeel|Wolfgang De Meuter|Roel Wuyts	  Future generations of processors will exhibit an increase of faults over their lifetime, and it becomes increasingly expensive to solve the resulting reliability issues purely at the hardware level. We propose to model computations in terms of restartable task graphs in order to improve reliability at the software level. As a proof of concept, we present Cobra, a novel design for a shared-memory work-stealing scheduler that realizes this notion of restartable task graphs, and enables computations to survive hardware failures due to soft errors. A comparison with the work-stealing scheduler of Threading Building Blocks on the PARSEC benchmark suite shows that Cobra incurs no performance overhead in the absence of failures, and low performance overheads in the presence of single and multiple failures. 	
1706.09688v1	http://arxiv.org/pdf/1706.09688v1	2017	On role of matrix behavior in compressive fracture of bovine cortical   bone	Ashwij Mayya|Anuradha Banerjee|R. Rajesh	  In compressive fracture of dry plexiform bone, we examine the individual roles of overall mean porosity, the connectivity of the porosity network, and the elastic as well as the failure properties of the non-porous matrix, using a random spring network model. Porosity network structure is shown to reduce the compressive strength by upto 30%. However, the load bearing capacity increases with increase in either of the matrix properties - elastic modulus or failure strain threshold. To validate the porosity-based RSNM model with available experimental data, bone-specific failure strain thresholds for the ideal matrix of similar elastic properties were estimated to be within 60% of each other. Further, we observe the avalanche size exponents to be independent of the bone-dependent parameters as well as the structure of the porosity network. 	
1709.01062v1	http://arxiv.org/pdf/1709.01062v1	2017	Hierarchical loss for classification	Cinna Wu|Mark Tygert|Yann LeCun	  Failing to distinguish between a sheepdog and a skyscraper should be worse and penalized more than failing to distinguish between a sheepdog and a poodle; after all, sheepdogs and poodles are both breeds of dogs. However, existing metrics of failure (so-called "loss" or "win") used in textual or visual classification/recognition via neural networks seldom view a sheepdog as more similar to a poodle than to a skyscraper. We define a metric that, inter alia, can penalize failure to distinguish between a sheepdog and a skyscraper more than failure to distinguish between a sheepdog and a poodle. Unlike previously employed possibilities, this metric is based on an ultrametric tree associated with any given tree organization into a semantically meaningful hierarchy of a classifier's classes. 	
1709.04701v1	http://arxiv.org/pdf/1709.04701v1	2017	Codes for Erasures over Directed Graphs	Lev Yohananov|Eitan Yaakobi	  In this work we continue the study of a new class of codes, called \emph{codes over graphs}. Here we consider storage systems where the information is stored on the edges of a complete directed graph with $n$ nodes. The failure model we consider is of \emph{node failures} which are erasures of all edges, both incoming and outgoing, connected to the failed node. It is said that a code over graphs is a \textit{$\rho$-node-erasure-correcting code} if it can correct the failure of any $\rho$ nodes in the graphs of the code. While the construction of such optimal codes is an easy task if the field size is ${\cal O} (n^2)$, our main goal in the paper is the construction of codes over smaller fields. In particular, our main result is the construction of optimal binary codes over graphs which correct two node failures with a prime number of nodes. 	
1709.04714v1	http://arxiv.org/pdf/1709.04714v1	2017	Trace and Stable Failures Semantics for CSP-Agda	Bashar Igried|Anton Setzer	  CSP-Agda is a library, which formalises the process algebra CSP in the interactive theorem prover Agda using coinductive data types. In CSP-Agda, CSP processes are in monadic form, which sup- ports a modular development of processes. In this paper, we implement two main models of CSP, trace and stable failures semantics, in CSP-Agda, and define the corresponding refinement and equal- ity relations. Because of the monadic setting, some adjustments need to be made. As an example, we prove commutativity of the external choice operator w.r.t. the trace semantics in CSP-Agda, and that refinement w.r.t. stable failures semantics is a partial order. All proofs and definitions have been type checked in Agda. Further proofs of algebraic laws will be available in the CSP-Agda repository. 	
1709.04749v1	http://arxiv.org/pdf/1709.04749v1	2017	Why Early-Stage Software Startups Fail: A Behavioral Framework	Carmine Giardino|Xiaofeng Wang|Pekka Abrahamsson	  Software startups are newly created companies with little operating history and oriented towards producing cutting-edge products. As their time and resources are extremely scarce, and one failed project can put them out of business, startups need effective practices to face with those unique challenges. However, only few scientific studies attempt to address characteristics of failure, especially during the early- stage. With this study we aim to raise our understanding of the failure of early-stage software startup companies. This state-of-practice investigation was performed using a literature review followed by a multiple-case study approach. The results present how inconsistency between managerial strategies and execution can lead to failure by means of a behavioral framework. Despite strategies reveal the first need to understand the problem/solution fit, actual executions prioritize the development of the product to launch on the market as quickly as possible to verify product/market fit, neglecting the necessary learning process. 	
1709.06449v1	http://arxiv.org/pdf/1709.06449v1	2017	A new restart procedure for combinatorial optimization and its   convergence	Davide Palmigiani|Giovanni Sebastiani	  We propose a new iterative procedure to optimize the restart for meta-heuristic algorithms to solve combinatorial optimization, which uses independent algorithm executions. The new procedure consists of either adding new executions or extending along time the existing ones. This is done on the basis of a criterion that uses a surrogate of the algorithm failure probability, where the optimal solution is replaced by the best so far one. Therefore, it can be applied in practice. We prove that, with probability one, the restart time of the proposed procedure approaches, as the number of iterations diverges, the optimal value that minimizes the expected time to find the solution. We apply the proposed restart procedure to several Traveling Salesman Problem instances with hundreds or thousands of cities. As basic algorithm, we used different versions of an Ant Colony Optimization algorithm. We compare the results from the restart procedure with those from the basic algorithm. This is done by considering the failure probability of the two approaches for equal computation cost. This comparison showed a significant gain when applying the proposed restart procedure, whose failure probability is several orders of magnitude lower. 	
1709.08928v1	http://arxiv.org/pdf/1709.08928v1	2017	Multi-Rack Distributed Data Storage Networks	Ali Tebbi|Terence H. Chan|Chi Wan Sung	  The majority of works in distributed storage networks assume a simple network model with a collection of identical storage nodes with the same communication cost between the nodes. In this paper, we consider a realistic multi-rack distributed data storage network and present a code design framework for this model. Considering the cheaper data transmission within the racks, our code construction method is able to locally repair the nodes failure within the same rack by using only the survived nodes in the same rack. However, in the case of severe failure patterns when the information content of the survived nodes is not sufficient to repair the failures, other racks will participate in the repair process. By employing the criteria of our multi-rack storage code, we establish a linear programming bound on the size of the code in order to maximize the code rate. 	
1710.00283v1	http://arxiv.org/pdf/1710.00283v1	2017	A Versatile Approach to Evaluating and Testing Automated Vehicles based   on Kernel Methods	Zhiyuan Huang|Yaohui Guo|Henry Lam|Ding Zhao	  Evaluation and validation of complicated control systems are crucial to guarantee usability and safety. Usually, failure happens in some very rarely encountered situations, but once triggered, the consequence is disastrous. Accelerated Evaluation is a methodology that efficiently tests those rarely-occurring yet critical failures via smartly-sampled test cases. The distribution used in sampling is pivotal to the performance of the method, but building a suitable distribution requires case-by-case analysis. This paper proposes a versatile approach for constructing sampling distribution using kernel method. The approach uses statistical learning tools to approximate the critical event sets and constructs distributions based on the unique properties of Gaussian distributions. We applied the method to evaluate the automated vehicles. Numerical experiments show proposed approach can robustly identify the rare failures and significantly reduce the evaluation time. 	
1711.00270v1	http://arxiv.org/pdf/1711.00270v1	2017	Determination of Checkpointing Intervals for Malleable Applications	K. Raghavendra|Sathish S Vadhiyar	  Selecting optimal intervals of checkpointing an application is important for minimizing the run time of the application in the presence of system failures. Most of the existing efforts on checkpointing interval selection were developed for sequential applications while few efforts deal with parallel applications where the applications are executed on the same number of processors for the entire duration of execution. Some checkpointing systems support parallel applications where the number of processors on which the applications execute can be changed during the execution. We refer to these kinds of parallel applications as {\em malleable} applications. In this paper, we develop a performance model for malleable parallel applications that estimates the amount of useful work performed in unit time (UWT) by a malleable application in the presence of failures as a function of checkpointing interval. We use this performance model function with different intervals and select the interval that maximizes the UWT value. By conducting a large number of simulations with the traces obtained on real supercomputing systems, we show that the checkpointing intervals determined by our model can lead to high efficiency of applications in the presence of failures. 	
1711.06832v1	http://arxiv.org/pdf/1711.06832v1	2017	Correlation and shear bands in a plastically deformed granular medium	Kamran Karimi|Jean-Louis Barrat	  Recent experiments (Le Bouil et al., Phys. Rev. Lett., 2014, 112, 246001) have analyzed the statistics of local deformation in a granular solid undergoing plastic deformation. Experiments report strongly anisotropic correlation between events, with a characteristic angle that was interpreted using elasticity theory and the concept of Eshelby transformations with dilation; interestingly, the shear bands that characterize macroscopic failure occur at an angle that is different from the one observed in microscopic correlations. Here, we interpret this behavior using a mesoscale elastoplastic model of solid flow that incorporates a local Mohr-Coulomb failure criterion. We show that the angle observed in the microscopic correlations can be understood by combining the elastic interactions associated with Eshelby transformation with the local failure criterion. At large strains, we also induce permanent shear bands at an angle that is different from the one observed in the correlation pattern. We interpret this angle as the one that leads to the maximal instability of slip lines. 	
1712.08342v2	http://arxiv.org/pdf/1712.08342v2	2018	Event-based Failure Prediction in Distributed Business Processes	Michael Borkowski|Walid Fdhila|Matteo Nardelli|Stefanie Rinderle-Ma|Stefan Schulte	  Traditionally, research in Business Process Management has put a strong focus on centralized and intra-organizational processes. However, today's business processes are increasingly distributed, deviating from a centralized layout, and therefore calling for novel methodologies of detecting and responding to unforeseen events, such as errors occurring during process runtime. In this article, we demonstrate how to employ event-based failure prediction in business processes. This approach allows to make use of the best of both traditional Business Process Management Systems and event-based systems. Our approach employs machine learning techniques and considers various types of events. We evaluate our solution using two business process data sets, including one from a real-world event log, and show that we are able to detect errors and predict failures with high accuracy. 	
1712.08407v1	http://arxiv.org/pdf/1712.08407v1	2017	Verifying Resiliency in Closed-Loop Structured Systems	Shana Moothedath|RaviTeja Gundeti|Prasanna Chaporkar	  This paper deals with the analysis of robustness of closed-loop structured system. Specifically, given a structured system and a structured feedback matrix, our aim is to verify whether the closed-loop system is robust to failure of any k feedback links, where k is an integer. We refer to this problem as the feedback resilience problem. Firstly, we show that the feedback resilience problem is NP-complete using a reduction from a known NP-complete problem, the blocker problem. This result also shows that even when the state digraph of the structured system is irreducible, the feedback resilience problem is NP-complete, since the structured system constructed in the reduction is irreducible. Subsequently, we propose an algorithm to verify the robustness of the feedback matrix of an irreducible system for two possible cases: (a) when the number of feedback link failure is one, and (b) when the number of feedback link failures are two. We also prove that the proposed algorithms verify the robustness of the feedback matrix accurately in polynomial time, and is more efficient than brute force validation. 	
1712.08537v1	http://arxiv.org/pdf/1712.08537v1	2017	Elastic interactions in damage models of brittle failure	Vincent Démery|Véronique Dansereau|Estelle Berthier|Laurent Ponson|Jérôme Weiss	  The failure of brittle solids involves, before macroscopic rupture, power-law distributed avalanches of local rupture events whereby microcracks nucleate and grow, which are also observed in for an elastic interface evolving in a non-homogeneous medium. For this reason, it is tempting to relate failure to the depinning of an elastic interface. Here we compute the elastic kernel of the interface representing the damage field of a brittle solid. In the case of a damage model of rupture under compression, which implements the Mohr-Coulomb criterion at the local scale, we show that the elastic kernel is unstable, and hence is very different from the kernels of usual interfaces. We show that the unstable modes are responsible for the localization of damage along a macroscopic fault observed in numerical simulations. At low disorder, the most unstable mode gives the orientation of the macroscopic fault that we measure in numerical simulations. The orientation of the fault changes when the level of disorder is increased, suggesting a complex interplay of the unstable modes and the disorder. 	
1712.09618v1	http://arxiv.org/pdf/1712.09618v1	2017	Power Plexus: A network based analysis	Malvika Singh|Sneha Mandan|Smriti Sharma	  Power generation and distribution remains an important topic of discussion since the industrial revolution. As the system continues to grow, it needs to evolve both in infrastructure, robustness and its resilience to deal with failures. One such potential failure that we target in this work is the cascading failure. This avalanche effect propagates through the network and we study this propagation by Percolation Theory and implement some solutions for mitigation. We have extended the percolation theory as given in Mark Newman. Networks: an introduction,for random nodes to targeted nodes having high load bearing which is eliminated from the network to study the cascade effect. We also implement mitigation strategy to improve the network performance. 	
1712.09856v1	http://arxiv.org/pdf/1712.09856v1	2017	Tight Bounds for Maximal Identifiability of Failure Nodes in Boolean   Network Tomography	Nicola Galesi|Fariba Ranjbar	  We study maximal identifiability, a measure recently introduced in Boolean Network Tomography to characterize networks' capability to localize failure nodes in end-to-end path measurements. Under standard assumptions on topologies and on monitors placement, we prove tight upper and lower bounds on the maximal identifiability of failure nodes for specific classes of network topologies, such as trees, bounded-degree graphs, $d$-dimensional grids, in both directed and undirected cases. Among other results we prove that directed $d$-dimensional grids with support $n$ have maximal identifiability $d$ using $nd$ monitors; and in the undirected case we show that $2d$ monitors suffice to get identifiability of $d-1$. We then study identifiability under embeddings: we establish relations between maximal identifiability, embeddability and dimension when network topologies are modelled as DAGs. Through our analysis we also refine and generalize results on limits of maximal identifiability recently obtained in [11] and [1]. Our results suggest the design of networks over $N$ nodes with maximal identifiability $\Omega(\sqrt{\log N})$ using $2\sqrt{\log N}$ monitors and heuristics to place monitors and edges in a network to boost maximal identifiability. 	
1802.05240v1	http://arxiv.org/pdf/1802.05240v1	2018	Adjoint Method to Calculate Shape Gradients of Failure Probabilaties for   Turbomachinery Components	Hanno Gottschalk|Mohamed Saadi|Onur Tanil Doganay|Kathrin Klamroth|Sebastian Schmitz	  In the optimization of turbomachinery components, shape sensitivities for fluid dynamical objective functions have been used for a long time. As peak stress is not a differential func- tional of the shape, such highly efficient procedures so far have been missing for objective functionals that stem from mechan- ical integrity. This changes, if deterministic lifing criteria are replaced by probabilistic criteria, which have been introduced recently to the field of low cycle fatigue (LCF). Here we present a finite element (FEA) based first discretize, then adjoin approach to the calculation of shape gradients (sen- sitivities) for the failure probability with regard to probabilistic LCF and apply it to simple and complex geometries, as e.g. a blisk geometry. We review the computation of failure probabilities with a FEA postprocessor and sketch the computation of the relevant quantities for the adjoint method. We demonstrate high accuracy and computational efficiency of the adjoint method compared to finite difference schemes. We discuss implementation details for rotating components with cyclic boundary conditions. Finally, we shortly comment on future development steps and on poten- tial applications in multi criteria optimization. 	
1802.07600v1	http://arxiv.org/pdf/1802.07600v1	2018	Randomized sliding window algorithms for regular languages	Moses Ganardi|Danny Hucke|Markus Lohrey	  A sliding window algorithm receives a stream of symbols and has to output at each time instant a certain value which only depends on the last $n$ symbols. If the algorithm is randomized, then at each time instant it produces an incorrect output with probability at most $\epsilon$, which is a constant error bound. This work proposes a more relaxed definition of correctness which is parameterized by the error bound $\epsilon$ and the failure ratio $\phi$: A randomized sliding window algorithm is required to err with probability at most $\epsilon$ at a portion of $1-\phi$ of all time instants of an input stream. This work continues the investigation of sliding window algorithms for regular languages. In previous works a trichotomy theorem was shown for deterministic algorithms: the optimal space complexity is either constant, logarithmic or linear in the window size. The main results of this paper concerns three natural settings (randomized algorithms with failure ratio zero and randomized/deterministic algorithms with bounded failure ratio) and provide natural language theoretic characterizations of the space complexity classes. 	
1409.7286v4	http://arxiv.org/pdf/1409.7286v4	2015	Reliability of Erasure Coded Storage Systems: A Geometric Approach	Antonio Campello|Vinay A. Vaishampayan	  We consider the probability of data loss, or equivalently, the reliability function for an erasure coded distributed data storage system under worst case conditions. Data loss in an erasure coded system depends on probability distributions for the disk repair duration and the disk failure duration. In previous works, the data loss probability of such systems has been studied under the assumption of exponentially distributed disk failure and disk repair durations, using well-known analytic methods from the theory of Markov processes. These methods lead to an estimate of the integral of the reliability function.   Here, we address the problem of directly calculating the data loss probability for general repair and failure duration distributions. A closed limiting form is developed for the probability of data loss and it is shown that the probability of the event that a repair duration exceeds a failure duration is sufficient for characterizing the data loss probability.   For the case of constant repair duration, we develop an expression for the conditional data loss probability given the number of failures experienced by a each node in a given time window. We do so by developing a geometric approach that relies on the computation of volumes of a family of polytopes that are related to the code. An exact calculation is provided and an upper bound on the data loss probability is obtained by posing the problem as a set avoidance problem. Theoretical calculations are compared to simulation results. 	
1605.05442v2	http://arxiv.org/pdf/1605.05442v2	2016	Resilience of antagonistic networks with regard to the effects of   initial failures and degree-degree correlations	Shunsuke Watanabe|Yoshiyuki Kabashima	  In this study, we investigate the resilience of duplex networked layers ($\alpha$ and $\beta$) coupled with antagonistic interlinks, each layer of which inhibits its counterpart at the microscopic level, changing the following factors: whether the influence of the initial failures in $\alpha$ remains (quenched (Case Q)) or not (free (Case F)); the effect of intralayer degree-degree correlations in each layer and interlayer degree-degree correlations; and the type of the initial failures, such as random failures (RFs) or targeted attacks (TAs). We illustrate that the percolation processes repeat in both Cases Q and F, although only in Case F are nodes that initially failed reactivated. To analytically evaluate the resilience of each layer, we develop a methodology based on the cavity method for deriving the size of a giant component (GC). Strong hysteresis, which is ignored in the standard cavity analysis, is observed in the repetition of the percolation processes particularly in Case F. To handle this, we heuristically modify interlayer messages for macroscopic analysis, the utility of which is verified by numerical experiments. The percolation transition in each layer is continuous in both Cases Q and F. We also analyze the influences of degree-degree correlations on the robustness of layer $\alpha$, in particular for the case of TAs. The analysis indicates that the critical fraction of initial failures that makes the GC size in layer $\alpha$ vanish depends only on its intralayer degree-degree correlations. Although our model is defined in a somewhat abstract manner, it may have relevance to ecological systems that are composed of endangered species (layer $\alpha$) and invaders (layer $\beta$), the former of which are damaged by the latter whereas the latter are exterminated in the areas where the former are active. 	
1703.03657v1	http://arxiv.org/pdf/1703.03657v1	2017	Using STPA in Compliance with ISO 26262 for Developing a Safe   Architecture for Fully Automated Vehicles	Asim Abdulkhaleq|Stefan Wagner|Daniel Lammering|Hagen Boehmert|Pierre Blueher	  Safety has become of paramount importance in the development lifecycle of the modern automobile systems. However, the current automotive safety standard ISO 26262 does not specify clearly the methods for safety analysis. Different methods are recommended for this purpose. FTA (Fault Tree Analysis) and FMEA (Failure Mode and Effects Analysis) are used in the most recent ISO 26262 applications to identify component failures, errors and faults that lead to specific hazards (in the presence of faults). However, these methods are based on reliability theory, and they are not adequate to address new hazards caused by dysfunctional component interactions, software failure or human error. A holistic approach was developed called STPA (Systems-Theoretic Process Analysis) which addresses more types of hazards and treats safety as a dynamic control problem rather than an individual component failure. STPA also addresses types of hazardous causes in the absence of failure. Accordingly, there is a need for investigating hazard analysis techniques like STPA. In this paper, we present a concept on how to use STPA to extend the safety scope of ISO 26262 and support the Hazard Analysis and Risk Assessments (HARA) process. We applied the proposed concept to a current project of a fully automated vehicle at Continental. As a result, we identified 24 system- level accidents, 176 hazards, 27 unsafe control actions, and 129 unsafe scenarios. We conclude that STPA is an effective and efficient approach to derive detailed safety constraints. STPA can support the functional safety engineers to evaluate the architectural design of fully automated vehicles and build the functional safety concept. 	
1706.05431v1	http://arxiv.org/pdf/1706.05431v1	2017	Centralized Multi-Node Repair Regenerating Codes	Marwen Zorgui|Zhiying Wang	  In a distributed storage system, recovering from multiple failures is a critical and frequent task that is crucial for maintaining the system's reliability and fault-tolerance. In this work, we focus on the problem of repairing multiple failures in a centralized way, which can be desirable in many data storage configurations, and we show that a significant repair traffic reduction is possible. The fundamental functional tradeoff between the repair bandwidth and the storage size for functional repair is established. Using a graph-theoretic formulation, the optimal tradeoff is identified as the solution to an integer optimization problem, for which a closed-form expression is derived. Expressions of the extreme points, namely the minimum storage multi-node repair (MSMR) and minimum bandwidth multi-node repair (MBMR) points, are obtained. We describe a general framework for converting single erasure minimum storage regenerating codes to MSMR codes. The repair strategy for $e$ failures is similar to that for single failure, however certain extra requirements need to be satisfied by the repairing functions for single failure. For illustration, the framework is applied to product-matrix codes and interference alignment codes. Furthermore, we prove that functional MBMR point is not achievable for linear exact repair codes. We also show that exact-repair minimum bandwidth cooperative repair (MBCR) codes achieve an interior point, that lies near the MBMR point, when $k \equiv 1 \mod e$, $k$ being the minimum number of nodes needed to reconstruct the entire data. Finally, for $k> 2e, e\mid k$ and $e \mid d$, where $d$ is the number of helper nodes during repair, we show that the functional repair tradeoff is not achievable under exact repair, except for maybe a small portion near the MSMR point, which parallels the results for single erasure repair by Shah et al. 	
1709.01651v2	http://arxiv.org/pdf/1709.01651v2	2017	Cascading failures in interdependent systems under a flow redistribution   model	Yingrui Zhang|Alex Arenas|Osman Yağan	  Robustness and cascading failures in interdependent systems has been an active research field in the past decade. However, most existing works use percolation-based models where only the largest component of each network remains functional throughout the cascade. Although suitable for communication networks, this assumption fails to capture the dependencies in systems carrying a flow (e.g., power systems, road transportation networks), where cascading failures are often triggered by redistribution of flows leading to overloading of lines. Here, we consider a model consisting of systems $A$ and $B$ with initial line loads and capacities given by $\{L_{A,i},C_{A,i}\}_{i=1}^{n}$ and $\{L_{B,i},C_{B,i}\}_{i=1}^{n}$, respectively. When a line fails in system $A$, $a$-fraction of its load is redistributed to alive lines in $B$, while remaining $(1-a)$-fraction is redistributed equally among all functional lines in $A$; a line failure in $B$ is treated similarly with $b$ giving the fraction to be redistributed to $A$. We give a thorough analysis of cascading failures of this model initiated by a random attack targeting $p_1$-fraction of lines in $A$ and $p_2$-fraction in $B$. We show that (i) the model captures the real-world phenomenon of unexpected large scale cascades and exhibits interesting transition behavior: the final collapse is always first-order, but it can be preceded by a sequence of first and second-order transitions; (ii) network robustness tightly depends on the coupling coefficients $a$ and $b$, and robustness is maximized at non-trivial $a,b$ values in general; (iii) unlike existing models, interdependence has a multi-faceted impact on system robustness in that interdependency can lead to an improved robustness for each individual network. 	
1712.06064v1	http://arxiv.org/pdf/1712.06064v1	2017	Computing Optimal Control of Cascading Failure in DC Networks	Qin Ba|Ketan Savla	  We consider discrete-time dynamics, for cascading failure in DC networks, whose map is composition of failure rule with control actions. Supply-demand at nodes is monotonically nonincreasing under admissible control. Under the failure rule, a link is removed permanently if its flow exceeds its capacities. We consider finite horizon optimal control to steer the network from an arbitrary initial state, defined in terms of active link set and supply-demand at nodes, to a feasible state, i.e., a state that is invariant under the failure rule. There is no running cost and the reward associated with a feasible terminal state is the associated cumulative supply-demand. We propose two approaches for computing optimal control, and provide time complexity analysis for these approaches. The first approach, geared towards tree reducible networks, decomposes the global problem into a system of coupled local problems, which can be solved to optimality in two iterations. In the first iteration, optimal solutions to the local problems are computed, from leaf nodes to the root node, in terms of the coupling variables. In the second iteration, in the reverse order, the local optimal solutions are instantiated with specific values of the coupling variables. Restricted to constant controls, the optimal solutions to the local problems possess a piecewise linear property, which facilitates analytical solution. The second approach computes optimal control by searching over the reachable set, which is shown to admit an equivalent finite representation by aggregation of control actions leading to the same reachable active link set. An algorithmic procedure to construct this representation is provided by leveraging and extending tools for arrangement of hyperplanes and convex polytopes. Illustrative simulations, including showing the effectiveness of a projection-based approximation algorithm, are also presented. 	
0311045v2	http://arxiv.org/pdf/math/0311045v2	2003	A Phase Transition and Stochastic Domination in Pippenger's   Probabilistic Failure Model for Boolean Networks with Unreliable Gates	Maxim Raginsky	  We study Pippenger's model of Boolean networks with unreliable gates. In this model, the conditional probability that a particular gate fails, given the failure status of any subset of gates preceding it in the network, is bounded from above by some $\epsilon$. We show that if we pick a Boolean network with $n$ gates at random according to the Barak-Erd\H{o}s model of a random acyclic digraph, such that the expected edge density is $c n^{-1}\log n$, and if $\epsilon$ is equal to a certain function of the size of the largest reflexive, transitive closure of a vertex (with respect to a particular realization of the random digraph), then Pippenger's model exhibits a phase transition at $c=1$. Namely, with probability $1-o(1)$ as $n\to\infty$, we have the following: for $0 \le c \le 1$, the minimum of the probability that no gate has failed, taken over all probability distributions of gate failures consistent with Pippenger's model, is equal to $o(1)$, whereas for $c >1$ it is equal to $\exp(-\frac{c}{e(c-1)}) + o(1)$. We also indicate how a more refined analysis of Pippenger's model, e.g., for the purpose of estimating probabilities of monotone events, can be carried out using the machinery of stochastic domination. 	
0407185v1	http://arxiv.org/pdf/math/0407185v1	2004	Routing Complexity of Faulty Networks	Omer Angel|Itai Benjamini|Eran Ofek|Udi Wieder	  One of the fundamental problems in distributed computing is how to efficiently perform routing in a faulty network in which each link fails with some probability. This paper investigates how big the failure probability can be, before the capability to efficiently find a path in the network is lost. Our main results show tight upper and lower bounds for the failure probability which permits routing, both for the hypercube and for the $d-$dimensional mesh. We use tools from percolation theory to show that in the $d-$dimensional mesh, once a giant component appears -- efficient routing is possible. A different behavior is observed when the hypercube is considered. In the hypercube there is a range of failure probabilities in which short paths exist with high probability, yet finding them must involve querying essentially the entire network. Thus the routing complexity of the hypercube shows an asymptotic phase transition. The critical probability with respect to routing complexity lies in a different location then that of the critical probability with respect to connectivity. Finally we show that an oracle access to links (as opposed to local routing) may reduce significantly the complexity of the routing problem. We demonstrate this fact by providing tight upper and lower bounds for the complexity of routing in the random graph $G_{n,p}$. 	
0606014v1	http://arxiv.org/pdf/nlin/0606014v1	2006	Cascades of Failure and Extinction in Evolving Complex Systems	Paul Ormerod|Rich Colbaugh	  There is empirical evidence from a range of disciplines that as the connectivity of a network increases, we observe an increase in the average fitness of the system. But at the same time, there is an increase in the proportion of failure/extinction events which are extremely large. The probability of observing an extreme event remains very low, but it is markedly higher than in the system with lower degrees of connectivity. We give examples from complex systems such as outages in the US power grid, the robustness properties of cell biology networks, and trade links and the propagation of both currency crises and disease. We consider networks which are populated by agents which are heterogeneous in terms of their fitness for survival. The network evolves over time, and in each period agents take self-interested decisions to increase their fitness for survival to form alliances which increase the connectivity of the network. The network is subjected to external negative shocks both with respect to the size of the shock and the spatial impact of the shock. We examine the size/frequency distribution of extinctions and how this distribution evolves as the connectivity of the network grows. The results are robust with respect to the choice of statistical distribution of the shocks. We find that increasing the number of connections causes an increase in the average fitness of agents, yet at the same time makes the system as whole more vulnerable to catastrophic failure/extinction events on an near-global scale. 	
0602001v1	http://arxiv.org/pdf/q-bio/0602001v1	2006	New Computational Approaches to Analysis of Interbeat Intervals in Human   Subjects	M. Reza Rahimi Tabar|Fatemeh Ghasemi|Joachim Peinke|Rudolf Friedrich|Kamran Kaviani|Fatemeh Taghavi|Sara Sadeghi|Golnoosh Bijani|Muhammad Sahimi	  We investigate the Markov nature, Cascade of information from large time scale to small scale and extended self similarity properties of the beat to beat fluctuations of healthy subjects as well as those with congestive heart failure. To check the Markov nature, we use a novel inverse method that utilizes a set of data to construct a simple equation that governs the stochastic process for which the data have been measured, hence enabling us to reconstruct the stochastic process. The inverse method provides a novel technique for distinguishing the two classes of subjects in terms of a drift and a diffusion coefficients which behave completely differently for the two classes of subjects.To investigate the cascade of information from large to small time scales we also analyze the statistical properties of interbeat intervals cascade by considering the joint probability distribution for two interbeat increments. As a result, the joint probability distributions of the increments in the interbeat intervals obey a Fokker-Planck equation. Finally we analyze the extended self-similarity (ESS) in the beat-to-beat fluctuations in the heart rates of healthy and congestive heart failure subjects.The proposed methods provide the novel techniques for distinguishing the two classes of subjects in terms of the drift and diffusion coefficients, intermittency exponents which behave differently for two classes of the subjects, namely, healthy subjects and those with congestive heart failure. 	
0207119v4	http://arxiv.org/pdf/quant-ph/0207119v4	2003	Overhead and noise threshold of fault-tolerant quantum error correction	Andrew M. Steane	  Fault tolerant quantum error correction (QEC) networks are studied by a combination of numerical and approximate analytical treatments. The probability of failure of the recovery operation is calculated for a variety of CSS codes, including large block codes and concatenated codes. Recent insights into the syndrome extraction process, which render the whole process more efficient and more noise-tolerant, are incorporated. The average number of recoveries which can be completed without failure is thus estimated as a function of various parameters. The main parameters are the gate (gamma) and memory (epsilon) failure rates, the physical scale-up of the computer size, and the time t_m required for measurements and classical processing. The achievable computation size is given as a surface in parameter space. This indicates the noise threshold as well as other information. It is found that concatenated codes based on the [[23,1,7]] Golay code give higher thresholds than those based on the [[7,1,3]] Hamming code under most conditions. The threshold gate noise gamma_0 is a function of epsilon/gamma and t_m; example values are {epsilon/gamma, t_m, gamma_0} = {1, 1, 0.001}, {0.01, 1, 0.003}, {1, 100, 0.0001}, {0.01, 100, 0.002}, assuming zero cost for information transport. This represents an order of magnitude increase in tolerated memory noise, compared with previous calculations, which is made possible by recent insights into the fault-tolerant QEC process. 	
0712.1001v4	http://arxiv.org/pdf/0712.1001v4	2011	Analysis based on the Wavelet & Hilbert transforms applied to the full   time series of interbeats, for a triad of failures at the heart	P. A. Ritto	  A tetra of sets which elements are time series of interbeats has been obtained from the databank Physionet-MIT-BIH, corresponding to the following failures at the humans' heart: Obstructive Sleep Apnea, Congestive Heart Failure, and Atrial Fibrillation. Those times series has been analyzed statistically using an already known technique based on the Wavelet and Hilbert Transforms. That technique has been applied to the time series of interbeats for 87 patients, in order to find out the dynamics of the heart. The size of the times series varies around 7 to 24 h. while the kind of wavelet selected for this study has been any one of: Daubechies, Biortoghonal, and Gaussian. The analysis has been done for the complet set of scales ranging from: 1-128 heartbeats. Choosing the Biorthogonal wavelet: bior3.1, it is observed: (a) That the time series hasn't to be cutted in shorter periods, with the purpose to obtain the collapsing of the data, (b) An analytical, universal behavior of the data, for the first and second diseases, but not for the third. 	
0808.0529v2	http://arxiv.org/pdf/0808.0529v2	2008	Rate dependent shear bands in a shear transformation zone model of   amorphous solids	M. L. Manning|E. G. Daub|J. S. Langer|J. M. Carlson	  We use Shear Transformation Zone (STZ) theory to develop a deformation map for amorphous solids as a function of the imposed shear rate and initial material preparation. The STZ formulation incorporates recent simulation results [Haxton and Liu, PRL 99 195701 (2007)] showing that the steady state effective temperature is rate dependent. The resulting model predicts a wide range of deformation behavior as a function of the initial conditions, including homogeneous deformation, broad shear bands, extremely thin shear bands, and the onset of material failure. In particular, the STZ model predicts homogeneous deformation for shorter quench times and lower strain rates, and inhomogeneous deformation for longer quench times and higher strain rates. The location of the transition between homogeneous and inhomogeneous flow on the deformation map is determined in part by the steady state effective temperature, which is likely material dependent. This model also suggests that material failure occurs due to a runaway feedback between shear heating and the local disorder, and provides an explanation for the thickness of shear bands near the onset of material failure. We find that this model, which resolves dynamics within a sheared material interface, predicts that the stress weakens with strain much more rapidly than a similar model which uses a single state variable to specify internal dynamics on the interface. 	
0907.1368v1	http://arxiv.org/pdf/0907.1368v1	2009	Fault prediction in aircraft engines using Self-Organizing Maps	Marie Cottrell|Patrice Gaubert|Cédric Eloy|Damien François|Geoffroy Hallaux|Jérôme Lacaille|Michel Verleysen	  Aircraft engines are designed to be used during several tens of years. Their maintenance is a challenging and costly task, for obvious security reasons. The goal is to ensure a proper operation of the engines, in all conditions, with a zero probability of failure, while taking into account aging. The fact that the same engine is sometimes used on several aircrafts has to be taken into account too. The maintenance can be improved if an efficient procedure for the prediction of failures is implemented. The primary source of information on the health of the engines comes from measurement during flights. Several variables such as the core speed, the oil pressure and quantity, the fan speed, etc. are measured, together with environmental variables such as the outside temperature, altitude, aircraft speed, etc. In this paper, we describe the design of a procedure aiming at visualizing successive data measured on aircraft engines. The data are multi-dimensional measurements on the engines, which are projected on a self-organizing map in order to allow us to follow the trajectories of these data over time. The trajectories consist in a succession of points on the map, each of them corresponding to the two-dimensional projection of the multi-dimensional vector of engine measurements. Analyzing the trajectories aims at visualizing any deviation from a normal behavior, making it possible to anticipate an operation failure. 	
1005.0965v1	http://arxiv.org/pdf/1005.0965v1	2010	Artificial Neural Network based Diagnostic Model For Causes of Success   and Failures	Bikrampal Kaur|Himanshu Aggarwal	  In this paper an attempt has been made to identify most important human resource factors and propose a diagnostic model based on the back-propagation and connectionist model approaches of artificial neural network (ANN). The focus of the study is on the mobile -communication industry of India. The ANN based approach is particularly important because conventional approaches (such as algorithmic) to the problem solving have their inherent disadvantages. The algorithmic approach is well-suited to the problems that are well-understood and known solution(s). On the other hand the ANNs have learning by example and processing capabilities similar to that of a human brain. ANN has been followed due to its inherent advantage over conversion algorithmic like approaches and having capabilities, training and human like intuitive decision making capabilities. Therefore, this ANN based approach is likely to help researchers and organizations to reach a better solution to the problem of managing the human resource. The study is particularly important as many studies have been carried in developed countries but there is a shortage of such studies in developing nations like India. Here, a model has been derived using connectionist-ANN approach and improved and verified via back-propagation algorithm. This suggested ANN based model can be used for testing the success and failure human factors in any of the communication Industry. Results have been obtained on the basis of connectionist model, which has been further refined by BPNN to an accuracy of 99.99%. Any company to predict failure due to HR factors can directly deploy this model. 	
1007.3706v2	http://arxiv.org/pdf/1007.3706v2	2011	Cooperative Convex Optimization in Networked Systems: Augmented   Lagrangian Algorithms with Directed Gossip Communication	Dusan Jakovetic|Joao Xavier|Jose M. F. Moura	  We study distributed optimization in networked systems, where nodes cooperate to find the optimal quantity of common interest, x=x^\star. The objective function of the corresponding optimization problem is the sum of private (known only by a node,) convex, nodes' objectives and each node imposes a private convex constraint on the allowed values of x. We solve this problem for generic connected network topologies with asymmetric random link failures with a novel distributed, decentralized algorithm. We refer to this algorithm as AL-G (augmented Lagrangian gossiping,) and to its variants as AL-MG (augmented Lagrangian multi neighbor gossiping) and AL-BG (augmented Lagrangian broadcast gossiping.) The AL-G algorithm is based on the augmented Lagrangian dual function. Dual variables are updated by the standard method of multipliers, at a slow time scale. To update the primal variables, we propose a novel, Gauss-Seidel type, randomized algorithm, at a fast time scale. AL-G uses unidirectional gossip communication, only between immediate neighbors in the network and is resilient to random link failures. For networks with reliable communication (i.e., no failures,) the simplified, AL-BG (augmented Lagrangian broadcast gossiping) algorithm reduces communication, computation and data storage cost. We prove convergence for all proposed algorithms and demonstrate by simulations the effectiveness on two applications: l_1-regularized logistic regression for classification and cooperative spectrum sensing for cognitive radio networks. 	
1011.0234v1	http://arxiv.org/pdf/1011.0234v1	2010	Cascade of failures in coupled network systems with multiple   support-dependent relations	Jia Shao|Sergey V. Buldyrev|Shlomo Havlin|H. Eugene Stanley	  We study, both analytically and numerically, the cascade of failures in two coupled network systems A and B, where multiple support-dependent relations are randomly built between nodes of networks A and B. In our model we assume that each node in one network can function only if it has at least a single support node in the other network. If both networks A and B are Erd\H{o}s-R\'enyi networks, A and B, with (i) sizes $N^A$ and $N^B$, (ii) average degrees $a$ and $b$, and (iii) $c^{AB}_0N^B$ support links from network A to B and $c^{BA}_0N^B$ support links from network B to A, we find that under random attack with removal of fractions $(1-R^A)N^A$ and $(1-R^B)N^B$ nodes respectively, the percolating giant components of both networks at the end of the cascading failures, $\mu^A_\infty$ and $\mu^B_\infty$, are given by the percolation laws $\mu^A_\infty = R^A [1-\exp{({-c^{BA}_0\mu^B_\infty})}] [1-\exp{({-a\mu^A_\infty})}]$ and $\mu^B_\infty = R^B [1-\exp{({-c^{AB}_0\mu^A_\infty})}] [1-\exp{({-b\mu^B_\infty})}]$. In the limit of $c^{BA}_0 \to \infty$ and $c^{AB}_0 \to \infty$, both networks become independent, and the giant components are equivalent to a random attack on a single Erd\H{o}s-R\'enyi network. We also test our theory on two coupled scale-free networks, and find good agreement with the simulations. 	
1106.4213v1	http://arxiv.org/pdf/1106.4213v1	2011	A New and Efficient Algorithm-Based Fault Tolerance Scheme for A Million   Way Parallelism	Erlin Yao|Mingyu Chen|Rui Wang|Wenli Zhang|Guangming Tan	  Fault tolerance overhead of high performance computing (HPC) applications is becoming critical to the efficient utilization of HPC systems at large scale. HPC applications typically tolerate fail-stop failures by checkpointing. Another promising method is in the algorithm level, called algorithmic recovery. These two methods can achieve high efficiency when the system scale is not very large, but will both lose their effectiveness when systems approach the scale of Exaflops, where the number of processors including in system is expected to achieve one million. This paper develops a new and efficient algorithm-based fault tolerance scheme for HPC applications. When failure occurs during the execution, we do not stop to wait for the recovery of corrupted data, but replace them with the corresponding redundant data and continue the execution. A background accelerated recovery method is also proposed to rebuild redundancy to tolerate multiple times of failures during the execution. To demonstrate the feasibility of our new scheme, we have incorporated it to the High Performance Linpack. Theoretical analysis demonstrates that our new fault tolerance scheme can still be effective even when the system scale achieves the Exaflops. Experiment using SiCortex SC5832 verifies the feasibility of the scheme, and indicates that the advantage of our scheme can be observable even in a small scale. 	
1107.3302v1	http://arxiv.org/pdf/1107.3302v1	2011	A Temporal Neuro-Fuzzy Monitoring System to Manufacturing Systems	Rafik Mahdaoui|Leila Hayet Mouss|Mohamed Djamel Mouss|Ouahiba Chouhal	  Fault diagnosis and failure prognosis are essential techniques in improving the safety of many manufacturing systems. Therefore, on-line fault detection and isolation is one of the most important tasks in safety-critical and intelligent control systems. Computational intelligence techniques are being investigated as extension of the traditional fault diagnosis methods. This paper discusses the Temporal Neuro-Fuzzy Systems (TNFS) fault diagnosis within an application study of a manufacturing system. The key issues of finding a suitable structure for detecting and isolating ten realistic actuator faults are described. Within this framework, data-processing interactive software of simulation baptized NEFDIAG (NEuro Fuzzy DIAGnosis) version 1.0 is developed.   This software devoted primarily to creation, training and test of a classification Neuro-Fuzzy system of industrial process failures. NEFDIAG can be represented like a special type of fuzzy perceptron, with three layers used to classify patterns and failures. The system selected is the workshop of SCIMAT clinker, cement factory in Algeria. 	
1109.5636v2	http://arxiv.org/pdf/1109.5636v2	2011	Distributed sensor failure detection in sensor networks	Tamara Tosic|Nikolaos Thomos|Pascal Frossard	  We investigate the problem of distributed sensors' failure detection in networks with a small number of defective sensors, whose measurements differ significantly from neighboring sensor measurements. Defective sensors are represented by non-zero values in binary sparse signals. We build on the sparse nature of the binary sensor failure signals and propose a new distributed detection algorithm based on Group Testing (GT). The distributed GT algorithm estimates the set of defective sensors from a small number of linearly independent binary messages exchanged by the sensors. The distributed GT algorithm uses a low complexity distance decoder that is robust to noisy messages. We first consider networks with only one defective sensor and determine the minimal number of linearly independent messages needed for detection of the defective sensor with high probability. We then extend our study to the detection of multiple defective sensors by modifying appropriately the message exchange protocol and the decoding procedure. We show through experimentation that, for small and medium sized networks, the number of messages required for successful detection is actually smaller than the minimal number computed in the analysis. Simulations demonstrate that the proposed method outperforms methods based on random walk measurements collection in terms of detection performance and convergence rate. Finally, the proposed method is resilient to network dynamics due to the effective gossip-based message dissemination protocol. 	
1201.0206v1	http://arxiv.org/pdf/1201.0206v1	2011	A Failure Self-recovery Strategy with Balanced Energy Consumption for   Wireless Ad Hoc Networks	Tie Qiu|Wei Wang|Feng Xia|Guowei Wu|Yu Zhou	  In energy constrained wireless sensor networks, it is significant to make full use of the limited energy and maximize the network lifetime even when facing some unexpected situation. In this paper, all sensor nodes are grouped into clusters, and for each cluster, it has a mobile cluster head to manage the whole cluster. We consider an emergent situation that one of the mobile cluster heads is broken down, and hence the whole cluster is consequently out of work. An efficient approach is proposed for recovering the failure cluster by selecting multiple static sensor nodes as the cluster heads to collect packets and transmit them to the sink node. Improved simulated annealing algorithm is utilized to achieve the uniform deployment of the cluster heads. The new cluster heads are dynamically changed in order to keep balanced energy consumption. Among the new cluster heads, packets are transmitted through multi-hop forwarding path which is cost-lowest path found by Dijkstra's algorithm. A balanced energy consumption model is provided to help find the cost-lowest path and prolong the lifetime of the network. The forwarding path is updated dynamically according to the cost of the path and residual energy of the node in that path. The experimental results show that the failure cluster is recovered and the lifetime of the cluster is prolonged. 	
1208.4176v1	http://arxiv.org/pdf/1208.4176v1	2012	Building User-defined Runtime Adaptation Routines for Stream Processing   Applications	Gabriela Jacques-Silva|Buğra Gedik|Rohit Wagle|Kun-Lung Wu|Vibhore Kumar	  Stream processing applications are deployed as continuous queries that run from the time of their submission until their cancellation. This deployment mode limits developers who need their applications to perform runtime adaptation, such as algorithmic adjustments, incremental job deployment, and application-specific failure recovery. Currently, developers do runtime adaptation by using external scripts and/or by inserting operators into the stream processing graph that are unrelated to the data processing logic. In this paper, we describe a component called orchestrator that allows users to write routines for automatically adapting the application to runtime conditions. Developers build an orchestrator by registering and handling events as well as specifying actuations. Events can be generated due to changes in the system state (e.g., application component failures), built-in system metrics (e.g., throughput of a connection), or custom application metrics (e.g., quality score). Once the orchestrator receives an event, users can take adaptation actions by using the orchestrator actuation APIs. We demonstrate the use of the orchestrator in IBM's System S in the context of three different applications, illustrating application adaptation to changes on the incoming data distribution, to application failures, and on-demand dynamic composition. 	
1210.4640v1	http://arxiv.org/pdf/1210.4640v1	2012	A Scalable Byzantine Grid	Alexandre Maurer|Sébastien Tixeuil	  Modern networks assemble an ever growing number of nodes. However, it remains difficult to increase the number of channels per node, thus the maximal degree of the network may be bounded. This is typically the case in grid topology networks, where each node has at most four neighbors. In this paper, we address the following issue: if each node is likely to fail in an unpredictable manner, how can we preserve some global reliability guarantees when the number of nodes keeps increasing unboundedly ? To be more specific, we consider the problem or reliably broadcasting information on an asynchronous grid in the presence of Byzantine failures -- that is, some nodes may have an arbitrary and potentially malicious behavior. Our requirement is that a constant fraction of correct nodes remain able to achieve reliable communication. Existing solutions can only tolerate a fixed number of Byzantine failures if they adopt a worst-case placement scheme. Besides, if we assume a constant Byzantine ratio (each node has the same probability to be Byzantine), the probability to have a fatal placement approaches 1 when the number of nodes increases, and reliability guarantees collapse. In this paper, we propose the first broadcast protocol that overcomes these difficulties. First, the number of Byzantine failures that can be tolerated (if they adopt the worst-case placement) now increases with the number of nodes. Second, we are able to tolerate a constant Byzantine ratio, however large the grid may be. In other words, the grid becomes scalable. This result has important security applications in ultra-large networks, where each node has a given probability to misbehave. 	
1212.5492v1	http://arxiv.org/pdf/1212.5492v1	2012	Transition wave in a supported heavy beam	Michele Brun|Alexander B. Movchan|Leonid I. Slepyan	  We consider a heavy, uniform, elastic beam rested on periodically distributed supports as a simplified model of a bridge. The supports are subjected to a partial destruction propagating as a failure wave along the beam. Three related models are examined and compared: (a) a uniform elastic beam on a distributed elastic foundation, (b) an elastic beam which mass is concentrated at a discrete set of points corresponding to the discrete set of the elastic supports and (c) a uniform elastic beam on a set of discrete elastic supports. Stiffness of the support is assumed to drop when the stress reaches a critical value. In the formulation, it is also assumed that, at the moment of the support damage, the value of the `added mass', which reflects the dynamic response of the support, is dropped too. Strong similarities in the behavior of the continuous and discrete-continuous models are detected. Three speed regimes, subsonic, intersonic and supersonic, where the failure wave is or is not accompanied by elastic waves excited by the moving jump in the support stiffness, are considered and related characteristic speeds are determined. With respect to these continuous and discrete-continuous models, the conditions are found for the failure wave to exists, to propagate uniformly or to accelerate. It is also found that such beam-related transition wave can propagate steadily only at the intersonic speeds. It is remarkable that the steady-state speed appears to decrease as the jump of the stiffness increases. 	
1301.3996v2	http://arxiv.org/pdf/1301.3996v2	2013	Parameterizable Byzantine Broadcast in Loosely Connected Networks	Alexandre Maurer|Sébastien Tixeuil	  We consider the problem of reliably broadcasting information in a multihop asynchronous network, despite the presence of Byzantine failures: some nodes are malicious and behave arbitrarly. We focus on non-cryptographic solutions. Most existing approaches give conditions for perfect reliable broadcast (all correct nodes deliver the good information), but require a highly connected network. A probabilistic approach was recently proposed for loosely connected networks: the Byzantine failures are randomly distributed, and the correct nodes deliver the good information with high probability. A first solution require the nodes to initially know their position on the network, which may be difficult or impossible in self-organizing or dynamic networks. A second solution relaxed this hypothesis but has much weaker Byzantine tolerance guarantees. In this paper, we propose a parameterizable broadcast protocol that does not require nodes to have any knowledge about the network. We give a deterministic technique to compute a set of nodes that always deliver authentic information, for a given set of Byzantine failures. Then, we use this technique to experimentally evaluate our protocol, and show that it significantely outperforms previous solutions with the same hypotheses. Important disclaimer: these results have NOT yet been published in an international conference or journal. This is just a technical report presenting intermediary and incomplete results. A generalized version of these results may be under submission. 	
1301.5475v1	http://arxiv.org/pdf/1301.5475v1	2013	Modelling major failures in power grids in the whole range	Faustino Prieto|José María Sarabia|Antonio José Sáez	  Empirical research with electricity transmission networks reliability data shows that the size of major failures - in terms of energy not supplied (ENS), total loss of power (TLP) or restoration time (RT) - appear to follow a power law behaviour in the upper tail of the distribution. However, this pattern - also known as Pareto distribution - is not valid in the whole range of those major events. We aimed to find a probability distribution that we could use to model them, and hypothesized that there is a two-parameter model that fits the pattern of those data well in the entire domain. We considered the major failures produced between 2002 and 2009 in the European power grid; analyzed those reliability indicators: ENS, TLP and RT; fitted six alternative models: Pareto II, Fisk, Lognormal, Pareto, Weibull and Gamma distributions, to the data by maximum likelihood; compared these models by the Bayesian information criterion; tested the goodness-of-fit of those models by a Kolmogorov-Smirnov test method based on bootstrap resampling; and validated them graphically by rank-size plots. We found that Pareto II distribution is, in the case of ENS and TLP, an adequate model to describe major events reliability data of power grids in the whole range, and in the case of RT, is the best choice of the six alternative models analyzed. 	
1303.4026v6	http://arxiv.org/pdf/1303.4026v6	2015	Relative performance of ancilla verification and decoding in the   [[7,1,3]] Steane code	Ali Abu-Nada|Ben Fortescue|Mark Byrd	  Ancilla post-selection is a common means of achieving fault-tolerance in quantum error-correction. However, it can lead to additional data errors due to movement or wait operations. Alternatives to post-selection may achieve lower overall failure rates due to avoiding such errors. We present numerical simulation results comparing the logical error rates for the fault-tolerant [[7,1,3]] Steane code using techniques of ancilla verification vs. the newer method of ancilla decoding, as described in [D.P. DiVincenzo and P. Aliferis, PRL 98, 020501 (2007)]. We simulate QEC procedures in which rhe possibility of ancilla verification failures requires the creation and storage of additional ancillas and/or additional waiting of the data until a new ancilla can be created. We find that the decoding method, which avoids verification failures, is advantageous in terms of overall error rate in various cases, even when measurement operations are no slower than others. We analyze the effect of different classes of physical error on the relative performance of these two methods. 	
1310.1478v1	http://arxiv.org/pdf/1310.1478v1	2013	Brittle to Ductile Transition in a Fiber Bundle with Strong   Heterogeneity	K. Kovacs|R. C. Hidalgo|I. Pagonabarraga|F. Kun	  We analyze the failure process of a two-component system with widely different fracture strength in the framework of a fiber bundle model with localized load sharing. A fraction 0\leq \alpha \leq 1 of the bundle is strong and it is represented by unbreakable fibers, while fibers of the weak component have randomly distributed failure strength. Computer simulations revealed that there exists a critical composition \alpha_c which separates two qualitatively different behaviors: below the critical point the failure of the bundle is brittle characterized by an abrupt damage growth within the breakable part of the system. Above \alpha_c, however, the macroscopic response becomes ductile providing stability during the entire breaking process. The transition occurs at an astonishingly low fraction of strong fibers which can have importance for applications. We show that in the ductile phase the size distribution of breaking bursts has a power law functional form with an exponent \mu=2 followed by an exponential cutoff. In the brittle phase the power law also prevails but with a higher exponent \mu=9/2. The transition between the two phases shows analogies to continuous phase transitions. Analyzing the microstructure of the damage, it was found that at the beginning of the fracture process cracks nucleate randomly, while later on growth and coalescence of cracks dominate which give rise to power law distributed crack sizes. 	
1312.1653v1	http://arxiv.org/pdf/1312.1653v1	2013	A Random Field Model and its Application in Industrial Production	Julie Oger|Emmanuel Lesigne|Philippe Leduc	  In competitive industries, a reliable yield forecasting is a prime factor to accurately determine the production costs and therefore ensure profitability. Indeed, quantifying the risks long before the effective manufacturing process enables fact-based decision-making. From the development stage, improvement efforts can be early identified and prioritized. In order to measure the impact of industrial process fluctuations on the product performances, the construction of a failure risk probability estimator is presented in this article. The complex relationship between the process technology and the product design (non linearities, multi-modal features...) is handled via random process regression. A random field encodes, for each product configuration, the available information regarding the risk of non-compliance. After a brief presentation of the Gaussian model approach, we describe a Bayesian reasoning avoiding a priori choices of location and scale parameters. The Gaussian mixture prior, conditioned by measured (or calculated) data, yields a posterior characterized by a multivariate Student distribution. The probabilistic nature of the model is then operated to derive a failure risk probability, defined as a random variable. To do this, our approach is to consider as random all unknown, inaccessible or fluctuating data. In order to propagate uncertainties, a fuzzy set approach provides an appropriate framework for the implementation of a Bayesian model mimicking expert elicitation. The underlying leitmotiv is to insert minimal a priori information in the failure risk model. The relevancy of this concept is illustrated with theoretical examples. 	
1401.1086v1	http://arxiv.org/pdf/1401.1086v1	2014	Power Grid Defense Against Malicious Cascading Failure	Paulo Shakarian|Hansheng Lei|Roy Lindelauf	  An adversary looking to disrupt a power grid may look to target certain substations and sources of power generation to initiate a cascading failure that maximizes the number of customers without electricity. This is particularly an important concern when the enemy has the capability to launch cyber-attacks as practical concerns (i.e. avoiding disruption of service, presence of legacy systems, etc.) may hinder security. Hence, a defender can harden the security posture at certain power stations but may lack the time and resources to do this for the entire power grid. We model a power grid as a graph and introduce the cascading failure game in which both the defender and attacker choose a subset of power stations such as to minimize (maximize) the number of consumers having access to producers of power. We formalize problems for identifying both mixed and deterministic strategies for both players, prove complexity results under a variety of different scenarios, identify tractable cases, and develop algorithms for these problems. We also perform an experimental evaluation of the model and game on a real-world power grid network. Empirically, we noted that the game favors the attacker as he benefits more from increased resources than the defender. Further, the minimax defense produces roughly the same expected payoff as an easy-to-compute deterministic load based (DLB) defense when played against a minimax attack strategy. However, DLB performs more poorly than minimax defense when faced with the attacker's best response to DLB. This is likely due to the presence of low-load yet high-payoff nodes, which we also found in our empirical analysis. 	
1402.0581v1	http://arxiv.org/pdf/1402.0581v1	2014	Qualitative Order of Magnitude Energy-Flow-Based Failure Modes and   Effects Analysis	Neal Andrew Snooke|Mark H Lee	  This paper presents a structured power and energy-flow-based qualitative modelling approach that is applicable to a variety of system types including electrical and fluid flow. The modelling is split into two parts. Power flow is a global phenomenon and is therefore naturally represented and analysed by a network comprised of the relevant structural elements from the components of a system. The power flow analysis is a platform for higher-level behaviour prediction of energy related aspects using local component behaviour models to capture a state-based representation with a global time. The primary application is Failure Modes and Effects Analysis (FMEA) and a form of exaggeration reasoning is used, combined with an order of magnitude representation to derive the worst case failure modes. The novel aspects of the work are an order of magnitude(OM) qualitative network analyser to represent any power domain and topology, including multiple power sources, a feature that was not required for earlier specialised electrical versions of the approach. Secondly, the representation of generalised energy related behaviour as state-based local models is presented as a modelling strategy that can be more vivid and intuitive for a range of topologically complex applications than qualitative equation-based representations.The two-level modelling strategy allows the broad system behaviour coverage of qualitative simulation to be exploited for the FMEA task, while limiting the difficulties of qualitative ambiguity explanation that can arise from abstracted numerical models. We have used the method to support an automated FMEA system with examples of an aircraft fuel system and domestic a heating system discussed in this paper. 	
1403.5623v2	http://arxiv.org/pdf/1403.5623v2	2014	Systemic risk in dynamical networks with stochastic failure criterion	B. Podobnik|D. Horvatic|M. Bertella|L. Feng|X. Huang|B. Li	  Complex non-linear interactions between banks and assets we model by two time-dependent Erd\H{o}s Renyi network models where each node, representing bank, can invest either to a single asset (model I) or multiple assets (model II). We use dynamical network approach to evaluate the collective financial failure---systemic risk---quantified by the fraction of active nodes. The systemic risk can be calculated over any future time period, divided on sub-periods, where within each sub-period banks may contiguously fail due to links to either (i) assets or (ii) other banks, controlled by two parameters, probability of internal failure $p$ and threshold $T_h$ ("solvency" parameter). The systemic risk non-linearly increases with $p$ and decreases with average network degree faster when all assets are equally distributed across banks than if assets are randomly distributed. The more inactive banks each bank can sustain (smaller $T_h$), the smaller the systemic risk---for some $T_h$ values in I we report a discontinuity in systemic risk. When contiguous spreading becomes stochastic (ii) controlled by probability $p_2$---a condition for the bank to be solvent (active) is stochastic---the systemic risk decreases with decreasing $p_2$. We analyse asset allocation for the U.S. banks. 	
1407.0637v2	http://arxiv.org/pdf/1407.0637v2	2016	Fault-Tolerant Approximate Shortest-Path Trees	Davide Bilò|Luciano Gualà|Stefano Leucci|Guido Proietti	  The resiliency of a network is its ability to remain \emph{effectively} functioning also when any of its nodes or links fails. However, to reduce operational and set-up costs, a network should be small in size, and this conflicts with the requirement of being resilient. In this paper we address this trade-off for the prominent case of the {\em broadcasting} routing scheme, and we build efficient (i.e., sparse and fast) \emph{fault-tolerant approximate shortest-path trees}, for both the edge and vertex \emph{single-failure} case. In particular, for an $n$-vertex non-negatively weighted graph, and for any constant $\varepsilon >0$, we design two structures of size $O(\frac{n \log n}{\varepsilon^2})$ which guarantee $(1+\varepsilon)$-stretched paths from the selected source also in the presence of an edge/vertex failure. This favorably compares with the currently best known solutions, which are for the edge-failure case of size $O(n)$ and stretch factor 3, and for the vertex-failure case of size $O(n \log n)$ and stretch factor 3. Moreover, we also focus on the unweighted case, and we prove that an ordinary $(\alpha,\beta)$-spanner can be slightly augmented in order to build efficient fault-tolerant approximate \emph{breadth-first-search trees}. 	
1407.3518v1	http://arxiv.org/pdf/1407.3518v1	2014	Robust Network Routing under Cascading Failures	Ketan Savla|Giacomo Como|Munther A. Dahleh	  We propose a dynamical model for cascading failures in single-commodity network flows. In the proposed model, the network state consists of flows and activation status of the links. Network dynamics is determined by a, possibly state-dependent and adversarial, disturbance process that reduces flow capacity on the links, and routing policies at the nodes that have access to the network state, but are oblivious to the presence of disturbance. Under the proposed dynamics, a link becomes irreversibly inactive either due to overload condition on itself or on all of its immediate downstream links. The coupling between link activation and flow dynamics implies that links to become inactive successively are not necessarily adjacent to each other, and hence the pattern of cascading failure under our model is qualitatively different than standard cascade models. The magnitude of a disturbance process is defined as the sum of cumulative capacity reductions across time and links of the network, and the margin of resilience of the network is defined as the infimum over the magnitude of all disturbance processes under which the links at the origin node become inactive. We propose an algorithm to compute an upper bound on the margin of resilience for the setting where the routing policy only has access to information about the local state of the network. For the limiting case when the routing policies update their action as fast as network dynamics, we identify sufficient conditions on network parameters under which the upper bound is tight under an appropriate routing policy. Our analysis relies on making connections between network parameters and monotonicity in network state evolution under proposed dynamics. 	
1408.5780v2	http://arxiv.org/pdf/1408.5780v2	2016	Fractional repetition codes with flexible repair from combinatorial   designs	Oktay Olmez|Aditya Ramamoorthy	  Fractional repetition (FR) codes are a class of regenerating codes for distributed storage systems with an exact (table-based) repair process that is also uncoded, i.e., upon failure, a node is regenerated by simply downloading packets from the surviving nodes. In our work, we present constructions of FR codes based on Steiner systems and resolvable combinatorial designs such as affine geometries, Hadamard designs and mutually orthogonal Latin squares. The failure resilience of our codes can be varied in a simple manner. We construct codes with normalized repair bandwidth ($\beta$) strictly larger than one; these cannot be obtained trivially from codes with $\beta = 1$. Furthermore, we present the Kronecker product technique for generating new codes from existing ones and elaborate on their properties. FR codes with locality are those where the repair degree is smaller than the number of nodes contacted for reconstructing the stored file. For these codes we establish a tradeoff between the local repair property and failure resilience and construct codes that meet this tradeoff. Much of prior work only provided lower bounds on the FR code rate. In our work, for most of our constructions we determine the code rate for certain parameter ranges. 	
1409.5510v3	http://arxiv.org/pdf/1409.5510v3	2015	Avoiding catastrophic failure in correlated networks of networks	Saulo D. S. Reis|Yanqing Hu|Andrés Babino|José S. Andrade Jr.|Santiago Canals|Mariano Sigman|Hernán A. Makse	  Networks in nature do not act in isolation but instead exchange information, and depend on each other to function properly. An incipient theory of Networks of Networks have shown that connected random networks may very easily result in abrupt failures. This theoretical finding bares an intrinsic paradox: If natural systems organize in interconnected networks, how can they be so stable? Here we provide a solution to this conundrum, showing that the stability of a system of networks relies on the relation between the internal structure of a network and its pattern of connections to other networks. Specifically, we demonstrate that if network inter-connections are provided by hubs of the network and if there is a moderate degree of convergence of inter-network connection the systems of network are stable and robust to failure. We test this theoretical prediction in two independent experiments of functional brain networks (in task- and resting states) which show that brain networks are connected with a topology that maximizes stability according to the theory. 	
1501.04938v1	http://arxiv.org/pdf/1501.04938v1	2015	Functional safety: matching the complexity of methods with the   complexity of systems	F Brissaud|B Declerck	  In line with the IEC 61508 functional safety standard, it is required to assess the safety integrity of a system due to random hardware failures. For a rarely used function (operating in a low demand mode), the measurement used is average probability of a dangerous failure on demand (PFDavg). In this paper, four methods have been applied to different configurations of a case study: failure tree analysis with the software GRIF/Tree, multi-phase Markov graphs with the software GRIF/Markov, stochastic Petri nets with predicates with the software GRIF/Petri, and approximate equations (developed by DNV and different from those given in the IEC 61508 standard) using the software OrbitSIL. It is shown that all these methods can lead to similar results for the estimating of the PFDavg, taking into account the required characteristics of the standard. The choice of method must be made without bias, based on an agreement between the modelling efforts, goals, and the system properties. To assist the analyst in this task, a discussion of the benefits and limitations of each of these methods is presented. 	
1502.02145v1	http://arxiv.org/pdf/1502.02145v1	2015	Porous LSCF/Dense 3YSZ Interface Fracture Toughness Measured by Single   Cantilever Beam Wedge Test	Xin Wang|Fei He|Zhangwei Chen|Alan Atkinson	  Sandwich specimens were prepared by firing a thin inter-layer of porous La0.6Sr0.4Co0.2Fe0.8O3-d (LSCF) to bond a thin tetragonal yttria-stabilised zirconia (YSZ) beam to a thick YSZ substrate. Fracture of the joint was evaluated by introducing a wedge between the two YSZ adherands so that the stored energy in the thin YSZ cantilever beam drives a stable crack in the adhesive bond and allows the critical energy release rate for crack extension (fracture toughness) to be measured. The crack path in most specimens showed a mixture of adhesive failure (at the YSZ-LSCF interface) and cohesive failure (within the LSCF). It was found that the extent of adhesive fracture increased with firing temperature and decreased with LSCF layer thickness. The adhesive failures were mainly at the interface between the LSCF and the thin YSZ beam and FEM modelling revealed that this is due to asymmetric stresses in the LSCF. Within the firing temperature range of 1000-1150C, the bonding fracture toughness appears to have a strong dependence on firing temperature. However, the intrinsic adhesive fracture toughness of the LSCF/YSZ interface was estimated to be 11 Jm2 and was not firing temperature dependent within the temperature range investigated. 	
1503.00925v1	http://arxiv.org/pdf/1503.00925v1	2015	How Damage Diversification Can Reduce Systemic Risk	Rebekka Burkholz|Antonios Garas|Frank Schweitzer	  We consider the problem of risk diversification in complex networks. Nodes represent e.g. financial actors, whereas weighted links represent e.g. financial obligations (credits/debts). Each node has a risk to fail because of losses resulting from defaulting neighbors, which may lead to large failure cascades. Classical risk diversification strategies usually neglect network effects and therefore suggest that risk can be reduced if possible losses (i.e., exposures) are split among many neighbors (exposure diversification, ED). But from a complex networks perspective diversification implies higher connectivity of the system as a whole which can also lead to increasing failure risk of a node. To cope with this, we propose a different strategy (damage diversification, DD), i.e. the diversification of losses that are imposed on neighboring nodes as opposed to losses incurred by the node itself. Here, we quantify the potential of DD to reduce systemic risk in comparison to ED. For this, we develop a branching process approximation that we generalize to weighted networks with (almost) arbitrary degree and weight distributions. This allows us to identify systemically relevant nodes in a network even if their directed weights differ strongly. On the macro level, we provide an analytical expression for the average cascade size, to quantify systemic risk. Furthermore, on the meso level we calculate failure probabilities of nodes conditional on their system relevance. 	
1505.06312v1	http://arxiv.org/pdf/1505.06312v1	2015	A network approach for power grid robustness against cascading failures	Xiangrong Wang|Yakup Koç|Robert E. Kooij|Piet Van Mieghem	  Cascading failures are one of the main reasons for blackouts in electrical power grids. Stable power supply requires a robust design of the power grid topology. Currently, the impact of the grid structure on the grid robustness is mainly assessed by purely topological metrics, that fail to capture the fundamental properties of the electrical power grids such as power flow allocation according to Kirchhoff's laws. This paper deploys the effective graph resistance as a metric to relate the topology of a grid to its robustness against cascading failures. Specifically, the effective graph resistance is deployed as a metric for network expansions (by means of transmission line additions) of an existing power grid. Four strategies based on network properties are investigated to optimize the effective graph resistance, accordingly to improve the robustness, of a given power grid at a low computational complexity. Experimental results suggest the existence of Braess's paradox in power grids: bringing an additional line into the system occasionally results in decrease of the grid robustness. This paper further investigates the impact of the topology on the Braess's paradox, and identifies specific sub-structures whose existence results in Braess's paradox. Careful assessment of the design and expansion choices of grid topologies incorporating the insights provided by this paper optimizes the robustness of a power grid, while avoiding the Braess's paradox in the system. 	
1506.03354v1	http://arxiv.org/pdf/1506.03354v1	2015	Internal Structure of Asteroids Having Surface Shedding due to   Rotational Instability	Masatoshi Hirabayashi|Diego Paul S'anchez|Daniel J. Scheeres	  Surface shedding of an asteroid is a failure mode where surface materials fly off due to strong centrifugal forces beyond the critical spin period, while the internal structure does not deform significantly. This paper proposes a possible structure of an asteroid interior that leads to such surface shedding due to rapid rotation rates. A rubble pile asteroid is modeled as a spheroid composed of a surface shell and a concentric internal core, the entire assembly called the test body. The test body is assumed to be uniformly rotating around a constant rotation axis. We also assume that while the bulk density and the friction angle are constant, the cohesion of the surface shell is different from that of the internal core. First, developing an analytical model based on limit analysis, we provide the upper and lower bounds for the actual surface shedding condition. Second, we use a Soft-Sphere Discrete Element Method (SSDEM) to study dynamical deformation of the test body due to a quasi-static spin-up. In this paper we show the consistency of both approaches. Additionally, the SSDEM simulations show that the initial failure always occurs locally and not globally. In addition, as the core becomes larger, the size of lofted components becomes smaller. These results imply that if there is a strong enough core in a progenitor body, surface shedding is the most likely failure mode. 	
1506.06394v4	http://arxiv.org/pdf/1506.06394v4	2017	Towards an Algebra for Cascade Effects	Elie M. Adam|Munther A. Dahleh|Asuman Ozdaglar	  We introduce a new class of (dynamical) systems that inherently capture cascading effects (viewed as consequential effects) and are naturally amenable to combinations. We develop an axiomatic general theory around those systems, and guide the endeavor towards an understanding of cascading failure. The theory evolves as an interplay of lattices and fixed points, and its results may be instantiated to commonly studied models of cascade effects.   We characterize the systems through their fixed points, and equip them with two operators. We uncover properties of the operators, and express global systems through combinations of local systems. We enhance the theory with a notion of failure, and understand the class of shocks inducing a system to failure. We develop a notion of mu-rank to capture the energy of a system, and understand the minimal amount of effort required to fail a system, termed resilience. We deduce a dual notion of fragility and show that the combination of systems sets a limit on the amount of fragility inherited. 	
1507.03562v1	http://arxiv.org/pdf/1507.03562v1	2015	Predicting Scheduling Failures in the Cloud	Mbarka Soualhia|Foutse Khomh|Sofiene Tahar	  Cloud Computing has emerged as a key technology to deliver and manage computing, platform, and software services over the Internet. Task scheduling algorithms play an important role in the efficiency of cloud computing services as they aim to reduce the turnaround time of tasks and improve resource utilization. Several task scheduling algorithms have been proposed in the literature for cloud computing systems, the majority relying on the computational complexity of tasks and the distribution of resources. However, several tasks scheduled following these algorithms still fail because of unforeseen changes in the cloud environments. In this paper, using tasks execution and resource utilization data extracted from the execution traces of real world applications at Google, we explore the possibility of predicting the scheduling outcome of a task using statistical models. If we can successfully predict tasks failures, we may be able to reduce the execution time of jobs by rescheduling failed tasks earlier (i.e., before their actual failing time). Our results show that statistical models can predict task failures with a precision up to 97.4%, and a recall up to 96.2%. We simulate the potential benefits of such predictions using the tool kit GloudSim and found that they can improve the number of finished tasks by up to 40%. We also perform a case study using the Hadoop framework of Amazon Elastic MapReduce (EMR) and the jobs of a gene expression correlations analysis study from breast cancer research. We find that when extending the scheduler of Hadoop with our predictive models, the percentage of failed jobs can be reduced by up to 45%, with an overhead of less than 5 minutes. 	
1508.02055v1	http://arxiv.org/pdf/1508.02055v1	2015	Scalable Reliability Modelling of RAID Storage Subsystems	Prasenjit Karmakar|K. Gopinath	  Reliability modelling of RAID storage systems with its various components such as RAID controllers, enclosures, expanders, interconnects and disks is important from a storage system designer's point of view. A model that can express all the failure characteristics of the whole RAID storage system can be used to evaluate design choices, perform cost reliability trade-offs and conduct sensitivity analyses. However, including such details makes the computational models of reliability quickly infeasible.   We present a CTMC reliability model for RAID storage systems that scales to much larger systems than heretofore reported and we try to model all the components as accurately as possible. We use several state-space reduction techniques at the user level, such as aggregating all in-series components and hierarchical decomposition, to reduce the size of our model. To automate computation of reliability, we use the PRISM model checker as a CTMC solver where appropriate. Our modelling techniques using PRISM are more practical (in both time and effort) compared to previously reported Monte-Carlo simulation techniques.   Our model for RAID storage systems (that includes, for example, disks, expanders, enclosures) uses Weibull distributions for disks and, where appropriate, correlated failure modes for disks, while we use exponential distributions with independent failure modes for all other components. To use the CTMC solver, we approximate the Weibull distribution for a disk using sum of exponentials and we confirm that this model gives results that are in reasonably good agreement with those from the sequential Monte Carlo simulation methods for RAID disk subsystems reported in literature earlier. Using a combination of scalable techniques, we are able to model and compute reliability for fairly large configurations with upto 600 disks using this model. 	
1511.00797v3	http://arxiv.org/pdf/1511.00797v3	2016	Is It Possible to Simultaneously Achieve Zero Handover Failure Rate and   Ping-Pong Rate?	Hyun-Seo Park|Yong-Seouk Choi|Tae-Joong Kim|Byung-Chul Kim|Jae-Yong Lee	  Network densification through the deployment of large number of small cells has been considered as the dominant driver for wireless evolution into 5G. However, it has increased the complexity of mobility management, and operators have been facing the technical challenges in handover (HO) parameter optimization. The trade-off between the HO failure (HOF) rate and the ping-pong (PP) rate has further complicated the challenges. In this article, we proposed ZEro handover failure with Unforced and automatic time-to-execute Scaling (ZEUS) HO. ZEUS HO assures HO signaling when a user equipment (UE) is in a good radio link condition and executes the HO at an optimal time. We analyzed the HO performance of Long-Term Evolution (LTE) and ZEUS theoretically using a geometry-based model, considering the most important HO parameter, i.e., HO margin (HOM). We derived the probabilities of HOF and PP from the analysis. The numerical results demonstrated that ZEUS HO can achieve zero HOF rate without increasing the PP rate, solving the trade-off. Furthermore, we showed that the ZEUS HO can accomplish zero HOF rate and zero PP rate simultaneously with an extension of keeping fast moving users out of small cells. 	
1511.08232v1	http://arxiv.org/pdf/1511.08232v1	2015	Beyond One Third Byzantine Failures	Cheng Wang|Carole Delporte-Gallet|Hugues Fauconnier|Rachid Guerraoui|Anne-Marie Kermarrec	  The Byzantine agreement problem requires a set of $n$ processes to agree on a value sent by a transmitter, despite a subset of $b$ processes behaving in an arbitrary, i.e. Byzantine, manner and sending corrupted messages to all processes in the system. It is well known that the problem has a solution in a (an eventually) synchronous message passing distributed system iff the number of processes in the Byzantine subset is less than one third of the total number of processes, i.e. iff $n > 3b+1$. The rest of the processes are expected to be correct: they should never deviate from the algorithm assigned to them and send corrupted messages. But what if they still do?   We show in this paper that it is possible to solve Byzantine agreement even if, beyond the $ b$ ($< n/3 $) Byzantine processes, some of the other processes also send corrupted messages, as long as they do not send them to all. More specifically, we generalize the classical Byzantine model and consider that Byzantine failures might be partial. In each communication step, some of the processes might send corrupted messages to a subset of the processes. This subset of processes - to which corrupted messages might be sent - could change over time. We compute the exact number of processes that can commit such faults, besides those that commit classical Byzantine failures, while still solving Byzantine agreement. We present a corresponding Byzantine agreement algorithm and prove its optimality by giving resilience and complexity bounds. 	
1512.02555v2	http://arxiv.org/pdf/1512.02555v2	2016	Recovery of Interdependent Networks	M. A. Di Muro|C. E. La Rocca|H. E. Stanley|S. Havlin|L. A. Braunstein	  Recent network research has focused on the cascading failures in a system of interdependent networks and the necessary preconditions for system collapse. An important question that has not been addressed is how to repair a failing system before it suffers total breakdown. Here we introduce a recovery strategy of nodes and develop an analytic and numerical framework for studying the concurrent failure and recovery of a system of interdependent networks based on an efficient and practically reasonable strategy. Our strategy consists of repairing a fraction of failed nodes, with probability of recovery $\gamma$, that are neighbors of the largest connected component of each constituent network. We find that, for a given initial failure of a fraction $1-p$ of nodes, there is a critical probability of recovery above which the cascade is halted and the system fully restores to its initial state and below which the system abruptly collapses. As a consequence we find in the plane $\gamma-p$ of the phase diagram three distinct phases. A phase in which the system never collapses without being restored, another phase in which the recovery strategy avoids the breakdown, and a phase in which even the repairing process cannot avoid the system collapse. 	
1601.06344v2	http://arxiv.org/pdf/1601.06344v2	2016	Interval Estimation for Conditional Failure Rates of Transmission Lines   with Limited Samples	Ming Yang|Jianhui Wang|Haoran Diao|Junjian Qi|Xueshan Han	  The estimation of the conditional failure rate (CFR) of an overhead transmission line (OTL) is essential for power system operational reliability assessment. It is hard to predict the CFR precisely, although great efforts have been made to improve the estimation accuracy. One significant difficulty is the lack of available outage samples, due to which the law of large numbers is no longer applicable and no convincing statistical result can be obtained. To address this problem, in this paper a novel imprecise probabilistic approach is proposed to estimate the CFR of an OTL. The imprecise Dirichlet model (IDM) is applied to establish the imprecise probabilistic relation between a single conditional variable and the failure rate of an OTL. Then a credal network is constructed to integrate the IDM estimation results corresponding to different conditional variables and infer the CFR. Instead of providing a single-valued estimation result, the proposed approach predicts the possible interval of the CFR in order to explicitly indicate the uncertainty of the estimation and more objectively represent the available knowledge. The proposed approach is illustrated by estimating the CFRs of two LGJ-300 transmission lines located in the same region; the test results validate its effectiveness. 	
1602.07763v1	http://arxiv.org/pdf/1602.07763v1	2016	Optimizing the robustness of electrical power systems against cascading   failures	Yingrui Zhang|Osman Yagan	  Electrical power systems are one of the most important infrastructures that support our society. However, their vulnerabilities have raised great concern recently due to several large-scale blackouts around the world. In this paper, we investigate the robustness of power systems against cascading failures initiated by a random attack. This is done under a simple yet useful model based on global and equal redistribution of load upon failures. We provide a complete understanding of system robustness by i) deriving an expression for the final system size as a function of the size of initial attacks; ii) deriving the critical attack size after which system breaks down completely; iii) showing that complete system breakdown takes place through a first-order (i.e., discontinuous) transition in terms of the attack size; and iv) establishing the optimal load-capacity distribution that maximizes robustness. In particular, we show that robustness is maximized when the difference between the capacity and initial load is the same for all lines; i.e., when all lines have the same redundant space regardless of their initial load. This is in contrast with the intuitive and commonly used setting where capacity of a line is a fixed factor of its initial load. 	
1603.07775v2	http://arxiv.org/pdf/1603.07775v2	2016	The Impact of Operators' Performance in the Reliability of   Cyber-Physical Power Distribution Systems	Michel Bessani|Rodrigo Z. Fanucchi|Alexandre C. C. Delbem|Carlos D. Maciel	  Cyber-Physical Systems are the result of integrating information and communication technologies into physical systems. One particular case are Cyber-Physical Power Systems (CPPS), which use communication technologies to perform real-time monitoring and operation. These kinds of systems have become more complex, impacting on the systems' characteristics, such as their reliability. In addition, it is already known that in terms of the reliability of Cyber-Physical Power Distribution Systems (CPPDS), the failures of the communication network are just as relevant as the electrical network failures. However, some of the operators' performances, such as response time and decision quality, during CPPDS contingencies have not been investigated yet. In this paper, we introduce a model to the operator response time, present a Sequential Monte Carlo Simulation methodology that incorporates the response time in CPPDS reliability indices estimation, and evaluate the impact of the operator response time in reliability indices. Our method is tested on a CPPDS using different values for the average response time of operators. The results show that the response time of the operators affects the reliability indices that are related to the durations of the failure, indicating that a fast decision directly contributes to the system performance. We conclude that the improvement of CPPDS reliability is not only dependent on the electric and communication components, but also dependent on operators' performance. 	
1606.05598v2	http://arxiv.org/pdf/1606.05598v2	2016	Identifiability and testability in GRT with Individual Differences	Noah H. Silbert|Robin D. Thomas	  Silbert and Thomas (2013) showed that failures of decisional separability are not, in general, identifiable in fully parameterized $2 \times 2$ Gaussian GRT models. A recent extension of $2 \times 2$ GRT models (GRTwIND) was developed to solve this problem and a conceptually similar problem with the simultaneous identifiability of means and marginal variances in GRT models. Central to the ability of GRTwIND to solve these problems is the assumption of universal perception, which consists of shared perceptual distributions modified by attentional and global scaling parameters (Soto et al., 2015). If universal perception is valid, GRTwIND solves both issues. In this paper, we show that GRTwIND with universal perception and subject-specific failures of decisional separability is mathematically, and thereby empirically, equivalent to a model with decisional separability and failure of universal perception. We then provide a formal proof of the fact that means and marginal variances are not, in general, simultaneously identifiable in $2 \times 2$ GRT models, including GRTwIND. These results can be taken to delineate precisely what the assumption of universal perception must consist of. Based on these results and related recent mathematical developments in the GRT framework, we propose that, in addition to requiring a fixed subset of parameters to determine the location and scale of any given GRT model, some subset of parameters must be set in GRT models to fix the orthogonality of the modeled perceptual dimensions, a central conceptual underpinning of the GRT framework. We conclude with a discussion of perceptual primacy and its relationship to universal perception. 	
1606.07310v2	http://arxiv.org/pdf/1606.07310v2	2016	Fault-Tolerant Adaptive Parallel and Distributed Simulation	Gabriele D'Angelo|Stefano Ferretti|Moreno Marzolla|Lorenzo Armaroli	  Discrete Event Simulation is a widely used technique that is used to model and analyze complex systems in many fields of science and engineering. The increasingly large size of simulation models poses a serious computational challenge, since the time needed to run a simulation can be prohibitively large. For this reason, Parallel and Distributes Simulation techniques have been proposed to take advantage of multiple execution units which are found in multicore processors, cluster of workstations or HPC systems. The current generation of HPC systems includes hundreds of thousands of computing nodes and a vast amount of ancillary components. Despite improvements in manufacturing processes, failures of some components are frequent, and the situation will get worse as larger systems are built. In this paper we describe FT-GAIA, a software-based fault-tolerant extension of the GAIA/ART\`IS parallel simulation middleware. FT-GAIA transparently replicates simulation entities and distributes them on multiple execution nodes. This allows the simulation to tolerate crash-failures of computing nodes; furthermore, FT-GAIA offers some protection against byzantine failures since synchronization messages are replicated as well, so that the receiving entity can identify and discard corrupted messages. We provide an experimental evaluation of FT-GAIA on a running prototype. Results show that a high degree of fault tolerance can be achieved, at the cost of a moderate increase in the computational load of the execution units. 	
1606.08904v1	http://arxiv.org/pdf/1606.08904v1	2016	Robust Multi-Agent Optimization: Coping with Packet-Dropping Link   Failures	Lili Su|Nitin H. Vaidya	  We study the problem of multi-agent optimization in the presence of communication failures, where agents are connected by a strongly connected communication network. Specifically, we are interested in optimizing $h(x)=\frac{1}{n}\sum_{i=1}^n h_i(x)$, where $\mathcal{V}=\{1, \ldots, n\}$ is the set of agents, and $h_i(\cdot)$ is agent $i$'s local cost function. We consider the scenario where the communication links may suffer packet-dropping failures (i.e., the sent messages are not guaranteed to be delivered in the same iteration), but each link is reliable at least once in every $B$ consecutive message transmissions. This bounded time reliability assumption is reasonable since it has been shown that with unbounded message delays, convergence is not guaranteed for reaching consensus -- a special case of the optimization problem of interest. We propose a robust distributed optimization algorithm wherein each agent updates its local estimate using slightly different routines in odd and even iterations. We show that these local estimates converge to a common optimum of $h(\cdot)$ sub-linearly at convergence rate $O(\frac{1}{\sqrt{t}})$, where $t$ is the number of iteration. Our proposed algorithm combines the Push-Sum Distributed Dual Averaging method with a robust average consensus algorithm. The main analysis challenges come from the fact that the effective communication network is time varying, and that each agent does not know the actual number of reliable outgoing links at each iteration. 	
1607.03562v1	http://arxiv.org/pdf/1607.03562v1	2016	Robustness analysis of bimodal networks in the whole range of degree   correlation	Shogo Mizutaka|Toshihiro Tanizawa	  We present exact analysis of the physical properties of bimodal networks specified by the two peak degree distribution fully incorporating the degree-degree correlation between node connection. The structure of the correlated bimodal network is uniquely determined by the Pearson coefficient of the degree correlation, keeping its degree distribution fixed. The percolation threshold and the giant component fraction of the correlated bimodal network are analytically calculated in the whole range of the Pearson coefficient from $-1$ to $1$ against two major types of node removal, which are the random failure and the degree-based targeted attack. The Pearson coefficient for next-nearest-neighbor pairs is also calculated, which always takes a positive value even when the correlation between nearest-neighbor pairs is negative. From the results, it is confirmed that the percolation threshold is a monotonically decreasing function of the Pearson coefficient for the degrees of nearest-neighbor pairs increasing from $-1$ and $1$ regardless of the types of node removal. In contrast, the node fraction of the giant component for bimodal networks with positive degree correlation rapidly decreases in the early stage of random failure, while that for bimodal networks with negative degree correlation remains relatively large until the removed node fraction reaches the threshold. In this sense, bimodal networks with negative degree correlation are more robust against random failure than those with positive degree correlation. 	
1607.07558v4	http://arxiv.org/pdf/1607.07558v4	2017	SLAM-Safe Planner: Preventing Monocular SLAM Failure using Reinforcement   Learning	Vignesh Prasad|Saurabh Singh|Nahas Pareekutty|Balaraman Ravindran|Madhava Krishna	  Effective SLAM using a single monocular camera is highly preferred due to its simplicity. However, when compared to trajectory planning methods using depth-based SLAM, Monocular SLAM in loop does need additional considerations. One main reason being that for the optimization, in the form of Bundle Adjustment (BA), to be robust, the SLAM system needs to scan the area for a reasonable duration. Most monocular SLAM systems do not tolerate large camera rotations between successive views and tend to breakdown. Other reasons for Monocular SLAM failure include ambiguities in decomposition of the Essential Matrix, feature-sparse scenes and more layers of non linear optimization apart from BA. This paper presents a novel formulation based on Reinforcement Learning (RL) that generates fail safe trajectories wherein the SLAM generated outputs (scene structure and camera motion) do not deviate largely from their true values. Quintessentially, the RL framework successfully learns the otherwise complex relation between motor actions and perceptual inputs that result in trajectories that do not cause failure of SLAM, which are almost intractable to capture in an obvious mathematical formulation. We show systematically in simulations how the quality of the SLAM map and trajectory dramatically improves when trajectories are computed by using RL. 	
1607.08882v1	http://arxiv.org/pdf/1607.08882v1	2016	The competing risks Cox model with and without auxiliary case covariates   under weaker or no missing-at-random cause of failure	Daniel Nevo|Reiko Nishihara|Shuji Ogino|Molin Wang	  In the analysis of time-to-event data with multiple causes using a competing risks Cox model, often the cause of failure is unknown for some of the cases. The probability of a missing cause is typically assumed to be independent of the cause given the time of the event and covariates measured before the event occurred. In practice, however, the underlying missing-at-random assumption does not necessarily hold. Motivated by colorectal cancer subtype analysis, we develop semiparametric methods to conduct valid analysis, first when additional auxiliary variables are available for cases only. We consider a weaker missing-at-random assumption, with missing pattern depending on the observed quantities, which include the auxiliary covariates. Overlooking these covariates will potentially result in biased estimates. We use an informative likelihood approach that will yield consistent estimates even when the underlying model for missing cause of failure is misspecified. We then consider a method to conduct valid statistical analysis when there are no auxiliary covariates in the not missing-at-random scenario. The superiority of our methods in finite samples is demonstrated by simulation study results. We illustrate the use of our method in an analysis of colorectal cancer data from the Nurses' Health Study cohort, where, apparently, the traditional missing-at-random assumption fails to hold for particular molecular subtypes. 	
1607.08891v1	http://arxiv.org/pdf/1607.08891v1	2016	Assessing Functional Neural Connectivity as an Indicator of Cognitive   Performance	Brian S. Helfer|James R. Williamson|Benjamin A. Miller|Joseph Perricone|Thomas F. Quatieri	  Studies in recent years have demonstrated that neural organization and structure impact an individual's ability to perform a given task. Specifically, individuals with greater neural efficiency have been shown to outperform those with less organized functional structure. In this work, we compare the predictive ability of properties of neural connectivity on a working memory task. We provide two novel approaches for characterizing functional network connectivity from electroencephalography (EEG), and compare these features to the average power across frequency bands in EEG channels. Our first novel approach represents functional connectivity structure through the distribution of eigenvalues making up channel coherence matrices in multiple frequency bands. Our second approach creates a connectivity network at each frequency band, and assesses variability in average path lengths of connected components and degree across the network. Failures in digit and sentence recall on single trials are detected using a Gaussian classifier for each feature set, at each frequency band. The classifier results are then fused across frequency bands, with the resulting detection performance summarized using the area under the receiver operating characteristic curve (AUC) statistic. Fused AUC results of 0.63/0.58/0.61 for digit recall failure and 0.58/0.59/0.54 for sentence recall failure are obtained from the connectivity structure, graph variability, and channel power features respectively. 	
1608.07535v1	http://arxiv.org/pdf/1608.07535v1	2016	Complex Network Analysis of Brazilian Power Grid	Gabriela C. Martins|Leonardo S. Oliveira|Fabiano L. Ribeiro|Fabricio L. Forgerini	  Power Grids and other delivery networks has been attracted some attention by the network literature last decades. Despite the Power Grids dynamics has been controlled by computer systems and human operators, the static features of this type of network can be studied and analyzed. The topology of the Brazilian Power Grid (BPG) was studied in this work. We obtained the spatial structure of the BPG from the ONS (electric systems national operator), consisting of high-voltage transmission lines, generating stations and substations. The local low-voltage substations and local power delivery as well the dynamic features of the network were neglected. We analyze the complex network of the BPG and identify the main topological information, such as the mean degree, the degree distribution, the network size and the clustering coefficient to caracterize the complex network. We also detected the critical locations on the network and, therefore, the more susceptible points to lead to a cascading failure and even to a blackouts. Surprisely, due to the characteristic of the topology and physical structure of the network, we show that the BPG is resilient against random failures, since the random removal of links does not affect significantly the size of the largest cluster. We observe that when a fraction of the links are randomly removed, the network may disintegrates into smaller and disconnected parts, however, the power grid largest component remains connected. We believe that the even a static study of the network topology can help to identify the critical situations and also prevent failures and possible blackouts on the network. 	
1608.08660v1	http://arxiv.org/pdf/1608.08660v1	2016	Tunable QoS-Aware Network Survivability	Jose Yallouz|Ariel Orda	  Coping with network failures has been recognized as an issue of major importance in terms of social security, stability and prosperity. It has become clear that current networking standards fall short of coping with the complex challenge of surviving failures. The need to address this challenge has become a focal point of networking research. In particular, the concept of \textbf{\emph{tunable survivability}} offers major performance improvements over traditional approaches. Indeed, while the traditional approach aims at providing full (100\%) protection against network failures through disjoint paths, it was realized that this requirement is too restrictive in practice. Tunable survivability provides a quantitative measure for specifying the desired level (0\%-100\%) of survivability and offers flexibility in the choice of the routing paths. Previous work focused on the simpler class of "bottleneck" criteria, such as bandwidth. In this study, we focus on the important and much more complex class of \emph{additive} criteria, such as delay and cost. First, we establish some (in part, counter-intuitive) properties of the optimal solution. Then, we establish efficient algorithmic schemes for optimizing the level of survivability under additive end-to-end QoS bounds. Subsequently, through extensive simulations, we show that, at the price of \emph{negligible} reduction in the level of survivability, a major improvement (up to a factor of $2$) is obtained in terms of end-to-end QoS performance. Finally, we exploit the above findings in the context of a network design problem, in which, for a given investment budget, we aim to improve the survivability of the network links. 	
1610.02881v2	http://arxiv.org/pdf/1610.02881v2	2016	Essential Properties of Numerical Integration for Time-optimal   Trajectory Planning Along a Specified Path	Peiyao Shen|Xuebo Zhang|Yongchun Fang	  This letter summarizes some known properties and also presents several new properties of the Numerical Integration (NI) method for time-optimal trajectory planning along a specified path. The contribution is that rigorous mathematical proofs of these properties are presented, most of which cannot be found in existing literatures. We first give some properties regarding switch points and accelerating/decelerating curves of the NI method. Then, for the fact that when kinematic constraints are considered, the original version of NI which only considers torque constraints may result in failure of trajectory planning, we give the concrete failure conditions with rigorous mathematical proof. Accordingly, a failure detection algorithm is given in a run-and-test manner. Some simulation results on a unicycle vehicle are provided to verify those presented properties. Note that though those known properties are not discovered first, their mathematical proofs are given first in this letter. The detailed proofs make the theory of NI more complete and help interested readers to gain a thorough understanding of the method. 	
1611.10007v1	http://arxiv.org/pdf/1611.10007v1	2016	Structural Controllability of Multi-Agent Networks: Robustness against   Simultaneous Failures	M. Amin Rahimian|Amir G. Aghdam	  In this paper, structural controllability of a leader-follower multi-agent system with multiple leaders is studied from a graph-theoretic point of view. The problem of preservation of structural controllability under simultaneous failures in both the communication links and the agents is investigated. The effects of the loss of agents and communication links on the controllability of an information flow graph are previously studied. In this work, the corresponding results are exploited to introduce some useful indices and importance measures that help characterize and quantify the role of individual links and agents in the controllability of the overall network. Existing results are then extended by considering the effects of losses in both links and agents at the same time. To this end, the concepts of joint (r,s)-controllability and joint t-controllability are introduced as quantitative measures of reliability for a multi-agent system, and their important properties are investigated. Lastly, the class of jointly critical digraphs is introduced and it is stated that if a digraph is jointly critical, then joint t-controllability is a necessary and sufficient condition for remaining controllable following the failure of any set of links and agents, with cardinality less than t. Various examples are exploited throughout the paper to elaborate on the analytical findings. 	
1702.05741v2	http://arxiv.org/pdf/1702.05741v2	2017	Locally Repairable Codes with Multiple $(r_{i}, δ_{i})$-Localities	Bin Chen|Shu-Tao Xia|Jie Hao	  In distributed storage systems, locally repairable codes (LRCs) are introduced to realize low disk I/O and repair cost. In order to tolerate multiple node failures, the LRCs with \emph{$(r, \delta)$-locality} are further proposed. Since hot data is not uncommon in a distributed storage system, both Zeh \emph{et al.} and Kadhe \emph{et al.} focus on the LRCs with \emph{multiple localities or unequal localities} (ML-LRCs) recently, which said that the localities among the code symbols can be different. ML-LRCs are attractive and useful in reducing repair cost for hot data. In this paper, we generalize the ML-LRCs to the $(r,\delta)$-locality case of multiple node failures, and define an LRC with multiple $(r_{i}, \delta_{i})_{i\in [s]}$ localities ($s\ge 2$), where $r_{1}\leq r_{2}\leq\dots\leq r_{s}$ and $\delta_{1}\geq\delta_{2}\geq\dots\geq\delta_{s}\geq2$. Such codes ensure that some hot data could be repaired more quickly and have better failure-tolerance in certain cases because of relatively smaller $r_{i}$ and larger $\delta_{i}$. Then, we derive a Singleton-like upper bound on the minimum distance for the proposed LRCs by employing the regenerating-set technique. Finally, we obtain a class of explicit and structured constructions of optimal ML-LRCs, and further extend them to the cases of multiple $(r_{i}, \delta)_{i\in [s]}$ localities. 	
1704.00945v1	http://arxiv.org/pdf/1704.00945v1	2017	ASB1 differential methylation in ischaemic cardiomyopathy. Relationship   with left ventricular performance in end stage heart failure patients	Ana Ortega|Estefanía Tarazón|Carolina Gil-Cayuela|Luis Martínez-Dolz|Francisca Lago|José Ramón González-Juanatey|Juan Sandoval|Manuel Portolés|Esther Roselló-Lletí|Miguel Rivera	  Aims: Ischaemic cardiomyopathy (ICM) leads to impaired contraction and ventricular dysfunction causing high rates of morbidity and mortality. Epigenomics allows the identification of epigenetic signatures in human diseases. We analyse the differential epigenetic patterns of ASB gene family in ICM patients and relate these alterations to their haemodynamic and functional status. Methods and Results: Epigenomic analysis was carried out using 16 left ventricular (LV) tissue samples, 8 from ICM patients undergoing heart transplantation and 8 from control (CNT) subjects without cardiac disease. We increased the sample size up to 13 ICM and 10 CNT for RNA-sequencing and to 14 ICM for pyrosequencing analyses. We found a hypermethylated profile (cg11189868) in the ASB1 gene that showed a differential methylation of 0.26 beta difference, P < 0.05. This result was validated by pyrosequencing technique (0.23 beta difference, P < 0.05). Notably, the methylation pattern was strongly related to LV ejection fraction (r = -0.849, P = 0.008) stroke volume (r = -0.929, P = 0.001) and end-systolic and diastolic LV diameters (r = -0.743, P = 0.035 for both). ASB1 showed a down regulation in mRNA levels (-1.2 fold, P < 0.05). Conclusion: Our findings link a specific ASB1 methylation pattern to LV structure and performance in end-stage ICM, opening new therapeutic opportunities and providing new insights regarding which is the functionally relevant genome in the ischemic failing myocardium. Keywords: ischaemic cardiomyopathy; epigenomics; heart failure; left ventricular dysfunction; stroke volume; ASB1. 	
ischaemic cardiomyopathy, epigenomics, heart failure, left ventricular
dysfunction, stroke volume, asb1 

1705.10208v1	http://arxiv.org/pdf/1705.10208v1	2017	Dependency-Aware Rollback and Checkpoint-Restart for Distributed   Task-Based Runtimes	Kiril Dichev|Herbert Jordan|Konstantinos Tovletoglou|Thomas Heller|Dimitrios S. Nikolopoulos|Georgios Karakonstantis|Charles Gillan	  With the increase in compute nodes in large compute platforms, a proportional increase in node failures will follow. Many application-based checkpoint/restart (C/R) techniques have been proposed for MPI applications to target the reduced mean time between failures. However, rollback as part of the recovery remains a dominant cost even in highly optimised MPI applications employing C/R techniques. Continuing execution past a checkpoint (that is, reducing rollback) is possible in message-passing runtimes, but extremely complex to design and implement. Our work focuses on task-based runtimes, where task dependencies are explicit and message passing is implicit. We see an opportunity for reducing rollback for such runtimes: we explore task dependencies in the rollback, which we call dependency-aware rollback. We also design a new C/R technique, which is influenced by recursive decomposition of tasks, and combine it with dependency-aware rollback. We expect the dependency-aware rollback to cancel and recompute less tasks in the presence of node failures. We describe, implement and validate the proposed protocol in a simulator, which confirms these expectations. In addition, we consistently observe faster overall execution time for dependency-aware rollback in the presence of faults, despite the fact that reduced task cancellation does not guarantee reduced overall execution time. 	
1706.04898v1	http://arxiv.org/pdf/1706.04898v1	2017	A Novel Construction of Low-Complexity MDS Codes with Optimal Repair   Capability for Distributed Storage Systems	Sheng Guan|Haibin Kan|Xin Wang	  Maximum-distance-separable (MDS) codes are a class of erasure codes that are widely adopted to enhance the reliability of distributed storage systems (DSS). In (n, k) MDS coded DSS, the original data are stored into n distributed nodes in an efficient manner such that each storage node only contains a small amount (i.e., 1/k) of the data and a data collector connected to any k nodes can retrieve the entire data. On the other hand, a node failure can be repaired (i.e., stored data at the failed node can be successfully recovered) by downloading data segments from other surviving nodes. In this paper, we develop a new approach to construction of simple (5, 3) MDS codes. With judiciously block-designed generator matrices, we show that the proposed MDS codes have a minimum stripe size {\alpha} = 2 and can be constructed over a small (Galois) finite field F4 of only four elements, both facilitating low-complexity computations and implementations for data storage, retrieval and repair. In addition, with the proposed MDS codes, any single node failure can be repaired through interference alignment technique with a minimum data amount downloaded from the surviving nodes; i.e., the proposed codes ensure optimal exact-repair of any single node failure using the minimum bandwidth. The low-complexity and all-node-optimal-repair properties of the proposed MDS codes make them readily deployed for practical DSS. 	
1706.06611v1	http://arxiv.org/pdf/1706.06611v1	2017	Individualized Treatment Effects with Censored Data via Fully   Nonparametric Bayesian Accelerated Failure Time Models	Nicholas C. Henderson|Thomas A. Louis|Gary L. Rosner|Ravi Varadhan	  Individuals often respond differently to identical treatments, and characterizing such variability in treatment response is an important aim in the practice of personalized medicine. In this article, we describe a non-parametric accelerated failure time model that can be used to analyze heterogeneous treatment effects (HTE) when patient outcomes are time-to-event. By utilizing Bayesian additive regression trees and a mean-constrained Dirichlet process mixture model, our approach offers a flexible model for the regression function while placing few restrictions on the baseline hazard. Our non-parametric method leads to natural estimates of individual treatment effect and has the flexibility to address many major goals of HTE assessment. Moreover, our method requires little user input in terms of tuning parameter selection or subgroup specification. We illustrate the merits of our proposed approach with a detailed analysis of two large clinical trials for the prevention and treatment of congestive heart failure using an angiotensin-converting enzyme inhibitor. The analysis revealed considerable evidence for the presence of HTE in both trials as demonstrated by substantial estimated variation in treatment effect and by high proportions of patients exhibiting strong evidence of having treatment effects which differ from the overall treatment effect. 	
1706.08209v1	http://arxiv.org/pdf/1706.08209v1	2017	A sequential surrogate method for reliability analysis based on radial   basis function	Xu Li|Chunlin Gong|Liangxian Gu|Wenkun Gao|Zhao Jing|Hua Su	  A radial basis function (RBF) based sequential surrogate reliability method (SSRM) is proposed, in which a special optimization problem is solved to update the surrogate model of the limit state function (LSF) iteratively. The objective of the optimization problem is to find a new point to maximize the probability density function (PDF), subject to the constraints that the new point is on the approximated LSF and the minimum distance to the existing points is greater than or equal to the given distance. By updating the surrogate model with the new points, the surrogate model of the LSF becomes more and more accurate in the important region with a high failure probability and on the LSF boundary. Moreover, the accuracy of the unimportant region is also improved within the iteration due to the minimum distance constraint. SSRM takes advantage of the information of PDF and LSF to capture the failure features, which decreases the number of the expensive LSF evaluations. Six numerical examples show that SSRM improves the accuracy of the surrogate model in the important region around the failure boundary with small number of samples and has better adaptability to the nonlinear LSF, hence increases the accuracy and efficiency of the reliability analysis. 	
1707.02309v1	http://arxiv.org/pdf/1707.02309v1	2017	Adaptive Correlation Filters with Long-Term and Short-Term Memory for   Object Tracking	Chao Ma|Jia-Bin Huang|Xiaokang Yang|Ming-Hsuan Yang	  Object tracking is challenging as target objects often undergo drastic appearance changes over time. Recently, adaptive correlation filters have been successfully applied to object tracking. However, tracking algorithms relying on highly adaptive correlation filters are prone to drift due to noisy updates. Moreover, as these algorithms do not maintain long-term memory of target appearance, they cannot recover from tracking failures caused by heavy occlusion or target disappearance in the camera view. In this paper, we propose to learn multiple adaptive correlation filters with both long-term and short-term memory of target appearance for robust object tracking. First, we learn a kernelized correlation filter with an aggressive learning rate for locating target objects precisely. We take into account the appropriate size of surrounding context and the feature representations. Second, we learn a correlation filter over a feature pyramid centered at the estimated target position for predicting scale changes. Third, we learn a complementary correlation filter with a conservative learning rate to maintain long-term memory of target appearance. We use the output responses of this long-term filter to determine if tracking failure occurs. In the case of tracking failures, we apply an incrementally learned detector to recover the target position in a sliding window fashion. Extensive experimental results on large-scale benchmark datasets demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods in terms of efficiency, accuracy, and robustness. 	
1707.03661v1	http://arxiv.org/pdf/1707.03661v1	2017	Instability in dynamic fracture and the failure of the classical theory   of cracks	Chih-Hung Chen|Eran Bouchbinder|Alain Karma	  Cracks, the major vehicle for material failure, tend to accelerate to high velocities in brittle materials. In three-dimensions, cracks generically undergo a micro-branching instability at about 40% of their sonic limiting velocity. Recent experiments showed that in sufficiently thin systems cracks unprecedentedly accelerate to nearly their limiting velocity without micro-branching, until they undergo an oscillatory instability. Despite their fundamental importance and apparent similarities to other instabilities in condensed-matter physics and materials science, these dynamic fracture instabilities remain poorly understood. They are not described by the classical theory of cracks, which assumes that linear elasticity is valid inside a stressed material and uses an extraneous local symmetry criterion to predict crack paths. Here we develop a model of two-dimensional dynamic brittle fracture capable of predicting arbitrary paths of ultra-high-speed cracks in the presence of elastic nonlinearity without extraneous criteria. We show, by extensive computations, that cracks undergo a dynamic oscillatory instability controlled by small-scale elastic nonlinearity near the crack tip. This instability occurs above a ultra-high critical velocity and features an intrinsic wavelength that increases proportionally to the ratio of the fracture energy to an elastic modulus, in quantitative agreement with experiments. This ratio emerges as a fundamental scaling length assumed to play no role in the classical theory of cracks, but shown here to strongly influence crack dynamics. Those results pave the way for resolving other long-standing puzzles in the failure of materials. 	
1709.03813v1	http://arxiv.org/pdf/1709.03813v1	2017	Dithymoquinone as a novel inhibitor for   3-carboxy-4-methyl-5-propyl-2-furanpropanoic acid (CMPF) to prevent renal   failure	Muniba Faiza|Tariq Abdullah|Prof. Yonghua Wang	  3-carboxy-4-methyl-5-propyl-2-furanpropanoic acid (CMPF) is a major endogenous ligand found in the human serum albumin (HSA) of renal failure patients. It gets accumulated in the HSA and its concentration in sera of patients may reflect the chronicity of renal failure [1-4]. It is considered uremic toxin due to its damaging effect on the renal cells. The high concentrations of CMPF inhibit the binding of other ligands to HSA. Removal of CMPF is difficult through conventional hemodialysis due to its strong binding affinity. We hypothesized that the competitive inhibition may be helpful in removal of CMPF binding to HSA. A compound with higher HSA binding affinity than CMPF could be useful to prevent CMPF from binding so that CMPF could be excreted by the body through the urine. We studied an active compound dihydrothymoquinone/ dithymoquinone (DTQ) found in black cumin seed (Nigella sativa), which has higher binding affinity for HSA. Molecular docking simulations were performed to find the binding affinity of CMPF and DTQ with HSA. DTQ was found to have higher binding affinity possessing more interactions with the binding residues than the CMPF. We studied the binding pocket flexibility of CMPF and DTQ to analyze the binding abilities of both the compounds. We have also predicted the ADME properties for DTQ which shows higher lipophilicity, higher gastrointestinal (GI) absorption, and blood-brain barrier (BBB) permeability. We discovered that DTQ has potential to act as an inhibitor of CMPF and can be considered as a candidate for the formation of the therapeutic drug against CMPF. 	
1710.04449v1	http://arxiv.org/pdf/1710.04449v1	2017	A critical comparison of methods for the determination of the ageing   sensitivity in biomedical grade yttria stabilized zirconia	Sylvain Deville|Laurent Gremillard|Jérôme Chevalier|Gilbert Fantozzi	  Since the recent failure events of two particular series of zirconia femoral heads for total hip replacement prosthesis, a large decrease in the use of zirconia ceramics for orthopaedic implants has been observed. In spite of the biomedical success of this material during the last ten years, this was required for safety reasons, until the cause of the failures is known. It has been shown that these failures were related to the low temperature hydrothermal degradation (also known as ageing). Thus it is crucial to better understand the ageing behaviour, in order to be able to assess its importance and then control it if required. In this paper, various techniques relevant to assess the hydrothermal degradation sensitivity of biomedical grade yttria stabilized zirconia are discussed and compared. The expected outputs of conventional methods, i.e. X-Ray diffraction and scanning electron microscopy are examined. More recent methods like optical interferometry and atomic force microscopy are presented, with their respective benefits and drawbacks. An up to date comparison of these different techniques is provided, and their use for ensuring the long term reliability of a particular batch of zirconia in terms of ageing degradation is demonstrated. 	
1711.07510v1	http://arxiv.org/pdf/1711.07510v1	2017	Robust Environmental Mapping by Mobile Sensor Networks	Hyongju Park|Jinsun Liu|Matthew Johnson-Roberson|Ram Vasudevan	  Constructing a spatial map of environmental parameters is a crucial step to preventing hazardous chemical leakages, forest fires, or while estimating a spatially distributed physical quantities such as terrain elevation. Although prior methods can do such mapping tasks efficiently via dispatching a group of autonomous agents, they are unable to ensure satisfactory convergence to the underlying ground truth distribution in decentralized manner when any of the agents fail. Since the types of agents utilized to perform such mapping are typically inexpensive and prone to failure, this typically results in poor overall mapping performance in real-world applications, which can in certain cases endanger human safety. To address this limitation of existing techniques, this paper presents a Bayesian approach for robust spatial mapping of environmental parameters by deploying a group of mobile robots capable of ad-hoc communication equipped with short-range sensors in the presence of hardware failures. Our approach first utilizes a variant of the Voronoi diagram to partition the region to be mapped into disjoint regions that are each associated with at least one robot. These robots are then deployed in a decentralized manner to maximize the likelihood that at least one robot detects every target in their associated region despite a non-zero probability of failure. A suite of simulation results is presented to demonstrate the effectiveness and robustness of the proposed method when compared to existing techniques. 	
1711.09400v1	http://arxiv.org/pdf/1711.09400v1	2017	A Multi Objective Reliable Location-Inventory Capacitated Disruption   Facility Problem with Penalty Cost Solve with Efficient Meta Historic   Algorithms	Elham Taghizadeh|Mostafa Abedzadeh|Mostafa Setak	  Logistics network is expected that opened facilities work continuously for a long time horizon without any failure, but in real world problems, facilities may face disruptions. This paper studies a reliable joint inventory location problem to optimize the cost of facility locations, customers assignment, and inventory management decisions when facilities face failure risks and do not work. In our model we assume when a facility is out of work, its customers may be reassigned to other operational facilities otherwise they must endure high penalty costs associated with losing service. For defining the model closer to real world problems, the model is proposed based on pmedian problem and the facilities are considered to have limited capacities. We define a new binary variable for showing that customers are not assigned to any facilities. Our problem involves a biobjective model, the first one minimizes the sum of facility construction costs and expected inventory holding costs, the second one function that mentions for the first one is minimized maximum expected customer costs under normal and failure scenarios. For solving this model we use NSGAII and MOSS algorithms have been applied to find the Pareto archive solution. Also, Response Surface Methodology (RSM) is applied for optimizing the NSGAII Algorithm Parameters. We compare the performance of two algorithms with three metrics and the results show NSGAII is more suitable for our model. 	
1711.11022v1	http://arxiv.org/pdf/1711.11022v1	2017	A Novel Data-Driven Framework for Risk Characterization and Prediction   from Electronic Medical Records: A Case Study of Renal Failure	Prithwish Chakraborty|Vishrawas Gopalakrishnan|Sharon M. H. Alford|Faisal Farooq	  Electronic medical records (EMR) contain longitudinal information about patients that can be used to analyze outcomes. Typically, studies on EMR data have worked with established variables that have already been acknowledged to be associated with certain outcomes. However, EMR data may also contain hitherto unrecognized factors for risk association and prediction of outcomes for a disease. In this paper, we present a scalable data-driven framework to analyze EMR data corpus in a disease agnostic way that systematically uncovers important factors influencing outcomes in patients, as supported by data and without expert guidance. We validate the importance of such factors by using the framework to predict for the relevant outcomes. Specifically, we analyze EMR data covering approximately 47 million unique patients to characterize renal failure (RF) among type 2 diabetic (T2DM) patients. We propose a specialized L1 regularized Cox Proportional Hazards (CoxPH) survival model to identify the important factors from those available from patient encounter history. To validate the identified factors, we use a specialized generalized linear model (GLM) to predict the probability of renal failure for individual patients within a specified time window. Our experiments indicate that the factors identified via our data-driven method overlap with the patient characteristics recognized by experts. Our approach allows for scalable, repeatable and efficient utilization of data available in EMRs, confirms prior medical knowledge and can generate new hypothesis without expert supervision. 	
1712.03361v1	http://arxiv.org/pdf/1712.03361v1	2017	Inforence: Effective Fault Localization Based on Information-Theoretic   Analysis and Statistical Causal Inference	Farid Feyzi|Saeed Parsa	  In this paper, a novel approach, Inforence, is proposed to isolate the suspicious codes that likely contain faults. Inforence employs a feature selection method, based on mutual information, to identify those bug-related statements that may cause the program to fail. Because the majority of a program faults may be revealed as undesired joint effect of the program statements on each other and on program termination state, unlike the state-of-the-art methods, Inforence tries to identify and select groups of interdependent statements which altogether may affect the program failure. The interdependence amongst the statements is measured according to their mutual effect on each other and on the program termination state. To provide the context of failure, the selected bug-related statements are chained to each other, considering the program static structure. Eventually, the resultant cause-effect chains are ranked according to their combined causal effect on program failure. To validate Inforence, the results of our experiments with seven sets of programs include Siemens suite, gzip, grep, sed, space, make and bash are presented. The experimental results are then compared with those provided by different fault localization techniques for the both single-fault and multi-fault programs. The experimental results prove the outperformance of the proposed method compared to the state-of-the-art techniques. 	
1712.07697v1	http://arxiv.org/pdf/1712.07697v1	2017	Renaissance: Self-Stabilizing Distributed SDN Control Plane	Marco Canini|Iosif Salem|Liron Schiff|Elad Michael Schiller|Stefan Schmid	  By introducing programmability, automated verification, and innovative debugging tools, Software-Defined Networks (SDNs) are poised to meet the increasingly stringent dependability requirements of today's communication networks. However, the design of fault-tolerant SDNs remains an open challenge. This paper considers the design of dependable SDNs through the lenses of self-stabilization - a very strong notion of fault-tolerance. In particular, we develop algorithms for an in-band and distributed control plane for SDNs, called Renaissance, which tolerate a wide range of (concurrent) controller, link, and communication failures. Our self-stabilizing algorithms ensure that after the occurrence of an arbitrary combination of failures, (i) every non-faulty SDN controller can eventually reach any switch in the network within a bounded communication delay (in the presence of a bounded number of concurrent failures) and (ii) every switch is managed by at least one non-faulty controller. We evaluate Renaissance through a rigorous worst-case analysis as well as a prototype implementation (based on OVS and Floodlight), and we report on our experiments using Mininet. 	
9404023v1	http://arxiv.org/pdf/cond-mat/9404023v1	1994	Failure of Fermi Liquid Theory in 2-D: A Signal from Perturbation Theory	G. Baskaran	  We study the perturbative correction to the ground state energy eigenvalue of a 2-dimensional dilute fermi gas with weak short-range two body repulsion. From the structure of the energy shift we infer the presence of an induced two body long range repulsive interaction $ {\displaystyle {1\over{r^2}} }$ among the constituent electrons indicating a potential instability of the fermi liquid ground state. 	
9506082v1	http://arxiv.org/pdf/cond-mat/9506082v1	1995	Two phase transitions in the fully frustrated $XY$ model	Peter Olsson	  The fully frustrated $XY$ model on a square lattice is studied by means of Monte Carlo simulations. A Kosterlitz-Thouless transition is found at $T_{\rm KT} \approx 0.446$, followed by an ordinary Ising transition at a slightly higher temperature, $T_c \approx 0.452$. The non-Ising exponents reported by others, are explained as a failure of finite size scaling due to the screening length associated with the nearby Kosterlitz-Thouless transition. 	
0610230v1	http://arxiv.org/pdf/cond-mat/0610230v1	2006	On the occurrence of Kosterlitz-Thouless behavior in cuprate   superconductors	T. Schneider	  The observation of the characteristic Kosterlitz-Thouless behavior requires the attaintment of the two dimensional limit where the correlation-length anisotropy, gamma=gsiab/gsic, diverges. Our findings strongly suggest that the failure of several experiments on films and single crystals to observe any trace of KT-behavior is attributable either to inhomogeneities or doping by means of chemical substitution. 	
0202088v1	http://arxiv.org/pdf/gr-qc/0202088v1	2002	Comment on ``Singularity-free Cosmological Solutions with Non-rotating   Perfect Fluids''	L. Fernandez-Jambrina	  A theorem stated by Raychaudhuri which claims that the only physical non-singular cosmological models are comprised in the Ruiz-Senovilla family is shown to be incorrect. An explicit counterexample is provided and the failure of the argument leading to the theorem is explicitly pointed out. 	
0210046v1	http://arxiv.org/pdf/gr-qc/0210046v1	2002	Failure of a Stability Conjecture in General Relativity	O. Gurtug|M. Halilsoy	  By employing an exact back-reaction geometry, Helliwell-Konkowski stability conjecture is shown to fail. This happens when a test null dust is inserted to the interaction region of cross-polarized Bell-Szekeres spacetime. 	
0503035v1	http://arxiv.org/pdf/gr-qc/0503035v1	2005	Comment on Singularity-free Cosmological Solutions with Non-rotating   Perfect Fluids	L. Fernandez-Jambrina	  A conjecture stated by Raychaudhuri which claims that the only physical perfect fluid non-rotating non-singular cosmological models are comprised in the Ruiz-Senovilla and Fernandez-Jambrina families is shown to be incorrect. An explicit counterexample is provided and the failure of the argument leading to the result is explicitly pointed out. 	
9207004v1	http://arxiv.org/pdf/hep-lat/9207004v1	1992	Fermions in Models with Wilson--Yukawa Couplings	M. F. L. Golterman|D. N. Petcher	  Our work on models with Wilson--Yukawa couplings is reviewed. Conclusions include the failure of such models to produce continuum chiral gauge theories. 	
9512028v1	http://arxiv.org/pdf/hep-lat/9512028v1	1995	On the Definition of the Partition Function in Quantum Regge Calculus	Jun Nishimura	  We argue that the definition of the partition function used recently to demonstrate the failure of Regge calculus is wrong. In fact, in the one-dimensional case, we show that there is a more natural definition, with which one can reproduce the correct results. 	
9406334v1	http://arxiv.org/pdf/hep-ph/9406334v1	1994	B To Light Meson Form Factors	R. Aleksan|A. Le Yaouanc|L. Oliver|O. Pène|J. -C. Raynal	  The heavy to light form factors in $B$ decays are discussed. Critical discussion of theoretical approaches is made with special emphasis on their failure to describe the $B \to K^{(\ast )}\psi$ data. 	
0212027v2	http://arxiv.org/pdf/hep-ph/0212027v2	2002	T-odd quark distributions: QCD versus chiral models	P. V. Pobylitsa	  The T-odd quark distrbutions are shown to vanish in the chiral sigma model in contrast to the opposite widespread opinion. This failure of the chiral sigma model is a feature of the model itself and has nothing to do with the recent progress in the clarification of the status of the T-odd distributions in QCD. 	
0501223v2	http://arxiv.org/pdf/hep-ph/0501223v2	2005	Operator Analysis for Proton Decay in SUSY SO(10) GUT Models	S. Wiesenfeldt	  Non-renormalizable operators both account for the failure of down quark and charged lepton Yukawa couplings to unify and reduce the proton decay rate via dimension-five operators in minimal SUSY SU(5) GUT. We extend the analysis to SUSY SO(10) GUT models. 	
9807123v1	http://arxiv.org/pdf/hep-th/9807123v1	1998	Hawking radiation by effective two-dimensional theories	R. Balbinot|A. Fabbri	  Recently proposed 2D anomaly induced effective actions for the matter-gravity system are critically reviewed. Their failure to correctly reproduce Hawking's black hole radiation or the stability of Minkowski space-time led us to a modification of the relevant ``quantum'' matter stress energy tensor that allows physically meaningful results to be extracted. 	
9207205v1	http://arxiv.org/pdf/math/9207205v1	1992	Remark on the Failure of Martin's Axiom	Avner Landver	  Let m be the least cardinal k such that MA(k) fails. The only known model for "m is singular" was constructed by Kunen. In Kunen's model cof(m)=omega_1. It is unknown whether "omega_1 < cof(m) < m" is consistent. The purpose of this paper is to present a proof of Kunen's result and to identify the difficulties of generalizing this result to an arbitrary uncountable cofinality. 	
0112286v1	http://arxiv.org/pdf/math/0112286v1	2001	Forcing axiom failure for any lambda>aleph_1	Saharon Shelah	  David Aspero asks on the possibility of having Forcing axiom FA_{aleph_2}(K), where K is the class of forcing notions preserving stationarity of subsets of aleph_1 and of aleph_2. We answer negatively, in fact we show the negative result for any regular lambda>aleph_1 even demanding adding no new sequence of ordinals of length<lambda. 	
0304201v1	http://arxiv.org/pdf/math/0304201v1	2003	Semiclassical analysis of a nonlinear eigenvalue problem and non   analytic hypoellipticity	Bernard Helffer|Didier Robert|Xue Ping Wang	  We give a semiclassical analysis of a nonlinear eigenvalue problem arising from the study of the failure of analytic hypoellipticity and obtain a general family of hypoelliptic, but not analytic hypoelliptic operators. 	
0401016v1	http://arxiv.org/pdf/math/0401016v1	2004	Fundamental groupoids of k-graphs	David Pask|John Quigg|Iain Raeburn	  k-graphs are higher-rank analogues of directed graphs which were first developed to provide combinatorial models for operator algebras of Cuntz-Krieger type. Here we develop a theory of the fundamental groupoid of a k-graph, and relate it to the fundamental groupoid of an associated graph called the 1-skeleton. We also explore the failure, in general, of k-graphs to faithfully embed into their fundamental groupoids. 	
0410516v1	http://arxiv.org/pdf/math/0410516v1	2004	Dimension filtration on loops	J. Mostovoy|J. M. Pérez-Izquierdo	  We show that the graded group associated to the dimension filtration on a loop acquires the structure of a Sabinin algebra after being tensored with a field of characteristic zero. The key to the proof is the interpretation of the primitive operations of Umirbaev and Shestakov in terms of the operations on a loop that measure the failure of the associator to be a homomorphism. 	
0601116v1	http://arxiv.org/pdf/math/0601116v1	2006	Arbitrary threshold widths for monotone symmetric properties	Raphaël Rossignol	  We investigate the threshold widths of some symmetric properties which range asymptotically between 1/\sqrt{n} and 1/(log n). These properties are built using a combination of failure sets arising from reliability theory. This combination of sets is simply called a product. Some general results on the threshold width of the product of two sets A and B in terms of the threshold locations and widths of A and B are provided. 	
0611308v1	http://arxiv.org/pdf/math/0611308v1	2006	Reliable Control for Parameterized Interconnected Systems using LMI   techniques	Gisela Pujol|Jose Rodellar|Josep M. Rossell	  This work presents the design of a reliable decentralized state feedback control for a class of uncertain interconnected polytopic continous systems. A model of failures in actuators is adopted which considers outages or partial degradation in independent actuators. The control is developed using the concept of guaranteed cost control and a new LMI characterization using polytopic Lyapunov functions. 	
0703091v1	http://arxiv.org/pdf/math/0703091v1	2007	A family of covering properties for forcing axioms and strongly compact   cardinals	Matteo Viale	  This paper presents the main results in my Ph.D. thesis. In what follows several proofs of SCH are presented introducing a family of covering properties which implies both SCH and the failure of various forms of square. These covering properties are also applied to investigate models of strongly compact cardinals or of strong forcing axioms like MM or PFA. 	
0007013v1	http://arxiv.org/pdf/nlin/0007013v1	2000	What you always wanted to know about genetic algorithms but were afraid   to hear	Tommaso Toffoli	  In spite of their seemingly "obvious" virtues as a search strategy, genetic algorithms have ended up playing only a modest role as design tools in science and engineering. We review the reasons for this apparent failure, and we suggest a more relaxed view of their utility. 	
9908085v1	http://arxiv.org/pdf/nucl-th/9908085v1	1999	Comment on "Evidence for the Existence of Supersymmetry in Atomic   Nuclei"	B G Wybourne	  Metz et al PRL (83), 1542 (1999) have presented evidence for supersymmetry in atomic nuclei but their model Hamiltonian fails to distinguish between irreducible representations that occur with multiplicities greater than one. This failure raises serious questions as to the validity of the evidence. 	
0007051v1	http://arxiv.org/pdf/nucl-th/0007051v1	2000	Nucleon-nucleon interaction in the Skyrme model	Isabela P. Cavalcante|Manoel R. Robilotta	  We consider the interaction of two skyrmions in the framework of the sudden approximation. The widely used product ansatz is investigated. Its failure in reproducing an attractive central potential is associated with terms that violate G-parity. We discuss the construction of alternative ans\"atze and identify a plausible solution to the problem. 	
9710011v1	http://arxiv.org/pdf/physics/9710011v1	1997	Nonuniqueness and Turbulence	Mark A. Peterson	  The possibility is considered that turbulence is described by differential equations for which uniqueness fails maximally, at least in some limit. The inviscid Burgers equation, in the context of Onsager's suggestion that turbulence should be described by a negative absolute temperature, is such a limit. In this picture, the onset of turbulence coincides with the proliferation of singularities which characterizes the failure of uniqueness. 	
0508071v1	http://arxiv.org/pdf/physics/0508071v1	2005	E = mc^2. Or Is It? : A Comment on Manuscript astro-ph/0504486	Ezzat G. Bakhoum	  Manuscript astro-ph/0504486 is a clear example of the fundamental conceptual flaw that had persisted for the past 100 years. Namely, the failure to realize that mass-energy equivalence, as applied to a propagating light particle (photon), is only a special case of a more general law that encompasses both photons and subluminal particles. 	
0506005v1	http://arxiv.org/pdf/q-bio/0506005v1	2005	Asymptotic construction of pulses in the Hodgkin Huxley model for   myelinated nerves	A. Carpio	  A quantitative description of pulses and wave trains in the spatially discrete Hodgkin-Huxley model for myelinated nerves is given. Predictions of the shape and speed of the waves and the thresholds for propagation failure are obtained. Our asymptotic predictions agree quite well with numerical solutions of the model and describe wave patterns generated by repeated firing at a boundary. 	
9703018v1	http://arxiv.org/pdf/quant-ph/9703018v1	1997	The analysis of Hardy's experiment revisited	Lev Vaidman	  Cohen and Hiley [Phys. Rev. A 52, 76 (1995)] have criticized the analysis of Hardy's gedanken experiment according to which the contradiction with quantum theory in Hardy's experiment arises due the failure of the "product rule" for the elements of reality of pre- and post-selected systems. It is argued that the criticism of Cohen and Hiley is not sound. 	
9712006v1	http://arxiv.org/pdf/quant-ph/9712006v1	1997	A Possible Einstein-Podolsky-Rosen Probe of the Momentum-Position   Uncertainty Relation	Roy Ringo	  It is suggested that a measurement of the products of photoemission by alkali atoms excited after extraction from a trap, might, using the EPR strategy, show a significant violation of the momentum-position uncertainty relation. If this failed, as is quite likely, possible causes, such as retroactive propagation of influences and retrodiction failure, could be tested on the proposed apparatus. 	
9912033v1	http://arxiv.org/pdf/quant-ph/9912033v1	1999	Mixedness and teleportation	S. Bose|V. Vedral	  We show that on exceeding a certain degree of mixedness (as quantified by the von Neumann entropy), entangled states become useless for teleporatation. By increasing the dimension of the entangled systems, this entropy threshold can be made arbitrarily close to maximal. This entropy is found to exceed the entropy threshold sufficient to ensure the failure of dense coding. 	
0011049v1	http://arxiv.org/pdf/quant-ph/0011049v1	2000	String Matching in ${\tilde O}(\sqrt{n}+\sqrt{m})$ Quantum Time	H. Ramesh|V. Vinay	  We show how to determine whether a given pattern p of length m occurs in a given text t of length n in ${\tilde O}(\sqrt{n}+\sqrt{m})$\footnote{${\tilde O}$ allows for logarithmic factors in m and $n/m$} time, with inverse polynomial failure probability. This algorithm combines quantum searching algorithms with a technique from parallel string matching, called {\em Deterministic Sampling}. 	
0210127v1	http://arxiv.org/pdf/quant-ph/0210127v1	2002	Quasi-deterministic generation of entangled atoms in a cavity	Jongcheol Hong|Hai-Woong Lee	  We present a scheme to generate a maximally entangled state of two three-level atoms in a cavity. The success or failure of the generation of the desired entangled state can be determined by detecting the polarization of the photon leaking out of the cavity. With the use of an automatic feedback, the success probability of the scheme can be made to approach unity. 	
0505189v1	http://arxiv.org/pdf/quant-ph/0505189v1	2005	Atom diode: Variants, stability, limits, and adiabatic interpretation	A. Ruschhaupt|J. G. Muga	  We examine and explain the stability properties of the ``atom diode'', a laser device that lets the ground state atom pass in one direction but not in the opposite direction. The diodic behavior and the variants that result by using different laser configurations may be understood with an adiabatic approximation. The conditions to break down the approximation, which imply also the diode failure, are analyzed. 	
0510026v2	http://arxiv.org/pdf/quant-ph/0510026v2	2006	Levinson's theorem and reflectionless one-dimensional potentials	D. E. Zambrano	  We studied two possible approaches to one-dimensional Levinson's theorem for Sch\"odinger equation. The first one, we restrict the 3-dimensional theorem. The other one, the theorem proposed by Dong, Ma and Klauss \cite{Dong}. We find failures in each approach for a one-dimensional reflectionless potential. In order to see this, we explicitly evaluate the phase shift using Schr\"odinger equation using solution procedure proposed by Jaffe \cite{Jaffe}. 	
0705.4131v3	http://arxiv.org/pdf/0705.4131v3	2011	No limit model in inaccessible	Saharon Shelah	  Our aim is to improve the negative results i.e. non-existence of limit models, and the failure of the generic pair property from math.LO/0609636 to inaccessible lambda as promised there. The motivation is that in [Sh:F756] the positive results are for lambda measurable hence inaccessible, whereas in math.LO/0609636 in the negative results obtained only on non-strong limit cardinals. 	
0706.3708v1	http://arxiv.org/pdf/0706.3708v1	2007	The Bergman kernel and projection on non-smooth worm domains	Steven G. Krantz|Marco M. Peloso	  This paper provides a precise asymptotic expansion for the Bergman kernel on the non-smooth worm domains of Christer Kiselman in complex 2-space. Applications are given to the failure of Condition R, to deviant boundary behavior of the kernel, and to L^p mapping properties of the kernel. 	
0708.1997v1	http://arxiv.org/pdf/0708.1997v1	2007	Time and nonlocal realism: Consequences of the before-before experiment	Antoine Suarez	  It is argued that recent experiments refuting nonlocal realism, can also be considered as experiments refuting time-ordered nonlocality and, hence, confirming the result of the before-before experiment. However, the before-before experiment provides a broader refutation because it also falsifies the testable relativistic version of Bohm's nonlocal model. All this stresses the interest of a new before-before experiment demonstrating together the failure of time-ordered nonlocality and the violation of the Leggett's inequality. 	
0711.1724v2	http://arxiv.org/pdf/0711.1724v2	2008	The Stability of an Expanding Circular Cavity and the Failure of   Amorphous Solids	Eran Bouchbinder|Ting-Shek Lo|Itamar Procaccia|Elad Shtilerman	  Recently, the existence and properties of unbounded cavity modes, resulting in extensive plastic deformation failure of two-dimensional sheets of amorphous media, were discussed in the context of the athermal Shear-Transformation-Zones (STZ) theory. These modes pertain to perfect circular symmetry of the cavity and the stress conditions. In this paper we study the shape stability of the expanding circular cavity against perturbations, in both the unbounded and the bounded growth regimes (for the latter the unperturbed theory predicts no catastrophic failure). Since the unperturbed reference state is time dependent, the linear stability theory cannot be cast into standard time-independent eigenvalue analysis. The main results of our study are: (i) sufficiently small perturbations are stable, (ii) larger perturbations within the formal linear decomposition may lead to an instability; this dependence on the magnitude of the perturbations in the linear analysis is a result of the non-stationarity of the growth, (iii) the stability of the circular cavity is particularly sensitive to perturbations in the effective disorder temperature; in this context we highlight the role of the rate sensitivity of the limiting value of this effective temperature. Finally we point to the importance of the form of the stress-dependence of the rate of STZ transitions. The present analysis indicates the importance of nonlinear effects that were not taken into account yet. Even more importantly, the analysis suggests that details of the constitutive relations appearing in the theory can be constrained by the modes of macroscopic failure in these amorphous systems. 	
0712.0181v1	http://arxiv.org/pdf/0712.0181v1	2007	Circumstellar magnetic fields in Herbig Ae stars	R. V. Yudin|M. A. Pogodin|S. Hubrig|M. Schoeller	  We present the results of our latest studies of the circumstellar magnetic fields in Herbig Ae stars and briefly discuss the cause of the failure of another recent study by our colleagues to confirm the Zeeman features in our spectra. 	
0712.1244v1	http://arxiv.org/pdf/0712.1244v1	2007	Projectively full ideals in Noetherian rings, a survey	Catalin Ciuperca|William Heinzer|Jack Ratliff|David Rush	  We discuss projective equivalence of ideals in Noetherian rings and the existence or failure of existence of projectively full ideals. We describe connections with the Rees valuations and Rees integers of an ideal, and consider the question of whether improvements can be made by passing to an integral extension ring. 	
0801.2493v1	http://arxiv.org/pdf/0801.2493v1	2008	Scarring for Quantum Maps with Simple Spectrum	Dubi Kelmer	  We previously introduced a family of symplectic maps of the torus whose quantization exhibits scarring on invariant co-isotropic submanifolds. The purpose of this note is to show that in contrast to other examples, where failure of Quantum Unique Ergodicity is attributed to high multiplicities in the spectrum, for these examples the spectrum is (generically) simple. 	
0805.1350v2	http://arxiv.org/pdf/0805.1350v2	2008	Comment on "'t Hooft vertices, partial quenching, and rooted staggered   QCD"	Michael Creutz	  A recent criticism of the proof of the failure of the rooting procedure with staggered fermions is shown to be incorrect. 	
0806.0160v1	http://arxiv.org/pdf/0806.0160v1	2008	Comment on ``Success of collinear expansion in the calculation of   induced gluon emission''	P. Aurenche|B. G. Zakharov|H. Zaraket	  We show that the arguments against our recent paper on the failure of the collinear expansion in the calculation of the induced gluon emission raised by X.N. Wang are either incorrect or irrelevant. 	
0807.0021v1	http://arxiv.org/pdf/0807.0021v1	2008	The SISCone and anti-kt jet algorithms	Gregory Soyez	  We illustrate how the midpoint and iterative cone (with progressive removal) algorithms fail to satisfy the fundamental requirements of infrared and collinear safety, causing divergences in the perturbative expansion. We introduce SISCone and the anti-kt algorithms as respective replacements that do not have those failures without any cost at the experimental level. 	
0901.2420v1	http://arxiv.org/pdf/0901.2420v1	2009	On the Amplitudes of Superhumps	J. Smak	  Amplitudes of bolometric light curves produced by 2D and 3D simulations are used to determine the corresponding visual amplitudes. They turn out to be about 10 times lower than typical amplitudes of superhumps. This means a major failure of the tidal model of superhumps. 	
0903.3461v1	http://arxiv.org/pdf/0903.3461v1	2009	Fault-Tolerant Consensus in Unknown and Anonymous Networks	Carole Delporte-Gallet|Hugues Fauconnier|Andreas Tielmann	  This paper investigates under which conditions information can be reliably shared and consensus can be solved in unknown and anonymous message-passing networks that suffer from crash-failures. We provide algorithms to emulate registers and solve consensus under different synchrony assumptions. For this, we introduce a novel pseudo leader-election approach which allows a leader-based consensus implementation without breaking symmetry. 	
0904.0574v1	http://arxiv.org/pdf/0904.0574v1	2009	Review on Extended Approaches in the Kaluza-Klein Model	Francesco Cianfrani|Giovanni Montani	  A review of the Kaluza-Klein formulation is provided, with a particular emphasis on the geometrization issue. The failure at reproducing quantum numbers of particles and the appearance of huge mass terms are outlined. The possibility to solve these points by an extended approach based on an averaging procedure is discussed. 	
0905.2484v1	http://arxiv.org/pdf/0905.2484v1	2009	Use of a Microcontroller for Fast Feedback Control of a Fiber Laser	M. R. Dietrich|B. B. Blinov	  An inexpensive, easily programmed microcontroller is demonstrated for the fast frequency stabilization of an infrared fiber laser. The microcontroller manages all digitalization and processing, with external circuitry only providing buffering, amplification and overvoltage protection. Several convenient features are included to recover from common failure modes of a laser lock system, and a final Nyquist frequency of 200 kHz is obtained, corresponding to a usable bandwidth of 40-60 kHz. 	
0911.0068v1	http://arxiv.org/pdf/0911.0068v1	2009	On the Failure of Fixed-Point Theorems for Chain-complete Lattices in   the Effective Topos	Andrej Bauer	  In the effective topos there exists a chain-complete distributive lattice with a monotone and progressive endomap which does not have a fixed point. Consequently, the Bourbaki-Witt theorem and Tarski's fixed-point theorem for chain-complete lattices do not have constructive (topos-valid) proofs. 	
1004.1678v1	http://arxiv.org/pdf/1004.1678v1	2010	Node Sensing & Dynamic Discovering Routes for Wireless Sensor Networks	Arabinda Nanda|Amiya Kumar Rath|Saroj Kumar Rout	  The applications of Wireless Sensor Networks (WSN) contain a wide variety of scenarios. In most of them, the network is composed of a significant number of nodes deployed in an extensive area in which not all nodes are directly connected. Then, the data exchange is supported by multihop communications. Routing protocols are in charge of discovering and maintaining the routes in the network. However, the correctness of a particular routing protocol mainly depends on the capabilities of the nodes and on the application requirements. This paper presents a dynamic discover routing method for communication between sensor nodes and a base station in WSN. This method tolerates failures of arbitrary individual nodes in the network (node failure) or a small part of the network (area failure). Each node in the network does only local routing preservation, needs to record only its neighbor nodes' information, and incurs no extra routing overhead during failure free periods. It dynamically discovers new routes when an intermediate node or a small part of the network in the path from a sensor node to a base station fails. In our planned method, every node decides its path based only on local information, such as its parent node and neighbor nodes' routing information. So, it is possible to form a loop in the routing path. We believe that the loop problem in sensor network routing is not as serious as that in the Internet routing or traditional mobile ad-hoc routing. We are trying to find all possible loops and eliminate the loops as far as possible in WSN. 	
1007.0515v3	http://arxiv.org/pdf/1007.0515v3	2012	Liquidity in Credit Networks: A Little Trust Goes a Long Way	Pranav Dandekar|Ashish Goel|Ramesh Govindan|Ian Post	  Credit networks represent a way of modeling trust between entities in a network. Nodes in the network print their own currency and trust each other for a certain amount of each other's currency. This allows the network to serve as a decentralized payment infrastructure---arbitrary payments can be routed through the network by passing IOUs between trusting nodes in their respective currencies---and obviates the need for a common currency. Nodes can repeatedly transact with each other and pay for the transaction using trusted currency. A natural question to ask in this setting is: how long can the network sustain liquidity, i.e., how long can the network support the routing of payments before credit dries up? We answer this question in terms of the long term failure probability of transactions for various network topologies and credit values.   We prove that the transaction failure probability is independent of the path along which transactions are routed. We show that under symmetric transaction rates, the transaction failure probability in a number of well-known graph families goes to zero as the size, density or credit capacity of the network increases. We also show via simulations that even networks of small size and credit capacity can route transactions with high probability if they are well-connected. Further, we characterize a centralized currency system as a special type of a star network (one where edges to the root have infinite credit capacity, and transactions occur only between leaf nodes) and compute the steady-state transaction failure probability in a centralized system. We show that liquidity in star networks, complete graphs and Erd\"{o}s-R\'{e}nyi networks is comparable to that in equivalent centralized currency systems; thus we do not lose much liquidity in return for their robustness and decentralized properties. 	
1009.2463v1	http://arxiv.org/pdf/1009.2463v1	2010	Concave Renewal Functions Do Not Imply DFR Inter-Renewal Times	Yaming Yu	  Brown (1980, 1981) proved that the renewal function is concave if the inter-renewal distribution is DFR (decreasing failure rate), and conjectured the converse. This note settles Brown's conjecture with a class of counter-examples. We also give a short proof of Shanthikumar's (1988) result that the DFR property is closed under geometric compounding. 	
1010.0459v1	http://arxiv.org/pdf/1010.0459v1	2010	Algebraic and geometric convergence of discrete representations into   PSL(2,C)	Ian Biringer|Juan Souto	  Anderson and Canary have shown that if the algebraic limit of a sequence of discrete, faithful representations of a finitely generated group into PSL(2,C) does not contain parabolics, then it is also the sequence's geometric limit. We construct examples that demonstrate the failure of this theorem for certain sequences of unfaithful representations, and offer a suitable replacement. 	
1011.4923v2	http://arxiv.org/pdf/1011.4923v2	2011	Irrational centers	Sándor Kovács	  The main purpose of this article is to get a handle on determining how far a non-rational singularity is from being rational, or in other words, introduce a measure of the failure of a singularity being rational. 	
1012.2040v1	http://arxiv.org/pdf/1012.2040v1	2010	The combinatorial essence of supercompactness	Christoph Weiß	  We introduce combinatorial principles that characterize strong compactness and supercompactness for inaccessible cardinals but also make sense for successor cardinals. Their consistency is established from what is supposedly optimal. Utilizing the failure of a weak version of square, we show that the best currently known lower bounds for the consistency strength of these principles can be applied. 	
1101.1071v1	http://arxiv.org/pdf/1101.1071v1	2011	On the Non-Termination of Ruppert's Algorithm	Alexander Rand	  A planar straight-line graph which causes the non-termination Ruppert's algorithm for a minimum angle threshold larger than about 29.5 degrees is given. The minimum input angle of this example is about 74.5 degrees meaning that failure is not due to small input angles. Additionally, a similar non-acute input is given for which Chew's second algorithm does not terminate for a minimum angle threshold larger than about 30.7 degrees. 	
1101.2979v1	http://arxiv.org/pdf/1101.2979v1	2011	Unbounded Laplacians on Graphs: Basic Spectral Properties and the Heat   Equation	Matthias Keller|Daniel Lenz	  We discuss Laplacians on graphs in a framework of regular Dirichlet forms. We focus on phenomena related to unboundedness of the Laplacians. This includes (failure of) essential selfadjointness, absence of essential spectrum and stochastic incompleteness. 	
1104.0764v1	http://arxiv.org/pdf/1104.0764v1	2011	Comparison of Weibull tail-coefficient estimators	Laurent Gardes|Stéphane Girard	  We address the problem of estimating the Weibull tail-coefficient which is the regular variation exponent of the inverse failure rate function. We propose a family of estimators of this coefficient and an associate extreme quantile estimator. Their asymptotic normality are established and their asymptotic mean-square errors are compared. The results are illustrated on some finite sample situations. 	
1109.2909v1	http://arxiv.org/pdf/1109.2909v1	2011	Diffusion dynamics in a Tevatron store	Tanaji Sen	  A separator failure during a store in 2002 led to a drop in luminosity, to increased emittance growth and to a drop in beam lifetimes. We show that a simple diffusion model can be used to explain the changes in emittance growth and beam lifetimes. 	
1112.5637v1	http://arxiv.org/pdf/1112.5637v1	2011	Field-parametrization dependence of Dirac's method for constrained   Hamiltonians with first-class constraints: failure or triumph? Non-covariant   models	N. Kiriushcheva|P. G. Komorowski|S. V. Kuzmin	  We argue that the field-parametrization dependence of Dirac's procedure, for Hamiltonians with first-class constraints not only preserves covariance in covariant theories, but in non-covariant gauge theories it allows one to find the natural field parametrization in which the Hamiltonian formulation automatically leads to the simplest gauge symmetry. 	
1201.3470v1	http://arxiv.org/pdf/1201.3470v1	2012	A counterexample to well-posedness of entropy solutions to the   compressible Euler system	Elisabetta Chiodaroli	  We deal with entropy solutions to the Cauchy problem for the isentropic compressible Euler equations in the space-periodic case. In more than one space dimension, the methods developed by De Lellis-Sz\'ekelyhidi enable us to show failure of uniqueness on a finite time-interval for entropy solutions starting from any continuously differentiable initial density and suitably constructed bounded initial linear momenta. 	
1206.1099v1	http://arxiv.org/pdf/1206.1099v1	2012	Power Grid Vulnerability to Geographically Correlated Failures -   Analysis and Control Implications	Andrey Bernstein|Daniel Bienstock|David Hay|Meric Uzunoglu|Gil Zussman	  We consider power line outages in the transmission system of the power grid, and specifically those caused by a natural disaster or a large scale physical attack. In the transmission system, an outage of a line may lead to overload on other lines, thereby eventually leading to their outage. While such cascading failures have been studied before, our focus is on cascading failures that follow an outage of several lines in the same geographical area. We provide an analytical model of such failures, investigate the model's properties, and show that it differs from other models used to analyze cascades in the power grid (e.g., epidemic/percolation-based models). We then show how to identify the most vulnerable locations in the grid and perform extensive numerical experiments with real grid data to investigate the various effects of geographically correlated outages and the resulting cascades. These results allow us to gain insights into the relationships between various parameters and performance metrics, such as the size of the original event, the final number of connected components, and the fraction of demand (load) satisfied after the cascade. In particular, we focus on the timing and nature of optimal control actions used to reduce the impact of a cascade, in real time. We also compare results obtained by our model to the results of a real cascade that occurred during a major blackout in the San Diego area on Sept. 2011. The analysis and results presented in this paper will have implications both on the design of new power grids and on identifying the locations for shielding, strengthening, and monitoring efforts in grid upgrades. 	
1206.6724v2	http://arxiv.org/pdf/1206.6724v2	2015	A Generalization of Martin's Axiom	David Asperó|Miguel Angel Mota	  We define the $\aleph_{1.5}$ chain condition. The corresponding forcing axiom is a generalization of Martin's Axiom and implies certain uniform failures of club--guessing on $\omega_1$ that don't seem to have been considered in the literature before. 	
1302.5401v1	http://arxiv.org/pdf/1302.5401v1	2013	Sparse Fault-Tolerant BFS Trees	Merav Parter|David Peleg	  This paper addresses the problem of designing a sparse {\em fault-tolerant} BFS tree, or {\em FT-BFS tree} for short, namely, a sparse subgraph $T$ of the given network $G$ such that subsequent to the failure of a single edge or vertex, the surviving part $T'$ of $T$ still contains a BFS spanning tree for (the surviving part of) $G$. Our main results are as follows. We present an algorithm that for every $n$-vertex graph $G$ and source node $s$ constructs a (single edge failure) FT-BFS tree rooted at $s$ with $O(n \cdot \min\{\Depth(s), \sqrt{n}\})$ edges, where $\Depth(s)$ is the depth of the BFS tree rooted at $s$. This result is complemented by a matching lower bound, showing that there exist $n$-vertex graphs with a source node $s$ for which any edge (or vertex) FT-BFS tree rooted at $s$ has $\Omega(n^{3/2})$ edges. We then consider {\em fault-tolerant multi-source BFS trees}, or {\em FT-MBFS trees} for short, aiming to provide (following a failure) a BFS tree rooted at each source $s\in S$ for some subset of sources $S\subseteq V$. Again, tight bounds are provided, showing that there exists a poly-time algorithm that for every $n$-vertex graph and source set $S \subseteq V$ of size $\sigma$ constructs a (single failure) FT-MBFS tree $T^*(S)$ from each source $s_i \in S$, with $O(\sqrt{\sigma} \cdot n^{3/2})$ edges, and on the other hand there exist $n$-vertex graphs with source sets $S \subseteq V$ of cardinality $\sigma$, on which any FT-MBFS tree from $S$ has $\Omega(\sqrt{\sigma}\cdot n^{3/2})$ edges. Finally, we propose an $O(\log n)$ approximation algorithm for constructing FT-BFS and FT-MBFS structures. The latter is complemented by a hardness result stating that there exists no $\Omega(\log n)$ approximation algorithm for these problems under standard complexity assumptions. 	
1304.2512v1	http://arxiv.org/pdf/1304.2512v1	2013	On the algebraic dual of D(Ω)	Michael Oberguggenberger	  This paper is concerned with the algebraic dual D*(\Omega) of the space of test functions D(\Omega). The emphasis is on failures and successes of D*(\Omega) as compared to the continuous dual D'(\Omega), the space of distributions. Topological properties, operations with elements of D*(\Omega) and applications to linear partial differential equations are discussed. 	
1304.2716v1	http://arxiv.org/pdf/1304.2716v1	2013	Do We Need Higher-Order Probabilities and, If So, What Do They Mean?	Judea Pearl	  The apparent failure of individual probabilistic expressions to distinguish uncertainty about truths from uncertainty about probabilistic assessments have prompted researchers to seek formalisms where the two types of uncertainties are given notational distinction. This paper demonstrates that the desired distinction is already a built-in feature of classical probabilistic models, thus, specialized notations are unnecessary. 	
1304.2758v1	http://arxiv.org/pdf/1304.2758v1	2013	Efficient Inference on Generalized Fault Diagrams	Ross D. Shachter|Leonard Bertrand	  The generalized fault diagram, a data structure for failure analysis based on the influence diagram, is defined. Unlike the fault tree, this structure allows for dependence among the basic events and replicated logical elements. A heuristic procedure is developed for efficient processing of these structures. 	
1306.6260v1	http://arxiv.org/pdf/1306.6260v1	2013	Information-Theoretic Security for the Masses	Oleksandr Nikitin	  We combine interactive zero-knowledge protocols and weak physical layer randomness properties to construct a protocol which allows bootstrapping an IT-secure and PF-secure channel from a memorizable shared secret. The protocol also tolerates failures of its components, still preserving most of its security properties, which makes it accessible to regular users. 	
1309.0276v2	http://arxiv.org/pdf/1309.0276v2	2013	Traffic analyzer for differentiating BitTorrent handshake failures from   port-scans	Kamran Khan|Affan Syed|Ali Khayam	  This paper aims to improve the accuracy of port-scan detectors by analyzing traffic of BitTorrent hosts and differentiating their respective BitTorrent connection (attempts) from port-scans.   It is shown that by looking at BitTorrent coordination traffic and modelling port-scanning behavior the number of BitTorrent-related false positives can be reduced by 80% without any loss of IDS accuracy. 	
1311.4596v1	http://arxiv.org/pdf/1311.4596v1	2013	Application of a linear elastic - brittle interface model to the crack   initiation and propagation at fibre-matrix interface under biaxial transverse   loads	V. Mantič|L. Távara|A. Blázquez|E. Graciani|F. París	  The crack onset and propagation at the fibre-matrix interface in a composite under tensile/compressive remote biaxial transverse loads is studied by a new linear elastic - (perfectly) brittle interface model. In this model the interface is represented by a continuous distribution of springs which simulates the presence of a thin elastic layer. The constitutive law for the continuous distribution of normal and tangential of initially linear elastic springs takes into account possible frictionless elastic contact between fibre and matrix once a portion of the interface is broken. A brittle failure criterion is employed for the distribution of springs, which enables the study of crack onset and propagation. This interface failure criterion takes into account the variation of the interface fracture toughness with the fracture mode mixity. The main advantages of the present interface model are its simplicity, robustness and its computational efficiency when the so-called sequentially linear analysis is applied. Moreover, in the present plane strain problem of a single fibre embedded in a matrix subjected to uniform remote transverse loads, this model can be used to obtain analytic predictions of interface crack onset. The numerical results provided by a 2D boundary element analysis show that a fibre-matrix interface failure initiates by onset of a finite debond in the neighbourhood of an interface point where the failure criterion is reached first (under increasing proportional load), this debond further propagating along the interface in mixed mode or even, in some configurations, with the crack tip under compression. The analytical predictions of the debond onset position and associated critical load are used for checking the computational procedure implemented, an excellent agreement being obtained. 	
1312.3347v1	http://arxiv.org/pdf/1312.3347v1	2013	A Distributed Deadlock Free Quorum Based Algorithm for Mutual Exclusion	M. Naimi|O. Thiare	  Quorum based mutual exclusion algorithms enjoy many advantages such as low message complexity and high failure resiliency. The use of quorums is a well known approach to achieving mutual exclusion in distributed environments. Several distributed based quorum mutual exclusion was presented. 	
1312.5173v1	http://arxiv.org/pdf/1312.5173v1	2013	New Repair strategy of Hadamard Minimum Storage Regenerating Code for   Distributed Storage System	Xiaohu Tang|Bin Yang|Jie Li	  The newly presented $(k+2,k)$ Hadamard minimum storage regenerating (MSR) code is the first class of high rate storage code with optimal repair property for all single node failures. In this paper, we propose a new simple repair strategy, which can considerably reduces the computation load of the node repair in contrast to the original one. 	
1401.7528v3	http://arxiv.org/pdf/1401.7528v3	2017	A Monte-Carlo Approach to Lifespan Failure Performance Analysis of the   Network Fabric in Modular Data Centers	Reza Farrahi Moghaddam|Vahid Asghari|Fereydoun Farrahi Moghaddam|Yves Lemieux|Mohamed Cheriet	  Data centers have been evolved from a passive element of compute infrastructure to become an active, core part of any ICT solution. In particular, modular data centers (MDCs), which are a promising design approach to improve resiliency of data centers, can play a key role in deploying ICT infrastructure in remote and inhospitable environments in order to take advantage of low temperatures and hydro- and wind-electric capabilities. This is because of capability of the modular data centers to survive even in lack of continuous on-site maintenance and support. The most critical part of a data center is its network fabric that could impede the whole system even if all other components are fully functional, assuming that other analyses has been already performed to ensure the reliability of the underlying infrastructure and support systems. In this work, a complete failure analysis of modular data centers using failure models of various components including servers, switches, and links is performed using a proposed Monte-Carlo approach. The proposed Monte-Carlo approach, which is based on the concept of snapshots, allows us to effectively calculate the performance of a design along its lifespan even up to the terminal stages. To show the capabilities of the proposed approach, various network topologies, such as FatTree, BCube, MDCube, and their modifications are considered. The performance and also the lifespan of each topology design in presence of failures of their components are studied against the topology parameters. 	
1401.7716v1	http://arxiv.org/pdf/1401.7716v1	2014	redMaPPer III: A Detailed Comparison of the Planck 2013 and SDSS DR8   RedMaPPer Cluster Catalogs	Eduardo Rozo|Eli S. Rykoff|James G. Bartlett|Jean B. Melin	  We compare the Planck Sunyaev-Zeldovich (SZ) cluster sample (PSZ1) to the Sloan Digital Sky Survey (SDSS) redMaPPer catalog, finding that all Planck clusters within the redMaPPer mask and within the redshift range probed by redMaPPer are contained in the redMaPPer cluster catalog. These common clusters define a tight scaling relation in the richness-SZ mass ($\lambda$--$M_{SZ}$) plane, with an intrinsic scatter in richness of $\sigma_{\lambda|M_{SZ}} = 0.266 \pm 0.017$. The corresponding intrinsic scatter in true cluster halo mass at fixed richness is $\approx 21\%$. The regularity of this scaling relation is used to identify failures in both the redMaPPer and Planck cluster catalogs. Of the 245 galaxy clusters in common, we identify three failures in redMaPPer and 36 failures in the PSZ1. Of these, at least 12 are due to clusters whose optical counterpart was correctly identified in the PSZ1, but where the quoted redshift for the optical counterpart in the external data base used in the PSZ1 was incorrect. The failure rates for redMaPPer and the PSZ1 are $1.2\%$ and $14.7\%$ respectively, or 9.8% in the PSZ1 after subtracting the external data base errors. We have further identified 5 PSZ1 sources that suffer from projection effects (multiple rich systems along the line-of-sight of the SZ detection) and 17 new high redshift ($z\gtrsim 0.6$) cluster candidates of varying degrees of confidence. Should all of the high-redshift cluster candidates identified here be confirmed, we will have tripled the number of high redshift Planck clusters in the SDSS region. Our results highlight the power of multi-wavelength observations to identify and characterize systematic errors in galaxy cluster data sets, and clearly establish photometric data both as a robust cluster finding method, and as an important part of defining clean galaxy cluster samples. 	
1402.3809v2	http://arxiv.org/pdf/1402.3809v2	2014	Toward Resilient Algorithms and Applications	Michael A. Heroux	  Over the past decade, the high performance computing community has become increasingly concerned that preserving the reliable, digital machine model will become too costly or infeasible. In this paper we discuss four approaches for developing new algorithms that are resilient to hard and soft failures. 	
1402.6586v1	http://arxiv.org/pdf/1402.6586v1	2014	Approach to failure in porous granular materials under compression	F. Kun|I. Varga|S. Lennartz-Sassinek|I. G. Main	  We investigate the approach to catastrophic failure in a model porous granular material undergoing uniaxial compression. A discrete element computational model is used to simulate both the micro-structure of the material and the complex dynamics and feedbacks involved in local fracturing and the production of crackling noise. Under strain-controlled loading micro-cracks initially nucleate in an uncorrelated way all over the sample. As loading proceeds the damage localizes into a narrow damage band inclined at 30-45 degrees to the load direction. Inside the damage band the material is crushed into a poorly-sorted mixture of mainly fine powder hosting some larger fragments. The mass probability density distribution of particles in the damage zone is a power law of exponent 2.1, similar to a value of 1.87 inferred from observations of the length distribution of wear products (gouge) in natural and laboratory faults. Dynamic bursts of radiated energy, analogous to acoustic emissions observed in laboratory experiments on porous sedimentary rocks, are identified as correlated trails or cascades of local ruptures that emerge from the stress redistribution process. As the system approaches macroscopic failure consecutive bursts become progressively more correlated. Their size distribution is also a power law, with an equivalent Gutenberg-Richter b-value of 1.22 averaged over the whole test, ranging from 3 down to 0.5 at the time of failure, all similar to those observed in laboratory tests on granular sandstone samples. The formation of the damage band itself is marked by a decrease in the average distance between consecutive bursts and an emergent power law correlation integral of event locations with a correlation dimension of 2.55, also similar to those observed in the laboratory (between 2.75 and 2.25). 	
1403.4371v2	http://arxiv.org/pdf/1403.4371v2	2014	The signed Euler characteristic of very affine varieties	Nero Budur|Botong Wang	  A conjecture of J. Huh and B. Sturmfels predicts that the sign of the Euler characteristic of a complex very affine variety depends only on the parity of the dimension. The conjecture is true for locally complete intersections. Beyond this case, we construct counterexamples with arbitrarily bad failure. 	
1404.6780v3	http://arxiv.org/pdf/1404.6780v3	2016	A "strange" functional equation for Eisenstein series and miraculous   duality on the moduli stack of bundles	D. Gaitsgory	  We show that the failure of the usual Verdier duality on Bun(G) leads to a new duality functor on the category of D-modules, and we study its relation to the operation of Eisenstein series. 	
1406.6169v1	http://arxiv.org/pdf/1406.6169v1	2014	Fault Tolerant Approximate BFS Structures	Merav Parter|David Peleg	  This paper addresses the problem of designing a {\em fault-tolerant} $(\alpha, \beta)$ approximate BFS structure (or {\em FT-ABFS structure} for short), namely, a subgraph $H$ of the network $G$ such that subsequent to the failure of some subset $F$ of edges or vertices, the surviving part of $H$ still contains an \emph{approximate} BFS spanning tree for (the surviving part of) $G$, satisfying $dist(s,v,H\setminus F) \leq \alpha \cdot dist(s,v,G\setminus F)+\beta$ for every $v \in V$. We first consider {\em multiplicative} $(\alpha,0)$ FT-ABFS structures resilient to a failure of a single edge and present an algorithm that given an $n$-vertex unweighted undirected graph $G$ and a source $s$ constructs a $(3,0)$ FT-ABFS structure rooted at $s$ with at most $4n$ edges (improving by an $O(\log n)$ factor on the near-tight result of \cite{BS10} for the special case of edge failures). Assuming at most $f$ edge failures, for constant integer $f>1$, we prove that there exists a (poly-time constructible) $(3(f+1), (f+1) \log n)$ FT-ABFS structure with $O(f n)$ edges.   We then consider {\em additive} $(1,\beta)$ FT-ABFS structures. In contrast to the linear size of $(\alpha,0)$ FT-ABFS structures, we show that for every $\beta \in [1, O(\log n)]$ there exists an $n$-vertex graph $G$ with a source $s$ for which any $(1,\beta)$ FT-ABFS structure rooted at $s$ has $\Omega(n^{1+\epsilon(\beta)})$ edges, for some function $\epsilon(\beta) \in (0,1)$. In particular, $(1,3)$ FT-ABFS structures admit a lower bound of $\Omega(n^{5/4})$ edges. Our lower bounds are complemented by an upper bound, showing that there exists a poly-time algorithm that for every $n$-vertex unweighted undirected graph $G$ and source $s$ constructs a $(1,4)$ FT-ABFS structure rooted at $s$ with at most $O(n^{4/3})$ edges. 	
1406.6170v1	http://arxiv.org/pdf/1406.6170v1	2014	Distributed Storage Systems based on Equidistant Subspace Codes	Netanel Raviv|Tuvi Etzion	  Distributed storage systems based on equidistant constant dimension codes are presented. These equidistant codes are based on the Pl\"{u}cker embedding, which is essential in the repair and the reconstruction algorithms. These systems posses several useful properties such as high failure resilience, minimum bandwidth, low storage, simple algebraic repair and reconstruction algorithms, good locality, and compatibility with small fields. 	
1411.1700v1	http://arxiv.org/pdf/1411.1700v1	2014	On Geometry and Topology of 4-Orbifolds	Dmytro Yeroshkin	  We prove an analogue of the result of Hsiang and Kleiner for 4-dimensional compact orbifolds with positive curvature and an isometric circle action. Additionally, we prove that when the underlying space is simply connected, then the orbifold fundamental group provides a bound on the failure of integer-valued Poincare Duality of the underlying space, and if the orbifold is simply connected, then ineteger-valued Poincre Duality holds for the underlying space. 	
1412.1242v2	http://arxiv.org/pdf/1412.1242v2	2015	A universal divergence rate for symmetric Birkhoff Sums in infinite   ergodic theory	Zemer Kosloff	  We show that there exists a universal gap in the failure of the ergodic theorem for symmetric Birkhoff sums in infinite ergodic theory. In addition, an application of this result to a question of fluctuations of the Birkhoff integrals of horocyclic flows on geometrically finite surfaces is given. 	
1412.8734v1	http://arxiv.org/pdf/1412.8734v1	2014	Fibration by non-smooth projective curves of arithmetic genus two in   characteristic two	Alejandro Simarra Cañate|Karl-Otto Stöhr	  Looking in positive characteristic for failures of the Bertini-Sard theorem, we determine, up to birational equivalence, the separable proper morphisms of smooth algebraic varieties in characteristic two, whose fibres are non-smooth curves of arithmetic genus two. 	
1502.00289v1	http://arxiv.org/pdf/1502.00289v1	2015	Radiation damage effects on detectors and eletronic devices in harsh   radiation environment	S. Fiore	  Radiation damage effects represent one of the limits for technologies to be used in harsh radiation environments as space, radiotherapy treatment, high-energy phisics colliders. Different technologies have known tolerances to different radiation fields and should be taken into account to avoid unexpected failures which may lead to unrecoverable damages to scientific missions or patient health. 	
1503.05578v3	http://arxiv.org/pdf/1503.05578v3	2016	Ultraproducts of continuous posets	H. Andréka|Z. Gyenis|I. Németi	  It is known that nontrivial ultraproducts of complete partially ordered sets (posets) are almost never complete. We show that complete additivity of functions is preserved in ultraproducts of posets. Since failure of this property is clearly preserved by ultraproducts, this implies that complete additivity of functions is an elementary property. 	
1504.08188v1	http://arxiv.org/pdf/1504.08188v1	2015	The theorem that was none - I. Early history	Alfred Gautschy	  The early history of the Vogt-Russell theorem is retraced following its route starting at the realization of a correlation between mass and luminosity of binary and pulsating stars, through the embossing of this observation into a theorem, and finally to the emerging first signs of its failure to serve as a theorem in the strict mathematical sense of the word. 	
1505.02877v1	http://arxiv.org/pdf/1505.02877v1	2015	The polyharmonic heat flow of closed plane curves	Scott Parkins|Glen Wheeler	  In this paper we consider the polyharmonic heat flow of a closed curve in the plane. Our main result is that closed initial data with initially small normalised oscillation of curvature and isoperimetric defect flows exponentially fast in the C^infty-topology to a simple circle. Our results yield a characterisation of the total amount of time during which the flow is not strictly convex, quantifying in a sense the failure of the maximum principle. 	
1505.06057v1	http://arxiv.org/pdf/1505.06057v1	2015	A problem in non-linear Diophantine approximation	Stephen Harrap|Mumtaz Hussain|Simon Kristensen	  In this paper we obtain the Lebesgue and Hausdorff measure results for the set of vectors satisfying infinitely many fully non-linear Diophantine inequalities. The set is also associated with a class of linear inhomogeneous partial differential equations whose solubility is related to a certain Diophantine condition. The failure of the Diophantine condition guarantees the existence of a smooth solution. 	
1506.08431v1	http://arxiv.org/pdf/1506.08431v1	2015	The Besicovitch-Federer projection theorem is false in every infinite   dimensional Banach space	David Bate|Marianna Csörnyei|Bobby Wilson	  We construct a purely unrectifiable set of finite $\mathcal H^1$-measure in every infinite dimensional separable Banach space $X$ whose image under every $0\neq x^*\in X^*$ has positive Lebesgue measure. This demonstrates completely the failure of the Besicovitch-Federer projection theorem in infinite dimensional Banach spaces. 	
1507.01211v1	http://arxiv.org/pdf/1507.01211v1	2015	Haar projection numbers and failure of unconditional convergence in   Sobolev spaces	Andreas Seeger|Tino Ullrich	  For $1<p<\infty$ we determine the precise range of $L_p$ Sobolev spaces for which the Haar system is an unconditional basis. We also consider the natural extensions to Triebel-Lizorkin spaces and prove upper and lower bounds for norms of projection operators depending on properties of the Haar frequency set. 	
1510.02047v1	http://arxiv.org/pdf/1510.02047v1	2015	Folding Catastrophes due to Viscosity in Multiferroic Domains:   Implications for Room-Temperature Multiferroic Switching	J. F. Scott	  Unusual domains with curved walls and failure to satisfy the Landau-Lifshitz-Kittel Law are modeled as folding catastrophes (saddle-node bifurcations). This description of ballistic motion in a viscous medium is based upon early work by Dawber et al., Appl. Phys. Lett. 82, 436 (2003). It suggests that ferroelectric films can exhibit folds or vortex patterns but not both. 	
1510.03287v1	http://arxiv.org/pdf/1510.03287v1	2015	A general tool for consistency results related to I1	Vincenzo Dimonte|Liuzhen Wu	  In this paper we provide a general tool to prove the consistency of $I1(\lambda)$ with various combinatorial properties at $\lambda$ typical at settings with $2^\lambda>\lambda^+$, that does not need a profound knowledge of the forcing notions involved. Examples of such properties are the first failure of GCH, a very good scale and the negation of the approachability property, or the tree property at $\lambda^+$ and $\lambda^{++}$. 	
1511.00212v1	http://arxiv.org/pdf/1511.00212v1	2015	Exploiting Redundant Computation in Communication-Avoiding Algorithms   for Algorithm-Based Fault Tolerance	Camille Coti	  Communication-avoiding algorithms allow redundant computations to minimize the number of inter-process communications. In this paper, we propose to exploit this redundancy for fault-tolerance purpose. We illustrate this idea with QR factorization of tall and skinny matrices, and we evaluate the number of failures our algorithm can tolerate under different semantics. 	
