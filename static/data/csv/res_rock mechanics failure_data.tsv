id	link	date	title	authors	summary
1210.3024v1	http://arxiv.org/pdf/1210.3024v1	2012	Sensitivity analysis of GSI based mechanical characterization of rock   mass	P. Ván|B. Vásárhelyi	  Recently, the rock mechanical and rock engineering designs and calculations are frequently based on Geological Strength Index (GSI) method, because it is the only system that provides a complete set of mechanical properties for design purpose. Both the failure criteria and the deformation moduli of the rock mass can be calculated with GSI based equations, which consists of the disturbance factor, as well. The aim of this paper is the sensitivity analysis of GSI and disturbance factor dependent equations that characterize the mechanical properties of rock masses. The survey of the GSI system is not our purpose. The results show that the rock mass strength calculated by the Hoek-Brown failure criteria and both the Hoek-Diederichs and modified Hoek-Diederichs deformation moduli are highly sensitive to changes of both the GSI and the D factor, hence their exact determination is important for the rock engineering design. 	
0808.3886v2	http://arxiv.org/pdf/0808.3886v2	2009	Stress dependent thermal pressurization of a fluid-saturated rock	Siavash Ghabezloo|Jean Sulem	  Temperature increase in saturated porous materials under undrained conditions leads to thermal pressurization of the pore fluid due to the discrepancy between the thermal expansion coefficients of the pore fluid and of the solid matrix. This increase in the pore fluid pressure induces a reduction of the effective mean stress and can lead to shear failure or hydraulic fracturing. The equations governing the phenomenon of thermal pressurization are presented and this phenomenon is studied experimentally for a saturated granular rock in an undrained heating test under constant isotropic stress. Careful analysis of the effect of mechanical and thermal deformation of the drainage and pressure measurement system is performed and a correction of the measured pore pressure is introduced. The test results are modelled using a non-linear thermo-poro-elastic constitutive model of the granular rock with emphasis on the stress-dependent character of the rock compressibility. The effects of stress and temperature on thermal pressurization observed in the tests are correctly reproduced by the model. 	
0709.2647v2	http://arxiv.org/pdf/0709.2647v2	2007	Rupture by damage accumulation in rocks	David Amitrano	  The deformation of rocks is associated with microcracks nucleation and propagation, i.e. damage. The accumulation of damage and its spatial localization lead to the creation of a macroscale discontinuity, so-called "fault" in geological terms, and to the failure of the material, i.e. a dramatic decrease of the mechanical properties as strength and modulus. The damage process can be studied both statically by direct observation of thin sections and dynamically by recording acoustic waves emitted by crack propagation (acoustic emission). Here we first review such observations concerning geological objects over scales ranging from the laboratory sample scale (dm) to seismically active faults (km), including cliffs and rock masses (Dm, hm). These observations reveal complex patterns in both space (fractal properties of damage structures as roughness and gouge), time (clustering, particular trends when the failure approaches) and energy domains (power-law distributions of energy release bursts). We use a numerical model based on progressive damage within an elastic interaction framework which allows us to simulate these observations. This study shows that the failure in rocks can be the result of damage accumulation. 	
1203.0850v2	http://arxiv.org/pdf/1203.0850v2	2012	Damage-cluster distributions and size effect on strength in compressive   failure	Lucas Girard|Jerome Weiss|David Amitrano	  We investigate compressive failure of heterogeneous materials on the basis of a continuous progressive damage model. The model explicitely accounts for tensile and shear local damage and reproduces the main features of compressive failure of brittle materials like rocks or ice. We show that the size distribution of damage-clusters, as well as the evolution of an order parameter, the size of the largest damage-cluster, argue for a critical interpretation of fracture. The compressive failure strength follows a normal distribution with a very small size effect on the mean strength, in good agreement with experiments. 	
0808.4081v1	http://arxiv.org/pdf/0808.4081v1	2008	Numerical modelling of the effect of weathering on the progressive   failure of underground limestone mines	Siavash Ghabezloo|Ahmad Pouya	  The observations show that the collapse of underground limestone mines results from a progressive failure due to gradual weathering of the rockmass. The following stages can be considered for the limestone weathering and degradation process in underground mines: condensation of the water on the roof of the gallery, infiltration of water in the porous rock, migration of the air CO2 molecules in the rock pore water by convection and molecular diffusion, dissolution of limestone by CO2 rich water and consequently, reduction of the strength properties of rock. Considering this process, a set of equations governing different hydrochemo-mechanical aspects of the weathering phenomenon and progressive failure occurring in these mines is presented. Then the feasibility of numerical modelling of this process is studied and a simple example of application is presented. 	
1605.06178v1	http://arxiv.org/pdf/1605.06178v1	2016	Lattice Discrete Particle Model (LDPM) for pressure-dependent   inelasticity in granular rocks	Shiva Esna Ashari|Giuseppe Buscarnera|Gianluca Cusatis	  This paper deals with the formulation, calibration, and validation of a Lattice Discrete Particle Model (LDPM) for the simulation of the pressure-dependent inelastic response of granular rocks. LDPM is formulated in the framework of discrete mechanics and it simulates the heterogeneous deformation of cemented granular systems by means of discrete compatibility/equilibrium equations defined at the grain scale. A numerical strategy is proposed to generate a realistic microstructure based on the actual grain size distribution of a sandstone and the capabilities of the method are illustrated with reference to the particular case of Bleurswiller sandstone, i.e. a granular rock that has been extensively studied at the laboratory scale. LDPM micromechanical parameters are calibrated based on evidences from triaxial experiments, such as hydrostatic compression, brittle failure at low confinement and plastic behavior at high confinement. Results show that LDPM allows exploring the effect of fine-scale heterogeneity on the inelastic response of rock cores, achieving excellent quantitative performance across a wide range of stress conditions. In addition, LDPM simulations demonstrate its capability of capturing different modes of strain localization within a unified mechanical framework, which makes this approach applicable for a wide variety of geomechanical settings. Such promising performance suggests that LDPM may constitute a viable alternative to existing discrete numerical methods for granular rocks, as well as a versatile tool for the interpretation of their complex deformation/failure patterns and for the development of continuum models capturing the effect of micro-scale heterogeneity. 	
1705.03377v1	http://arxiv.org/pdf/1705.03377v1	2017	Real time observation of granular rock analogue material deformation and   failure using nonlinear laser interferometry	Pierre Walczak|Francesco Mezzapesa|Abderrahmane Bouakline|Julien Ambre|Stéphane Bouissou|Stéphane Barland	  A better understanding and anticipation of natural processes such as landsliding or seismic fault activity requires detailed theoretical and experimental analysis of rock mechanics and geomaterial dynamics. These last decades, considerable progress has been made towards understanding deformation and fracture process in laboratory experiment on granular rock materials, as the well-known shear banding experiment. One of the reasons for this progress is the continuous improvement in the instrumental techniques of observation. But the lack of real time methods does not allow the detection of indicators of the upcoming fracture process and thus to anticipate the phenomenon. Here, we have performed uniaxial compression experiments to analyse the response of a granular rock material sample to different shocks. We use a novel interferometric laser sensor based on the nonlinear self-mixing interferometry technique to observe in real time the deformations of the sample and assess its usefulness as a diagnostic tool for the analysis of geomaterial dynamics. Due to the high spatial and temporal resolution of this approach, we observe both vibrations processes in response to a dynamic loading and the onset of failure. The latter is preceded by a continuous variation of vibration period of the material. After several shocks, the material response is no longer reversible and we detect a progressive accumulation of irreversible deformation leading to the fracture process. We demonstrate that material failure is anticipated by the critical slowing down of the surface vibrational motion, which may therefore be envisioned as an early warning signal or predictor to the macroscopic failure of the sample. The nonlinear self-mixing interferometry technique is readily extensible to fault propagation measurements. As such, it opens a new window of observation for the study of geomaterial deformation and failure. 	
1406.1052v2	http://arxiv.org/pdf/1406.1052v2	2014	A Conceptual Approach to Two-Scale Constitutive Modelling For   Hydro-Mechanical Coupling	Giang D. Nguyen|Abbas El-Zein|Terry Bennett	  Large scale modelling of fluid flow coupled with solid failure in geothermal reservoirs or hydrocarbon extraction from reservoir rocks usually involves behaviours at two scales: lower scale of the inelastic localization zone, and larger scale of the bulk continuum where elastic behaviour can be reasonably assumed. The hydraulic conductivities corresponding to the mechanical properties at these two scales are different. In the bulk elastic host rock, the hydraulic conductivity does not vary much with the deformation, while it significantly changes in the lower scale of the localization zone due to inelastic deformation. Increase of permeability due to fracture and/or dilation, or reduction of permeability due to material compaction can take place inside this zone. The challenge is to predict the evolution of hydraulic conductivities coupled with the mechanical behaviour of the material in all stages of the deformation process. In the early stage of diffuse deformation, the permeability of the material can be reasonably assumed to be homogenous over the whole Representative Volume Element (RVE) However, localized failure results in distinctly different conductivities in different parts of the RVE. This paper establishes a general framework and corresponding field equations to describe the hydro-mechanical coupling in both diffuse and localized stages of deformation in rocks. In particular, embedding the lower scale hydro-mechanical behaviour of the localization zone inside an elastic bulk, together with their corresponding effective sizes, helps effectively deal with scaling issues in large-scale modelling. Preliminary results are presented which demonstrate the promising features of this new approach. 	
1710.04182v1	http://arxiv.org/pdf/1710.04182v1	2017	Modeling Dynamic Helium Release as a Tracer of Rock Deformation	W. Payton Gardner|Stephen J. Bauer|Kristopher L. Kuhlman|Jason E. Heath	  We use helium released during mechanical deformation of shales as a signal to explore the effects of deformation and failure on material transport properties. A dynamic dual-permeability model with evolving pore and fracture networks is used to simulate gases released from shale during deformation and failure. Changes in material properties required to reproduce experimentally observed gas signals are explored. We model two different experiments of $^4$He flow rate measured from shale undergoing mechanical deformation, a core parallel to bedding and a core perpendicular to bedding. We find that the helium signal is sensitive to fracture development and evolution as well as changes in the matrix transport properties. We constrain the timing and effective fracture aperture, as well as the increase in matrix porosity and permeability. Increases in matrix permeability are required to explain gas flow prior to macroscopic failure, and the short-term gas flow post failure. Increased matrix porosity, is required to match the long-term, post-failure gas flow. Our model provides the first quantitative interpretation of helium release as a result of mechanical deformation. The sensitivity of this model to changes in the fracture network, as well as to matrix properties during deformation, indicates that helium release can be used as a quantitative tool to evaluate the state of stress and strain in earth materials. 	
1111.4924v1	http://arxiv.org/pdf/1111.4924v1	2011	The precursory electric signals, observed before the Izmit Turkey EQ (Mw   = 7.6, August 17th, 1999), analyzed in terms of a hypothetically   pre-activated, in the focal area, large scale piezoelectric mechanism	C. Thanassoulas|V. Klentos	  The generated, prior to the Izmit Turkey large EQ, preseismic electric signals were recorded in Greece by the VOL Earth's electric field monitoring site. In order to explain their peculiar character and their generating mechanism, a large scale piezoelectric mechanism was assumed that was initiated in the Izmit seismogenic region long before the EQ occurrence time. The theoretical analysis of the adopted physical model justifies the generation of a number of specific electric signals that can be emitted from the focal area before the rock formation failure. The processing of the registered by the VOL monitoring site raw data revealed the presence of similar signals as the expected theoretical ones. Therefore, it is concluded that long before the Izmit EQ occurrence a large scale piezoelectric mechanism was initiated that was modulated too by the tidally triggered lithospheric oscillation and therefore generated the observed preseismic electric signals. The adopted piezoelectric model provides critical information about the time of occurrence of the seismogenic area rock formation failure and therefore the possibility for a real short-term time prediction of a large EQ. The other two predictive EQ parameters, location and magnitude, are discussed in the frame of electric field triangulation and the Lithospheric Seismic Energy Flow Model (LSEFM). 	
1005.3385v1	http://arxiv.org/pdf/1005.3385v1	2010	Current reversals in a rocking ratchet: dynamical vs symmetry-breaking   mechanisms	D. Cubero|V. Lebedev|F. Renzoni	  Directed transport in ratchets is determined by symmetry-breaking in a system out of equilibrium. A hallmark of rocking ratchets is current reversals: an increase in the rocking force changes the direction of the current. In this work for a bi-harmonically driven spatially symmetric rocking ratchet we show that a class of current reversal is precisely determined by symmetry-breaking, thus creating a link between dynamical and symmetry-breaking mechanisms. 	
1306.6587v1	http://arxiv.org/pdf/1306.6587v1	2013	SPH-based simulation of multi-material asteroid collisions	Thomas I. Maindl|Christoph Schäfer|Roland Speith|Áron Süli|Emese Forgács-Dajka|Rudolf Dvorak	  We give a brief introduction to smoothed particle hydrodynamics methods for continuum mechanics. Specifically, we present our 3D SPH code to simulate and analyze collisions of asteroids consisting of two types of material: basaltic rock and ice. We consider effects like brittle failure, fragmentation, and merging in different impact scenarios. After validating our code against previously published results we present first collision results based on measured values for the Weibull flaw distribution parameters of basalt. 	
0805.1802v1	http://arxiv.org/pdf/0805.1802v1	2008	Depinning transition in failure of inhomogeneous brittle materials	Laurent Ponson	  The dynamics of a crack propagating in an elastic inhomogeneous material is investigated. The variations of the average crack velocity with the external loading are measured for a brittle rock and are shown to display two distinct regimes: Below a given threshold Gc, the crack velocity is well described by an exponential law v ~ exp^{-(C/(G-(Gamma))} characteristic of subcritical propagation, while for larger values of the driving force G > Gc, the velocity evolves as a power law v ~ (G - G_c)^theta with theta = 0.80 $\pm$ 0.15. These results can be explained extending the continuum theory of Fracture Mechanics to disordered systems. In this description, the motion of a crack is analogue to the one of an elastic line driven in a random medium and critical failure occurs when the loading is sufficiently large to depinne the crack front from the heterogeneities of the material. 	
1607.07360v2	http://arxiv.org/pdf/1607.07360v2	2016	A cohesive granular material with tunable elasticity	Arnaud Hemmerle|Matthias Schröter|Lucas Goehring	  By mixing glass beads with a curable polymer we create a well-defined cohesive granular medium, held together by solidified, and hence elastic, capillary bridges. This material has a geometry similar to a wet packing of beads, but with an additional control over the elasticity of the bonds holding the particles together. We show that its mechanical response can be varied over several orders of magnitude by adjusting the size and stiffness of the bridges, and the size of the particles. We also investigate its mechanism of failure under unconfined uniaxial compression in combination with in situ x-ray microtomography. We show that a broad linear-elastic regime ends at a limiting strain of about 8%, whatever the stiffness of the agglomerate, which corresponds to the beginning of shear failure. The possibility to finely tune the stiffness, size and shape of this simple material makes it an ideal model system for investigations on, for example, fracturing of porous rocks, seismology, or root growth in cohesive porous media. 	
1302.0483v1	http://arxiv.org/pdf/1302.0483v1	2013	Multifractal analysis of the pore space of real and simulated   sedimentary rocks	Abhra Giri|Sujata Tarafdar|Philippe Gouze|Tapati Dutta	  It is well known that sedimentary rocks having same porosity can have very different pore size distribution. The pore distribution determines many characteristics of the rock among which, its transport property is often the most useful. Multifractal analysis is a powerful tool that is increasingly used to characterize the pore space. In this study we have done multifractal analysis of pore distribution on sedimentary rocks simulated using the Relaxed Bidisperse Ballistic Model (RBBDM). The RBBDM can generate a $3-D$ structure of sedimentary rocks of variable porosity by tuning the fraction $p$ of particles of two different sizes. We have also done multifractal analysis on two samples of real sedimentary rock to compare with the simulation studies. One sample, an oolitic limestone is of high porosity (40%)while the other is a reefal carbonate of low porosity around 7%. $2-D$ sections of X-ray micro-tomographs of the real rocks were stacked sequentially to reconstruct the real rock specimens. Both samples show a multifractal character, but we show that RBBDM gives a very realistic representation of a typical high porosity sedimentary rock. 	
0312138v2	http://arxiv.org/pdf/cond-mat/0312138v2	2005	Power Laws, Precursors and Predictability During Failure	Rumi De|G. Ananthakrishna	  We investigate the dynamics of a modified Burridge-Knopoff model by introducing a dissipative term to mimic the bursts of acoustic emission (AE) from rock samples. The model explains many features of the statistics of AE signals observed in experiments such as the crossover in the exponent value from relatively small amplitude AE signals to larger regime, and their dependence on the pulling speed. Significantly, we find that the cumulative energy dissipated identified with acoustic emission can be used to predict a major slip event. We also find a data collapse of the acoustic activity for several major slip events describable by a universal stretched exponential with corrections in terms of time-to-failure. 	
1712.02320v1	http://arxiv.org/pdf/1712.02320v1	2017	Elastic, strength, and fracture properties of Marcellus shale	Zhefei Jin|Weixin Li|Congrui Jin|James Hambleton|Congrui Jin|Gianluca Cusatis	  Shale, a fine-grained sedimentary rock, is the key source rock for many of the world's most important oil and natural gas deposits. A deep understanding of the mechanical properties of shale is of vital importance in various geotechnical applications, including oil and gas exploitation. In this work, deformability, strength, and fracturing properties of Marcellus shale were investigated through an experimental study. Firstly, uniaxial compression, direct tension, and Brazilian tests were performed on the Marcellus shale specimens in various bedding plane orientations with respect to loading directions to measure the static mechanical properties and their anisotropy. Furthermore, the deformability of Marcellus shale was also studied through seismic velocity measurements for comparison with the static measurements. The experimental results revealed that the transversely isotropic model is applicable for describing the elastic behaviors of Marcellus shale in pure tension and compression. The elastic properties measured from these two experiments, however, were not exactly the same. Strength results showed that differences exist between splitting (Brazilian) and direct tensile strengths, both of which varied with bedding plane orientations and loading directions and were associated with different failure modes. Finally, a series of three-point-bending tests were conducted on specimens of increasing size in three different principal notch orientations to investigate the fracture properties of the material. It was found that there exists a significant size effect on the fracture properties calculated from the measured peak loads and by using the Linear Elastic Fracture Mechanics (LEFM) theory. The fracture properties can be uniquely identified, however, by using Bazant's Size Effect Law and they were found to be anisotropic. 	
1103.3033v1	http://arxiv.org/pdf/1103.3033v1	2011	Ellipsoidal anisotropy in elasticity for rocks and rock masses	Ahmad Pouya|Michel Chalhoub	  One of the interesting features with the ellipsoidal models of anisotropy presented in this paper is their acceptance of analytical solutions for some of the basic elasticity problems. It was shown by Pouya (2000) and Pouya and Zaoui (2006) that many closed-form solutions for basic problems involving linear isotropic materials could be extended by linear transformation to cover a variety of "ellipsoidal" materials. This paper will describe two main varieties of ellipsoidal elastic models and show how well they fit the in situ data for sedimentary rocks; numerical homogenization results for several varieties of fractured rock masses will also be provided. 	
1111.6568v2	http://arxiv.org/pdf/1111.6568v2	2011	Polyhedral colloidal `rocks': low-dimensional networks	Rebecca Rice|Roland Roth|C. Patrick Royall	  We introduce a model system of anisotropic colloidal `rocks'. Due to their shape, the bonding introduced via non-absorbing polymers is profoundly different from spherical particles: bonds between rocks are rigid against rotation, leading to strong frustration. We develop a geometric model which captures the essence of the rocks. Experiments and simulations show that the colloid geometry leads to structures of low fractal dimension. This is in stark contrast to gels of spheres, whose rigidity results from locally dense regions. At high density the rocks form a quasi one-component glass. 	
0801.0559v1	http://arxiv.org/pdf/0801.0559v1	2008	Failure patterns caused by localized rise in pore-fluid overpressure and   effective strength of rocks	Alexander Rozhko|Yuri Podladchikov|François Renard	  In order to better understand the interaction between pore-fluid overpressure and failure patterns in rocks we consider a porous elasto-plastic medium in which a laterally localized overpressure line source is imposed at depth below the free surface. We solve numerically the fluid filtration equation coupled to the gravitational force balance and poro-elasto-plastic rheology equations. Systematic numerical simulations, varying initial stress, intrinsic material properties and geometry, show the existence of five distinct failure patterns caused by either shear banding or tensile fracturing. The value of the critical pore-fluid overpressure at the onset of failure is derived from an analytical solution that is in excellent agreement with numerical simulations. Finally, we construct a phase-diagram that predicts the domains of the different failure patterns and at the onset of failure. 	
0908.4140v1	http://arxiv.org/pdf/0908.4140v1	2009	Investigation of a hydraulic impact: a technology in rock breaking	Martin Genet|Wenyi Yan|Thanh Tran-Cong	  The finite element method and dimensional analysis have been applied in the present paper to study a hydraulic impact, which is utilized in a non-explosive rock breaking technology in mining industry. The impact process of a high speed piston on liquid water, previously introduced in a borehole drilled in rock, is numerically simulated. The research is focused on the influences of all the parameters involved in the technology on the largest principal stress in the rock, which is considered as one of the key factors to break the rock. Our detailed parametric investigation reveals that the variation of the isotropic rock material properties, especially its density, has no significant influence on the largest principal stress. The influences of the depth of the hole and the depth of the water column are also very small. On the other hand, increasing the initial kinetic energy of the piston can dramatically increase the largest principal stress and the best way to increase the initial kinetic energy of the piston is to increase its initial velocity. Results from the current dimensional analysis can be applied to optimize this non-explosive rock breaking technology. 	
1608.04144v2	http://arxiv.org/pdf/1608.04144v2	2016	A multiscale framework for the simulation of the anisotropic mechanical   behavior of shale	Weixin Li|Roozbeh Rezakhani|Congrui Jin|Xinwei Zhou|Gianluca Cusatis	  Shale, like many other sedimentary rocks, is typically heterogeneous, anisotropic, and is characterized by partial alignment of anisotropic clay minerals and naturally formed bedding planes. In this study, a micromechanical framework based on the Lattice Discrete Particle Model (LDPM) is formulated to capture these features. Material anisotropy is introduced through an approximated geometric description of shale internal structure, which includes representation of material property variation with orientation and explicit modeling of parallel lamination. The model is calibrated by carrying out numerical simulations to match various experimental data, including the ones relevant to elastic properties, Brazilian tensile strength, and unconfined compressive strength. Furthermore, parametric study is performed to investigate the relationship between the mesoscale parameters and the macroscopic properties. It is shown that the dependence of the elastic stiffness, strength, and failure mode on loading orientation can be captured successfully. Finally, a homogenization approach based on the asymptotic expansion of field variables is applied to upscale the proposed micromechanical model, and the properties of the homogenized model are analyzed. 	
0902.3941v2	http://arxiv.org/pdf/0902.3941v2	2009	Rocking feedback controlled ratchets	M. Feito|J. P. Baltanas|F. J. Cao	  We investigate the different regimes that emerge when a periodic driving force, the rocking force, acts on a collective feedback flashing ratchet. The interplay of the rocking and the feedback control gives a rich dynamics with different regimes presenting several unexpected novel features. In particular, we show that for both the one-particle ratchet and the collective version of the ratchet an appropriate rocking increases the flux. This mechanism gives the maximum flux that has been achieved in a ratchet device without an a priori bias. 	
1204.0917v1	http://arxiv.org/pdf/1204.0917v1	2012	Reactive-infiltration instabilities in rocks. Fracture dissolution	Piotr Szymczak|Anthony J. C. Ladd	  A reactive fluid dissolving the surface of a uniform fracture will trigger an instability in the dissolution front, leading to spontaneous formation of pronounced well-spaced channels in the surrounding rock matrix. Although the underlying mechanism is similar to the wormhole instability in porous rocks there are significant differences in the physics, due to the absence of a steadily propagating reaction front. In previous work we have described the geophysical implications of this instability in regard to the formation of long conduits in soluble rocks. Here we describe a more general linear stability analysis, including axial diffusion, transport limited dissolution, non-linear kinetics, and a finite length system. 	
1205.6299v1	http://arxiv.org/pdf/1205.6299v1	2012	Interaction between Injection Points during Hydraulic Fracturing	Kjetil M. D. Hals|Inga Berre	  We present a model of the hydraulic fracturing of heterogeneous poroelastic media. The formalism is an effective continuum model that captures the coupled dynamics of the fluid pressure and the fractured rock matrix and models both the tensile and shear failure of the rock. As an application of the formalism, we study the geomechanical stress interaction between two injection points during hydraulic fracturing (hydrofracking) and how this interaction influences the fracturing process. For injection points that are separated by less than a critical correlation length, we find that the fracturing process around each point is strongly correlated with the position of the neighboring point. The magnitude of the correlation length depends on the degree of heterogeneity of the rock and is on the order of 30-45 m for rocks with low permeabilities. In the strongly correlated regime, we predict a novel effective fracture-force that attracts the fractures toward the neighboring injection point. 	
1205.6418v1	http://arxiv.org/pdf/1205.6418v1	2012	Comment on "Compositional and Microchemical Evidence of Piezonuclear   Fission Reactions in Rock Specimens Subjected to Compression Tests" [Strain   47 (Suppl. 2), 282 (2011)]	G. Amato|G. Bertotti|O. Bottauscio|G. Crotti|F. Fiorillo|G. Mana|M. L. Rastello|P. Tavella|F. Vinai	  It is shown that the chemical composition data published by Carpinteri et al. in the article "Compositional and Microchemical Evidence of Piezonuclear Fission Reactions in Rock Specimens Subjected to Compression Tests" [Strain 47 (Suppl. 2), 282 (2011)] cannot be the result of independent measurements as claimed by the authors. Therefore, no conclusion can be drawn from them about compositional modifications induced in the stone by hypothetical piezonuclear reactions taking place during catastrophic failure of the material at fracture. 	
0709.3028v1	http://arxiv.org/pdf/0709.3028v1	2007	Emerging complexity in a simple model of the mechanical behaviour of   rocks	David Amitrano	  We propose a mechanical model for the behaviour of rocks based on progressive damage at the elementary scale and elastic interaction. It allows us to simulate several experimental observations: mechanical behaviour ranging from brittle to ductile, fractal structure of the damage, powerlaw distribution of the damage avalanches. These macroscopic properties are not incorporated at the elementary scale, but are the results of the interaction between elements. This emerging complexity permits us to consider the strain rock process as a complex system characterized by non-linear dynamics. 	
0606290v1	http://arxiv.org/pdf/cond-mat/0606290v1	2006	Bistable phase control via rocking in a nonlinear electronic oscillator	Javier M. Buldu|K. Staliunas|J. A. Casals|Jordi Garcia-Ojalvo	  We experimentally demonstrate the effective rocking of a nonlinear electronic circuit operating in a periodic regime. Namely, we show that driving a Chua circuit with a periodic signal, whose phase alternates (also periodically) in time, we lock the oscillation frequency of the circuit to that of the driving signal, and its phase to one of two possible values shifted by pi, and lying between the alternating phases of the input signal. In this way, we show that a rocked nonlinear oscillator displays phase bistability. We interpret the experimental results via a theoretical analysis of rocking on a simple oscillator model, based on a normal form description (complex Landau equation) of the rocked Hopf bifurcation 	
1111.6520v1	http://arxiv.org/pdf/1111.6520v1	2011	Two-dimensional rocking ratchet for cold atoms	V. Lebedev|F. Renzoni	  We investigate experimentally a two-dimensional rocking ratchet for cold atoms, realized by using a driven three-beam dissipative optical lattice. AC forces are applied in perpendicular directions by phase-modulating two of the lattice beams. As predicted by the general theory [S. Denisov et al., Phys. Rev. Lett. 100, 224102 (2008)], we observe a rectification phenomenon unique to high-dimensional rocking ratchets, as determined by two single-harmonic drivings applied in orthogonal directions. Also, by applying two bi-harmonic forces in perpendicular directions, we demonstrate the possibility of generating a current in an arbitrary direction within the optical lattice plane. 	
1201.0065v1	http://arxiv.org/pdf/1201.0065v1	2011	Analysis of wasp-waisted hysteresis loops in magnetic rocks	R S Kharwanlang|Prabodh Shukla	  The random-field Ising model of hysteresis is generalized to dilute magnets and solved on a Bethe lattice. Exact expressions for the major and minor hysteresis loops are obtained. In the strongly dilute limit the model provides a simple and useful understanding of the shapes of hysteresis loops in magnetic rock samples. 	
1402.6475v1	http://arxiv.org/pdf/1402.6475v1	2014	Rupture cascades in a discrete element model of a porous sedimentary   rock	F. Kun|I. Varga|S. Lennartz-Sassinek|I. G. Main	  We investigate the scaling properties of the sources of crackling noise in a fully-dynamic numerical model of sedimentary rocks subject to uniaxial compression. The model is initiated by filling a cylindrical container with randomly-sized spherical particles which are then connected by breakable beams. Loading at a constant strain rate the cohesive elements fail and the resulting stress transfer produces sudden bursts of correlated failures, directly analogous to the sources of acoustic emissions in real experiments. The source size, energy, and duration can all be quantified for an individual event, and the population analyzed for their scaling properties, including the distribution of waiting times between consecutive events. Despite the non-stationary loading, the results are all characterized by power law distributions over a broad range of scales in agreement with experiments. As failure is approached temporal correlation of events emerge accompanied by spatial clustering. 	
0709.2651v1	http://arxiv.org/pdf/0709.2651v1	2007	Seismic precursory patterns before a cliff collapse and critical-point   phenomena	David Amitrano|Jean Robert Grasso|Gloria Senfaute	  We analyse the statistical pattern of seismicity before a 1-2 103 m3 chalk cliff collapse on the Normandie ocean shore, Western France. We show that a power law acceleration of seismicity rate and energy in both 40 Hz-1.5 kHz and 2 Hz-10kHz frequency range, is defined on 3 order of magnitude, within 2 hours from the collapse time. Simultaneously, the average size of the seismic events increases toward the time to failure. These in-situ results are derived from the only station located within one rupture length distance from the rock fall rupture plane. They mimic the "critical point" like behavior recovered from physical and numerical experiments before brittle failures and tertiary creep failures. Our analysis of this first seismic monitoring data of a cliff collapse suggests that the thermodynamic phase transition models for failure may apply for cliff collapse. 	
0902.1311v1	http://arxiv.org/pdf/0902.1311v1	2009	Rock Joint Surfaces Measurement and Analysis of Aperture Distribution   under Different Normal and Shear Loading Using GIS	Mostafa Sharifzadeh|Yasuhiro Mitani|Tetsuro Esaki	  Geometry of the rock joint is a governing factor for joint mechanical and hydraulic behavior. A new method of evaluating aperture distribution based on measurement of joint surfaces and three dimensional characteristics of each surface is developed. Artificial joint of granite surfaces are measured,processed, analyzed and three dimensional approaches are carried out for surface characterization. Parameters such as asperity's heights, slope angles, and aspects distribution at micro scale,local concentration of elements and their spatial localization at local scale are determined by Geographic Information System (GIS). Changes of aperture distribution at different normal stresses and various shear displacements are visualized and interpreted. Increasing normal load causes negative changes in aperture frequency distribution which indicates high joint matching. However, increasing shear displacement causes a rapid increase in the aperture and positive changes in the aperture frequency distribution which could be due to unmatching, surface anisotropy and spatial localization of contact points with proceeding shear. 	
1709.01847v2	http://arxiv.org/pdf/1709.01847v2	2017	Three-Dimensional Numerical Modeling of Shear Stimulation of Naturally   Fractured Reservoirs	Eren Ucar|Inga Berre|Eirik Keilegavlen	  Shear dilation based hydraulic stimulations enable exploitation of geothermal energy from reservoirs with inadequate initial permeability. While contributing to enhancing the reservoir's permeability, hydraulic stimulation processes may lead to undesired seismic activity. Here, we present a three dimensional numerical model aiming to increase understanding of this mechanism and its consequences. The fractured reservoir is modeled as a network of explicitly represented large scale fractures immersed in a permeable rock matrix. The numerical formulation is constructed by coupling three physical processes: fluid flow, fracture deformation, and rock matrix deformation. For flow simulations, the discrete fracture matrix model is used, which allows the fluid transport from high permeable conductive fractures to the rock matrix and vice versa. The mechanical behavior of the fractures is modeled using a hyperbolic model with reversible and irreversible deformations. Linear elasticity is assumed for the mechanical deformation and stress alteration of the rock matrix. Fractures are modeled as lower dimensional surfaces embodied in the domain, subjected to specific governing equations for their deformation along the tangential and normal directions. Both the fluid flow and momentum balance equations are approximated by finite volume discretizations. The new numerical model is demonstrated considering a three dimensional fractured formation with a network of 20 explicitly represented fractures. The effects of fluid exchange between fractures and rock matrix on the permeability evolution and the generated seismicity are examined for test cases resembling realistic reservoir conditions. 	
1509.06333v1	http://arxiv.org/pdf/1509.06333v1	2015	Network Capability in Localizing Node Failures via End-to-end Path   Measurements	Liang Ma|Ting He|Ananthram Swami|Don Towsley|Kin K. Leung	  We investigate the capability of localizing node failures in communication networks from binary states (normal/failed) of end-to-end paths. Given a set of nodes of interest, uniquely localizing failures within this set requires that different observable path states associate with different node failure events. However, this condition is difficult to test on large networks due to the need to enumerate all possible node failures. Our first contribution is a set of sufficient/necessary conditions for identifying a bounded number of failures within an arbitrary node set that can be tested in polynomial time. In addition to network topology and locations of monitors, our conditions also incorporate constraints imposed by the probing mechanism used. We consider three probing mechanisms that differ according to whether measurement paths are (i) arbitrarily controllable, (ii) controllable but cycle-free, or (iii) uncontrollable (determined by the default routing protocol). Our second contribution is to quantify the capability of failure localization through (1) the maximum number of failures (anywhere in the network) such that failures within a given node set can be uniquely localized, and (2) the largest node set within which failures can be uniquely localized under a given bound on the total number of failures. Both measures in (1-2) can be converted into functions of a per-node property, which can be computed efficiently based on the above sufficient/necessary conditions. We demonstrate how measures (1-2) proposed for quantifying failure localization capability can be used to evaluate the impact of various parameters, including topology, number of monitors, and probing mechanisms. 	
0803.1308v1	http://arxiv.org/pdf/0803.1308v1	2008	Subsidence and capillary effects in chalks	Pierre Delage|Christian Schroeder|Yu Jun Cui	  Based on the concepts of the mechanics of unsaturated soils where capillary phenomena arise between the wetting fluid (water) and the non-wetting one (air), the subsidence of chalks containing oil (non-wetting fluid) during water injection (wetting fluid) is analysed. It is shown that the collapse phenomenon of unsaturated soils under wetting provides a physical explanation and a satisfactory prediction of the order of magnitude of the subsidence of the chalk. The use of a well established constitutive model for unsaturated soils allows a description of the hydro-mechanical history of the chalk, from its deposition to the oil exploitation. 	
1112.0766v1	http://arxiv.org/pdf/1112.0766v1	2011	Current reversals in a rocking ratchet: the frequency domain	A. Wickenbrock|D. Cubero|N. A. Abdul Wahab|P. Phoonthong|F. Renzoni	  Motivated by recent work [D. Cubero et al., Phys. Rev. E 82, 041116 (2010)], we examine the mechanisms which determine current reversals in rocking ratchets as observed by varying the frequency of the drive. We found that a class of these current reversals in the frequency domain are precisely determined by dissipation-induced symmetry breaking. Our experimental and theoretical work thus extends and generalizes the previously identified relationship between dynamical and symmetry-breaking mechanisms in the generation of current reversals. 	
1311.6700v2	http://arxiv.org/pdf/1311.6700v2	2014	Failure mechanisms of load sharing complex systems	Shahnewaz Siddique|Vitali Volovoi	  We investigate the failure mechanisms of load sharing complex systems. The system is composed of multiple nodes or components whose failures are determined based on the interaction of their respective strengths and loads (or capacity and demand respectively) as well as the ability of a component to share its load with its neighbors when needed. We focus on two distinct mechanisms to model the interaction between components' strengths and loads. The failure mechanisms of these two models demonstrate temporal scaling phenomena, phase transitions and multiple distinct failure modes excited by extremal dynamics. For critical ranges of parameters the models demonstrate power law and exponential failure patterns. We identify the similarities and differences between the two mechanisms and the implications of our results to the failure mechanisms of complex systems in the real world. 	
1502.02300v1	http://arxiv.org/pdf/1502.02300v1	2015	Paradox of Peroxy Defects and Positive Holes in Rocks Part II: Outflow   of Electric Currents from Stressed Rocks	John Scoville|Jaufray Sornette|Friedemann Freund	  Understanding the electrical properties of rocks is of fundamental interest. We report on currents generated when stresses are applied. Loading the center of gabbro tiles, 30x30x0.9 cm$^3$, across a 5 cm diameter piston, leads to positive currents flowing from the center to the unstressed edges. Changing the constant rate of loading over 5 orders of magnitude from 0.2 kPa/s to 20 MPa/s produces positive currents, which start to flow already at low stress levels, <5 MPa. The currents increase as long as stresses increase. At constant load they flow for hours, days, even weeks and months, slowly decreasing with time. When stresses are removed, they rapidly disappear but can be made to reappear upon reloading. These currents are consistent with the stress-activation of peroxy defects, such as O$_3$Si-OO-SiO$_3$, in the matrix of rock-forming minerals. The peroxy break-up leads to positive holes h$^{\bullet}$, i.e. electronic states associated with O$^-$ in a matrix of O$^{2-}$, plus electrons, e'. Propagating along the upper edge of the valence band, the holes are able to flow from stressed to unstressed rock, traveling fast and far by way of a phonon-assisted electron hopping mechanism using energy levels at the upper edge of the valence band. Impacting the tile center leads to h$^{\bullet}$ pulses, 4-6 ms long, flowing outward at ~100 m/sec at a current equivalent to 1-2 x 10$^9$ A/km$^3$. Electrons, trapped in the broken peroxy bonds, are also mobile, but only within the stressed volume. 	
1502.07224v1	http://arxiv.org/pdf/1502.07224v1	2015	An analytical study of seismoelectric signals produced by 1D mesoscopic   heterogeneities	Leonardo B. Monachesi|J. German Rubino|Marina Rosas-Carbajal|Damien Jougnot|Niklas Linde|Beatriz Quintal|Klaus Holliger	  The presence of mesoscopic heterogeneities in fluid-saturated porous rocks can produce measurable seismoelectric signals due to wave-induced fluid flow between regions of differing compressibility. The dependence of these signals on the petrophysical and structural characteristics of the probed rock mass remains largely unexplored. In this work, we derive an analytical solution to describe the seismoelectric response of a rock sample, containing a horizontal layer at its center, that is subjected to an oscillatory compressibility test. We then adapt this general solution to compute the seismoelectric signature of a particular case related to a sample that is permeated by a horizontal fracture located at its center. Analyses of the general and particular solutions are performed to study the impact of different petrophysical and structural parameters on the seismoelectric response. We find that the amplitude of the seismoelectric signal is directly proportional to the applied stress, to the Skempton coefficient contrast between the host rock and the layer, and to a weighted average of the effective excess charge of the two materials. Our results also demonstrate that the frequency at which the maximum electrical potential amplitude prevails does not depend on the applied stress or the Skempton coefficient contrast. In presence of strong permeability variations, this frequency is rather controlled by the permeability and thickness of the less permeable material. The results of this study thus indicate that seismoelectric measurements can potentially be used to estimate key mechanical and hydraulic rock properties of mesoscopic heterogeneities, such as compressibility, permeability, and fracture compliance. 	
1506.04437v1	http://arxiv.org/pdf/1506.04437v1	2015	mTORC1 regulates cytokinesis through activation of Rho-ROCK signaling	Timothy R. Peterson|Mathieu Laplante|Ed Van Veen|Marcel Van Vugt|Carson C. Thoreen|David M. Sabatini	  Understanding the mechanisms by which cells coordinate their size with their ability to divide has long attracted the interest of biologists. The Target of Rapamycin (TOR) pathway is becoming increasingly recognized as a master regulator of cell size, however less is known how TOR activity might be coupled with the cell cycle. Here, we establish that mTOR complex 1 (mTORC1) promotes cytokinesis through activation of a Rho GTPase-Rho Kinase (ROCK) signaling cascade. Hyperactivation of mTORC1 signaling by depletion of any of its negative regulators: TSC1, TSC2, PTEN, or DEPTOR, induces polyploidy in a rapamycin-sensitive manner. mTORC1 hyperactivation-mediated polyploidization occurs by a prolonged, but ultimately failed attempt at abcission followed by re-fusion. Similar to the effects of ROCK2 overexpression, these mTORC1-driven aberrant cytokinesis events are accompanied by increased Rho-GTP loading, extensive plasma membrane blebbing, and increased actin-myosin contractility, all of which can be rescued by either mTORC1 or ROCK inhibition. These results provide evidence for the existence of a novel mTORC1-Rho-ROCK pathway during cytokinesis and suggest that mTORC1 might play a critical role in setting the size at which a mammalian cell divides. 	
1310.5948v1	http://arxiv.org/pdf/1310.5948v1	2013	Failure modes of complex materials with spatially-correlated mechanical   properties -- the critical role of internal damage	Jerome Faillettaz|Dani Or	  The study reports a systematic evaluation of the role of spatially correlated mechanical elements on failure behavior of heterogeneous materials represented by fiber bundle models (FBM) with different load redistribution rules. The increase of spatial correlation FBM for a local load sharing, results in a transition from ductile-like failure characteristics into brittle-like failure. The study identified a global failure criterion based on macroscopic properties (external load and cumulative damage) which is independent of spatial correlation or load redistribution rules. This invariant metric could be applied for early warning of a class of geophysical ruptures. 	
1612.03221v1	http://arxiv.org/pdf/1612.03221v1	2016	Continuum modeling of the effect of surface area growth due to crushing   and damage on the permeability of granular rocks	Shiva Esna Ashari|Arghya Das|Giuseppe Buscarnera	  This paper discusses a continuum approach to track the evolution of permeability in granular rocks by accounting for the combined effect of porosity changes, grain breakage and cement bond damage. To account for such a broad range of microscopic processes under general loading paths, the Breakage Mechanics theory is used and the computed mechanical response is linked with the Kozeny equation, i.e. a permeability model able to evaluate the reduction of the hydraulic conductivity resulting from the simultaneous loss of porosity and growth of surface area. In particular, the evolution of the internal variables of the model has been linked to idealized geometric schemes at particle scale, with the goal to distinguish the contribution of the fines generated by the disaggregation of the cement matrix from that of the broken fragments resulting from the crushing of the skeleton. Compression/flow experiments available in the literature for different granular rocks are used to validate the proposed methodology. The analyses illustrate that the drop of the permeability of damaged rocks would be severely underestimated without an accurate computation of the growth of surface area, as well as that the distributed fragmentation of skeleton particles tends to have stronger implications than the generation of cement fines. These findings, along with the satisfactory agreement between model predictions and experiments, stress the benefits of adopting microstructure-based constitutive laws for the analysis of coupled hydro-mechanical problems. 	
1607.07502v1	http://arxiv.org/pdf/1607.07502v1	2016	Cascading Node Failure with Continuous States in Random Geometric   Networks	Khashayar Kamran|Edmund Yeh	  The increasing complexity and interdependency of today's networks highlight the importance of studying network robustness to failure and attacks. Many large-scale networks are prone to cascading effects where a limited number of initial failures (due to attacks, natural hazards or resource depletion) propagate through a dependent mechanism, ultimately leading to a global failure scenario where a substantial fraction of the network loses its functionality. These cascading failure scenarios often take place in networks which are embedded in space and constrained by geometry. Building on previous results on cascading failure in random geometric networks, we introduce and analyze a continuous cascading failure model where a node has an initial continuously-valued state, and fails if the aggregate state of its neighbors fall below a threshold. Within this model, we derive analytical conditions for the occurrence and non-occurrence of cascading node failure, respectively. 	
0008360v1	http://arxiv.org/pdf/cond-mat/0008360v1	2000	Multiple current reversals in forced inhomogeneous ratchets	Debasis Dan|Mangal C. Mahato|A. M. Jayannavar	  Transport properties of overdamped Brownian paricles in a rocked thermal ratchet with space dependent friction coefficient is studied. By tuning the parameters, the direction of current exhibit multiple reversals, both as a function of the thermal noise strength as well as the amplitude of rocking force. Current reversals also occur under deterministic conditions and exhibits intriguing structure. All these features arise due to mutual interplay between potential asymmetry,noise, driving frequency and inhomogeneous friction. 	
0106631v2	http://arxiv.org/pdf/cond-mat/0106631v2	2001	Energetics of rocked inhomogeneous ratchets	Debasis Dan|A. M. Jayannavar	  We study the efficiency of frictional thermal ratchets driven by finite frequency driving force and in contact with a heat bath. The efficiency exhibits varied behavior with driving frequency. Both nonmonotonic and monotonic behavior have been observed. In particular the magnitude of efficiency in finite frequency regime may be more than the efficiency in the adiabatic regime. This is our central result for rocked ratchets. We also show that for the simple potential we have chosen, the presence of only spatial asymmetry (homogeneous system) or only frictional ratchet (symmetric potential profile), the adiabatic efficiency is always more than in the nonadiabatic case. 	
0310726v2	http://arxiv.org/pdf/cond-mat/0310726v2	2003	Entropy production, energy loss and currents in adiabatically rocked   thermal ratchets	Raishma Krishnan|A. M. Jayannavar	  We study the nature of currents, input energy and entropy production in different types of adiabatically rocked ratchets using the method of stochastic energetics. The currents exhibit a peak as a function of noise strength. We show that there is no underlying resonance or synchronisation phenomena in the dynamics of the particle with these current peaks. This follows from the analysis of energy loss in the medium. We also show that the maxima seen in current as well as the total entropy production are not directly correlated. 	
0407322v1	http://arxiv.org/pdf/cond-mat/0407322v1	2004	Three-state Potts model in combination with the rock-scissors-paper game	Attila Szolnoki|Gyorgy Szabo|Maria Ravasz	  We study a three-state Potts model extended by allowing cyclic dominance between the states as it appears for the rock-scissors-paper game. Monte Carlo simulations are performed on a square lattice when varying the temperature and the strength of cyclic dominance. It is shown that the critical phase transition from the disordered state to the ordered one is destroyed by the cyclic dominance that yields a self-organizing pattern even at low temperatures. The differences and similarities are discussed between the present model and the half-filled, driven lattice gases with repulsive interaction. 	
0407425v1	http://arxiv.org/pdf/cond-mat/0407425v1	2004	Phase transitions for rock-scissors-paper game on different networks	Attila Szolnoki|Gyorgy Szabo	  Monte Carlo simulations and dynamical mean-field approximations are performed to study the phase transitions in rock-scissors-paper game on different host networks. These graphs are originated from lattices by introducing quenched and annealed randomness simultaneously. In the resulting phase diagrams three different stationary states are identified for all structures. The comparison of results on different networks suggests that the value of clustering coefficient plays an irrelevant role in the emergence of a global oscillating phase. The critical behavior of phase transitions seems to be universal and can be described by the same exponents. 	
0610720v1	http://arxiv.org/pdf/cond-mat/0610720v1	2006	Scaling and universality in rock fracture	Jörn Davidsen|Sergei Stanchits|Georg Dresen	  We present a detailed statistical analysis of acoustic emission time series from laboratory rock fracture obtained from different experiments on different materials including acoustic emission controlled triaxial fracture and punch-through tests. In all considered cases, the waiting time distribution can be described by a unique scaling function indicating its universality. This scaling function is even indistinguishable from that for earthquakes suggesting its general validity for fracture processes independent of time, space and magnitude scales. 	
0902.1309v1	http://arxiv.org/pdf/0902.1309v1	2009	Numerical Simulation of Gas Storage Caverns in Qom Region	Mostafa Sharifzadeh|Ali Moradi Ghasr	  The rock mechanical design of gas storage cavern in salt requires the analysis of the stability and the usability of the cavern over the planned operating time period. The design includes the build up of a rock mass model and a numerical model taking into account the geological situation, load condition, geometrical condition, and material parameters. In this paper multiple caverns in salt formation with geological and geomechanical situation in Qom (central part of Iran) was investigated a using creep model. Minimum safe center to center distances (CTCD) of multiple horizontal caverns also were studied. CTCD of caverns interact at less than two times of cavern diameter. With increasing the CTCD to 2.5 times cavern diameters, diminish most interaction. 	
1007.3396v1	http://arxiv.org/pdf/1007.3396v1	2010	Negative activation volume for dielectric relaxation in hydrated rocks	A. N. Papathanassiou|I. Sakellis|J. Grammatikakis	  Negative defect activation volumes are extremely rare in solids. Here, we report for the first time that this holds in a couple of hydrated rocks for dielectric relaxation by exploring the complex impedance spectra at various pressures and temperatures. The present findings mean that the relaxation time of the relevant relaxation mechanisms decreases upon increasing pressure, thus it may become too short at higher pressure and hence lead to the emission of transient electric signals before fracture. This may constitute the long-standing laboratory confirmation for the explanation of the generation of electric signals prior to an earthquake, as recently pointed out by Uyeda et al [Tectonophysics 470 (2009) 205-213]. 	
1109.4911v2	http://arxiv.org/pdf/1109.4911v2	2012	Neutron Production from the Fracture of Piezoelectric Rocks	A. Widom|J. Swain|Y. N. Srivastava	  A theoretical explanation is provided for the experimental evidence that fracturing piezoelectric rocks produces neutrons. The elastic energy micro-crack production ultimately yields the macroscopic fracture. The mechanical energy is converted by the piezoelectric effect into electric field energy. The electric field energy decays via radio frequency (microwave) electric field oscillations. The radio frequency electric fields accelerate the condensed matter electrons which then collide with protons producing neutrons and neutrinos. 	
1401.4302v1	http://arxiv.org/pdf/1401.4302v1	2014	Stochastic patterns in a 1D Rock-Paper-Scissor model with mutation	Cianci Claudia|Timoteo Carletti	  In the framework of a 1D cyclic competition model, the Rock-Paper-Scissor model, where bacteria are allowed to mutate and move in space, we study the formation of stochastic patterns, where all the bacteria species do coexist. We modelled the problem using an individual-based setting and using the system size van Kampen expansion to deal with the Master Equation, we have been able to characterise the spatio-temporal patterns using the power spectrum of the fluctuations. We proved that such patterns are robust against the intrinsic noise and they can be found for parameters values beyond the ones fixed by the deterministic approach. We complement such analytical results with numerical simulations based on the Gillespie's algorithm. 	
1307.1767v1	http://arxiv.org/pdf/1307.1767v1	2013	Measurement of a Phase of a Radio Wave Reflected from Rock Salt and Ice   Irradiated by an Electron Beam for Detection of Ultra-High-Energy Neutrinos	Masami Chiba|Toshio Kamijo|Takahiro Tanikawa|Hiroyuki Yano|Fumiaki Yabuki|Osamu Yasuda|Yuichi Chikashige|Tadashi Kon|Yutaka Shimizu|Souichirou Watanabe|Michiaki Utsumi|Masatoshi Fujii	  We have found a radio-wave-reflection effect in rock salt for the detection of ultra-high energy neutrinos which are expected to be generated in Greisen, Zatsepin, and Kuzmin (GZK) processes in the universe. When an UHE neutrino interacts with rock salt or ice as a detection medium, a shower is generated. That shower is formed by hadronic and electromagnetic avalanche processes. The energy of the UHE neutrino shower converts to thermal energy through ionization processes. Consequently, the temperature rises along the shower produced by the UHE neutrino. The refractive index of the medium rises with temperature. The irregularity of the refractive index in the medium leads to a reflection of radio waves. This reflection effect combined with the long attenuation length of radio waves in rock salt and ice would yield a new method to detect UHE neutrinos. We measured the phase of the reflected radio wave under irradiation with an electron beam on ice and rock salt powder. The measured phase showed excellent consistence with the power reflection fraction which was measured directly. A model taking into account the temperature change explained the phase and the amplitude of the reflected wave. Therefore the reflection mechanism was confirmed. The power reflection fraction was compared with that calculated with the Fresnel equations, the ratio between the measured result and that obtained with the Fresnel equations in ice was larger than that of rock salt. 	
0801.0543v1	http://arxiv.org/pdf/0801.0543v1	2008	Numerical modeling of carbon dioxide sequestration on the rate of   pressure solution creep in limestone: Preliminary results	Francois Renard|Elisabeth Gundersen|Roland Hellmann|Marielle Collombet|Yvi Le Guen	  When carbon dioxide (CO2) is injected into an aquifer or a depleted geological reservoir, its dissolution into solution results in acidification of the pore waters. As a consequence, the pore waters become more reactive, which leads to enhanced dissolution-precipitation processes and a modification of the mechanical and hydrological properties of the rock. This effect is especially important for limestones given that the solubility and reactivity of carbonates is strongly dependent on pH and the partial pressure of CO2. The main mechanism that couples dissolution, precipitation and rock matrix deformation is commonly referred to as intergranular pressure solution creep (IPS) or pervasive pressure solution creep (PSC). This process involves dissolution at intergranular grain contacts subject to elevated stress, diffusion of dissolved material in an intergranular fluid, and precipitation in pore spaces subject to lower stress. This leads to an overall and pervasive reduction in porosity due to both grain indentation and precipitation in pore spaces. The percolation of CO2-rich fluids may influence on-going compaction due to pressure solution and can therefore potentially affect the reservoir and its long-term CO2 storage capacity. We aim at quantifying this effect by using a 2D numerical model to study the coupling between dissolution-precipitation processes, local mass transfer, and deformation of the rock over long time scales. We show that high partial pressures of dissolved CO2 (up to 30 MPa) significantly increase the rates of compaction by a factor of ~ 50 to ~ 75, and also result in a concomitant decrease in the viscosity of the rock matrix. 	
1104.4813v1	http://arxiv.org/pdf/1104.4813v1	2011	The mechanics of rocking stones: equilibria on separated scales	Gábor Domokos|András Árpád Sipos|Tímea Szabó	  Rocking stones, balanced in counter-intuitive positions have always intrigued geologists. In our paper we explain this phenomenon based on high-precision scans of pebbles which exhibit similar behavior. We construct their convex hull and the heteroclinic graph carrying their equilibrium points. By systematic simplification of the arising Morse-Smale complex in a one-parameter process we show that equilibria occur typically in highly localized groups (flocks), the number of the latter can be reliably observed and determined by hand experiments. Both local and global (micro and macro) equilibria can be either stable or unstable. Most commonly, rocks and pebbles are balanced on stable local equilibria belonging to stable flocks. However, it is possible to balance a convex body on a stable local equilibrium belonging to an unstable flock and this is the intriguing mechanical scenario corresponding to rocking stones. Since outside observers can only reliably perceive flocks, the last described situation will appear counter-intuitive. Comparison of computer experiments to hand experiments reveals that the latter are consistent, i.e. the flocks can be reliably counted and the pebble classification system proposed in our previous work (Domokos et al 2010) is robustly applicable. We also find an interesting logarithmic relationship between the Zingg parameters and the average number of global equilibrium points, indicating a close relationship between the two systems. 	
1110.2270v1	http://arxiv.org/pdf/1110.2270v1	2011	Noise Analysis and Detection Based on RF Energy Duration in wireless LAN	R. Seshadri|N. Penchalaiah	  Noise is the major problem while working with wireless LAN. In this paper we analyze the noise by using active receiving antenna and also propose the detection mechanism based on RF energy duration. The standard back off mechanism of 802.11 wireless LAN (WLAN) increases the contention window when a transmission failure occurs in order to alleviate contentions in a WLAN. In addition, many proposed schemes for 802.11 WLAN behave adaptively to transmission failures. Transmission failures in WLANs occur mostly by two causes: collision and channel noise. However, in 802.11 WLAN, a station cannot know the cause of a transmission failure, thus the adaptive schemes assume the ideal situation in which all transmission failures occur by only one of two causes. For this reason, they may behave erroneously in a real world where transmission failures occur by both causes. In this paper, we propose a novel scheme to detect collision, which utilizes transmission time information and RF energy duration on the channel. By detecting collisions, a station can differentiate the causes of transmission failures and the adaptive schemes can operate correctly by using the detection information. 	
1604.01666v1	http://arxiv.org/pdf/1604.01666v1	2016	Record breaking bursts during the compressive failure of porous   materials	Gergo Pal|Frank Raischel|Sabine Lennartz-Sassinek|Ferenc Kun|Ian G. Main	  An accurate understanding of the interplay between random and deterministic processes in generating extreme events is of critical importance in many fields, from forecasting extreme meteorological events to the catastrophic failure of materials and in the Earth. Here we investigate the statistics of record-breaking events in the time series of crackling noise generated by local rupture events during the compressive failure of porous materials. The events are generated by computer simulations of the uni-axial compression of cylindrical samples in a discrete element model of sedimentary rocks that closely resemble those of real experiments. The number of records grows initially as a decelerating power law of the number of events, followed by an acceleration immediately prior to failure. We demonstrate the existence of a characteristic record rank k^* which separates the two regimes of the time evolution. Up to this rank deceleration occurs due to the effect of random disorder. Record breaking then accelerates towards macroscopic failure, when physical interactions leading to spatial and temporal correlations dominate the location and timing of local ruptures. Sub-sequences of bursts between consecutive records are characterized by a power law size distribution with an exponent which decreases as failure is approached. High rank records are preceded by bursts of increasing size and waiting time between consecutive events and they are followed by a relaxation process. As a reference, surrogate time series are generated by reshuffling the crackling bursts. The record statistics of the uncorrelated surrogates agrees very well with the corresponding predictions of independent identically distributed random variables, which confirms that the temporal and spatial correlation of cracking bursts are responsible for the observed unique behaviour. 	
1706.04579v1	http://arxiv.org/pdf/1706.04579v1	2017	Limits of Predictability of Cascading Overload Failures in   Spatially-Embedded Networks with Distributed Flows	Alaa Moussawi|Noemi Derzsy|Xin Lin|Boleslaw K. Szymanski|Gyorgy Korniss	  Cascading failures are a critical vulnerability of complex information or infrastructure networks. Here we investigate the properties of load-based cascading failures in real and synthetic spatially-embedded network structures, and propose mitigation strategies to reduce the severity of damages caused by such failures. We introduce a stochastic method for optimal heterogeneous distribution of resources (node capacities) subject to a fixed total cost. Additionally, we design and compare the performance of networks with N-stable and (N-1)-stable network-capacity allocations by triggering cascades using various real-world node-attack and node-failure scenarios. We show that failure mitigation through increased node protection can be effectively achieved against single node failures. However, mitigating against multiple node failures is much more difficult due to the combinatorial increase in possible failures. We analyze the robustness of the system with increasing protection, and find that a critical tolerance exists at which the system undergoes a phase transition, and above which the network almost completely survives an attack. Moreover, we show that cascade-size distributions measured in this region exhibit a power-law decay. Finally, we find a strong correlation between cascade sizes induced by individual nodes and sets of nodes. We also show that network topology alone is a weak factor in determining the progression of cascading failures. 	
0912.4997v1	http://arxiv.org/pdf/0912.4997v1	2009	A Simple Hydromechanical Modeling of Carbon Sequestration in Sedimentary   Rocks	Hamed. O. Ghaffari|Mamadou Fall	  In this study, over different scenarios we will simulate a week coupling of hydromechanical loads in a long term CO2 injection with a hypothetical reservoir while the effect of pore water pressure and then multi-phase flow procedure has been ignored. In the first basic case the homogenous case has been considered when the theory of poroelasticity was employed. Second case covers the effects of directional heterogeneity, constructed by random faults, on the flow paths of gas and other attributes of the system. Also, in the latter case the impact of stress state as an active loads (body loads) has been regarded. Thanks to multiple directional heterogeneity, which induces only one heterogenic parameter (intrinsic permeability), distinguishable flow paths can be recognized. In another process, the failure ability of system regard to Mohr-Columb criterion is measured as well as options that, presumably, the system has continuum faults (zero cohesion). The results over different cases shows absedince of ground surface (heave), more probable propagation of failure area and the role of directional heterogeneity to change the evolution path of system. 	
1501.03994v1	http://arxiv.org/pdf/1501.03994v1	2015	Numerical modelling of sandstone uniaxial compression test using a   mix-mode cohesive fracture model	Yilin Gui|Ha H. Bui|Jayantha Kodikara	  A mix-mode cohesive fracture model considering tension, compression and shear material behaviour is presented, which has wide applications to geotechnical problems. The model considers both elastic and inelastic displacements. Inelastic displacement comprises fracture and plastic displacements. The norm of inelastic displacement is used to control the fracture behaviour. Meantime, a failure function describing the fracture strength is proposed. Using the internal programming FISH, the cohesive fracture model is programmed into a hybrid distinct element algorithm as encoded in Universal Distinct Element Code (UDEC). The model is verified through uniaxial tension and direct shear tests. The developed model is then applied to model the behaviour of a uniaxial compression test on Gosford sandstone. The modelling results indicate that the proposed cohesive fracture model is capable of simulating combined failure behaviour applicable to rock. 	
1101.5330v1	http://arxiv.org/pdf/1101.5330v1	2011	A contact model for the yielding of caked granular materials	L. Brendel|J. Török|R. Kirsch|U. Bröckel	  We present a visco-elastic coupling model between caked spheres, suitable for DEM simulations, which incorporates the different loading mechanisms (tension, shear, bending, torsion) in a combined manner and allows for a derivation of elastic and failure properties on a common basis. In pull, shear, and torsion failure tests with agglomerates of up to 10000 particles, we compare the failure criterion to different approximative variants of it, with respect to accuracy and computational cost. The failure of the agglomerates, which behave according to elastic parameters derived from the contact elasticity, gives also insight into the relative relevance of the different load modes. 	
0410086v1	http://arxiv.org/pdf/cond-mat/0410086v1	2004	Theory of frequency and phase synchronization in a rocked bistable   stochastic system	Jesús Casado-Pascual|José Gómez-Ordóñez|Manuel Morillo|Jörg Lehmann|Igor Goychuk|Peter Hänggi	  We investigate the role of noise in the phenomenon of stochastic synchronization of switching events in a rocked, overdamped bistable potential driven by white Gaussian noise, the archetype description of Stochastic Resonance. We present a new approach to the stochastic counting process of noise-induced switching events: starting from the Markovian dynamics of the nonstationary, continuous particle dynamics one finds upon contraction onto two states a non-Markovian renewal dynamics. The output frequency is determined as the velocity of the underlying discrete phase dynamics. The phenomenon of noise-assisted phase synchronization is investigated in terms of an effective, instantaneous phase diffusion. The theory is applied to rectangular-shaped rocking signals versus increasing input-noise strengths. Precise numerical simulations corroborate very favorably our analytical results. The novel theoretical findings are also compared with prior findings. 	
0611408v2	http://arxiv.org/pdf/cond-mat/0611408v2	2008	Smoothing a Rock by Chipping	P. L. Krapivsky|S. Redner	  We investigate an idealized model for the size reduction and smoothing of a polygonal rock due to repeated chipping at corners. Each chip is sufficiently small so that only a single corner and a fraction of its two adjacent sides are cut from the object in a single chipping event. After many chips have been cut away, the resulting shape of the rock is generally anisotropic, with facet lengths and corner angles distributed over a broad range. Although a well-defined shape is quickly reached for each realization, there are large fluctuations between realizations. 	
0805.4560v1	http://arxiv.org/pdf/0805.4560v1	2008	Rock mechanics modeling based on soft granulation theory	H. Owladeghaffari	  This paper describes application of information granulation theory, on the design of rock engineering flowcharts. Firstly, an overall flowchart, based on information granulation theory has been highlighted. Information granulation theory, in crisp (non-fuzzy) or fuzzy format, can take into account engineering experiences (especially in fuzzy shape-incomplete information or superfluous), or engineering judgments, in each step of designing procedure, while the suitable instruments modeling are employed. In this manner and to extension of soft modeling instruments, using three combinations of Self Organizing Map (SOM), Neuro-Fuzzy Inference System (NFIS), and Rough Set Theory (RST) crisp and fuzzy granules, from monitored data sets are obtained. The main underlined core of our algorithms are balancing of crisp(rough or non-fuzzy) granules and sub fuzzy granules, within non fuzzy information (initial granulation) upon the open-close iterations. Using different criteria on balancing best granules (information pockets), are obtained. Validations of our proposed methods, on the data set of in-situ permeability in rock masses in Shivashan dam, Iran have been highlighted. 	
0806.0360v1	http://arxiv.org/pdf/0806.0360v1	2008	"Short-term time prediction" of large EQs by the use of "Large Scale   Piezoelectricity" generated by the focal areas loaded with excess stress load	C. Thanassoulas	  In this work, it is demonstrated that the Earth's preseismic electric field, which is registered by a pair of electrodes in contact to the ground surface at certain distance from the epicentral area, corresponds to the gradient of the total field generated in the focal area as a function of time and distance from it. The original form of the generated preseismic field follows closely the theoretical piezoelectric potential form. The later is obtained after integration in time of the original registered potential grad data values. Consequently, the time of occurrence of the imminent earthquake (collapsing of the rock formation) is estimated by the classical laws of rock fracturing based on theoretical Rock Mechanics. The methodology has been tested on the grad potential data, registered in Greece, before four large EQs which took place in the regional area as follows: Izmit, Turkey EQ (M = 7.8R, 17th August, 1999), Milos, Greece EQ (M = 5.6R, 21st May, 2002), Kythira Greece EQ (M = 6.9R, 8th January, 2006) and Methoni, Greece EQ (M = 6.7R, 14th February, 2008). The obtained results prove the validity of the postulated methodology. 	
0901.0979v1	http://arxiv.org/pdf/0901.0979v1	2009	Rocking ratchet based on F1-ATPase in the absence of ATP	Kumiko Hayashi|Hisatsugu Yamasaki|Mitsunori Takano	  Bartussek, Hanggi and Kissner studied a rocking ratchet system, in which a Brownian particle is subject to an asymmetric periodic potential together with an oscillating force, and found that the direction of the macroscopic current can be reversed by changing the parameter values characterizing the model [Europhys. Lett., 28 (1994) 459]. In this letter, we apply their ratchet theory to a rotary motor-protein, F1-ATPase. In this work, we construct a model of a rocking ratchet in which F1-ATPase rotates not as a result of ATP hydrolysis but through the influence of an oscillating force. We then study the motion of F1-ATPase on the basis of molecular dynamics simulations of this coarse-grained protein model. Although in the absence of ATP, F1-ATPase exhibits directionless Brownian motion when there exists no oscillating force, we observe directional motion when we do apply an oscillating force. Furthermore, we observe that the direction of rotation is reversed when we change the oscillation frequency. 	
1208.1233v1	http://arxiv.org/pdf/1208.1233v1	2012	A fibre optic sensor for the in situ determination of rock physical   properties	Thomas Reinsch|Guido Blöcher|Harald Milsch|Kort Bremer|Elfed Lewis|Gabriel Leen|Steffen Lochmann	  To understand the behaviour of rocks under changing load or temperature conditions, the determination of physical parameters like pore pressure or temperature within the pore space is essential. Within this study, the implementation of a novel fibre optic point sensor for pressure and temperature determination into a high pressure / high temperature triaxial cell is presented. For the first time, pressure was measured directly within the pore space of a Flechtinger sandstone specimen during a hydrostatic compression test at up to 70 MPa. The sensor used within this study consists of a miniature all-silica fibre optic Extrinsic Fabry-Perot Interferometer (EFPI) sensor which has an embedded Fibre Bragg Grating (FBG) reference sensor element to determine temperature and pressure directly at the point of measurement. 	
1301.3238v3	http://arxiv.org/pdf/1301.3238v3	2013	Cycle frequency in standard Rock-Paper-Scissors games: Evidence from   experimental economics	Bin Xu|Hai-Jun Zhou|Zhijian Wang	  The Rock-Paper-Scissors (RPS) game is a widely used model system in game theory. Evolutionary game theory predicts the existence of persistent cycles in the evolutionary trajectories of the RPS game, but experimental evidence has remained to be rather weak. In this work we performed laboratory experiments on the RPS game and analyzed the social-state evolutionary trajectories of twelve populations of N=6 players. We found strong evidence supporting the existence of persistent cycles. The mean cycling frequency was measured to be $0.029 \pm 0.009$ period per experimental round. Our experimental observations can be quantitatively explained by a simple non-equilibrium model, namely the discrete-time logit dynamical process with a noise parameter. Our work therefore favors the evolutionary game theory over the classical game theory for describing the dynamical behavior of the RPS game. 	
1309.6737v1	http://arxiv.org/pdf/1309.6737v1	2013	Rocking Subdiffusive Ratchets: Origin, Optimization and Efficiency	I. Goychuk|V. O. Kharchenko	  We study origin, parameter optimization, and thermodynamic efficiency of isothermal rocking ratchets based on fractional subdiffusion within a generalized non-Markovian Langevin equation approach. A corresponding multi-dimensional Markovian embedding dynamics is realized using a set of auxiliary Brownian particles elastically coupled to the central Brownian particle (see video on the journal web site). We show that anomalous subdiffusive transport emerges due to an interplay of nonlinear response and viscoelastic effects for fractional Brownian motion in periodic potentials with broken space-inversion symmetry and driven by a time-periodic field. The anomalous transport becomes optimal for a subthreshold driving when the driving period matches a characteristic time scale of interwell transitions. It can also be optimized by varying temperature, amplitude of periodic potential and driving strength. The useful work done against a load shows a parabolic dependence on the load strength. It grows sublinearly with time and the corresponding thermodynamic efficiency decays algebraically in time because the energy supplied by the driving field scales with time linearly. However, it compares well with the efficiency of normal diffusion rocking ratchets on an appreciably long time scale. 	
1407.0621v2	http://arxiv.org/pdf/1407.0621v2	2014	Characterization of spiraling patterns in spatial rock-paper-scissors   games	Bartosz Szczesny|Mauro Mobilia|Alastair M. Rucklidge	  The spatio-temporal arrangement of interacting populations often influences the maintenance of species diversity and is a subject of intense research. Here, we study the spatio-temporal patterns arising from the cyclic competition between three species in two dimensions. Inspired by recent experiments, we consider a generic metapopulation model comprising "rock-paper-scissors" interactions via dominance removal and replacement, reproduction, mutations, pair-exchange and hopping of individuals. By combining analytical and numerical methods, we obtain the model's phase diagram near its Hopf bifurcation and quantitatively characterize the properties of the spiraling patterns arising in each phase. The phases characterizing the cyclic competition away far from the Hopf bifurcation (at low mutation rate) are also investigated. Our analytical approach relies on the careful analysis of the properties of the complex Ginzburg-Landau equation derived through a controlled (perturbative) multiscale expansion around the model's Hopf bifurcation. Our results allows us to clarify when spatial "rock-paper-scissors" competition leads to stable spiral waves and under which circumstances they are influenced by nonlinear mobility. 	
1712.06032v1	http://arxiv.org/pdf/1712.06032v1	2017	Reactivation of Fractures in Subsurface Reservoirs - a Numerical   Approach using a Static-Dynamic Friction Model	Runar L. Berge|Inga Berre|Eirik Keilegavlen	  Fluid-induced slip of fractures is characterized by strong multiphysics couplings. Three physical processes are considered: Flow, rock deformation and fracture deformation. The fractures are represented as lower-dimensional objects embedded in a three-dimensional domain. Fluid is modeled as slightly compressible, and flow in both fractures and matrix is accounted for. The deformation of rock is inherently different from the deformation of fractures; thus, two different models are needed to describe the mechanical deformation of the rock. The medium surrounding the fractures is modeled as a linear elastic material, while the slip of fractures is modeled as a contact problem, governed by a static-dynamic friction model. We present an iterative scheme for solving the non-linear set of equations that arise from the models, and suggest how the step parameter in this scheme should depend on the shear modulus and mesh size. 	
0103552v2	http://arxiv.org/pdf/cond-mat/0103552v2	2001	Pumping Heat with Quantum Ratchets	T. E. Humphrey|H. Linke|R. Newbury	  We describe how adiabatically rocked quantum electron ratchets can act as heat pumps. In general, ratchets may be described as non-equilibrium systems in which directed particle motion is generated using spatial or temporal asymmetry. In a rocked ratchet, which may also be described as a non-linear rectifier, an asymmetric potential is tilted symmetrically and periodically. The potential deforms differently during each half-cycle, producing a net current of particles when averaged over a full period of rocking. Recently it was found that in the quantum regime, where tunnelling contributes to transport, the net current may change sign with temperature. Here we show that a Landauer model of an experimental tunnelling ratchet [Linke et. al., Science 286, 2314 (1999)] predicts the existence of a net heat current even when the net particle current goes through zero. We quantify this heat current and define a coefficient of performance for the ratchet as a heat pump, finding that more heat is deposited in each of the two electron reservoirs due to the process of rocking than is pumped from one reservoir to the other by the ratchet. 	
0709.0217v2	http://arxiv.org/pdf/0709.0217v2	2008	Mobility promotes and jeopardizes biodiversity in rock-paper-scissors   games	Tobias Reichenbach|Mauro Mobilia|Erwin Frey	  Biodiversity is essential to the viability of ecological systems. Species diversity in ecosystems is promoted by cyclic, non-hierarchical interactions among competing populations. Such non-transitive relations lead to an evolution with central features represented by the `rock-paper-scissors' game, where rock crushes scissors, scissors cut paper, and paper wraps rock. In combination with spatial dispersal of static populations, this type of competition results in the stable coexistence of all species and the long-term maintenance of biodiversity. However, population mobility is a central feature of real ecosystems: animals migrate, bacteria run and tumble. Here, we observe a critical influence of mobility on species diversity. When mobility exceeds a certain value, biodiversity is jeopardized and lost. In contrast, below this critical threshold all subpopulations coexist and an entanglement of travelling spiral waves forms in the course of temporal evolution. We establish that this phenomenon is robust, it does not depend on the details of cyclic competition or spatial environment. These findings have important implications for maintenance and evolution of ecological systems and are relevant for the formation and propagation of patterns in excitable media, such as chemical kinetics or epidemic outbreaks. 	
0811.4036v1	http://arxiv.org/pdf/0811.4036v1	2008	Protein Kinase C-related Kinase and ROCK Are Required for   Thrombin-induced Endothelial Cell Permeability Downstream from G{alpha}12/13   and G{alpha}11/q	Julie Gavard|J Silvio Gutkind	  Increase in vascular permeability occurs under many physiological conditions and is central in diverse human pathologies. Thrombin is a pro-coagulant serine protease, which causes the local loss of endothelial barrier integrity thereby enabling the rapid extravasation of plasma proteins and the local formation of fibrin-containing clots. Available information suggests that thrombin induces endothelial permeability by promoting actomyosin contractility through the Rho/ROCK signaling pathway. Here we took advantage of pharmacological inhibitors, knockdown approaches, and the emerging knowledge on how permeability factors affect endothelial junctions to investigate in detail the mechanism underlying thrombin-induced endothelial permeability. We show that thrombin signals through PAR-1 and its coupled G proteins G(12/13) and G(11/q) to induce RhoA activation and intracellular calcium elevation, and that these events are interrelated. In turn, this leads to the stimulation of ROCK, which causes actin stress-fiber formation. However, this alone is not sufficient to account for thrombin-induced permeability. Instead, we found that protein kinase C-related kinase, a Rho-dependent S/T kinase, is activated in endothelial cells upon thrombin stimulation and that its expression is required for endothelial permeability and remodeling of cell-extracellular matrix and cell-cell adhesions. Our results demonstrate that the signal initiated by thrombin bifurcates at the level of RhoA to promote changes in the cytoskeletal architecture through ROCK and the remodeling of focal adhesion components through PRK. Ultimately, both pathways converge to cause cell-cell junction disruption and provoke vascular leakage. 	
1408.6828v1	http://arxiv.org/pdf/1408.6828v1	2014	Cyclic dominance in evolutionary games: A review	Attila Szolnoki|Mauro Mobilia|Luo-Luo Jiang|Bartosz Szczesny|Alastair M. Rucklidge|Matjaz Perc	  Rock is wrapped by paper, paper is cut by scissors, and scissors are crushed by rock. This simple game is popular among children and adults to decide on trivial disputes that have no obvious winner, but cyclic dominance is also at the heart of predator-prey interactions, the mating strategy of side-blotched lizards, the overgrowth of marine sessile organisms, and the competition in microbial populations. Cyclical interactions also emerge spontaneously in evolutionary games entailing volunteering, reward, punishment, and in fact are common when the competing strategies are three or more regardless of the particularities of the game. Here we review recent advances on the rock-paper-scissors and related evolutionary games, focusing in particular on pattern formation, the impact of mobility, and the spontaneous emergence of cyclic dominance. We also review mean-field and zero-dimensional rock-paper-scissors models and the application of the complex Ginzburg-Landau equation, and we highlight the importance and usefulness of statistical physics for the successful study of large-scale ecological systems. Directions for future research, related for example to dynamical effects of coevolutionary rules and invasion reversals due to multi-point interactions, are outlined as well. 	
1508.03137v2	http://arxiv.org/pdf/1508.03137v2	2015	Torsion of a cylinder of partially molten rock with a spherical   inclusion: theory and simulation	Laura Alisic|Sander Rhebergen|John F. Rudge|Richard F. Katz|Garth N. Wells	  The processes that are involved in migration and extraction of melt from the mantle are not yet fully understood. Gaining a better understanding of material properties of partially molten rock could help shed light on the behavior of melt on larger scales in the mantle. In this study, we simulate three-dimensional torsional deformation of a partially molten rock that contains a rigid, spherical inclusion. We compare the computed porosity patterns to those found in recent laboratory experiments. The laboratory experiments show emergence of melt-rich bands throughout the rock sample, and pressure shadows around the inclusion. The numerical model displays similar melt-rich bands only for a small bulk-to-shear-viscosity ratio (five or less). The results are consistent with earlier two-dimensional numerical simulations; however, we show that it is easier to form melt-rich bands in three dimensions compared to two. The addition of strain-rate dependence of the viscosity causes a distinct change in the shape of pressure shadows around the inclusion. This change in shape presents an opportunity for experimentalists to identify the strain-rate dependence and therefore the dominant deformation mechanism in torsion experiments with inclusions. 	
1303.5304v1	http://arxiv.org/pdf/1303.5304v1	2013	A laboratory investigation of thermally induced pore pressures in the   Callovo-Oxfordian Claystone	Mehrdokht Mohajerani|Pierre Delage|Jean Sulem|Mohammad Monfared|Anh-Minh Tang|Behrouz Gatmiri	  In the framework of research into radioactive waste disposal, it was decided to investigate the thermally induce pore pressure occurring in the Callovo-Oxfordian claystone, a possible host rock in which the ANDRA underground laboratory of Bure (East of France) has been excavated. Thermal pore pressures appear in low permeability soils and rocks because the thermal expansion coefficient of water is significantly higher than that of the solid grains (Campanella and Mitchell; 1968 [1], Ghabezloo and Sulem; 2009 [2]). This phenomenon has clearly been observed in various in-situ heating tests conducted in Opalinus claystone in the Mont-Terri Underground Research Laboratory (URL) in Switzerland (HE-D test) and in Callovo-Oxfordian (COx) claystone in the Bure URL in France (TER test, Wileveau and Su; 2007 [3]) The processes of coring, transportation, storage and specimen trimming induce some desaturation in the sample. Due to the very low permeability (10-20 m2) of the COx claystone, a long period of time is necessary to properly resaturate the sample, a mandatory condition to satisfactorily investigate thermal pressurisation. Particular emphasis was hence put on the previous saturation procedure that was carried out under in-situ effective stress condition. Thermal pressurization has been investigated by performing undrained heating tests while measuring pore pressures changes in a specially adapted thermal isotropic compression cell. Special care was devoted to calibration procedures to account for the effects of the system on the pore pressure measurements. The thermal pressurization coefficient measured appeared to change with temperature, mainly because of the changes with temperature of both the water thermal expansion coefficient of water and the drained compression coefficient of the claystone. 	
0706.1134v1	http://arxiv.org/pdf/0706.1134v1	2007	Comment on: Failure of the work-Hamiltonian connection for free energy   calculations	A. Imparato|L. Peliti	  We argue that the apparent failure of the work-Hamiltonian connection for free energy calculations reported by Vilar and Rubi' (cond-mat arXiv:0704.0761v2) stems from their incorrect expression for the work. 	
0208359v2	http://arxiv.org/pdf/cond-mat/0208359v2	2003	Failure due to fatigue in fiber bundles and solids	Srutarshi Pradhan|Bikas K. Chakrabarti	  We consider first a homogeneous fiber bundle model where all the fibers have got the same stress threshold beyond which all fail simultaneously in absence of noise. At finite noise, the bundle acquires a fatigue behavior due to the noise-induced failure probability at any stress. We solve this dynamics of failure analytically and show that the average failure time of the bundle decreases exponentially as the stress increases. We also determine the avalanche size distribution during such failure and find a power law decay. We compare this fatigue behavior with that obtained phenomenologically for the nucleation of Griffith cracks. Next we study numerically the fatigue behavior of random fiber bundles having simple distributions of individual fiber strengths, at stress less than the bundle's strength (beyond which it fails instantly). The average failure time is again seen to decrease exponentially as the stress increases and the avalanche size distribution shows similar power law decay. These results are also in broad agreement with experimental observations on fatigue in solids. We believe, these observations regarding the failure time are useful for quantum breakdown phenomena in disordered systems. 	
1505.04628v1	http://arxiv.org/pdf/1505.04628v1	2015	Building a fault tolerant application using the GASPI communication   layer	Faisal Shahzad|Moritz Kreutzer|Thomas Zeiser|Rui Machado|Andreas Pieper|Georg Hager|Gerhard Wellein	  It is commonly agreed that highly parallel software on Exascale computers will suffer from many more runtime failures due to the decreasing trend in the mean time to failures (MTTF). Therefore, it is not surprising that a lot of research is going on in the area of fault tolerance and fault mitigation. Applications should survive a failure and/or be able to recover with minimal cost. MPI is not yet very mature in handling failures, the User-Level Failure Mitigation (ULFM) proposal being currently the most promising approach is still in its prototype phase. In our work we use GASPI, which is a relatively new communication library based on the PGAS model. It provides the missing features to allow the design of fault-tolerant applications. Instead of introducing algorithm-based fault tolerance in its true sense, we demonstrate how we can build on (existing) clever checkpointing and extend applications to allow integrate a low cost fault detection mechanism and, if necessary, recover the application on the fly. The aspects of process management, the restoration of groups and the recovery mechanism is presented in detail. We use a sparse matrix vector multiplication based application to perform the analysis of the overhead introduced by such modifications. Our fault detection mechanism causes no overhead in failure-free cases, whereas in case of failure(s), the failure detection and recovery cost is of reasonably acceptable order and shows good scalability. 	
1511.01446v2	http://arxiv.org/pdf/1511.01446v2	2015	ATLAS: An Adaptive Failure-aware Scheduler for Hadoop	Mbarka Soualhia|Foutse Khomh|Sofiene Tahar	  Hadoop has become the de facto standard for processing large data in today's cloud environment. The performance of Hadoop in the cloud has a direct impact on many important applications ranging from web analytic, web indexing, image and document processing to high-performance scientific computing. However, because of the scale, complexity and dynamic nature of the cloud, failures are common and these failures often impact the performance of jobs running in Hadoop. Although Hadoop possesses built-in failure detection and recovery mechanisms, several scheduled jobs still fail because of unforeseen events in the cloud environment. A single task failure can cause the failure of the whole job and unpredictable job running times. In this report, we propose ATLAS (AdapTive faiLure-Aware Scheduler), a new scheduler for Hadoop that can adapt its scheduling decisions to events occurring in the cloud environment. Using statistical models, ATLAS predicts task failures and adjusts its scheduling decisions on the fly to reduce task failure occurrences. We implement ATLAS in the Hadoop framework of Amazon Elastic MapReduce (EMR) and perform a case study to compare its performance with those of the FIFO, Fair and Capacity schedulers. Results show that ATLAS can reduce the percentage of failed jobs by up to 28% and the percentage of failed tasks by up to 39%, and the total execution time of jobs by 10 minutes on average. ATLAS also reduces CPU and memory usages. 	
0306509v5	http://arxiv.org/pdf/cond-mat/0306509v5	2003	Bug propagation and debugging in asymmetric software structures	Damien Challet|Andrea Lombardoni	  Software dependence networks are shown to be scale-free and asymmetric. We then study how software components are affected by the failure of one of them, and the inverse problem of locating the faulty component. Software at all levels is fragile with respect to the failure of a random single component. Locating a faulty component is easy if the failures only affect their nearest neighbors, while it is hard if the failures propagate further. 	
0909.4185v2	http://arxiv.org/pdf/0909.4185v2	2010	Stochastic Load-Redistribution Model for Cascading Failure Propagation	Jörg Lehmann|Jakob Bernasconi	  A new class of probabilistic models for cascading failure propagation in interconnected systems is proposed. The models take into account important characteristics of real systems that are not considered in existing generic approaches. Specifically, it is assumed that the load increments after a failure are proportional to the failed load and that the load redistribution among the remaining elements is stochastic. The models are solved analytically in terms of generalized branching processes, and the failure propagation properties of a prototype example are analyzed in detail. 	
1606.04895v3	http://arxiv.org/pdf/1606.04895v3	2016	Magnetar Outbursts from Avalanches of Hall Waves and Crustal Failures	Xinyu Li|Yuri Levin|Andrei M. Beloborodov	  We explore the interaction between Hall waves and mechanical failures inside a magnetar crust, using detailed one-dimentional models that consider temperature-sensitive plastic flow, heat transport and cooling by neutrino emission, as well as the coupling of the crustal motion to the magnetosphere. We find that the dynamics is enriched and accelerated by the fast, short-wavelength Hall waves that are emitted by each failure. The waves propagate and cause failures elsewhere, triggering avalanches. We argue that these avalanches are the likely sources of outbursts in transient magnetars. 	
1711.05802v1	http://arxiv.org/pdf/1711.05802v1	2017	Avalanche precursors of failure in hierarchical fuse networks	Paolo Moretti|Bastien Dietemann|Michael Zaiser	  We study precursors of failure in hierarchical random fuse network models which can be considered as idealizations of hierarchical (bio)materials where fibrous assemblies are held together by multi-level (hierarchical) cross-links. When such structures are loaded towards failure, the patterns of precursory avalanche activity exhibit generic scale invariance: Irrespective of load, precursor activity is characterized by power-law avalanche size distributions without apparent cut-off, with power-law exponents that decrease continuously with increasing load. This failure behavior and the ensuing super-rough crack morphology differ significantly from the findings in non-hierarchical structures. 	
0309026v1	http://arxiv.org/pdf/cs/0309026v1	2003	A thought experiment on Quantum Mechanics and Distributed Failure   Detection	Mark C. Little	  One of the biggest problems in current distributed systems is that presented by one machine attempting to determine the liveness of another in a timely manner. Unfortunately, the symptoms exhibited by a failed machine can also be the result of other causes, e.g., an overloaded machine or network which drops messages, making it impossible to detect a machine failure with cetainty until that machine recovers. This is a well understood problem and one which has led to a large body of research into failure suspectors: since it is not possible to detect a failure, the best one can do is suspect a failure and program accordingly. However, one machine's suspicions may not be the same as another's; therefore, these algorithms spend a considerable effort in ensuring a consistent view among all available machines of who is suspects of being failed. This paper describes a thought experiment on how quantum mechanics may be used to provide a failure detector that is guaranteed to give both accurate and instantaneous information about the liveness of machines, no matter the distances involved. 	
9809423v2	http://arxiv.org/pdf/cond-mat/9809423v2	1999	Grain Segregation Mechanism in Aeolian Sand Ripples	Hernan A. Makse	  Many sedimentary rocks are formed by migration of sand ripples. Thin layers of coarse and fine sand are present in these rocks, and understanding how layers in sandstone are created has been a longstanding question. Here, we propose a mechanism for the origin of the most common layered sedimentary structures such as inverse graded climbing ripple lamination and cross-stratification patterns. The mechanism involves a competition between three segregation processes: (i) size-segregation and (ii) shape-segregation during transport and rolling, and (iii) size segregation due to different hopping lengths of the small and large grains. We develop a discrete model of grain dynamics which incorporates the coupling between moving grains and the static sand surface, as well as the different properties of grains, such as size and roughness, in order to test the plausibility of this physical mechanism. 	
1705.10831v3	http://arxiv.org/pdf/1705.10831v3	2017	Self-Organization of Dragon Kings	Yuansheng Lin|Keith Burghardt|Martin Rohden|Pierre-André Noël|Raissa M. D'Souza	  The mechanisms underlying cascading failures are often modeled via the paradigm of self-organized criticality. Here we introduce a simple model where nodes self-organize to be either weak or strong to failure which captures the trade-off between degradation and reinforcement of nodes inherent in many network systems. If strong nodes cannot fail, this leads to power law distributions of failure sizes with so-called "Black Swan" rare events. In contrast, if strong nodes fail once a sufficient fraction of their neighbors fail, this leads to "Dragon Kings", which are massive failures caused by mechanisms distinct from smaller failures. In our model, we find that once an initial failure size is above a critical value, the Dragon King mechanism kicks in, leading to piggybacking system-wide failures. We demonstrate that the size of the initial failed weak cluster predicts the likelihood of a Dragon King event with high accuracy and we develop a simple control strategy which also reveals that a random upgrade can inadvertently make the system more vulnerable. The Dragon Kings observed are self-organized, existing throughout the parameter regime. 	
1401.2448v1	http://arxiv.org/pdf/1401.2448v1	2014	A 4D synchrotron X-ray tomography study of the formation of hydrocarbon   migration pathways in heated organic-rich shale	Hamed Panahi|Maya Kobchenko|Francois Renard|Adriano Mazzini|Julien Scheibert|Dag Kristian Dysthe|Bjorn Jamtveit|Anders Malthe-Sørenssen|Paul Meakin	  Recovery of oil from oil shales and the natural primary migration of hydrocarbons are closely related processes that have received renewed interests in recent years because of the ever tightening supply of conventional hydrocarbons and the growing production of hydrocarbons from low permeability tight rocks. Quantitative models for conversion of kerogen into oil and gas and the timing of hydrocarbon generation have been well documented. However, lack of consensus about the kinetics of hydrocarbon formation in source rocks, expulsion timing and how the resulting hydrocarbons escape from or are retained in the source rocks motivates further investigation. In particular, many mechanisms for the transport of hydrocarbons from the source rocks in which they are generated into adjacent rocks with higher permeabilities and smaller capillary entry pressures have been proposed, and a better understanding of this complex process (primary migration) is needed. To characterize these processes it is imperative to use the latest technological advances. In this study, it is shown how insights into hydrocarbon migration in source rocks can be obtained by using sequential high resolution synchrotron X-ray tomography. Three-dimensional (3D) images of several immature "shale" samples were constructed at resolutions close to 5 micrometers. This is sufficient to resolve the source rock structure down to the grain level, but very fine grained silt particles, clay particles and colloids cannot be resolved. Samples used in this investigation came from the R-8 unit in the upper part of the Green River Shale, which is organic rich, varved, lacustrine marl formed in Eocene Lake Uinta, United States of America. One Green River Shale sample was heated in-situ up to 400{\deg}C as X-ray tomography images were recorded. The other samples were scanned before and after heating at 400{\deg}C. During the heating phase, the organic matter was decomposed, and gas was released. Gas expulsion from the low permeability shales was coupled with formation of microcracks. The main technical difficulty was numerical extraction of microcracks that have apertures in the 5 to 30 micrometer range (with 5 micrometers being the resolution limit) from a large 3D volume of X-ray attenuation data. The main goal of the work presented here is to develop a methodology to process these 3D data and image the cracks. This methodology is based on several levels of spatial filtering and automatic recognition of connected domains. Supportive petrographic and thermogravimetric data were an important complement to this study. An investigation of the strain field using two-dimensional image correlation analyses was also performed. As one application of the four-dimensional (4D, space + time) microtomography and the developed workflow, we show that fluid generation was accompanied by crack formation. Under different conditions, in the subsurface, this might provide paths for primary migration. 	
9509004v1	http://arxiv.org/pdf/quant-ph/9509004v1	1995	Quantum Mechanics as an Exotic Probability Theory	Saul Youssef	  Recent results suggest that quantum mechanical phenomena may be interpreted as a failure of standard probability theory and may be described by a Bayesian complex probability theory. 	
1608.00308v3	http://arxiv.org/pdf/1608.00308v3	2017	Experimental evidence that electrical fatigue failure obeys a   generalized Coffin-Manson law	Xiangtong He|John Y. Fu	  The empirical Coffin-Manson law has been used to characterize the low-cycle mechanical fatigue failure of metallic materials for decades. Our experimental studies reported in this letter have shown that the electrical fatigue failure in dielectrics can be well described by a fitting function having the same mathematical expression as that of the Coffin-Manson law. This observation indicates that the physical mechanism beneath the formation and evolution of atomic disordered structures, the key factor influencing both mechanical and electrical fatigue, might be the same. 	
1712.09116v1	http://arxiv.org/pdf/1712.09116v1	2017	A Mechanism of Failure in Shear Bands	Mohammad E. Torki|A. Amine Benzerga	  We have carried out dilatant plasticity simulations to investigate the process of failure inside a shear band. The constitutive model accounts for possibly inhomogeneous flow within the band, void rotation and void elongation. We found that the material in the band may soften with no increase in the void volume fraction. For a given matrix hardening capacity, the rate of softening was found to depend strongly on the ratio of shear band width to in-plane void spacing. The emergent softening led to complete loss of load bearing capacity thereby providing a physical mechanism of failure in shear bands. The mechanism is consistent with essential features of shear-fractured specimens in terms of surface roughness, porosity and dimple shape. 	
0207685v1	http://arxiv.org/pdf/cond-mat/0207685v1	2002	Optimizing a Ratchet Gear	I. M. Sokolov	  The energetic efficiencies of rocked ratchets reported in the literature typically lie in the sub-percent range. We discuss the problem of optimization of the energetic efficiency of a ratchet, and show that considerably higher efficiencies can be achieved; however this assumes a fine-tuning of the parameters of the system. The domain of parameters corresponding to high efficiencies is typically narrow. 	
0307336v1	http://arxiv.org/pdf/cond-mat/0307336v1	2003	Drift by Dichotomous Markov Noise	I. Bena|C. Van den Broeck|R. Kawai|Katja Lindenberg	  We derive explicit results for the asymptotic probability density and drift velocity in systems driven by dichotomous Markov noise, including the situation in which the asymptotic dynamics crosses {\em unstable} fixed points. The results are illustrated on the problem of the rocking ratchet. 	
0409287v1	http://arxiv.org/pdf/cond-mat/0409287v1	2004	Diffusion accompanying noise induced transport in frictional ratchets	Raishma Krishnan|Debasis Dan|A. M. Jayannavar	  We study the noise induced transport of an overdamped Brownian particle in frictional ratchet systems in the presence of external Gaussian white noise fluctuations. The analytical expressions for current and diffusion coefficient are derived and the reliability or coherence of transport are discussed by means of their ratio. We show that frictional ratchets exhibit larger coherence as compared to the flashing and rocking ratchets. 	
1705.01051v1	http://arxiv.org/pdf/1705.01051v1	2017	Impact of rough potentials in rocked ratchet performance	S. Camargo|C. Anteneodo	  We consider thermal ratchets modeled by overdamped Brownian motion in a spatially periodic potential with a tilting process, both unbiased on average. We investigate the impact of the introduction of roughness in the potential profile, over the flux and efficiency of the ratchet. Both amplitude and wavelength that characterize roughness are varied. We show that depending on the ratchet parameters, rugosity can either spoil or enhance the ratchet performance. 	
0701015v2	http://arxiv.org/pdf/cs/0701015v2	2007	Asynchronous Implementation of Failure Detectors with partial   connectivity and unknown participants	Pierre Sens|Luciana Arantes|Mathieu Bouillaguet|Véronique Martin|Fabiola Greve	  We consider the problem of failure detection in dynamic networks such as MANETs. Unreliable failure detectors are classical mechanisms which provide information about process failures. However, most of current implementations consider that the network is fully connected and that the initial number of nodes of the system is known. This assumption is not applicable to dynamic environments. Furthermore, such implementations are usually timer-based while in dynamic networks there is no upper bound for communication delays since nodes can move. This paper presents an asynchronous implementation of a failure detector for unknown and mobile networks. Our approach does not rely on timers and neither the composition nor the number of nodes in the system are known. We prove that our algorithm can implement failure detectors of class <>S when behavioral properties and connectivity conditions are satisfied by the underlying system. 	
0205090v1	http://arxiv.org/pdf/physics/0205090v1	2002	Effects of Defects on the Strength of Nanotubes:   Experimental-Computational Comparisons	T. Belytschko|S. P. Xiao|R. Ruoff	  The failure stresses and strains of nanotubes given by theoretical or numerical predictions are much higher than observed in experiments. We show that defects can explain part of this discrepancy: for an n-atom defect with 2<=n<=8, the range of failure stresses for a molecular mechanics calculation is found to be 36GPa to 64GPa. This compares quite well with upper end of the experimental failure stresses, 11GPa to 63GPa. The computed failure strains are 4% to 8%, whereas the experimental values are 2% to 13%. The underprediction of failure strains can be explained by the slippage that occurred in the experiments. The failure processes of nanotubes are clearly brittle in both the experiments and our calculations. 	
0910.0708v1	http://arxiv.org/pdf/0910.0708v1	2009	Robust Failure Detection Architecture for Large Scale Distributed   Systems	Ciprian Mihai Dobre|Florin Pop|Alexandru Costan|Mugurel Ionut Andreica|Valentin Cristea	  Failure detection is a fundamental building block for ensuring fault tolerance in large scale distributed systems. There are lots of approaches and implementations in failure detectors. Providing flexible failure detection in off-the-shelf distributed systems is difficult. In this paper we present an innovative solution to this problem. Our approach is based on adaptive, decentralized failure detectors, capable of working asynchronous and independent on the application flow. The proposed solution considers an architecture for the failure detectors, based on clustering, the use of a gossip-based algorithm for detection at local level and the use of a hierarchical structure among clusters of detectors along which traffic is channeled. The solution can scale to a large number of nodes, considers the QoS requirements of both applications and resources, and includes fault tolerance and system orchestration mechanisms, added in order to asses the reliability and availability of distributed systems. 	
1506.08352v1	http://arxiv.org/pdf/1506.08352v1	2015	Threshold for the Outbreak of Cascading Failures in Degree-degree   Uncorrelated Networks	Junbiao Liu|Xinyu Jin|Lurong Jiang|Yongxiang Xia|Bo Ouyang|Fang Dong|Yicong Lang|Wenping Zhang	  In complex networks, the failure of one or very few nodes may cause cascading failures. When this dynamical process stops in steady state, the size of the giant component formed by remaining un-failed nodes can be used to measure the severity of cascading failures, which is critically important for estimating the robustness of networks. In this paper, we provide a cascade of overload failure model with local load sharing mechanism, and then explore the threshold of node capacity when the large-scale cascading failures happen and un-failed nodes in steady state cannot connect to each other to form a large connected sub-network. We get the theoretical derivation of this threshold in degree-degree uncorrelated networks, and validate the effectiveness of this method in simulation. This threshold provide us a guidance to improve the network robustness under the premise of limited capacity resource when creating a network and assigning load. Therefore, this threshold is useful and important to analyze the robustness of networks. 	
1610.07997v1	http://arxiv.org/pdf/1610.07997v1	2016	Artificial Intelligence Safety and Cybersecurity: a Timeline of AI   Failures	Roman V. Yampolskiy|M. S. Spellchecker	  In this work, we present and analyze reported failures of artificially intelligent systems and extrapolate our analysis to future AIs. We suggest that both the frequency and the seriousness of future AI failures will steadily increase. AI Safety can be improved based on ideas developed by cybersecurity experts. For narrow AIs safety failures are at the same, moderate, level of criticality as in cybersecurity, however for general AI, failures have a fundamentally different impact. A single failure of a superintelligent system may cause a catastrophic event without a chance for recovery. The goal of cybersecurity is to reduce the number of successful attacks on the system; the goal of AI Safety is to make sure zero attacks succeed in bypassing the safety mechanisms. Unfortunately, such a level of performance is unachievable. Every security system will eventually fail; there is no such thing as a 100% secure system. 	
1703.03106v1	http://arxiv.org/pdf/1703.03106v1	2017	Oxygen migration during resistance switching and failure of hafnium   oxide memristors	Suhas Kumar|Ziwen Wang|Xiaopeng Huang|Niru Kumari|Noraica Davila|John Paul Strachan|David Vine|A. L. David Kilcoyne|Yoshio Nishi|R. Stanley Williams	  While the recent establishment of the role of thermophoresis/diffusion-driven oxygen migration during resistance switching in metal oxide memristors provided critical insights required for memristor modeling, extended investigations of the role of oxygen migration during ageing and failure remain to be detailed. Such detailing will enable failure-tolerant design, which can lead to enhanced performance of memristor-based next-generation storage-class memory. Here we directly observed lateral oxygen migration using in-situ synchrotron x-ray absorption spectromicroscopy of HfOx memristors during initial resistance switching, wear over millions of switching cycles, and eventual failure, through which we determined potential physical causes of failure. Using this information, we reengineered devices to mitigate three failure mechanisms, and demonstrated an improvement in endurance of about three orders of magnitude. 	
1703.00626v1	http://arxiv.org/pdf/1703.00626v1	2017	The RowHammer Problem and Other Issues We May Face as Memory Becomes   Denser	Onur Mutlu	  As memory scales down to smaller technology nodes, new failure mechanisms emerge that threaten its correct operation. If such failure mechanisms are not anticipated and corrected, they can not only degrade system reliability and availability but also, perhaps even more importantly, open up security vulnerabilities: a malicious attacker can exploit the exposed failure mechanism to take over the entire system. As such, new failure mechanisms in memory can become practical and significant threats to system security.   In this work, we discuss the RowHammer problem in DRAM, which is a prime (and perhaps the first) example of how a circuit-level failure mechanism in DRAM can cause a practical and widespread system security vulnerability. RowHammer, as it is popularly referred to, is the phenomenon that repeatedly accessing a row in a modern DRAM chip causes bit flips in physically-adjacent rows at consistently predictable bit locations. It is caused by a hardware failure mechanism called DRAM disturbance errors, which is a manifestation of circuit-level cell-to-cell interference in a scaled memory technology. We analyze the root causes of the RowHammer problem and examine various solutions. We also discuss what other vulnerabilities may be lurking in DRAM and other types of memories, e.g., NAND flash memory or Phase Change Memory, that can potentially threaten the foundations of secure systems, as the memory technologies scale to higher densities. We conclude by describing and advocating a principled approach to memory reliability and security research that can enable us to better anticipate and prevent such vulnerabilities. 	
0603205v2	http://arxiv.org/pdf/physics/0603205v2	2006	Electro-Magnetic Earthquake Bursts and Critical Rupture of Peroxy Bond   Networks in Rocks	F. Freund|D. Sornette	  We propose a mechanism for the low frequency electromagnetic emissions and other electromagnetic phenomena which have been associated with earthquakes. The mechanism combines the critical earthquake concept and the concept of crust acting as a charging electric battery under increasing stress. The electric charges are released by activation of dormant charge carriers in the oxygen anion sublattice, called peroxy bonds or positive hole pairs (PHP), where a PHP represents an $O_3X/^{OO}\backslash YO_3$ with $X,Y = Si^{4+}, Al^{3+}...$, i.e. an $O^-$ in a matrix of $O^{2-}$ of silicates. We propose that PHP are activated by plastic deformations during the slow cooperative build-up of stress and the increasingly correlated damage culminating in a large ``critical'' earthquake. Recent laboratory experiments indeed show that stressed rocks form electric batteries which can release their charge when a conducting path closes the equivalent electric circuit. We conjecture that the intermittent and erratic occurrences of EM signals are a consequence of the progressive build-up of the battery charges in the Earth crust and their erratic release when crack networks are percolating throughout the stressed rock volumes, providing a conductive pathway for the battery currents to discharge. EM signals are thus expected close to the rupture, either slightly before or after, that is, when percolation is most favored. 	
0804.2749v2	http://arxiv.org/pdf/0804.2749v2	2008	Solvent viscosity dependence for enzymatic reactions	A. E. Sitnitsky	  A mechanism for relationship of solvent viscosity with reaction rate constant at enzyme action is suggested. It is based on fluctuations of electric field in enzyme active site produced by thermally equilibrium rocking (cranckshaft motion) of the rigid plane (in which the dipole moment $\approx 3.6 D$ lies) of a favourably located and oriented peptide group (or may be a few of them). Thus the rocking of the plane leads to fluctuations of the electric field of the dipole moment. These fluctuations can interact with the reaction coordinate because the latter in its turn has transition dipole moment due to separation of charges at movement of the reacting system along it. The rocking of the plane of the peptide group is sensitive to the microviscosity of its environment in protein interior and the latter is a function of the solvent viscosity. Thus we obtain an additional factor of interrelationship for these characteristics with the reaction rate constant. We argue that due to the properties of the cranckshaft motion the frequency spectrum of the electric field fluctuations has a sharp resonance peak at some frequency and the corresponding Fourier mode can be approximated as oscillations. We employ a known result from the theory of thermally activated escape with periodic driving to obtain the reaction rate constant and argue that it yields reliable description of the preexponent where the dependence on solvent viscosity manifests itself. The suggested mechanism is shown to grasp the main feature of this dependence known from the experiment and satisfactorily yields the upper limit of the fractional index of a power in it. 	
0010023v1	http://arxiv.org/pdf/quant-ph/0010023v1	2000	Macroscopic Local Realism Incompatible with Quantum Mechanics: Failure   of Local Realism where Measurements give Macroscopic Uncertainties	M. D. Reid	  We show that quantum mechanics predicts a contradiction with local hidden variable theories for photon number measurements which have limited resolving power, to the point of imposing an uncertainty in the photon number result which is macroscopic in absolute terms. We show how this can be interpreted as a failure of a new premise, macroscopic local realism. 	
0808.2855v2	http://arxiv.org/pdf/0808.2855v2	2008	Comment on "Failure of the work-Hamiltonian connection for free-energy   calculations" by Jose M. G. Vilar and J. Miguel Rubi	Luca Peliti	  I point out that the arguments raised by Vilar and Rubi against the work-Hamiltonian connection in free-energy calculations imply, if correct, the failure of the statistical mechanical expression of the thermodynamical free-energy via the logarithm of the partition function. 	
1402.6586v1	http://arxiv.org/pdf/1402.6586v1	2014	Approach to failure in porous granular materials under compression	F. Kun|I. Varga|S. Lennartz-Sassinek|I. G. Main	  We investigate the approach to catastrophic failure in a model porous granular material undergoing uniaxial compression. A discrete element computational model is used to simulate both the micro-structure of the material and the complex dynamics and feedbacks involved in local fracturing and the production of crackling noise. Under strain-controlled loading micro-cracks initially nucleate in an uncorrelated way all over the sample. As loading proceeds the damage localizes into a narrow damage band inclined at 30-45 degrees to the load direction. Inside the damage band the material is crushed into a poorly-sorted mixture of mainly fine powder hosting some larger fragments. The mass probability density distribution of particles in the damage zone is a power law of exponent 2.1, similar to a value of 1.87 inferred from observations of the length distribution of wear products (gouge) in natural and laboratory faults. Dynamic bursts of radiated energy, analogous to acoustic emissions observed in laboratory experiments on porous sedimentary rocks, are identified as correlated trails or cascades of local ruptures that emerge from the stress redistribution process. As the system approaches macroscopic failure consecutive bursts become progressively more correlated. Their size distribution is also a power law, with an equivalent Gutenberg-Richter b-value of 1.22 averaged over the whole test, ranging from 3 down to 0.5 at the time of failure, all similar to those observed in laboratory tests on granular sandstone samples. The formation of the damage band itself is marked by a decrease in the average distance between consecutive bursts and an emergent power law correlation integral of event locations with a correlation dimension of 2.55, also similar to those observed in the laboratory (between 2.75 and 2.25). 	
0209124v1	http://arxiv.org/pdf/cond-mat/0209124v1	2002	Fracture of disordered solids in compression as a critical phenomenon:   I. Statistical mechanics formalism	Renaud Toussaint|Steven R. Pride	  This is the first of a series of three articles that treats fracture localization as a critical phenomenon. This first article establishes a statistical mechanics based on ensemble averages when fluctuations through time play no role in defining the ensemble. Ensembles are obtained by dividing a huge rock sample into many mesoscopic volumes. Because rocks are a disordered collection of grains in cohesive contact, we expect that once shear strain is applied and cracks begin to arrive in the system, the mesoscopic volumes will have a wide distribution of different crack states. These mesoscopic volumes are the members of our ensembles. We determine the probability of observing a mesoscopic volume to be in a given crack state by maximizing Shannon's measure of the emergent crack disorder subject to constraints coming from the energy-balance of brittle fracture. The laws of thermodynamics, the partition function, and the quantification of temperature are obtained for such cracking systems. 	
0508513v1	http://arxiv.org/pdf/cond-mat/0508513v1	2005	Mechanical Failure of a Small and Confined Solid	Debasish Chaudhuri|Surajit Sengupta	  Starting from a commensurate triangular thin solid strip, confined within two hard structureless walls, a stretch along its length introduces a rectangular distortion. Beyond a critical strain the solid fails through nucleation of "smectic"-like bands. We show using computer simulations and simple density functional based arguments, how a solid-smectic transition mediates the failure. Further, we show that the critical strain introducing failure is {\em inversely} proportional to the channel width i.e. thinner strips are stronger! 	
1411.5047v1	http://arxiv.org/pdf/1411.5047v1	2014	On the thermal impact on the excavation damaged zone around deep   radioactive waste disposal	Pierre Delage	  Clays and claystones are considered in some countries (including Belgium, France and Switzerland) as a potential host rock for high activity long lived radioactive waste disposal at great depth. One of the aspects to deal with in performance assessment is related to the effects on the host rock of the temperature elevation due to the placement of exothermic wastes. The potential effects of the thermal impact on the excavated damaged zone in the close field are another important issue that was the goal of the TIMODAZ European research project. In this paper, some principles of waste disposal in clayey host rocks at great depth are first presented and a series of experimental investigations carried out on specific equipment specially developed to face the problem are presented. Both drained and undrained tests have been developed to investigate the drained thermal volume changes of clays and claystone and the thermal pressurization occurring around the galleries. This importance of proper initial saturation (under in-situ stresses) and of satisfactory drainage conditions (in spite of the significantly low permeability of claystones) is emphasized, leading to the development of a new hollow cylinder apparatus. It is observed that claystones cannot be considered as overconsolidated clays given that they can exhibit, as the Callovo-Oxfordian claystone does, a thermoplastic contraction. Mechanical and thermal hardening are however observed, extending to claystones the knowledge already gained on clays. A new method of determining in the laboratory the thermal pressurization coefficient is described and the data obtained allow completing existing data in the field. Finally, the hollow cylinder apparatus makes it possible to demonstrate that the good self-sealing properties of clays and claystones can be extended to temperature effects, an important conclusion in terms of performance assessment. 	
1509.06949v1	http://arxiv.org/pdf/1509.06949v1	2015	Co-detection of acoustic emissions during failure of heterogeneous   media: new perspectives for natural hazard early warning	J. Faillettaz|D. Or|I. Reiweger	  A promising method for real time early warning of gravity driven rupture that considers both the heterogeneity of natural media and characteristics of acoustic emissions attenuation is proposed. The method capitalizes on co-detection of elastic waves emanating from micro-cracks by multiple and spatially separated sensors. Event co-detection is considered as surrogate for large event size with more frequent co-detected events marking imminence of catastrophic failure. Using a spatially explicit fiber bundle numerical model with spatially correlated mechanical strength and two load redistribution rules, we constructed a range of mechanical failure scenarios and associated failure events (mapped into AE) in space and time. Analysis considering hypothetical arrays of sensors and consideration of signal attenuation demonstrate the potential of the co-detection principles even for insensitive sensors to provide early warning for imminent global failure. 	
1401.0045v2	http://arxiv.org/pdf/1401.0045v2	2014	The role of material strength in collisions -- Comparing solid body and   hydrodynamic physics for simulating collisions of planetesimals with icy   shells	Thomas I. Maindl|Rudolf Dvorak|Roland Speith|Christoph Schäfer	  Context. We investigate the effects of including material strength in multi-material planetesimal collisions. Aims. The differences between strengthless material models and including the full elasto-plastic model for solid bodies with brittle failure and fragmentation when treating collisions of asteroid-sized bodies as they occur frequently in early planetary systems are demonstrated. Methods. We study impacts of bodies of Ceres-mass with a solid rock impactor and a target with 30 weight-% water content as surface ice.} The initial impact velocities and impact parameters are varied between the escape velocity $v_\mathrm{esc}$ to about 6 $v_\mathrm{esc}$ and from head-on collisions to close fly-bys, respectively. We simulate the collisions using our own SPH code utilizing both strengthless material and the full elasto-plastic material model including brittle failure. Results. One of the most prominent differences is the higher degree of fragmentation and shattered debris clouds in the solid model. In most collision scenarios however, the final outcomes are very similar and differ primarily by the about one order of magnitude higher degree of fragmentation in the solid case. Also, the survivors tend to be of less mass in the solid case which also predicts a higher water loss than the strengthless hydro model. This may be an effect of the relatively low-energy impacts that cannot destroy the solid material instantly. As opposed to giant impacts we also observe an indication that some water ice gets transferred between the bodies. 	
1004.5275v2	http://arxiv.org/pdf/1004.5275v2	2010	Spatial Rock-Paper-Scissors Models with Inhomogeneous Reaction Rates	Qian He|Mauro Mobilia|Uwe C. Täuber	  We study several variants of the stochastic four-state rock-paper-scissors game or, equivalently, cyclic three-species predator-prey models with conserved total particle density, by means of Monte Carlo simulations on one- and two-dimensional lattices. Specifically, we investigate the influence of spatial variability of the reaction rates and site occupancy restrictions on the transient oscillations of the species densities and on spatial correlation functions in the quasi-stationary coexistence state. For small systems, we also numerically determine the dependence of typical extinction times on the number of lattice sites. In stark contrast with two-species stochastic Lotka-Volterra systems, we find that for our three-species models with cyclic competition quenched disorder in the reaction rates has very little effect on the dynamics and the long-time properties of the coexistence state. Similarly, we observe that site restriction only has a minor influence on the system's dynamical properties. Our results therefore demonstrate that the features of the spatial rock-paper-scissors system are remarkably robust with respect to model variations, and stochastic fluctuations as well as spatial correlations play a comparatively minor role. 	
1007.3212v1	http://arxiv.org/pdf/1007.3212v1	2010	Water/Icy Super-Earths: Giant Impacts and Maximum Water Content	Robert A. Marcus|Dimitar Sasselov|Sarah T. Stewart|Lars Hernquist	  Water-rich super-Earth exoplanets are expected to be common. We explore the effect of late giant impacts on the final bulk abundance of water in such planets. We present the results from smoothed particle hydrodynamics simulations of impacts between differentiated water(ice)-rock planets with masses between 0.5 and 5 M_Earth and projectile to target mass ratios from 1:1 to 1:4. We find that giant impacts between bodies of similar composition never decrease the bulk density of the target planet. If the commonly assumed maximum water fraction of 75wt% for bodies forming beyond the snow line is correct, giant impacts between similar composition bodies cannot serve as a mechanism for increasing the water fraction. Target planets either accrete materials in the same proportion, leaving the water fraction unchanged, or lose material from the water mantle, decreasing the water fraction. The criteria for catastrophic disruption of water-rock planets are similar to those found in previous work on super-Earths of terrestrial composition. Changes in bulk composition for giant impacts onto differentiated bodies of any composition (water-rock or rock-iron) are described by the same equations. These general laws can be incorporated into future N-body calculations of planet formation to track changes in composition from giant impacts. 	
1305.0782v2	http://arxiv.org/pdf/1305.0782v2	2013	Cell body rocking is a dominant mechanism for flagellar synchronization   in a swimming alga	Veikko Geyer|Frank Jülicher|Jonathon Howard|Benjamin M Friedrich	  The unicellular green algae Chlamydomonas swims with two flagella, which can synchronize their beat. Synchronized beating is required to swim both fast and straight. A long-standing hypothesis proposes that synchronization of flagella results from hydrodynamic coupling, but the details are not understood. Here, we present realistic hydrodynamic computations and high-speed tracking experiments of swimming cells that show how a perturbation from the synchronized state causes rotational motion of the cell body. This rotation feeds back on the flagellar dynamics via hydrodynamic friction forces and rapidly restores the synchronized state in our theory. We calculate that this `cell body rocking' provides the dominant contribution to synchronization in swimming cells, whereas direct hydrodynamic interactions between the flagella contribute negligibly. We experimentally confirmed the coupling between flagellar beating and cell body rocking predicted by our theory. This work appeared also in the Proceedings of the National Academy of Science of the U.S.A as: Geyer et al., PNAS 110(45), p. 18058(6), 2013. 	
1412.0203v1	http://arxiv.org/pdf/1412.0203v1	2014	An experimental test of the viscous anisotropy hypothesis for partially   molten rocks	Chao Qi|David L. Kohlstedt|Richard F. Katz|Yasuko Takei	  Chemical differentiation of rocky planets occurs by melt segregation away from the region of melting. The mechanics of this process, however, are complex and incompletely understood. In partially molten rocks undergoing shear deformation, melt pockets between grains align coherently in the stress field; it has been hypothesized that this anisotropy in microstructure creates an anisotropy in the viscosity of the aggregate. With the inclusion of anisotropic viscosity, continuum, two-phase-flow models reproduce the emergence and angle of melt-enriched bands that form in laboratory experiments. In the same theoretical context, these models also predict sample-scale melt migration due to a gradient in shear stress. Under torsional deformation, melt is expected to segregate radially inward. Here we present new torsional deformation experiments on partially molten rocks that test this prediction. Microstructural analyses of the distribution of melt and solid reveal a radial gradient in melt fraction, with more melt toward the centre of the cylinder. The extent of this radial melt segregation grows with progressive strain, consistent with theory. The agreement between theoretical prediction and experimental observation provides a validation of this theory, which is critical to understanding the large-scale geodynamic and geochemical evolution of Earth. 	
1607.04269v1	http://arxiv.org/pdf/1607.04269v1	2016	Zealots tame oscillations in the spatial rock-paper-scissors game	Attila Szolnoki|Matjaz Perc	  The rock-paper-scissors game is a paradigmatic model for biodiversity, with applications ranging from microbial populations to human societies. Research has shown, however, that mobility jeopardizes biodiversity by promoting the formation of spiral waves, especially if there is no conservation law in place for the total number of competing players. Firstly, we show that even if such a conservation law applies, mobility still jeopardizes biodiversity in the spatial rock-paper-scissors game if only a small fraction of links of the square lattice is randomly rewired. Secondly, we show that zealots are very effective in taming the amplitude of oscillations that emerge due to mobility and/or interaction randomness, and this regardless of whether the later is quenched or annealed. While even a tiny fraction of zealots brings significant benefits, at 5\% occupancy zealots practically destroy all oscillations regardless of the intensity of mobility, and regardless of the type and strength of randomness in the interaction structure. Interestingly, by annealed randomness the impact of zealots is qualitatively the same as by mobility, which highlights that fast diffusion does not necessarily destroy the coexistence of species, and that zealotry thus helps to recover the stable mean-field solution. Our results strengthen the important role of zealots in models of cyclic dominance, and they reveal fascinating evolutionary outcomes in structured populations that are a unique consequence of such uncompromising behavior. 	
1705.02182v1	http://arxiv.org/pdf/1705.02182v1	2017	Modeling and simulation of an acoustic well stimulation method	Carlos Pérez-Arancibia|Eduardo Godoy|Mario Durán	  This paper presents a mathematical model and a numerical procedure to simulate an acoustic well stimulation (AWS) method for enhancing the permeability of the rock formation surrounding oil and gas wells. The AWS method considered herein aims to exploit the well-known permeability-enhancing effect of mechanical vibrations in acoustically porous materials, by transmitting time-harmonic sound waves from a sound source device---placed inside the well---to the well perforations made into the formation. The efficiency of the AWS is assessed by quantifying the amount of acoustic energy transmitted from the source device to the rock formation in terms of the emission frequency and the well configuration. A simple methodology to find optimal emission frequencies for a given well configuration is presented. The proposed model is based on the Helmholtz equation and an impedance boundary condition that effectively accounts for the porous solid-fluid interaction at the interface between the rock formation and the well perforations. Exact non-reflecting boundary conditions derived from Dirichlet-to-Neumann maps are utilized to truncate the circular cylindrical waveguides considered in the model. The resulting boundary value problem is then numerically solved by means of the finite element method. A variety of numerical examples are presented in order to demonstrate the effectiveness of the proposed procedure for finding optimal emission frequencies. 	
0410320v1	http://arxiv.org/pdf/cond-mat/0410320v1	2004	Ratchet behavior in nonlinear Klein-Gordon systems with point-like   inhomogeneities	Luis Morales-Molina|Franz G. Mertens|Angel Sanchez	  We investigate the ratchet dynamics of nonlinear Klein-Gordon kinks in a periodic, asymmetric lattice of point-like inhomogeneities. We explain the underlying rectification mechanism within a collective coordinate framework, which shows that such system behaves as a rocking ratchet for point particles. Careful attention is given to the kink width dynamics and its role in the transport. We also analyze the robustness of our kink rocking ratchet in the presence of noise. We show that the noise activates unidirectional motion in a parameter range where such motion is not observed in the noiseless case. This is subsequently corroborated by the collective variable theory. An explanation for this new phenomenom is given. 	
0610728v1	http://arxiv.org/pdf/cond-mat/0610728v1	2006	Low compressible noble metal carbides with rock-salt structure: ab   initio total energy calculations of the elastic stability	Chang-Zeng Fan|Song-Yan Zeng|Zai-Ji Zhan|Ri-Ping Liu|Wen-Kui Wang|Ping Zhang|Yu-Gui Yao	  We have systematically studied the mechanical stability of all noble metal carbides with the rock-salt structure by calculating their elastic constants within the density function theory scheme. It was found that only four carbides (RuC, PdC, AgC and PtC) are mechanically stable. In particular, we have shown that RuC, PdC, and PtC have very high bulk modulus, which has been remarkably observed by the most recent experiment for the case of PtC. From the calculated density of states, we can conclude that these compounds are metallic, like the conventional group IV and group V transition metal carbides. 	
0510066v3	http://arxiv.org/pdf/math/0510066v3	2006	Modeling 1-D elastic P-waves in a fractured rock with hyperbolic jump   conditions	Bruno Lombard|Joël Piraux	  The propagation of elastic waves in a fractured rock is investigated, both theoretically and numerically. Outside the fractures, the propagation of compressional waves is described in the simple framework of one-dimensional linear elastodynamics. The focus here is on the interactions between the waves and fractures: for this purpose, the mechanical behavior of the fractures is modeled using nonlinear jump conditions deduced from the Bandis-Barton model classicaly used in geomechanics. Well-posedness of the initial-boundary value problem thus obtained is proved. Numerical modeling is performed by coupling a time-domain finite-difference scheme with an interface method accounting for the jump conditions. The numerical experiments show the effects of contact nonlinearities. The harmonics generated may provide a non-destructive means of evaluating the mechanical properties of fractures. 	
0808.3906v1	http://arxiv.org/pdf/0808.3906v1	2008	Developing of New Facets of Indirect Modeling in the Geosciences	Hamed Owladeghaffari|Hadi Shakeri|Mostafa Sharifzadeh	  In this paper, we describe some applications of Self Organizing feature map Neuro-Fuzzy Inference System (SONFIS) and Self Organizing feature map Rough Set (SORST) in analysis of permeability at a dam site and lost circulation in the drilling of three wells in Iran. Elicitation of the best rules on the information tables, exploration of the dominant structures on the behaviour of systems while they fall in to the balance of the second granulation level (rules) and highlighting of most effective attributes (parameters) on the selected systems, are some of the benefits of the proposed methods. In the other process, using complex networks (graphs) theory - as another method in not 1:1 modelling branch- mechanical behaviour of a rock joint has been investigated. Keywords: Information Granules; SONFIS; SORST; Complex Networks; Permeability; Lost Circulation; Mechanical Behavior of a Rock Joint 	 information granules, sonfis, sorst, complex networks,
permeability, lost circulation, mechanical behavior of a rock joint

1307.2064v2	http://arxiv.org/pdf/1307.2064v2	2014	Cycles of strategies and changes of distribution in public goods game:   An experimental investigation	Bin Xu	  In this communication, a simple mechanism in the optional public goods game is experimentally investigated using two experimental settings; and first time, the cyclic strategy pattern in full state space is demonstrated by means of velocity. It is, furthermore, elaborated that the strategies of cooperation, defection and nonparticipant form a Rock-Paper-Scissors type cycle, and the cycle of three strategies are persistent over 200 rounds. This cycle is very similar to the cycle given by evolutionary dynamics e.g. replicator dynamics. The mechanism that nonparticipant can sustain cooperation is driven by the Rock-Paper-Scissors type of cyclic dominance in the three strategies. That is, if the cycle is existent, the cooperation will always sustain. Meanwhile, the distribution of social states changes in the state space and from cooperation as the most frequent strategy to defection and, from defection to nonparticipant, forms a clear rotation path in a long run. These results seem to implicate that the evolutionary dynamics has ability to capture the real dynamics applying not only on biosphere, but also on human society. 	
1410.1424v1	http://arxiv.org/pdf/1410.1424v1	2014	Long-time evolution of sequestered CO$_2$ in porous media	Yossi Cohen|Daniel H. Rothman	  CO$_2$ sequestration in subsurface reservoirs is important for limiting atmospheric CO$_2$ concentrations. However, a complete physical picture able to predict the structure developing within the porous medium is lacking. We investigate theoretically reactive transport in the long-time evolution of carbon in the brine-rock environment. As CO$_2$ is injected into a brine-rock environment, a carbonate-rich region is created amid brine. Within the carbonate-rich region minerals dissolve and migrate from regions of high concentration to low concentration, along with other dissolved carbonate species. This causes mineral precipitation at the interface between the two regions. We argue that precipitation in a small layer reduces diffusivity, and eventually causes mechanical trapping of the CO$_2$. Consequently, only a small fraction of the CO$_2$ is converted to solid mineral; the remainder either dissolves in water or is trapped in its original form. We also study the case of a pure CO$_2$ bubble surrounded by brine and suggest a mechanism that may lead to a carbonate-encrusted bubble due to structural diffusion. 	
1607.08562v1	http://arxiv.org/pdf/1607.08562v1	2016	Designing allostery-inspired response in mechanical networks	Jason W. Rocks|Nidhi Pashine|Irmgard Bischofberger|Carl P. Goodrich|Andrea J. Liu|Sidney R. Nagel	  Recent advances in designing meta-materials have demonstrated that global mechanical properties of disordered spring networks can be tuned by selectively modifying only a small subset of bonds. Here, using a computationally-efficient approach, we extend this idea in order to tune more general properties of networks. With nearly complete success, we are able to produce a strain between any pair of target nodes in a network in response to an applied source strain on any other pair of nodes by removing only ~1% of the bonds. We are also able to control multiple pairs of target nodes, each with a different individual response, from a single source, and to tune multiple independent source/target responses simultaneously into a network. We have fabricated physical networks in macroscopic two- and three-dimensional systems that exhibit these responses. This targeted behavior is reminiscent of the long-range coupled conformational changes that often occur during allostery in proteins. The ease with which we create these responses may give insight into why allostery is a common means for the regulation of activity in biological molecules. 	
0801.2295v1	http://arxiv.org/pdf/0801.2295v1	2008	Ship-induced solitons as a manifestation of critical phenomena	Stanyslav Zakharov|Alexey Kryukov	  A ship, moving with small acceleration in a reservoir of uniform depth, can be subjected to a sudden hydrodynamical impact similar to collision with an underwater rock, and on water surface unusual solitary wave will start running. The factors responsible for formation of solitons induced by a moving ship are analyzed. Emphasis is given to a phenomenon observed by John Scott Russell more 170 years ago when a sudden stop of a boat preceded the occurrence of exotic water dome. In dramatic changes of polemic about the stability and mathematical description of a solitary wave, the question why "Russell's wave" occurred has not been raised, though attempts its recreation invariably suffered failure. In our report the conditions disclosing the principle of the famous event as a critical phenomenon are described. In a reservoir of uniform depth a ship can confront by a dynamic barrier within narrow limits of ship's speed and acceleration. In a wider interval of parameters a ship generates a satellite wave, which can be transformed in a different-locking soliton. These phenomena can be classified into an extensive category of dynamic barrier effects including the transition of aircrafts through the sound barrier. 	
1309.2175v2	http://arxiv.org/pdf/1309.2175v2	2014	Cascading failures in spatially-embedded random networks	Andrea Asztalos|Sameet Sreenivasan|Boleslaw K. Szymanski|Gyorgy Korniss	  Cascading failures constitute an important vulnerability of interconnected systems. Here we focus on the study of such failures on networks in which the connectivity of nodes is constrained by geographical distance. Specifically, we use random geometric graphs as representative examples of such spatial networks, and study the properties of cascading failures on them in the presence of distributed flow. The key finding of this study is that the process of cascading failures is non-self-averaging on spatial networks, and thus, aggregate inferences made from analyzing an ensemble of such networks lead to incorrect conclusions when applied to a single network, no matter how large the network is. We demonstrate that this lack of self-averaging disappears with the introduction of a small fraction of long-range links into the network. We simulate the well studied preemptive node removal strategy for cascade mitigation and show that it is largely ineffective in the case of spatial networks. We introduce an altruistic strategy designed to limit the loss of network nodes in the event of a cascade triggering failure and show that it performs better than the preemptive strategy. Finally, we consider a real-world spatial network viz. a European power transmission network and validate that our findings from the study of random geometric graphs are also borne out by simulations of cascading failures on the empirical network. 	
0701003v1	http://arxiv.org/pdf/cond-mat/0701003v1	2006	Comment on "Failure of the Jarzynski identity for a simple quantum   system"	Shaul Mukamel	  The distribution of work done on a quantum system by instantaneously changing the Hamiltonian is shown to satisfy the Jarzynski identity. 	
0905.3860v3	http://arxiv.org/pdf/0905.3860v3	2009	A Cellular Automaton Model of Damage	C. A. Serino|W. Klein|J. B. Rundle	  We investigate the role of equilibrium methods and stress transfer range in describing the process of damage. We find that equilibrium approaches are not applicable to the description of damage and the catastrophic failure mechanism if the stress transfer is short ranged. In the long range limit, equilibrium methods apply only if the healing mechanism associated with ruptured elements is instantaneous. Furthermore we find that the nature of the catastrophic failure depends strongly on the stress transfer range. Long range transfer systems have a failure mechanism that resembles nucleation. In short range stress transfer systems, the catastrophic failure is a continuous process that, in some respects, resembles a critical point. 	
1509.03437v1	http://arxiv.org/pdf/1509.03437v1	2015	Generic failure mechanisms in adhesive bonds	Philipp Hass|Falk K. Wittel|Peter Niemz	  The failure of adhesive bondlines has been studied at the microscopic level via tensile tests. Stable crack propagation could be generated by means of samples with improved geometry, which made in-situ observations possible. The interaction of cracks with adhesive bondlines under various angles to the crack propagation was the focus of this study as well as the respective loading situations for the adhesives UF, PUR, and PVAc, which have distinctly different mechanical behaviors. It is shown how adhesive properties influence the occurrence of certain failure mechanisms and determine their appearance and order of magnitude. With the observed failure mechanisms, it becomes possible to predict the propagation path of a crack through the specimen. 	
1307.4915v1	http://arxiv.org/pdf/1307.4915v1	2013	Tectonic "short circuit" of sub-horizontal fluid-saturated bodies as a   possible mechanism of the earthquake	Andrei Nechayev	  An alternative earthquake mechanism is proposed. The traditional stress mechanism of fracture formation assigned a support role. As a proximate cause of the earthquake the destruction of the roofs of sub-horizontal fluid-saturated bodies (SHFB) is considered. This collapse may occur due to redistribution of fluid pressure within the system of SHFB connected by cracks (tectonic or other nature). It can cause both shifts of rock blocks contributing to seismic shocks and various effects characteristic of foreshocks and aftershocks. 	
0307734v1	http://arxiv.org/pdf/cond-mat/0307734v1	2003	Failure properties of fiber bundle models	Srutarshi Pradhan|Bikas K. Chakrabarti	  We study the failure properties of fiber bundles when continuous rupture goes on due to the application of external load on the bundles. We take the two extreme models: equal load sharing model (democratic fiber bundles) and local load sharing model. The strength of the fibers are assumed to be distributed randomly within a finite interval. The democratic fiber bundles show a solvable phase transition at a critical stress (load per fiber). The dynamic critical behavior is obtained analytically near the critical point and the critical exponents are found to be universal. This model also shows elastic-plastic like nonlinear deformation behavior when the fiber strength distribution has a lower cut-off. We solve analytically the fatigue-failure in a democratic bundle, and the behavior qualitatively agrees with the experimental observations. The strength of the local load sharing bundles is obtained numerically and compared with the existing results. Finally we map the failure phenomena of fiber bundles in terms of magnetic model (Ising model) which may resolve the ambiguity of studying the failure properties of fiber bundles in higher dimensions. 	
0310723v1	http://arxiv.org/pdf/cond-mat/0310723v1	2003	Precursors of catastrophic failures	Srutarshi Pradhan|Bikas K. Chakrabarti	  We review here briefly the nature of precursors of global failures in three different kinds of many-body dynamical systems. First, we consider the lattice models of self-organised criticality in sandpiles and investigate numerically the effect of pulsed perturbations to the systems prior to reaching their respective critical points. We consider next, the random strength fiber bundle models, under global load sharing approximation, and derive analytically the partial failure response behavior at loading level less than its global failure or critical point. Finally, we consider the two-fractal overlap model of earthquake and analyse numerically the overlap time series data as one fractal moves over the other with uniform velocity. The precursors of global or major failure in all three cases are shown to be very well characterized and prominent. 	
0404035v1	http://arxiv.org/pdf/cond-mat/0404035v1	2004	Andrade and Critical Time-to-Failure Laws in Fiber-Matrix Composites:   Experiments and Model	H. Nechad|A. Helmstetter|R. El Guerjouma|D. Sornette	  We present creep experiments on fiber composite materials. Recorded strain rates and acoustic emission (AE) rates exhibit both a power law relaxation in the primary creep regime and a power-law acceleration before global failure. In particular, we observe time-to-failure power laws in the tertiary regime for acoustic emissions over four decades in time. We also discover correlations between some characteristics of the primary creep (exponent of the power-law and duration) and the time to failure of the samples. This result indicates that the tertiary regime is dependent on the relaxation and damage processes that occur in the primary regime and suggests a method of prediction of the time to failure based on the early time recording of the strain rate or AE rate. We consider a simple model of representative elements, interacting via democratic load sharing, with a large heterogeneity of strengths. Each element consists of a non-linear dashpot in parallel with a spring. This model recovers the experimental observations of the strain rate as a function of time. 	
1001.2202v1	http://arxiv.org/pdf/1001.2202v1	2010	Local Coulomb versus Global Failure Criterion for Granular Packings	Silke Henkes|Carolina Brito|Olivier Dauchot|Wim Van Saarloos	  Contacts at the Coulomb threshold are unstable to tangential perturbations and thus contribute to failure at the microscopic level. How is such a local property related to global failure, beyond the effective picture given by a Mohr-Coulomb type failure criterion? Here, we use a simulated bed of frictional disks slowly tilted under the action of gravity to investigate the link between the avalanche process and a global generalized isostaticity criterion. The avalanche starts when the packing as a whole is still stable according to this criterion, underlining the role of large heterogeneities in the destabilizing process: the clusters of particles with fully mobilized contacts concentrate local failure. We demonstrate that these clusters, at odds with the pile as a whole, are also globally marginal with respect to generalized isostaticity. More precisely, we observe how the condition of their stability from a local mechanical proprety progressively builds up to the generalized isostaticity criterion as they grow in size and eventually span the whole system when approaching the avalanche. 	
1211.2330v3	http://arxiv.org/pdf/1211.2330v3	2014	Simultaneous first and second order percolation transitions in   interdependent networks	Dong Zhou|Amir Bashan|Reuven Cohen|Yehiel Berezin|Nadav Shnerb|Shlomo Havlin	  In a system of interdependent networks, an initial failure of nodes invokes a cascade of iterative failures that may lead to a total collapse of the whole system in a form of an abrupt first order transition. When the fraction of initial failed nodes $1-p$ reaches criticality, $p=p_c$, the abrupt collapse occurs by spontaneous cascading failures. At this stage, the giant component decreases slowly in a plateau form and the number of iterations in the cascade, $\tau$, diverges. The origin of this plateau and its increasing with the size of the system remained unclear. Here we find that simultaneously with the abrupt first order transition a spontaneous second order percolation occurs during the cascade of iterative failures. This sheds light on the origin of the plateau and on how its length scales with the size of the system. Understanding the critical nature of the dynamical process of cascading failures may be useful for designing strategies for preventing and mitigating catastrophic collapses. 	
1502.02538v2	http://arxiv.org/pdf/1502.02538v2	2015	Consensus using Asynchronous Failure Detectors	Nancy Lynch|Srikanth Sastry	  The FLP result shows that crash-tolerant consensus is impossible to solve in asynchronous systems, and several solutions have been proposed for crash-tolerant consensus under alternative (stronger) models. One popular approach is to augment the asynchronous system with appropriate failure detectors, which provide (potentially unreliable) information about process crashes in the system, to circumvent the FLP impossibility.   In this paper, we demonstrate the exact mechanism by which (sufficiently powerful) asynchronous failure detectors enable solving crash-tolerant consensus. Our approach, which borrows arguments from the FLP impossibility proof and the famous result from CHT, which shows that $\Omega$ is a weakest failure detector to solve consensus, also yields a natural proof to $\Omega$ as a weakest asynchronous failure detector to solve consensus. The use of I/O automata theory in our approach enables us to model execution in a more detailed fashion than CHT and also addresses the latent assumptions and assertions in the original result in CHT. 	
1507.04933v1	http://arxiv.org/pdf/1507.04933v1	2015	Distributed Monitoring for Prevention of Cascading Failures in   Operational Power Grids	Martijn Warnier|Stefan Dulman|Yakup Koç|Eric Pauwels	  Electrical power grids are vulnerable to cascading failures that can lead to large blackouts. Detection and prevention of cascading failures in power grids is impor- tant. Currently, grid operators mainly monitor the state (loading level) of individual components in power grids. The complex architecture of power grids, with many interdependencies, makes it difficult to aggregate data provided by local compo- nents in a timely manner and meaningful way: monitoring the resilience with re- spect to cascading failures of an operational power grid is a challenge. This paper addresses this challenge. The main ideas behind the paper are that (i) a robustness metric based on both the topology and the operative state of the power grid can be used to quantify power grid robustness and (ii) a new proposed a distributed computation method with self-stabilizing properties can be used to achieving near real-time monitoring of the robustness of the power grid. Our con- tributions thus provide insight into the resilience with respect to cascading failures of a dynamic operational power grid at runtime, in a scalable and robust way. Com- putations are pushed into the network, making the results available at each node, allowing automated distributed control mechanisms to be implemented on top. 	
1610.06942v3	http://arxiv.org/pdf/1610.06942v3	2018	Modes of failures in disordered solids	Subhadeep Roy|Soumyajyoti Biswas|Purusattam Ray	  The two principal ingredients determining the failure modes of disordered solids are the level of heterogeneity and the length scale of the region affected in the solid following a local failure. While the latter facilitates damage nucleation, the former leads to diffused damage, the two extreme failure modes. In this study, using the random fiber bundle model as a prototype for disorder solids, we classify every failure modes that are the results of interplay between these two effects. We obtain scaling criteria for the different modes and propose a general phase diagram that provides a framework for understanding previous theoretical and experimental attempts of interpolation between these modes. 	
1711.07620v2	http://arxiv.org/pdf/1711.07620v2	2017	Stability in fiber bundle model : Existence of strong links and the   effect of disorder	Subhadeep Roy	  In this paper I have studied the fiber bundle model with a fraction {\alpha} of infinitely strong fibers. Inclusion of such unbreakable fraction has been proven to affect the failure process in early studies, especially around a critical value {\alpha}_c . The present work has a twofold purpose: (i) study of failure abruptness, mainly the brittle to quasi-brittle transition point ({\delta}_c ) with varying {\alpha} and (ii) variation of {\alpha}_c as we change the disorder introduced in the model. The brittle to quasi-brittle transition is confirmed from the failure abruptness. On the other hand, the {\alpha}_c is obtained from the knowledge of failure abruptness and statistics of avalanches. It is observed that {\delta}_c scales to lower values, suggesting more quasi-brittle like continuous failure even at low strength of disorder, when {\alpha} is increased. Also, the critical fraction {\alpha}_c, required to make the model deviate from the conventional results, increases with decreasing {\delta} values. The analytical expression for {\alpha}_c shows good agreement with the numerical result. Finally, the findings in the paper are compared with previous results as well as with the real life application of composite materials. 	
9712313v1	http://arxiv.org/pdf/cond-mat/9712313v1	1997	Conditions for abrupt failure in the democratic fiber bundle model	D. Sornette|K. -T. Leung|J. V. Andersen	  We argue that the existence of abrupt failure in the democratic fiber bundle model is more general than concluded by da Silveira in his comment (cond-mat/9709327). We refute his claim that the nature of the rupture process in the DFBM depends on the ``disorder distribution only via its large failure strength behavior''. 	
9906031v1	http://arxiv.org/pdf/cond-mat/9906031v1	1999	Time to failure of hierarchical load-transfer models of fracture	M. Vazquez-Prada|J. B. Gomez|Y. Moreno|A. F. Pacheco	  The time to failure, $T$, of dynamical models of fracture for a hierarchical load-transfer geometry is studied. Using a probabilistic strategy and juxtaposing hierarchical structures of height $n$, we devise an exact method to compute $T$, for structures of height $n+1$. Bounding $T$, for large $n$, we are able to deduce that the time to failure tends to a non-zero value when $n$ tends to infinity. This numerical conclusion is deduced for both power law and exponential breakdown rules. 	
0108514v1	http://arxiv.org/pdf/cond-mat/0108514v1	2001	The effect of disorder on the fracture nucleation process	S. Ciliberto|A. Guarino|R. Scorretti	  The statistical properties of failure are studied in a fiber bundle model with thermal noise. We show that the macroscopic failure is produced by a thermal activation of microcracks. Most importantly the effective temperature of the system is amplified by the spatial disorder (heterogeneity) of the fiber bundle. The case of a time dependent force and the validity of the Kaiser effects are also discussed. These results can give more insight to the recent experimental observations on thermally activated crack and can be useful to study the failure of electrical networks. 	
0204368v1	http://arxiv.org/pdf/cond-mat/0204368v1	2002	Functional Correlation Approach to Operational Risk in Banking   Organizations	Reimer Kuehn|Peter Neu	  A Value-at-Risk based model is proposed to compute the adequate equity capital necessary to cover potential losses due to operational risks, such as human and system process failures, in banking organizations. Exploring the analogy to a lattice gas model from physics, correlations between sequential failures are modeled by as functionally defined, heterogeneous couplings between mutually supportive processes. In contrast to traditional risk models for market and credit risk, where correlations are described by the covariance of Gaussian processes, the dynamics of the model shows collective phenomena such as bursts and avalanches of process failures. 	
0509290v1	http://arxiv.org/pdf/cond-mat/0509290v1	2005	Optimization of scale-free network for random failures	Jian-Guo Liu|Zhong-Tuo Wang|Yan-Zhong Dang	  It has been found that the networks with scale-free distribution are very resilient to random failures. The purpose of this work is to determine the network design guideline which maximize the network robustness to random failures with the average number of links per node of the network is constant. The optimal value of the distribution exponent and the minimum connectivity to different network size are given in this paper. Finally, the optimization strategy how to improve the evolving network robustness is given. 	
1109.6679v1	http://arxiv.org/pdf/1109.6679v1	2011	Studies of VCSEL Failures in the Optical Readout Systems of the ATLAS   Silicon Trackers and Liquid Argon Calorimeters	Mark S. Cooke	  The readout systems for the ATLAS silicon trackers and liquid argon calorimeters utilize vertical-cavity surface-emitting laser diodes to communicate between on and off detector readout components. A number of these VCSEL devices have failed well before their expected lifetime. We summarize the failure history and present what has been learned thus far about failure mechanisms and the dependence of the lifetime on environmental conditions. 	
1607.01858v2	http://arxiv.org/pdf/1607.01858v2	2016	Mechanical Properties of Phosphorene Nanotubes: A Density Functional   Tight-Binding Study	V. Sorkin|Y. W. Zhang	  Using density functional tight-binding method, we studied the elastic properties, deformation and failure of armchair (AC) and zigzag (ZZ) phosphorene nano tubes (PNTs) under uniaxial tensile strain. We found that the deformation and failure of PNTs are very anisotropic. For ZZ PNTs, three deformation phases are recognized: The primary linear elastic phase, which is associated with the interactions between the neighboring puckers, succeeded by the bond rotation phase, where the puckered configuration of phosphorene is smoothed via bond rotation, and lastly the bond elongation phase, where the P-P bonds are directly stretched up to the maximally allowed limit and the failure is initiated by the rupture of the most stretched bonds. 	
1210.6647v2	http://arxiv.org/pdf/1210.6647v2	2013	Relative acceleration approach for conduction failure of cardiac   excitation propagation on anisotropic curved surfaces	Sehun Chun	  In cardiac electrophysiology, it is important to predict the necessary conditions for conduction failure, the failure of the cardiac excitation propagation even in the presence of normal excitable tissue, in high-dimensional anisotropic space because these conditions may provide feasible mechanisms for abnormal excitation propagations such as atrial re-entry and, subsequently, atrial fibrillation even without taking into account the time-dependent refractory region. Some conditions of conduction failure have been studied for anisotropy or simple curved surfaces, but the general conditions on anisotropic curved surfaces (anisotropic and curved surface) remain unknown. To predict and analyze conduction failure on anisotropic curved surfaces, a new analytic approach is proposed, called the relative acceleration approach borrowed from spacetime physics. Motivated by a discrete model of cardiac excitation propagation, this approach is based on the hypothesis that a large relative acceleration can translate to a dramatic increase in the curvature of the wavefront and, subsequently, to conduction failure. For simple anisotropic surfaces, the relative acceleration approach is validated by computational simulations or the previously known results from the kinematics approach. As a practical application, this approach is proposed to provide theoretical explanations of the mechanism of cardiac excitation propagation around the pulmonary vein with anatomically observed anisotropy. 	
1307.3182v1	http://arxiv.org/pdf/1307.3182v1	2013	Flaw-driven Failure in Nanostructures	X. Wendy Gu|Zhaoxuan Wu|Yong-Wei Zhang|David J. Srolovitz|Julia R. Greer	  Understanding failure in nanomaterials is critical for the design of reliable structural materials and small-scale devices that have components or microstructural elements at the nanometer length scale. No consensus exists on the effect of flaws on fracture in bulk nanostructured materials or in nanostructures. Proposed theories include nanoscale flaw tolerance and maintaining macroscopic fracture relationships at the nanoscale with virtually no experimental support. We explore fracture mechanisms in nanomaterials via nanomechanical experiments on nanostructures with pre-fabricated surface flaws in combination with molecular dynamics simulations. Nanocrystalline Pt cylinders with diameters of 120 nm with intentionally introduced surface notches were created using a template-assisted electroplating method and tested in uniaxial tension in in-situ SEM. Experiments demonstrate that 8 out of 12 samples failed at the notches and that tensile failure strengths were 1.8 GPa regardless of whether failure occurred at or away from the flaw. These findings suggest that failure location was sensitive to the presence of flaws, while strength was flaw-insensitive. Molecular dynamics simulations support these observations and show that incipient plastic deformation commences via nucleation and motion of dislocations in concert with grain boundary sliding. We postulate that such local plasticity reduces stress concentration ahead of the flaw to levels comparable with the strengths of intrinsic microstructural features like grain boundary triple junctions, a phenomenon unique to nano-scale solids that contain an internal microstructural energy landscape. This mechanism causes failure to occur at the weakest link, be it an internal inhomogeneity or a surface feature with a high local stress. 	
1706.04345v1	http://arxiv.org/pdf/1706.04345v1	2017	Towards Adaptive Resilience in High Performance Computing	Siavash Ghiasvand|Florina M. Ciorba	  Failure rates in high performance computers rapidly increase due to the growth in system size and complexity. Hence, failures became the norm rather than the exception. Different approaches on high performance computing (HPC) systems have been introduced, to prevent failures (e. g., redundancy) or at least minimize their impacts (e. g., checkpoint and restart). In most cases, when these approaches are employed to increase the resilience of certain parts of a system, energy consumption rapidly increases, or performance significantly degrades. To address this challenge, we propose on-demand resilience as an approach to achieve adaptive resilience in HPC systems. In this work, the HPC system is considered in its entirety and resilience mechanisms such as checkpointing, isolation, and migration, are activated on-demand. Using the proposed approach, the unavoidable increase in total energy consumption and system performance degradation is decreased compared to the typical checkpoint/restart and redundant resilience mechanisms. Our work aims to mitigate a large number of failures occurring at various layers in the system, to prevent their propagation, and to minimize their impact, all of this in an energy-saving manner. In the case of failures that are estimated to occur but cannot be mitigated using the proposed on-demand resilience approach, the system administrators will be notified in view of performing further investigations into the causes of these failures and their impacts. 	
1610.07340v1	http://arxiv.org/pdf/1610.07340v1	2016	Mechanical Properties and Failure Behavior of Phosphorene with Grain   Boundaries	V. Sorkin|Y. W. Zhang	  Using density functional tight-binding method, we studied the effect of grain boundaries on the mechanical properties and failure behavior of phosphorene. We found that the large angle tilt boundaries with a higher density of (5|7) defect pairs (oriented along the AC direction) are stronger than the low-angle tilt boundaries with a lower defect density, and similarly the large angle boundaries with a higher density of (4|8) defect pairs (oriented along the ZZ direction) are stronger than the low-angle boundaries with a lower defect density. The failure is due to the rupture of the most pre-strained bonds in the heptagons of the (5|7) defect pair or octagons of the (4|8) pairs. The large-angle grain boundaries are better off in accommodating the pre-strained bonds in heptagons and octagons defects, leading to a higher failure stress and strain. The results cannot be described by Griffith-type fracture mechanics criterion since it does not take into account the bond pre-stretching. Interestingly, these anomalous mechanical and failure characteristics of tilt grain boundaries in phosphorene are also shared by graphene and hexagonal born-nitride, signifying that they may be universal for 2D materials. The findings revealed here may be useful in tuning the mechanical properties of phosphorene via defect engineering for specific applications. 	
0107315v2	http://arxiv.org/pdf/cond-mat/0107315v2	2003	A Thermodynamic Model for Prebiotic Protein Function	Ayse Erzan|Erkan Tuzel	  We propose a scenario for the prebiotic co-evolution of RNA and of fast folding proteins with large entropy gaps as observed today. We show from very general principles that the folding and unfolding of the proteins synthesized by RNA can function as a heat pump. Rock surfaces can facilitate the folding of amino acid chains having polar and hydrophobic residues, with an accompanying heat loss to the surrounding rock. These chains then absorb heat from the soup as they unfold. This opens the way to the enhancement of RNA replication rates, by the enzymatic action of folded proteins present in greater numbers at reduced temperatures. This gives an evolutionary advantage to those RNA coding amino acid sequences with non-degenerate folded states which would provide the most efficient refrigeration. 	
0305133v2	http://arxiv.org/pdf/cond-mat/0305133v2	2004	Rock-scissors-paper game on regular small-world networks	Gyorgy Szabo|Attila Szolnoki|Rudolf Izsak	  The spatial rock-scissors-paper game (or cyclic Lotka-Volterra system) is extended to study how the spatiotemporal patterns are affected by the constructed backgrounds providing uniform number of neighbors (degree) at each site. On the square lattice this system exhibits a self-organizing pattern with equal concentration of the competing strategies (species). If the quenched background is constructed by substituting random links for the nearest neighbor bonds of a square lattice then a limit cycle occurs when the portion of random links exceeds a threshold value. This transition can also be observed if the standard link is replaced temporarily by a random one with a probability $P$ at each step of iteration. Above a second threshold value of $P$ the amplitude of global oscillation increases with time and finally the system reaches one of the homogeneous (absorbing) states. In this case the results of Monte Carlo simulations are compared with the predictions of the dynamical cluster technique evaluating all the configuration probabilities on one-, two-, four-, and six-site clusters. 	
0603778v2	http://arxiv.org/pdf/cond-mat/0603778v2	2006	Stoke's efficiency of temporally rocked ratchets	Raishma Krishnan|Jim Chacko|Mamata Sahoo|A. M. Jayannavar	  We study the generalized efficiency of an adiabatically rocked ratchet with both spatial and temporal asymmetry. We obtain an analytical expression for the generalized efficiency in the deterministic case. Generalized efficiency of the order of 50% is obtained by fine tuning of the parameter range. This is unlike the case of thermodynamic efficiency where we could readily get an enhanced efficiency of upto 90%. The observed higher values of generalized efficiency is attributed to be due to the suppression of backward current. We have also discussed briefly the differences between thermodynamic, rectification or generalized efficiency and Stoke's efficiency. Temperature is found to optimize the generalized efficiency over a wide range of parameter space unlike in the case of thermodynamic efficiency. 	
0602023v2	http://arxiv.org/pdf/nlin/0602023v2	2006	Phase-locking of a Nonlinear Optical Cavity via Rocking: Transmuting   Vortices into Phase Patterns	Adolfo Esteban Martin|Manuel Martinez Quesada|Victor B. Taranenko|Eugenio Roldan|German J. de Valcarcel	  We report experimental observation of the conversion of a phase-invariant nonlinear system into a phase-locked one via the mechanism of rocking [G. J. de Valcarcel and K. Staliunas, Phys. Rev. E 67, 026604 (2003)]. This conversion results in that vortices of the phase-invariant system are being replaced by phase patterns such as domain walls. The experiment is carried out on a photorefractive oscillator in two-wave mixing configuration.A model for the experimental device is given that reproduces the observed behavior. 	
0802.0225v1	http://arxiv.org/pdf/0802.0225v1	2008	Nonlinear Dynamics, Magnitude-Period Formula and Forecasts on Earthquake	Yi-Fang Chang	  Based on the geodynamics, an earthquake does not take place until the momentum-energy excess a faulting threshold value of rock due to the movement of the fluid layer under the rock layer and the transport and accumulation of the momentum. From the nonlinear equations of fluid mechanics, a simplified nonlinear solution of momentum corresponding the accumulation of the energy could be derived. Otherwise, a chaos equation could be obtained, in which chaos corresponds to the earthquake, which shows complexity on seismology, and impossibility of exact prediction of earthquakes. But, combining the Carlson-Langer model and the Gutenberg-Richter relation, the magnitude-period formula of the earthquake may be derived approximately, and some results can be calculated quantitatively. For example, we forecast a series of earthquakes of 2004, 2009 and 2014, especially in 2019 in California. Combining the Lorenz model, we discuss the earthquake migration to and fro. Moreover, many external causes for earthquake are merely the initial conditions of this nonlinear system. 	
0902.4626v2	http://arxiv.org/pdf/0902.4626v2	2009	Directed transport in periodically rocked random sawtooth potentials	S. I. Denisov|T. V. Lyutyy|E. S. Denisova|P. Hänggi|H. Kantz	  We study directed transport of overdamped particles in a periodically rocked random sawtooth potential. Two transport regimes can be identified which are characterized by a nonzero value of the average velocity of particles and a zero value, respectively. The properties of directed transport in these regimes are investigated both analytically and numerically in terms of a random sawtooth potential and a periodically varying driving force. Precise conditions for the occurrence of transition between these two transport regimes are derived and analyzed in detail. 	
0905.1372v1	http://arxiv.org/pdf/0905.1372v1	2009	Complex Networks on a Rock Joint	H. O. Ghaffari|M. Sharifzadeh|M. Fall|E. Evgin	  A complex network approach on a rough fracture is developed. In this manner, some hidden metric spaces (similarity measurements) between apertures profiles are set up and a general evolutionary network in two directions (in parallel and perpendicular to the shear direction) is constructed. Also, an algorithm (COmplex Networks on Apertures: CONA) is proposed in which evolving of a network is accomplished using preferential detachments and attachments of edges (based on a competition and game manner) while the number of nodes is fixed. Also, evolving of clustering coefficients and number of edges display similar patterns as well as are appeared in shear stress, hydraulic conductivity and dilation changes, which can be engaged to estimate shear strength distribution of asperities. 	
0912.5179v2	http://arxiv.org/pdf/0912.5179v2	2010	Oscillatory Dynamics in Rock-Paper-Scissors Games with Mutations	Mauro Mobilia	  We study the oscillatory dynamics in the generic three-species rock-paper-scissors games with mutations. In the mean-field limit, different behaviors are found: (a) for high mutation rate, there is a stable interior fixed point with coexistence of all species; (b) for low mutation rates, there is a region of the parameter space characterized by a limit cycle resulting from a Hopf bifurcation; (c) in the absence of mutations, there is a region where heteroclinic cycles yield oscillations of large amplitude (not robust against noise). After a discussion on the main properties of the mean-field dynamics, we investigate the stochastic version of the model within an individual-based formulation. Demographic fluctuations are therefore naturally accounted and their effects are studied using a diffusion theory complemented by numerical simulations. It is thus shown that persistent erratic oscillations (quasi-cycles) of large amplitude emerge from a noise-induced resonance phenomenon. We also analytically and numerically compute the average escape time necessary to reach a (quasi-)cycle on which the system oscillates at a given amplitude. 	
1002.0516v1	http://arxiv.org/pdf/1002.0516v1	2010	Mobility and asymmetry effects in one-dimensional rock-paper-scissors   games	Siddharth Venkat|Michel Pleimling	  As the behavior of a system composed of cyclically competing species is strongly influenced by the presence of fluctuations, it is of interest to study cyclic dominance in low dimensions where these effects are the most prominent. We here discuss rock-paper-scissors games on a one-dimensional lattice where the interaction rates and the mobility can be species dependent. Allowing only single site occupation, we realize mobility by exchanging individuals of different species. When the interaction and swapping rates are symmetric, a strongly enhanced swapping rate yields an increased mixing of the species, leading to a mean-field like coexistence even in one-dimensional systems. This coexistence is transient when the rates are asymmetric, and eventually only one species will survive. Interestingly, in our spatial games the dominating species can differ from the species that would dominate in the corresponding nonspatial model. We identify different regimes in the parameter space and construct the corresponding dynamical phase diagram. 	
1003.2427v1	http://arxiv.org/pdf/1003.2427v1	2010	Mean extinction times in cyclic coevolutionary rock-paper-scissors   dynamics	Markus Schütt|Jens Christian Claussen	  Dynamical mechanisms that can stabilize the coexistence or diversity in biology are generally of fundamental interest. In contrast to many two-strategy evolutionary games, games with three strategies and cyclic dominance like the rock-paper-scissors game (RPS) stabilize coexistence and thus preserve biodiversity in this system. In the limit of infinite populations, resembling the traditional picture of evolutionary game theory, replicator equations predict the existence of a fixed point in the interior of the phase space. But in finite populations, strategy frequencies will run out of the fixed point because of stochastic fluctuations, and strategies can even go extinct. For three different processes and for zero-sum and non-zero-sum RPS as well, we present results of extensive simulations for the mean extinction time (MET), depending on the number of agents N, and we introduce two analytical approaches for the derivation of the MET. 	
1103.3816v1	http://arxiv.org/pdf/1103.3816v1	2011	Instabilities in the dissolution of a porous matrix	Piotr Szymczak|Anthony J. C. Ladd	  A reactive fluid dissolving the surrounding rock matrix can trigger an instability in the dissolution front, leading to spontaneous formation of pronounced channels or wormholes. Theoretical investigations of this instability have typically focused on a steadily propagating dissolution front that separates regions of high and low porosity. In this paper we show that this is not the only possible dissolutional instability in porous rocks; there is another instability that operates instantaneously on any initial porosity field, including an entirely uniform one. The relative importance of the two mechanisms depends on the ratio of the porosity increase to the initial porosity. We show that the "inlet" instability is likely to be important in limestone formations where the initial porosity is small and there is the possibility of a large increase in permeability. In quartz-rich sandstones, where the proportion of easily soluble material (e.g. carbonate cements) is small, the instability in the steady-state equations is dominant. 	
1202.2951v1	http://arxiv.org/pdf/1202.2951v1	2012	Species Diversity in Rock-Paper-Scissors Game Coupling with Levy Flight	Dong Wang|Qian Zhuang|Jing Zhang|Zengru Di	  Rock-paper-scissors (RPS) game is a nice model to study the biodiversity in ecosystem. However, the previous studies only consider the nearest- neighbor- interaction among the species. In this paper, taking the long range migration into account, the effects of the interplay between nearest-neighbor-interaction and long-range-interaction of Levy flight obey the power law distance distribution with the exponent h (-0.3<h<-0.1) in spatial RPS game is investigated. Taking the probability of long range Levy flight and the power exponent as parameters, the coexistence conditions of three species are found. The critical curves for stable coexistence of three species in the parameters space are presented. It is also found that long-range-interaction with Levy flight has interesting effects on the final spatiotemporal pattern of the system. The results reveal that the long-range-interaction of Levy flight exhibit pronounced effects on biodiversity of ecosystem. 	
1306.4176v1	http://arxiv.org/pdf/1306.4176v1	2013	Dynamical analysis of an optical rocking ratchet: Theory and experiment	Alejandro V. Arzola|Karen Volke-Sepúlveda|José L. Mateos	  A thorough analysis of the dynamics in a deterministic optical rocking ratchet (introduced in A. V. Arzola et al., Phys. Rev. Lett. 106, 168104 (2011)) and a comparison with experimental results are presented. The studied system consists of a microscopic particle interacting with a periodic and asymmetric light pattern, which is driven away from equilibrium by means of an unbiased time- periodic external force. It is shown that the asymmetry of the effective optical potential depends on the relative size of the particle with respect to the spatial period, and this is analyzed as an effective mechanism for particle fractionation. The necessary conditions to obtain current reversals in the deterministic regime are discussed in detail. 	
1403.5905v1	http://arxiv.org/pdf/1403.5905v1	2014	Delay-induced transport in a rocking ratchet under feedback control	Sarah A. M. Loos|Robert Gernert|Sabine H. L. Klapp	  Based on the Fokker-Planck equation we investigate the transport of an overdamped colloidal particle in a static, asymmetric periodic potential supplemented by a time-dependent, delayed feedback force, $F_{\mathrm{fc}}$. For a given time $t$, $F_\mathrm{fc}$ depends on the status of the system at a previous time $t-\tau_\mathrm{D}$, with $\tau_\mathrm{D}$ being a delay time, specifically on the delayed mean particle displacement (relative to some "switching position"). For non-zero delay times $F_{\mathrm{fc}}(t)$ develops nearly regular oscillations generating a net current in the system. Depending on the switching position, this current is nearly as large or even larger than that in a conventional open-loop rocking ratchet. We also investigate thermodynamic properties of the delayed non-equilibrium system and we suggest an underlying Langevin equation which reproduces the Fokker-Planck results. 	
1410.2262v1	http://arxiv.org/pdf/1410.2262v1	2014	Pore-scale study of dissolution-induced changes in hydrologic properties   of rocks with binary minerals	Li Chen|Qinjun Kang|Hari S. Viswanathan|Wenquan Tao	  A pore-scale numerical model for reactive transport processes based on the Lattice Boltzmann method is used to study the dissolution-induced changes in hydrologic properties of a fractured medium and a porous medium. The solid phase of both media consists of two minerals, and a structure reconstruction method called quartet structure generation set is employed to generate the distributions of both minerals. Emphasis is put on the effects of undissolved minerals on the changes of permeability and porosity under different Peclet and Damkohler numbers. The simulation results show porous layers formed by the undissolved mineral remain behind the dissolution reaction front. Due to the large flow resistance in these porous layers, the permeability increases very slowly or even remains at a small value although the porosity increases by a large amount. Besides, due to the heterogeneous characteristic of the dissolution, the chemical, mechanical and hydraulic apertures are very different from each other. Further, simulations in complex porous structures demonstrate that the existence of the porous layers of the nonreactive mineral suppresses the wormholing phenomena observed in the dissolution of mono-mineralic rocks. 	
1604.03822v1	http://arxiv.org/pdf/1604.03822v1	2016	The role of ionized impurity scattering on the thermoelectric   performances of rock salt AgPbmSnSe2+m	Lin Pan|Sunanda Mitra|Li-Dong Zhao|Yawei Shen|Yifeng Wang|Claudia Felser|David Berardan	  We report on the successful synthesis and on the properties of polycrystalline AgPbmSnSe2+m (m = ++++, 100, 50, 25) samples with a rock salt structure. Between 160 K and 400 K, the dominant scattering process of the carriers in this system changes from acoustic phonon scattering in PbSe to ionized impurity scattering in AgPbmSnSe2+m, which synergistically optimizes electrical and thermal transport properties. Thanks to the faint amount of AgSnSe2, the Seebeck coefficient is enhanced by boosting the scattering factor, the electric conductivity is improved by the increase of the concentration of holes coupled to a limited degradation of their mobility, and the total thermal conductivity is reduced by suppressing bipolar thermal conductivity. Therefore, ZT of AgPbmSnSe2+m (m = 50) reaches 1.3 at 889 K. The mechanism suggested in this study opens new paths to improve the thermoelectric performances of other families of materials. 	
1608.08932v2	http://arxiv.org/pdf/1608.08932v2	2016	The Influence of Mobility Rate on Spiral Waves in Spatial   Rock-Paper-Scissors Games	Mauro Mobilia|Alastair M. Rucklidge|Bartosz Szczesny	  We consider a two-dimensional model of three species in rock-paper-scissors competition and study the self-organisation of the population into fascinating spiraling patterns. Within our individual-based metapopulation formulation, the population composition changes due to cyclic dominance (dominance-removal and dominance-replacement), mutations, and pair-exchange of neighboring individuals. Here, we study the influence of mobility on the emerging patterns and investigate when the pair-exchange rate is responsible for spiral waves to become elusive in stochastic lattice simulations. In particular, we show that the spiral waves predicted by the system's deterministic partial equations are found in lattice simulations only within a finite range of the mobility rate. We also report that in the absence of mutations and dominance-replacement, the resulting spiraling patterns are subject to convective instability and far-field breakup at low mobility rate. Possible applications of these resolution and far-field breakup phenomena are discussed. 	
1706.02260v1	http://arxiv.org/pdf/1706.02260v1	2017	Apex predator and the cyclic competition in a rock-paper-scissors game   of three species	C. A Souza-Filho|D. Bazeia|J. G. G. S. Ramos	  This work deals with the effects of an apex predator on the cyclic competition among three distinct species that follow the rules of the rock-paper-scissors game. The investigation develops standard stochastic simulations but is motivated by a novel procedure which is explained in the work. We add the apex predator as the fourth species in the system that contains three species that evolve following the standard rules of migration, reproduction and predation, and study how the system evolves in this new environment, in comparison with the case in the absence of the apex predator. The results show that the apex predator engenders the tendency to spread uniformly in the lattice, contributing to destroy the spiral patterns, keeping biodiversity but diminishing the average size of the clusters of the species that compete cyclically. 	
1707.08956v1	http://arxiv.org/pdf/1707.08956v1	2017	Are Triggering Rates of Labquakes Universal? Inferring Triggering Rates   From Incomplete Information	Jordi Baró|Jörn Davidsen	  The acoustic emission activity associated with recent rock fracture experiments under different conditions has indicated that some features of event-event triggering are independent of the details of the experiment and the materials used and are often even indistinguishable from tectonic earthquakes. While the event-event triggering rates or aftershock rates behave pretty much identical for all rock fracture experiments at short times, this is not the case for later times. Here, we discuss how these differences can be a consequence of the aftershock identification method used and show that the true aftershock rates might have two distinct regimes. Specifically, tests on a modified Epidemic Type Aftershock Sequence model show that the model rates cannot be correctly inferred at late times based on temporal information only if the activity rates or the branching ratio are high. We also discuss both the effect of the two distinct regimes in the aftershock rates and the effect of the background rate on the inter-event time distribution. Our findings should be applicable for inferring event-event triggering rates for many other types of triggering and branching processes as well. 	
1711.02754v1	http://arxiv.org/pdf/1711.02754v1	2017	Hamming distance and mobility behavior in generalized   rock-paper-scissors models	D. Bazeia|J. Menezes|B. F. de Oliveira|J. G. G. S. Ramos	  This work reports on two related investigations of stochastic simulations which are widely used to study biodiversity and other related issues. We first deal with the behavior of the Hamming distance under the increase of the number of species and the size of the lattice, and then investigate how the mobility of the species contributes to jeopardize biodiversity. The investigations are based on the standard rules of reproduction, mobility and predation or competition, which are described by specific rules, guided by generalization of the rock-paper-scissors game, valid in the case of three species. The results on the Hamming distance indicate that it engenders universal behavior, independently of the number of species and the size of the square lattice. The results on the mobility confirm the prediction that it may destroy diversity, if it is increased to higher and higher values. 	
0701268v1	http://arxiv.org/pdf/cond-mat/0701268v1	2007	Burst statistics as a criterion for imminent failure	Srutarshi Pradhan|Alex Hansen|Per C. Hemmer	  The distribution of the magnitudes of damage avalanches during a failure process typically follows a power law. When these avalanches are recorded close to the point at which the system fails catastrophically, we find that the power law has an exponent which differs from the one characterizing the size distribution of all avalanches. We demonstrate this analytically for bundles of many fibers with statistically distributed breakdown thresholds for the individual fibers. In this case the magnitude distribution $D(\Delta)$ for the avalanche size $\Delta$ follows a power law $\Delta^{-\xi}$ with $\xi=3/2$ near complete failure, and $\xi=5/2$ elsewhere. We also study a network of electric fuses, and find numerically an exponent 2.0 near breakdown, and 3.0 elsewhere. We propose that this crossover in the size distribution may be used as a signal for imminent system failure. 	
0910.5179v1	http://arxiv.org/pdf/0910.5179v1	2009	Damage nucleation phenomena: Statistics of times to failure	S. G. Abaimov|A. Roy|J. P. Cusumano	  In this paper we investigate the statistical behavior of an annealed continuous damage model. For different model variations we study distributions of times to failure and compare these results with the classical case of metastable nucleation in statistical mechanics. We show that our model has a tuning parameter which significantly determines the model behavior. Depending on the values of this tuning parameter, our model exhibits statistical behavior either similar to nucleation of systems in statistical mechanics or an absolutely different type of behavior intrinsic only for systems with damage. This lets us investigate the possible similarities and differences between damage phenomena and classical phenomena of nucleation in statistical mechanics. 	
0103625v1	http://arxiv.org/pdf/cond-mat/0103625v1	2001	Defensive alliances in spatial models of cyclical population   interactions	G. Szabo|T. Czaran	  As a generalization of the 3-strategy Rock-Scissors-Paper game dynamics in space, cyclical interaction models of six mutating species are studied on a square lattice, in which each species is supposed to have two dominant, two subordinated and a neutral interacting partner. Depending on their interaction topologies, these systems can be classified into four (isomorphic) groups exhibiting significantly different behaviors as a function of mutation rate. On three out of four cases three (or four) species form defensive alliances which maintain themselves in a self-organizing polydomain structure via cyclic invasions. Varying the mutation rate this mechanism results in an ordering phenomenon analogous to that of magnetic Ising model. 	
0905.0421v2	http://arxiv.org/pdf/0905.0421v2	2010	Realistic spatial and temporal earthquake distributions in a modified   Olami-Feder-Christiensen model	E. A. Jagla	  We propose and study a modified version of the Olami-Feder-Christiensen model of seismicity, that includes a mechanism of structural relaxation. We obtain realistic features of seismicity that are not obtained with the original version, mainly: aftershocks that obey the Omori law and cluster spatially around the slip surface of the main shock, and averaged frictional properties qualitatively similar to those observed in rock friction, in particular the velocity weakening effect. 	
1711.02579v1	http://arxiv.org/pdf/1711.02579v1	2017	A Numerical Study of Thermal-Hydraulic-Mechanical (THM) Simulation with   the Application of Thermal Recovery in Fractured Shale Gas Reservoirs	HanYi Wang	  We presented a general multi-physics model for shale gas flow in fractured systems, first the first time, with fully coupled thermal-hydraulic-mechanical (THM) properties. The impact of gas adsorption, real gas properties, gas flow in nano-scale pore space and geomechanics effects on total gas flow capacity are investigated. We also showed that by elevating shale rock temperature, the characteristic of gas adsorption behavior can be substantially altered. 	
9706054v1	http://arxiv.org/pdf/quant-ph/9706054v1	1997	On the failure of Bell's theorem	Gyula Bene	  Using a new approach to quantum mechanics we revisit Hardy's proof for Bell's theorem and point out a loophole in it. We also demonstrate on this example that quantum mechanics is a local realistic theory. 	
0107036v2	http://arxiv.org/pdf/cond-mat/0107036v2	2001	Precursors of catastrophe in the BTW, Manna and random fiber bundle   models of failure	Srutarshi Pradhan|Bikas K. Chakrabarti	  We have studied precursors of the global failure in some self-organised critical models of sand-pile (in BTW and Manna models) and in the random fiber bundle model (RFB). In both BTW and Manna model, as one adds a small but fixed number of sand grains (heights) to any central site of the stable pile, the local dynamics starts and continues for an average relaxation time (\tau) and an average number of topplings (\Delta) spread over a radial distance (\xi). We find that these quantities all depend on the average height (h_{av}) of the pile and they all diverge as (h_{av}) approaches the critical height (h_{c}) from below: (\Delta) (\sim (h_{c}-h_{av}))(^{-\delta}), (\tau \sim (h_{c}-h_{av})^{-\gamma}) and (\xi) (\sim) ((h_{c}-h_{av})^{-\nu}). Numerically we find (\delta \simeq 2.0), (\gamma \simeq 1.2) and (\nu \simeq 1.0) for both BTW and Manna model in two dimensions. In the strained RFB model we find that the breakdown susceptibility (\chi) (giving the differential increment of the number of broken fibers due to increase in external load) and the relaxation time (\tau), both diverge as the applied load or stress (\sigma) approaches the network failure threshold (\sigma_{c}) from below: (\chi) (\sim) ((\sigma_{c}) (-)(\sigma)^{-1/2}) and (\tau) (\sim) ((\sigma_{c}) (-)(\sigma)^{-1/2}). These self-organised dynamical models of failure therefore show some definite precursors with robust power laws long before the failure point. Such well-characterised precursors should help predicting the global failure point of the systems in advance. 	
0601290v2	http://arxiv.org/pdf/cond-mat/0601290v2	2006	Failure process of a bundle of plastic fibers	F. Raischel|F. Kun|H. J. Herrmann	  We present an extension of fiber bundle models considering that failed fibers still carry a fraction $0 \leq \alpha \leq 1$ of their failure load. The value of $\alpha$ interpolates between the perfectly brittle failure $(\alpha = 0)$ and perfectly plastic behavior $(\alpha=1)$ of fibers. We show that the finite load bearing capacity of broken fibers has a substantial effect on the failure process of the bundle. In the case of global load sharing it is found that for $\alpha \to 1$ the macroscopic response of the bundle becomes perfectly plastic with a yield stress equal to the average fiber strength. On the microlevel, the size distribution of avalanches has a crossover from a power law of exponent $\approx 2.5$ to a faster exponential decay. For localized load sharing, computer simulations revealed a sharp transition at a well defined value $\alpha_c$ from a phase where macroscopic failure occurs due to localization as a consequence of local stress enhancements, to another one where the disordered fiber strength dominates the damage process. Analysing the microstructure of damage, the transition proved to be analogous to percolation. At the critical point $\alpha_c$, the spanning cluster of damage is found to be compact with a fractal boundary. The distribution of bursts of fiber breakings shows a power law behaviour with a universal exponent $\approx 1.5$ equal to the mean field exponent of fiber bundles of critical strength distributions. The model can be relevant to understand the shear failure of glued interfaces where failed regions can still transmit load by remaining in contact. 	
0509114v1	http://arxiv.org/pdf/physics/0509114v1	2005	Comparison of compact bone failure under two different loadings rates:   experimental and modelling approaches	Martine Pithioux|D. Subit|P. Chabrand	  Understanding the mechanical behaviour of bones up to failure is necesary for diagnosis and prevention of accident and trauma. As far as we know, no authors have yet studied the tensile behaviour of compact bone including failure under dynamic loadings (1m/s). The originality of this study comes from not only the analysis of compact bone failure under dynamic loadings, the results of which are compared to those obtained under quasi static loadings but also the development of a statistical model. We developed a protocol using three different devices. Firstly, an X-ray scanner to analyse bone density, secondly, a common tensile device to perform quasi static experiments and thirdly, a special device based upon a hydraulic cylinder to perform dynamic tests. For all the tests, we used the same sample shape which took into account the brittleness of the compact bone. We first performed relaxation and hysteresis tests followed by tensile tests up to failure. Viscous and plastic effects were not relevant to the compact bone behaviour so its behaviour was considered elastic and brittle. The bovine compact bone was three to four times more brittle under a dynamic load than under a quasi static one. Numerically, a statistical model, based upon the Weibull theory is used to predict the failure stress in compact bone. 	
1405.2038v1	http://arxiv.org/pdf/1405.2038v1	2014	Predicting Failure: Acoustic Emission of Berlinite under Compression	Guillaume F Nataf|Pedro O Castillo-Villa|Pathikumar Sellappan|Waltraud M Kriven|Eduard Vives|Antoni Planes|Ekhard K H Salje	  Acoustic emission has been measured and statistical characteristics have been analyzed during the stress-induced collapse of porous berlinite, AlPO4, containing up to 50 vol% porosity. Stress collapse occurs in a series of individual events (avalanches), and each avalanche leads to a jerk in sample compression with corresponding acoustic emission (AE) signals. The distribution of AE avalanche energies can be approximately described by a power law over a large stress interval. We observed several collapse mechanisms whereby less porous minerals show the superposition of independent jerks, which were not related to the major collapse at the failure stress. In highly porous berlinite (40% and 50%) an increase of the energy emission occurred near the failure point. In contrast, the less porous samples did not show such an increase in energy emission. Instead, in the near vicinity of the main failure point they showed a reduction in the energy exponent to ~ 1.4, which is consistent with the value reported for compressed porous systems displaying critical behavior. This indicated that a critical avalanche regime with a lack of precursor events occurs. In this case, all preceding large events were false alarms and unrelated to the main failure event. Our results identify a method to use pico-seismicity detection of foreshocks to warn of mine collapse before the main failure collapse occurs, which can be applied for highly porous materials only. 	
0609135v2	http://arxiv.org/pdf/cond-mat/0609135v2	2008	Spontaneous thermal runaway as an ultimate failure mechanism of   materials	S. Braeck|Y. Y. Podladchikov	  The first theoretical estimate of the shear strength of a perfect crystal was given by Frenkel [Z. Phys. 37, 572 (1926)]. He assumed that as slip occurred, two rigid atomic rows in the crystal would move over each other along a slip plane. Based on this simple model, Frenkel derived the ultimate shear strength to be about one tenth of the shear modulus. Here we present a theoretical study showing that catastrophic material failure may occur below Frenkel's ultimate limit as a result of thermal runaway. We demonstrate that the condition for thermal runaway to occur is controlled by only two dimensionless variables and, based on the thermal runaway failure mechanism, we calculate the maximum shear strength $\sigma_c$ of viscoelastic materials. Moreover, during the thermal runaway process, the magnitude of strain and temperature progressively localize in space producing a narrow region of highly deformed material, i.e. a shear band. We then demonstrate the relevance of this new concept for material failure known to occur at scales ranging from nanometers to kilometers. 	
0808.1375v3	http://arxiv.org/pdf/0808.1375v3	2009	Failure Processes in Elastic Fiber Bundles	Srutarshi Pradhan|Alex Hansen|Bikas K. Chakrabarti	  The fiber bundle model describes a collection of elastic fibers under load. the fibers fail successively and for each failure, the load distribution among the surviving fibers change. Even though very simple, the model captures the essentials of failure processes in a large number of materials and settings. We present here a review of fiber bundle model with different load redistribution mechanism from the point of view of statistics and statistical physics rather than materials science, with a focus on concepts such as criticality, universality and fluctuations. We discuss the fiber bundle model as a tool for understanding phenomena such as creep, and fatigue, how it is used to describe the behavior of fiber reinforced composites as well as modelling e.g. network failure, traffic jams and earthquake dynamics. 	
1707.06539v1	http://arxiv.org/pdf/1707.06539v1	2017	Vitality of Neural Networks under Reoccurring Catastrophic Failures	Shira Sardi|Amir Goldental|Hamutal Amir|Roni Vardi|Ido Kanter	  Catastrophic failures are complete and sudden collapses in the activity of large networks such as economics, electrical power grids and computer networks, which typically require a manual recovery process. Here we experimentally show that excitatory neural networks are governed by a non-Poissonian reoccurrence of catastrophic failures, where their repetition time follows a multimodal distribution characterized by a few tenths of a second and tens of seconds timescales. The mechanism underlying the termination and reappearance of network activity is quantitatively shown here to be associated with nodal time-dependent features, neuronal plasticity, where hyperactive nodes damage the response capability of their neighbors. It presents a complementary mechanism for the emergence of Poissonian catastrophic failures from damage conductivity. The effect that hyperactive nodes degenerate their neighbors represents a type of local competition which is a common feature in the dynamics of real-world complex networks, whereas their spontaneous recoveries represent a vitality which enhances reliable functionality. 	
1701.03486v1	http://arxiv.org/pdf/1701.03486v1	2017	Multi-step Steady-State Measurements of Low Permeability Using Series   Circuit with A Reference Rock Sample	Jun Li	  A multi-step steady-state (MSSS) method is proposed here for the measurement of low permeability. This new method can accurately and easily measure very low permeabilities of rock samples using a new setup, where the targeted rock sample and ordinary apparatus components are connected with a reference rock sample to form a series circuit. Any conventional rock sample with high permeability could be used as a reference rock sample such that the traditional steady-state measurement is feasible to accurately determine its permeability to be used as a reference value in the MSSS method. The challenging measurement of tiny mass flux rate by advanced pump system is avoided and the permeability of targeted rock sample can be directly computed using the pressure drops, sectional areas and lengths of the two connected rock samples, and the known permeability of the reference rock sample based on the mass conservation principle in a series circuit at steady state. Multi-step measurements using additional reference rock samples will be needed if the pressure drop over the first reference rock sample is too small to be accurately measured due to high permeability ratio when it is connected with the targeted rock sample to form a series circuit. The relative pressure drops can be small since the measurement of flow speed is unnecessary, which improves the accuracy in studying the dependence of gas permeability on the pore pressure. Consequently, the advantages of the MSSS method include low expense, simplicity, high accuracy and efficiency. 	
1303.2294v1	http://arxiv.org/pdf/1303.2294v1	2013	Comprehensive Analysis on the Vulnerability and Efficiency of P2P   Networks under Static Failures and Targeted Attacks	Farshad Safaei|Hamidreza Sotoodeh	  Peer to peer systems are the networks consisting of a group of nodes possible to be as wide as the Internet. These networks are required of evaluation mechanisms and distributed control and configurations, so each peer will be able to communicate with other peers. Resilience to faults, failures and attacks, are the main requirements of most communication systems and networks today. Thus, since P2P networks can be individually used as an infrastructure and an alternative for many other communication networks, they have to be more reliable, and resilient to the faults, failures and attacks compared to the client and server approach. In this work, we present a detailed study on the behavior of various P2P networks toward faults and failures, and focus on fault-tolerance subject. We consider two different static failure scenarios: a)a random strategy in which nodes or edges of the network will be removed with an equal probability and without any knowledge of the networks infrastructure, b)a targeted strategy that uses some information about the nodes, and in which the nodes with the highest degree have the most priority to be attacked. By static faults, we mean a situation where the nodes or components encounter some faults before the network starts to work or through its operation, and will remain faulty to the end of the work session. Our goal is to introduce various measures to analyzing P2P networks evaluating their vulnerability rate. The presented criteria can be used for evaluating the reliability and vulnerability of P2P networks toward both random and targeted failures. There is no limit to the number and types of failures, the presented measures are able to be used for different types of failures and even a wide range of networks. 	
1207.1980v2	http://arxiv.org/pdf/1207.1980v2	2012	Integration of natural data within a numerical model of ablative   subduction: A possible interpretation for the Alpine dynamics of the   Austroalpine crust	Manuel Roda|Maria Iole Spalla|Anna Maria Marotta	  A numerical modelling approach is used to validate the physical and ge- ological reliability of the ablative subduction mechanism during Alpine con- vergence in order to interpret the tectonic and metamorphic evolution of an inner portion of the Alpine belt: the Austroalpine Domain. The model pre- dictions and the natural data for the Austroalpine of the Western Alps agree very well in terms of P-T peak conditions, relative chronology of peak and exhumation events, P-T-t paths, thermal gradients and the tectonic evolu- tion of the continental rocks. These findings suggest that a pre-collisional evolution of this domain, with the burial of the continental rocks (induced by ablative subduction of the overriding Adria plate) and their exhumation (driven by an upwelling flow generated in a hydrated mantle wedge) could be a valid mechanism that reproduces the actual tectono-metamorphic config- uration of this part of the Alps. There is less agreement between the model predictions and the natural data for the Austroalpine of the Central-Eastern Alps. Based on the natural data available in the literature, a critical discus- sion of the other proposed mechanisms is presented, and additional geological factors that should be considered within the numerical model are suggested to improve the fitting to the numerical results; these factors include varia- tions in the continental and/or oceanic thickness, variation of the subduction rate and/or slab dip, the initial thermal state of the passive margin, the oc- currence of continental collision and an oblique convergence. 	
1407.5869v1	http://arxiv.org/pdf/1407.5869v1	2014	The application of high-resolution 3D seismic data to model the   distribution of mechanical and hydrogeological properties of a potential host   rock for the deep storage of radioactive waste in France	Jean-Luc Mari|Béatrice Yven	  In the context of a deep geological repository of high-level radioactive wastes, the French National Radioactive Waste Management Agency (Andra) has conducted an extensive characterization of the Callovo-Oxfordian argillaceous rock and surrounding formations in the Eastern Paris Basin. As part of this project, an accurate 3D seismic derived geological model is needed. The paper shows the procedure used for building the 3D seismic constrained geological model in depth by combining time-to-depth conversion of seismic horizons, consistent seismic velocity model and elastic impedance in time. It also shows how the 3D model is used for mechanical and hydrogeological studies. The 3D seismic field data example illustrates the potential of the proposed depth conversion procedure for estimating density and velocity distributions, which are consistent with the depth conversion of seismic horizons using the Bayesian Kriging method. The geological model shows good agreement with well log data obtained from a reference well, located closest to the 3D seismic survey area. Modeling of the mechanical parameters such as shear modulus, Young modulus, bulk modulus indicates low variability of parameters confirming the homogeneity of the target formation (Callovo-Oxfordian claystone). 3D modeling of a permeability index (Ik-Seis) computed from seismic attributes (instantaneous frequency, envelope, elastic impedance) and validated at the reference well shows promising potential for supporting hydrogeological simulation and decision making related to safety issues. 	
1507.04539v1	http://arxiv.org/pdf/1507.04539v1	2015	On comparison of simulated and observed seismicity	Aleksandr M. Linkov|Liliana Rybarska-Rusinek|Victor V. Zoubkov	  Numerical simulation of seismicity has been successfully developed and used for the two last decades. Presently, the general theory of modeling and the progress in computational techniques provide wide options for simulation of seismic and aseismic events with various source mechanisms accounting for blocky structure of rock mass, inclusions, faults, cracks, complicated contact conditions and various mechanical properties of rock. Meanwhile, in practical applications, the input data are limited and uncertain. The data on observed seismicity are also often limited with a few parameters, like coordinates and time. The paper aims to agree the input and output data, used in and provided by numerical simulations, with uncertain and limited data of direct observations. For the input parameters, we suggest their minimal set, which complies with commonly available data. For output seismic parameters, we distinguish three major groups, which are provided by field observations. The first group includes the common (minimal) data on distributions of the event location. These distributions are of special value for improving the input data on geometrical features of a problem. The second group employs the data (commonly available, as well) on the event magnitude. These distributions are of exceptional need for evaluating the risk of strong events. The third group employs data on the event source mechanism. It is based on the tensor of seismic moment/potency, provided by advanced mining seismic systems. This group includes distributions of the geometrical parameters of the event source (orientation of nodal planes, B, P and T directions). It is especially important when establishing and using the connection between stresses and seismicity. The exposition is illustrated by considering an example of long-wall mining in a coal seam. 	
1602.04032v1	http://arxiv.org/pdf/1602.04032v1	2016	A Truthful Mechanism with Biparameter Learning for Online Crowdsourcing	Satyanath Bhat|Divya Padmanabhan|Shweta Jain|Y Narahari	  We study a problem of allocating divisible jobs, arriving online, to workers in a crowdsourcing setting which involves learning two parameters of strategically behaving workers. Each job is split into a certain number of tasks that are then allocated to workers. Each arriving job has to be completed within a deadline and each task has to be completed satisfying an upper bound on probability of failure. The job population is homogeneous while the workers are heterogeneous in terms of costs, completion times, and times to failure. The job completion time and time to failure of each worker are stochastic with fixed but unknown means. The requester is faced with the challenge of learning two separate parameters of each (strategically behaving) worker simultaneously, namely, the mean job completion time and the mean time to failure. The time to failure of a worker depends on the duration of the task handled by the worker. Assuming non-strategic workers to start with, we solve this biparameter learning problem by applying the Robust UCB algorithm. Then, we non-trivially extend this algorithm to the setting where the workers are strategic about their costs. Our proposed mechanism is dominant strategy incentive compatible and ex-post individually rational with asymptotically optimal regret performance. 	
1607.08300v2	http://arxiv.org/pdf/1607.08300v2	2017	Nonlinear viscoelasticity and generalized failure criterion for polymer   gels	Bavand Keshavarz|Thibaut Divoux|Sébastien Manneville|Gareth H. McKinley	  Polymer gels behave as soft viscoelastic solids and exhibit a generic nonlinear mechanical response characterized by pronounced stiffening prior to irreversible failure, most often through macroscopic fractures. Here, we aim at capturing the latter scenario for a protein gel using a nonlinear integral constitutive equation built upon ($i$) the linear viscoelastic response of the gel, here well described by a power-law relaxation modulus, and ($ii$) the nonlinear viscoelastic properties of the gel, encoded into a "damping function". Such formalism predicts quantitatively the gel mechanical response to a shear start-up experiment, up to the onset of macroscopic failure. Moreover, as the gel failure involves the irreversible growth of macroscopic cracks, we couple the latter stress response with Bailey's durability criterion for brittle solids in order to predict the critical values of the stress $\sigma_c$ and strain $\gamma_c$ at the failure point, and how they scale with the applied shear rate. The excellent agreement between theory and experiments suggests that the crack growth in this soft viscoelastic gel is a Markovian process, and that Baileys' criterion extends well beyond hard materials such as metals, glasses, or minerals. 	
0112406v1	http://arxiv.org/pdf/astro-ph/0112406v1	2001	Rapid Formation of Ice Giant Planets	Alan P. Boss|George W. Wetherill|Nader Haghighipour	  The existence of Uranus and Neptune presents severe difficulties for the core accretion model for the formation of ice giant planets. We suggest an alternative mechanism, namely disk instability leading to the formation of gas giant protoplanets, coagulation and settling of dust grains to form ice/rock cores at their centers, and photoevaporation of their gaseous envelopes by a nearby OB star, as a possible means of forming ice giant planets. 	
9605171v2	http://arxiv.org/pdf/cond-mat/9605171v2	1996	Voltage rectification by a SQUID ratchet	Ivar Zapata|Roland Bartussek|Fernando Sols|Peter Hänggi	  We argue that the phase across an asymmetric dc SQUID threaded by a magnetic flux can experience an effective ratchet (periodic and asymmetric) potential. Under an external ac current, a rocking ratchet mechanism operates whereby one sign of the time derivative of the phase is favored. We show that there exists a range of parameters in which a fixed sign (and, in a narrower range, even a fixed value) of the average voltage across the ring occurs, regardless of the sign of the external current dc component. 	
9908338v2	http://arxiv.org/pdf/cond-mat/9908338v2	2000	Stokes' drift: a rocking ratchet	I. Bena|M. Copelli|C. Van den Broeck	  We derive the explicit analytic expression for the Stokes' drift in one dimension in the presence of a dichotomic Markov forcing. For small amplitudes of the forcing, the drift is enhanced, but the enhancement is reduced with increasing frequency of the forcing. On the other hand, a reduction of the drift or even a flux reversal can be induced at larger amplitudes, while the flux is now found to be an increasing function of the perturbation frequency. 	
0101369v1	http://arxiv.org/pdf/cond-mat/0101369v1	2001	Modellization of hydraulic fracturing of porous materials	F. Tzschichholz|M. Wangen	  We review microstructural fracture growth models suitable for the study of hydraulic fracture processes in disordered porous materials and present some basic results. It is shown that microstructural models exhibit certain similarities to corresponding theories of continua. These similarities are most easily demonstrated for simple crack geometries, i.e., straight cracks (finite size scalings). However, there exist even scaling relations which are completely independent of the particular employed crack structure. Furthermore it is demonstrated that disorder in cohesional/flow properties can influence the crack growth and the resulting fracture geometry in an essential way. 	
0202258v1	http://arxiv.org/pdf/cond-mat/0202258v1	2002	Rocking bistable systems: use and abuse of Linear Response Theory	J. Casado-Pascual|J. Gómez-Ordóñez|M. Morillo|P. Hänggi	  The response of a nonlinear stochastic system driven by an external sinusoidal time dependent force is studied by a variety of numerical and analytical approximations. The validity of linear response theory is put to a critical test by comparing its predictions with numerical solutions over an extended parameter regime of driving amplitudes and frequencies. The relevance of the driving frequency for the applicability of linear response theory is explored. 	
0208549v1	http://arxiv.org/pdf/cond-mat/0208549v1	2002	Multi-band quantum ratchets	M. Grifoni|M. S. Ferreira|J. Peguiron|J. B. Majer	  We investigate directed motion in non-adiabatically rocked ratchet systems sustaining few bands below the barrier. Upon restricting the dynamics to the lowest M bands, the total system-plus-bath Hamiltonian is mapped onto a discrete tight-binding model containing all the information both on the intra- and inter-well tunneling motion. A closed form for the current in the incoherent tunneling regime is obtained. In effective single-band ratchets, no current rectification occurs. We apply our theory to describe rectification effects in vortex quantum ratchets devices. Current reversals upon variation of the ac-field amplitude or frequency are predicted. 	
0401188v1	http://arxiv.org/pdf/cond-mat/0401188v1	2004	Brownian rectifiers in the presence of temporally asymmetric unbiased   forces	Raishma Krishnan|Mangal C. Mahato|A. M. Jayannavar	  The efficiency of energy transduction in a temporally asymmetric rocked ratchet is studied. Time asymmetry favours current in one direction and suppresses it in the opposite direction due to which large efficiency ~ 50% is readily obtained. The spatial asymmetry in the potential together with system inhomogeneity may help in further enhancing the efficiency. Fine tuning of system parameters considered leads to multiple current reversals even in the adiabatic regime. 	
0512152v1	http://arxiv.org/pdf/cond-mat/0512152v1	2005	Forcing inertial Brownian motors: efficiency and negative differential   mobility	Marcin Kostur|Lukasz Machura|Peter Hänggi|Jurek Luczka|Peter Talkner	  The noise-assisted, directed transport in a one-dimensional dissipative, inertial Brownian motor of the rocking type that is exposed to an external bias is investigated. We demonstrate that the velocity-load characteristics is distinctly non-monotonic, possessing regimes with a {\em negative differential mobility}. In addition, we evaluate several possible efficiency quantifiers which are compared among each other. These quantifiers characterize the mutual interplay between the viscous drag and the external load differently, weighing the inherent rectification features from different physical perspectives. 	
0605093v1	http://arxiv.org/pdf/cond-mat/0605093v1	2006	Nonergodic Brownian Dynamics and the Fluctuation-Dissipation Theorem	Jing-Dong Bao|Yi-Zhong Zhuo|Fernando A. Oliveira|Peter Hänggi	  Nonergodic Brownian motion is elucidated within the framework of the generalized Langevin equation. For thermal noise yielding either a vanishing or a divergent zero-frequency friction strength, the non-Markovian Browninan dynamics exhibits a riveting, anomalous diffusion behavior being characterized by a ballistic or possibly also a localized dynamics. As a consequence, such tailored thermal noise may cause a net acceleration of directed transport in a rocking Brownian motor. Two notable conditions for the thermal noise are identified in order to guarantee the fluctuation-dissipation theorem of first kind. 	
0409105v2	http://arxiv.org/pdf/physics/0409105v2	2006	Spectroscopy in the Presence of Geometrical Constraints: A Torsional   Pendulum	Jason N. Hancock|Trieu T. Mai|Zack Schlesinger	  We demonstrate that an effect other than anharmonicity can severely distort the spectroscopic signatures of quantum mechanical systems. This is done through an analytic calculation of the spectroscopic response of a simple system, a charged torsional pendulum. One may look for these effects in the optical data of real systems when for example a significant rocking component of rigid polyhedra plays a significant role in the lattice dynamics. 	
0104091v3	http://arxiv.org/pdf/quant-ph/0104091v3	2001	Quantum mechanics gives stability to a Nash equilibrium	A. Iqbal|A. H. Toor	  We consider a slightly modified version of the Rock-Scissors-Paper (RSP) game from the point of view of evolutionary stability. In its classical version the game has a mixed Nash equilibrium (NE) not stable against mutants. We find a quantized version of the RSP game for which the classical mixed NE becomes stable. 	
0711.1947v2	http://arxiv.org/pdf/0711.1947v2	2008	Pre-asymptotic corrections to fractional diffusion equations	M. Marseguerra|A. Zoia	  The motion of contaminant particles through complex environments such as fractured rocks or porous sediments is often characterized by anomalous diffusion: the spread of the transported quantity is found to grow sublinearly in time due to the presence of obstacles which hinder particle migration. The asymptotic behavior of these systems is usually well described by fractional diffusion, which provides an elegant and unified framework for modeling anomalous transport. We show that pre-asymptotic corrections to fractional diffusion might become relevant, depending on the microscopic dynamics of the particles. To incorporate these effects, we derive a modified transport equation and validate its effectiveness by a Monte Carlo simulation. 	
0711.2004v1	http://arxiv.org/pdf/0711.2004v1	2007	Stress-driven phase transformation and the roughening of solid-solid   interfaces	L. Angheluta|E. Jettestuen|J. Mathiesen|F. Renard|B. Jamtveit	  The application of stress to multiphase solid-liquid systems often results in morphological instabilities. Here we propose a solid-solid phase transformation model for roughening instability in the interface between two porous materials with different porosities under normal compression stresses. This instability is triggered by a finite jump in the free energy density across the interface, and it leads to the formation of finger-like structures aligned with the principal direction of compaction. The model is proposed as an explanation for the roughening of stylolites - irregular interfaces associated with the compaction of sedimentary rocks that fluctuate about a plane perpendicular to the principal direction of compaction. 	
0801.4859v1	http://arxiv.org/pdf/0801.4859v1	2008	Vortex generation in the RSP game on the triangular lattice	Hiroyuki Nishiuchi|Naomichi Hatano|Kenn Kubo	  A new model of population dynamics on lattices is proposed. The model consists of players on lattice points, each of which plays the RSP game with neighboring players. Each player copies the next hand from the hand of the neighbouring player with the maximum point. The model exhibits a steady pattern with pairs of vortices and sinks on the triangular lattice. It is shown that the stationary vortex is due to the frustrations on the triangular lattice. A frustration is the three-sided situation where each of the three players around a triangle chooses the rock, the scissors and the paper, respectively. 	
0801.4943v1	http://arxiv.org/pdf/0801.4943v1	2008	The Impact of Transit Observations on Planetary Physics	Jonathan J. Fortney	  We highlight the importance of transit observations on understanding the physics of planetary atmospheres and interiors. Transmission spectra and emission spectra allow us to characterize this exotic atmospheres, which possess TiO, VO, H2O, CO, Na, and K, as principal absorbers. We calculate mass-radius relations for water-rock-iron and gas giant planets and examine these relations in light of current and future transit observations. A brief review is given of mechanisms that could lead to the large radii observed for some transiting planets. 	
0803.3892v1	http://arxiv.org/pdf/0803.3892v1	2008	Continuous-time random-walk approach to normal and anomalous   reaction-diffusion processes	A. Zoia	  We study the dynamics of a radioactive species flowing through a porous material, within the Continuous-Time Random Walk (CTRW) approach to the modelling of stochastic transport processes. Emphasis is given to the case where radioactive decay is coupled to anomalous diffusion in locally heterogeneous media, such as porous sediments or fractured rocks. In this framework, we derive the distribution of the number of jumps each particle can perform before a decay event. On the basis of the obtained results, we compute the moments of the cumulative particle distribution, which can be then used to quantify the overall displacement and spread of the contaminant species. 	
0804.3630v3	http://arxiv.org/pdf/0804.3630v3	2008	Ratcheting Heat Flux against a Thermal Bias	Nianbei Li|Peter Hanggi|Baowen Li	  Merely rocking the temperature in one heat bath can direct a steady heat flux from cold to hot against a non-zero thermal bias in stylized nonlinear lattice junctions that are sandwiched between two heat baths. Likewise, for an average zero-temperature difference between the two contacts a net, ratchet-like heat flux emerges. Computer simulations show that this very heat flux can be controlled and reversed by suitably tailoring the frequency ($\lesssim$ 100 MHz) of the alternating temperature field. 	
1006.0383v1	http://arxiv.org/pdf/1006.0383v1	2010	Coexistence in a One-Dimensional Cyclic Dominance Process	Anton A. Winkler|Tobias Reichenbach|Erwin Frey	  Cyclic (rock-paper-scissors-type) population models serve to mimic complex species interactions. Focusing on a paradigmatic three-species model with mutations in one dimension, we observe an interplay between equilibrium and non-equilibrium processes in the stationary state. We exploit these insights to obtain asymptotically exact descriptions of the emerging reactive steady state in the regimes of high and low mutation rates. The results are compared to stochastic lattice simulations. Our methods and findings are potentially relevant for the spatio-temporal evolution of other non-equilibrium stochastic processes. 	
1407.7616v1	http://arxiv.org/pdf/1407.7616v1	2014	Chaos in Kicked Ratchets	Daniel G. Zarlenga|Hilda A. Larrondo|Miguel Arizmendi|Fereydoon Family	  We present a minimal one-dimensional deterministic continuous dynamical system that exhibits chaotic behavior and complex transport properties. Our model is an overdamped rocking ratchet that is periodically kicked with a delta function potential. We develop an analytical approach that predicts many key features of the system, such as current reversals, as well as the presence of chaotic behavior and bifurcation. We show that our approach can be easily extended to other types of periodic forces, including the square wave. 	
1503.01047v1	http://arxiv.org/pdf/1503.01047v1	2015	Generalized ballistic deposition in 2 dimensions : scaling of surface   width, porosity and conductivity	Subhankar Ray|Baisakhi Mal|J. Shamanna	  A deposition process with particles having realistic intermediate stickiness is studied in 2+1 dimensions. At each stage of the deposition process, for any given configuration, a newly depositing particle gives rise to allowed set of configurations that are vastly larger than those for deposition of a mixture of purely non-sticky (random like) and purely sticky (ballistic like) particles. We obtain scaling behavior and demonstrate collapse of scaled data for surface width and porosity. Scaling of conductivity, when a porous structure thus formed, is saturated with conductive fluid, e.g. brine, is studied. The results obtained are in good agreement with Archie's law for porous sedimentary rocks. 	
1506.04563v1	http://arxiv.org/pdf/1506.04563v1	2015	Avalanches in wood compression	Tero Mäkinen|Amandine Miksic|Markus Ovaska|Mikko J. Alava	  Wood is a multi-scale material exhibiting a complex viscoplastic response. We study avalanches in small wood samples in compression. "Woodquakes" measured by acoustic emission are surprisingly similar to earthquakes and crackling noise in rocks and laboratory tests on brittle materials. Both the distributions of event energies and of waiting (silent) times follow power-laws. The stress- strain response exhibits clear signatures of localization of deformation to "weak spots" or softwood layers, as identified using Digital Image Correlation. Even though material structure-dependent localization takes place, the avalanche behavior remains scale-free. 	
1511.08554v1	http://arxiv.org/pdf/1511.08554v1	2015	Formation of non-cubic nanoparticles from cubic MgO in intensified   self-burning of magnesium	Sukbyung Chae|Peter V. Pikhitsa|Seungha Shin|Chang Hyuk Kim|Sekwon Jung|Mansoo Choi	  When Mg metal burns in air the resulting rock-salt MgO smoke consists of perfect [100] cubes of about 100 nm. On contrast, we found that intensification of self-burning of Mg micropowder either by injecting it into oxy-hydrogen diffusion flame or under an infrared laser beam switches the growth mechanism producing mostly single-crystalline spheres and terraced nanoparticles. MgO molecule condensation onto primary spherical nanoparticles can account for generation of terraced nanoparticles with regular steps proportional to the nanoparticle size. 	
1609.08468v1	http://arxiv.org/pdf/1609.08468v1	2016	Maximum range of a projectile thrown from constant-speed circular motion	Nikola Poljak	  The problem of determining the angle at which a point mass launched from ground level with a given speed is a standard exercise in mechanics. Similar, yet conceptually and calculationally more difficult problems have been suggested to improve student proficiency in projectile motion. The problem of determining the maximum distance of a rock thrown from a rotating arm motion is presented and analyzed in detail in this text. The calculational results confirm several conceptually derived conclusions regarding the initial throw position and provide some details on the angles and the way of throwing (underhand or overhand) which produce the maximum throw distance. 	
1704.01659v1	http://arxiv.org/pdf/1704.01659v1	2017	Trapping Probability and Superdiffusive Motion on a Disordered Ratchet   Potential	D. G. Zarlenga|G. L. Frontini|F. Family|C. M. Arizmendi	  The relationship between anomalous superdiffusive behavior and particle trapping probability is analyzed on a rocking ratchet potential with spatially correlated weak disorder. The trapping probability allows us to obtain analytical expressions for the number of wells where a given number of particles get trapped. We have also calculated the second-moment of the particle distribution function $C_2$ as a function of time, when the untrapped particle has a constant velocity. We also use the expression for $C_2$ to characterize anomalous superdiffusive motion. 	
0401074v2	http://arxiv.org/pdf/cond-mat/0401074v2	2004	Cascade control and defense in complex networks	Adilson E. Motter	  Complex networks with heterogeneous distribution of loads may undergo a global cascade of overload failures when highly loaded nodes or edges are removed due to attacks or failures. Since a small attack or failure has the potential to trigger a global cascade, a fundamental question regards the possible strategies of defense to prevent the cascade from propagating through the entire network. Here we introduce and investigate a costless strategy of defense based on a selective further removal of nodes and edges, right after the initial attack or failure. This intentional removal of network elements is shown to drastically reduce the size of the cascade. 	
0403679v1	http://arxiv.org/pdf/cond-mat/0403679v1	2004	Two-Peak and Three-Peak Optimal Complex Networks	Andre X. C. N. Valente|Abhijit Sarkar|Howard A. Stone	  A central issue in complex networks is tolerance to random failures and intentional attacks. Current literature emphasizes the dichotomy between networks with a power-law node connectivity distribution, which are robust to random failures but fragile to targeted attacks, versus networks with an exponentially decaying connectivity distribution, which are less tolerant to failures but more resilient to attacks. We prove analytically that the optimal network configuration under a classic measure of robustness is altogether different from both of the above: in all cases, failure and/or attack, there are no more than three distinct node connectivities in the optimal network. 	
0607606v1	http://arxiv.org/pdf/cond-mat/0607606v1	2006	Damage and Healing in Fatigue Fracture	F. Kun|M. H. A. S. Costa|R. N. Costa Filho|J. S. Andrade Jr|J. B. Soares|S. Zapperi|H. J. Herrmann	  We present an experimental and theoretical study of the fatigue failure of asphalt under cyclic compression. Varying the load amplitude, experiments reveal a finite fatigue limit below which the specimen does not break, while approaching the tensile strength of the material a rapid failure occurs. In the intermediate load range, the lifetime decreases with the load as a power law. We introduce two novel theoretical approaches, namely, a fiber bundle model and a fuse model, and show that both capture the major microscopic mechanisms of the fatigue failure of asphalt, providing an excellent agreement with the experimental findings. Model calculations show that the competition of damage accumulation and healing of microcracks gives rise to novel scaling laws for fatigue failure. 	
0911.1911v2	http://arxiv.org/pdf/0911.1911v2	2010	Random neighbour model for yielding	Fergal Dalton|Alberto Petri|Giorgio Pontuale	  We introduce a model for yielding, inspired by fracture models and the failure of a sheared granular medium in which the applied shear is resisted by self-organized force chains. The force chains in the granular medium (GM) are considered as a bundle of fibres of finite strength amongst which stress is randomly redistributed after any other fibre breaks under excessive load. The model provides an exponential distribution of the internal stress and a log-normal shaped distribution of failure stress, in agreement with experimental observations. The model displays critical behaviour which approaches mean field as the number of random neighbours $k$ becomes large and also displays a failure strength which remains finite in the limit of infinite size. From comparison with different models it is argued that this is an effect of uncorrelation. All these macroscopic properties appear statistically stable with respect to the choice of the chains' initial strength distribution. The investigated model is relevant for all systems in which some generic external load or pressure is borne by a number of units, independent of one another except when failure of a unit causes load transfer to some random choice of neighbouring units. 	
1009.1264v1	http://arxiv.org/pdf/1009.1264v1	2010	Can we predict the failure point of a loaded composite material?	Srutarshi Pradhan	  As a model of composite material, the fiber bundle model has been chosen -where a bundle of fibers is subjected to external load and fibers have distributed thresholds. For different loading conditions, such a system shows few precursors which indicate that the complete failure is imminent. When external load is increased quasi-statically - \textit{bursts} (number of failing fibers) of different sizes are produced. The burst statistics shows a robust crossover behavior near the failure point, around which the average burst size seems to diverge. If the load is increased by discrete steps, susceptibility and relaxation time diverge as failure point is approached. When the bundle is overloaded (external load is more than critical load) the rate of breaking shows a minimum at half way to the collapse point. The pattern and statistics of energy emission bursts show characteristic difference for below-critical and over-critical load levels. 	
1010.5198v1	http://arxiv.org/pdf/1010.5198v1	2010	Creep rupture of materials: insights from a fiber bundle model with   relaxation	E. A. Jagla	  I adapted a model recently introduced in the context of seismic phenomena, to study creep rupture of materials. It consists of linear elastic fibers that interact in an equal load sharing scheme, complemented with a local viscoelastic relaxation mechanism. The model correctly describes the three stages of the creep process, namely an initial Andrade regime of creep relaxation, an intermediate regime of rather constant creep rate, and a tertiary regime of accelerated creep towards final failure of the sample. In the tertiary regime creep rate follows the experimentally observed one over time-to-failure dependence. The time of minimum strain rate is systematically observed to be about 60-65 % of the time to failure, in accordance with experimental observations. In addition, burst size statistics of breaking events display a -3/2 power law for events close to the time of failure, and a steeper decay for the all-time distribution. Statistics of interevent times shows a tendency of the events to cluster temporarily. This behavior should be observable in acoustic emission experiments. 	
1206.2062v1	http://arxiv.org/pdf/1206.2062v1	2012	The extreme vulnerability of interdependent spatially embedded networks	Amir Bashan|Yehiel Berezin|Sergey V. Buldyrev|Shlomo Havlin	  Recent studies show that in interdependent networks a very small failure in one network may lead to catastrophic consequences. Above a critical fraction of interdependent nodes, even a single node failure can invoke cascading failures that may abruptly fragment the system, while below this "critical dependency" (CD) a failure of few nodes leads only to small damage to the system. So far, the research has been focused on interdependent random networks without space limitations. However, many real systems, such as power grids and the Internet, are not random but are spatially embedded. Here we analytically and numerically analyze the stability of systems consisting of interdependent spatially embedded networks modeled as lattice networks. Surprisingly, we find that in lattice systems, in contrast to non-embedded systems, there is no CD and \textit{any} small fraction of interdependent nodes leads to an abrupt collapse. We show that this extreme vulnerability of very weakly coupled lattices is a consequence of the critical exponent describing the percolation transition of a single lattice. Our results are important for understanding the vulnerabilities and for designing robust interdependent spatial embedded networks. 	
1307.6998v1	http://arxiv.org/pdf/1307.6998v1	2013	CDPM2: A damage-plasticity approach to modelling the failure of concrete	Peter Grassl|Dimitrios Xenos|Ulrika Nystrom|Rasmus Rempling|Kent Gylltoft	  A constitutive model based on the combination of damage mechanics and plasticity is developed to analyse the failure of concrete structures. The aim is to obtain a model, which describes the important characteristics of the failure process of concrete subjected to multiaxial loading. This is achieved by combining an effective stress based plasticity model with a damage model based on plastic and elastic strain measures. The model response in tension, uni-, bi- and triaxial compression is compared to experimental results. The model describes well the increase in strength and displacement capacity for increasing confinement levels. Furthermore, the model is applied to the structural analyses of tensile and compressive failure. 	
1310.4999v1	http://arxiv.org/pdf/1310.4999v1	2013	Creep rupture as a non-homogeneous Poissonian process	Zsuzsa Danku|Ferenc Kun	  Creep rupture of heterogeneous materials occurring under constant sub-critical external loads is responsible for the collapse of engineering constructions and for natural catastrophes. Acoustic monitoring of crackling bursts provides microscopic insight into the failure process. Based on a fiber bundle model, we show that the accelerating bursting activity when approaching failure can be described by the Omori law. For long range load redistribution the time series of bursts proved to be a non-homogeneous Poissonian process with power law distributed burst sizes and waiting times. We demonstrate that limitations of experiments such as finite detection threshold and time resolution have striking effects on the characteristic exponents, which have to be taken into account when comparing model calculations with experiments. Recording events solely within the Omori time to failure the size distribution of bursts has a crossover to a lower exponent which is promising for forecasting the imminent catastrophic failure. 	
1408.5303v1	http://arxiv.org/pdf/1408.5303v1	2014	Fracture in Disordered Heterogeneous Materials as a Stochastic Process	Yon Visell|Guillaume Millet	  Fracture processes in heterogeneous materials comprise a large number of disordered spatial degrees of freedom, representing the dynamical state of a sample over the entire domain of interest. This complexity is usually modeled directly, obscuring the underlying physics, which can often be characterized by a small number of physical parameters. In this paper, we derive a closed-form expression for a low dimensional model that reproduces the stochastic dynamical evolution of time-dependent failure in heterogeneous materials, and efficiently captures the spatial fluctuations and critical behavior near failure. Our construction is based on a novel time domain formulation of Fiber Bundle Models, which represent spatial variations in material strength via lattices of brittle, viscoelastic fiber elements. We apply the inverse transform method of random number sampling in order to construct an exact stochastic jump process for the failure sequence in a material with arbitrary strength distributions. We also complement this with a mean field approximation that captures the coupled constitutive dynamics, and validate both with numerical simulations. Our method provides a compact representation of random fiber lattices with arbitrary failure distributions, even in the presence of rapid loading and nontrivial fiber dynamics. 	
1410.8525v4	http://arxiv.org/pdf/1410.8525v4	2015	Information Theory Perspective on Network Robustness	Tiago A. Schieber|Laura Carpi|Alejandro C. Frery|Osvaldo A Rosso|Panos M. Pardalos|Martin G. Ravetti	  A crucial challenge in network theory is the study of the robustness of a network after facing a sequence of failures. In this work, we propose a dynamical definition of network's robustness based on Information Theory, that considers measurements of the structural changes caused by failures of the network's components. Failures are defined here, as a temporal process defined in a sequence. The robustness of the network is then evaluated by measuring dissimilarities between topologies after each time step of the sequence, providing a dynamical information about the topological damage. We thoroughly analyze the efficiency of the method in capturing small perturbations by considering both, the degree and distance distributions. We found the network's distance distribution more consistent in capturing network structural deviations, as better reflects the consequences of the failures. Theoretical examples and real networks are used to study the performance of this methodology. 	
1411.7827v2	http://arxiv.org/pdf/1411.7827v2	2015	Nucleation versus percolation: Scaling criterion for failure in   disordered solids	Soumyajyoti Biswas|Subhadeep Roy|Purusattam Ray	  One of the major factors governing the mode of failure in disordered solids is the effective range $R$, over which the stress field is modified following a local rupture event. In random fiber bundle model, considered as a prototype of disordered solids, we show that the failure mode is nucleation dominated in the large system size limit, as long as $R$ scales slower than $L^{\zeta}$, with $\zeta=2/3$. For a faster increase in $R$, the failure properties are dominated by the mean-field critical point, where the damages are uncorrelated in space. In that limit, the precursory avalanches of all sizes are obtained even in the large system size limit. We expect these results to be valid for systems with finite (normalizable) disorder. 	
1507.00121v1	http://arxiv.org/pdf/1507.00121v1	2015	Robustness of scale-free networks to cascading failures induced by   fluctuating loads	Shogo Mizutaka|Kousuke Yakubo	  Taking into account the fact that overload failures in real-world functional networks are usually caused by extreme values of temporally fluctuating loads that exceed the allowable range, we study the robustness of scale-free networks against cascading overload failures induced by fluctuating loads. In our model, loads are described by random walkers moving on a network and a node fails when the number of walkers on the node is beyond the node capacity. Our results obtained by using the generating function method shows that scale-free networks are more robust against cascading overload failures than Erd\H{o}s-R\'enyi random graphs with homogeneous degree distributions. This conclusion is contrary to that predicted by previous works which neglect the effect of fluctuations of loads. 	
1609.01784v1	http://arxiv.org/pdf/1609.01784v1	2016	Optimal Cloning of Quantum States with a Fixed Failure Rate	E. Bagan|V. Yerokhin|A. Shehu|E. Feldman|J. A. Bergou	  Perfect cloning of a known set of states with arbitrary prior probabilities is possible if we allow the cloner to sometimes fail completely. In the optimal case the probability of failure is at its minimum allowed by the laws of quantum mechanics. Here we show that it is possible to lower the failure rate below that of the perfect probabilistic cloner but the price to pay is that the clones are not perfect; the global fidelity is less than one. We determine the optimal fidelity of a cloner with a Fixed Failure Rate (FFR cloner) in the case of a pair of known states. Optimality is shown to be attainable by a measure-and-prepare protocol in the limit of infinitely many clones. The optimal protocol consists of discrimination with a fixed rate of inconclusive outcome followed by preparation of the appropriate clones. The convergence shows a symmetry-breaking second-order phase transition in the fidelity of the approximate infinite clones. 	
1610.09705v2	http://arxiv.org/pdf/1610.09705v2	2017	The sounds of failure: forecasting granular slip events with passive   acoustic measurements	Theodore A. Brzinski|Karen E. Daniels	  Granular materials can fail through spontaneous events like earthquakes or brittle fracture. However, measurements and analytic models which forecast failure in this class of materials, while of both fundamental and practical interest, remain elusive. Materials including numerical packings of spheres, colloidal glasses, and granular materials have been known to develop an excess of low-frequency vibrational modes as the confining pressure is reduced. Here, we report experiments on sheared granular materials in which we monitor the evolving density of excited modes via passive monitoring of acoustic emissions. We observe a broadening of the distribution of excited modes coincident with both bulk and local plasticity, and clear evolution in the shape of the distribution before and after bulk failure. These results provide a new interpretation of the changing state of the material on its approach to stick-slip failure. 	
1801.01930v1	http://arxiv.org/pdf/1801.01930v1	2018	Universal avalanche statistics and triggering close to failure in a mean   field model of rheological fracture	Jordi Baró|Jörn Davidsen	  The hypothesis of critical failure relates the presence of an ultimate stability point in the structural constitutive equation of materials to a divergence of characteristic scales in the microscopic dynamics responsible for deformation. Avalanche models involving critical failure have determined common universality classes for stick-slip processes and fracture. However, not all empirical failure processes exhibit the trademarks of criticality. The rheological properties of materials introduce dissipation, usually reproduced in conceptual models as a hardening of the coarse grained elements of the system. Here, we investigate the effects of transient hardening on (i) the activity rate and (ii) the statistical properties of avalanches. We find the explicit representation of transient hardening in the presence of generalized viscoelasticity and solve the corresponding mean field model of fracture. In the quasistatic limit, the accelerated energy release is invariant with respect to rheology and the avalanche propagation can be reinterpreted in terms of a stochastic counting process. A single universality class can be defined from such analogy, and all statistical properties depend only on the distance to criticality. We also prove that inter-event correlations emerge due to the hardening --- even in the quasistatic limit --- that can be interpreted as "aftershocks" and "foreshocks". 	
1703.01945v3	http://arxiv.org/pdf/1703.01945v3	2017	Point-Cloud-Based Aerial Fragmentation Analysis for Application in the   Minerals Industry	Thomas Bamford|Kamran Esmaeili|Angela P. Schoellig	  This work investigates the application of Unmanned Aerial Vehicle (UAV) technology for measurement of rock fragmentation without placement of scale objects in the scene to determine image scale. Commonly practiced image-based rock fragmentation analysis requires a technician to walk to a rock pile, place a scale object of known size in the area of interest, and capture individual 2D images. Our previous work has used UAV technology for the first time to acquire real-time rock fragmentation data and has shown comparable quality of results; however, it still required the (potentially dangerous) placement of scale objects, and continued to make the assumption that the rock pile surface is planar and that the scale objects lie on the surface plane. This work improves our UAV-based approach to enable rock fragmentation measurement without placement of scale objects and without the assumption of planarity. This is achieved by first generating a point cloud of the rock pile from 2D images, taking into account intrinsic and extrinsic camera parameters, and then taking 2D images for fragmentation analysis. This work represents an important step towards automating post-blast rock fragmentation analysis. In experiments, a rock pile with known size distribution was photographed by the UAV with and without using scale objects. For fragmentation analysis without scale objects, a point cloud of the rock pile was generated and used to compute image scale. Comparison of the rock size distributions show that this point-cloud-based method enables producing measurements with better or comparable accuracy (within 10% of the ground truth) to the manual method with scale objects. 	
9207260v1	http://arxiv.org/pdf/hep-ph/9207260v1	1992	Color Confinement, Abelian Gauge and Renormalization Group Flow	H. Hata|I. Niigata	  Under the assumption that the color charge can be written in a BRST exact form, the color confinement mechanism proposed by Kugo and Ojima (KO) explains the confinement of any colored particles including dynamical quarks and gluons. This mechanism, however, is known to break down in the Abelian gauge which treats the maximal Abelian subgroup of the gauge group in a special manner. In order to study whether the failure of the KO mechanism is particular only to the Abelian gauge or whether this failure occurs in a wide class of gauges including the ordinary Lorentz type gauge, we carry out a renormalization group study of the $SU(2)$ gauge theory in the gauge fixing space. Our gauge fixing space consists of four distinct regions that are not connected with each other by renormalization group flows, and we find that the Abelian gauge is {\it infrared unstable} in three regions which include the Lorentz type gauge. This suggests that the failure of the KO mechanism is a phenomenon which occurs only in the Abelian gauge. We also find that the Lorentz gauge is infrared stable. 	
1004.1849v1	http://arxiv.org/pdf/1004.1849v1	2010	Failure mechanisms of graphene under tension	Chris A. Marianetti|Hannah G. Yevick	  Recent experiments established pure graphene as the strongest material known to mankind, further invigorating the question of how graphene fails. Using density functional theory, we reveal the mechanisms of mechanical failure of pure graphene under a generic state of tension. One failure mechanism is a novel soft-mode phonon instability of the $K_1$-mode, whereby the graphene sheet undergoes a phase transition and is driven towards isolated benzene rings resulting in a reduction of strength. The other is the usual elastic instability corresponding to a maximum in the stress-strain curve. Our results indicate that finite wave vector soft modes can be the key factor in limiting the strength of monolayer materials. 	
1204.2605v1	http://arxiv.org/pdf/1204.2605v1	2012	On the dynamics of mechanical failures in magnetized neutron-star crusts	Yuri Levin|Maxim Lyutikov	  We consider the dynamics of a mechanical failure induced by a shear stress in a strongly magnetized neutron-star crust. We show that even if the elastic properties of the crust allow the creation of a shear crack, the strongly sheared magnetic field around the crack leads to a back-reaction from the Lorentz force which does not allow large relative displacement of the crack surfaces. Instead, the global evolution of the crack proceeds on a slow resistive time scale, and is unable to release any substantial mechanical energy. Our calculations demostrate that for {\it some} magnetic-field configurations, the magnetic forces cause, effectively, a plastic deformation of the crust when the resulting elastic shear stress exceeds the critical value for mechanical failure. 	
1606.06283v1	http://arxiv.org/pdf/1606.06283v1	2016	Failure mechanisms of single-crystal silicon electrodes in lithium-ion   batteries	Feifei Shi|Zhichao Song|Philip N. Ross|Gabor A. Somorjai|Robert O. Ritchie|Kyriakos Komvopoulos	  Long-term durability is a major obstacle limiting the widespread use of lithium ion batteries (LIBs) in heavy-duty applications and others demanding extended lifetime. As one of the root causes of degradation and failure of battery performance, the electrode failure mechanisms are still unknown. Here, we reveal the fundamental fracture mechanisms of single-crystal silicon electrodes over extended lithiation/delithiation cycles, using electrochemical testing, microstructure characterization, fracture mechanics, and finite element analysis. Anisotropic lithium invasion causes crack initiation perpendicular to the electrode surface, followed by growth through the electrode thickness. The low fracture energy of the lithiated/unlithiated silicon interface provides a weak microstructural path for crack deflection, accounting for the crack patterns and delamination observed after repeated cycling. Based on this physical understanding, we demonstrate how electrolyte additives can heal electrode cracks and provide strategies to enhance the fracture resistance in future LIBs from surface chemical, electrochemical, and material science perspectives. 	
0903.2748v1	http://arxiv.org/pdf/0903.2748v1	2009	Effect of the volume of the drainage system on the measurement of   undrained thermo-poro-elastic parameters	Siavash Ghabezloo|Jean Sulem	  For evaluation of the undrained thermo-poro-elastic properties of saturated porous materials in conventional triaxial cells, it is important to take into account the effect of the dead volume of the drainage system. The compressibility and the thermal expansion of the drainage system along with the dead volume of the fluid filling this system, influence the measured pore pressure and volumetric strain during an undrained thermal or mechanical loading in a triaxial cell. The correction methods previously presented by Wissa (1969), Bishop (1976) and Ghabezloo and Sulem (2009) only permit to correct the measured pore pressures during an undrained isotropic compression test or an undrained heating test. An extension of these methods is presented in this paper to correct also the measured volumetric strain and consequently the measured undrained bulk compressibility and undrained thermal expansion coefficients during these tests. Two examples of application of the proposed correction method are presented on the results of an undrained isotropic compression test and an undrained heating test performed on a fluid-saturated granular rock. A parametric study has demonstrated that the porosity and the drained compressibility of the tested material, and the ratio of the volume of the drainage system to the one of the tested sample are the key parameters which influence the most the error induced on the measurements by the drainage system. 	
1212.2763v1	http://arxiv.org/pdf/1212.2763v1	2012	Thermal Fracturing of Geothermal Wells and the Effects of Borehole   Orientation	Kjetil M. D. Hals|Inga Berre	  An enhanced geothermal system (EGS) expands the potential of geothermal energy by enabling the exploitation of regions that lack conventional hydrothermal resources. The EGS subsurface system is created by engineering enhanced flow paths between injection and production wells. Hydraulic stimulation of existing fracture networks has been successfully achieved for unconventional geothermal resources. More recently proposed concepts increase the use of drilled wellbores in hard rock to connect the injection and production wells. The present work investigates the long-term thermal effects of deviated geothermal wellbores and studies how the cooling of the borehole wall results in thermally induced tensile fractures. The results show that induced fractures are created by a combination of in situ and thermal stresses, and that the extent to which thermally induced tensile wall fractures are created largely depends on how the wellbores are oriented with respect to the pre-existing stresses of the reservoir. If the system is not optimized with respect to in situ stresses, the risk of wellbore instability becomes severe within less than a year of production. In contrast, if the orientation of the wellbores is optimized, thermally induced instabilities can be completely excluded as potential risks for the operational lifetime of the system. Furthermore, our results show that the thermal failure process strongly depends on the temperature of the injected water but is only weakly affected by the injection rate. 	
1803.02818v1	http://arxiv.org/pdf/1803.02818v1	2018	Path Planning and Navigation Inside Off-World Lava Tubes and Caves	Himangshu Kalita|Steven Morad|Jekan Thangavelautham	  Detailed surface images of the Moon and Mars reveal hundreds of cave-like openings. These cave-like openings are theorized to be remnants of lava-tubes and their interior maybe in pristine conditions. These locations may have well preserved geological records of the Moon and Mars, including evidence of past water flow and habitability. Exploration of these caves using wheeled rovers remains a daunting challenge. These caves are likely to have entrances with caved-in ceilings much like the lava-tubes of Arizona and New Mexico. Thus, the entrances are nearly impossible to traverse even for experienced human hikers. Our approach is to utilize the SphereX robot, a 3 kg, 30 cm diameter robot with computer hardware and sensors of a smartphone attached to rocket thrusters. Each SphereX robot can hop, roll or fly short distances in low gravity, airless or low-pressure environments. Several SphereX robots maybe deployed to minimize single-point failure and exploit cooperative behaviors to traverse the cave. There are some important challenges for navigation and path planning in these cave environments. Localization systems such as GPS are not available nor are they easy to install due to the signal blockage from the rocks. These caves are too dark and too large for conventional sensor such as cameras and miniature laser sensors to perform detailed mapping and navigation. In this paper, we identify new techniques to map these caves by performing localized, cooperative mapping and navigation. 	
1506.00391v1	http://arxiv.org/pdf/1506.00391v1	2015	CCNCheck: Enabling Checkpointed Distributed Applications in Content   Centric Networks	Nitinder Mohan|Pushpendra Singh	  We consider the problem of checkpointing a distributed application efficiently in Content Centric Networks so that it can withstand transient failures. We present CCNCheck, a system which enables a sender optimized way of checkpointing distributed applications in CCN's and provides an efficient mechanism for failure recovery in such applications. CCNCheck's checkpointing mechanism is a fork of DMTCP repository CCNCheck is capable of running any distributed application written in C/C++ language. 	
1612.08911v1	http://arxiv.org/pdf/1612.08911v1	2016	Fiber networks below the isostatic point: fracture without stress   concentration	Leyou Zhang|D. Zeb Rocklin|Leonard M. Sander|Xiaoming Mao	  Crack nucleation is a ubiquitous phenomena during materials failure, because stress focuses on crack tips. It is known that exceptions to this general rule arise in the limit of strong disorder or vanishing mechanical stability, where stress distributes over a divergent length scale and the material displays diffusive damage. Here we show, using simulations, that a class of diluted lattices displays a new critical phase when they are below isostaticity, where stress never concentrates, damage always occurs over a divergent length scale, and catastrophic failure is avoided. 	
1704.02928v1	http://arxiv.org/pdf/1704.02928v1	2017	Observation of oscillatory relaxation in the Sn-terminated surface of   epitaxial rock-salt SnSe $\{111\}$ topological crystalline insulator	Wencan Jin|Suresh Vishwanath|Jianpeng Liu|Lingyuan Kong|Rui Lou|Zhongwei Dai|Jerzy T. Sadowski|Xinyu Liu|Huai-Hsun Lien|Alexander Chaney|Yimo Han|Micheal Cao|Junzhang Ma|Tian Qian|Jerry I. Dadap|Shancai Wang|Malgorzata Dobrowolska|Jacek Furdyna|David A. Muller|Karsten Pohl|Hong Ding|Huili Grace Xing|Richard M. Osgood, Jr	  Topological crystalline insulators have been recently predicted and observed in rock-salt structure SnSe $\{111\}$ thin films. Previous studies have suggested that the Se-terminated surface of this thin film with hydrogen passivation, has a reduced surface energy and is thus a preferred configuration. In this paper, synchrotron-based angle-resolved photoemission spectroscopy, along with density functional theory calculations, are used to demonstrate conclusively that a rock-salt SnSe $\{111\}$ thin film epitaxially-grown on \ce{Bi2Se3} has a stable Sn-terminated surface. These observations are supported by low energy electron diffraction (LEED) intensity-voltage measurements and dynamical LEED calculations, which further show that the Sn-terminated SnSe $\{111\}$ thin film has undergone a surface structural relaxation of the interlayer spacing between the Sn and Se atomic planes. In sharp contrast to the Se-terminated counterpart, the observed Dirac surface state in the Sn-terminated SnSe $\{111\}$ thin film is shown to yield a high Fermi velocity, $0.50\times10^6$m/s, which suggests a potential mechanism of engineering the Dirac surface state of topological materials by tuning the surface configuration. 	
0808.1224v1	http://arxiv.org/pdf/0808.1224v1	2008	Comment on: Failure of the Work-Hamiltonian Connection for Free-Energy   Calculations [Phys Rev Lett 100, 020601 (2008), arXiv:0704.0761]	Jordan Horowitz|Christopher Jarzynski	  We comment on a Letter by Vilar and Rubi [arXiv:0704.0761]. 	
1604.03021v1	http://arxiv.org/pdf/1604.03021v1	2016	Tautochrone and Brachistochrone Shape Solutions for Rocking Rigid Bodies	Patrick Glaschke	  Rocking rigid bodies appear in several shapes in everyday life: As furniture like rocking chairs and rocking cradles or as toys like rocking horses or tilting dolls. The familiar rocking motion of these objects, a non-linear combination of a rigid rotation and a translation of the center of mass, gives rise to a number of interesting dynamical properties. However, their study has received little attention in the literature.   This work presents a comprehensive introduction to the dynamics of rocking rigid bodies, including a concise derivation of the equations of motion as well as a general inversion procedure to construct rocking rigid body shapes with specified dynamical properties. Moreover, two novel rigid body shapes are derived - the tautochrone shape and the brachistochrone shape - which represent an intriguing generalization of the well-know tautochrone and brachistochrone curves. In particular, tautochrone shapes offer an alternative construction of a tautochrone pendulum, in addition to Huygens' cycloid pendulum solution. 	
1803.02669v1	http://arxiv.org/pdf/1803.02669v1	2018	Study on Dynamics of an Elastic Oscillator Coupled with a Rocking Wall	Mehrdad Aghagholizadeh	  This paper studies the dynamics of an elastic single degree of freedom oscillator (representing an elastic frame) coupled with a rocking wall. Two types of rocking walls namely stepping rocking wall and pinned rocking wall are presented and analyzed. For each case, full nonlinear equations of motions are calculated. The dynamic behavior of the systems shows mixed results in suppressing the dynamic response of the elastic oscillator. Through comprehensive analysis, pinned rocking wall amplifies the displacement along wide range of the spectrum, in the other hand, stepping rocking wall is the most effective especially in relatively flexible structures and with a heavier wall. This is mainly because of the pinned wall's mass works against its stability. In this study, a simple, oscillator-rocking-wall model is defined and analyzed using OpenSees and, the results from OpenSees shows a good agreement with equation of motion solution using MATLAB. 	
0201067v3	http://arxiv.org/pdf/cond-mat/0201067v3	2002	Dynamic critical behavior of failure and plastic deformation in the   random fiber bundle model	S. Pradhan|P. Bhattacharyya|B. K. Chakrabarti	  The random fiber bundle (RFB) model, with the strength of the fibers distributed uniformly within a finite interval, is studied under the assumption of global load sharing among all unbroken fibers of the bundle. At any fixed value of the applied stress (load per fiber initially present in the bundle), the fraction of fibers that remain unbroken at successive time steps is shown to follow simple recurrence relations. The model is found to have stable fixed point for applied stress in the range 0 and 1; beyond which total failure of the bundle takes place discontinuously. The dynamic critical behavior near this failure point has been studied for this model analysing the recurrence relations. We also investigated the finite size scaling behavior. At the critical point one finds strict power law decay (with time t) of the fraction of unbroken fibers. The avalanche size distribution for this mean-field dynamics of failure has been studied. The elastic response of the RFB model has also been studied analytically for a specific probability distribution of fiber strengths, where the bundle shows plastic behavior before complete failure, following an initial linear response. 	
0709.2642v1	http://arxiv.org/pdf/0709.2642v1	2007	Techniques, advances, problems and issues in numerical modelling of   landslide hazard	Theo Van Asch|Jean-Philippe Malet|Ludovicus Van Beek|David Amitrano	  Slope movements (e.g. landslides) are dynamic systems that are complex in time and space and closely linked to both inherited and current preparatory and triggering controls. It is not yet possible to assess in all cases conditions for failure, reactivation and rapid surges and successfully simulate their transient and multi-dimensional behaviour and development, although considerable progress has been made in isolating many of the key variables and elementary mechanisms and to include them in physically-based models for landslide hazard assessments. Therefore, the objective of this paper is to review the state-of-the-art in the understanding of landslide processes and to identify some pressing challenges for the development of our modelling capabilities in the forthcoming years for hazard assessment. This paper focuses on the special nature of slope movements and the difficulties related to simulating their complex time-dependent behaviour in mathematical, physically-based models. It analyses successively the research frontiers in the recognition of first-time failures (pre-failure and failure stages), reactivation and the catastrophic transition to rapid gravitational processes (post-failure stage). Subsequently, the paper discusses avenues to transfer local knowledge on landslide activity to landslide hazard forecasts on regional scales and ends with an outline how geomorphological investigations and supporting monitoring techniques could be applied to improve the theoretical concepts and the modelling performance of physically-based landslide models at different spatial and temporal scales. 	
1312.7126v1	http://arxiv.org/pdf/1312.7126v1	2013	Link Quality and MAC-Overhead aware Predictive Preemptive Routing   Protocol for Mobile Ad hoc Network	Ali Cherif Moussa|Faraoun Mohamed Kamel	  In Ad Hoc networks, route failure may occur due to less received power, mobility, congestion and node failures. Many approaches have been proposed in literature to solve this problem, where a node predicts pre-emptively the route failure that occurs with the less received power. However, this approach encounters some difficulties, especially in scenario without mobility where route failures may arise. In this paper, we propose an improvement of AODV protocol called LO-PPAODV (Link Quality and MAC-Overhead aware Predictive Preemptive AODV). This protocol is based on new metric combine more routing metrics (Link Quality, MAC Overhead) between each node and one hop neighbor. Also we propose a cross-layer networking mechanism to distinguish between both situations, failures due to congestion or mobility, and consequently avoiding unnecessary route repair process. The LO-PPAODV was implemented using NS-2. The simulation results show that our approach improves the overall performance of the network. It reduces the average end to end delay, the routing overhead, MAC errors and route errors, and increases the packet delivery fraction of the network. 	
1407.6177v1	http://arxiv.org/pdf/1407.6177v1	2014	Power laws statistics of cliff failures, scaling and percolation	Andrea Baldassarri|Bernard Sapoval	  The size of large cliff failures may be described in several ways, for instance considering the horizontal eroded area at the cliff top and the maximum local retreat of the coastline. Field studies suggest that, for large failures, the frequencies of these two quantities decrease as power laws of the respective magnitudes, defining two different decay exponents. Moreover, the horizontal area increases as a power law of the maximum local retreat, identifying a third exponent. Such observation suggests that the geometry of cliff failures are statistically similar for different magnitudes. Power laws are familiar in the physics of critical systems. The corresponding exponents satisfy precise relations and are proven to be universal features, common to very different systems. Following the approach typical of statistical physics, we propose a "scaling hypothesis" resulting in a relation between the three above exponents: there is a precise, mathematical relation between the distributions of magnitudes of erosion events and their geometry. Beyond its theoretical value, such relation could be useful for the validation of field catalogs analysis. Pushing the statistical physics approach further, we develop a numerical model of marine erosion that reproduces the observed failure statistics. Despite the minimality of the model, the exponents resulting from extensive numerical simulations fairly agree with those measured on the field. These results suggest that the mathematical theory of percolation, which lies behind our simple model, can possibly be used as a guide to decipher the physics of rocky coast erosion and could provide precise predictions to the statistics of cliff collapses. 	
1602.03153v1	http://arxiv.org/pdf/1602.03153v1	2016	Limiting Self-Propagating Malware Based on Connection Failure Behavior   through Hyper-Compact Estimators	You Zhou|Yian Zhou|Shigang Chen|O. Patrick Kreidl	  Self-propagating malware (e.g., an Internet worm) exploits security loopholes in software to infect servers and then use them to scan the Internet for more vulnerable servers. While the mechanisms of worm infection and their propagation models are well understood, defense against worms remains an open problem. One branch of defense research investigates the behavioral difference between worm-infected hosts and normal hosts to set them apart. One particular observation is that a worm-infected host, which scans the Internet with randomly selected addresses, has a much higher connection-failure rate than a normal host. Rate-limit algorithms have been proposed to control the spread of worms by traffic shaping based on connection failure rate. However, these rate-limit algorithms can work properly only if it is possible to measure failure rates of individual hosts efficiently and accurately. This paper points out a serious problem in the prior method. To address this problem, we first propose a solution based on a highly efficient double-bitmap data structure, which places only a small memory footprint on the routers, while providing good measurement of connection failure rates whose accuracy can be tuned by system parameters. Furthermore, we propose another solution based on shared register array data structure, achieving better memory efficiency and much larger estimation range than our double-bitmap solution. 	
0412478v1	http://arxiv.org/pdf/cond-mat/0412478v1	2004	Characterization of Failure Mechanism in Composite Materials Through   Fractal Analysis of Acoustic Emission Signals	F. E. Silva|L. L. Goncalves|D. B. B. Fereira|J. M. A. Rebello	  In this paper it is presented a detailed numerical investigation of acoustic emission signals obtained from test samples of fibreglass reinforced polymeric matrix composites, when subjected to tensile and flexural tests. Various fractal indices, characteristic of the signals emitted at the different structural failures of the test samples and which satisfy non-stationary distributions, have been determined. From the results obtained for these indices, related to the Hurst analysis, detrended fluctuation analysis, minimal cover analysis and to the boxcounting dimension analysis, it has been shown they can discriminate the different failure mechanisms and, therefore, they constitute their signature. 	
9710002v1	http://arxiv.org/pdf/patt-sol/9710002v1	1997	Propagation Failure in Excitable Media	Aric Hagberg|Ehud Meron	  We study a mechanism of pulse propagation failure in excitable media where stable traveling pulse solutions appear via a subcritical pitchfork bifurcation. The bifurcation plays a key role in that mechanism. Small perturbations, externally applied or from internal instabilities, may cause pulse propagation failure (wave breakup) provided the system is close enough to the bifurcation point. We derive relations showing how the pitchfork bifurcation is unfolded by weak curvature or advective field perturbations and use them to demonstrate wave breakup. We suggest that the recent observations of wave breakup in the Belousov-Zhabotinsky reaction induced either by an electric field or a transverse instability are manifestations of this mechanism. 	
1310.8486v1	http://arxiv.org/pdf/1310.8486v1	2013	On the Combination of Silent Error Detection and Checkpointing	Guillaume Aupy|Anne Benoit|Thomas Hérault|Yves Robert|Frédéric Vivien|Dounia Zaidouni	  In this paper, we revisit traditional checkpointing and rollback recovery strategies, with a focus on silent data corruption errors. Contrarily to fail-stop failures, such latent errors cannot be detected immediately, and a mechanism to detect them must be provided. We consider two models: (i) errors are detected after some delays following a probability distribution (typically, an Exponential distribution); (ii) errors are detected through some verification mechanism. In both cases, we compute the optimal period in order to minimize the waste, i.e., the fraction of time where nodes do not perform useful computations. In practice, only a fixed number of checkpoints can be kept in memory, and the first model may lead to an irrecoverable failure. In this case, we compute the minimum period required for an acceptable risk. For the second model, there is no risk of irrecoverable failure, owing to the verification mechanism, but the corresponding overhead is included in the waste. Finally, both models are instantiated using realistic scenarios and application/architecture parameters. 	
1601.03237v1	http://arxiv.org/pdf/1601.03237v1	2016	Dynamic weakening by acoustic fluidization during stick-slip motion	Ferdinando Giacco|Luigi Saggese|Lucilla de Arcangelis|Eugenio Lippiello|Massimo Pica Ciamarra	  The unexpected weakness of some faults has been attributed to the emergence of acoustic waves that promote failure by reducing the confining pressure through a mechanism known as acoustic fluidization, also proposed to explain earthquake remote triggering. Here we validate this mechanism via the numerical investigation of a granular fault model system. We find that the stick-slip dynamics is affected only by perturbations applied at a characteristic frequency corresponding to oscillations normal to the fault, leading to gradual dynamical weakening as failure is approaching. Acoustic waves at the same frequency spontaneously emerge at the onset of failure in absence of perturbations, supporting the relevance of acoustic fluidization in earthquake triggering. 	
9607160v2	http://arxiv.org/pdf/astro-ph/9607160v2	1996	Life Extinction Due To Neutron Star Mergers	Arnon Dar|Ari Laor|Nir J. Shaviv	  Cosmic ray bursts (CRBs) from mergers or accretion induced collapse of neutron stars that hit an Earth-like planet closer than $\sim 1 kpc$ from the explosion produce lethal fluxes of atmospheric muons at ground level, underground and underwater. These CRBs also destroy the ozone layer and radioactivate the environment. The mean rate of such life devastating CRBs is one in 100 million years (Myr), consistent with the observed 5 ``great'' extinctions in the past 600 Myr. Unlike the previously suggested extraterrestrial extinction mechanisms the CRBs explain massive life extinction on the ground, underground and underwater and the higher survival levels of radiation resistant species and of terrain sheltered species. More distant mergers can cause smaller extinctions. Biological mutations caused by ionizing radiation produced by the CRB may explain a fast appearance of new species after mass extinctions. The CRB extinction predicts detectable enrichment of rock layers which formed during the extinction periods with cosmogenically produced radioactive nucleides such as $^{129}$I, $^{146}$Sm, $^{205}$Pb with and $^{244}$Pu. Tracks of high energy particles in rock layers on Earth and on the moon may also contain records of intense CRBs. An early warning of future extinctions due to neutron star mergers can be obtained by identifying, mapping and timing all the nearby binary neutron stars systems. A final warning of an approaching CRB from a nearby neutron stars merger will be provided by a gamma ray burst a few days before the arrival of the CRB. 	
0505248v3	http://arxiv.org/pdf/cond-mat/0505248v3	2006	Optimal strategy for controlling transport in inertial Brownian motors	Lukasz Machura|Marcin Kostur|Fabio Marchesoni|Peter Talkner|Peter Hänggi|Jerzy Luczka	  In order to optimize the directed motion of an inertial Brownian motor, we identify the operating conditions that both maximize the motor current and minimize its dispersion. Extensive numerical simulation of an inertial rocked ratchet displays that two quantifiers, namely the energetic efficiency and the P\'eclet number (or equivalently the Fano factor), suffice to determine the regimes of optimal transport. The effective diffusion of this rocked inertial Brownian motor can be expressed as a generalized fluctuation theorem of the Green -- Kubo type.   Addendum and Erratum   The expression for the effective diffusion of an inertial, periodically driven Brownian particle in an asymmetric, periodic potential is compared with the step number diffusion which is extracted from the corresponding coarse grained hopping process specifying the number of covered spatial periods within each temporal period. The two expressions are typically different and involve the correlations between the number of hops. 	
0801.1798v2	http://arxiv.org/pdf/0801.1798v2	2008	Self-Organization of Mobile Populations in Cyclic Competition	Tobias Reichenbach|Mauro Mobilia|Erwin Frey	  The formation of out-of-equilibrium patterns is a characteristic feature of spatially-extended, biodiverse, ecological systems. Intriguing examples are provided by cyclic competition of species, as metaphorically described by the `rock-paper-scissors' game. Both experimentally and theoretically, such non-transitive interactions have been found to induce self-organization of static individuals into noisy, irregular clusters. However, a profound understanding and characterization of such patterns is still lacking. Here, we theoretically investigate the influence of individuals' mobility on the spatial structures emerging in rock-paper-scissors games. We devise a quantitative approach to analyze the spatial patterns self-forming in the course of the stochastic time evolution. For a paradigmatic model originally introduced by May and Leonard, within an interacting particle approach, we demonstrate that the system's behavior - in the proper continuum limit - is aptly captured by a set of stochastic partial differential equations. The system's stochastic dynamics is shown to lead to the emergence of entangled rotating spiral waves. While the spirals' wavelength and spreading velocity is demonstrated to be accurately predicted by a (deterministic) complex Ginzburg-Landau equation, their entanglement results from the inherent stochastic nature of the system.These findings and our methods have important applications for understanding the formation of noisy patterns, e.g., in ecological and evolutionary contexts, and are also of relevance for the kinetics of (bio)-chemical reactions. 	
0808.4084v1	http://arxiv.org/pdf/0808.4084v1	2008	Effective stress law for the permeability of a limestone	Siavash Ghabezloo|Jean Sulem|Sylvine Guédon|François Martineau	  The effective stress law for the permeability of a limestone is studied experimentally by performing constant head permeability tests in a triaxial cell with different conditions of confining pressure and pore pressure. Test results have shown that a pore pressure increase and a confining pressure decrease both result in an increase of the permeability, and that the effect of the pore pressure change on the variation of the permeability is more important than the effect of a change of the confining pressure. A power law is proposed for the variation of the permeability with the effective stress. The permeability effective stress coefficient increases linearly with the differential pressure and is greater than one as soon the differential pressure exceeds few bars. The test results are well reproduced using the proposed permeability-effective stress law. A conceptual pore-shell model based on a detailed observation of the microstructure of the studied limestone is proposed. This model is able to explain the experimental observations on the effect of the total stress and of the pore pressure on the permeability of the limestone. Effective stress coefficients for the stress-dependent permeability which are greater than one are obtained. It is shown that the controlling factor is the ratio of the different bulk moduli of the various constituents of the rock. This ratio is studied experimentally by performing microhardness tests. 	
1003.1244v1	http://arxiv.org/pdf/1003.1244v1	2010	Subdiffusive Brownian ratchets rocked by a periodic force	Igor Goychuk	  This work puts forward a generalization of the well-known rocking Markovian Brownian ratchets to the realm of antipersistent non-Markovian subdiffusion in viscoelastic media. A periodically forced subdiffusion in a parity-broken ratchet potential is considered within the non-Markovian generalized Langevin equation (GLE) description with a power-law memory kernel $\eta(t)\propto t^{-\alpha}$ ($0<\alpha<1$). It is shown that subdiffusive rectification currents, defined through the mean displacement and subvelocity $v_{\alpha}$, $<\delta x(t)>\sim v_{\alpha} t^{\alpha}/ \Gamma(1+\alpha)$, emerge asymptotically due to the breaking of the detailed balance symmetry by driving. The asymptotic exponent is $\alpha$, the same as for free subdiffusion, $<\delta x^2(t)>\propto t^\alpha$.   However, a transient to this regime with some time-dependent $\alpha_{\rm eff}(t)$ gradually decaying in time, $\alpha\leq \alpha_{\rm eff}(t)\leq 1$, can be very slow depending on the barrier height and the driving field strength. In striking contrast to its normal diffusion counterpart, the anomalous rectification current is absent asymptotically in the limit of adiabatic driving with frequency $\Omega\to 0$, displaying a resonance like dependence on the driving frequency. However, an anomalous current inversion occurs for a sufficiently fast driving, like in the normal diffusion case. In the lowest order of the driving field, such a rectification current presents a quadratic response effect. Beyond perturbation regime it exhibits a broad maximum versus the driving field strength. Moreover, anomalous current exhibits a maximum versus the potential amplitude. 	
1108.1790v1	http://arxiv.org/pdf/1108.1790v1	2011	Effects of competition on pattern formation in the rock-paper-scissors   game	Luo-Luo Jiang|Tao Zhou|Matjaz Perc|Bing-Hong Wang	  We investigate the impact of cyclic competition on pattern formation in the rock-paper-scissors game. By separately considering random and prepared initial conditions, we observe a critical influence of the competition rate $p$ on the stability of spiral waves and on the emergence of biodiversity. In particular, while increasing values of $p$ promote biodiversity, they may act detrimental on spatial pattern formation. For random initial conditions, we observe a phase transition from biodiversity to an absorbing phase, whereby the critical value of mobility grows linearly with increasing values of $p$ on a log-log scale, but then saturates as $p$ becomes large. For prepared initial conditions, we observe the formation of single-armed spirals, but only for values of $p$ that are below a critical value. Once above, the spirals break up and form disordered spatial structures, mainly because of the percolation of vacant sites. Thus, there exists a critical value of the competition rate $p_{c}$ for stable single-armed spirals in finite populations. Importantly though, $p_{c}$ increases with increasing system size, because noise reinforces the disintegration of ordered patterns. In addition, we also find that $p_{c}$ increases with the mobility. These phenomena are reproduced by a deterministic model that is based on nonlinear partial differential equations. Our findings indicate that competition is vital for the sustenance of biodiversity and emergence of pattern formation in ecosystems governed by cyclical interactions. 	
1305.1582v1	http://arxiv.org/pdf/1305.1582v1	2013	The scattering map in two coupled piecewise-smooth systems, with   numerical application to rocking blocks	A. Granados|S. J. Hogan|T. M. Seara	  We consider a non-autonomous dynamical system formed by coupling two piecewise-smooth systems in $\RR^2$ through a non-autonomous periodic perturbation. We study the dynamics around one of the heteroclinic orbits of one of the piecewise-smooth systems. In the unperturbed case, the system possesses two $C^0$ normally hyperbolic invariant manifolds of dimension two with a couple of three dimensional heteroclinic manifolds between them. These heteroclinic manifolds are foliated by heteroclinic connections between $C^0$ tori located at the same energy levels. By means of the {\em impact map} we prove the persistence of these objects under perturbation. In addition, we provide sufficient conditions of the existence of transversal heteroclinic intersections through the existence of simple zeros of Melnikov-like functions. The heteroclinic manifolds allow us to define the {\em scattering map}, which links asymptotic dynamics in the invariant manifolds through heteroclinic connections. First order properties of this map provide sufficient conditions for the asymptotic dynamics to be located in different energy levels in the perturbed invariant manifolds. Hence we have an essential tool for the construction of a heteroclinic skeleton which, when followed, can lead to the existence of Arnol'd diffusion: trajectories that, on large time scales, destabilize the system by further accumulating energy. We validate all the theoretical results with detailed numerical computations of a mechanical system with impacts, formed by the linkage of two rocking blocks with a spring. 	
1308.0491v3	http://arxiv.org/pdf/1308.0491v3	2014	Time-shift invariance determines the functional shape of the current in   dissipative rocking ratchets	José A. Cuesta|Niurka R. Quintero|Renato Alvarez-Nodarse	  Ratchets are devices able to rectify an otherwise oscillatory behavior by exploiting an asymmetry of the system. In rocking ratchets the asymmetry is induced through a proper choice of external forces and modulations of nonlinear symmetric potentials. The ratchet currents thus obtained in systems as different as semiconductors, Josephson junctions, optical lattices, or ferrofluids, show a set of universal features. A satisfactory explanation for them has challenged theorist for decades, and so far we still lack a general theory of this phenomenon. Here we provide such a theory by exploring ---through functional analysis--- the constraints that the simple assumption of time-shift invariance of the ratchet current imposes on its dependence on the external drivings. Because the derivation is based on so general a principle, the resulting expression is valid irrespective of the details and the nature of the physical systems to which it is applied, and of whether they are classical, quantum, or stochastic. The theory also explains deviations observed from universality under special conditions, and allows to make predictions of phenomena not yet observed in any experiment or simulation. 	
1309.6749v1	http://arxiv.org/pdf/1309.6749v1	2013	Subdiffusive rocking ratchets in viscoelastic media: transport   optimization and thermodynamic efficiency in overdamped regime	Vasyl O. Kharchenko|I. Goychuk	  We study subdiffusive overdamped Brownian ratchets periodically rocked by an external zero-mean force in viscoelastic media within the framework of non-Markovian Generalized Langevin equation (GLE) approach and associated multi-dimensional Markovian embedding dynamics. Viscoelastic deformations of the medium caused by the transport particle are modeled by a set of auxiliary Brownian quasi-particles elastically coupled to the transport particle and characterized by a hierarchy of relaxation times which obey a fractal scaling. The most slowly relaxing deformations which cannot immediately follow to the moving particle imprint long-range memory about its previous positions and cause subdiffusion and anomalous transport on a sufficiently long time scale. This anomalous behavior is combined with normal diffusion and transport on an initial time scale of overdamped motion. Anomalously slow directed transport in a periodic ratchet potential with broken space inversion symmetry emerges due to a violation of the thermal detailed balance by a zero-mean periodic driving and is optimized with frequency of driving, its amplitude, and temperature. Such optimized anomalous transport can be low dispersive and characterized by a large generalized Peclet number. Moreover, we show that overdamped subdiffusive ratchets can sustain a substantial load and do a useful work. The corresponding thermodynamic efficiency decays algebraically in time since the useful work done against a load scales sublinearly with time following to the transport particle position, but the energy pumped by an external force scales with time linearly. Nevertheless, it can be transiently appreciably high and compare well with the thermodynamical efficiency of the normal diffusion overdamped ratchets on sufficiently long temporal and spatial scales. 	
1510.08619v1	http://arxiv.org/pdf/1510.08619v1	2015	High-resolution dielectric characterization of minerals: a step towards   understanding the basic interactions between microwaves and rocks	Tamara Monti|Alexander Tselev|Ofonime Udoudo|Ilia N. Ivanov|Samuel W. Kingman	  Microwave energy has been demonstrated to be beneficial for reducing the energetic cost of several steps of the mining process. Significant literature has been developed about this topic but few studies are focused on understanding the interaction between microwaves and minerals at a fundamental level in order to elucidate the underlying physical processes that control the observed phenomena. This is ascribed to the complexity of such phenomena, related to chemical and physical transformations, where electrical, thermal and mechanical forces play concurrent roles. In this work a new characterization method for the dielectric properties of mineral samples at microwave frequencies is presented. The method is based upon the scanning microwave microscopy technique that enables measurement of the dielectric constant, loss factor and conductivity with extremely high spatial resolution and accuracy. As opposed to conventional bulk dielectric techniques, the scanning microwave microscope can then access and measure the dielectric properties of micrometer-sized mineral inclusions within a complex structure of natural rock. In this work a 5 by 20 micrometers size hematite inclusion has been characterized at a microwave frequency of 3 GHz. Scanning electron microscopy/energy-dispersive x-ray spectroscopy and confocal micro Raman spectroscopy were used to determine the structural details and chemical and elemental composition of mineral sample on similar scale. 	
1608.03904v2	http://arxiv.org/pdf/1608.03904v2	2016	Elastic Microplane Formulation for Transversely Isotropic Materials	Congrui Jin|Marco Salviato|Weixin Li|Gianluca Cusatis	  This contribution investigates the extension of the microplane formulation to the description of transversely isotropic materials such as shale rock, foams, unidirectional composites, and ceramics. Two possible approaches are considered: 1) the spectral decomposition of the stiffness tensor to define the microplane constitutive laws in terms of energetically orthogonal eigenstrains and eigenstresses; and 2) the definition of orientation-dependent microplane elastic moduli. It is shown that the first approach provides a rigorous way to tackle anisotropy within the microplane framework whereas the second approach represents an approximation which, however, makes the formulation of nonlinear constitutive equations much simpler. The efficacy of the second approach in modeling the macroscopic elastic behavior is compared to the thermodynamic restrictions of the anisotropic parameters showing that a significant range of elastic properties can be modeled with excellent accuracy. Further, it is shown that it provides a very good approximation of the microplane stresses provided by the first approach, with the advantage of a simpler formulation.   It is concluded that the spectral stiffness decomposition represents the best approach in such cases as for modeling unidirectional composites, in which accurately capturing the elastic behavior is important. The introduction of orientation-dependent microplane elastic moduli provides a simpler framework for the modeling of transversely isotropic materials with remarked inelastic behavior, as in the case, for example, of shale rock. 	
1610.07600v1	http://arxiv.org/pdf/1610.07600v1	2016	Towards Rock-paper-scissors patterns in the Optional Public Goods Game   under random mobility	Pablo A. Valverde|Roberto da Silva|Eduardo V. Stock	  Social dilemmas concern a natural conflict between cooperation and self interests among individuals in large populations. The emergence of cooperation and its maintenance is the key for the understanding of fundamental concepts about the evolution of species. In order to understand the mechanisms involved in this framework, here we study the Optional Public Good Games with focus on the effects of diffusive aspects in the emergent patterns of cyclic dominance between the strategies. Differently from other works, we showed that rock-paper-scissors (RPS) patterns occur by introducing a simple kind of random mobility in a lattice sparsely occupied. Such pattern has been revealed to be very important in the conservation of the species in ecological and social environments. The goal of this paper is to show that we do not need more elaborated schemes for construction of the neighbourhood in the game to observe RPS patterns as suggested in the literature. As an interesting additional result, in this contribution we also propose an alternative method to quantify the RPS density in a quantitative context of the game theory which becomes possible to perform a finite size scaling study. Such approach can be very interesting to be applied in other games generically. 	
1702.01265v3	http://arxiv.org/pdf/1702.01265v3	2017	Astral microtubule dynamics regulate anaphase oscillation onset and set   a robust final position for the Caenorhabditis elegans zygote spindle	Hélène Bouvrais|Laurent Chesneau|Sylvain Pastezeur|Marie Delattre|Jacques Pécréaux	  Background: During asymmetric division of the Caenorhabditis elegans nematode zygote, the polarity cues distribution and daughter cell fates depend on the correct positioning of the mitotic spindle which results from both centering and cortical pulling forces. Revealed by spindle rocking, these pulling forces are regulated by the force generator dynamics, which are related to mitosis progression. This may be combined with a second regulation, this one by the posterior spindle pole position, which can be seen when comparing related species. Results: After delaying anaphase onset, we identified a positional pulling force regulation in C. elegans, which we ascribed to microtubule dynamics at the cortex. Indeed, in mapping the contacts we found a correlation between the centrosome-cortex distance and the microtubule contact density. This density in turn modulates pulling force generator activity. We expanded our model of spindle rocking and predicted then experimentally validated that the oscillation onset position resists changes in cellular geometry and number of force generators. Consistent with final spindle position measurements, this new model accounts for a lower dependence on force generator dynamics and quantities than predicted by the previous model. Conclusion: The spindle position regulates the rapid increase in forces needed for anaphase oscillation and positioning through the spatial modulation of microtubule-cortex contacts. This regulation superimposes that of force generator processivity, putatively linked to the cell cycle. This novel control confers resistance to variations in zygote geometry and dynamics of cortical force generators. Interestingly, this robustness originates in cell mechanics rather than biochemical networks. 	
1711.01780v1	http://arxiv.org/pdf/1711.01780v1	2017	Earthquake precursors in the light of peroxy defects theory: critical   review of systematic observations	Friedemann Freund|Guy Ouillon|John Scoville|Didier Sornette	  The starting point of the present review is to acknowledge that there are innumerable reports of non-seismic types of earthquake precursory phenomena that are intermittent and seem not to occur systematically, while associated reports are not widely accepted by the geoscience community at large because no one could explain their origins. We review a unifying theory for a solid-state mechanism, based on decades of research bridging semi-conductor physics, chemistry and rock physics. A synthesis has emerged that all pre-earthquake phenomena could trace back to one fundamental physical process: the activation of electronic charges (electrons and positive holes) in rocks subjected to ever-increasing tectonic stresses prior to any major seismic activity, via the rupture of peroxy bonds. In the second part of the review, we critically examine satellite and ground station data, recorded before past large earthquakes, as they have been claimed to provide evidence that precursory signals tend to become measurable days, sometimes weeks before the disasters. We review some of the various phenomena that can be directly predicted by the peroxy defect theory , namely, radon gas emanations, corona discharges, thermal infrared emissions, air ionization, ion and electron content in the ionosphere, and electro-magnetic anomalies. Our analysis demonstrates the need for further systematic investigations, in particular with strong continuous statistical testing of the relevance and confidence of the precursors. Only then, the scientific community will be able to assess and improve the performance of earthquake forecasts. 	
1802.04415v1	http://arxiv.org/pdf/1802.04415v1	2018	Excitation Mechanisms for Jovian Seismic Modes	Steve Markham|Dave Stevenson	  Recent (2011) results from the Nice Observatory indicate the existence of global seismic modes on Jupiter in the frequency range between 0.7 and 1.5mHz with amplitudes of tens of cm/s. Currently, the driving force behind these modes is a mystery; the measured amplitudes are many orders of magnitude larger than anticipated based on theory analogous to heliosiesmology (that is, turbulent convection as a source of stochastic excitation). One of the most promising hypotheses is that these modes are driven by Jovian storms. This work constructs a framework to analytically model the expected equilibrium normal mode amplitudes arising from convective columns in storms. We also place rough constraints on Jupiter's seismic modal quality factor. Using this model, neither meteor strikes, turbulent convection, nor water storms can feasibly excite the order of magnitude of observed amplitudes. Next we speculate about the potential role of rock storms deeper in Jupiter's atmosphere, because the rock storms' expected energy scales make them promising candidates to be the chief source of excitation for Jovian seismic modes, based on simple scaling arguments. We also suggest some general trends in the expected partition of energy between different frequency modes. Finally we supply some commentary on potential applications to gravity, Juno, Cassini and Saturn, and future missions to Uranus and Neptune. 	
0502303v1	http://arxiv.org/pdf/cond-mat/0502303v1	2005	Intrinsic vulnerabilities to mechanical failure in nanoscale films	Pooja Shah|Thomas M. Truskett	  We use molecular simulations to explore how sample dimensions and interfacial properties impact some generic aspects of the mechanical and structural behavior of nanoconfined materials. Specifically, we calculate the strain-dependent properties of minimum-energy thin-film particle configurations (i.e., inherent structures) confined between attractive, parallel substrates. We examine how the relationship between the transverse strain and the stress tensor (the equation of state of the energy landscape) depends on the properties of the film and substrate. We find that both film thickness and film-substrate attractions influence not only the mechanical properties of thin films, but also the shape and location of the "weak spots" where voids preferentially form in a film as it is strained beyond its point of maximum tensile stress. The sensitivity of weak spots to film properties suggests that nanoscale materials may be intrinsically vulnerabile to specific mechanisms of mechanical failure. 	
1710.08334v1	http://arxiv.org/pdf/1710.08334v1	2017	Micromechanical Analysis of Strength of Polymer Networks with   Polydisperse Structures	Mohammad Tehrani	  The effect of network chain distribution on the mechanical behavior of elastomers is one of the long-standing problems in rubber mechanics. The classical theory of rubber elasticity is built upon the assumption of entropic elasticity of networks whose constitutive strands are of uniform length. The kinetic theories for vulcanization, computer simulations, and indirect experimental measurements all indicate that the microstructure of vulcanizates is made of polymer strands with a random distribution of length. The polydispersity in strand length is expected to control the mechanical strength of rubber as the overloaded short strands break at small deformations and transfer the load to the longer strands. The purpose of the contributions presented in this thesis is to present simple theories of rubber mechanics that take into account the length distribution of strands and its effect on elasticity and the onset of bulk failure in unfilled and filled elastomers. In unfilled system, the population of short chains is identified as the culprits for damage initiation. Upon deformation of a polydisperse network, shorter strands break at considerably smaller stretches compared to the longer ones. The network alteration continues concurrent with increasing deformation and controls the onset of mechanical failure. In the filled networks, the degradation in network mechanical behavior is assumed to be controlled by the adhesive failure of the short strands adsorbed onto the filler surface. The finite extensibility of the short adsorbed strands is a key determinant of mechanical strength. 	
1011.5072v1	http://arxiv.org/pdf/1011.5072v1	2010	A self-managing fault management mechanism for wireless sensor networks	Muhammad Asim|Hala Mokhtar|Madjid Merabti	  A sensor network can be described as a collection of sensor nodes which co-ordinate with each other to perform some specific function. These sensor nodes are mainly in large numbers and are densely deployed either inside the phenomenon or very close to it. They can be used for various application areas (e.g. health, military, home). Failures are inevitable in wireless sensor networks due to inhospitable environment and unattended deployment. Therefore, it is necessary that network failures are detected in advance and appropriate measures are taken to sustain network operation. We previously proposed a cellular approach for fault detection and recovery. In this paper we extend the cellular approach and propose a new fault management mechanism to deal with fault detection and recovery. We propose a hierarchical structure to properly distribute fault management tasks among sensor nodes by introducing more 'self-managing' functions. The proposed failure detection and recovery algorithm has been compared with some existing related work and proven to be more energy efficient. 	
1310.2466v1	http://arxiv.org/pdf/1310.2466v1	2013	Geometric analysis on the unidirectionality of the pulmonary veins for   atrial reentry	Sehun Chun	  It is widely believed that the pulmonary veins (PVs) of the atrium play the central role in the generation of atrial reentry leading to atrial fibrillation, but its mechanism has not been analytically explained. In order to improve the current clinical procedures for atrial reentry by understanding its mechanism, geometrical analysis is proposed on the conditions of conduction failure at the PVs and is validated by various computational modeling. To achieve this, a new analytic approach is proposed by adapting the geometric relative acceleration analysis from spacetime physics on the hypothesis that a large relative acceleration can translate to a dramatic increase in the curvature of the wavefront and subsequently to conduction failure. This analytic method is applied to a simplified model of the PV to reveal the strong dependency of the propagational direction and the magnitude of anisotropy for conduction failure. The unidirectionality of the PVs follows directly and is validated by computational tests in a T-shaped domain, computational simulations for three-dimensional atrial reentry and previous in-silico reports for atrial reentry. 	
1411.7711v2	http://arxiv.org/pdf/1411.7711v2	2015	Detour Planning for Fast and Reliable Failure Recovery in SDN with   OpenState	Antonio Capone|Carmelo Cascone|Alessandro Q. T. Nguyen|Brunilde Sansò	  A reliable and scalable mechanism to provide protection against a link or node failure has additional requirements in the context of SDN and OpenFlow. Not only it has to minimize the load on the controller, but it must be able to react even when the controller is unreachable. In this paper we present a protection scheme based on precomputed backup paths and inspired by MPLS crankback routing, that guarantees instantaneous recovery times and aims at zero packet-loss after failure detection, regardless of controller reachability, even when OpenFlow's "fast-failover" feature cannot be used. The proposed mechanism is based on OpenState, an OpenFlow extension that allows a programmer to specify how forwarding rules should autonomously adapt in a stateful fashion, reducing the need to rely on remote controllers. We present the scheme as well as two different formulations for the computation of backup paths. 	
1502.05179v1	http://arxiv.org/pdf/1502.05179v1	2015	Dependability Tests Selection Based on the Concept of Layered Networks	Andrey A. Shchurov|Radek Marik	  Nowadays, the consequences of failure and downtime of distributed systems have become more and more severe. As an obvious solution, these systems incorporate protection mechanisms to tolerate faults that could cause systems failures and system dependability must be validated to ensure that protection mechanisms have been implemented correctly and the system will provide the desired level of reliable service. This paper presents a systematic approach for identifying (1) characteristic sets of critical system elements for dependability testing (single points of failure and recovery groups) based on the concept of layered networks; and (2) the most important combinations of components from each recovery group based on a combinatorial technique. Based on these combinations, we determine a set of test templates to be performed to demonstrate system dependability. 	
1502.07433v4	http://arxiv.org/pdf/1502.07433v4	2015	Maximizing the strength of fiber bundles under uniform loading	Soumyajyoti Biswas|Parongama Sen	  The collective strength of a system of fibers, each having a failure threshold drawn randomly from a distribution, indicates the maximum load carrying capacity of different disordered systems ranging from disordered solids, power-grid networks, to traffic in a parallel system of roads. In many of the cases where the redistribution of load following a local failure can be controlled, it is a natural requirement to find the most efficient redistribution scheme, i.e., following which system can carry the maximum load. We address the question here and find that the answer depends on the mode of loading. We analytically find the maximum strength and corresponding redistribution schemes for sudden and quasi static loading. The associated phase transition from partial to total failure by increasing the load has been studied. The universality class is found to be dependent on the redistribution mechanism. 	
1510.01133v3	http://arxiv.org/pdf/1510.01133v3	2016	A New Method for Compensation and Rematch of Cavity Failure in C-ADS   Linac	Zhou Xue|JianPing Dai|Cai Meng	  For proton linear accelerators used in applications such as accelerator-driven systems, due to the nature of the operation, it is essential for the beam failure rate to be several orders of magnitude lower than usual performance of similar accelerators. A fault-tolerant mechanism should be mandatorily imposed in order to maintain short recovery time, high uptime and extremely low frequency of beam loss. This paper proposes an innovative and challenging way for compensation and rematch of cavity failure using fast electronic devices and Field Programmable Gate Arrays (FPGAs) instead of embedded computers to complete the computation of beam dynamics. A method of building an equivalent model for the FPGA, with optimization using a genetic algorithm, is shown. Results based on the model and algorithm are compared with TRACEWIN simulation to show the precision and correctness of the mechanism. 	
1511.00235v1	http://arxiv.org/pdf/1511.00235v1	2015	Broadband Macroscopic Cortical Oscillations Emerge from Intrinsic   Neuronal Response Failures	Amir Goldental|Roni Vardi|Shira Sardi|Pinhas Sabo|Ido Kanter	  Broadband spontaneous macroscopic neural oscillations are rhythmic cortical firing which were extensively examined during the last century, however, their possible origination is still controversial. In this work we show how macroscopic oscillations emerge in solely excitatory random networks and without topological constraints. We experimentally and theoretically show that these oscillations stem from the counterintuitive underlying mechanism - the intrinsic stochastic neuronal response failures. These neuronal response failures, which are characterized by short-term memory, lead to cooperation among neurons, resulting in sub- or several- Hertz macroscopic oscillations which coexist with high frequency gamma oscillations. A quantitative interplay between the statistical network properties and the emerging oscillations is supported by simulations of large networks based on single-neuron in-vitro experiments and a Langevin equation describing the network dynamics. Results call for the examination of these oscillations in the presence of inhibition and external drives. 	
1702.04138v1	http://arxiv.org/pdf/1702.04138v1	2017	Agent Failures in All-Pay Auctions	Yoad Lewenberg|Omer Lev|Yoram Bachrach|Jeffrey S. Rosenschein	  All-pay auctions, a common mechanism for various human and agent interactions, suffers, like many other mechanisms, from the possibility of players' failure to participate in the auction. We model such failures, and fully characterize equilibrium for this class of games, we present a symmetric equilibrium and show that under some conditions the equilibrium is unique. We reveal various properties of the equilibrium, such as the lack of influence of the most-likely-to-participate player on the behavior of the other players. We perform this analysis with two scenarios: the sum-profit model, where the auctioneer obtains the sum of all submitted bids, and the max-profit model of crowdsourcing contests, where the auctioneer can only use the best submissions and thus obtains only the winning bid.   Furthermore, we examine various methods of influencing the probability of participation such as the effects of misreporting one's own probability of participating, and how influencing another player's participation chances changes the player's strategy. 	
1704.00943v3	http://arxiv.org/pdf/1704.00943v3	2017	Mechanics of disordered auxetic metamaterials	Maryam Hanifpour|Charlotte F. Petersen|Mikko J. Alava|Stefano Zapperi	  Auxetic materials are of great engineering interest not only because of their fascinating negative Poisson's ratio, but also due to their increased toughness and indentation resistance. These materials are typically synthesized polyester foams with a very heterogeneous structure, but the role of disorder in auxetic behavior is not fully understood. Here, we provide a systematic theoretical and experimental investigation in to the effect of disorder on the mechanical properties of a paradigmatic auxetic lattice with a re-entrant hexagonal geometry. We show that disorder has a marginal effect on the Poisson's ratio unless the lattice topology is altered, and in all cases examined the disorder preserves the auxetic characteristics. Depending on the direction of loading applied to these disordered auxetic lattices, either brittle or ductile failure is observed. It is found that brittle failure is associated with a disorder-dependent tensile strength, whereas in ductile failure disorder does not affect strength. Our work thus provides general guidelines to optimize elasticity and strength of disordered auxetic metamaterials. 	
9811297v1	http://arxiv.org/pdf/cond-mat/9811297v1	1998	Scaling in the time-dependent failure of a fiber bundle with local load   sharing	Shu-dong Zhang	  We study the scaling behaviors of a time-dependent fiber-bundle model with local load sharing. Upon approaching the complete failure of the bundle, the breaking rate of fibers diverges according to $r(t)\propto (T_f-t)^{-\xi}$, where $T_f$ is the lifetime of the bundle, and $\xi \approx 1.0$ is a quite universal scaling exponent. The average lifetime of the bundle $<T_f>$ scales with the system size as $N^{-\delta}$, where $\delta$ depends on the distribution of individual fiber as well as the breakdown rule. 	
9909273v1	http://arxiv.org/pdf/cond-mat/9909273v1	1999	Universal Scaling of Wave Propagation Failure in Arrays of Coupled   Nonlinear Cells	Konstantin Kladko|Igor Mitkov|A. R. Bishop	  We study the onset of the propagation failure of wave fronts in systems of coupled cells. We introduce a new method to analyze the scaling of the critical external field at which fronts cease to propagate, as a function of intercellular coupling. We find the universal scaling of the field throughout the range of couplings, and show that the field becomes exponentially small for large couplings. Our method is generic and applicable to a wide class of cellular dynamics in chemical, biological, and engineering systems. We confirm our results by direct numerical simulations. 	
0012256v1	http://arxiv.org/pdf/cond-mat/0012256v1	2000	What do emulsification failure and Bose-Einstein condensation have in   common?	R. P. Sear|J. A. Cuesta	  Ideal bosons and classical ring polymers formed via self-assembly, are known to have the same partition function, and so analogous phase transitions. In ring polymers, the analogue of Bose-Einstein condensation occurs when a ring polymer of macroscopic size appears. We show that a transition of the same general form occurs within a whole class of systems with self-assembly, and illustrate it with the emulsification failure of a microemulsion phase of water, oil and surfactant. As with Bose-Einstein condensation, the transition occurs even in the absence of interactions. 	
0206201v1	http://arxiv.org/pdf/cond-mat/0206201v1	2002	Failure time in the fiber-bundle model with thermal noise and disorder	Antonio Politi|Segio Ciliberto|Riccardo Scorretti	  The average time for the onset of macroscopic fractures is analytically and numerically investigated in the fiber-bundle model with quenched disorder and thermal noise under a constant load. We find an implicit exact expression for the failure time in the low-temperature limit that is accurately confirmed by direct simulations. The effect of the disorder is to lower the energy barrier. 	
0209474v1	http://arxiv.org/pdf/cond-mat/0209474v1	2002	Critical load and congestion instabilities in scale-free networks	Y. Moreno|R. Pastor-Satorras|A. Vazquez|A. Vespignani	  We study the tolerance to congestion failures in communication networks with scale-free topology. The traffic load carried by each damaged element in the network must be partly or totally redistributed among the remaining elements. Overloaded elements might fail on their turn, triggering the occurrence of failure cascades able to isolate large parts of the network. We find a critical traffic load above which the probability of massive traffic congestions destroying the network communication capabilities is finite. 	
0511457v1	http://arxiv.org/pdf/cond-mat/0511457v1	2005	Electrical transport in deformed nanostrips: electrical signature of   reversible mechanical failure	Soumendu Datta|Debasish Chaudhuri|Tanusri Saha-Dasgupta|Surajit Sengupta	  We calculate the electrical conductivity of a thin crystalline strip of atoms confined within a quasi one dimensional channel of fixed width. The conductivity shows anomalous behavior as the strip is deformed under tensile loading. Beyond a critical strain, the solid fails by the nucleation of alternating bands of solid and {\em smectic} like phases accompanied by a jump in the conductivity. Since the failure of the strip in this system is known to be reversible, the onductivity anomaly may have practical use as a sensitive strain transducer. 	
0603839v1	http://arxiv.org/pdf/cond-mat/0603839v1	2006	A Fiber Bundle Model of Traffic Jams	Bikas K. Chakrabarti	  We apply the equal load-sharing fiber bundle model of fracture failure in composite materials to model the traffic failure in a system of parallel road network in a city. For some special distributions of traffic handling capacities (thresholds) of the roads, the critical behavior of the jamming transition can be studied analytically. This has been compared with that for the asymmetric simple exclusion process in a single channel or road. 	
0507101v1	http://arxiv.org/pdf/physics/0507101v1	2005	Production networks and failure avalanches	Gerard Weisbuch|Stefano Battiston	  Although standard economics textbooks are seldom interested in production networks, modern economies are more and more based upon suppliers/customers interactions. One can consider entire sectors of the economy as generalised supply chains. We will take this view in the present paper and study under which conditions local failures to produce or simply to deliver can result in avalanches of shortage and bankruptcies across the network. We will show that a large class of models exhibit scale free distributions of production and wealth among firms and that metastable regions of high production are highly localised. 	
0803.0190v2	http://arxiv.org/pdf/0803.0190v2	2008	Crackling dynamics in material failure as the signature of a   self-organized dynamic phase transition	Daniel Bonamy|Stéphane Santucci|Laurent Ponson	  We derive here a linear elastic stochastic description for slow crack growth in heterogeneous materials. This approach succeeds in reproducing quantitatively the intermittent crackling dynamics observed recently during the slow propagation of a crack along a weak heterogeneous plane of a transparent Plexiglas block [M{\aa}l{\o}y {\it et al.}, PRL {\bf 96} 045501]. In this description, the quasi-static failure of heterogeneous media appears as a self-organized critical phase transition. As such, it exhibits universal and to some extent predictable scaling laws, analogue to that of other systems like for example magnetization noise in ferromagnets. 	
0811.3407v3	http://arxiv.org/pdf/0811.3407v3	2008	Lambda-prophage induction modeled as a cooperative failure mode of lytic   repression	Nicholas Chia|Ido Golding|Nigel Goldenfeld	  We analyze a system-level model for lytic repression of lambda-phage in E. coli using reliability theory, showing that the repressor circuit comprises 4 redundant components whose failure mode is prophage induction. Our model reflects the specific biochemical mechanisms involved in regulation, including long-range cooperative binding, and its detailed predictions for prophage induction in E. coli under ultra-violet radiation are in good agreement with experimental data. 	
1004.5095v1	http://arxiv.org/pdf/1004.5095v1	2010	Failure mechanisms in thin electroactive polymer actuators	D. De Tommasi|G. Puglisi|G. Saccomandi|G. Zurlo	  We propose a model to analyze the insurgence of pull-in and wrinkling failures in electroactive thin films. We take in consideration both cases of voltage and charge control, and study the role of prestretch and size of activated regions, which are essential in the analysis of realistic applications of EAPs. Based on simple geometrical and material assumptions we deduce an explicit analytical description of these phenomena, allowing a clear physical interpretation. Despite our simplifying assumptions, the comparison with experiments shows a satisfying qualitative and, interestingly, quantitative agreement. In particular our model shows, in accordance with experiments, the existence of different optimal prestretch values, depending on the choice of the actuating parameter of the EAP. 	
1312.4136v1	http://arxiv.org/pdf/1312.4136v1	2013	Percolating Plastic Failure as a Mechanism for Shear Softening in   Amorphous Solids	Vijayakumar Chikkadi|Oleg Gendelman|Valery Ilyin|J Ashwin|Itamar Procaccia	  ``Shear softening" refers to the observed reduction in shear modulus when the stress on an amorphous solid is increased beyond the initial linear region. Careful numerical quasi-static simulations reveal an intimate relation between plastic failure and shear softening. The attaintment of the steady-state value of the shear modulus associated with plastic flow is identified with a percolation of the regions that underwent a plastic event. We present an elementary ``two-state" model that interpolates between failed and virgin regions and provides a simple and effective characterization of the shear softening. 	
1408.5731v2	http://arxiv.org/pdf/1408.5731v2	2014	Effect of small-world topology on wave propagation on networks of   excitable elements	Thomas Isele|Eckehard Schöll	  We study excitation waves on a Newman-Watts small-world network model of coupled excitable elements. Depending on the global coupling strength, we find differing resilience to the added long-range links and different mechanisms of propagation failure. For high coupling strengths, we show agreement between the network and a reaction-diffusion model with additional mean-field term. Employing this approximation, we are able to estimate the critical density of long-range links for propagation failure. 	
1504.04618v1	http://arxiv.org/pdf/1504.04618v1	2015	Critical analysis and remedy of switching failures in straintronic logic   using Bennett clocking in the presence of thermal fluctuations	Kuntal Roy	  Straintronic logic is a promising platform for beyond Moore's law computing. Using Bennett clocking mechanism, information can propagate through an array of strain-mediated multiferroic nanomagnets exploiting the dipolar coupling between the magnets without having to physically interconnect them. Here we perform a critical analysis of switching failures, i.e., error in information propagation due to thermal fluctuations through a chain of such straintronic devices. We solved stochastic Landau-Lifshitz-Gilbert equation considering room-temperature thermal perturbations and show that magnetization switching may fail due to inherent magnetization dynamics accompanied by thermally broadened switching delay distribution. Avenues available to circumvent such issue are proposed. 	
1505.03116v1	http://arxiv.org/pdf/1505.03116v1	2015	Distributed Cohesive Control for Robot Swarms: Maintaining Good   Connectivity in the Presence of Exterior Forces	Dominik Krupke|Maximilian Ernestus|Michael Hemmer|Sandor P. Fekete	  We present a number of powerful local mechanisms for maintaining a dynamic swarm of robots with limited capabilities and information, in the presence of external forces and permanent node failures. We propose a set of local continuous algorithms that together produce a generalization of a Euclidean Steiner tree. At any stage, the resulting overall shape achieves a good compromise between local thickness, global connectivity, and flexibility to further continuous motion of the terminals. The resulting swarm behavior scales well, is robust against node failures, and performs close to the best known approximation bound for a corresponding centralized static optimization problem. 	
1511.05490v2	http://arxiv.org/pdf/1511.05490v2	2015	SPIDER: Fault Resilient SDN Pipeline with Recovery Delay Guarantees	Carmelo Cascone|Luca Pollini|Davide Sanvito|Antonio Capone|Brunilde Sansò	  When dealing with node or link failures in Software Defined Networking (SDN), the network capability to establish an alternative path depends on controller reachability and on the round trip times (RTTs) between controller and involved switches. Moreover, current SDN data plane abstractions for failure detection (e.g. OpenFlow "Fast-failover") do not allow programmers to tweak switches' detection mechanism, thus leaving SDN operators still relying on proprietary management interfaces (when available) to achieve guaranteed detection and recovery delays. We propose SPIDER, an OpenFlow-like pipeline design that provides i) a detection mechanism based on switches' periodic link probing and ii) fast reroute of traffic flows even in case of distant failures, regardless of controller availability. SPIDER can be implemented using stateful data plane abstractions such as OpenState or Open vSwitch, and it offers guaranteed short (i.e. ms) failure detection and recovery delays, with a configurable trade off between overhead and failover responsiveness. We present here the SPIDER pipeline design, behavioral model, and analysis on flow tables' memory impact. We also implemented and experimentally validated SPIDER using OpenState (an OpenFlow 1.3 extension for stateful packet processing), showing numerical results on its performance in terms of recovery latency and packet losses. 	
1702.07688v5	http://arxiv.org/pdf/1702.07688v5	2018	What determines the ultimate precision of a quantum computer?	Xavier Waintal	  A quantum error correction (QEC) code uses $N_{\rm c}$ quantum bits to construct one "logical" quantum bits of better quality than the original "physical" ones. QEC theory predicts that the failure probability $p_L$ of logical qubits decreases exponentially with $N_{\rm c}$ provided the failure probability $p$ of the physical qubit is below a certain threshold $p<p_{\rm th}$. In particular QEC theorems imply that the logical qubits can be made arbitrarily precise by simply increasing $N_{\rm c}$. In this letter, we search for physical mechanisms that lie outside of the hypothesis of QEC theorems and set a limit $\eta_{\rm L}$ to the precision of the logical qubits (irrespectively of $N_{\rm c}$). $\eta_{\rm L}$ directly controls the maximum number of operations $\propto 1/\eta_{\rm L}^2$ that can be performed before the logical quantum state gets randomized, hence the depth of the quantum circuits that can be considered. We identify a type of error - silent stabilizer failure - as a mechanism responsible for finite $\eta_{\rm L}$ and discuss its possible causes. Using the example of the topological surface code, we show that a single local event can provoke the failure of the logical qubit, irrespectively of $N_c$. 	
1706.01600v2	http://arxiv.org/pdf/1706.01600v2	2017	Atomistic Representation of Anomalies in the Failure Behaviour of   Nanocrystalline Silicene	Tawfiqur Rakib|Sourav Saha|Mohammad Motalab|Satyajit Mojumder|Md Mahbubul Islam	  Silicene, a 2D analogue of graphene, has spurred a tremendous research interest in the scientific community for its unique properties essential for next generation electronic devices. In this work, for the first time, we present a molecular dynamics (MD) investigation to determine the fracture strength and toughness of nanocrystalline silicene (nc silicene) sheet of varied grain size and pre existing crack length at room temperature. Our results suggest that the transition from an inverse pseudo Hall Petch to a pseudo Hall Petch behavior in nc silicene occurs at a critical grain size of 17.32 nm. This phenomenon is also prevalent in nanocrystalline graphene. However, nc silicene with pre existing cracks exhibits anomalous crack propagation and fracture toughness behaviour. We have observed two distinct types of failure mechanisms (crack sensitive and insensitive failure) and devised the mechanophysical conditions under which they occur. Fracture toughness calculated from both Griffiths theory and MD simulations indicate that the former overpredicts the fracture toughness of nc silicene. The most striking outcome, however, is that despite the presence of a pre existing crack, the crack sensitivity of nc silicene is found to be dependent on the grain size and their orientations. This study is the first direct comparison of atomistic simulations to the continuum theories to predict the anomalous behaviour in deformation and failure mechanisms of nc silicene. 	
1708.02030v1	http://arxiv.org/pdf/1708.02030v1	2017	CRAFT: A library for easier application-level Checkpoint/Restart and   Automatic Fault Tolerance	Faisal Shahzad|Jonas Thies|Moritz Kreutzer|Thomas Zeiser|Georg Hager|Gerhard Wellein	  In order to efficiently use the future generations of supercomputers, fault tolerance and power consumption are two of the prime challenges anticipated by the High Performance Computing (HPC) community. Checkpoint/Restart (CR) has been and still is the most widely used technique to deal with hard failures. Application-level CR is the most effective CR technique in terms of overhead efficiency but it takes a lot of implementation effort. This work presents the implementation of our C++ based library CRAFT (Checkpoint-Restart and Automatic Fault Tolerance), which serves two purposes. First, it provides an extendable library that significantly eases the implementation of application-level checkpointing. The most basic and frequently used checkpoint data types are already part of CRAFT and can be directly used out of the box. The library can be easily extended to add more data types. As means of overhead reduction, the library offers a build-in asynchronous checkpointing mechanism and also supports the Scalable Checkpoint/Restart (SCR) library for node level checkpointing. Second, CRAFT provides an easier interface for User-Level Failure Mitigation (ULFM) based dynamic process recovery, which significantly reduces the complexity and effort of failure detection and communication recovery mechanism. By utilizing both functionalities together, applications can write application-level checkpoints and recover dynamically from process failures with very limited programming effort. This work presents the design and use of our library in detail. The associated overheads are thoroughly analyzed using several benchmarks. 	
1411.3990v2	http://arxiv.org/pdf/1411.3990v2	2015	Dynamic Modeling of Cascading Failure in Power Systems	Jiajia Song|Eduardo Cotilla-Sanchez|Goodarz Ghanavati|Paul D. H. Hines	  The modeling of cascading failure in power systems is difficult because of the many different mechanisms involved; no single model captures all of these mechanisms. Understanding the relative importance of these different mechanisms is an important step in choosing which mechanisms need to be modeled for particular types of cascading failure analysis. This work presents a dynamic simulation model of both power networks and protection systems, which can simulate a wider variety of cascading outage mechanisms, relative to existing quasi-steady state (QSS) models. The model allows one to test the impact of different load models and protections on cascading outage sizes. This paper describes each module of the developed dynamic model and demonstrates how different mechanisms interact. In order to test the model we simulated a batch of randomly selected $N-2$ contingencies for several different static load configurations, and found that the distribution of blackout sizes and event lengths from the proposed dynamic simulator correlates well with historical trends. The results also show that load models have significant impacts on the cascading risks. This dynamic model was also compared against a QSS model based on the dc power flow approximations; we find that the two models largely agree, but produce substantially different results for later stages of cascading. 	
1301.3455v1	http://arxiv.org/pdf/1301.3455v1	2013	3D Geological Modeling and Visualization of Rock Masses Based on Google   Earth: A Case Study	Gang Mei|John C. Tipper|Nengxiong Xu	  Google Earth (GE) has become a powerful tool for geological modeling and visualization. An interesting and useful feature of GE, Google Street View, can allow the GE users to view geological structure such as layers of rock masses at a field site. In this paper, we introduce a practical solution for building 3D geological models for rock masses based on the data acquired by use with GE. A real study case at Haut-Barr, France is presented to demonstrate our solution. We first locate the position of Haut-Barr in GE, and then determine the shape and scale of the rock masses in the study area, and thirdly acquire the layout of layers of rock masses in the Google Street View, and finally create the approximate 3D geological models by extruding and intersecting. The generated 3D geological models can simply reflect the basic structure of the rock masses at Haut-Barr, and can be used for visualizing the rock bodies interactively. 	
1610.04827v1	http://arxiv.org/pdf/1610.04827v1	2016	On the effect of a two-rocks boundary on the propagation of nonlinear   transients of temperature and pressure in deformable porous rocks	E. Salusti|R. Droghei|R. Garra	  We here analyze the propagation of transients of fluid-rock temperature and pressure through a thin boundary layer, where a steady trend is present, between two adjacent homogeneous rocks. We focus on the effect of convection on transients crossing such thin layer. In comparison with early models where this boundary was assumed a sharp mathematical plane separating the two rocks, here we show a realistic analysis of such boundary layer that implies a novel nonlinear model. Its solutions describe large amplitude, quick and sharp transients characterized by a novel drift and variations of the signal amplitude, leading to a nonlinear wave propagation. Possible applications are in volcanic, hydrologic, hydrothermal systems as well as for deep oil drilling. In addition, this formalism could easily be generalized for the case of a signal arriving in a rock characterized by a steady trend of pressure and/or temperature. These effects, being proportional to the initial conditions, can also give velocity variations not particularly important. A further heuristic model has therefore been analyzed, i.e. assuming a pressure dependent rock permeability. In this way, a remarkable increase of the system velocities is obtained. 	
1702.04385v1	http://arxiv.org/pdf/1702.04385v1	2017	Accretion of Saturn's Inner Mid-sized Moons from a Massive Primordial   Ice Ring	Julien Salmon|Robin M. Canup	  Saturn's rings are rock-poor, containing 90%-95% ice by mass. As a group, Saturn's moons interior to and including Tethys are also about 90% ice. Tethys itself contains <6% rock by mass, in contrast to its similar-mass outer neighbor Dione, which contains >40% rock. Here we simulate the evolution of a massive primordial ice-rich ring and the production of satellites as ring material spreads beyond the Roche limit. We describe the Roche-interior ring with an analytic model, and use an N-body code to describe material beyond the Roche limit. We track the accretion and interactions of spawned satellites, including tidal interaction with the planet, assuming a tidal dissipation factor for Saturn of $Q~10^4$. We find that ring torques and capture of moons into mutual resonances produce a system of ice-rich inner moons that extends outward to approximately Tethys's orbit in $10^9$ years, even with relatively slow orbital expansion due to tides. The resulting mass and semimajor axis distribution of spawned moons resembles that of Mimas, Enceladus, and Tethys. We estimate the mass of rock delivered to the moons by external cometary impactors during a late heavy bombardment. We find that the inner moons receive a mass in rock comparable to their current total rock content, while Dione and Rhea receive an order-of-magnitude less rock than their current rock content. This suggests that external ontamination may have been the primary source of rock in the inner moons, and that Dione and Rhea formed from much more rock-rich source material. Reproducing the distribution of rock among the current inner moons is challenging, and appears to require large impactors stochasticity and/or the presence of some rock in the initial ring. 	
1707.08403v2	http://arxiv.org/pdf/1707.08403v2	2017	Atomistic study of hardening mechanism in Al-Cu nanostructure	Satyajit Mojumder|Tawfiqur Rakib|Mohammad Motalab|Dibakar Datta	  Nanostructures have the immense potential to supplant the traditional metallic structure as they show enhanced mechanical properties through strain hardening. In this paper, the effect of grain size on the hardening mechanism of Al-Cu nanostructure is elucidated by molecular dynamics simulation. Al-Cu (50-54% Cu by weight) nanostructure having an average grain size of 4.57 to 7.26 nm are investigated for tensile simulation at different strain rate using embedded atom method (EAM) potential at a temperature of 50~500K. It is found that the failure mechanism of the nanostructure is governed by the temperature, grain size as well as strain rate effect. At the high temperature of 300-500K, the failure strength of Al-Cu nanostructure increases with the decrease of average grain size following Hall-Petch relation. Dislocation motions are hindered significantly when the grain size is decreased which play a vital role on the hardening of the nanostructure. The failure is always found to initiate at a particular Al grain due to its weak link and propagates through grain boundary (GB) sliding, diffusion, dislocation nucleation and propagation. We also visualize the dislocation density at different grain size to show how the dislocation affects the material properties at the nanoscale. These results will further aid investigation on the deformation mechanism of nanostructure. 	
0901.1907v1	http://arxiv.org/pdf/0901.1907v1	2009	The mechanisms of spatial and temporal earthquake clustering	E. A. Jagla|A. B. Kolton	  The number of earthquakes as a function of magnitude decays as a power law. This trend is usually justified using spring-block models, where slips with the appropriate global statistics have been numerically observed. However, prominent spatial and temporal clustering features of earthquakes are not reproduced by this kind of modeling. We show that when a spring-block model is complemented with a mechanism allowing for structural relaxation, realistic earthquake patterns are obtained. The proposed model does not need to include a phenomenological velocity weakening friction law, as traditional spring-block models do, since this behavior is effectively induced by the relaxational mechanism as well. In this way, the model provides also a simple microscopic basis for the widely used phenomenological rate-and-state equations of rock friction. 	
1211.4656v1	http://arxiv.org/pdf/1211.4656v1	2012	A mathematical framework for inverse wave problems in heterogeneous   media	Kirk D. Blazek|Christiaan C. Stolk|William W. Symes	  This paper provides a theoretical foundation for some common formulations of inverse problems in wave propagation, based on hyperbolic systems of linear integro-differential equations with bounded and measurable coefficients. The coefficients of these time-dependent partial differential equations respresent parametrically the spatially varying mechanical properties of materials. Rocks, manufactured materials, and other wave propagation environments often exhibit spatial heterogeneity in mechanical properties at a wide variety of scales, and coefficient functions representing these properties must mimic this heterogeneity. We show how to choose domains (classes of nonsmooth coefficient functions) and data definitions (traces of weak solutions) so that optimization formulations of inverse wave problems satisfy some of the prerequisites for application of Newton's method and its relatives. These results follow from the properties of a class of abstract first-order evolution systems, of which various physical wave systems appear as concrete instances. Finite speed of propagation for linear waves with bounded, measurable mechanical parameter fields is one of the by-products of this theory. 	
1311.4275v1	http://arxiv.org/pdf/1311.4275v1	2013	The Superposition Principle in Quantum Mechanics - did the rock enter   the foundation surreptitiously?	N. D. Hari Dass	  The superposition principle forms the very backbone of quantum theory. The resulting linear structure of quantum theory is structurally so rigid that tampering with it may have serious, seemingly unphysical, consequences. This principle has been succesful at even the highest available accelerator energies. Is this aspect of quantum theory forever then? The present work is an attempt to understand the attitude of the founding fathers, particularly of Bohr and Dirac, towards this principle. The Heisenberg matrix mechanics on the one hand, and the Schrodinger wave mechanics on the other, are critically examined to shed light as to how this principle entered the very foundations of quantum theory. 	
1508.07082v1	http://arxiv.org/pdf/1508.07082v1	2015	Does a dissolution-precipitation mechanism explain concrete creep in   moist environments?	Isabella Pignatelli|Aditya Kumar|Rouhollah Alizadeh|Yann Le Pape|Mathieu Bauchy|Gaurav Sant	  Long-term creep (i.e., deformation under sustained load) is a significant material response that needs to be accounted for in concrete structural design. However, the nature and origin of creep remains poorly understood, and controversial. Here, we propose that concrete creep at RH (relative humidity) > 50%, but fixed moisture-contents (i.e., basic creep), arises from a dissolution-precipitation mechanism, active at nanoscale grain contacts, as is often observed in a geological context, e.g., when rocks are exposed to sustained loads, in moist environments. Based on micro-indentation and vertical scanning interferometry experiments, and molecular dynamics simulations carried out on calcium-silicate-hydrates (C-S-H's), the major binding phase in concrete, of different compositions, we show that creep rates are well correlated to dissolution rates - an observation which supports the dissolution-precipitation mechanism as the origin of concrete creep. C-S-H compositions featuring high resistance to dissolution, and hence creep are identified - analysis of which, using topological constraint theory, indicates that these compositions present limited relaxation modes on account of their optimally connected (i.e., constrained) atomic networks. 	
1611.09624v2	http://arxiv.org/pdf/1611.09624v2	2017	Spatial organization and cyclic dominance in asymmetric predator-prey   spatial games	Annette Cazaubiel|Alessandra F. Lütz|Jeferson J. Arenzon	  Predators may attack isolated or grouped prey in a cooperative, collective way. Whether a gregarious behavior is advantageous to each species depends on several conditions and game theory is a useful tool to deal with such a problem. We here extend the Lett-Auger-Gaillard model [Theor. Pop. Biol. 65 (2004) 263] to spatially distributed populations and compare the resulting behavior with their mean-field predictions for the coevolving densities of predator and prey strategies. Besides its richer behavior in the presence of spatial organization, we also show that the coexistence phase in which collective and individual strategies for each group are present is stable because of an effective, cyclic dominance mechanism similar to a well-studied generalization of the Rock-Paper-Scissors game with four species, a further example of how ubiquitous this coexistence mechanism is. 	
9712018v1	http://arxiv.org/pdf/chao-dyn/9712018v1	1997	Failure of linear control in noisy coupled map lattices	David A. Egolf|Joshua E. S. Socolar	  We study a 1D ring of diffusively coupled logistic maps in the vicinity of an unstable, spatially homogeneous fixed point. The failure of linear controllers due to additive noise is discussed with the aim of clarifying the failure mechanism. A criterion is suggested for estimating the noise level that can be tolerated by the given controller. The criterion implies the loss of control for surprisingly low noise levels in certain cases of interest, and accurately accounts for the results of numerical experiments over a broad range of parameter values. Previous results of Grigoriev, et al (Phys. Rev. Lett., 79, 2795) are reviewed and compared with our numerical and analytical results. 	
9612096v1	http://arxiv.org/pdf/cond-mat/9612096v1	1996	Modeling Acoustic Emission in Microfracturing Phenomena	Stefano Zapperi|Alessandro Vespignani|H. Eugene Stanley	  It has been recently observed that synthetic materials subjected to an external elastic stress give rise to scaling phenomena in the acoustic emission signal. Motivated by this experimental finding we develop a mesoscopic model in order to clarify the nature of this phenomenon. We model the synthetic material by an array of resistors with random failure thresholds. The failure of a resistor produces an decrease in the conductivity and a redistribution of the disorder. By increasing the applied voltage the system organizes itself in a stationary state. The acoustic emission signal is associated with the failure events. We find scaling behavior in the amplitude of these events and in the times between different events. The model allows us to study the geometrical and topological properties of the micro-fracturing process that drives the system to the self-organized stationary state. 	
9811153v3	http://arxiv.org/pdf/cond-mat/9811153v3	1998	Bounds for the time to failure of hierarchical systems of fracture	J. B. Gomez|M. Vazquez-Prada|Y. Moreno|A. F. Pacheco	  For years limited Monte Carlo simulations have led to the suspicion that the time to failure of hierarchically organized load-transfer models of fracture is non-zero for sets of infinite size. This fact could have a profound significance in engineering practice and also in geophysics. Here, we develop an exact algebraic iterative method to compute the successive time intervals for individual breaking in systems of height $n$ in terms of the information calculated in the previous height $n-1$. As a byproduct of this method, rigorous lower and higher bounds for the time to failure of very large systems are easily obtained. The asymptotic behavior of the resulting lower bound leads to the evidence that the above mentioned suspicion is actually true. 	
0201257v1	http://arxiv.org/pdf/cond-mat/0201257v1	2002	Failure time and critical behaviour of fracture precursors in   heterogeneous materials	A. Guarino|S. Ciliberto|A. Garcimartin|M. Zei|R. Scorretti	  The acoustic emission of fracture precursors, and the failure time of samples of heterogeneous materials (wood, fiberglass) are studied as a function of the load features and geometry. It is shown that in these materials the failure time is predicted with a good accuracy by a model of microcrack nucleation proposed by Pomeau. We find that the time interval $% \delta t$ between events (precursors) and the energy $\varepsilon$ are power law distributed and that the exponents of these power laws depend on the load history and on the material. In contrast, the cumulated acoustic energy $E$ presents a critical divergency near the breaking time $\tau $ which is $% E\sim \left( \frac{\tau -t}\tau \right) ^{-\gamma }$. The positive exponent $% \gamma $ is independent, within error bars, on all the experimental parameters. 	
0410132v4	http://arxiv.org/pdf/cond-mat/0410132v4	2009	Quantum phase transitions in the sub-ohmic spin-boson model: Failure of   the quantum-classical mapping	Matthias Vojta|Ning-Hua Tong|Ralf Bulla	  The effective theories for many quantum phase transitions can be mapped onto those of classical transitions. Here we show that such a mapping fails for the sub-ohmic spin-boson model which describes a two-level system coupled to a bosonic bath with power-law spectral density, J(omega) ~ omega^s. Using an epsilon expansion we prove that this model has a quantum transition controlled by an interacting fixed point at small s, and support this by numerical calculations. In contrast, the corresponding classical long-range Ising model is known to have an upper-critical dimension at s = 1/2, with mean-field transition behavior controlled by a non-interacting fixed point for 0 < s < 1/2. The failure of the quantum-classical mapping is argued to arise from the long-ranged interaction in imaginary time in the quantum model. 	
0502122v2	http://arxiv.org/pdf/cond-mat/0502122v2	2005	A simple beam model for the shear failure of interfaces	F. Raischel|F. Kun|H. J. Herrmann	  We propose a novel model for the shear failure of a glued interface between two solid blocks. We model the interface as an array of elastic beams which experience stretching and bending under shear load and break if the two deformation modes exceed randomly distributed breaking thresholds. The two breaking modes can be independent or combined in the form of a von Mises type breaking criterion. Assuming global load sharing following the beam breaking, we obtain analytically the macroscopic constitutive behavior of the system and describe the microscopic process of the progressive failure of the interface. We work out an efficient simulation technique which allows for the study of large systems. The limiting case of very localized interaction of surface elements is explored by computer simulations. 	
0503231v1	http://arxiv.org/pdf/cond-mat/0503231v1	2005	Failure of a granular step	Saloome Siavoshi|Arshad Kudrolli	  We investigate the gravity driven rapid failure of a granular step composed of non-cohesive steel beads. The step is initially held together with electromagnets, and released when the current is switched off. We visualize the surface and the motion of the grains during the entire relaxation. The initial failure occurs at the surface and the subsequent flow is also confined to the surface as the step relaxes to its final state. The final shape of the surface is almost linear, depends on the initial angle of the step, and is not sensitive to the size of the grains. The average final slope of the pile is only slightly lower than the angle of repose of a pile formed by slowly pouring particles on to a flat surface. The evolution of the step is compared with a proposed convective-diffusion model of our system. The qualitative features of the relaxation are captured by the model after a flow-dependent dissipation parameter is introduced. 	
0506699v1	http://arxiv.org/pdf/cond-mat/0506699v1	2005	Optimization of robustness of scale-free network to random and targeted   attacks	Jian-Guo Liu|Zhong-Tuo Wang|Yan-Zhong Dang	  The scale-fee networks, having connectivity distribution $P(k)\sim k^{-\alpha}$ (where $k$ is the site connectivity), is very resilient to random failures but fragile to intentional attack. The purpose of this paper is to find the network design guideline which can make the robustness of the network to both random failures and intentional attack maximum while keeping the average connectivity $<k>$ per node constant. We find that when $<k>=3$ the robustness of the scale-free networks reach its maximum value if the minimal connectivity $m=1$, but when $<k>$ is larger than four, the networks will become more robust to random failures and targeted attacks as the minimal connectivity $m$ gets larger. 	
0601402v2	http://arxiv.org/pdf/cond-mat/0601402v2	2006	Mechanism for the failure of the Edwards hypothesis in the SK spin glass	P. R. Eastham|R. A. Blythe|A. J. Bray|M . A. Moore	  The dynamics of the SK model at T=0 starting from random spin configurations is considered. The metastable states reached by such dynamics are atypical of such states as a whole, in that the probability density of site energies, $p(\lambda)$, is small at $\lambda=0$. Since virtually all metastable states have a much larger $p(0)$, this behavior demonstrates a qualitative failure of the Edwards hypothesis. We look for its origins by modelling the changes in the site energies during the dynamics as a Markov process. We show how the small $p(0)$ arises from features of the Markov process that have a clear physical basis in the spin-glass, and hence explain the failure of the Edwards hypothesis. 	
0006008v1	http://arxiv.org/pdf/cs/0006008v1	2000	Performing work efficiently in the presence of faults	Cynthia Dwork|Joseph Y. Halpern|O. Waarts	  We consider a system of t synchronous processes that communicate only by sending messages to one another, and that together must perform $n$ independent units of work. Processes may fail by crashing; we want to guarantee that in every execution of the protocol in which at least one process survives, all n units of work will be performed. We consider three parameters: the number of messages sent, the total number of units of work performed (including multiplicities), and time. We present three protocols for solving the problem. All three are work-optimal, doing O(n+t) work. The first has moderate costs in the remaining two parameters, sending O(t\sqrt{t}) messages, and taking O(n+t) time. This protocol can be easily modified to run in any completely asynchronous system equipped with a failure detection mechanism. The second sends only O(t log{t}) messages, but its running time is large (exponential in n and t). The third is essentially time-optimal in the (usual) case in which there are no failures, and its time complexity degrades gracefully as the number of failures increases. 	
0209060v2	http://arxiv.org/pdf/nlin/0209060v2	2002	Spectral theory for the failure of linear control in a nonlinear   stochastic system	Roman O. Grigoriev|Andreas Handel	  We consider the failure of localized control in a nonlinear spatially extended system caused by extremely small amounts of noise. It is shown that this failure occurs as a result of a nonlinear instability. Nonlinear instabilities can occur in systems described by linearly stable but strongly nonnormal evolution operators. In spatially extended systems the nonnormality manifests itself in two different but complementary ways: transient amplification and spectral focusing of disturbances. We show that temporal and spatial aspects of the nonnormality and the type of nonlinearity are all crucially important to understanding and describing the mechanism of nonlinear instability. Presented results are expected to apply equally to other physical systems where strong nonnormality is due to the presence of mean flow rather than the action of control. 	
0701002v2	http://arxiv.org/pdf/nlin/0701002v2	2007	Failure of Parameter Identification Based on Adaptive Synchronization   Techniques	Wei Lin|Huan-fei Ma	  In the paper, several concrete examples, as well as their numerical simulations, are given to show that parameter identification based on the so-called adaptive synchronization techniques might be failed if those functions with parameters pending for identification in coupled systems are designed to be mutually linearly dependent or approximately linearly dependent on the orbit in the synchronization manifold. This failure might be emergent not only when the synchronized orbit is selected to be some sort of equilibrium or some sort of periodic oscillation, but also when it is taken as some type of chaotic attractor produced by driving system. This result implies that chaotic property of driving signal is not necessary to achievement of parameter identification. The mechanism inducing such a failure, as well as the bounded property of all trajectories generated by coupled systems, is theoretically expatiated. New synchronization techniques are proposed to rigorously realize the complete synchronization and parameter identification in a class of systems where the nonlinearity is not globally Lipschitz. In addition, parameter identification are discussed for systems with time delay. 	
0706.1554v1	http://arxiv.org/pdf/0706.1554v1	2007	Continuous Damage Fiber Bundle Model for Strongly Disordered Materials	F. Raischel|F. Kun|H. J. Herrmann	  We present an extension of the continuous damage fiber bundle model to describe the gradual degradation of highly heterogeneous materials under an increasing external load. Breaking of a fiber in the model is preceded by a sequence of partial failure events occurring at random threshold values. In order to capture the subsequent propagation and arrest of cracks, furthermore, the disorder of the number of degradation steps of material constituents, the failure thresholds of single fibers are sorted into ascending order and their total number is a Poissonian distributed random variable over the fibers. Analytical and numerical calculations showed that the failure process of the system is governed by extreme value statistics, which has a substantial effect on the macroscopic constitutive behaviour and on the microscopic bursting activity as well. 	
0708.2709v1	http://arxiv.org/pdf/0708.2709v1	2007	Bicomponents and the robustness of networks to failure	M. E. J. Newman|Gourab Ghoshal	  A common definition of a robust connection between two nodes in a network such as a communication network is that there should be at least two independent paths connecting them, so that the failure of no single node in the network causes them to become disconnected. This definition leads us naturally to consider bicomponents, subnetworks in which every node has a robust connection of this kind to every other. Here we study bicomponents in both real and model networks using a combination of exact analytic techniques and numerical methods. We show that standard network models predict there to be essentially no small bicomponents in most networks, but there may be a giant bicomponent, whose presence coincides with the presence of the ordinary giant component, and we find that real networks seem by and large to follow this pattern, although there are some interesting exceptions. We study the size of the giant bicomponent as nodes in the network fail, using a specially developed computer algorithm based on data trees, and find in some cases that our networks are quite robust to failure, with large bicomponents persisting until almost all vertices have been removed. 	
0711.1602v2	http://arxiv.org/pdf/0711.1602v2	2008	Preservation of network Degree Distributions from non-uniform failures	Brian Karrer|Gourab Ghoshal	  There has been a considerable amount of interest in recent years on the robustness of networks to failures. Many previous studies have concentrated on the effects of node and edge removals on the connectivity structure of a static network; the networks are considered to be static in the sense that no compensatory measures are allowed for recovery of the original structure. Real world networks such as the world wide web, however, are not static and experience a considerable amount of turnover, where nodes and edges are both added and deleted. Considering degree-based node removals, we examine the possibility of preserving networks from these types of disruptions. We recover the original degree distribution by allowing the network to react to the attack by introducing new nodes and attaching their edges via specially tailored schemes. We focus particularly on the case of non-uniform failures, a subject that has received little attention in the context of evolving networks. Using a combination of analytical techniques and numerical simulations, we demonstrate how to preserve the exact degree distribution of the studied networks from various forms of attack. 	
0805.3235v1	http://arxiv.org/pdf/0805.3235v1	2008	Classical and quantum breakdown in disordered materials	Debashis Samanta|Bikas K. Chakrabarti|Purusattam Ray	  We have discussed the classical failure of the fuse system, the dielectric breakdown and the quantum breakdown in the Anderson insulators. We have discussed how the extreme value statistics and the resulting Gumbel distribution arises in breakdown and failure processes, especially when the disorder concentration is low. At high concentration of disorder near the percolation threshold, we have discussed how the cross-over might take place from extreme value to percolation statistics. We have discussed the system size dependence that arises at the distribution of the failure current at low disordered regime. Finally, the extension of Zener breakdown phenomenon for band insulators to the disordered-induced Anderson insulators has been discussed. 	
0812.3591v1	http://arxiv.org/pdf/0812.3591v1	2008	How to make a fragile network robust and vice versa	Andre A. Moreira|Jose S. Andrade|Hans J. Herrmann|Joseph O. Indekeu	  We investigate topologically biased failure in scale-free networks with degree distribution $P(k) \propto k^{-\gamma}$. The probability $p$ that an edge remains intact is assumed to depend on the degree $k$ of adjacent nodes $i$ and $j$ through $p_{ij}\propto(k_{i}k_{j})^{-\alpha}$. By varying the exponent $\alpha$, we interpolate between random ($\alpha=0$) and systematic failure. For $\alpha >0 $ ($<0$) the most (least) connected nodes are depreciated first. This topological bias introduces a characteristic scale in $P(k)$ of the depreciated network, marking a crossover between two distinct power laws. The critical percolation threshold, at which global connectivity is lost, depends both on $\gamma$ and on $\alpha$. As a consequence, network robustness or fragility can be controlled through fine tuning of the topological bias in the failure process. 	
1001.4350v1	http://arxiv.org/pdf/1001.4350v1	2010	Phase field modeling of crack propagation	R. Spatschek|E. Brener|A. Karma	  Fracture is a fundamental mechanism of materials failure. Propagating cracks can exhibit a rich dynamical behavior controlled by a subtle interplay between microscopic failure processes in the crack tip region and macroscopic elasticity. We review recent approaches to understand crack dynamics using the phase field method. This method, developed originally for phase transformations, has the well-known advantage of avoiding explicit front tracking by making material interfaces spatially diffuse. In a fracture context, this method is able to capture both the short-scale physics of failure and macroscopic linear elasticity within a self-consistent set of equations that can be simulated on experimentally relevant length and time scales. We discuss the relevance of different models, which stem from continuum field descriptions of brittle materials and crystals, to address questions concerning crack path selection and branching instabilities, as well as models that are based on mesoscale concepts for crack tip scale selection. Open questions which may be addressed using phase field models of fracture are summarized. 	
1012.0206v1	http://arxiv.org/pdf/1012.0206v1	2010	Catastrophic Cascade of Failures in Interdependent Networks	S. Havlin|N. A. M. Araujo|S. V. Buldyrev|C. S. Dias|R. Parshani|G. Paul|H. E. Stanley	  Modern network-like systems are usually coupled in such a way that failures in one network can affect the entire system. In infrastructures, biology, sociology, and economy, systems are interconnected and events taking place in one system can propagate to any other coupled system. Recent studies on such coupled systems show that the coupling increases their vulnerability to random failure. Properties for interdependent networks differ significantly from those of single-network systems. In this article, these results are reviewed and the main properties discussed. 	
1012.0815v1	http://arxiv.org/pdf/1012.0815v1	2010	Statistical Classification of Cascading Failures in Power Grids	René Pfitzner|Konstantin Turitsyn|Michael Chertkov	  We introduce a new microscopic model of the outages in transmission power grids. This model accounts for the automatic response of the grid to load fluctuations that take place on the scale of minutes, when the optimum power flow adjustments and load shedding controls are unavailable. We describe extreme events, initiated by load fluctuations, which cause cascading failures of loads, generators and lines. Our model is quasi-static in the causal, discrete time and sequential resolution of individual failures. The model, in its simplest realization based on the Directed Current description of the power flow problem, is tested on three standard IEEE systems consisting of 30, 39 and 118 buses. Our statistical analysis suggests a straightforward classification of cascading and islanding phases in terms of the ratios between average number of removed loads, generators and links. The analysis also demonstrates sensitivity to variations in line capacities. Future research challenges in modeling and control of cascading outages over real-world power networks are discussed. 	
1101.3520v1	http://arxiv.org/pdf/1101.3520v1	2011	Error-Free Multi-Valued Consensus with Byzantine Failures	Guanfeng Liang|Nitin Vaidya	  In this paper, we present an efficient deterministic algorithm for consensus in presence of Byzantine failures. Our algorithm achieves consensus on an $L$-bit value with communication complexity $O(nL + n^4 L^{0.5} + n^6)$ bits, in a network consisting of $n$ processors with up to $t$ Byzantine failures, such that $t<n/3$. For large enough $L$, communication complexity of the proposed algorithm approaches $O(nL)$ bits. In other words, for large $L$, the communication complexity is linear in the number of processors in the network. This is an improvement over the work of Fitzi and Hirt (from PODC 2006), who proposed a probabilistically correct multi-valued Byzantine consensus algorithm with a similar complexity for large $L$. In contrast to the algorithm by Fitzi and Hirt, our algorithm is guaranteed to be always error-free. Our algorithm require no cryptographic technique, such as authentication, nor any secret sharing mechanism. To the best of our knowledge, we are the first to show that, for large $L$, error-free multi-valued Byzantine consensus on an $L$-bit value is achievable with $O(nL)$ bits of communication. 	
1102.5085v2	http://arxiv.org/pdf/1102.5085v2	2016	Robustness and modular structure in networks	James P. Bagrow|Sune Lehmann|Yong-Yeol Ahn	  Complex networks have recently attracted much interest due to their prevalence in nature and our daily lives [1, 2]. A critical property of a network is its resilience to random breakdown and failure [3-6], typically studied as a percolation problem [7-9] or by modeling cascading failures [10-12]. Many complex systems, from power grids and the Internet to the brain and society [13-15], can be modeled using modular networks comprised of small, densely connected groups of nodes [16, 17]. These modules often overlap, with network elements belonging to multiple modules [18, 19]. Yet existing work on robustness has not considered the role of overlapping, modular structure. Here we study the robustness of these systems to the failure of elements. We show analytically and empirically that it is possible for the modules themselves to become uncoupled or non-overlapping well before the network disintegrates. If overlapping modular organization plays a role in overall functionality, networks may be far more vulnerable than predicted by conventional percolation theory. 	
1105.3574v3	http://arxiv.org/pdf/1105.3574v3	2012	Robustness and Assortativity for Diffusion-like Processes in Scale-free   Networks	Gregorio D'Agostino|Antonio Scala|Vinko Zlatić|Guido Caldarelli	  By analysing the diffusive dynamics of epidemics and of distress in complex networks, we study the effect of the assortativity on the robustness of the networks. We first determine by spectral analysis the thresholds above which epidemics/failures can spread; we then calculate the slowest diffusional times. Our results shows that disassortative networks exhibit a higher epidemiological threshold and are therefore easier to immunize, while in assortative networks there is a longer time for intervention before epidemic/failure spreads. Moreover, we study by computer simulations the sandpile cascade model, a diffusive model of distress propagation (financial contagion). We show that, while assortative networks are more prone to the propagation of epidemic/failures, degree-targeted immunization policies increases their resilience to systemic risk. 	
1108.3883v1	http://arxiv.org/pdf/1108.3883v1	2011	Exact Regenerating Codes for Byzantine Fault Tolerance in Distributed   Storage	Yunghsiang S. Han|Rong Zheng|Wai Ho Mow	  Due to the use of commodity software and hardware, crash-stop and Byzantine failures are likely to be more prevalent in today's large-scale distributed storage systems. Regenerating codes have been shown to be a more efficient way to disperse information across multiple nodes and recover crash-stop failures in the literature. In this paper, we present the design of regeneration codes in conjunction with integrity check that allows exact regeneration of failed nodes and data reconstruction in presence of Byzantine failures. A progressive decoding mechanism is incorporated in both procedures to leverage computation performed thus far. The fault-tolerance and security properties of the schemes are also analyzed. 	
1204.1529v1	http://arxiv.org/pdf/1204.1529v1	2012	Ensuring QOS Guarantees in a Hybrid OCS/OBS Network	Sunish Kumar O S	  The bursting aggregation assembly in edge nodes is one of the key technologies in OBS (Optical Burst Switching) network, which has a direct impact on flow characteristics and packet loss rate. An optical burst assembly technique supporting QoS is presented through this paper, which can automatically adjust the threshold along with the increasing and decreasing volume of business, reduce the operational burst, and generate corresponding BDP (Burst Data Packet) and BCP (Burst Control Packet). In addition to the burst aggregation technique a packet recovery technique by restoration method is also described. The data packet loss due to the physical optical link failure is not currently included in the QoS descriptions. This link failure is also a severe problem which reduces the data throughput of the transmitter node. A mechanism for data recovery from this link failure is vital for guaranteeing the QoS demanded by each user. So this paper will also discusses a specific protocol for reducing the packet loss by utilizing the features of both optical circuit switching (OCS) and Optical Burst switching (OBS) techniques. 	
1205.2909v3	http://arxiv.org/pdf/1205.2909v3	2012	Evolution of robust network topologies: Emergence of central backbones	Tiago P. Peixoto|Stefan Bornholdt	  We model the robustness against random failure or intentional attack of networks with arbitrary large-scale structure. We construct a block-based model which incorporates --- in a general fashion --- both connectivity and interdependence links, as well as arbitrary degree distributions and block correlations. By optimizing the percolation properties of this general class of networks, we identify a simple core-periphery structure as the topology most robust against random failure. In such networks, a distinct and small "core" of nodes with higher degree is responsible for most of the connectivity, functioning as a central "backbone" of the system. This centralized topology remains the optimal structure when other constraints are imposed, such as a given fraction of interdependence links and fixed degree distributions. This distinguishes simple centralized topologies as the most likely to emerge, when robustness against failure is the dominant evolutionary force. 	
1210.8421v2	http://arxiv.org/pdf/1210.8421v2	2014	Distribution of the Number of Retransmissions of Bounded Documents	Predrag R. Jelenković|Evangelia D. Skiani	  Retransmission-based failure recovery represents a primary approach in existing communication networks that guarantees data delivery in the presence of channel failures. Recent work has shown that, when data sizes have infinite support, retransmissions can cause long (-tailed) delays even if all traffic and network characteristics are light-tailed. In this paper we investigate the practically important case of bounded data units 0 <= L_b <= b under the condition that the hazard functions of the distributions of data sizes and channel statistics are proportional. To this end, we provide an explicit and uniform characterization of the entire body of the retransmission distribution Pr[N_b > n] in both n and b. Our main discovery is that this distribution can be represented as the product of a power law and Gamma distribution. This rigorous approximation clearly demonstrates the coupling of a power law distribution, dominating the main body, and the Gamma distribution, determining the exponential tail. Our results are validated via simulation experiments and can be useful for designing retransmission-based systems with the required performance characteristics. From a broader perspective, this study applies to any other system, e.g., computing, where restart mechanisms are employed after a job processing failure. 	
1301.2851v1	http://arxiv.org/pdf/1301.2851v1	2013	Efficient algorithm to study interconnected networks	Christian M. Schneider|Nuno A. M. Araújo|Hans J. Herrmann	  Interconnected networks have been shown to be much more vulnerable to random and targeted failures than isolated ones, raising several interesting questions regarding the identification and mitigation of their risk. The paradigm to address these questions is the percolation model, where the resilience of the system is quantified by the dependence of the size of the largest cluster on the number of failures. Numerically, the major challenge is the identification of this cluster and the calculation of its size. Here, we propose an efficient algorithm to tackle this problem. We show that the algorithm scales as O(N log N), where N is the number of nodes in the network, a significant improvement compared to O(N^2) for a greedy algorithm, what permits studying much larger networks. Our new strategy can be applied to any network topology and distribution of interdependencies, as well as any sequence of failures. 	
1305.2060v1	http://arxiv.org/pdf/1305.2060v1	2013	Nonlocal failures in complex supply networks by single link additions	Dirk Witthaut|Marc Timme	  How do local topological changes affect the global operation and stability of complex supply networks? Studying supply networks on various levels of abstraction, we demonstrate that and how adding new links may not only promote but also degrade stable operation of a network. Intriguingly, the resulting overloads may emerge remotely from where such a link is added, thus resulting in nonlocal failure. We link this counter-intuitive phenomenon to Braess' paradox originally discovered in traffic networks. We use elementary network topologies to explain its underlying mechanism for different types of supply networks and find that it generically occurs across these systems. As an important consequence, upgrading supply networks such as communication networks, biological supply networks or power grids requires particular care because even adding only single connections may destabilize normal network operation and induce disturbances remotely from the location of structural change and even global cascades of failures. 	
1306.1861v1	http://arxiv.org/pdf/1306.1861v1	2013	Online Parallel Scheduling of Non-uniform Tasks: Trading Failures for   Energy	Antonio Fernández Anta|Chryssis Georgiou|Dariusz R. Kowalski|Elli Zavou	  Consider a system in which tasks of different execution times arrive continuously and have to be executed by a set of processors that are prone to crashes and restarts. In this paper we model and study the impact of parallelism and failures on the competitiveness of such an online system. In a fault-free environment, a simple Longest-in-System scheduling policy, enhanced by a redundancy-avoidance mechanism, guarantees optimality in a long-term execution. In the presence of failures though, scheduling becomes a much more challenging task. In particular, no parallel deterministic algorithm can be competitive against an offline optimal solution, even with one single processor and tasks of only two different execution times. We find that when additional energy is provided to the system in the form of processor speedup, the situation changes. Specifically, we identify thresholds on the speedup under which such competitiveness cannot be achieved by any deterministic algorithm, and above which competitive algorithms exist. Finally, we propose algorithms that achieve small bounded competitive ratios when the speedup is over the threshold. 	
1401.0280v2	http://arxiv.org/pdf/1401.0280v2	2016	Failure Processes in Embedded Monolayer Graphene under Axial Compression	Charalampos Androulidakis|Emmanuel N. Koukaras|Otakar Frank|Georgia Tsoukleri|Dimitris Sfyris|John Parthenios|Nicola Pugno|Konstantinos Papagelis|Kostya S. Novoselov|Costas Galiotis	  Exfoliated monolayer graphene flakes were embedded in a polymer matrix and loaded under axial compression. By monitoring the shifts of the 2D Raman phonons of rectangular flakes of various sizes under load, the critical strain to failure was determined. Prior to loading care was taken for the examined area of the flake to be free of residual stresses. The critical strain values for first failure were found to be independent of flake size at a mean value of -0.60 % corresponding to a yield stress of -6 GPa. By combining Euler mechanics with a Winkler approach, we show that unlike buckling in air, the presence of the polymer constraint results in graphene buckling at a fixed value of strain with an estimated wrinkle wavelength of the order of 1-2 nm. These results were compared with DFT computations performed on analogue coronene/ PMMA oligomers and a reasonable agreement was obtained. 	
1501.05976v1	http://arxiv.org/pdf/1501.05976v1	2015	Robustness of Spatial Micronetworks	Thomas C. McAndrew|Christopher M. Danforth|James P. Bagrow	  Power lines, roadways, pipelines and other physical infrastructure are critical to modern society. These structures may be viewed as spatial networks where geographic distances play a role in the functionality and construction cost of links. Traditionally, studies of network robustness have primarily considered the connectedness of large, random networks. Yet for spatial infrastructure physical distances must also play a role in network robustness. Understanding the robustness of small spatial networks is particularly important with the increasing interest in microgrids, small-area distributed power grids that are well suited to using renewable energy resources. We study the random failures of links in small networks where functionality depends on both spatial distance and topological connectedness. By introducing a percolation model where the failure of each link is proportional to its spatial length, we find that, when failures depend on spatial distances, networks are more fragile than expected. Accounting for spatial effects in both construction and robustness is important for designing efficient microgrids and other network infrastructure. 	
1502.05817v1	http://arxiv.org/pdf/1502.05817v1	2015	A Hybrid Model to Extend Vehicular Intercommunication V2V through D2D   Architecture	Emad Abd-Elrahman|Adel Mounir Said|Thouraya Toukabri|Hossam Afifi|Michel Marot	  In the recent years, many solutions for Vehicle to Vehicle (V2V) communication were proposed to overcome failure problems (also known as dead ends). This paper proposes a novel framework for V2V failure recovery using Device-to-Device (D2D) communications. Based on the unified Intelligent Transportation Systems (ITS) architecture, LTE-based D2D mechanisms can improve V2V dead ends failure recovery delays. This new paradigm of hybrid V2V-D2D communications overcomes the limitations of traditional V2V routing techniques. According to NS2 simulation results, the proposed hybrid model decreases the end to end delay (E2E) of messages delivery. A complete comparison of different D2D use cases (best & worst scenarios) is presented to show the enhancements brought by our solution compared to traditional V2V techniques. 	
1505.02571v4	http://arxiv.org/pdf/1505.02571v4	2015	Criticality in the approach to failure in amorphous solids	Jie Lin|Thomas Gueudré|Alberto Rosso|Matthieu Wyart	  Failure of amorphous solids is fundamental to various phenomena, including landslides and earthquakes. Recent experiments indicate that highly plastic regions form elongated structures that are especially apparent near the maximal shear stress $\Sigma_{\max}$ where failure occurs. This observation suggested that $\Sigma_{\max}$ acts as a critical point where the length scale of those structures diverges, possibly causing macroscopic transient shear bands. Here we argue instead that the entire solid phase ($\Sigma<\Sigma_{\max}$) is critical, that plasticity always involves system-spanning events, and that their magnitude diverges at $\Sigma_{\max}$ independently of the presence of shear bands. We relate the statistics and fractal properties of these rearrangements to an exponent $\theta$ that captures the stability of the material, which is observed to vary continuously with stress, and we confirm our predictions in elastoplastic models. 	
1507.05716v1	http://arxiv.org/pdf/1507.05716v1	2015	Fractal and Small-World Networks Formed by Self-Organized Critical   Dynamics	Akitomo Watanabe|Shogo Mizutaka|Kousuke Yakubo	  We propose a dynamical model in which a network structure evolves in a self-organized critical (SOC) manner and explain a possible origin of the emergence of fractal and small-world networks. Our model combines a network growth and its decay by failures of nodes. The decay mechanism reflects the instability of large functional networks against cascading overload failures. It is demonstrated that the dynamical system surely exhibits SOC characteristics, such as power-law forms of the avalanche size distribution, the cluster size distribution, and the distribution of the time interval between intermittent avalanches. During the network evolution, fractal networks are spontaneously generated when networks experience critical cascades of failures that lead to a percolation transition. In contrast, networks far from criticality have small-world structures. We also observe the crossover behavior from fractal to small-world structure in the network evolution. 	
1705.07998v1	http://arxiv.org/pdf/1705.07998v1	2017	Synaptic Noise Facilitates the Emergence of Self-Organized Criticality   in the Caenorhabditis elegans Neuronal Network	Koray Çiftçi	  Avalanches with power-law distributed size parameters have been observed in neuronal networks. This observation might be a manifestation of the self-organized criticality (SOC). Yet, the physiological mechanicsm of this behavior is currently unknown. Describing synaptic noise as transmission failures mainly originating from the probabilistic nature of neurotransmitter release, this study investigates the potential of this noise as a mechanism for driving the functional architecture of the neuronal networks towards SOC. To this end, a simple finite state neuron model, with activity dependent and synapse specific failure probabilities, was built based on the known anatomical connectivity data of the nematode Ceanorhabditis elegans. Beginning from random values, it was observed that synaptic noise levels picked out a set of synapses and consequently an active subnetwork which generates power-law distributed neuronal avalanches. The findings of this study brings up the possibility that synaptic failures might be a component of physiological processes underlying SOC in neuronal networks. 	
1707.01974v1	http://arxiv.org/pdf/1707.01974v1	2017	A Tissue Engineered Model of Aging: Interdependence and Cooperative   Effects in Failing Tissues	Aylin Acun|Dervis Can Vural|Pinar Zorlutuna	  Aging remains a fundamental open problem in modern biology. Although there exist a number of theories on aging on the cellular scale, nearly nothing is known about how microscopic failures cascade to macroscopic failures of tissues, organs and ultimately the organism. The goal of this work is to bridge microscopic cell failure to macroscopic manifestations of aging. We use tissue engineered constructs to control the cellular-level damage and cell-cell distance in individual tissues to establish the role of complex interdependence and interactions between cells in aging tissues. We found that while microscopic mechanisms drive aging, the interdependency between cells plays a major role in tissue death, providing evidence on how cellular aging is connected to its higher systemic consequences. 	
1707.08136v1	http://arxiv.org/pdf/1707.08136v1	2017	Monte-Carlo acceleration: importance sampling and hybrid dynamic systems	H. Chraibi|A. Dutfoy|T. Galtier|J. Garnier	  The reliability of a complex industrial system can rarely be assessed analytically. As system failure is often a rare event, crude Monte-Carlo methods are prohibitively expensive from a computational point of view. In order to reduce computation times, variance reduction methods such as importance sampling can be used. We propose an adaptation of this method for a class of multi-component dynamical systems. We address a system whose failure corresponds to a physical variable of the system (temperature, pressure, water level) entering a critical region. Such systems are common in hydraulic and nuclear industry. In these systems, the statuses of the components (on, off, or out-of-order) determine the dynamics of the physical variables, and is altered both by deterministic feedback mechanisms and random failures or repairs. In order to deal with this interplay between components status and physical variables we model trajectory using piecewise deterministic Markovian processes (PDMP). We show how to adapt the importance sampling method to PDMP, by introducing a reference measure on the trajectory space, and we present a biasing strategy for importance sampling. A simulation study compares our importance sampling method to the crude Monte-Carlo method for a three-component-system. 	
1711.00964v1	http://arxiv.org/pdf/1711.00964v1	2017	Ergodicity breaking dynamics of arch collapse	Carl Merrigan|Sumit Kumar Birwa|Shubha Tewari|Bulbul Chakraborty	  Gravity driven flows such as in hoppers and silos are susceptible to clogging due to the formation of arches at the exit whose failure is the key to re-initiation of flow. In vibrated hoppers, clog durations exhibit a broad distribution, which poses a challenge for devising efficient unclogging protocols. Using numerical simulations, we demonstrate that the dynamics of arch shapes preceding failure can be modeled as a continuous time random walk (CTRW) with a broad distribution of waiting times, which breaks ergodicity. Treating arch failure as a first passage process of this random walk, we argue that the distribution of unclogging times is determined by this waiting time distribution. We hypothesize that this is a generic feature of unclogging, and that specific characteristics, such as hopper geometry, and mechanical properties of the grains modify the waiting time distribution. 	
1712.06934v1	http://arxiv.org/pdf/1712.06934v1	2017	Effect of NBTI/PBTI Aging and Process Variations on Write Failures in   MOSFET and FinFET Flip-Flops	Usman Khalid|Antonio Mastrandrea|Mauro Olivieri	  The assessment of noise margins and the related probability of failure in digital cells has growingly become essential, as nano-scale CMOS and FinFET technologies are confronting reliability issues caused by aging mechanisms, such as NBTI, and variability in process parameters. The influence of such phenomena is particularly associated to the Write Noise Margins (WNM) in memory elements, since a wrong stored logic value can result in an upset of the system state. In this work, we calculated and compared the effect of process variations and NBTI aging over the years on the actual WNM of various CMOS and FinFET based flip-flop cells. The massive transistor-level Monte Carlo simulations produced both nominal (i.e. mean) values and associated standard deviations of the WNM of the chosen flip-flops. This allowed calculating the consequent write failure probability as a function of an input voltage shift on the flip-flop cells, and assessing a comparison for robustness among different circuit topologies and technologies. 	
1008.3967v1	http://arxiv.org/pdf/1008.3967v1	2010	Statistical Physics of the Yielding Transition in Amorphous Solids	Smarajit Karmakar|Edan Lerner|Itamar Procaccia	  The art of making structural, polymeric and metallic glasses is rapidly developing with many applications. A limitation to their use is their mechanical stability: under increasing external strain all amorphous solids respond elastically to small strains but have a finite yield stress which cannot be exceeded without effecting a plastic response which typically leads to mechanical failure. Understanding this is crucial for assessing the risk of failure of glassy materials under mechanical loads. Here we show that the statistics of the energy barriers \Delta E that need to be surmounted changes from a probability distribution function (pdf) that goes smoothly to zero to a pdf which is finite at \Delta E=0. This fundamental change implies a dramatic transition in the mechanical stability properties with respect to external strain. We derive exact results for the scaling exponents that characterize the magnitudes of average energy and stress drops in plastic events as a function of system size. 	
1801.08557v1	http://arxiv.org/pdf/1801.08557v1	2018	Fracturing of topological Maxwell lattices	Leyou Zhang|Xiaoming Mao	  We present fracturing analysis of topological Maxwell lattices when they are stretched by applied stress. Maxwell lattices are mechanical structures containing equal numbers of degrees of freedom and constraints in the bulk and are thus on the verge of mechanical instability. Recent progress in topological mechanics led to the discovery of topologically protected floppy modes and states of self stress at edges and domain walls of Maxwell lattices. When normal brittle materials are being stretched, stress focuses on crack tips, leading to catastrophic failure. In contrast, we find that when topological Maxwell lattices are being stretched, stress focuses on states of self stress domain walls instead, and bond-breaking events start at these domain walls, even in presence of cracks. Remarkably, we find that the stress-focusing feature of the self-stress domain walls persists deep into the the failure process, when a lot of damages already occurred at these domain walls. We explain the results using topological mechanics theory and discuss the potential use of these topological Maxwell lattice structures as mechanical metamaterials that exhibit high strength against fracturing and well controlled fracturing process. 	
1410.1917v1	http://arxiv.org/pdf/1410.1917v1	2014	Tensile Fracture of Welded Polymer Interfaces: Miscibility,   Entanglements and Crazing	Ting Ge|Gary S. Grest|Mark O. Robbins	  Large-scale molecular simulations are performed to investigate tensile failure of polymer interfaces as a function of welding time $t$. Changes in the tensile stress, mode of failure and interfacial fracture energy $G_I$ are correlated to changes in the interfacial entanglements as determined from Primitive Path Analysis. Bulk polymers fail through craze formation, followed by craze breakdown through chain scission. At small $t$ welded interfaces are not strong enough to support craze formation and fail at small strains through chain pullout at the interface. Once chains have formed an average of about one entanglement across the interface, a stable craze is formed throughout the sample. The failure stress of the craze rises with welding time and the mode of craze breakdown changes from chain pullout to chain scission as the interface approaches bulk strength. The interfacial fracture energy $G_I$ is calculated by coupling the simulation results to a continuum fracture mechanics model. As in experiment, $G_I$ increases as $t^{1/2}$ before saturating at the average bulk fracture energy $G_b$. As in previous simulations of shear strength, saturation coincides with the recovery of the bulk entanglement density. Before saturation, $G_I$ is proportional to the areal density of interfacial entanglements. Immiscibiltiy limits interdiffusion and thus suppresses entanglements at the interface. Even small degrees of immisciblity reduce interfacial entanglements enough that failure occurs by chain pullout and $G_I \ll G_b$. 	
1509.04012v1	http://arxiv.org/pdf/1509.04012v1	2015	Mechanical Properties of Norway Spruce: Intra-Ring Variation and Generic   Behavior of Earlywood and Latewood until Failure	Christian Lanvermann|Philipp Hass|Falk K. Wittel|Peter Niemz	  The alternating earlywood and latewood growth ring structure has a strong influence on the mechanical performance of Norway spruce. In the current study, tensile tests in the longitudinal and tangential directions were performed on a series of specimens representing one growth ring at varying relative humidities. All tested mechanical parameters, namely modulus of elasticity and ultimate tensile stress, followed the density distribution in the growth ring, with the minimum values in earlywood and the maximum values in latewood. The samples were conditioned at three the relative humidities 50%, 65% and 95%. With increasing relative humidity, the values of the mechanical parameters were found to decrease. However, due to the high local variability, this decrease was not statistically significant. The test in the tangential direction on a set of earlywood and latewood specimens at 65% relative humidity revealed a similar limit of linear elasticity for both early- and latewood. Where the strength of both tissues was equal, the strain at failure was significantly greater for earlywood. Furthermore, the portion of the non-linear stress/strain behavior for earlywood was significantly greater. A Weibull analysis on the ultimate tensile strength revealed a tissue-independent Weibull modulus, which indicates similar defect distributions. For both, the failure occurred in the middle lamella. 	
0710.5462v1	http://arxiv.org/pdf/0710.5462v1	2007	Rock blocks	W. Turner	  Consider representation theory associated to symmetric groups, or to Hecke algebras in type A, or to q-Schur algebras, or to finite general linear groups in non-describing characteristic. Rock blocks are certain combinatorially defined blocks appearing in such a representation theory, first observed by R. Rouquier. Rock blocks are much more symmetric than general blocks, and every block is derived equivalent to a Rock block. Motivated by a theorem of J. Chuang and R. Kessar in the case of symmetric group blocks of abelian defect, we pursue a structure theorem for these blocks. 	
0808.2011v1	http://arxiv.org/pdf/0808.2011v1	2008	Finite element analysis of the indentation test on rocks with   microstructure	Jean Sulem|Miguel Cerrolaza	  Strength parameters of rocks are currently determined from indentation tests. In this paper, a finite element analysis of this test is presented and scale effect is studied. The rock is modelled as an elasto-plastic medium with Cosserat microstructure and consequently possesses an internal length. The response of the indentation curve is studied for various values of the size of the indentor as compared to the internal length of the rock in order to assess the scale effect. 	
1104.5171v1	http://arxiv.org/pdf/1104.5171v1	2011	Experimental Control of Transport and Current Reversals in a   Deterministic Optical Rocking Ratchet	Alejandro V. Arzola|Karen Volke-Sepúlveda|José L. Mateos	  We present an experimental demonstration of a deterministic optical rocking ratchet. A periodic and asymmetric light pattern is created to interact with dielectric microparticles in water, giving rise to a ratchet potential. The sample is moved with respect to the pattern with an unbiased time-periodic rocking function, which tilts the potential in alternating opposite directions. We obtain a current of particles whose direction can be controlled in real time and show that particles of different sizes may experience opposite currents. Moreover, we observed current reversals as a function of the magnitude and period of the rocking force. 	
1708.06343v1	http://arxiv.org/pdf/1708.06343v1	2017	Aerial Rock Fragmentation Analysis in Low-Light Condition Using UAV   Technology	Thomas Bamford|Kamran Esmaeili|Angela P. Schoellig	  In recent years, Unmanned Aerial Vehicle (UAV) technology has been introduced into the mining industry to conduct terrain surveying. This work investigates the application of UAVs with artificial lighting for measurement of rock fragmentation under poor lighting conditions, representing night shifts in surface mines or working conditions in underground mines. The study relies on indoor and outdoor experiments for rock fragmentation analysis using a quadrotor UAV. Comparison of the rock size distributions in both cases show that adequate artificial lighting enables similar accuracy to ideal lighting conditions. 	
1007.2180v1	http://arxiv.org/pdf/1007.2180v1	2010	A damage-mechanics model for fracture nucleation and propagation	G. Yakovlev|J. D. Gran|D. L. Turcotte|J. B. Rundle|W. Klein	  In this paper a composite model for earthquake rupture initiation and propagation is proposed. The model includes aspects of damage mechanics, fiber-bundle models, and slider-block models. An array of elements is introduced in analogy to the fibers of a fiber bundle. Time to failure for each element is specified from a Poisson distribution. The hazard rate is assumed to have a power-law dependence on stress. When an element fails it is removed, the stress on a failed element is redistributed uniformly to a specified number of neighboring elements in a given range of interaction. Damage is defined to be the fraction of elements that have failed. Time to failure and modes of rupture propagation are determined as a function of the hazard-rate exponent and the range of interaction. 	
1409.2040v2	http://arxiv.org/pdf/1409.2040v2	2014	Strain shielding from mechanically-activated covalent bond formation in   nanoindentation	Sandeep Kumar|David M. Parks	  Mechanical failure of an ideal crystal is dictated either by an elastic instability or a soft-mode instability. We show that the ideal strength measurement of graphene based on nano-indentation experiments \cite{lee2008measurement, lee2013high}, however, indicates an anomaly: the inferred strain beneath the diamond indenter at the failure load is anomalously large compared to the fracture strain predicted by soft-mode analysis or acoustic analysis. Here we present a systematic investigation - based on multi-scale modeling combining the results of continuum, atomistic, and quantum calculations; and analysis of experiments - that identifies the operative mechanism responsible for the anomalous difference between the fracture strains. We suggest that a strain-shielding effect due to mechanically-activated covalent bond formation at graphene-indenter interface is responsible for this anomaly. Using Finite Elements Analysis (FEA) and MD simulations of the nanoindentation experiments, we explicitly show that the bonded interaction at the graphene-indenter interface substantially disperses (shields) the strain beneath the indenter, preventing the intensification of strain. Our calculations indicate that the extent of strain shielding depends upon the hydrogen coverage at the indenter surface; and at optimal hydrogen coverage, the strain-shielding effect can delay the onset of fracture to the experimentally-observed indentation load and depth. 	
1703.09984v1	http://arxiv.org/pdf/1703.09984v1	2017	Mechanics of a granular skin	Somnath Karmakar|Anit Sane|S. Bhattacharya|Shankar Ghosh	  Magic Sand, a hydrophobic toy granular material, is widely used in popular science instructions because of its non-intuitive mechanical properties. A detailed study of the failure of an underwater column of magic sand shows that these properties can be traced to a single phenomenon: the system self-generates a cohesive skin that encapsulates the material inside. The skin, consists of pinned air-water-grain interfaces, shows multi-scale mechanical properties: they range from contact-line dynamics in the intra-grain roughness scale, plastic flow at the grain scale, all the way to the sample-scale mechanical responses. With decreasing rigidity of the skin, the failure mode transforms from brittle to ductile (both of which are collective in nature) to a complete disintegration at the single grain scale. 	
1712.03378v1	http://arxiv.org/pdf/1712.03378v1	2017	Mechanical Twinning in Phosphorene	V. Sorkin|Y. Q. Cai|D. J. Srolovitz|Y. W. Zhang	  We investigate the deformation and failure mechanisms of phosphorene sheet and nanoribbons under uniaxial tensile strain along the zigzag direction using the density functional tight-binding method. Surprisingly, twin-like deformation occurs homogenously across the phosphorene sheet, which significantly increases its failure strain. Vacancies within the sheet lead to the heterogeneous nucleation of twins at a lower critical strain which, subsequently, propagate across the entire sheet. Twin-like deformation always occurs heterogeneously in phosphorene nanoribbons (with or without vacancies). Propagation of the twins is interrupted by fracture which initiates along the ribbon edge. The underlying mechanism is bond breaking between the atoms within phosphorene puckers and simultaneous bond formation between the atoms in neighboring puckers. This unusual deformation behavior in phosphorene may be exploited in novel nano-electronic-mechanical applications. 	
1708.07055v3	http://arxiv.org/pdf/1708.07055v3	2017	Stochastic population dynamics in spatially extended predator-prey   systems	Ulrich Dobramysl|Mauro Mobilia|Michel Pleimling|Uwe C. Täuber	  Spatially extended population dynamics models that incorporate intrinsic noise serve as case studies for the role of fluctuations and correlations in biological systems. Including spatial structure and stochastic noise in predator-prey competition invalidates the deterministic Lotka-Volterra picture of neutral population cycles. Stochastic models yield long-lived erratic population oscillations stemming from a resonant amplification mechanism. In spatially extended predator-prey systems, one observes noise-stabilized activity and persistent correlations. Fluctuation-induced renormalizations of the oscillation parameters can be analyzed perturbatively. The critical dynamics and the non-equilibrium relaxation kinetics at the predator extinction threshold are characterized by the directed percolation universality class. Spatial or environmental variability results in more localized patches which enhances both species densities. Affixing variable rates to individual particles and allowing for trait inheritance subject to mutations induces fast evolutionary dynamics for the rate distributions. Stochastic spatial variants of cyclic competition with rock-paper-scissors interactions illustrate connections between population dynamics and evolutionary game theory, and demonstrate how space can help maintain diversity. In two dimensions, three-species cyclic competition models of the May-Leonard type are characterized by the emergence of spiral patterns whose properties are elucidated by a mapping onto a complex Ginzburg-Landau equation. Extensions to general food networks can be classified on the mean-field level, which provides both a fundamental understanding of ensuing cooperativity and emergence of alliances. Novel space-time patterns emerge as a result of the formation of competing alliances, such as coarsening domains that each incorporate rock-paper-scissors competition games. 	
0009448v1	http://arxiv.org/pdf/cond-mat/0009448v1	2000	Simulating Dynamical Features of Escape Panic	Dirk Helbing|Illes Farkas|Tamas Vicsek	  One of the most disastrous forms of collective human behaviour is the kind of crowd stampede induced by panic, often leading to fatalities as people are crushed or trampled. Sometimes this behaviour is triggered in life-threatening situations such as fires in crowded buildings; at other times, stampedes can arise from the rush for seats or seemingly without causes. Tragic examples within recent months include the panics in Harare, Zimbabwe, and at the Roskilde rock concert in Denmark. Although engineers are finding ways to alleviate the scale of such disasters, their frequency seems to be increasing with the number and size of mass events. Yet, systematic studies of panic behaviour, and quantitative theories capable of predicting such crowd dynamics, are rare. Here we show that simulations based on a model of pedestrian behaviour can provide valuable insights into the mechanisms of and preconditions for panic and jamming by incoordination. Our results suggest practical ways of minimising the harmful consequences of such events and the existence of an optimal escape strategy, corresponding to a suitable mixture of individualistic and collective behaviour. 	
0210013v1	http://arxiv.org/pdf/cond-mat/0210013v1	2002	The high-pressure alpha/beta phase transition in lead sulphide (PbS):   X-ray powder diffraction and quantum mechanical calculations	K. Knorr|L. Ehm|M. Hytha|B. Winkler|W. Depmeier	  The high-pressure behaviour of PbS was investigated by angular dispersive X-ray powder diffraction up to pressures of 6.8 GPa. Experiments were accompanied by first principles calculations at the density functional theory level. By combining both methods reliable data for the elastic properties of rock-salt type alpha- and high-pressure beta-PbS could be obtained. beta-PbS could be determined to crystallise in the CrB-type (B33), with space group Cmcm. The reversible ferro-elastic alpha/beta transition is of first order. It is accompanied by a large volume discontinuity of about 5% and a coexistence region of the two phases. A gliding mechanism of {001} bilayers along one of the cubic <110>-directions governs the phase transition which can be described in terms of group/subgroup relationships via a common subgroup, despite its reconstructive character. The quadrupling of the primitive unit cell indicates a wave vector (0,0,pi/a) on the Delta-line of the Brillouin zone. 	
0605042v3	http://arxiv.org/pdf/q-bio/0605042v3	2006	Coexistence versus extinction in the stochastic cyclic Lotka-Volterra   model	Tobias Reichenbach|Mauro Mobilia|Erwin Frey	  Cyclic dominance of species has been identified as a potential mechanism to maintain biodiversity, see e.g. B. Kerr, M. A. Riley, M. W. Feldman and B. J. M. Bohannan [Nature {\bf 418}, 171 (2002)] and B. Kirkup and M. A. Riley [Nature {\bf 428}, 412 (2004)]. Through analytical methods supported by numerical simulations, we address this issue by studying the properties of a paradigmatic non-spatial three-species stochastic system, namely the `rock-paper-scissors' or cyclic Lotka-Volterra model. While the deterministic approach (rate equations) predicts the coexistence of the species resulting in regular (yet neutrally stable) oscillations of the population densities, we demonstrate that fluctuations arising in the system with a \emph{finite number of agents} drastically alter this picture and are responsible for extinction: After long enough time, two of the three species die out. As main findings we provide analytic estimates and numerical computation of the extinction probability at a given time. We also discuss the implications of our results for a broad class of competing population systems. 	
1010.2575v1	http://arxiv.org/pdf/1010.2575v1	2010	Newly Disrupted Main Belt Asteroid P/2010 A2	David Jewitt|Harold Weaver|Jessica Agarwal|Max Mutchler|Michal Drahus	  Most main-belt asteroids are primitive rock and metal bodies in orbit about the Sun between Mars and Jupiter. Disruption, through high velocity collisions or rotational spin-up, is believed to be the primary mechanism for the production and destruction of small asteroids and a contributor to dust in the Sun's Zodiacal cloud, while analogous collisions around other stars feed dust to their debris disks. Unfortunately, direct evidence about the mechanism or rate of disruption is lacking, owing to the rarity of events. Here we present observations of P/2010 A2, a previously unknown inner-belt asteroid with a peculiar, comet-like morphology that is most likely the evolving remnant of a recent asteroidal disruption. High resolution Hubble Space Telescope observations reveal an approximately 120 meter diameter nucleus with an associated tail of millimeter-sized dust particles formed in February/March 2009, all evolving slowly under the action of solar radiation pressure. 	
1012.0020v1	http://arxiv.org/pdf/1012.0020v1	2010	Experimental artefacts in undrained triaxial testing	Siavash Ghabezloo|Jean Sulem	  For evaluation of the undrained thermo-poro-elastic properties of saturated porous materials in conventional triaxial cells, it is important to take into account the effect of the dead volume of the drainage system. The compressibility and the thermal expansion of the drainage system along with the dead volume of the fluid filling this system, influence the measured pore pressure and volumetric strain during undrained thermal or mechanical loading in a triaxial cell. A correction method is presented in this paper to correct these effects during an undrained isotropic compression test or an undrained heating test. A parametric study has demonstrated that the porosity and the drained compressibility of the tested material and the ratio of the vol-ume of the drainage system to the one of the tested sample are the key parameters which influence the most the error induced on the measurements by the drainage system. 	
1411.6092v1	http://arxiv.org/pdf/1411.6092v1	2014	Fluctuations of global energy release and crackling in nominally brittle   heterogeneous fracture	Jonathan Barés|Lamine Hattali|Davy Dalmas|Daniel Bonamy	  The temporal evolution of mechanical energy and spatially-averaged crack speed are both monitored in slowly fracturing artificial rocks. Both signals display an irregular burst-like dynamics, with power-law distributed fluctuations spanning a broad range of scales. Yet, the elastic power released at each time step is proportional to the global velocity all along the process, which enables defining a material-constant fracture energy. We characterize the intermittent dynamics by computing the burst statistics. This latter displays the scale-free features signature of crackling dynamics, in qualitative but not quantitative agreement with the depinning interface models derived for fracture problems. The possible sources of discrepancies are pointed out and discussed. 	
1611.01717v1	http://arxiv.org/pdf/1611.01717v1	2016	Duration and rapid shutdown of Mars lake-forming climates explained by   methane bursts	Edwin S. Kite|Colin Goldblatt|Peter Gao|David P. Mayer	  Build-up of relatively young ($<\sim$3.6 Ga) deltas and alluvial fans on Mars required lakes to persist for $>$3 Kyr (assuming dilute flow), but the watersheds' little-weathered soils indicate a climate history that was $>$99% dry. The lake-forming climates' trigger mechanism remains unknown. Here we show that these intermittency constraints, while inconsistent with many previously-proposed triggers for lake-forming climates, are consistent with a novel CH$_4$-burst mechanism. Chaotic transitions in mean obliquity drive latitudinal shifts in temperature and ice loading that destabilize CH$_4$ clathrate. For past clathrate hydrate stability zone occupancy fractions $>\sim$0.2, we show that CH$_4$($\pm$C$_2$H$_6$) builds up to levels whose radiative forcing ($>$15 W/m$^2$, plus feedbacks) is sufficient to modulate lake-forming climates. Such occupancy fractions are consistent with CH$_4$+C$_2$H$_6$ production by $>$3 Ga water-rock reactions. Sub-lake CH$_4$ destabilization provides positive feedback. UV-limited CH$_4$ photolysis curtails individual lake-forming climates to $<$10$^6$ yr duration, consistent with data. Our results show how a warmer early Mars can undergo intermittent excursions to a warm, wet climate state. 	
0609650v1	http://arxiv.org/pdf/cond-mat/0609650v1	2006	Statistical Models of Fracture	Mikko J. Alava|Phani K. V. V. Nukala|Stefano Zapperi	  Disorder and long-range interactions are two of the key components that make material failure an interesting playfield for the application of statistical mechanics. The cornerstone in this respect has been lattice models of the fracture in which a network of elastic beams, bonds or electrical fuses with random failure thresholds are subject to an increasing external load. These models describe on a qualitative level the failure processes of real, brittle or quasi-brittle materials. This has been particularly important in solving the classical engineering problems of material strength: the size dependence of maximum stress and its sample to sample statistical fluctuations. At the same time, lattice models pose many new fundamental questions in statistical physics, such as the relation between fracture and phase transitions. Experimental results point out to the existence of an intriguing crackling noise in the acoustic emission and of self-affine fractals in the crack surface morphology. Recent advances in computer power have enabled considerable progress in the understanding of such models. Among these partly still controversial issues, are the scaling and size effects in material strength and accumulated damage, the statistics of avalanches or bursts of microfailures, and the morphology of the crack surface. Here we present an overview of the results obtained with lattice models for fracture, highlighting the relations with statistical physics theories and more conventional fracture mechanics approaches. 	
0704.2925v1	http://arxiv.org/pdf/0704.2925v1	2007	Failure mechanisms and surface roughness statistics of fractured   Fontainebleau sandstone	Laurent Ponson|Harold Auradou|Marc Pessel|Véronique Lazarus|Jean-Pierre Hulin	  In an effort to investigate the link between failure mechanisms and the geometry of fractures of compacted grains materials, a detailed statistical analysis of the surfaces of fractured Fontainebleau sandstones has been achieved. The roughness of samples of different widths W is shown to be self affine with an exponent zeta=0.46 +- 0.05 over a range of length scales ranging from the grain size d up to an upper cut-off length \xi = 0.15 W. This low zeta value is in agreement with measurements on other sandstones and on sintered materials. The probability distributions P(delta z,delta h) of the variations of height over different distances delta z > d can be collapsed onto a single Gaussian distribution with a suitable normalisation and do not display multifractal features. The roughness amplitude, as characterized by the height-height correlation over fixed distances delta z, does not depend on the sample width, implying that no anomalous scaling of the type reported for other materials is present. It is suggested, in agreement with recent theoretical work, to explain these results by the occurence of brittle fracture (instead of damage failure in materials displaying a higher value of zeta = 0.8). 	
1202.2613v1	http://arxiv.org/pdf/1202.2613v1	2012	Molecular Simulation of Fracture Dynamics of Symmetric Tilt Grain   Boundaries in Graphene	Young In Jhon|Pil Seung Chung|Robert Smith|Myung S. Jhon	  Atomistic simulations were utilized to obtain microscopic information of the elongation process in graphene sheets consisting of various embedded symmetric tilt grain boundaries (GBs). In contrast to pristine graphene, these GBs fractured in an extraordinary pattern under transverse uniaxial elongation in all but the largest misorientation angle case, which exhibited intermittent crack propagation and formed many stringy residual connections after quasi mechanical failure. The strings known as monoatomic carbon chains (MACCs), whose importance was recently highlighted, gradually extended to a maximum of a few nanometers as the elongation proceeded. These features, which critically affect the tensile stress and the shape of stress-strain curve, were observed in both armchair and zigzag-oriented symmetric tilt GBs. However, there exist remarkable differences in the population density and the achievable length of MACCs appearing after quasi mechanical failure which were higher in the zigzag-oriented GBs. In addition, the maximum stress and ultimate strain for armchair-oriented GBs were significantly greater than those of zigzag-oriented GBs in case of the largest misorientation angle while they were slightly smaller in other cases. The maximum stress was larger as the misorientation angle increased for both armchair and zigzag-oriented GBs ranging between 32~80 GPa, and the ultimate strains were between 0.06~0.11, the lower limit of which agrees very well with the experimental value of threshold strain beyond which mechanical failure often occurred in polycrystalline graphene. 	
1604.03226v2	http://arxiv.org/pdf/1604.03226v2	2017	Fast Failure Recovery for Main-Memory DBMSs on Multicores	Yingjun Wu|Wentian Guo|Chee-Yong Chan|Kian-Lee Tan	  Main-memory database management systems (DBMS) can achieve excellent performance when processing massive volume of on-line transactions on modern multi-core machines. But existing durability schemes, namely, tuple-level and transaction-level logging-and-recovery mechanisms, either degrade the performance of transaction processing or slow down the process of failure recovery. In this paper, we show that, by exploiting application semantics, it is possible to achieve speedy failure recovery without introducing any costly logging overhead to the execution of concurrent transactions. We propose PACMAN, a parallel database recovery mechanism that is specifically designed for lightweight, coarse-grained transaction-level logging. PACMAN leverages a combination of static and dynamic analyses to parallelize the log recovery: at compile time, PACMAN decomposes stored procedures by carefully analyzing dependencies within and across programs; at recovery time, PACMAN exploits the availability of the runtime parameter values to attain an execution schedule with a high degree of parallelism. As such, recovery performance is remarkably increased. We evaluated PACMAN in a fully-fledged main-memory DBMS running on a 40-core machine. Compared to several state-of-the-art database recovery mechanisms, PACMAN can significantly reduce recovery time without compromising the efficiency of transaction processing. 	
1608.01193v1	http://arxiv.org/pdf/1608.01193v1	2016	Crazing of Nanocomposites with Polymer-Tethered Nanoparticles	Dong Meng|Sanat K. Kumar|Ting Ge|Mark O. Robbins|Gary S. Grest	  The crazing behavior of polymer nanocomposites formed by blending polymer grafted nanoparticles with an entangled polymer melt is studied by molecular dynamics simulations. We focus on the three key differences in the crazing behavior of a composite relative to the pure homopolymer matrix, namely, a lower yield stress, a smaller extension ratio and a grafted chain length dependent failure stress. The yield behavior is found to be mostly controlled by the local nanoparticle-grafted polymer interfacial energy, with the grafted polymer-polymer matrix interfacial structure being of little to no relevance. Increasing the attraction between nanoparticle core and the grafted polymer inhibits void nucleation and leads to a higher yield stress. In the craze growth regime, the presence of grafted chain sections of 100 monomers alters the mechanical response of composite samples, giving rise to smaller extension ratios and higher drawing stresses than for the homopolymer matrix. The dominant failure mechanism of composite samples depends strongly on the length of the grafted chains, with disentanglement being the dominant mechanism for short chains, while bond breaking is the failure mode for chain lengths greater than 10Ne, where Ne is the entanglement length. 	
0407710v1	http://arxiv.org/pdf/cond-mat/0407710v1	2004	Xenon NMR Measurements of Permeability and Tortuosity in Reservoir Rocks	R. Wang|T. Pavlin|M. S. Rosen|R. W. Mair|D. G. Cory|R. L. Walsworth	  In this work we present measurements of permeability, effective porosity and tortuosity on a variety of rock samples using NMR/MRI of thermal and laser-polarized gas. Permeability and effective porosity are measured simultaneously using MRI to monitor the inflow of laser-polarized xenon into the rock core. Tortuosity is determined from measurements of the time-dependent diffusion coefficient using thermal xenon in sealed samples. The initial results from a limited number of rocks indicate inverse correlations between tortuosity and both effective porosity and permeability. Further studies to widen the number of types of rocks studied may eventually aid in explaining the poorly understood connection between permeability and tortuosity of rock cores. 	
1109.1614v1	http://arxiv.org/pdf/1109.1614v1	2011	Are there rings around Pluto?	J. J. Rawal|Bijan Nikouravan	  Considering effects of tidal plus centrifugal stress acting on icy-rocks and the tensile strength thereof, icy-rocks being in the density range (1-2.4) g cm-3 which had come into existence as collisional ejecta (debris) in the vicinity of Pluto at the time when Pluto-Charon system came into being as a result of a giant impact of a Kuiper Belt Object on the primordial Pluto, it is shown, here, that these rocks going around Pluto in its vicinity are under slow disruption generating a stable ring structure consisting of icy-rocks of diameters in the range (20-90) km, together with fine dust and particles disrupted off the rocks, and spread all over the regions in their respective Roche Zones, various Roche radii being in ~1/2 three-body mean motion resonance. Calculations of gravitational spheres of influence of Pluto which turns out to be 4.2 x 106 km for prograde orbits and 8.5 x 106 km for retrograde orbits together with the existence of Kuiper Belt in the vicinity of Pluto assure that there may exist a few rocks (satellites)/dust rings/sheets so far undiscovered moving in prograde orbits around the planet and few others which are distant ones and move around Pluto in the region between 4.2x106 km and 8.5x106 km in retrograde orbits. 	
1503.08958v1	http://arxiv.org/pdf/1503.08958v1	2015	Strength of Fractured Rocks	Chandreyee Roy|Srutarshi Pradhan|Anna Stroisz|Erling Fjaer	  In this report we present a study on the strength of rocks which are partially fractured from before. We have considered a two dimensional case of a rock in the form of a lattice structure. The fiber bundle model is used for modelling the $2-D$ rock. Each lattice site is considered to be a fiber which has a breaking threshold. Fractures in this system will be of the form a cluster of sites and the length is defined as the number of sites belonging to a single cluster. We introduce fractures in the system initially and apply load until the rock breaks. The breaking of a rock is characterized by a horizontal fracture which connects the left side of the lattice to the right side. The length distribution and the strength of such systems have been measured. 	
1512.05184v1	http://arxiv.org/pdf/1512.05184v1	2015	Fracturing tests on reservoir rocks: Analysis of AE events and radial   strain evolution	S. Pradhan|A. M. Stroisz|E. Fjær|J. Stenebråten|H. K. Lund|E. F. Sønstebø|S. Roy	  Fracturing in reservoir rocks is an important issue for the petroleum industry - as productivity can be enhanced by a controlled fracturing operation. Fracturing also has a big impact on CO2 storage, geothermal installation and gas production at and from the reservoir rocks. Therefore, understanding the fracturing behavior of different types of reservoir rocks is a basic need for planning field operations towards these activities. In our study, the fracturing of rock sample is monitored by Acoustic Emission (AE) and post-experiment Computer Tomography (CT) scans. The fracturing experiments have been performed on hollow cylinder cores of different rocks - sandstones and chalks. Our analysis show that the amplitudes and energies of acoustic events clearly indicate initiation and propagation of the main fractures. The amplitudes of AE events follow an exponential distribution while the energies follow a power law distribution. Time-evolution of the radial strain measured in the fracturing-test will later be compared to model predictions of fracture size. 	
1802.06276v1	http://arxiv.org/pdf/1802.06276v1	2018	Interactive Estimation of the Fractal Properties of Carbonate Rocks	Adewale Amosu|Hamdi Mahmood|Paul Ofoche|Mohamed Imsalem	  Scale invariance of intrinsic patterns is an important concept in geology that can be observed in numerous geological objects and phenomena. These geological objects and phenomena are described as containing statistically selfsimilar patterns often modeled with fractal geometry. Fractal geometry has been used extensively to characterize pore space and fracture distribution of both carbonate and clastic rocks as well as the transport properties of porous media and fluid flow in reservoirs. The fractal properties are usually estimated from thin-section photomicrograph images or scanning electron microscope images. For complex rock such as carbonate rocks, automatic feature detection methods are often inaccurate. In addition, the rocks may be have been subjected to facies selective diagenesis which preferentially affect some of the rock fabric, thus increasing the difficulty in automatic detection of certain features. We present an interactive program, GeoBoxCount, for analyzing thin-section images and calculating the fractal dimension interactively. The program relies on the geologists insight in interpreting the features of interest; this significantly improves the accuracy of feature selection. The program provides two options for calculating the fractal dimension: the Hausdorff and the Minkowsi-Bouligand box-counting methods. 	
1301.0595v1	http://arxiv.org/pdf/1301.0595v1	2012	Mechanism Design with Execution Uncertainty	Ryan Porter|Amir Ronen|Yoav Shoham|Moshe Tennenholtz	  We introduce the notion of fault tolerant mechanism design, which extends the standard game theoretic framework of mechanism design to allow for uncertainty about execution. Specifically, we define the problem of task allocation in which the private information of the agents is not only their costs to attempt the tasks, but also their probabilities of failure. For several different instances of this setting we present technical results, including positive ones in the form of mechanisms that are incentive compatible, individually rational and efficient, and negative ones in the form of impossibility theorems. 	
0201016v1	http://arxiv.org/pdf/cs/0201016v1	2002	A computer scientist looks at game theory	Joseph Y. Halpern	  I consider issues in distributed computation that should be of relevance to game theory. In particular, I focus on (a) representing knowledge and uncertainty, (b) dealing with failures, and (c) specification of mechanisms. 	
0108113v2	http://arxiv.org/pdf/quant-ph/0108113v2	2002	The N-box paradox in orthodox quantum mechanics	Conall Boyle|Roger Schafir	  The prediction of the N-box paradox, that whichever box is opened will contain the record of the particle having passed through it, is traced to a failure to specify whether the other boxes are distinguishable or indistinguishable. These correspond to different ways of lifting the degeneracy of a certain measurement, and have incompatible consequences. 	
0709.0992v1	http://arxiv.org/pdf/0709.0992v1	2007	Graphene nano-ribbon under tension	Zhiping Xu	  The mechanical response of graphene nano-ribbon under tensile loading has been investigated using atomistic simulation. Lattice symmetry dependence of elastic properties are found, which fits prediction from Cauchy-Born rule well. Concurrent brittle and ductile behaviors are observed in the failure process at elastic limit, which dominates at low and high temperature respectively. In addition, the free edges of finite width ribbon help to activate bond-flip events and initialize ductile behavior. 	
0808.3272v1	http://arxiv.org/pdf/0808.3272v1	2008	Reply to Comment on "Failure of the work-Hamiltonian connection for   free-energy calculations" by Luca Peliti	J. M. G. Vilar|J. M. Rubi	  We show that Peliti's Comment [arXiv:0808.2855] fails to appreciate basic physical principles and consequently misrepresents both our work as well as the classical work of Gibbs and Tolman. 	
1210.2232v1	http://arxiv.org/pdf/1210.2232v1	2012	Application of hyperbolic scaling for calculation of   reaction-subdiffusion front propagation	A. Iomin|I. M. Sokolov	  A technique of hyperbolic scaling is applied to calculate a reaction front velocity in an irreversible autocatalytic conversion reaction $A+B\,\rightarrow\, 2A$ under subdiffusion. The method, based on the geometric optics approach is a technically elegant observation of the propagation front failure obtained in Phys. Rev. E {\bf 78}, 011128 (2008). 	
1502.06710v1	http://arxiv.org/pdf/1502.06710v1	2015	Actin polymerization front propagation in a comb-reaction system	A. Iomin|V. Zaburdaev|T. Pfohl	  Anomalous transport and reaction dynamics are considered by providing the theoretical grounds for the possible experimental realization of actin polymerization in comb-like geometry. Two limiting regimes are recovered, depending on the concentration of reagents (magnesium and actin). These are both the failure of the reaction front propagation and a finite speed corresponding to the Fisher-KPP long time asymptotic regime. 	
1504.06460v1	http://arxiv.org/pdf/1504.06460v1	2015	Epistemic nature of quantum reasoning	Alfredo B. Henriques|Amílcar Sernadas	  Doubts are raised concerning the usual interpretation of the alleged failure, by quantum mechanics, of the distributive law of classical logic. The difficulty raised by incompatible sets of observables is overcome within an epistemic enrichment of classical logic that provides the means for distinguishing between the value of a variable and its observation while retaining the classical connectives. 	
0907.5325v2	http://arxiv.org/pdf/0907.5325v2	2010	Systemic Risk in a Unifying Framework for Cascading Processes on   Networks	Jan Lorenz|Stefano Battiston|Frank Schweitzer	  We introduce a general framework for models of cascade and contagion processes on networks, to identify their commonalities and differences. In particular, models of social and financial cascades, as well as the fiber bundle model, the voter model, and models of epidemic spreading are recovered as special cases. To unify their description, we define the net fragility of a node, which is the difference between its fragility and the threshold that determines its failure. Nodes fail if their net fragility grows above zero and their failure increases the fragility of neighbouring nodes, thus possibly triggering a cascade. In this framework, we identify three classes depending on the way the fragility of a node is increased by the failure of a neighbour. At the microscopic level, we illustrate with specific examples how the failure spreading pattern varies with the node triggering the cascade, depending on its position in the network and its degree. At the macroscopic level, systemic risk is measured as the final fraction of failed nodes, $X^\ast$, and for each of the three classes we derive a recursive equation to compute its value. The phase diagram of $X^\ast$ as a function of the initial conditions, thus allows for a prediction of the systemic risk as well as a comparison of the three different model classes. We could identify which model class lead to a first-order phase transition in systemic risk, i.e. situations where small changes in the initial conditions may lead to a global failure. Eventually, we generalize our framework to encompass stochastic contagion models. This indicates the potential for further generalizations. 	
1802.08887v1	http://arxiv.org/pdf/1802.08887v1	2018	Water from Two Rocks: Maximizing the Mutual Information	Yuqing Kong|Grant Schoenebeck	  Our goal is to forecast ground truth $Y$ using two sources of information $X_A,X_B$, without access to any data labeled with ground truth. That is, we are aiming to learn two predictors/hypotheses $P_A^*,P_B^*$ such that $P_A^*(X_A)$ and $P_B^*(X_B)$ provide high quality forecasts for ground truth $Y$, without labeled data. We also want to elicit a high quality forecast for $Y$ from the crowds and pay the crowds immediately, without access to $Y$. We build a natural connection between the learning question and the mechanism design question and deal with them using the same information theoretic approach.   Learning: With a natural assumption---conditioning on $Y$, $X_A$ and $X_B$ are independent, we reduce the learning question to an optimization problem $\max_{P_A,P_B}MIG^f(P_A,P_B)$ such that solving the learning question is equivalent to picking the $P_A^*,P_B^*$ that maximize $MIG^f(P_A,P_B)$---the \emph{$f$-mutual information gain} between $P_A$ and $P_B$.   Moreover, we apply our results to the "learning with noisy labels" problem to learn a predictor that forecasts the ground truth label rather than the noisy label with some side information, without pre-estimating the relationship between the ground truth labels and noisy labels.   Mechanism design: We design mechanisms that elicit high quality forecasts without verification and have instant rewards for agents by assuming the agents' information is independent conditioning on $Y$. In the single-task setting, we propose a forecast elicitation mechanism where truth-telling is a strict equilibrium, in the multi-task setting, we propose a family of forecast elicitation mechanisms where truth-telling is a strict equilibrium and pays better than any other equilibrium. 	
0905.4851v1	http://arxiv.org/pdf/0905.4851v1	2009	Craters Formed in Granular Beds by Impinging Jets of Gas	Philip T. Metzger|Robert C. Latta III|Jason M. Schuler|Christopher D. Immer	  When a jet of gas impinges vertically on a granular bed and forms a crater, the grains may be moved by several different mechanisms: viscous erosion, diffused gas eruption, bearing capacity failure, and/or diffusion-driven shearing. The relative importance of these mechanisms depends upon the flow regime of the gas, the mechanical state of the granular material, and other physical parameters. Here we report research in two specific regimes: viscous erosion forming scour holes as a function of particle size and gravity; and bearing capacity failure forming deep transient craters as a function of soil compaction. 	
0909.3174v1	http://arxiv.org/pdf/0909.3174v1	2009	Fiber bundle model with stick-slip dynamics	Zoltan Halasz|Ferenc Kun	  We propose a generic model to describe the mechanical response and failure of systems which undergo a series of stick-slip events when subjected to an external load. We model the system as a bundle of fibers, where single fibers can gradually increase their relaxed length with a stick-slip mechanism activated by the increasing load. We determine the constitutive equation of the system and show by analytical calculations that on the macro-scale a plastic response emerges followed by a hardening or softening regime. Releasing the load, an irreversible permanent deformation occurs which depends on the properties of sliding events. For quenched and annealed disorder of the failure thresholds the same qualitative behavior is found, however, in the annealed case the plastic regime is more pronounced. 	
1006.3770v1	http://arxiv.org/pdf/1006.3770v1	2010	Modeling Vacuum Arcs	Z. Insepov|J. Norem|T. Proslier|D. Huang|S. Mahalingam|S. Veitzer	  We are developing a model of vacuum arcs. This model assumes that arcs develop as a result of mechanical failure of the surface due to Coulomb explosions, followed by ionization of fragments by field emission and the development of a small, dense plasma that interacts with the surface primarily through self sputtering and terminates as a unipolar arc capable of producing breakdown sites with high enhancement factors. We have attempted to produce a self consistent picture of triggering, arc evolution and surface damage. We are modeling these mechanisms using Molecular Dynamics (mechanical failure, Coulomb explosions, self sputtering), Particle-In-Cell (PIC) codes (plasma evolution), mesoscale surface thermodynamics (surface evolution), and finite element electrostatic modeling (field enhancements). We can present a variety of numerical results. We identify where our model differs from other descriptions of this phenomenon. 	
1007.4985v1	http://arxiv.org/pdf/1007.4985v1	2010	Anomalous Strength Characteristics of Tilt Grain Boundaries in Graphene	Rassin Grantab|Vivek B. Shenoy|Rodney S. Ruoff	  Using molecular dynamics simulations and first principles calculations, we have studied the structure and mechanical strength of tilt grain boundaries in graphene sheets that arise during CVD growth of graphene on metal substrates. Surprisingly, we find that for tilt boundaries in the vicinity of both the zig-zag and arm-chair orientations, large angle boundaries with a higher density of 5-7 defect pairs are stronger than the low-angle boundaries which are comprised of fewer defects per unit length. Interestingly, the trends in our results cannot be explained by a continuum Griffith-type fracture mechanics criterion, which predicts the opposite trend due to that fact that it does not account for the critical bonds that are responsible for the failure mechanism. We have identified the highly-strained bonds in the 7-member rings that lead to the failure of the sheets, and we have found that large angle boundaries are able to better accommodate the strained 7-rings. Our results provide guidelines for designing growth methods to obtain grain boundary structures that can have strengths close to that of pristine graphene. 	
1106.0837v2	http://arxiv.org/pdf/1106.0837v2	2011	Mechanochemical reaction in graphane under uniaxial tension	N. A. Popova|E. F. Sheka	  The quantum-mechanochemical-reaction-coordinate simulations have been performed to investigate the mechanical properties of hydrogen functionalized graphene. The simulations disclosed atomically matched peculiarities that accompany the deformation-failure-rupture process occurred in the body. A comparative study of the deformation peculiarities related to equi-carbon-core (5,5) nanographene and nanographane sheets exhibited a high stiffness of both bodies that is provided by the related hexagon units, namely benzenoid and cyclohexanoid, respectively. The two units are characterized by anisotropy in the microscopic behavior under elongation along mechanochemical internal coordinates when the later are oriented either along (zg) or normally (ach) to the C-C bonds chain. The unit feature in combination with different configuration of their packing with respect to the body C-C bond chains forms the ground for the structure-sensitive mechanical behavior that is different for zg and ach deformation modes. Hydrogenation of graphene drastically influences behavior and numerical characteristics of the body making tricotage-like pattern of the graphene failure less pronounced and inverting it from the zg to ach mode as well as providing less mechanical resistance of graphane it total. 	
1107.1027v1	http://arxiv.org/pdf/1107.1027v1	2011	Failure of Mineralized Collagen Microfibrils Using Finite Element   Simulation Coupled to Mechanical Quasi-brittle Damage	Abdelwahed Barkaoui|Awad Bettamer|Ridha Hambli	  Bone is a multiscale heterogeneous materiel of which principal function is to support the body structure and to resist mechanical loading and fractures. Bone strength does not depend only on the quantity and quality of bone which is characterized by the geometry and the shape of bones but also on the mechanical proprieties of its compounds, which have a significant influence on its deformation and failure. This work aim to use a 3D nano-scale finite element model coupled to the concept of quasi-brittle damage with the behaviour law isotropic elasticity to investigate the fracture behaviour of composite materiel collagen-mineral (mineralized collagen microfibril). Fracture stress-number of cross-links and damping capacity-number of cross-links curves were obtained under tensile loading conditions at different densities of the mineral phase. The obtained results show that number of cross-links as well as the density of mineral has an important influence on the strength of microfibrils which in turn clarify the bone fracture at macro-scale. 	
1405.7924v2	http://arxiv.org/pdf/1405.7924v2	2014	Combining mechanical and chemical effects in the deformation and failure   of a cylindrical electrode particle in a Li-ion battery	Jeevanjyoti Chakraborty|Colin P. Please|Alain Goriely|S. Jonathan Chapman	  A general framework to study the mechanical behaviour of a cylindrical silicon anode particle in a lithium ion battery as it undergoes lithiation is presented. The two-way coupling between stress and concentration of lithium in silicon, including the possibility of plastic deformation, is taken into account and two particular cases are considered. First, the cylindrical particle is assumed to be free of surface traction and second, the axial deformation of the cylinder is prevented. In both cases plastic stretches develop through the entire cylinder and not just near the surface as is commonly found in spherical anode particles. It is shown that the stress evolution depends both on the lithiation rate and the external constraints. Furthermore, as the cylinder expands during lithiation it can develop a compressive axial stress large enough to induce buckling, which in turn may lead to mechanical failure. An explicit criterion for swelling-induced buckling obtained as a modification of the classical Euler buckling criterion shows the competition between the stabilising effect of radius increase and the destabilising effect of axial stress. 	
1602.08473v1	http://arxiv.org/pdf/1602.08473v1	2016	Modeling, Minimizing and Managing the Risk of Fatigue for Mechanical   Components	L. Bittner|H. Gottschalk|M. Gröger|N. Moch|M. Saadi|S. Schmitz	  Mechanical components that are exposed to cyclic mechanical loading fail at loads that are well below the ultimate tensile strength. This process is known as fatigue. The failure time, that is the time when a first crack forms, is highly random. In this work we review some recent developments in the modelling of probabilistic failure times, understood as the time to the formation of a fatigue crack.   We also discuss the how probabilistic models can be used in shape design with the design intent of optimizing the component's reliability. We give review a recent existence result for optimal shapes and we discuss continuous and discrete shape derivatives. Another application is optimal service scheduling. The mathematical fields involved range from reliability statistics over stochastic point processes, multiscale modeling, PDEs on variable geometries, shape optimization and numerical analysis to operations research. 	
1611.00345v2	http://arxiv.org/pdf/1611.00345v2	2016	Fracture of a model cohesive granular material	Alexander Schmeink|Lucas Goehring|Arnaud Hemmerle	  We study experimentally the fracture mechanisms of a model cohesive granular medium consisting of glass beads held together by solidified polymer bridges. The elastic response of this material can be controlled by changing the cross-linking of the polymer phase, for example. Here we show that its fracture toughness can be tuned over an order of magnitude by adjusting the stiffness and size of the polymer bridges. We extract a well-defined fracture energy from fracture testing under a range of material preparations. This energy is found to scale linearly with the cross-sectional area of the bridges. Finally, X-ray microcomputed tomography shows that crack propagation is driven by adhesive failure of about one polymer bridge per bead located at the interface, along with microcracks in the vicinity of the failure plane. Our findings provide insight to the fracture mechanisms of this model material, and the mechanical properties of disordered cohesive granular media in general. 	
0910.5687v2	http://arxiv.org/pdf/0910.5687v2	2010	Dynamics stabilization and transport coherency in a rocking ratchet for   cold atoms	A. B. Kolton|F. Renzoni	  Cold atoms in optical lattices have emerged as an ideal system to investigate the ratchet effect, as demonstrated by several recent experiments. In this work we analyze theoretically two aspects of ac driven transport in cold atoms ratchets. We first address the issue of whether, and to which extent, an ac driven ratchet for cold atoms can operate as a motor. We thus study theoretically a dissipative motor for cold atoms, as obtained by adding a load to a 1D non-adiabatically driven rocking ratchet. We demonstrate that a current can be generated also in the presence of a load, e.g. the ratchet device can operate as a motor. Correspondingly, we determine the stall force for the motor, which characterizes the range of loads over which the device can operate as a motor, and the differential mobility, which characterizes the response to a change in the magnitude of the load. Second, we compare our results for the transport in an ac driven ratchet device with the transport in a dc driven system. We observe a peculiar phenomenon: the bi-harmonic ac force stabilizes the dynamics, allowing the generation of uniform directed motion over a range of momentum much larger than what is possible with a dc bias. We explain such a stabilization of the dynamics by observing that a non-adiabatic ac drive broadens the effective cooling momentum range, and forces the atom trajectories to cover such a region. Thus the system can dissipate energy and maintain a steady-state energy balance. Our results show that in the case of a finite-range velocity-dependent friction, a ratchet device may offer the possibility of controlling the particle motion over a broader range of momentum with respect to a purely biased system, although this is at the cost of a reduced coherency. 	
1107.3499v1	http://arxiv.org/pdf/1107.3499v1	2011	Applying Advanced Spaceborne Thermal Emission and Reflection Radiometer   (ASTER) spectral indices for geological mapping and mineral identification on   the Tibetan Plateau	Robert Corrie|Yoshiki Ninomiya|Jonathan Aitchison	  The Tibetan Plateau holds clues to understanding the dynamics and mechanisms associated with continental growth. Part of the region is characterized by zones of ophiolitic melange believed to represent the remnants of ancient oceanic crust and underlying upper mantle emplaced during oceanic closures. However, due to the remoteness of the region and the inhospitable terrain many areas have not received detailed investigation. Increased spatial and spectral resolution of satellite sensors have made it possible to map in greater detail the mineralogy and lithology than in the past. Recent work by Yoshiki Ninomiya of the Geological Survey of Japan has pioneered the use of several spectral indices for the mapping of quartzose, carbonate, and silicate rocks using Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER) thermal infrared (TIR) data. In this study, ASTER TIR indices have been applied to a region in western-central Tibet for the purposes of assessing their effectiveness for differentiating ophiolites and other lithologies. The results agree well with existing geological maps and other published data. The study area was chosen due to its diverse range of rock types, including an ophiolitic melange, associated with the Bangong-Nujiang suture (BNS) that crops out on the northern shores of Lagkor Tso and Dong Tso ("Tso" is Tibetan for lake). The techniques highlighted in this paper could be applied to other geographical regions where similar geological questions need to be resolved. The results of this study aim to show the utility of ASTER TIR imagery for geological mapping in semi-arid and sparsely vegetated areas on the Tibetan Plateau. 	
1712.00460v2	http://arxiv.org/pdf/1712.00460v2	2017	PorePy: An Open-Source Simulation Tool for Flow and Transport in   Deformable Fractured Rocks	Eirik Keilegavlen|Alessio Fumagalli|Runar Berge|Ivar Stefansson|Inga Berre	  Fractures are ubiquitous in the subsurface and strongly affect flow and deformation. The physical shape of the fractures, they are long and thin objects, puts strong limitations on how the effect of this dynamics can be incorporated into standard reservoir simulation tools. This paper reports the development of an open-source software framework, termed PorePy, which is aimed at simulation of flow and transport in three-dimensional fractured reservoirs, as well as deformation of the reservoir due to shearing along fracture and fault planes. Starting from a description of fractures as polygons embedded in a 3D domain, PorePy provides semi-automatic gridding to construct a discrete-fracture-matrix model, which forms the basis for subsequent simulations. PorePy allows for flow and transport in all lower-dimensional objects, including planes (2D) representing fractures, and lines (1D) and points (0D), representing fracture intersections. Interaction between processes in neighboring domains of different dimension is implemented as a sequence of couplings of objects one dimension apart. This readily allows for handling of complex fracture geometries compared to capabilities of existing software. In addition to flow and transport, PorePy provides models for rock mechanics, poro-elasticity and coupling with fracture deformation models. The software is fully open, and can serve as a framework for transparency and reproducibility of simulations. We describe the design principles of PorePy from a user perspective, with focus on possibilities within gridding, covered physical processes and available discretizations. The power of the framework is illustrated with two sets of simulations; involving respectively coupled flow and transport in a fractured porous medium, and low-pressure stimulation of a geothermal reservoir. 	
1304.5402v1	http://arxiv.org/pdf/1304.5402v1	2013	Context-Independent Centrality Measures Underestimate the Vulnerability   of Power Grids	Trivik Verma|Wendy Ellens|Robert E. Kooij	  Power grids vulnerability is a key issue in society. A component failure may trigger cascades of failures across the grid and lead to a large blackout. Complex network approaches have shown a direction to study some of the problems faced by power grids. Within Complex Network Analysis structural vulnerabilities of power grids have been studied mostly using purely topological approaches, which assumes that flow of power is dictated by shortest paths. However, this fails to capture the real flow characteristics of power grids. We have proposed a flow redistribution mechanism that closely mimics the flow in power grids using the PTDF. With this mechanism we enhance existing cascading failure models to study the vulnerability of power grids.   We apply the model to the European high-voltage grid to carry out a comparative study for a number of centrality measures. `Centrality' gives an indication of the criticality of network components. Our model offers a way to find those centrality measures that give the best indication of node vulnerability in the context of power grids, by considering not only the network topology but also the power flowing through the network. In addition, we use the model to determine the spare capacity that is needed to make the grid robust to targeted attacks. We also show a brief comparison of the end results with other power grid systems to generalise the result. 	
0006244v1	http://arxiv.org/pdf/cond-mat/0006244v1	2000	Motion in a rocked ratchet with spatially periodic friction	Debasis Dan|Mangal C. Mahato|A. M. Jayannavar	  We present a detailed study of the transport and energetics of a Brownian particle moving in a periodic potential in the presence of an adiabatic external periodic drive. The particle is considered to move in a medium with periodic space dependent friction with the same periodicity as that of the potential but with a phase lag. We obtain several results, most of them arising due to the medium being inhomogeneous and are sensitive to the phase lag. When the potential is symmetric we show that efficiency of energy transduction can be maximised as a function of noise strength or temperature. However, in the case of asymmtertic potential the temperature may or may not facilitate the energy conversion but current reversals can be obtained as a function of temperature and the amplitude of the periodic drive. The reentrant behaviour of current can also be seen as a function of phase lag. 	
0009466v1	http://arxiv.org/pdf/cond-mat/0009466v1	2000	Irreversible and reversible modes of operation of deterministic ratchets	I. M. Sokolov	  We discuss a problem of optimization of the energetic efficiency of a simple rocked ratchet. We concentrate on a low-temperature case in which the particle's motion in a ratchet potential is deterministic. We show that the energetic efficiency of a ratchet working adiabatically is bounded from above by a value depending on the form of ratchet potential. The ratchets with strongly asymmetric potentials can achieve ideal efficiency of unity without approaching reversibility. On the other hand we show that for any form of the ratchet potential a set of time-protocols of the outer force exist under which the operation is reversible and the ideal value of efficiency is also achieved. The mode of operation of the ratchet is still quasistatic but not adiabatic. The high values of efficiency can be preserved even under elevated temperatures. 	
0107079v1	http://arxiv.org/pdf/cond-mat/0107079v1	2001	Inhomogeneous Systems and their Rectification Properties	A. M. Jayannavar	  We explore the possibility of obtaining unidirectional current in a symmetric (periodic) potential system without the application of any obvious (apparent) externally applied bias. There are many physical models proposed to accomplish this nonequilibrium effect. In the present work we consider inhomogeneous systems so that the friction coefficient and/or temperature could vary in space. We find out a model with minimal conditions that the inhomogeneous system assisted by fluctuating forces must satisfy, in order to obtain unidirectional current. In the process we discuss about thermal and frictional ratchets that are of current interest. We argue that different models of frictional ratchets work under the same basic principle of alteration of relative stability of otherwise locally stable states in the presence of temperature inhomogeneity. We also discuss in detail the nature of currents in rocked frictional ratchets. In particular we analyse a novel phenomenon of multiple current reversals and the efficiency of the energy transduction in these systems. 	
0207569v1	http://arxiv.org/pdf/cond-mat/0207569v1	2002	Phase transitions and volunteering in spatial public goods games	Gyorgy Szabo|Christoph Hauert	  Cooperative behavior among unrelated individuals in human and animal societies represents a most intriguing puzzle to scientists in various disciplines. Here we present a simple yet effective mechanism promoting cooperation under full anonymity by allowing for voluntary participation in public goods games. This natural extension leads to rock--scissors--paper type cyclic dominance of the three strategies cooperate, defect and loner i.e. those unwilling to participate in the public enterprise. In spatial settings with players arranged on a regular lattice this results in interesting dynamical properties and intriguing spatio-temporal patterns. In particular, variations of the value of the public good leads to transitions between one-, two- and three-strategy states which are either in the class of directed percolation or show interesting analogies to Ising-type models. Although volunteering is incapable of stabilizing cooperation, it efficiently prevents successful spreading of selfish behavior and enables cooperators to persist at substantial levels. 	
0208478v1	http://arxiv.org/pdf/cond-mat/0208478v1	2002	Evolutionary prisoner's dilemma games with optional participation	Gyorgy Szabo|Christoph Hauert	  Competition among cooperators, defectors, and loners is studied in an evolutionary prisoner's dilemma game with optional participation. Loners are risk averse i.e. unwilling to participate and rather rely on small but fixed earnings. This results in a rock-scissors-paper type cyclic dominance of the three strategies. The players are located either on square lattices or random regular graphs with the same connectivity. Occasionally, every player reassesses its strategy by sampling the payoffs in its neighborhood. The loner strategy efficiently prevents successful spreading of selfish, defective behavior and avoids deadlocks in states of mutual defection. On square lattices, Monte Carlo simulations reveal self-organizing patterns driven by the cyclic dominance, whereas on random regular graphs different types of oscillatory behavior are observed: the temptation to defect determines whether damped, periodic or increasing oscillations occur. These results are compared to predictions by pair approximation. Although pair approximation is incapable of distinguishing the two scenarios because of the equal connectivity, the average frequencies as well as the oscillations on random regular graphs are well reproduced. 	
0309617v3	http://arxiv.org/pdf/cond-mat/0309617v3	2003	Transport Coherence in Frictional Ratchets	Raishma Krishnan|Debasis Dan|A. M. Jayannavar	  We study the phenomena of noise induced transport in frictional ratchet systems. For this we consider a Brownian particle moving in a space dependent frictional medium in the presence of external white noise fluctuations. To get the directed transport, unlike in other ratchet models like flashing or rocking ratchets etc., we do not require the potential experienced by the particle to be asymmetric nor do we require the external fluctuations to be correlated. We have obtained analytical expressions for current and the diffusion coefficient. We show that the frictional ratchet do not exhibit a pronounced coherence in the transport in that the diffusion spread overshadows the accompanying directed transport in system with finite spatial extensions. 	
0501050v1	http://arxiv.org/pdf/cond-mat/0501050v1	2005	The roughness of stylolites: Implications of 3D high resolution   topography measurements	Jean Schmittbuhl|Francois Renard|Jean-Pierre Gratier|Renaud Toussaint	  Stylolites are natural pressure-dissolution surfaces in sedimentary rocks. We present 3D high resolution measurements at laboratory scales of their complex roughness. The topography is shown to be described by a self-affine scaling invariance. At large scales, the Hurst exponent is $\zeta_1 \approx 0.5$ and very different from that at small scales where $\zeta_2 \approx 1.2$. A cross-over length scale at around $\L_c =1$~mm is well characterized. Measurements are consistent with a Langevin equation that describes the growth of a stylolitic interface as a competition between stabilizing long range elastic interactions at large scales or local surface tension effects at small scales and a destabilizing quenched material disorder. 	
0504630v1	http://arxiv.org/pdf/cond-mat/0504630v1	2005	Localization of elastic waves in heterogeneous media with off-diagonal   disorder and long-range correlations	F. Shahbazi|Alireza Bahraminasab|S. Mehdi Vaez Allaei|Muhammad Sahimi|M. Reza Rahimi Tabar	  Using the Martin-Siggia-Rose method, we study propagation of acoustic waves in strongly heterogeneous media which are characterized by a broad distribution of the elastic constants. Gaussian-white distributed elastic constants, as well as those with long-range correlations with non-decaying power-law correlation functions, are considered. The study is motivated in part by a recent discovery that the elastic moduli of rock at large length scales may be characterized by long-range power-law correlation functions. Depending on the disorder, the renormalization group (RG) flows exhibit a transition to localized regime in {\it any} dimension. We have numerically checked the RG results using the transfer-matrix method and direct numerical simulations for one- and two-dimensional systems, respectively. 	
0511519v3	http://arxiv.org/pdf/cond-mat/0511519v3	2006	Giant coherence in driven systems	Soumen Roy|Debasis Dan|A. M. Jayannavar	  We study the noise-induced currents and reliability or coherence of transport in two different classes of rocking ratchets. For this, we consider the motion of Brownian particles in the over damped limit in both adiabatic and non-adiabatic regimes subjected to unbiased temporally symmetric and asymmetric periodic driving force. In the case of a time symmetric driving, we find that even in the presence of a spatially symmetric simple sinusoidal potential, highly coherent transport occurs. These ratchet systems exhibit giant coherence of transport in the regime of parameter space where unidirectional currents in the deterministic case are observed. Outside this parameter range, i.e., when current vanishes in the deterministic regime, coherence in transport is very low. The transport coherence decreases as a function of temperature and is a non-monotonic function of the amplitude of driving. The transport becomes unreliable as we go from the adiabatic to the non-adiabatic domain of operation. 	
0607344v3	http://arxiv.org/pdf/cond-mat/0607344v3	2007	Evolutionary games on graphs	Gyorgy Szabo|Gabor Fath	  Game theory is one of the key paradigms behind many scientific disciplines from biology to behavioral sciences to economics. In its evolutionary form and especially when the interacting agents are linked in a specific social network the underlying solution concepts and methods are very similar to those applied in non-equilibrium statistical physics. This review gives a tutorial-type overview of the field for physicists. The first three sections introduce the necessary background in classical and evolutionary game theory from the basic definitions to the most important results. The fourth section surveys the topological complications implied by non-mean-field-type social network structures in general. The last three sections discuss in detail the dynamic behavior of three prominent classes of models: the Prisoner's Dilemma, the Rock-Scissors-Paper game, and Competing Associations. The major theme of the review is in what sense and how the graph structure of interactions can modify and enrich the picture of long term behavioral patterns emerging in evolutionary games. 	
0612090v1	http://arxiv.org/pdf/cond-mat/0612090v1	2006	Intermediate dynamics between Newton and Langevin	Jing-Dong Bao|Yi-Zhong Zhuo|Fernando A. Oliveira|Peter Hanggi	  A dynamics between Newton and Langevin formalisms is elucidated within the framework of the generalized Langevin equation. For thermal noise yielding a vanishing zero-frequency friction the corresponding non-Markovian Brownian dynamics exhibits anomalous behavior which is characterized by ballistic diffusion and accelerated transport. We also investigate the role of a possible initial correlation between the system degrees of freedom and the heat-bath degrees of freedom for the asymptotic long-time behavior of the system dynamics. As two test beds we investigate (i) the anomalous energy relaxation of free non-Markovian Brownian motion that is driven by a harmonic velocity noise and (ii) the phenomenon of a net directed acceleration in noise-induced transport of an inertial rocking Brownian motor. 	
0612390v1	http://arxiv.org/pdf/cond-mat/0612390v1	2006	Rocking ratchets in 2D Josephson networks: collective effects and   current reversal	Verónica I. Marconi	  A detailed numerical study on the directed motion of ac-driven vortices and antivortices in 2D Josephson junction arrays (JJA) with an asymmetric periodic pinning potential is reported. Dc-voltage rectification shows a strong dependence on vortex density as well as an inversion of the vortex flow direction with ac amplitude for a wide range of vortex density around $f$=1/2 ($f$=$Ha^2 / \Phi_0$), in good agreement with recent experiments by Shal\'om and Pastoriza [Phys. Rev. Lett. {\bf 94}, 177001 (2005)]. The study of vortex structures, spatial and temporal correlations, and vortex-antivortex pairs formation gives insight into a purely collective mechanism behind the current reversal effect. 	
0702668v1	http://arxiv.org/pdf/cond-mat/0702668v1	2007	Dissipationless Directed Transport in Rocked Single-Band Quantum   Dynamics	Jiangbin Gong|Dario Poletti|Peter Hanggi	  Using matter waves that are trapped in a deep optical lattice, dissipationless directed transport is demonstrated to occur if the single-band quantum dynamics is periodically tilted on one half of the lattice by a monochromatic field. Most importantly, the directed transport can exist for almost all system parameters, even after averaged over a broad range of single-band initial states. The directed transport is theoretically explained within ac-scattering theory. Total reflection phenomena associated with the matter waves travelling from a tilting-free region to a tilted region are emphasized. The results are of relevance to ultracold physics and solid-state physics, and may lead to powerful means of selective, coherent, and directed transport of cold particles in optical lattices. 	
0109047v1	http://arxiv.org/pdf/cs/0109047v1	2001	Between a rock and a hard place: assessing the application of domestic   policy and South Africa's commitments under the WTO'S Basic   Telecommunications Agreement	Tracy Cohen	  South Africa adopted the GATS Basic Agreement on Telecommunications and the regulatory principles in 1998. Obligations undertaken by South Africa mirrored the framework for the gradual telecommunications reform process that was begun in 1996. In the light of two threatened actions for anti-competitive practices in violation of the Agreement, this paper reviews the nature of the commitments undertaken by South Africa and assesses the country's compliance to date. This paper also seeks to explore the tension that arises between domestic policy reforms and international trade aspirations. It is argued that the dynamic produced through this tension affords domestic governments a mechanism with which to balance the seemingly opposing goals of competition and development. It is further argued that the broad regulatory principles, adopted by all signatories and often criticized for lack of precision, facilitate this fine balancing and affords domestic governments an opportunity to advance sovereign concerns while pursuing international trade ideals. 	
0008235v1	http://arxiv.org/pdf/physics/0008235v1	2000	Emergence of Bulk CsCl Structure in (CsCl)nCs+ Cluster Ions	Andres Aguado	  The emergence of CsCl bulk structure in (CsCl)nCs+ cluster ions is investigated using a mixed quantum-mechanical/semiempirical theoretical approach. We find that rhombic dodecahedral fragments (with bulk CsCl symmetry) are more stable than rock-salt fragments after the completion of the fifth rhombic dodecahedral atomic shell. From this size (n=184) on, a new set of magic numbers should appear in the experimental mass spectra. We also propose another experimental test for this transition, which explicitely involves the electronic structure of the cluster. Finally, we perform more detailed calculations in the size range n=31--33, where recent experimental investigations have found indications of the presence of rhombic dodecahedral (CsCl)32Cs+ isomers in the cluster beams. 	
0705.1474v2	http://arxiv.org/pdf/0705.1474v2	2007	First passage times and distances along critical curves	A. Zoia|Y. Kantor|M. Kardar	  We propose a model for anomalous transport in inhomogeneous environments, such as fractured rocks, in which particles move only along pre-existing self-similar curves (cracks). The stochastic Loewner equation is used to efficiently generate such curves with tunable fractal dimension $d_f$. We numerically compute the probability of first passage (in length or time) from one point on the edge of the semi-infinite plane to any point on the semi-circle of radius $R$. The scaled probability distributions have a variance which increases with $d_f$, a non-monotonic skewness, and tails that decay faster than a simple exponential. The latter is in sharp contrast to predictions based on fractional dynamics and provides an experimental signature for our model. 	
0801.0463v2	http://arxiv.org/pdf/0801.0463v2	2008	Asymptotic Stability of Ascending Solitary Magma Waves	Gideon Simpson|Michael I. Weinstein	  Coherent structures, such as solitary waves, appear in many physical problems, including fluid mechanics, optics, quantum physics, and plasma physics. A less studied setting is found in geophysics, where highly viscous fluids couple to evolving material parameters to model partially molten rock, magma, in the Earth's interior. Solitary waves are also found here, but the equations lack useful mathematical structures such as an inverse scattering transform or even a variational formulation.   A common question in all of these applications is whether or not these structures are stable to perturbation. We prove that the solitary waves in this Earth science setting are asymptotically stable and accomplish this without any pre-exisiting Lyapunov stability. This holds true for a family of equations, extending beyond the physical parameter space. Furthermore, this extends existing results on well-posedness to data in a neighborhood of the solitary waves. 	
0803.1023v1	http://arxiv.org/pdf/0803.1023v1	2008	Oscillatory dynamics in evolutionary games are suppressed by   heterogeneous adaptation rates of players	Naoki Masuda	  Game dynamics in which three or more strategies are cyclically competitive, as represented by the rock-scissors-paper game, have attracted practical and theoretical interests. In evolutionary dynamics, cyclic competition results in oscillatory dynamics of densities of individual strategists. In finite-size populations, it is known that oscillations blow up until all but one strategies are eradicated if without mutation. In the present paper, we formalize replicator dynamics with players that have different adaptation rates. We show analytically and numerically that the heterogeneous adaptation rate suppresses the oscillation amplitude. In social dilemma games with cyclically competing strategies and homogeneous adaptation rates, altruistic strategies are often relatively weak and cannot survive in finite-size populations. In such situations, heterogeneous adaptation rates save coexistence of different strategies and hence promote altruism. When one strategy dominates the others without cyclic competition, fast adaptors earn more than slow adaptors. When not, mixture of fast and slow adaptors stabilizes population dynamics, and slow adaptation does not imply inefficiency for a player. 	
0806.2465v1	http://arxiv.org/pdf/0806.2465v1	2008	Pre-Seismic Electrical Signals (SES) generation and their relation to   the lithospheric tidal oscillations K2, S2, M1 (T = 12hours / 14 days)	Constantine Thanassoulas	  It is postulated that the preseismic electric signals (SES) are generated by the piezoelectric mechanism applied on small rock grains - blocks during their stress load until fracturing. Specifically, the square electric train pulses are generated by the combination of a stress increase phase which generates a positive piezostimulated polarized current pulse (PSPC) followed, in a short time, by the stress decrease phase at fracturing level which generates a negative piezostimulated depolarized current pulse (PSDC). Moreover, it is shown that the SES signals are closely related to the tidally triggered lithospheric stress maxima - minima. Examples of SES signals are presented in relation to the tidally triggered lithospheric oscillation (k2, S2, M1) of T = 12hours / 14 days, while some comments are made as far as it concerns their use in short-term earthquake prediction. 	
0806.4567v3	http://arxiv.org/pdf/0806.4567v3	2009	Energy fluctuations in a biharmonically driven nonlinear system	Navinder Singh|Sourabh Lahiri|A. M. Jayannavar	  We study the fluctuations of work done and dissipated heat of a Brownian particle in a symmetric double well system. The system is driven by two periodic input signals that rock the potential simultaneously. Confinement in one preferred well can be achieved by modulating the relative phase between the drives. We show that in the presence of pumping the stochastic resonance signal is enhanced when analyzed in terms of the average work done on the system per cycle. This is in contrast to the case when pumping is achieved by applying an external static bias, which degrades resonance. We analyze the nature of work and heat fluctuations and show that the steady state fluctuation theorem holds in this system. 	
0808.1951v2	http://arxiv.org/pdf/0808.1951v2	2008	Half-metallic ferromagnetism in binary compounds of alkali metals with   nitrogen: Ab initio calculations	Krzysztof Zberecki|Leszek Adamowicz|Michał Wierzbicki	  The first-principles full-potential linearized augmented plane-wave method based on density functional theory is used to investigate electronic structure and magnetic properties of hypothetical binary compounds of I$^{A}$ subgroup elements with nitrogen (LiN, NaN, KN and RbN) in assumed three types of cristalline structure (rock salt, wurtzite and zinc-blende). We find that, due to the spin polarized \textit{p} orbitas of N, all four compounds are half-metallic ferromagnets with wide energy bandgaps (up to 2.0 eV). The calculated total magnetic moment in all investigated compounds for all three types of crystal structure is exactly 2.00 $\mu_{\text{B}}$ per formula unit. The predicted half-metallicity is robust with respect to lattice-constant contraction. In all the cases ferromagnetic phase is energetically favored with respect to the paramagnetic one. The mechanism leading to half-metallic ferromagnetism and synthesis possibilities are discussed. 	
0901.4189v1	http://arxiv.org/pdf/0901.4189v1	2009	Delayed feedback induced directed inertia particle transport in a   washboard potential	D. Hennig|L. Schimansky-Geier|P. Hänggi	  We consider motion of an underdamped Brownian particle in a washboard potential that is subjected to an unbiased time-periodic external field. While in the limiting deterministic system in dependence of the strength and phase of the external field directed net motion can exist, for a finite temperature the net motion averages to zero. Strikingly, with the application of an additional time-delayed feedback term directed particle motion can be accomplished persisting up to fairly high levels of the thermal noise. In detail, there exist values of the feedback strength and delay time for which the feedback term performs oscillations that are phase locked to the time-periodic external field. This yields an effective biasing rocking force promoting periods of forward and backward motion of distinct duration, and thus directed motion. In terms of phase space dynamics we demonstrate that with applied feedback desymmetrization of coexisting attractors takes place leaving the ones supporting either positive or negative velocities as the only surviving ones. Moreover, we found parameter ranges for which in the presence of thermal noise the directed transport is enhanced compared to the noise-less case. 	
0904.4845v2	http://arxiv.org/pdf/0904.4845v2	2009	Magnetism, entropy, and the first nano-machines	Gargi Mitra-Delmotte|A. N. Mitra	  The efficiency of bio-molecular motors stems from reversible interactions $\sim$ $k_B T$; weak bonds stabilizing intermediate states (enabling $direct$ conversion of chemical into mechanical energy). For their (unknown) origins, we suggest that a magnetically structured phase (MSP) formed via accretion of super-paramagnetic particles (S-PPs) by magnetic rocks on the Hadean Ocean floor had hosted motor-like diffusion of ligand-bound S-PPs through its template-layers; its ramifications range from optical activity to quantum coherence. A gentle flux gradient offers both detailed-balance breaking non-equilibrium and $asymmetry$ to a magnetic dipole, undergoing infinitesimal spin-alignment changes. Periodic perturbation of this background by local H-fields of template-partners can lead to periodic high and low-template affinity states, due to the dipole's magnetic degree of freedom. An accompanying magnetocaloric effect allows interchange between system-entropy and bath temperature. We speculate on a magnetic reproducer in a setting close to the mound-scenario of Russell and coworkers that could evolve bio- ratchets. 	
1002.1426v1	http://arxiv.org/pdf/1002.1426v1	2010	\textit{Additional} carrier-mediated ferromagnetism in GdN	A. Sharma|W. Nolting	  The mechanism behind ferromagnetic exchange interaction in GdN is not well understood. It has been argued that it can be due to fourth order cross process of \textit{d-f} mixing and \textit{d-f} exchange. An alternative explanation suggests an anti- ferromagnetic interaction between Gd \textit{d} and N \textit{p} induced moments on the rock salt structure which aligns the nearest neighbor Gd \textit{f} moments ferromagnetically through the \textit{d-f} exchange. In this paper we present results of Curie temperature in GdN as a function of carrier density calculated within our multiband modified RKKY- like exchange interaction. It includes realistic bandstructure of the 5\textit{d} conduction band as an input for single particle energies. We analyze the possibility of carrier- mediated ferromagnetism in GdN and also demonstrate a simple phenomenological model which justifies the role of charge carriers. 	
1003.2922v1	http://arxiv.org/pdf/1003.2922v1	2010	Stabilization of biodiversity in the coevolutionary rock-paper-scissors   game on complex networks	Markus Schütt|Jens Christian Claussen	  The dynamical mechanisms that can stabilize the coexistence of species (or strategies) are of substantial interest for the maintenance of biodiversity and in sociobehavioural dynamics. We investigate the mean extinction time in the coevolutionary dynamics of three cyclically invading strategies for different evolutionary processes on various classes of complex networks, including random graphs, scale-free and small world networks. We find that scale-free and random graphs lead to a strong stabilization of coexistence both for the Moran process and the Local Update process. The stabilization is of an order of magnitude stronger compared to a lattice topology, and is mainly caused by the degree heterogeneity of the graph. However, evolutionary processes on graphs can be defined in many variants, and we show that in a process using effective payoffs the effect of the network topology can be completely reversed. Thus, stabilization of coexistence depends on both network geometry and underlying evolutionary process. 	
1005.4335v1	http://arxiv.org/pdf/1005.4335v1	2010	Discreteness of populations enervates biodiversity in evolution	Yen-Chih Lin|Tzay-Ming Hong|Hsiu-Hau Lin	  Biodiversity widely observed in ecological systems is attributed to the dynamical balance among the competing species. The time-varying populations of the interacting species are often captured rather well by a set of deterministic replicator equations in the evolutionary game theory. However, intrinsic fluctuations arisen from the discreteness of populations lead to stochastic derivations from the smooth evolution trajectories. The role of these fluctuations is shown to be critical at causing extinction and deteriorating the biodiversity of ecosystem. We use children's rock-paper-scissors game to demonstrate how the intrinsic fluctuations arise from the discrete populations and why the biodiversity of the ecosystem decays exponentially, disregarding the detail parameters for competing mechanism and initial distributions. The dissipative trend in biodiversity can be analogized to the gradual erosion of kinetic energy of a moving particle due to air drag or fluid viscosity. The dissipation-fluctuation theorem in statistical physics seals the fate of these originally conserved quantities. This concept in physics can be generalized to scrutinize the errors that might be incurred in the ecological, biological, and quantitative economic modeling for which the ingredients are all discrete in number. 	
1006.1161v1	http://arxiv.org/pdf/1006.1161v1	2010	A Robust Approach for the Growth of Epitaxial Spinel Ferrite Films	J. X. Ma|D. Mazumdar|G. Kim|H. Sato|N. Z. Bao|A. Gupta	  Heteroepitaxial spinel ferrites NiFe2O4 and CoFe2O4 films have been prepared by pulsed laser deposition (PLD) at various temperatures (175 - 690 {\deg}C) under ozone/oxygen pressure of 10 mTorr. Due to enhanced kinetic energy of ablated species at low pressure and enhanced oxidation power of ozone, epitaxy has been achieved at significantly lower temperatures than previously reported. Films grown at temperature below 550 {\deg}C show a novel growth mode, which we term "vertical step-flow" growth mode. Epitaxial spinel ferrite films with atomically flat surface over large areas and enhanced magnetic moment can be routinely obtained. Interestingly, the growth mode is independent of the nature of substrates (spinel MgAl2O4, perovskite SrTiO3, and rock salt MgO) and film thicknesses. The underlying growth mechanism is discussed. 	
1009.0299v1	http://arxiv.org/pdf/1009.0299v1	2010	A simple model for asset price bubble formation and collapse	Alexander Kiselev|Lenya Ryzhik	  We consider a simple stochastic differential equation for modeling bubbles in social context. A prime example is bubbles in asset pricing, but similar mechanisms may control a range of social phenomena driven by psychological factors (for example, popularity of rock groups, or a number of students pursuing a given major). Our goal is to study the simplest possible model in which every term has a clear meaning and which demonstrates several key behaviors. The main factors that enter are tendency of mean reversion to a stable value, speculative social response triggered by trend following and random fluctuations. The interplay of these three forces may lead to bubble formation and collapse. Numerical simulations show that the equation has distinct regimes depending on the values of the parameters. We perform rigorous analysis of the weakly random regime, and study the role of change in fundamentals in igniting the bubble. 	
1011.4794v1	http://arxiv.org/pdf/1011.4794v1	2010	Cyclic competition of four species: mean field theory and stochastic   evolution	Sara O. Case|Clinton H. Durney|Michel Pleimling|R. K. P. Zia	  Generalizing the cyclically competing three-species model (often referred to as the rock-paper-scissors game), we consider a simple system of population dynamics without spatial structures that involves four species. Unlike the previous model, the four form alliance pairs which resemble partnership in the game of Bridge. In a finite system with discrete stochastic dynamics, all but 4 of the absorbing states consist of coexistence of a partner-pair. From a master equation, we derive a set of mean field equations of evolution. This approach predicts complex time dependence of the system and that the surviving partner-pair is the one with the larger product of their strengths (rates of consumption). Simulations typically confirm these scenarios. Beyond that, much richer behavior is revealed, including complicated extinction probabilities and non-trivial distributions of the population ratio in the surviving pair. These discoveries naturally raise a number of intriguing questions, which in turn suggests a variety of future avenues of research, especially for more realistic models of multispecies competition in nature. 	
1102.0624v1	http://arxiv.org/pdf/1102.0624v1	2011	Phase diagrams for the spatial public goods game with pool-punishment	Attila Szolnoki|Gyorgy Szabo|Matjaz Perc	  The efficiency of institutionalized punishment is studied by evaluating the stationary states in the spatial public goods game comprising unconditional defectors, cooperators, and cooperating pool-punishers as the three competing strategies. Fine and cost of pool-punishment are considered as the two main parameters determining the stationary distributions of strategies on the square lattice. Each player collects its payoff from five five-person public goods games, and the evolution of strategies is subsequently governed by imitation based on pairwise comparisons at a low level of noise. The impact of pool-punishment on the evolution of cooperation in structured populations is significantly different from that reported previously for peer-punishment. Representative phase diagrams reveal remarkably rich behavior, depending also on the value of the synergy factor that characterizes the efficiency of investments payed into the common pool. Besides traditional single and two-strategy stationary states, a rock-paper-scissors type cyclic dominance can emerge in strikingly different ways. 	
1103.5834v1	http://arxiv.org/pdf/1103.5834v1	2011	From invasion percolation to flow in rock fracture networks	Salomon J. Wettstein|Falk K. Wittel|Nuno A. M. Araujo|Bill Lanyon|Hans J. Herrmann	  The main purpose of this work is to simulate two-phase flow in the form of immiscible displacement through anisotropic, three-dimensional (3D) discrete fracture networks (DFN). The considered DFNs are artificially generated, based on a general distribution function or are conditioned on measured data from deep geological investigations. We introduce several modifications to the invasion percolation (MIP) to incorporate fracture inclinations, intersection lines, as well as the hydraulic path length inside the fractures. Additionally a trapping algorithm is implemented that forbids any advance of the invading fluid into a region, where the defending fluid is completely encircled by the invader and has no escape route. We study invasion, saturation, and flow through artificial fracture networks, with varying anisotropy and size and finally compare our findings to well studied, conditioned fracture networks. 	
1110.3066v1	http://arxiv.org/pdf/1110.3066v1	2011	Mass-Radius Relationships for Exoplanets II: Grueneisen Equation of   State for Ammonia	Damian C. Swift	  We describe a mechanical equation of state for NH3, based on shock wave measurements for liquid ammonia. The shock measurements, for an initial temperature of 203 K, extended to 1.54 g/cc and 38.6 GPa. The shock and particle speeds were fitted well with a straight line, so extrapolations to higher compressions are numerically stable, but the accuracy is undetermined outside the range of the data. The isentrope through the same initial state was estimated, along with its sensitivity to the Grueneisen parameter. Mass-radius relations were calculated for self-gravitating bodies of pure ammonia, and for differentiated ammonia-rock bodies. The relations were insensitive to variations in the Grueneisen parameter, indicating that they should be accurate for studies of planetary structure. 	
1112.0851v1	http://arxiv.org/pdf/1112.0851v1	2011	Driven Ratchets for Cold Atoms	F. Renzoni	  Brownian motors, or ratchets, are devices which "rectify" Brownian motion, i.e. they can generate a current of particles out of unbiased fluctuations. The ratchet effect is a very general phenomenon which applies to a wide range of physical systems, and indeed ratchets have been realized with a variety of solid state devices, with optical trap setups as well as with synthetic molecules and granular gases. The present article reviews recent experimental realizations of ac driven ratchets with cold atoms in driven optical lattices. This is quite an unusual system for a Brownian motor as there is no a real thermal bath, and both the periodic potential for the atoms and the fluctuations are determined by laser fields. Such a system allowed us to realize experimentally rocking and gating ratchets, and to precisely investigate the relationship between symmetry and transport in these ratchets, both for the case of periodic and quasiperiodic driving. 	
1203.3367v1	http://arxiv.org/pdf/1203.3367v1	2012	Stochastic differential equations for evolutionary dynamics with   demographic noise and mutations	Arne Traulsen|Jens Christian Claussen|Christoph Hauert	  We present a general framework to describe the evolutionary dynamics of an arbitrary number of types in finite populations based on stochastic differential equations (SDE). For large, but finite populations this allows to include demographic noise without requiring explicit simulations. Instead, the population size only rescales the amplitude of the noise. Moreover, this framework admits the inclusion of mutations between different types, provided that mutation rates, $\mu$, are not too small compared to the inverse population size 1/N. This ensures that all types are almost always represented in the population and that the occasional extinction of one type does not result in an extended absence of that type. For $\mu N\ll1$ this limits the use of SDE's, but in this case there are well established alternative approximations based on time scale separation. We illustrate our approach by a Rock-Scissors-Paper game with mutations, where we demonstrate excellent agreement with simulation based results for sufficiently large populations. In the absence of mutations the excellent agreement extends to small population sizes. 	
1205.6411v2	http://arxiv.org/pdf/1205.6411v2	2013	Intransitivity and coexistence in four species cyclic games	Alessandra F. Lütz|Sebastián Risau-Gusman|Jeferson J. Arenzon	  Intransitivity is a property of connected, oriented graphs representing species interactions that may drive their coexistence even in the presence of competition, the standard example being the three species Rock-Paper-Scissors game. We consider here a generalization with four species, the minimum number of species allowing other interactions beyond the single loop (one predator, one prey). We show that, contrary to the mean field prediction, on a square lattice the model presents a transition, as the parameter setting the rate at which one species invades another changes, from a coexistence to a state in which one species gets extinct. Such a dependence on the invasion rates shows that the interaction graph structure alone is not enough to predict the outcome of such models. In addition, different invasion rates permit to tune the level of transitiveness, indicating that for the coexistence of all species to persist, there must be a minimum amount of intransitivity. 	
1206.3604v2	http://arxiv.org/pdf/1206.3604v2	2012	Effective Conductivity of Spiral and other Radial Symmetric Assemblages	Andrej Cherkaev|Alexander D. Pruss	  Assemblies of circular inclusions with spiraling laminate structure inside them are studied, such as spirals with inner inclusions, spirals with shells, assemblies of "wheels" - structures from laminates with radially dependent volume fractions, complex axisymmetric three-dimensional micro-geometries called Connected Hubs and Spiky Balls. The described assemblages model structures met in rock mechanics, biology, etc. The classical effective medium theory coupled with hierarchical homogenization is used. It is found that fields in spiral assemblages satisfy a coupled system of two second order differential equations, rather than a single differential equation; a homogeneous external field applied to the assembly is transformed into a rotated homogeneous field inside of the inclusions. The effective conductivity of the two-dimensional Star assembly is equivalent to that of Hashin-Shtrikman coated circles, but the conductivity of analogous three-dimensional Spiky Ball is different from the conductivity of coated sphere geometry. 	
1208.5299v4	http://arxiv.org/pdf/1208.5299v4	2012	Optical properties and hardness of highly a-axis oriented AlN films	Feby Jose|R. Ramaseshan|S. Tripura Sundari|S. Dash|M. S. R. N. Kiran|A. K. Tyagi|U. Ramamurty	  This paper reports optical and nanomechanical properties of seldom studied highly a-axis oriented AlN thin films for the first time. These films were deposited by reactive DC magnetron sputtering technique at an optimal target to substrate distance of 180 mm. Bragg-Brentano geometry X-ray and rocking curve (FWHM = 52 arcsec) studies confirmed the preferred orientation. Spectroscopic ellipsometry revealed that these films exhibit a refractive index of 1.93 at a wavelength of 546 nm. The hardness and elastic modulus of these films were 17 GPa and 190 GPa, respectively. The mechanical properties obtained here are much higher than the earlier reported and therefore can be useful as protective coating in thermo printing devices, piezoelectric films in bulk acoustic wave resonators. 	
1306.1622v2	http://arxiv.org/pdf/1306.1622v2	2014	The Strength of Regolith and Rubble Pile Asteroids	Paul Sanchez|Daniel J. Scheeres	  We explore the hypothesis that, due to small van der Waals forces between constituent grains, small rubble pile asteroids have a small but non-zero cohesive strength. The nature of this model predicts that the cohesive strength should be constant independent of asteroid size, which creates a scale dependence with relative strength increasing as size decreases. This model counters classical theory that rubble pile asteroids should behave as scale-independent cohesionless collections of rocks. We explore a simple model for asteroid strength that is based on these weak forces, validate it through granular mechanics simulations and comparisons with properties of lunar regolith, and then explore its implications and ability to explain and predict observed properties of small asteroids in the NEA and Main Belt populations, and in particular of asteroid 2008 TC3. One conclusion is that the population of rapidly rotating asteroids could consist of both distributions of smaller grains (i.e., rubble piles) and of monolithic boulders. 	
1307.0463v1	http://arxiv.org/pdf/1307.0463v1	2013	ALMA and CARMA observations of Brown Dwarfs disks: testing the models of   dust evolution	L. Ricci|L. Testi|A. Natta|A. Scholz|I. de Gregorio-Monsalvo|A. Isella|J. M. Carpenter	  The first steps toward planet formation involve the coagulation of small microscopic grains into larger and larger pebbles and rocks in gas-rich disks around young stars and brown dwarfs. Observations in the sub-millimeter can trace mm/cm-sized pebbles in the outer disks, and investigate the mechanisms of coagulation/fragmentation and radial migration of these solids. These represent key, yet not fully understood ingredients for our understanding of the formation of planetesimals, the building blocks of planets. Here we present the first results from an observational program using the ALMA and CARMA sub-mm/mm interferometers aimed at characterizing the dust properties and disk structure of young disks around brown dwarfs and very low mass stars. Given the physical conditions expected for these disks, they represent critical test beds for the models of the early stages of planet formation in proto-planetary disks. 	
1308.6210v3	http://arxiv.org/pdf/1308.6210v3	2013	Hyperbolic Illusion of the Mid-Holocene Turning Point	Ron W. Nielsen aka Jan Nurzynski	  Using the growth of population in Australia in the past 10,000 years it is illustrated here how an illusion created by hyperbolic distributions may lead easily to incorrect conclusions. Contrary to the published claim, there was no change in the mechanism of growth of the ancient human population in Australia around 5000 years before present (BP). Data for the number of rock-shelter sites, interpreted as reflecting the growth of human population, are analysed. They are shown to be monotonically increasing with time without any sign of intensification in the past 10,000 years. The growth of human population in Australia is in excellent agreement with the similar pattern describing the growth of the world population. 	
1311.3988v1	http://arxiv.org/pdf/1311.3988v1	2013	Numerical evaluation of relative permeability using   Johnson--Koplik--Dashen model	Andrea Cortis|Dmitriy Silin	  We present a numerical study aimed at comparing two approaches to the evaluation of relative permeability curves from 3D binary images of porous media. One approach hinges on the numerical solution of Stokes equations, while the other is based on the Johnson-Koplik-Dashen (JKD) universal scaling theory of viscous frequency-dependent flow [D.~L. Johnson, J.~Koplik, and R.~Dashen, \emph{Theory of dynamic permeability and tortuosity in fluid--saturated porous media}, Journal of Fluid Mechanics \textbf{176} (1987), 379--402.] and the method of maximal inscribed spheres.   JKD steady-flow simulations only require the solution of a boundary-value problem for the Laplace equation, which is computationally less intensive than the solution of Stokes equations.   A series of numerical calculations performed on 3D pore-space images of natural rock demonstrate that JKD-based estimates are in good agreement with the corresponding Stokes-flow numerical simulations. 	
1311.4810v1	http://arxiv.org/pdf/1311.4810v1	2013	Impact of the frequency dependence of tidal Q on the evolution of   planetary systems	P. Auclair-Desrotour|C. Le Poncin-Lafitte|S. Mathis	  Context. Tidal dissipation in planets and in stars is one of the key physical mechanisms that drive the evolution of planetary systems.   Aims. Tidal dissipation properties are intrisically linked to the internal structure and the rheology of studied celestial bodies. The resulting dependence of the dissipation upon the tidal frequency is strongly different in the cases of solids and fluids.   Methods. We compute the tidal evolution of a two-body coplanar system, using the tidal quality factor's frequency-dependencies appropriate to rocks and to convective fluids.   Results. The ensuing orbital dynamics comes out smooth or strongly erratic, dependent on how the tidal dissipation depends upon frequency.   Conclusions. We demonstrate the strong impact of the internal structure and of the rheology of the central body on the orbital evolution of the tidal perturber. A smooth frequency-dependence of the tidal dissipation renders a smooth orbital evolution while a peaked dissipation can furnish erratic orbital behaviour. 	
1312.0595v1	http://arxiv.org/pdf/1312.0595v1	2013	Three-dimensional phase-field study of crack-seal microstructures -   Insights from innovative post-processing techniques	Kumar Ankit|Michael Selzer|Britta Nestler	  Numerical simulations of vein evolution contribute to a better understanding of processes involved in their formation and possess the potential to provide invaluable insights into the rock deformation history and fluid flow pathways. The primary aim of the present article is to investigate the influence of a realistic boundary condition, i.e. an algorithmically generated fractal surface, on the vein evolution in 3-D using a thermodynamically consistent approach, while explaining the benefits of accounting for an extra dimensionality. The 3-D simulation results are supplemented by innovative numerical post-processing and advanced visualization techniques. The new methodologies to measure the tracking efficiency demonstrate the importance of accounting the temporal evolution; no such information is usually accessible in field studies and notoriously difficult to obtain from laboratory experiments as well. The grain growth statistics obtained by numerically post-processing the 3-D computational microstructures explain the pinning mechanism which leads to arrest of grain boundaries/multi-junctions by crack peaks, thereby, enhancing the tracking behavior. 	
1312.7467v1	http://arxiv.org/pdf/1312.7467v1	2013	Phonon self-energy and origin of anomalous neutron scattering spectra in   SnTe and PbTe thermoelectrics	C. W. Li|O. Hellman|J. Ma|A. F. May|H. B. Cao|X. Chen|A. D. Christianson|G. Ehlers|D. J. Singh|B. C. Sales|O. Delaire	  The anharmonic lattice dynamics of rock-salt thermoelectric compounds SnTe and PbTe are investigated with inelastic neutron scattering (INS) and first-principles calculations. The experiments show that, surprisingly, although SnTe is closer to the ferroelectric instability, phonon spectra in PbTe exhibit a more anharmonic character. This behavior is reproduced in first-principles calculations of the temperature-dependent phonon self-energy. Our simulations reveal how the nesting of phonon dispersions induces prominent features in the self-energy, which account for the measured INS spectra and their temperature dependence. We establish that the phase-space for three-phonon scattering processes, rather than just the proximity to the lattice instability, is the mechanism determining the complex spectrum of the transverse-optical ferroelectric mode. 	
1401.5812v1	http://arxiv.org/pdf/1401.5812v1	2014	Faulting of rocks at three-dimensional stress field by micro-anticracks	H. O. Ghaffari|M. H. B. Nasseri|R. Paul Young	  Using 9-sets of different laboratory earthquake tests, we examined the nature of cracking under true triaxial stress conditions in the lithosphere . We found that 3D stress state can induce oblique nucleation of many fractures, forming final plane of complex polymodal faults. Our fully 3D experiments indicate unconventional fault nucleation with 2-3 times faster slip phase, implying a new slip-weakening mechanism for earthquakes in upper crust. In addition, We compared our observations of irregular cracks with typical anti-cracks signals in multi-anvil High Pressure and Temperature test. For the first time, we showed oblique faulting can change slip-weakening rate and accelerate the rate of energy release. This implicates a sharper source time function for further modeling of our experiments. Indeed, our results can be assumed as an ex tension to detachment fronts in micro-faults to a general concept of quasi-anti rupture fronts. We showed events from deep-focus earthquakes can share some similarity to shallow earthquakes, promoting recent approach on similarity of deep earthquakes to their shallow counterparts . 	
1402.6624v1	http://arxiv.org/pdf/1402.6624v1	2014	Fixation in cyclically competing species on a directed graph with   quenched disorder	Hiroki Ohta|Namiko Mitarai	  A simple model of cyclically competing species on a directed graph with quenched disorder is proposed as an extension of the rock-paper-scissors model. By assuming that the effects of loops in a directed random graph can be ignored in the thermodynamic limit, it is proved for any finite disorder that the system fixates to a frozen configuration when the species number $s$ is larger than the spatial connectivity $c$, and otherwise stays active. Nontrivial lower and upper bounds for the persistence probability of a site never changing its state are also analytically computed. The obtained bounds and numerical simulations support the existence of a phase transition as a function of disorder for $1<c_l(s)\le c <s$, with a $s$-dependent threshold of the connectivity $c_l(s)$. 	
1404.4165v1	http://arxiv.org/pdf/1404.4165v1	2014	Universal asymptotic umbrella for hydraulic fracture modeling	Aleksandr M. Linkov	  The paper presents universal asymptotic solution needed for efficient modeling of hydraulic fractures. We show that when neglecting the lag, there is universal asymptotic equation for the near-front opening. It appears that apart from the mechanical properties of fluid and rock, the asymptotic opening depends merely on the local speed of fracture propagation. This implies that, on one hand, the global problem is ill-posed, when trying to solve it as a boundary value problem under a fixed position of the front. On the other hand, when properly used, the universal asymptotics drastically facilitates solving hydraulic fracture problems (both analytically and numerically). We derive simple universal asymptotics and comment on their employment for efficient numerical simulation of hydraulic fractures, in particular, by well-established Level Set and Fast Marching Methods. 	
1409.7710v1	http://arxiv.org/pdf/1409.7710v1	2014	The formation and arrangement of pits by a corrosive gas	James Burridge|Robert Inkpen	  When corroding or otherwise aggressive particles are incident on a surface, pits can form. For example, under certain circumstances rock surfaces that are exposed to salts can form regular tessellating patterns of pits known as "tafoni". We introduce a simple lattice model in which a gas of corrosive particles, described by a discrete convection diffusion equation, drifts onto a surface. Each gas particle has a fixed probability of being absorbed and causing damage at each contact. The surface is represented by a lattice of strength numbers which reduce after each absorbtion event, with sites being removed when their strength becomes negative. The model generates regular formations of pits, with each pit having a characteristic trapezoidal geometry determined by the particle bias, absorbtion probability and surface strength. The formation of this geometry may be understood in terms of a first order partial differential equation. By viewing pits as particle funnels, we are able to relate the gradient of pit walls to absorbtion probability and particle bias. 	
1412.5611v1	http://arxiv.org/pdf/1412.5611v1	2014	Analytical Parameterization of Self-Consistent Polycrystal Mechanics:   Fast Calculation of Upper Mantle Anisotropy	Neil J. Goulding|Neil M. Ribe|Olivier Castelnau|Andrew M. Walker|James Wookey	  Progressive deformation of upper mantle rocks via dislocation creep causes their constituent crystals to take on a non-random orientation distribution (crystallographic preferred orientation or CPO) whose observable signatures include shear-wave splitting and azimuthal dependence of surface wave speeds. Comparison of these signatures with mantle flow models thus allows mantle dynamics to be unraveled on global and regional scales. However, existing self-consistent models of CPO evolution are computationally expensive when used in 3-D and/or time-dependent convection models. Here we propose a new method, called ANPAR, which is based on an analytical parameterisation of the crystallographic spin predicted by the second-order (SO) self-consistent theory. Our parameterisation runs approximately 2-3 x 10^4 times faster than the SO model and fits its predictions for CPO and crystallographic spin with a variance reduction > 99%. We illustrate the ANPAR model predictions for three uniform deformations (uniaxial compression, pure shear, simple shear) and for a corner-flow model of a spreading ridge. 	
1503.01703v1	http://arxiv.org/pdf/1503.01703v1	2015	A damage model for fracking	J. Quinn Norris|Donald L. Turcotte|John B. Rundle	  Injections of large volumes of water into tight shale reservoirs allows the extraction of oil and gas not previously accessible. This large volume "super" fracking induces damage that allows the oil and/or gas to flow to an extraction well. The purpose of this paper is to provide a model for understanding super fracking. We assume that water is injected from a small spherical cavity into a homogeneous elastic medium. The high pressure of the injected water generates hoop stresses that reactivate natural fractures in the tight shales. These fractures migrate outward as water is added creating a spherical shell of damaged rock. The porosity associated with these fractures is equal to the water volume injected. We obtain an analytic expression for this volume. We apply our model to a typical tight shale reservoir and show that the predicted water volumes are in good agreement with the volumes used in super fracking. 	
1504.07816v1	http://arxiv.org/pdf/1504.07816v1	2015	Evolutionary games of condensates in coupled birth-death processes	Johannes Knebel|Markus F. Weber|Torben Krueger|Erwin Frey	  Condensation phenomena arise through a collective behaviour of particles. They are observed in both classical and quantum systems, ranging from the formation of traffic jams in mass transport models to the macroscopic occupation of the energetic ground state in ultra-cold bosonic gases (Bose-Einstein condensation). Recently, it has been shown that a driven and dissipative system of bosons may form multiple condensates. Which states become the condensates has, however, remained elusive thus far. The dynamics of this condensation are described by coupled birth-death processes, which also occur in evolutionary game theory. Here, we apply concepts from evolutionary game theory to explain the formation of multiple condensates in such driven-dissipative bosonic systems. We show that vanishing of relative entropy production determines their selection. The condensation proceeds exponentially fast, but the system never comes to rest. Instead, the occupation numbers of condensates may oscillate, as we demonstrate for a rock-paper-scissors game of condensates. 	
1506.01170v1	http://arxiv.org/pdf/1506.01170v1	2015	A Game-Theoretic Model and Best-Response Learning Method for Ad Hoc   Coordination in Multiagent Systems	Stefano V. Albrecht|Subramanian Ramamoorthy	  The ad hoc coordination problem is to design an autonomous agent which is able to achieve optimal flexibility and efficiency in a multiagent system with no mechanisms for prior coordination. We conceptualise this problem formally using a game-theoretic model, called the stochastic Bayesian game, in which the behaviour of a player is determined by its private information, or type. Based on this model, we derive a solution, called Harsanyi-Bellman Ad Hoc Coordination (HBA), which utilises the concept of Bayesian Nash equilibrium in a planning procedure to find optimal actions in the sense of Bellman optimal control. We evaluate HBA in a multiagent logistics domain called level-based foraging, showing that it achieves higher flexibility and efficiency than several alternative algorithms. We also report on a human-machine experiment at a public science exhibition in which the human participants played repeated Prisoner's Dilemma and Rock-Paper-Scissors against HBA and alternative algorithms, showing that HBA achieves equal efficiency and a significantly higher welfare and winning rate. 	
1512.05078v1	http://arxiv.org/pdf/1512.05078v1	2015	Rate and state friction law as derived from atomistic processes at   asperities	Takahiro Hatano	  A theoretical account is given of the microscopic basis of the rate- and state-dependent friction (RSF) law. The RSF law describes rock friction quantitatively and therefore it is commonly used to model earthquakes and the related phenomena. But the RSF law is rather empirical and the theoretical basis has not been very clear. Here we derive the RSF law starting from constitutive laws for asperities, and give the atomistic expressions for the empirical RSF parameters. In particular, we show that both the length constant and the state variable are given as the 0th weighted power means of the corresponding microscopic quantities: a linear dimension and the contact duration of each asperity. As a result, evolution laws for the state variable can be derived systematically. We demonstrate that the aging and the slip laws can be derived and clarify the approximations behind these two major evolution laws. Additionally, the scaling properties of the length constant are clarified for fractal distribution of asperities. 	
1601.06585v1	http://arxiv.org/pdf/1601.06585v1	2016	Measurement of the Pressure induced by salt crystallization in   confinement	Julie Desarnaud|Daniel Bonn|Noushine Shahidzadeh	  Salt crystallization is a major cause of weathering of artworks, monuments and rocks. Damage will occur if crystals continue to grow in confinement, i.e. within the pore space of these materials generating mechanical stresses. We report on a novel method that allows to directly measure, at the microscale, the resulting pressure while visualizing the spontaneous nucleation and growth of alkali halide salts. The experiments reveal the crucial role of the wetting films between the growing crystal and the confining walls for the development of the pressure. The results suggest that the pressure originates from a charge repulsion between the similarly charged wall and the crystal separated by a ~1.5 nm salt solution film. Consequently, if the walls are made hydrophobic, no film and no crystallization pressure are detected. The magnitude of the pressure is system-specific and explains how a growing crystal exerts stresses at the scale of individual grains in porous materials. 	
1604.08410v1	http://arxiv.org/pdf/1604.08410v1	2016	Comparison between cell-centered and nodal based discretization schemes   for linear elasticity	Nilsen Halvor|Nordbotten Jan|Raynaud Xavier	  In this paper we study newly developed methods for linear elasticity on polyhedral meshes. Our emphasis is on applications of the methods to geological models. Models of subsurface, and in particular sedimentary rocks, naturally lead to general polyhedral meshes. Numerical methods which can directly handle such representation are highly desirable. Many of the numerical challenges in simulation of subsurface applications come from the lack of robustness and accuracy of numerical methods in the case of highly distorted grids. In this paper we investigate and compare the Multi-Point Stress Approximation (MPSA) and the Virtual Element Method (VEM) with regards to grid features that are frequently seen in geological models and likely to lead to a lack of accuracy of the methods. In particular we look how the methods perform near the incompressible limit. This work shows that both methods are promising for flexible modeling of subsurface mechanics. 	
1606.04840v2	http://arxiv.org/pdf/1606.04840v2	2016	Recent Tectonic Activity on Pluto Driven by Phase Changes in the Ice   Shell	Noah P. Hammond|Amy C. Barr|Edgar M. Parmentier	  The New Horizons spacecraft has found evidence for geologic activity on the surface of Pluto, including extensional tectonic deformation of its water ice bedrock (see Moore et al., 2016). One mechanism that could drive extensional tectonic activity is global surface expansion due to the partial freezing of an ocean. We use updated physical properties for Pluto and simulate its thermal evolution to understand the survival of a possible subsurface ocean. For thermal conductivities of rock less than 3 W m$^{-1}$ K$^{-1}$, an ocean forms and at least partially freezes, leading to recent extensional stresses in the ice shell. In scenarios where the ocean freezes and the ice shell is thicker than $260$ km, ice II forms and causes global volume contraction. Since there is no evidence for recent compressional tectonic features, we argue that ice II has not formed and that Pluto's ocean has likely survived to present day. 	
1607.04178v2	http://arxiv.org/pdf/1607.04178v2	2016	Presence of Exotic Electronic Surface States in LaBi and LaSb	X. H. Niu|D. F. Xu|Y. H. Bai|Q. Song|X. P. Shen|B. P. Xie|Z. Sun|Y. B. Huang|D. C. Peets|D. L. Feng	  Extremely high magnetoresistance (XMR) in the lanthanum monopnictides La$X$ ($X$ = Sb, Bi) has recently attracted interest in these compounds as candidate topological materials. However, their perfect electron-hole compensation provides an alternative explanation, so the possible role of topological surface states requires verification through direct observation. Our angle-resolved photoemission spectroscopy (ARPES) data reveal multiple Dirac-like surface states near the Fermi level in both materials. Intriguingly, we have observed circular dichroism in both surface and near-surface bulk bands. Thus the spin-orbit coupling-induced orbital and spin angular momentum textures may provide a mechanism to forbid backscattering in zero field, suggesting that surface and near-surface bulk bands may contribute strongly to XMR in La$X$. The extremely simple rock salt structure of these materials and the ease with which high-quality crystals can be prepared suggests that they may be an ideal platform for further investigation of topological matter. 	
1611.09635v3	http://arxiv.org/pdf/1611.09635v3	2017	Testability of evolutionary game dynamics models based on experimental   economics data	Yijia Wang|Xiaojie Chen|Zhijian Wang	  Understanding the dynamic processes of a real game system requires an appropriate dynamics model, and rigorously testing a dynamics model is non-trivial. In our methodological research, we develop an approach to testing the validity of game dynamics models that considers the dynamic patterns of angular momentum and speed as measurement variables. Using Rock-Paper-Scissors (RPS) games as an example, we illustrate the geometric patterns in the experiment data. We then derive the related theoretical patterns from a series of typical dynamics models. By testing the goodness-of-fit between the experimental and theoretical patterns, we show that the validity of these models can be evaluated quantitatively. Our approach establishes a link between dynamics models and experimental systems, which is, to the best of our knowledge, the most effective and rigorous strategy for ascertaining the testability of evolutionary game dynamics models. 	
1612.04446v1	http://arxiv.org/pdf/1612.04446v1	2016	Transport coherence in a time-asymmetric rocked ratchet model	Mamata Sahoo|A M Jayannavar	  We study the dynamics of an over damped Brownian particle in a saw tooth potential in the presence of a temporal asymmetric driving force. We observe that in the deterministic limit, the transport coherence, which is determined by a dimensionless quantity called Peclet number, $Pe$ is quite high for larger spatial asymmetry in the ratchet potential. For all the regime of parameter space of this model, $Pe$ follows the nature of current like Stokes efficiency. Diffusion as a function of amplitude of drive shows a minimum exactly at which the current shows a maximum. Unlike the previously studied models, the $Pe$ as a function of temperature shows a peaking behavior and the coherence in transport decreases for high temperatures. In the nonadiabatic regime, the $Pe$ as a function of amplitude of drive decreases and the peak gets broader as a result the transport becomes unreliable. 	
1702.02318v1	http://arxiv.org/pdf/1702.02318v1	2017	Resonant current in coupled inertial Brownian particles with   delayed-feedback control	Tianfu Gao|Zhigang Zheng|Jincan Chen	  The transport of a walker in rocking feedback-controlled ratchets are investigated. The walker consists of two coupled "feet" that allow the interchange of the order of the particles while the walker moves. In the underdamped case, the deterministic dynamics of the walker in a tilted asymmetric ratchet with an external periodic force is considered. It is found that the delayed feedback ratchets with a switching-on-and-off dependence of the states of the system can lead to the absolute negative mobility (ANM). In such a novel phenomenon the particles move against the bias. Moreover, the walker can acquire a series of resonant steps for different values of the current. Remarkably, it is interesting to find that the resonant current of the walker are induced by the phase locked motion that corresponds to the synchronization of the motion with the change in the frequency of the external driving. These resonant steps can be well predicted in terms of time-space symmetry analysis, which is in good agreement with dynamics simulations. The transport performances can be optimized and furthermore controlled by suitably adjusting the parameters of the delayed-feedback ratchets. 	
1702.03677v1	http://arxiv.org/pdf/1702.03677v1	2017	Origins of bond and spin order in rare-earth nickelate bulk and   heterostructures	Yi Lu|Zhicheng Zhong|Maurits W. Haverkort|Philipp Hansmann	  We analyze the charge- and spin response functions of rare-earth nickelates RNiO3 and their heterostructures using random-phase approximation in a two-band Hubbard model. The inter-orbital charge fluctuation is found to be the driving mechanism for the rock-salt type bond order in bulk RNiO3, and good agreement of the ordering temperature with experimental values is achieved for all RNiO3 using realistic crystal structures and interaction parameters. We further show that magnetic ordering in bulk is not driven by the spin fluctuation and should be instead explained as ordering of localized moments. This picture changes for low-dimensional heterostructures, where the charge fluctuation is suppressed and overtaken by the enhanced spin instability, which results in a spin-density-wave ground state observed in recent experiments. Predictions for spectroscopy allow for further experimental testing of our claims. 	
1705.07198v1	http://arxiv.org/pdf/1705.07198v1	2017	Laboratory spectra of hot molecules: data needs for hot super-Earth   exoplanets	Jonathan Tennyson|Sergei N. Yurchenko	  The majority of stars are now thought to support exoplanets. Many of those exoplanets discovered thus far are categorized as rocky objects with an atmosphere. Most of these objects are however hot due to their short orbital period. Models suggest that water is the dominant species in their atmospheres. The hot temperatures are expected to turn these atmospheres into a (high pressure) steam bath containing remains of melted rock. The spectroscopy of these hot rocky objects will be very different from that of cooler objects or hot gas giants. Molecules suggested to be important for the spectroscopy of these objects are reviewed together with the current status of the corresponding spectroscopic data. Perspectives of building a comprehensive database of linelist/cross sections applicable for atmospheric models of rocky super-Earths as part of the ExoMol project are discussed. The quantum-mechanical approaches used in linelist productions and their challenges are summarized. 	
1706.05315v1	http://arxiv.org/pdf/1706.05315v1	2017	Towards colloidal spintronics through Rashba spin-orbit interaction in   lead sulphide nanosheets	Mohammad Mehdi Ramin Moayed|Thomas Bielewicz|Martin Sebastian Zollner|Carmen Herrmann|Christian Klinke	  Employing the spin degree of freedom of charge carriers offers the possibility to extend the functionality of conventional electronic devices, while colloidal chemistry can be used to synthesize inexpensive and tuneable nanomaterials. In order to benefit from both concepts, Rashba spin-orbit interaction has been investigated in colloidal lead sulphide nanosheets by electrical measurements on the circular photo-galvanic effect. Lead sulphide nanosheets possess rock salt crystal structure, which is centrosymmetric. The symmetry can be broken by quantum confinement, asymmetric vertical interfaces and a gate electric field leading to Rashba-type band splitting in momentum space at the M points, which results in an unconventional selection mechanism for the excitation of the carriers. The effect, which is supported by simulations of the band structure using density functional theory, can be tuned by the gate electric field and by the thickness of the sheets. Spin-related electrical transport phenomena in colloidal materials open a promising pathway towards future inexpensive spintronic devices. 	
1707.03447v1	http://arxiv.org/pdf/1707.03447v1	2017	Coarsening with non-trivial in-domain dynamics: correlations and   interface fluctuations	Barton L. Brown|Michel Pleimling	  Using numerical simulations we investigate the space-time properties of a system in which spirals emerge within coarsening domains, thus giving rise to non-trivial internal dynamics. Initially proposed in the context of population dynamics, the studied six-species model exhibits growing domains composed of three species in a rock-paper-scissors relationship. Through the investigation of different quantities, such as space-time correlations and the derived characteristic length, autocorrelation, density of empty sites, and interface width, we demonstrate that the non-trivial dynamics inside the domains affects the coarsening process as well as the properties of the interfaces separating different domains. Domain growth, aging, and interface fluctuations are shown to be governed by exponents whose values differ from those expected in systems with curvature driven coarsening. 	
1710.06827v1	http://arxiv.org/pdf/1710.06827v1	2017	A non-ordinary peridynamics implementation for anisotropic materials	Gabriel Hattori|Jon Trevelyan|William M. Coombs	  Peridynamics (PD) represents a new approach for modelling fracture mechanics, where a continuum domain is modelled through particles connected via physical bonds. This formulation allows us to model crack initiation, propagation, branching and coalescence without special assumptions. Up to date, anisotropic materials were modelled in the PD framework as different isotropic materials (for instance, fibre and matrix of a composite laminate), where the stiffness of the bond depends on its orientation. A non-ordinary state-based formulation will enable the modelling of generally anisotropic materials, where the material properties are directly embedded in the formulation. Other material models include rocks, concrete and biomaterials such as bones. In this paper, we implemented this model and validated it for anisotropic composite materials. A composite damage criterion has been employed to model the crack propagation behaviour. Several numerical examples have been used to validate the approach, and compared to other benchmark solution from the finite element method (FEM) and experimental results when available. 	
1506.01274v3	http://arxiv.org/pdf/1506.01274v3	2016	On the possibility of Galactic Cosmic Ray-induced radiolysis-powered   life in subsurface environments in the Universe	Dimitra Atri	  Photosynthesis is a mechanism developed by terrestrial life to utilize the energy from photons of solar origin for biological use. Subsurface regions are isolated from the photosphere, and consequently are incapable of utilizing this energy. This opens up the opportunity for life to evolve alternate mechanisms for harvesting available energy. Bacterium Candidatus Desulforudis audaxviator, found 2.8 km deep in a South African mine, harvests energy from radiolysis, induced by particles emitted from radioactive U, Th and K present in surrounding rock. Another radiation source in the subsurface environments is secondary particles generated by Galactic Cosmic Rays (GCRs). Using Monte Carlo simulations, it is shown that it is a steady source of energy comparable to that produced by radioactive substances, and the possibility of a slow metabolizing life flourishing on it cannot be ruled out. Two mechanisms are proposed through which GCR-induced secondary particles can be utilized for biological use in subsurface environments: (1) GCRs injecting energy in the environment through particle-induced radiolysis, and (2) organic synthesis from GCR secondaries interacting with the medium. Laboratory experiments to test these hypotheses are also proposed. Implications of these mechanisms on finding life in the Solar System and elsewhere in the Universe are discussed. 	
1511.01891v1	http://arxiv.org/pdf/1511.01891v1	2015	Vortices determine the dynamics of biodiversity in cyclical interactions   with protection spillovers	Attila Szolnoki|Matjaz Perc	  If rock beats scissors and scissors beat paper, one might assume that rock beats paper too. But this is not the case for intransitive relationships that make up the famous rock-paper-scissors game. However, the sole presence of paper might prevent rock from beating scissors, simply because paper beats rock. This is the blueprint for the rock-paper-scissors game with protection spillovers, which has recently been introduced as a new paradigm for biodiversity in well-mixed microbial populations. Here we study the game in structured populations, demonstrating that protection spillovers give rise to spatial patterns that are impossible to observe in the classical rock-paper-scissors game. We show that the spatiotemporal dynamics of the system is determined by the density of stable vortices, which may ultimately transform to frozen states, to propagating waves, or to target waves with reversed propagation direction, depending further on the degree and type of randomness in the interactions among the species. If vortices are rare, the fixation to waves and complex oscillatory solutions is likelier. Moreover, annealed randomness in interactions favors the emergence of target waves, while quenched randomness favors collective synchronization. Our results demonstrate that protection spillovers may fundamentally change the dynamics of cyclic dominance in structured populations, and they outline the possibility of programming pattern formation in microbial populations. 	
1610.05070v1	http://arxiv.org/pdf/1610.05070v1	2016	A realistic transport model with pressure dependent parameters for gas   flow in tight porous media with application to determining shale rock   properties	Iftikhar Ali|Nadeem A. Malik	  Shale gas recovery has seen a major boom in recent years due to the increasing global energy demands; but the extraction technologies are very expensive. It is therefore important to develop realistic transport modelling and simulation methods, for porous rocks and porous media, that can compliment the field work. Here, a new nonlinear transport model for single phase gas flow in tight porous media is derived, incorporating many important physical processes that occur in such porous systems: continuous flow, transition flow, slip flow, Knudsen diffusion, adsorption and desorption in to and out of the rock material, and a correction for high flow rates (turbulence). This produces a nonlinear advection-diffusion type of partial differential equation (PDE) with pressure dependent model parameters and associated compressibility coefficients, and highly nonlinear apparent convective flux (velocity) and apparent diffusivity. An important application is to the determination of shale rock properties, such as porosity and permeability, by history matching of the the simulation results to data from pressure-pulse decay tests in a shale rock core sample [Pong K., Ho C., Liu J., Tai Y. Non-linear pressure distribution in uniform microchannels. ASME Fluids Eng. Div. (FED) Vol. 197, 51--56, (1994)]. The estimates of the rock porosity and the permeability from our model simulations are realistic of shale rocks, more realistic than obtained from previous models, and illustrates the potential of the modelling strategy presented here in producing accurate simulations of shale gas flow in tight reservoirs. 	
0209308v1	http://arxiv.org/pdf/cond-mat/0209308v1	2002	Scaling laws of creep rupture of fiber bundles	Ferenc Kun|Raul Cruz Hidalgo|Hans J. Herrmann|Karoly F. Pal	  We study the creep rupture of fiber composites in the framework of fiber bundle models. Two novel fiber bundle models are introduced based on different microscopic mechanisms responsible for the macroscopic creep behavior. Analytical and numerical calculations show that above a critical load the deformation of the creeping system monotonically increases in time resulting in global failure at a finite time $t_f$, while below the critical load the system suffers only partial failure and the deformation tends to a constant value giving rise to an infinite lifetime. It is found that approaching the critical load from below and above the creeping system is characterized by universal power laws when the fibers have long range interaction. The lifetime of the composite above the critical point has a universal dependence on the system size. 	
0305319v3	http://arxiv.org/pdf/cond-mat/0305319v3	2004	Entropy of Pseudo Random Number Generators	Stephan Mertens|Heiko Bauke	  Since the work of Ferrenberg et al.[PRL 69, (1992)] some pseudo random number generators are known to yield wrong results in cluster Monte Carlo simulations. In this contribution the fundamental mechanism behind this failure is discussed. Almost all random number generators calculate a new pseudo random number $x_i$ from preceding values, $x_i = f(x_{i-1}, x_{i-2},..., x_{i-q})$. Failure of these generators in cluster Monte Carlo simulations and related experiments can be attributed to the low entropy of the production rule $f()$ conditioned on the statistics of the input values $x_{i-1},...,x_{i-q}$. Being a measure only of the arithmetic operations in the generator rule, the conditional entropy is independent of the lag in the recurrence or the period of the sequence. In that sense it measures a more profound quality of a random number generator than empirical tests with their limited horizon. 	
0411019v1	http://arxiv.org/pdf/cs/0411019v1	2004	Programmable Ethernet Switches and Their Applications	Srikant Sharma|Tzi-cker Chiueh	  Modern Ethernet switches support many advanced features beyond route learning and packet forwarding such as VLAN tagging, IGMP snooping, rate limiting, and status monitoring, which can be controlled through a programmatic interface. Traditionally, these features are mostly used to statically configure a network. This paper proposes to apply them as dynamic control mechanisms to maximize physical network link resources, to minimize failure recovery time, to enforce QoS requirements, and to support link-layer multicast without broadcasting. With these advanced programmable control mechanisms, standard Ethernet switches can be used as effective building blocks for metropolitan-area Ethernet networks (MEN), storage-area networks (SAN), and computation cluster interconnects. We demonstrate the usefulness of this new level of control over Ethernet switches with a MEN architecture that features multi-fold throughput gains and sub-second failure recovery time. 	
0607201v1	http://arxiv.org/pdf/physics/0607201v1	2006	A two phase harmonic model for left ventricular function	S. Dubi|C. Dubi|Y. Dubi	  A minimal model for mechanical motion of the left ventricle is proposed. The model assumes the left ventricle to be a harmonic oscillator with two distinct phases, simulating the systolic and diastolic phases, at which both the amplitude and the elastic constant of the oscillator are different. Taking into account the pressure within the left ventricle, the model shows qualitative agreement with functional parameters of the left ventricle. The model allows for a natural explanation of heart failure with preserved systolic left ventricular function, also termed diastolic heart failure. Specifically, the rise in left ventricular filling pressures following increased left-ventricular wall stiffness is attributed to a mechanism aimed at preserving heart rate and cardiac output. 	
0106072v6	http://arxiv.org/pdf/quant-ph/0106072v6	2003	"Quantal" behavior in classical probability	K. A. Kirkpatrick	  A number of phenomena generally believed characteristic of quantum mechanics and seen as interpretively problematic--the incompatibility and value-indeterminacy of variables, the non-existence of dispersion-free states, the failure of the standard marginal-probability formula, the failure of the distributive law of disjunction and interference--are exemplified in an emphatically non-quantal system: a deck of playing cards. Thus the appearance, in quantum mechanics, of incompatibility and these associated phenomena requires neither explanation nor interpretation. 	
0705.3694v1	http://arxiv.org/pdf/0705.3694v1	2007	Calculation of The Lifetimes of Thin Stripper Targets Under Bombardment   of Intense Pulsed Ions	S. G. Lebedev|A. S. Lebedev	  The problems of stripper target behavior in the nonstationary intense particle beams are considered. The historical sketch of studying of radiation damage failure of carbon targets under ion bombardment is presented. The simple model of evaporation of a target by an intensive pulsing beam is supposed. Stripper foils lifetimes in the nonstationary intense particle can be described by two failure mechanisms: radiation damage accumulation and evaporation of target. At the maximal temperatures less than 2500K the radiation damage are dominated; at temperatures above 2500K the mechanism of evaporation of a foil prevails. The proposed approach has been applied to the discription of behaviour of stripper foils in the BNL linac and SNS conditions. 	
0705.4321v2	http://arxiv.org/pdf/0705.4321v2	2007	Coulomb oscillations as a remedy for the helium atom	Manfred Bucher	  The largest failure of the old, Bohr-Sommerfeld quantum theory was with the helium atom. It brought about the theory's demise. I show that this failure does not originate, as commonly believed, with the orbit concept per se. Instead, it was caused by the wrong choice of orbits, compounded by ignorance of the exclusion principle. Choosing semiclassical electron oscillations through the He nucleus, I calculate a singlet ground-state energy that rivals in accuracy with quantum-mechanical results. The same method reveals Bohr's historic energy value as the forbidden triplet ground state--a result beyond the reach of quantum mechanics. At the qualitative level, the concept of Coulomb oscillations visually explains the major features in the He double spectrum in terms of crossed or parallel orbit orientation. 	
0903.4534v1	http://arxiv.org/pdf/0903.4534v1	2009	Geometrical and transport properties of single fractures: influence of   the roughness of the fracture walls	Harold Auradou	  This article reviews the main features of the transport properties of single fractures. A particular attention paid to fractures in geological materials which often display a roughness covering a broad range of length scales. Because of the small distance separating the fracture walls, the surface roughness is a key parameter influencing the structure of the void space. Studies devoted to the characterization of the surface roughness are presented as well as works aimed at characterizing the void space geometry. The correlation of the free space is found to be crucially function of the failure mechanism (brittle, quasi brittle or plastic...) but also of possible shear displacements during the failure. The influence of the surface roughness on the mechanical behavior of fractures under a normal load and a shear stress is also described. Finally, experimental, numerical and theoretical works devoted to the study of the influence of the fracture void geometry on the permeability and on the hydrodynamic dispersion of a dissolved species are discussed. 	
0910.0940v3	http://arxiv.org/pdf/0910.0940v3	2010	A homoclinic route to asymptotic full cooperation in adaptive networks   and its failure	Gerd Zschaler|Arne Traulsen|Thilo Gross	  We consider the evolutionary dynamics of a cooperative game on an adaptive network, where the strategies of agents (cooperation or defection) feed back on their local interaction topology. While mutual cooperation is the social optimum, unilateral defection yields a higher payoff and undermines the evolution of cooperation. Although no a priori advantage is given to cooperators, an intrinsic dynamical mechanism can lead asymptotically to a state of full cooperation. In finite systems, this state is characterized by long periods of strong cooperation interrupted by sudden episodes of predominant defection, suggesting a possible mechanism for the systemic failure of cooperation in real-world systems. 	
1008.1361v1	http://arxiv.org/pdf/1008.1361v1	2010	Pressure-Induced Critical Influences on Workpiece-Tool Thermal   Interaction in High Speed Dry Machining of Titanium	H. A. Abdel-Aal|M. El Mansori	  Cutting tools are subject to extreme thermal and mechanical loads during operation. The state of loading is intensified in dry cutting environment especially when cutting the so called hard-to-cut-materials. Although, the effect of mechanical loads on tool failure have been extensively studied, detailed studies on the effect of thermal loads on the deterioration of the cutting tool are rather scarce. In this paper we study failure of coated carbide tools due to thermal loading. The study emphasizes the role assumed by the thermo-physical properties of the tool material in enhancing or preventing mass attrition of the cutting elements within the tool. It is shown that within a comprehensive view of the nature of conduction in the tool zone, thermal conduction is not solely affected by temperature. Rather it is a function of the so called thermodynamic forces. These are the stress, the strain, strain rate, rate of temperature rise, and the temperature gradient. Although that within such consideration description of thermal conduction is non-linear, it is beneficial to employ such a form because it facilitates a full mechanistic understanding of thermal activation of tool wear. 	
1203.4196v3	http://arxiv.org/pdf/1203.4196v3	2012	Mechanical properties of polycrystalline graphene based on a realistic   atomistic model	Jani Kotakoski|Jannik C. Meyer	  Graphene can at present be grown at large quantities only by the chemical vapor deposition method, which produces polycrystalline samples. Here, we describe a method for constructing realistic polycrystalline graphene samples for atomistic simulations, and apply it for studying their mechanical properties. We show that cracks initiate at points where grain boundaries meet and then propagate through grains predominantly in zigzag or armchair directions, in agreement with recent experimental work. Contrary to earlier theoretical predictions, we observe normally distributed intrinsic strength (~ 50% of that of the mono-crystalline graphene) and failure strain which do not depend on the misorientation angles between the grains. Extrapolating for grain sizes above 15 nm results in a failure strain of ~ 0.09 and a Young's modulus of ~ 600 GPa. The decreased strength can be adequately explained with a conventional continuum model when the grain boundary meeting points are identified as Griffith cracks. 	
1307.3370v1	http://arxiv.org/pdf/1307.3370v1	2013	Crackling vs. continuum-like dynamics in brittle failure	Jonathan Barés|Luc Barbier|Daniel Bonamy	  We study how the loading rate, specimen geometry and microstructural texture select the dynamics of a crack moving through an heterogeneous elastic material in the quasi-static approximation. We find a transition, fully controlled by two dimensionless variables, between dynamics ruled by continuum fracture mechanics and crackling dynamics. Selection of the latter by the loading, microstructure and specimen parameters is formulated in terms of scaling laws on the power spectrum of crack velocity. This analysis defines the experimental conditions required to observe crackling in fracture. Beyond failure problems, the results extend to a variety of situations described by models of the same universality class, e.g. the dynamics in wetting or of domain walls in amorphous ferromagnets. 	
1308.1210v2	http://arxiv.org/pdf/1308.1210v2	2014	Cavity-based robustness analysis of interdependent networks: Influences   of intranetwork and internetwork degree-degree correlations	Shunsuke Watanabe|Yoshiyuki Kabashima	  We develop a methodology for analyzing the percolation phenomena of two mutually coupled (interdependent) networks based on the cavity method of statistical mechanics. In particular, we take into account the influence of degree-degree correlations inside and between the networks on the network robustness against targeted attacks and random failures. We show that the developed methodology is reduced to the well-known generating function formalism in the absence of degree-degree correlations. The validity of the developed methodology is confirmed by a comparison with the results of numerical experiments. Our analytical results imply that the robustness of the interdependent networks depends considerably on both the intra- and internetwork degree-degree correlations in the case of targeted attacks, whereas the significance of the degree-degree correlations is relatively low for random failures. 	
1309.3150v2	http://arxiv.org/pdf/1309.3150v2	2013	How (Not) to Shoot in Your Foot with SDN Local Fast Failover: A   Load-Connectivity Tradeoff	Michael Borokhovich|Stefan Schmid	  This paper studies the resilient routing and (in-band) fast failover mechanisms supported in Software-Defined Networks (SDN). We analyze the potential benefits and limitations of such failover mechanisms, and focus on two main metrics: (1) correctness (in terms of connectivity and loop-freeness) and (2) load-balancing. We make the following contributions. First, we show that in the worst-case (i.e., under adversarial link failures), the usefulness of local failover is rather limited: already a small number of failures will violate connectivity properties under any fast failover policy, even though the underlying substrate network remains highly connected. We then present randomized and deterministic algorithms to compute resilient forwarding sets; these algorithms achieve an almost optimal tradeoff. Our worst-case analysis is complemented with a simulation study. 	
1310.8040v1	http://arxiv.org/pdf/1310.8040v1	2013	Homophyly and Randomness Resist Cascading Failure in Networks	Angsheng Li|Wei Zhang|Yicheng Pan|Xuechen Li	  The universal properties of power law and small world phenomenon of networks seem unavoidably obstacles for security of networking systems. Existing models never give secure networks. We found that the essence of security is the security against cascading failures of attacks and that nature solves the security by mechanisms. We proposed a model of networks by the natural mechanisms of homophyly, randomness and preferential attachment. It was shown that homophyly creates a community structure, that homophyly and randomness introduce ordering in the networks, and that homophyly creates inclusiveness and introduces rules of infections. These principles allow us to provably guarantee the security of the networks against any attacks. Our results show that security can be achieved provably by structures, that there is a tradeoff between the roles of structures and of thresholds in security engineering, and that power law and small world property are never obstacles for security of networks. 	
1405.5504v1	http://arxiv.org/pdf/1405.5504v1	2014	Mechanical Properties of Graphene Nanowiggles	R. A. Bizao|T. Botari|D. S. Galvao	  In this work we have investigated the mechanical properties and fracture patterns of some graphene nanowiggles (GNWs). Graphene nanoribbons are finite graphene segments with a large aspect ratio, while GNWs are nonaligned periodic repetitions of graphene nanoribbons. We have carried out fully atomistic molecular dynamics simulations using a reactive force field (ReaxFF), as implemented in the LAMPPS (Large-scale Atomic/Molecular Massively Parallel Simulator) code. Our results showed that the GNW fracture patterns are strongly dependent on the nanoribbon topology and present an interesting behavior, since some narrow sheets have larger ultimate failure strain values. This can be explained by the fact that narrow nanoribbons have more angular freedom when compared to wider ones, which can create a more efficient way to accumulate and to dissipate strain/stress. We have also observed the formation of linear atomic chains (LACs) and some structural defect reconstructions during the material rupture. The reported graphene failure patterns, where zigzag/armchair edge terminated graphene structures are fractured along armchair/zigzag lines, were not observed in the GNW analyzed cases. 	
1511.06472v1	http://arxiv.org/pdf/1511.06472v1	2015	Enhancing the Performance of the T-Peel Test for Thin and Flexible   Adhered Laminates	Nikhil Padhye|David M. Parks|Alexander H. Slocum|Bernhardt L. Trout	  Symmetrically bonded thin and flexible T-peel specimens, when tested on vertical travel machines, can be subject to significant gravitational loading; with the associated asymmetry and mixed-mode failure during peeling. This can cause erroneously high experimental peel forces to be recorded which leads to uncertainty in estimating interfacial fracture toughness and failure mode. To overcome these issues, a mechanical test fixture has been designed for use with vertical test machines, that supports the unpeeled portion of the test specimen and suppresses parasitic loads due to gravity from affecting the peel test. The mechanism, driven by the test machine cross-head, moves at one-half of the velocity of the cross-head such that the unpeeled portion always lies in the plane of the instantaneous center of motion. Several specimens such as bonded polymeric films, laminates, and commercial tapes were tested with and without the fixture, and the importance of the proposed T-peel procedure has been demonstrated. 	
1601.03162v1	http://arxiv.org/pdf/1601.03162v1	2016	Jump-starting coordination in a stag hunt: Motivation, mechanisms, and   their analysis	Ioannis Avramopoulos	  The stag hunt (or assurance game) is a simple game that has been used as a prototype of a variety of social coordination problems (ranging from the social contract to the adoption of technical standards). Players have the option to either use a superior cooperative strategy whose payoff depends on the other players' choices or use an inferior strategy whose payoff is independent of what other players do; the cooperative strategy may incur a loss if sufficiently many other players do not cooperate. Stag hunts have two (strict) pure Nash equilibria, namely, universal cooperation and universal defection (as well as a mixed equilibrium of low predictive value). Selection of the inferior (pure) equilibrium is called a coordination failure. In this paper, we present and analyze using game-theoretic techniques mechanisms aiming to avert coordination failures and incite instead selection of the superior equilibrium. Our analysis is based on the solution concepts of Nash equilibrium, dominance solvability, as well as a formalization of the notion of "incremental deployability," which is shown to be keenly relevant to the sink equilibrium. 	
1609.09411v1	http://arxiv.org/pdf/1609.09411v1	2016	Seismic collapse prediction of frame structures by means of genetic   algorithms	A. Greco|F. Cannizzaro|A. Pluchino	  This paper presents an automatic approach for the evaluation of the plastic load and failure modes of planar frames. The method is based on the generation of elementary collapse mechanisms and on their linear combination aimed at minimizing the collapse load factor. The minimization procedure is efficiently performed by means of genetic algorithms which allow to compute an approximate collapse load factor, and the correspondent failure mode, with sufficient accuracy in a very short computing time. A user-friendly original software in the agent-based programming language Netlogo, here employed for the first time with structural engineering purposes, has been developed showing its great versatility and advantages. Many applications have been performed both with reference to the classical plastic analysis approach, in which all the loads increase proportionally, and with a seismic point of view considering a system of horizontal forces whose magnitude increases while the vertical loads are assumed to be constant. In this latter case a parametric study has been performed aiming at evaluating the influence of some geometric, mechanical and load distribution parameters on the ultimate collapse load of planar frames. 	
1302.0792v3	http://arxiv.org/pdf/1302.0792v3	2014	Probe Scheduling for Efficient Detection of Silent Failures	Edith Cohen|Avinatan Hassidim|Haim Kaplan|Yishay Mansour|Danny Raz|Yoav Tzur	  Most discovery systems for silent failures work in two phases: a continuous monitoring phase that detects presence of failures through probe packets and a localization phase that pinpoints the faulty element(s). This separation is important because localization requires significantly more resources than detection and should be initiated only when a fault is present.   We focus on improving the efficiency of the detection phase, where the goal is to balance the overhead with the cost associated with longer failure detection times. We formulate a general model which unifies the treatment of probe scheduling mechanisms, stochastic or deterministic, and different cost objectives - minimizing average detection time (SUM) or worst-case detection time (MAX).   We then focus on two classes of schedules. {\em Memoryless schedules} -- a subclass of stochastic schedules which is simple and suitable for distributed deployment. We show that the optimal memorlyess schedulers can be efficiently computed by convex programs (for SUM objectives) or linear programs (for MAX objectives), and surprisingly perhaps, are guaranteed to have expected detection times that are not too far off the (NP hard) stochastic optima. {\em Deterministic schedules} allow us to bound the maximum (rather than expected) cost of undetected faults, but like stochastic schedules, are NP hard to optimize. We develop novel efficient deterministic schedulers with provable approximation ratios.   An extensive simulation study on real networks, demonstrates significant performance gains of our memoryless and deterministic schedulers over previous approaches. Our unified treatment also facilitates a clear comparison between different objectives and scheduling mechanisms. 	
9807305v1	http://arxiv.org/pdf/cond-mat/9807305v1	1998	Earthquakes: from chemical alteration to mechanical rupture	D. Sornette	  In the standard rebound theory of earthquakes, elastic deformation energy is progressively stored in the crust until a threshold is reached at which it is suddenly released in an earthquake. We review three important paradoxes, the strain paradox, the stress paradox and the heat flow paradox, that are difficult to account for in this picture, either individually or when taken together. Resolutions of these paradoxes usually call for additional assumptions on the nature of the rupture process (such as novel modes of deformations and ruptures) prior to and/or during an earthquake, on the nature of the fault and on the effect of trapped fluids within the crust at seismogenic depths. We review the evidence for the essential importance of water and its interaction with the modes of deformations. Water is usually seen to have mainly the mechanical effect of decreasing the normal lithostatic stress in the fault core on one hand and to weaken rock materials via hydrolytic weakening and stress corrosion on the other hand. We also review the evidences that water plays a major role in the alteration of minerals subjected to finite strains into other structures in out-of-equilibrium conditions. This suggests novel exciting routes to understand what is an earthquake, that requires to develop a truly multidisciplinary approach involving mineral chemistry, geology, rupture mechanics and statistical physics. 	
0801.0542v1	http://arxiv.org/pdf/0801.0542v1	2008	Experimental microstylolites in quartz and modelling of natural   stylolitic structures	Jean-Pierre Gratier|Laurent Muquet|Riad Hassani|Francois Renard	  Experimental microstylolites have been observed at stressed contacts between quartz grains loaded for several weeks in the presence of an aqueous silica solution, at 350 8C and 50 MPa of differential stress. Stereoscopic analysis of pairs of SEM images yielded a digital elevation model of the surface of the microstylolites. Fourier analyses of these microstylolites reveal a self-affine roughness (with a roughness exponent H of 1.2). Coupled with observations of close interactions between dissolution pits and stylolitic peaks, these data illustrate a possible mechanism for stylolite formation. The complex geometry of stylolite surfaces is imposed by the interplay between the development of dissolution peaks in preferential locations (fast dissolution pits) and the mechanical properties of the solid-fluid-solid interfaces. Simple mechanical modeling expresses the crucial competition that could rule the development of microstylolites: (i) a stress-related process, modeled in terms of the stiffness of springs that activate the heterogeneous dissolution rates of the solid interface, promotes the deflection. In parallel, (ii) the strength of the solid interface, modeled in terms of the stiffness of a membrane, is equivalent to a surface tension that limits the deflection and opposes its development. The modeling produces stylolitic surfaces with characteristic geometries varying from conical to columnar when both the effect of dissolution-rate heterogeneity and the strength properties of the rock are taken into account. A self-affine roughness exponent (Hz1.2) measured on modeled surfaces is comparable with natural stylolites at small length scale and experimental microstylolites. 	
0806.2929v3	http://arxiv.org/pdf/0806.2929v3	2009	Influence of sedimentary layering on tsunami generation	Denys Dutykh|Frédéric Dias	  The present article is devoted to the influence of sediment layers on the process of tsunami generation. The main scope here is to demonstrate and especially quantify the effect of sedimentation on vertical displacements of the seabed due to an underwater earthquake. The fault is modelled as a Volterra-type dislocation in an elastic half-space. The elastodynamics equations are integrated with a finite element method. A comparison between two cases is performed. The first one corresponds to the classical situation of an elastic homogeneous and isotropic half-space, which is traditionally used for the generation of tsunamis. The second test case takes into account the presence of a sediment layer separating the oceanic column from the hard rock. Some important differences are revealed. We conjecture that deformations in the generation region may be amplified by sedimentary deposits, at least for some parameter values. The mechanism of amplification is studied through careful numerical simulations. 	
0812.1028v1	http://arxiv.org/pdf/0812.1028v1	2008	Dynamical Tides in Rotating Planets and Stars	Jeremy Goodman|Claire Lackner	  Tidal dissipation may be important for the internal evolution as well as the orbits of short-period massive planets--hot Jupiters. We revisit a mechanism proposed by Ogilvie and Lin for tidal forcing of inertial waves, which are short-wavelength, low-frequency disturbances restored primarily by Coriolis rather than buoyancy forces. This mechanism is of particular interest for hot Jupiters because it relies upon a rocky core, and because these bodies are otherwise largely convective. Compared to waves excited at the base of the stratified, externally heated atmosphere, waves excited at the core are more likely to deposit heat in the convective region and thereby affect the planetary radius. However, Ogilvie and Lin's results were numerical, and the manner of the wave excitation was not clear. Using WKB methods, we demonstrate the production of short waves by scattering of the equilibrium tide off the core at critical latitudes. The tidal dissipation rate associated with these waves scales as the fifth power of the core radius, and the implied tidal $Q$ is of order ten million for nominal values of the planet's mass, radius, orbital period, and core size. We comment upon an alternative proposal by Wu for exciting inertial waves in an unstratified fluid body by means of compressibility rather than a core. We also find that even a core of rock is unlikely to be rigid. But Ogilvie and Lin's mechanism should still operate if the core is substantially denser than its immediate surroundings. 	
1106.3841v1	http://arxiv.org/pdf/1106.3841v1	2011	Giant strain-sensitivity of acoustic energy dissipation in solids   containing dry and saturated cracks with wavy interfaces	V. Yu. Zaitsev|L. A. Matveev	  Mechanisms of acoustic energy dissipation in heterogeneous solids attract much attention in view of their importance for material characterization, nondestructive testing, and geophysics. Due to the progress in measurement techniques in recent years it has been revealed that rocks can demonstrate extremely high strain sensitivity of seismo-acoustic loss. In particular, it has been found that strains of order $10^{-8}$ produced by lunar and solar tides are capable to cause variations in the seismoacoustic decrement on the order of several percents. Some laboratory data (although obtained for higher frequencies) also indicate the presence of very high dissipative nonlinearity. Conventionally discussed dissipation mechanisms (thermoelastic loss in dry solids, Biot and squirt-type loss in fluid-saturated ones) do not suffice to interpret such data. Here, the dissipation at individual cracks is revised taking into account the influence of wavy asperities of their surfaces quite typical of real cracks, which can drastically change the values of the relaxation frequencies and can result in giant strain sensitivity of the dissipation without the necessity to assume the presence of unrealistically thin (and, therefore, unrealistically soft) cracks. In particular, these mechanisms suggest interpretation for observations of pronounced amplitude modulation of seismo-acoustic waves by tidal strains. 	
1708.09510v1	http://arxiv.org/pdf/1708.09510v1	2017	Desorption of hydrocarbon chains by association with ionic and nonionic   surfactants under flow as a mechanism for enhanced oil recovery	Ketzasmin A. Terrón-Mejía|Roberto López-Rendon|Armando Gama Goicochea	  The need to extract oil from wells where it is embedded on the surfaces of rocks has led to the development of new and improved enhanced oil recovery techniques. One of those is the injection of surfactants with water vapor, which promotes desorption of oil that can then be extracted using pumps, as the surfactants encapsulate the oil in foams. However, the mechanisms that lead to the optimal desorption of oil and the best type of surfactants to carry out desorption are not well known yet, which warrants the need to carry out basic research on this topic. In this work, we report non equilibrium dissipative particle dynamics simulations of model surfactants and oil molecules adsorbed on surfaces, with the purpose of studying the efficiency of the surfactants to desorb hydrocarbon chains, that are found adsorbed over flat surfaces. The model surfactants studied correspond to nonionic and cationic surfactants, and the hydrocarbon desorption is studied as a function of surfactant concentration under increasing Poiseuille flow. We obtain various hydrocarbon desorption isotherms for every model of surfactant proposed, under flow. Nonionic surfactants are found to be the most effective to desorb oil and the mechanisms that lead to this phenomenon are presented and discussed. 	
0807.1943v1	http://arxiv.org/pdf/0807.1943v1	2008	Failure of antibiotic treatment in microbial populations	Patrick De Leenheer|Nick Cogan	  The tolerance of bacterial populations to biocidal or antibiotic treatment has been well documented in both biofilm and planktonic settings. However, there is still very little known about the mechanisms that produce this tolerance. Evidence that small, non-mutant subpopulations of bacteria are not affected by antibiotic challenge has been accumulating and provides an attractive explanation for the failure of typical dosing protocols. Although a dosing challenge can kill all the susceptible bacteria, the remaining persister cells can serve as a source of population regrowth. We give a robust condition for the failure of a periodic dosing protocol for a general chemostat model, which supports the mathematical conclusions and simulations of an earlier, more specialized batch model. Our condition implies that the treatment protocol fails globally, in the sense that a mixed bacterial population will ultimately persist above a level that is independent of the initial composition of the population. We also give a sufficient condition for treatment success, at least for initial population compositions near the steady state of interest, corresponding to bacterial washout. Finally, we investigate how the speed at which the bacteria are wiped out depends on the duration of administration of the antibiotic. We find that this dependence is not necessarily monotone, implying that optimal dosing does not necessarily correspond to continuous administration of the antibiotic. Thus, genuine periodic protocols can be more advantageous in treating a wide variety of bacterial infections. 	
0908.3154v1	http://arxiv.org/pdf/0908.3154v1	2009	Impact of Random Failures and Attacks on Poisson and Power-Law Random   Networks	Clemence Magnien|Matthieu Latapy|Jean-Loup Guillaume	  It appeared recently that the underlying degree distribution of networks may play a crucial role concerning their robustness. Empiric and analytic results have been obtained, based on asymptotic and mean-field approximations. Previous work insisted on the fact that power-law degree distributions induce high resilience to random failure but high sensitivity to attack strategies, while Poisson degree distributions are quite sensitive in both cases. Then, much work has been done to extend these results.   We aim here at studying in depth these results, their origin, and limitations. We review in detail previous contributions and give full proofs in a unified framework, and identify the approximations on which these results rely. We then present new results aimed at enlightening some important aspects. We also provide extensive rigorous experiments which help evaluate the relevance of the analytic results.   We reach the conclusion that, even if the basic results of the field are clearly true and important, they are in practice much less striking than generally thought. The differences between random failures and attacks are not so huge and can be explained with simple facts. Likewise, the differences in the behaviors induced by power-law and Poisson distributions are not as striking as often claimed. 	
1104.3667v1	http://arxiv.org/pdf/1104.3667v1	2011	Reliability-based design optimization using kriging surrogates and   subset simulation	V. Dubourg|B. Sudret|J. -M. Bourinet	  The aim of the present paper is to develop a strategy for solving reliability-based design optimization (RBDO) problems that remains applicable when the performance models are expensive to evaluate. Starting with the premise that simulation-based approaches are not affordable for such problems, and that the most-probable-failure-point-based approaches do not permit to quantify the error on the estimation of the failure probability, an approach based on both metamodels and advanced simulation techniques is explored. The kriging metamodeling technique is chosen in order to surrogate the performance functions because it allows one to genuinely quantify the surrogate error. The surrogate error onto the limit-state surfaces is propagated to the failure probabilities estimates in order to provide an empirical error measure. This error is then sequentially reduced by means of a population-based adaptive refinement technique until the kriging surrogates are accurate enough for reliability analysis. This original refinement strategy makes it possible to add several observations in the design of experiments at the same time. Reliability and reliability sensitivity analyses are performed by means of the subset simulation technique for the sake of numerical efficiency. The adaptive surrogate-based strategy for reliability estimation is finally involved into a classical gradient-based optimization algorithm in order to solve the RBDO problem. The kriging surrogates are built in a so-called augmented reliability space thus making them reusable from one nested RBDO iteration to the other. The strategy is compared to other approaches available in the literature on three academic examples in the field of structural mechanics. 	
1105.5329v1	http://arxiv.org/pdf/1105.5329v1	2011	Ising model for distribution networks	H. Hooyberghs|S. Van Lombeek|C. Giuraniuc|B. Van Schaeybroeck|J. O. Indekeu	  An elementary Ising spin model is proposed for demonstrating cascading failures (break-downs, blackouts, collapses, avalanches, ...) that can occur in realistic networks for distribution and delivery by suppliers to consumers. A ferromagnetic Hamiltonian with quenched random fields results from policies that maximize the gap between demand and delivery. Such policies can arise in a competitive market where firms artificially create new demand, or in a solidary environment where too high a demand cannot reasonably be met. Network failure in the context of a policy of solidarity is possible when an initially active state becomes metastable and decays to a stable inactive state. We explore the characteristics of the demand and delivery, as well as the topological properties, which make the distribution network susceptible of failure. An effective temperature is defined, which governs the strength of the activity fluctuations which can induce a collapse. Numerical results, obtained by Monte Carlo simulations of the model on (mainly) scale-free networks, are supplemented with analytic mean-field approximations to the geometrical random field fluctuations and the thermal spin fluctuations. The role of hubs versus poorly connected nodes in initiating the breakdown of network activity is illustrated and related to model parameters. 	
1202.5931v1	http://arxiv.org/pdf/1202.5931v1	2012	Predicting effects of structural stress in a genome-reduced model   bacterial metabolism	Oriol Güell|Francesc Sagués|M. Ángeles Serrano	  We studied in silico effects of structural stress in Mycoplasma pneumoniae, a genome-reduced model bacterial organism, by tracking the damage propagating on its metabolic network after a deleterious perturbation. First, we analyzed failure cascades spreading from individual reactions and pairs of reactions and compared the results to those in Staphylococcus aureus and Escherichia coli. To alert to the potential damage caused by the failure of individual reactions, we propose a generic predictor based on local information that identifies target reactions for structural vulnerability. With respect to the simultaneous failure of pairs of reactions, we detected strong non-linear amplification effects that can be predicted by the presence of specific motifs in the intersection of single cascades. We further connected the metabolic and gene co-expression networks of M. pneumoniae through enzyme activities, and studied the consequences of knocking out individual genes and clusters of genes. Damage caused by single gene knockouts reveals a strong correlation between genome-scale cascades of large impact and gene essentiality. At the same time, we found that genes controlling high-damage reactions tend to operate in functional isolation, as a metabolic protection mechanism. We conclude that the architecture of M. pneumoniae, both at the level of metabolism and genome, seems to have evolved towards increased structural robustness, similarly to other more complex model bacterial organisms, despite its reduced genome size and its greater metabolic network linearity. Our approach, although motivated biochemically, is generic enough to be of potential use toward analyzing and predicting spreading of structural stress in any bipartite complex network. 	
1206.3838v1	http://arxiv.org/pdf/1206.3838v1	2012	On The Recovery Performance of Single- and Multipath OLSR in Wireless   Multi-Hop Networks	Inès Doghri|Laurent Reynaud|Isabelle Guérin-Lassous	  In this paper, we study and improve the recovery properties of single and multipath routing strategies when facing network failure situations. In particular, we focus our study on two MANET routing protocols: OLSR and its multipath extension MP-OLSR. In various wireless multi-hop network environments, especially in multiple chain topologies, we define and seek to evaluate the latency introduced by these protocols to find a new path after a link failure. Theoretical estimations and simulation results show that, under dual chain-topologies, this latency can be too long and incompatible with the needs of loss and delay constrained applications. As the source nodes cannot detect link failures immediately because of the delay incurred by the well-known nature of link state protocols in general, and of OLSR Topology Control (TC) messages in particular, these nodes keep sending packets along broken paths. We thus study the inconsistencies between the actual network topology and the nodes' own representation. After analyzing the consequences of this long latency, we seek to alleviate these problems with the introduction of adapted mechanisms. We propose three new different schemes and accordingly extend the original OLSR and MP-OLSR protocols in order to decrease the expected latency and improve the protocol performance. Simulation results show a steep decrease of the latency when using these new schemes in dual chain-topologies. We also discuss these results in terms of packet loss, end-to-end delay and overhead. 	
1210.4954v2	http://arxiv.org/pdf/1210.4954v2	2013	Optimal Reliability in Design for Fatigue Life	Hanno Gottschalk|Sebastian Schmitz	  The failure of a component often is the result of a degradation process that originates with the formation of a crack. Fatigue describes the crack formation in the material under cyclic loading. Activation and deactivation operations of technical units are important examples in engineering where fatigue and especially low-cycle fatigue (LCF) play an essential role. A significant scatter in fatigue life for many materials results in the necessity of advanced probabilistic models for fatigue. Moreover, optimization of reliability is of vital interest in engineering, where with respect to fatigue the cost functionals are motivated by the predicted probability for the integrity of the component after a certain number of load cycles. The natural mathematical language to model failure, here understood as crack initiation, is the language of spatio-temporal point processes and their first failure times. This translates the problem of optimal reliability in the framework of shape optimization. The cost functionals derived in this way for realistic optimal reliability problems are too singular to be $H^1$-lower semi-continuous as many damage mechanisms, like LCF, lead to crack initiation as a function of the stress at the component's surface. Realistic crack formation models therefore impose a new challenge to the theory of shape optimization. In this work, we have to modify the existence proof of optimal shapes, for the case of sufficiently smooth shapes using elliptic regularity, uniform Schauder estimates and compactness of strong solutions via the Arzela-Ascoli theorem. This result applies to a variety of crack initiation models and in particular applies to a recent probabilistic model for LCF. 	
1306.3416v1	http://arxiv.org/pdf/1306.3416v1	2013	Percolation of a general network of networks	Jianxi Gao|Sergey V. Buldyrev|H. Eugene Stanley|Xiaoming Xu|Shlomo Havlin	  Percolation theory is an approach to study vulnerability of a system. We develop analytical framework and analyze percolation properties of a network composed of interdependent networks (NetONet). Typically, percolation of a single network shows that the damage in the network due to a failure is a continuous function of the fraction of failed nodes. In sharp contrast, in NetONet, due to the cascading failures, the percolation transition may be discontinuous and even a single node failure may lead to abrupt collapse of the system. We demonstrate our general framework for a NetONet composed of $n$ classic Erd\H{o}s-R\'{e}nyi (ER) networks, where each network depends on the same number $m$ of other networks, i.e., a random regular network of interdependent ER networks. In contrast to a \emph{treelike} NetONet in which the size of the largest connected cluster (mutual component) depends on $n$, the loops in the RR NetONet cause the largest connected cluster to depend only on $m$. We also analyzed the extremely vulnerable feedback condition of coupling. In the case of ER networks, the NetONet only exhibits two phases, a second order phase transition and collapse, and there is no first phase transition regime unlike the no feedback condition. In the case of NetONet composed of RR networks, there exists a first order phase transition when $q$ is large and second order phase transition when $q$ is small. Our results can help in designing robust interdependent systems. 	
1310.8388v1	http://arxiv.org/pdf/1310.8388v1	2013	Provable Security of Networks	Angsheng Li|Yicheng Pan|Wei Zhang	  We propose a definition of {\it security} and a definition of {\it robustness} of networks against the cascading failure models of deliberate attacks and random errors respectively, and investigate the principles of the security and robustness of networks. We propose a {\it security model} such that networks constructed by the model are provably secure against any attacks of small sizes under the cascading failure models, and simultaneously follow a power law, and have the small world property with a navigating algorithm of time complex $O(\log n)$. It is shown that for any network $G$ constructed from the security model, $G$ satisfies some remarkable topological properties, including: (i) the {\it small community phenomenon}, that is, $G$ is rich in communities of the form $X$ of size poly logarithmic in $\log n$ with conductance bounded by $O(\frac{1}{|X|^{\beta}})$ for some constant $\beta$, (ii) small diameter property, with diameter $O(\log n)$ allowing a navigation by a $O(\log n)$ time algorithm to find a path for arbitrarily given two nodes, and (iii) power law distribution, and satisfies some probabilistic and combinatorial principles, including the {\it degree priority theorem}, and {\it infection-inclusion theorem}. By using these principles, we show that a network $G$ constructed from the security model is secure for any attacks of small scales under both the uniform threshold and random threshold cascading failure models. Our security theorems show that networks constructed from the security model are provably secure against any attacks of small sizes, for which natural selections of {\it homophyly, randomness} and {\it preferential attachment} are the underlying mechanisms. 	
1503.03774v1	http://arxiv.org/pdf/1503.03774v1	2015	Record breaking bursts in a fiber bundle model of creep rupture	Zsuzsa Danku|Ferenc Kun	  We investigate the statistics of record breaking events in the time series of crackling bursts in a fiber bundle model of the creep rupture of heterogeneous materials. In the model fibers break due to two mechanisms: slowly accumulating damage triggers bursts of immediate breakings analogous to acoustic emissions in experiments. The rupture process accelerates such that the size of breaking avalanches increases while the waiting time between consecutive events decreases towards failure. Record events are defined as bursts which have a larger size than all previous events in the time series. We analyze the statistics of records focusing on the limit of equal load sharing (ELS) of the model and compare the results to the record statistics of sequences of independent identically distributed random variables. Computer simulations revealed that the number of records grows with the logarithm of the event number except for the close vicinity of macroscopic failure where an exponential dependence is evidenced. The two regimes can be attributed to the dominance of disorder with small burst sizes and to stress enhancements giving rise efficient triggering of extended bursts, respectively. Both the size of records and the increments between consecutive record events are characterized by power law distributions with a common exponent 1.33 significantly different from the usual ELS burst size exponents of fiber bundles. The distribution of waiting times follows the same behavior, however, with two distinct exponents for low and high loads. Studying the evolution of records we identify a load dependent characteristic scale of the system which separates slow down and acceleration of record breaking as failure is approached. 	
1601.06230v1	http://arxiv.org/pdf/1601.06230v1	2016	Coping with Prospective Memory Failures: An Optimal Reminder System   Design	Jinghua Hou	  Forgetting is in common in daily life, and 50-80% everyday's forgetting is due to prospective memory failures, which have significant impacts on our life. More seriously, some of these memory lapses can bring fatal consequences such as forgetting a sleeping infant in the back seat of a car. People tend to use various techniques to improve their prospective memory performance. Setting up a reminder is one of the most important techniques. The existing studies provide evidences in support of using reminders to cope with prospective memory failures. However, people are not satisfied with existing reminders because of their limitations in different aspects including reliability, optimization, and adaption.   Through analysing the functions and features of existing reminder systems, this book draft summarizes their advantages and limitations. We are motivated to improve the performance of reminder systems. For the improvements, the relevant theories and mechanisms of prospective memory from psychology must be complied with, incorporated, and applied in this new study.   Based on the literature review, a new reminder model is proposed, which includes a novel reminder planer, a prospective memory based agent, and a personalized user model. The reminder planer is responsible for determining the optimal reminder plan (including the optimal number of reminders, the optimal reminding schedule and the optimal reminding way). The prospective memory agent is responsible for executing the reminding processes. The personalized user model is proposed to learn from users' behaviors and preferences based on human-system interactions and is responsible for adapting the reminder plan to meet users' preferences as much as possible. 	
1601.06496v1	http://arxiv.org/pdf/1601.06496v1	2016	Lightweight Fault Tolerance in Large-Scale Distributed Graph Processing	Da Yan|James Cheng|Fan Yang	  The success of Google's Pregel framework in distributed graph processing has inspired a surging interest in developing Pregel-like platforms featuring a user-friendly "think like a vertex" programming model. Existing Pregel-like systems support a fault tolerance mechanism called checkpointing, which periodically saves computation states as checkpoints to HDFS, so that when a failure happens, computation rolls back to the latest checkpoint. However, a checkpoint in existing systems stores a huge amount of data, including vertex states, edges, and messages sent by vertices, which significantly degrades the failure-free performance. Moreover, the high checkpointing cost prevents frequent checkpointing, and thus recovery has to replay all the computations from a state checkpointed some time ago.   In this paper, we propose a novel checkpointing approach which only stores vertex states and incremental edge updates to HDFS as a lightweight checkpoint (LWCP), so that writing an LWCP is typically tens of times faster than writing a conventional checkpoint. To recover from the latest LWCP, messages are generated from the vertex states, and graph topology is recovered by replaying incremental edge updates. We show how to realize lightweight checkpointing with minor modifications of the vertex-centric programming interface. We also apply the same idea to a recently-proposed log-based approach for fast recovery, to make it work efficiently in practice by significantly reducing the cost of garbage collection of logs. Extensive experiments on large real graphs verified the effectiveness of LWCP in improving both failure-free performance and the performance of recovery. 	
1605.01994v2	http://arxiv.org/pdf/1605.01994v2	2016	Rolex: Resilience-Oriented Language Extensions for Extreme-Scale Systems	Saurabh Hukerikar|Robert F. Lucas	  Future exascale high-performance computing (HPC) systems will be constructed from VLSI devices that will be less reliable than those used today, and faults will become the norm, not the exception. This will pose significant problems for system designers and programmers, who for half-a-century have enjoyed an execution model that assumed correct behavior by the underlying computing system. The mean time to failure (MTTF) of the system scales inversely to the number of components in the system and therefore faults and resultant system level failures will increase, as systems scale in terms of the number of processor cores and memory modules used. However every error detected need not cause catastrophic failure. Many HPC applications are inherently fault resilient. Yet it is the application programmers who have this knowledge but lack mechanisms to convey it to the system.   In this paper, we present new Resilience Oriented Language Extensions (Rolex) which facilitate the incorporation of fault resilience as an intrinsic property of the application code. We describe the syntax and semantics of the language extensions as well as the implementation of the supporting compiler infrastructure and runtime system. Our experiments show that an approach that leverages the programmer's insight to reason about the context and significance of faults to the application outcome significantly improves the probability that an application runs to a successful conclusion. 	
1705.09829v4	http://arxiv.org/pdf/1705.09829v4	2017	Universality and scaling laws in the cascading failure model with   healing	Marcell Stippinger|János Kertész	  Cascading failures may lead to dramatic collapse in interdependent networks, where the breakdown takes place as a discontinuity of the order parameter. In the cascading failure (CF) model with healing there is a control parameter which at some value suppresses the discontinuity of the order parameter. However, up to this value of the healing parameter the breakdown is a hybrid transition, meaning that, besides this first order character, the transition shows scaling too. In this paper we investigate the question of universality related to the scaling behavior. Recently we showed that the hybrid phase transition in the original CF model has two sets of exponents describing respectively the order parameter and the cascade statistics, which are connected by a scaling law. In the CF model with healing we measure these exponents as a function of the healing parameter. We find two universality classes: In the wide range below the critical healing value the exponents agree with those of the original model, while above this value the model displays trivial scaling meaning that fluctuations follow the central limit theorem. 	
1710.07845v1	http://arxiv.org/pdf/1710.07845v1	2017	Seamless Paxos Coordinators	Gustavo M. D. Vieira|Islene C. Garcia|Luiz E. Buzato	  The Paxos algorithm requires a single correct coordinator process to operate. After a failure, the replacement of the coordinator may lead to a temporary unavailability of the application implemented atop Paxos. So far, this unavailability has been addressed by reducing the coordinator replacement rate through the use of stable coordinator selection algorithms. We have observed that the cost of recovery of the newly elected coordinator's state is at the core of this unavailability problem. In this paper we present a new technique to manage coordinator replacement that allows the recovery to occur concurrently with new consensus rounds. Experimental results show that our seamless approach effectively solves the temporary unavailability problem, its adoption entails uninterrupted execution of the application. Our solution removes the restriction that the occurrence of coordinator replacements is something to be avoided, allowing the decoupling of the application execution from the accuracy of the mechanism used to choose a coordinator. This result increases the performance of the application even in the presence of failures, it is of special importance to the autonomous operation of replicated applications that have to adapt to varying network conditions and partial failures. 	
1801.04668v1	http://arxiv.org/pdf/1801.04668v1	2018	The decoding failure probability of MDPC codes	Jean-Pierre Tillich	  Moderate Density Parity Check (MDPC) codes are defined here as codes which have a parity-check matrix whose row weight is $O(\sqrt{n})$ where $n$ is the length $n$ of the code. They can be decoded like LDPC codes but they decode much less errors than LDPC codes: the number of errors they can decode in this case is of order $\Theta(\sqrt{n})$. Despite this fact they have been proved very useful in cryptography for devising key exchange mechanisms. They have also been proposed in McEliece type cryptosystems. However in this case, the parameters that have been proposed in \cite{MTSB13} were broken in \cite{GJS16}. This attack exploits the fact that the decoding failure probability is non-negligible. We show here that this attack can be thwarted by choosing the parameters in a more conservative way. We first show that such codes can decode with a simple bit-flipping decoder any pattern of $O\left(\frac{\sqrt{n} \log \log n}{\log n}\right)$ errors. This avoids the previous attack at the cost of significantly increasing the key size of the scheme. We then show that under a very reasonable assumption the decoding failure probability decays almost exponentially with the codelength with just two iterations of bit-flipping. With an additional assumption it has even been proved that it decays exponentially with an unbounded number of iterations and we show that in this case the increase of the key size which is required for resisting to the attack of \cite{GJS16} is only moderate. 	
1306.6286v1	http://arxiv.org/pdf/1306.6286v1	2013	Photo-Disintegration of the Iron Nucleus in Fractured Magnetite Rocks   with Magnetostriction	A. Widom|J. Swain|Y. N. Srivastava	  There has been considerable interest in recent experiments on iron nuclear disintegrations observed when rocks containing such nuclei are crushed and fractured. The resulting nuclear transmutations are particularly strong for the case of magnetite rocks, i.e. loadstones. We argue that the fission of the iron nucleus is a consequence of photo-disintegration. The electro-strong coupling between electromagnetic fields and nuclear giant dipole resonances are central for producing observed nuclear reactions. The large electron energies produced during the fracture of piezomagnetic rocks are closely analogous to the previously discussed case of the fracture of piezoelectric rocks. In both cases electro-weak interactions can produce neutrons and neutrinos from energetic protons and electrons thus inducing nuclear transmutations. The electro-strong condensed matter coupling discussed herein represents new many body collective nuclear photo-disintegration effects. 	
1403.3083v2	http://arxiv.org/pdf/1403.3083v2	2014	A Novel Method to Extract Rocks from Mars Images	Shuliang Wang|Yasen Chen	  In this paper, a novel method is proposed to extract rocks from Martian surface images by using 8 data field. It models the interaction between two pixels of an image in the context of imagery 9 characteristics. First, foreground rocks are differed from background information by binarizing 10 image on roughly partitioned images. Second, foreground rocks are grouped into clusters by 11 locating the centers and edges of clusters in data field via hierarchical grids. Third, the target 12 rocks are discovered for the Mars Exploration Rover (MER) to keep healthy paths. The 13 experiment with images taken by MER shows the proposed method is practical and potential. 	
1502.03067v1	http://arxiv.org/pdf/1502.03067v1	2015	Multiporosity Flow in Fractured Low-Permeability Rocks	Kristopher L. Kuhlman|Bwalya Malama|Jason E. Heath	  A multiporosity extension of classical double and triple porosity fractured rock flow models for slightly compressible fluids is presented. The multiporosity model is an adaptation of the multirate solute transport model of Haggerty and Gorelick (1995) to viscous flow in fractured rock reservoirs. It is a generalization of both pseudo-steady-state and transient interporosity flow double porosity models. The model includes a fracture continuum and an overlapping distribution of multiple rock matrix continua, whose fracture-matrix exchange coefficients are specified through a discrete probability mass function. Semi-analytical cylindrically symmetric solutions to the multiporosity mathematical model are developed using the Laplace transform to illustrate its behavior. The multiporosity model presented here is conceptually simple, yet flexible enough to simulate common conceptualizations of double and triple porosity flow. This combination of generality and simplicity makes the multiporosity model a good choice for flow in low-permeability fractured rocks. 	
1602.03854v1	http://arxiv.org/pdf/1602.03854v1	2016	Estimating the unconfined compressive strength of carbonate rocks using   gene expression programming	Saeid R. Dindarloo|Elnaz Siami-Irdemoosa	  Conventionally, many researchers have used both regression and black box techniques to estimate the unconfined compressive strength (UCS) of different rocks. The advantage of the regression approach is that it can be used to render a functional relationship between the predictive rock indices and its UCS. The advantage of the black box techniques is in rendering more accurate predictions. Gene expression programming (GEP) is proposed, in this study, as a robust mathematical alternative for predicting the UCS of carbonate rocks. The two parameters of total porosity and P-wave speed were selected as predictive indices. The proposed GEP model had the advantage of the both traditionally used approaches by proposing a mathematical model, similar to a regression, while keeping the prediction errors as low as the black box methods. The GEP outperformed both artificial neural networks and support vector machines in terms of yielding more accurate estimates of UCS. Both the porosity and the P-wave velocity were sufficient predictive indices for estimating the UCS of the carbonate rocks in this study. Nearly, 95% of the observed variation in the UCS values was explained by these two parameters (i.e., R2 =95%). 	
1406.1701v1	http://arxiv.org/pdf/1406.1701v1	2014	A computational study of the effects of remodelled electrophysiology and   mechanics on initiation of ventricular fibrillation in human heart failure	Nathan Kirk|Alan Benson|Christopher Goodyer|Matthew Hubbard	  The study of pathological cardiac conditions such as arrhythmias, a major cause of mortality in heart failure, is becoming increasingly informed by computational simulation, numerically modelling the governing equations. This can provide insight where experimental work is constrained by technical limitations and/or ethical issues.   As the models become more realistic, the construction of efficient and accurate computational models becomes increasingly challenging. In particular, recent developments have started to couple the electrophysiology models with mechanical models in order to investigate the effect of tissue deformation on arrhythmogenesis, thus introducing an element of nonlinearity into the mathematical representation. This paper outlines a biophysically-detailed computational model of coupled electromechanical cardiac activity which uses the finite element method to approximate both electrical and mechanical systems on unstructured, deforming, meshes. An ILU preconditioner is applied to improve performance of the solver.   This software is used to examine the role of electrophysiology, fibrosis and mechanical deformation on the stability of spiral wave dynamics in human ventricular tissue by applying it to models of both healthy and failing tissue. The latter was simulated by modifying (i) cellular electrophysiological properties, to generate an increased action potential duration and altered intracellular calcium handling, and (ii) tissue-level properties, to simulate the gap junction remodelling, fibrosis and increased tissue stiffness seen in heart failure. The resulting numerical experiments suggest that, for the chosen mathematical models of electrophysiology and mechanical response, introducing tissue level fibrosis can have a destabilising effect on the dynamics, while the net effect of the electrophysiological remodelling stabilises the system. 	
0205130v1	http://arxiv.org/pdf/cond-mat/0205130v1	2002	Chemical fracture and distribution of extreme values	A. Baldassarri|A. Gabrielli|B. Sapoval	  When a corrosive solution reaches the limits of a solid sample, a chemical fracture occurs. An analytical theory for the probability of this chemical fracture is proposed and confirmed by extensive numerical experiments on a two dimensional model. This theory follows from the general probability theory of extreme events given by Gumbel. The analytic law differs from the Weibull law commonly used to describe mechanical failures for brittle materials. However a three parameters fit with the Weibull law gives good results, confirming the empirical value of this kind of analysis. 	
0412195v1	http://arxiv.org/pdf/quant-ph/0412195v1	2004	Complementarity and Scientific Rationality	Simon Saunders	  Bohr's interpretation of quantum mechanics has been criticized as incoherent and opportunistic, and based on doubtful philosophical premises. If so Bohr's influence, in the pre-war period of 1927-1939, is the harder to explain, and the acceptance of his approach to quantum mechanics over de Broglie's had no reasonable foundation. But Bohr's interpretation changed little from the time of its first appearance, and stood independent of any philosophical presuppositions. The principle of complementarity is itself best read as a conjecture of unusually wide scope, on the nature and future course of explanations in the sciences (and not only the physical sciences). If it must be judged a failure today, it is not because of any internal inconsistency. 	
0708.3298v2	http://arxiv.org/pdf/0708.3298v2	2007	Collapse times for attractive Bose-Einstein condensates	Esteban Calzetta	  We argue that the main mechanism for condensate collapse in an attractive Bose-Einstein condensate is the loss of coherence between atoms a finite distance apart, rather than the growth of the occupation number in noncondensate modes. Since the former mechanism is faster than the latter by a factor of approximately 3/2, this helps to dispel the apparent failure of field theoretical models in predicting the collapse time of the condensate. 	
0902.1424v1	http://arxiv.org/pdf/0902.1424v1	2009	Non-monotonic dependence of the rupture force in polymer chains on their   lengths	S. Fugmann|I. M. Sokolov	  We consider the rupture dynamics of a homopolymer chain pulled at one end at a constant loading rate. Our model of the breakable polymer is related to the Rouse chain, with the only difference that the interaction between the monomers is described by the Morse potential instead of the harmonic one, and thus allows for mechanical failure. We show that in the experimentally relevant domain of parameters the dependence of the most probable rupture force on the chain length may be non-monotonic, so that the medium-length chains break easier than the short and the long ones. The qualitative theory of the effect is presented. 	
0904.3746v2	http://arxiv.org/pdf/0904.3746v2	2009	Breakdown of thermalization in finite one-dimensional systems	Marcos Rigol	  We use quantum quenches to study the dynamics and thermalization of hardcore bosons in finite one-dimensional lattices. We perform exact diagonalizations and find that, far away from integrability, few-body observables thermalize. We then study the breakdown of thermalization as one approaches an integrable point. This is found to be a smooth process in which the predictions of standard statistical mechanics continuously worsen as the system moves toward integrability. We establish a direct connection between the presence or absence of thermalization and the validity or failure of the eigenstate thermalization hypothesis, respectively. 	
0906.3244v1	http://arxiv.org/pdf/0906.3244v1	2009	Mechanical, Electrical, and Magnetic Properties of Ni Nanocontacts	M. R. Calvo|M. J. Caturla|D. Jacob|C. Untiedt|J. J. Palacios	  The dynamic deformation upon stretching of Ni nanowires as those formed with mechanically controllable break junctions or with a scanning tunneling microscope is studied both experimentally and theoretically. Molecular dynamics simulations of the breaking process are performed. In addition, and in order to compare with experiments, we also compute the transport properties in the last stages before failure using the first-principles implementation of Landauer's formalism included in our transport package ALACANT. 	
1106.0689v3	http://arxiv.org/pdf/1106.0689v3	2012	Mechanical properties of carbynes investigated by ab initio total-energy   calculations	Ivano E. Castelli|Paolo Salvestrini|Nicola Manini	  As sp carbon chains (carbynes) are relatively rigid molecular objects, can we exploit them as construction elements in nanomechanics? To answer this question, we investigate their remarkable mechanical properties by ab-initio total-energy simulations. In particular, we evaluate their linear response to small longitudinal and bending deformations and their failure limits for longitudinal compression and elongation. 	
1302.5418v1	http://arxiv.org/pdf/1302.5418v1	2013	Failure of the Bell Locality Condition over a Space of Ideal Particles   and their Paths	Warren Leffler	  We construct a space of ideal elements (particles and their paths) to analyze certain aspects of quantum physics. The particles are taken from a model of particle interaction first described by David Deutsch (based on a different but related framework, that of MWI), and the paths are based on Richard Feynman's path-integral formulation of quantum mechanics. By combining the two systems we develop a new approach to quantum mechanics that eliminates various quantum paradoxes. 	
1310.3762v1	http://arxiv.org/pdf/1310.3762v1	2013	Vortex distribution in a confining potential	Yan Levin|Matheus Girotto|Alexandre Pereira dos Santos	  We study a model of interacting vortices in a type II superconductor. In the weak coupling limit, we constructed a mean-field theory which allows us to accurately calculate the vortex density distribution inside a confining potential. In the strong coupling limit, the correlations between the particles become important and the mean-field theory fails. Contrary to recent suggestions, this does not imply failure of the Boltzmann-Gibbs statistical mechanics, as we clearly demonstrate by comparing the results of Molecular Dynamics and Monte Carlo simulations. 	
1311.7190v2	http://arxiv.org/pdf/1311.7190v2	2014	Disintegration of graphene nanoribbons in large electrostatic fields	Haiming Huang|Zhibing Li|H. J. Kreuzer|Weiliang Wang	  The deformation and disintegration of a graphene nanoribbon under external electrostatic fields are investigated by first principle quantum mechanical calculations to establish its stability range. The zigzag edges terminated by various functional groups are considered. By analyzing the phonon spectrum, the critical fracture field for each edge structure is obtained. It is found that different terminal groups on the zigzag graphene nanoribbons lead to different fracture patterns at different fracture fields. The failure mechanism is demonstrated to rely on both the carbon bond alternation feature across the ribbon and terminal group electronegativity. 	
1404.0073v1	http://arxiv.org/pdf/1404.0073v1	2014	General dynamic recovery for compensating CSP	Abeer S. Al-Humaimeedy|Maribel Fernández	  Compensation is a technique to roll-back a system to a consistent state in case of failure. Recovery mechanisms for compensating calculi specify the order of execution of compensation sequences. Dynamic recovery means that the order of execution is determined at runtime. In this paper, we define an extension of Compensating CSP, called DEcCSP, with general dynamic recovery. We provide a formal, operational semantics for the calculus, and illustrate its expressive power with a case study. In contrast with previous versions of Compensating CSP, DEcCSP provides mechanisms to replace or discard compensations at runtime. Additionally, we bring back to DEcCSP standard CSP operators that are not available in other compensating CSP calculi, and introduce channel communication. 	
1409.3682v1	http://arxiv.org/pdf/1409.3682v1	2014	A novel recovery mechanism enabling fine-granularity locking and fast,   REDO-only recovery	Caetano Sauer|Theo Härder	  We present a series of novel techniques and algorithms for transaction commit, logging, recovery, and propagation control. In combination, they provide a recovery component that maintains the persistent state of the database (both log and data pages) always in a committed state. Recovery from system and media failures only requires only REDO operations, which can happen concurrently with the processing of new transactions. The mechanism supports fine-granularity locking, partial rollbacks, and snapshot isolation for reader transactions. Our design does not assume a specific hardware configuration such as non-volatile RAM or flash---it is designed for traditional disk environments. Nevertheless, it can exploit modern I/O devices for higher transaction throughput and reduced recovery time with a high degree of flexibility. 	
1604.02526v1	http://arxiv.org/pdf/1604.02526v1	2016	Practical Recovery Solution for Information Loss in Real-Time Network   Environment	Hengky Susanto|ByungGuk Kim	  Feedback mechanism based algorithms are frequently used to solve network optimization problems. These schemes involve users and network exchanging information (e.g. requests for bandwidth allocation and pricing) to achieve convergence towards an optimal solution. However, in the implementation, these algorithms do not guarantee that messages will be delivered to the destination when network congestion occurs. This in turn often results in packet drops, which may cause information loss, and this condition may lead to algorithm failing to converge. To prevent this failure, we propose least square (LS) estimation algorithm to recover the missing information when packets are dropped from the network. The simulation results involving several scenarios demonstrate that LS estimation can provide the convergence for feedback mechanism based algorithm. 	
1604.03811v1	http://arxiv.org/pdf/1604.03811v1	2016	Microstructural modeling of ductile fracture initiation in multi-phase   materials	T. W. J. de Geus|R. H. J. Peerlings|M. G. D. Geers	  The precise mechanisms underlying the failure of multi-phase materials may be strongly dependent on the material's microstructural morphology. Micromechanical modeling has provided much insight into this dependence, but uncertainties remain about crucial modeling assumptions. This paper assesses the influence of different grain shapes, damage indicators, and stress states using a structured numerical model. A distinct spatial arrangement of phases around fracture incidents is found, consisting of hard regions in the tensile direction interrupted by soft regions in the directions of shear. These key features are only mildly sensitive to the studied variations. 	
1802.03460v1	http://arxiv.org/pdf/1802.03460v1	2018	Electrochemical and mechanical behaviors of dissimilar friction stir   welding between 5086 and 6061 aluminum alloy	Zhitong Chen|Shengxi Li|Lloyd H. Hihara	  The electrochemical behavior and mechanical properties of friction stir welded AA5086 and AA6061 Al alloys were investigated. Micro-hardness measurements and tensile tests showed that the heat-affected zone (HAZ) in AA6061 had minimum hardness value (i.e., 88 HV) and served as failure site in the dissimilar weld. Corrosion testing revealed that the minimum value of Icorr appeared in the HAZ 5086 (0.54 uA/cm2) and HAZ 5086 was most resistant to corrosion. The AA 5086 side of the weld showed better corrosion resistance than the AA 6061 side. 	
1312.1993v4	http://arxiv.org/pdf/1312.1993v4	2014	Enhancing resilience of interdependent networks by healing	Marcell Stippinger|János Kertész	  Interdependent networks are characterized by two kinds of interactions: The usual connectivity links within each network and the dependency links coupling nodes of different networks. Due to the latter links such networks are known to suffer from cascading failures and catastrophic breakdowns. When modeling these phenomena, usually one assumes that a fraction of nodes gets damaged in one of the networks, which is followed possibly by a cascade of failures. In real life the initiating failures do not occur at once and effort is made replace the ties eliminated due to the failing nodes. Here we study a dynamic extension of the model of interdependent networks and introduce the possibility of link formation with a probability w, called healing, to bridge non-functioning nodes and enhance network resilience. A single random node is removed, which may initiate an avalanche. After each removal step healing sets in resulting in a new topology. Then a new node fails and the process continues until the giant component disappears either in a catastrophic breakdown or in a smooth transition. Simulation results are presented for square lattices as starting networks under random attacks of constant intensity. We find that the shift in the position of the breakdown has a power-law scaling as a function of the healing probability with an exponent close to 1. Below a critical healing probability, catastrophic cascades form and the average degree of surviving nodes decreases monotonically, while above this value there are no macroscopic cascades and the average degree has first an increasing character and decreases only at the very late stage of the process. These findings facilitate to plan intervention in case of crisis situation by describing the efficiency of healing efforts needed to suppress cascading failures. 	
1605.01784v1	http://arxiv.org/pdf/1605.01784v1	2016	Neural mechanisms underlying catastrophic failure in human-machine   interaction during aerial navigation	Sameer Saproo|Victor Shih|David C. Jangraw|Paul Sajda	  Objective. We investigated the neural correlates of workload buildup in a fine visuomotor task called the boundary avoidance task (BAT). The BAT has been known to induce naturally occurring failures of human-machine coupling in high performance aircraft that can potentially lead to a crash; these failures are termed pilot induced oscillations (PIOs). Approach. We recorded EEG and pupillometry data from human subjects engaged in a flight BAT simulated within a virtual 3D environment. Main results. We find that workload buildup in a BAT can be successfully decoded from oscillatory features in the electroencephalogram (EEG). Information in delta, theta, alpha, beta, and gamma spectral bands of the EEG all contribute to successful decoding, however gamma band activity with a lateralized somatosensory topography has the highest contribution, while theta band activity with a frontocentral topography has the most robust contribution in terms of real world usability. We show that the output of the spectral decoder can be used to predict PIO susceptibility. We also find that workload buildup in the task induces pupil dilation, the magnitude of which is significantly correlated with the magnitude of the decoded EEG signals. These results suggest that PIOs may result from the dysregulation of cortical networks such as the locus coeruleus (LC) anterior cingulate cortex (ACC) circuit. Significance. Our findings may generalize to similar control failures in other cases of tight man machine coupling where gains and latencies in the control system must be inferred and compensated for by the human operators. A closed-loop intervention using neurophysiological decoding of workload buildup that targets the LC ACC circuit may positively impact operator performance in such situations. 	
0704.0879v1	http://arxiv.org/pdf/0704.0879v1	2007	A Hierarchical Approach for Dependability Analysis of a Commercial   Cache-Based RAID Storage Architecture	Mohamed Kaaniche|Luigi Romano|Zbigniew Kalbarczyk|Ravishankar Iyer|Rick Karcich	  We present a hierarchical simulation approach for the dependability analysis and evaluation of a highly available commercial cache-based RAID storage system. The archi-tecture is complex and includes several layers of overlap-ping error detection and recovery mechanisms. Three ab-straction levels have been developed to model the cache architecture, cache operations, and error detection and recovery mechanism. The impact of faults and errors oc-curring in the cache and in the disks is analyzed at each level of the hierarchy. A simulation submodel is associated with each abstraction level. The models have been devel-oped using DEPEND, a simulation-based environment for system-level dependability analysis, which provides facili-ties to inject faults into a functional behavior model, to simulate error detection and recovery mechanisms, and to evaluate quantitative measures. Several fault models are defined for each submodel to simulate cache component failures, disk failures, transmission errors, and data errors in the cache memory and in the disks. Some of the parame-ters characterizing fault injection in a given submodel cor-respond to probabilities evaluated from the simulation of the lower-level submodel. Based on the proposed method-ology, we evaluate and analyze 1) the system behavior un-der a real workload and high error rate (focusing on error bursts), 2) the coverage of the error detection mechanisms implemented in the system and the error latency distribu-tions, and 3) the accumulation of errors in the cache and in the disks. 	
1704.05285v1	http://arxiv.org/pdf/1704.05285v1	2017	Mechanical Failure in Amorphous Solids: Scale Free Spinodal Criticality	Itamar Procaccia|Corrado Rainone|Murari Singh	  The mechanical failure of amorphous media is a ubiquitous phenomenon from material engineering to geology. It has been noticed for a long time that the phenomenon is "scale-free", indicating some type of criticality. In spite of attempts to invoke "Self-Organized Criticality", the physical origin of this criticality, and also its universal nature, being quite insensitive to the nature of microscopic interactions, remained elusive. Recently we proposed that the precise nature of this critical behavior is manifested by a spinodal point of a thermodynamic phase transition. Moreover, at the spinodal point there exists a divergent correlation length which is associated with the system-spanning instabilities (known also as shear bands) which are typical to the mechanical yield. Demonstrating this requires the introduction of an "order parameter" that is suitable for distinguishing between disordered amorphous systems, and an associated correlation function, suitable for picking up the growing correlation length. The theory, the order parameter, and the correlation functions used are universal in nature and can be applied to any amorphous solid that undergoes mechanical yield. Critical exponents for the correlation length divergence and the system size dependence are estimated. The phenomenon is seen at its sharpest in athermal systems, as is explained below; in this paper we extend the discussion also to thermal systems, showing that at sufficiently high temperatures the spinodal phenomenon is destroyed by thermal fluctuations. 	
0508533v2	http://arxiv.org/pdf/cond-mat/0508533v2	2005	First-principles calculations of a high-pressure synthesized compound   PtC	L. Y. Li|W. Yu|C. Q. Jin	  First-principles density-functional method is used to study the recently high-pressure synthesized compound PtC. It is confirmed by our calculations that the platinum carbide has a zinc-blende ground-state phase at zero pressure and the rock-salt structure is a high-pressure phase. The theoretical transition pressure from zinc-blende to rock-salt is determined to be 52GPa. Furthermore, our calculation shows the possibility that the experimentally synthesized PtC by Ono et al. under high pressure condition might undergo a transition from rock-salt structure to zinc-blende after the pressure quench to ambient condition. 	
0512163v1	http://arxiv.org/pdf/cond-mat/0512163v1	2005	Millimeter wave spectroscopy of rocks and fluids	John A. Scales|Michael Batzle	  One region of the electromagnetic spectrum that is relatively unexploited for materials characterization is the millimeter wave band (frequencies roughly between 40 and 300 GHz). Millimeter wave techniques involve free-space (non-contacting) measurements which have a length scale that makes them ideal for characterizing bulk properties of multicomponent composites where the scale of homogeneity is on the order of millimeters. Such composites include granular materials such as rocks, fluid mixtures, suspensions and emulsions. Here we show measurements on partially saturated rocks and an oil/water mixture, demonstrating that millimeter wave spectroscopy is sensitive yet rapid measure of changing composition. 	
1101.1386v1	http://arxiv.org/pdf/1101.1386v1	2011	High-pressure synthesis of rock salt LiMeO2-ZnO (Me = Fe3+, Ti3+) solid   solutions	P. S. Sokolov|A. N. Baranov|V. A. Tafeenko|V. L. Solozhenko	  Metastable LiMeO2-ZnO (Me = Fe3+, Ti3+) solid solutions with rock salt crystal structure have been synthesized by solid state reaction of ZnO with LiMeO2 complex oxides at 7.7 GPa and 1350-1450 K. Structure, phase composition, thermal stability and thermal expansion of the recovered samples have been studied by X-ray diffraction with synchrotron radiation. At ambient pressure rock salt LiMeO2-ZnO solid solutions are kinetically stable up to 670-800 K depending on the composition. 	
1110.0202v1	http://arxiv.org/pdf/1110.0202v1	2011	Dielectric properties of Granodiorite partially saturated with water and   its correlation to the detection of seismic electric signals	A. N. Papathanassiou|I. Sakellis|J. Grammatikakis	  Transient electric signals emitted prior to earthquake occurrence are recorded at certain sites in the Earth's crust termed sensitive. These field observations enforce the laboratory investigation of the dielectric response of rocks forming these localities. The dielectric relaxation of granodiorite rock coming from such a sensitive locality (Keratea, Greece) reveals, through complex impedance spectroscopy, that the activation volume for relaxation of this rock is negative which so far has been reported only rarely. This result, however, supports a theoretical model on the pre-seismic electric signals and is likely to be correlated with the sensitivity of the site and hence with the selectivity. 	
1212.2628v1	http://arxiv.org/pdf/1212.2628v1	2012	Relative Dating and Classification of Minerals and Rocks Based on   Statistical Calculations Related to Their Potential Energy Index	Mikhail M. Labushev|Alexander N. Khokhlov	  Index of proportionality of atomic weights of chemical elements is proposed for determining the relative age of minerals and rocks. Their chemical analysis results serve to be initial data for calculations. For rocks of different composition the index is considered to be classification value as well. Crystal lattice energy change in minerals and their associations can be measured by the index value change, thus contributing to the solution of important practical problems. There was determined the effect of more rapid increase of potential energy of limestone with relatively low lattice energy as compared with the others. 	
1303.2216v1	http://arxiv.org/pdf/1303.2216v1	2013	Low-temperature thermal expansion of rock-salt ZnO	Petr S. Sokolov|Andrey N. Baranov|Anthony M. T. Bell|Vladimir L. Solozhenko	  Lattice parameter of metastable high-pressure phase of zinc oxide, rock-salt ZnO was measured in the 10-300 K temperature range using synchrotron X-ray powder diffraction. No phase transition was observed down to 10 K. The lattice parameter of rock-salt ZnO was found to increase from 4.266 {\AA} in the 10-80 K range up to 4.2752(3) {\AA} at 298 K, while the volume thermal expansion coefficient increases from slight negative values below 40 K up to 4.77\times10^-5 K^-1 at 298 K. 	
1601.07221v1	http://arxiv.org/pdf/1601.07221v1	2016	A modified model of a single rock joint shear behavior in limestone   specimens	Saeid R Dindarloo|Elnaz Siami-Irdemoosa	  The shear behavior of a single rock joint in limestone specimens, under a constant normal load (CNL), was analyzed in this study. Test specimens with different asperity roughness were prepared and tested. The Goodman model of a rock joint shear behavior, under CNL, was modified to render a better representation of the data obtained. The model applicability was validated. The proposed model shows better correlation with experimental data. It also, requires fewer variables. The steps to calculate all the necessary variables for the model are discussed. 	
1706.03368v1	http://arxiv.org/pdf/1706.03368v1	2017	On the thermodynamic aspect of zinc oxide polymorphism. Calorimetric   study of metastable rock salt ZnO	Felix Yu. Sharikov|Petr S. Sokolov|Andrey N. Baranov|Vladimir L. Solozhenko	  The enthalpies of dissolution of metastable rock salt and thermodynamically stable wurtzite polymorphs of zinc oxide in aqueous H2SO4 have been measured in direct calorimetric experiments at 303 K and 0.1 MPa and the obtained results enabled determination of the standard enthalpy of the rock salt-to-wurtzite phase transition in ZnO, {\Delta}trH = -11.7+/-0.3 kJ/mol. 	
0407096v1	http://arxiv.org/pdf/quant-ph/0407096v1	2004	The Emergence of Classical Dynamics in a Quantum World	Tanmoy Bhattacharya|Salman Habib|Kurt Jacobs	  Ever since the advent of quantum mechanics, it has been clear that the atoms composing matter do not obey Newton's laws. Instead, their behavior is described by the Schroedinger equation. Surprisingly though, until recently, no clear explanation was given for why everyday objects, which are merely collections of atoms, are observed to obey Newton's laws. It would seem that, if quantum mechanics explains all the properties of atoms accurately, they, too, should obey quantum mechanics. This reasoning led some scientists to believe in a distinct macroscopic, or ``big and complicated,'' world in which quantum mechanics fails and classical mechanics takes over, although there has never been experimental evidence for such a failure. Even those who insisted that Newtonian mechanics would somehow emerge from the underlying quantum mechanics as the system became increasingly macroscopic were hindered by the lack of adequate experimental and theoretical tools. In the last decade, however, this quantum-to-classical transition has become accessible to experimental study and quantitative description, and the resulting insights are the subject of this article. 	
0012058v1	http://arxiv.org/pdf/cond-mat/0012058v1	2000	A model for complex aftershock sequences	Y. Moreno|A. Correig|J. B. Gomez|A. F. Pacheco	  The decay rate of aftershocks is commonly very well described by the modified Omori law, $n(t) \propto t^{-p}$, where n(t) is the number of aftershocks per unit time, t is the time after the main shock, and p is a constant in the range 0.9<p<1.5, and usually close to 1. But there are also more complex aftershock sequences for which the Omori law can be considered only as a first approximation. One of these complex aftershock sequences took place in the Eastern Pyrenees on February 18, 1996, and was described in detail by {\it Correig et al.} [1997]. In this paper, we propose a new model inspired by dynamic fiber-bundle models to interpret this type of complex aftershock sequences with sudden increases in the rate of aftershock production not directly related to the magnitude of the aftershocks (as in the epidemic-type aftershock sequences). The model is a simple, discrete, stochastic fracture model where the elements (asperities or barriers) break because of static fatigue, transfer stress according to a local load-sharing rule and then are regenerated. We find a very good agreement between the model and the Eastern Pyrenees aftershock sequence and we propose that the key mechanism for explaining aftershocks, apart from a time-dependent rock strength, is the presence of dynamic stress fluctuations which constantly reset the initial conditions for the next aftershock in the sequence. 	
0709.0479v1	http://arxiv.org/pdf/0709.0479v1	2007	Granular Fluids	James W. Dufty	  The terminology granular matter refers to systems with a large number of hard objects (grains) of mesoscopic size ranging from millimeters to meters. Geological examples include desert sand and the rocks of a landslide. But the scope of such systems is much broader, including powders and snow, edible products such a seeds and salt, medical products like pills, and extraterrestrial systems such as the surface regolith of Mars and the rings of Saturn. The importance of a fundamental understanding for granular matter properties can hardly be overestimated. Practical issues of current concern range from disaster mitigation of avalanches and explosions of grain silos to immense economic consequences within the pharmaceutical industry. In addition, they are of academic and conceptual importance as well as examples of systems far from equilibrium. Under many conditions of interest, granular matter flows like a normal fluid. In the latter case such flows are accurately described by the equations of hydrodynamics. Attention is focused here on the possibility for a corresponding hydrodynamic description of granular flows. The tools of nonequilibrium statistical mechanics, developed over the past fifty years for fluids composed of atoms and molecules, are applied here to a system of grains for a fundamental approach to both qualitative questions and practical quantitative predictions. The nonlinear Navier-Stokes equations and expressions for the associated transport coefficients are obtained. 	
1101.4639v3	http://arxiv.org/pdf/1101.4639v3	2011	Binary Asteroid Systems: Tidal End States and Estimates of Material   Properties	Patrick A. Taylor|Jean-Luc Margot	  The locations of the fully despun, double synchronous end states of tidal evolution are derived for spherical components. With the exception of nearly equal-mass binaries, binary asteroid systems are in the midst of lengthy tidal evolutions, far from their fully synchronous tidal end states. Calculations of material strength indicate that binaries in the main belt with 100-km-scale primary components are consistent with being made of monolithic or fractured rock as expected for binaries likely formed from sub-catastrophic impacts in the early solar system. To tidally evolve in their dynamical lifetime, near-Earth binaries with km-scale primaries or smaller must be much weaker mechanically than their main-belt counterparts even if formed in the main belt prior to injection into the near-Earth region. Small main-belt binaries with primary components less than 10 km in diameter, depending on their ages, could either be as strong as large main-belt binaries or as weak as near-Earth binaries because the inherent uncertainty in the age of a binary system can affect the calculation of material strength by orders of magnitude. Several other issues are considered, though these typically affect the calculation of material strength by no more than a factor of two. We also find indirect evidence within all three groups of binary asteroids that the inter-component separation may evolve via another mechanism(s) with the binary YORP effect being a likely candidate. 	
1112.3630v1	http://arxiv.org/pdf/1112.3630v1	2011	Primordial Planets Explain Interstellar Dust, the Formation of Life; and   Falsify Dark Energy	Carl H. Gibson|N. Chandra Wickramasinghe|Rudolph E. Schild	  Hydrogravitional-dynamics (HGD) cosmology of Gibson/Schild 1996 predicts proto-globular-star-cluster PGC clumps of Earth-mass planets fragmented from plasma at ~0.3 Myr. Protogalaxies retained the ~0.03 Myr baryonic density existing at the time of the first viscous-gravitational plasma fragmentation. Stars promptly formed from mergers of these gas planets, seeded by chemicals C, N, O, Fe etc. created by the first stars and their supernovae at ~ 0.33 Myr. Hot hydrogen gas planets reduced seeded oxides to hot water oceans over metal-rock cores at water critical temperature 647 K, at ~2 Myr. Merging planets and moons hosted the first organic chemistry and the first life, distributed to the 10^80 planets of the cosmological big bang by comets produced by the (HGD) binary-planet-merger star formation mechanism: the biological big bang. Life distributed by the Hoyle/Wickramasinghe cometary-panspermia mechanism thus evolves in a cosmological primordial soup of the merging planets throughout the universe space-time. A primordial astrophysical origin is provided for astrobiology by planets of HGD cosmology. Concordance {\Lambda}CDMHC cosmology is rendered obsolete by the observation of complex life on Earth, falsifying the dark energy and cold dark matter concepts. The dark matter of galaxies is mostly primordial planets in protoglobularstarcluster clumps, 30,000,000 planets per star (not 8!). Complex organic chemicals observed in the interstellar dust is formed by life on these planets, and distributed by their comets. 	
1203.4633v1	http://arxiv.org/pdf/1203.4633v1	2012	Random Motion with Interfacial Contact: Driven Diffusion vis-a-vis   Mechanical Activation	P. S. Goohpattader|M. K. Chaudhury	  Rolling of a small sphere on a solid support is governed by a non-linear friction that is akin to the Coulombic dry fiction. No motion occurs when the external field is weaker than the frictional resistance. However, with the intervention of an external noise, a viscous friction like property emerges; thus the sphere rolls with an uniform drift velocity that is proportional to the applied field. As the sphere rolls, it rocks forward and backward resulting in substantial fluctuation of displacement opposite to the net drift. The ratio of the integrated probabilities of the negative to positive work fluctuations decreases monotonically with the time of observation, from which a temperature like intensive parameter can be estimated. This parameter conforms to the Einstein's ratio of diffusivity and mobility that increases almost linearly, even though the diffusivity increases super-linearly, with the strength of the noise. A new barrier crossing experiment is introduced that can be performed either with a hard (e.g. a steel ball) or with a soft (e.g. a water drop) sphere in contact with a periodically undulated substrate. The frequency of barrier crossing follows the classical transition state equation allowing a direct estimation of the effective temperature. These experiments as well as certain numerical simulations suggest that the effective temperature of a system controlled by a non-linear friction may not have a unique value. 	
1204.4781v1	http://arxiv.org/pdf/1204.4781v1	2012	Ab-initio study of different structures of CaC ionic compound:   Magnetism, Bonding, Exchange interaction and Lattice Dynamics	Zahra Nourbakhsh|S. Javad Hashemifar|Hadi Akbarzadeh	  On the basis of density functional - pseudopotential calculations, we study structural, electronic, magnetic, and mechanical properties of the hypothetical CaC ionic compound in the rock-salt (RS), B2, zinc-blende (ZB), wurtzite (WZ), NiAs (NA), and anti-NiAs (NA*) structures. The results show that RS-CaC is the most stable system at equilibrium while applying negative hydrostatic pressures may stabilize the half-metallic WZ and ZB structures. The ferromagnetic equilibrium state observed in the RS, ZB, NA and WZ structures of CaC is attributed to the sharp partially filled p band of the carbon atom. It is argued that the ionic interaction increases the sharpness of the p band and hence enhances ferromagnetism while the covalent interaction increases the band dispersion and weakens magnetism. We investigate various properties of the exchange interaction in the ferromagnetic CaC structures. It is observed that the interatomic exchange interaction in these systems have consistent behavior with the spin splittings of the bond points. Comparing the structural properties in the ferromagnetic and nonmagnetic states. indicates a weak magneto-structural coupling in CaC. Applying non-local corrections to the exchange functional enhance the exchange splitting and hence give rise to a half-metallic electronic structure for the WZ, RS, and NA structures of CaC. In the framework of density functional perturbation theory, the phonon spectra of these systems are investigated and the observed dynamical instabilities in the NA* and B2 structures are attributed to the tendency of the carbon atoms toward dimerization. Various mechanical properties of the dynamically stable structures of CaC are determined from their phonon spectra. 	
1602.07842v1	http://arxiv.org/pdf/1602.07842v1	2016	Colossal dielectric constant in high entropy oxides	David Berardan|Sylvain Franger|Diana Dragoe|Arun Kumar Meena|Nita Dragoe	  Entropic contributions to the stability of solids are very well understood and the mixing entropy has been used for forming various solids, for instance such as inverse spinels. A particular development was related to high entropy alloys in which the configurational disorder is responsible for forming simple solid solutions and which are thoroughly studied for various applications especially due to their mechanical properties but also electrical properties, hydrogen storage, magnetic properties. Many unexplored compositions and properties still remain for this class of materials due to their large phase space. In a recent report it has been shown that the configurational disorder can be used for stabilizing simple solid solutions of oxides, which should normally not form solid solutions, these new materials were called "entropy-stabilized oxides". In this pioneering report, it was shown that mixing five equimolar binary oxides yielded, after heating at high temperature and quenching, an unexpected rock salt structure compound with statistical distribution of the cations in a face centered cubic lattice. Following this seminal study, we show here that these high entropy oxides (named HEOx hereafter) can be substituted by aliovalent elements with a charge compensation mechanism. This possibility largely increases the potential development of new materials by widening their (already complex) phase space. As a first example, we report here that at least one HEOx composition exhibits colossal dielectric constants, which could make it very promising for applications as large-k dielectric materials. 	
1701.03510v1	http://arxiv.org/pdf/1701.03510v1	2017	The Origins of Asteroidal Rock Disaggregation: Interplay of Thermal   Fatigue and Microstructure	Kavan Hazeli|Charles El Mir|Stefanos Papanikolaou|Marco Delbo|KT Ramesh	  The distributions of size and chemical composition in the regolith on airless bodies provides clues to the evolution of the solar system. Recently, the regolith on asteroid (25143) Itokawa, visited by the JAXA Hayabusa spacecraft, was observed to contain millimeter to centimeter sized particles. Itokawa boulders commonly display well-rounded profiles and surface textures that appear inconsistent with mechanical fragmentation during meteorite impact; the rounded profiles have been hypothesized to arise from rolling and movement on the surface as a consequence of seismic shaking. We provide a possible explanation of these observations by exploring the primary crack propagation mechanisms during thermal fatigue of a chondrite. We present the in situ evolution of the full-field strains on the surface as a function of temperature and microstructure, and observe and quantify the crack growth during thermal cycling. We observe that the primary fatigue crack path preferentially follows the interfaces between monominerals, leaving them intact after fragmentation. These observations are explained through a microstructure-based finite element model that is quantitatively compared with our experimental results. These results on the interactions of thermal fatigue cracking with the microstructure may ultimately allow us to distinguish between thermally induced fragments and impact products. 	
1709.05478v1	http://arxiv.org/pdf/1709.05478v1	2017	Impact cratering on porous targets in the strength regime	Akiko M. Nakamura	  Cratering on small bodies is crucial for the collision cascade and also contributes to the ejection of dust particles into interplanetary space. A crater cavity forms against the mechanical strength of the surface, gravitational acceleration, or both. The formation of moderately sized craters that are sufficiently larger than the thickness of the regolith on small bodies, in which mechanical strength plays the dominant role rather than gravitational acceleration, is in the strength regime. The formation of microcraters on blocks on the surface is also within the strength regime. On the other hand, the formation of a crater of a size comparable to the thickness of the regolith is affected by both gravitational acceleration and cohesion between regolith particles. In this short review, we compile data from the literature pertaining to impact cratering experiments on porous targets, and summarize the ratio of spall diameter to pit diameter, the depth, diameter, and volume of the crater cavity, and the ratio of depth to diameter. Among targets with various porosities studied in the laboratory to date, based on conventional scaling laws (Holsapple and Schmidt, J. Geophys. Res., 87, 1849-1870, 1982) the cratering efficiency obtained for porous sedimentary rocks (Suzuki et al., J. Geophys. Res. 117, E08012, 2012) is intermediate. A comparison with microcraters formed on a glass target with impact velocities up to 14 $km s^{-1}$ indicates a different dependence of cratering efficiency and depth-to-diameter ratio on impact velocity. 	
1710.07366v1	http://arxiv.org/pdf/1710.07366v1	2017	Characterization of Marcellus Shale Fracture Properties through Size   Effect Tests and Computations	Weixin Li|Zhefei Jin|Gianluca Cusatis	  Mechanical characterization of shale-like rocks requires understanding the scaling of the measured properties to enable the extrapolation from small scale laboratory tests to field study. In this paper, the size effect of Marcellus shale was analyzed, and the fracture properties were obtained through size effect tests. A number of fracture tests were conducted on Three-Point-Bending (TPB) specimens with increasing size. Test results show that the nominal strength decreases with increasing specimen size, and can be fitted well by Bazant's Size Effect Law (SEL). It is shown that SEL accounts for the effects of both specimen size and geometry, allowing an accurate identification of the initial fracture energy of the material, Gf, and the effective Fracture Process Zone (FPZ) length, cf. The obtained fracture properties were verified by the numerical simulations of the investigated specimens using standard Finite Element technique with cohesive model. Significant anisotropy was observed in the fracture properties determined in three principal notch orientations: arrester, divider, and short-transverse. The size effect of the measured structural strength and apparent fracture toughness was discussed. Neither strength-based criterion which neglects size effect, nor classic LEFM which does not account for the finiteness of the FPZ can predict the reported size effect data, and nonlinear fracture mechanics of the quasibrittle type is instead applicable. 	
0501176v1	http://arxiv.org/pdf/astro-ph/0501176v1	2005	Supernova Ia without Accelerated Expansion The First Global Failure of   Relativity Theory	Charles B. Leffert	  A new cosmological model has been developed that shows great promise for solving many of the present problems of physics. A new concept of space and its production, spatial condensation (SC) is the cause of the expansion. Dark mass (not matter) scales with the expansion differently than matter. Many other non-relativistic concepts predict a simple beginning, absence of singularities, a definition of energy and the cause of space curvature and gravity. Predicted cosmological parameters agree with recent measurements including t0=13.5 Gy, H0=68.6 km/(s Mpc), Omega (mass)=0.28 and no dark energy. Other predictions include: Hubble flow at the Planck level, vacuum energy (no mass), Evac/Emass=10^123 in agreement with quantum mechanics, and the pattern in the CMB is the distribution of very early dark mass black holes of average mass 10^8 Msun. Excellent agreement with supernova Ia data is obtained with no acceleration of the expansion rate. It is concluded that the SC-model announces the first global failure of relativity theory. 	
0509514v1	http://arxiv.org/pdf/astro-ph/0509514v1	2005	Low Mass X-ray Binaries and Metallicity Dependence: Story of Failures	Natalia Ivanova	  Observations of galactic and extra-galactic globular clusters have shown that on average metal-rich clusters are ~3 times as likely to contain a bright X-ray source than their metal-poor counterparts. We propose that this can be explained by taking into account the difference in the stellar structure of main sequence donors with masses between ~0.85 Msun and ~1.25 Msun at different metallicities. Metal-poor main sequence stars in this mass range do not have an outer convective zone while metal-rich stars do. The absence of this zone turns off magnetic braking, a powerful mechanism of orbital shrinkage, leading to the failure of dynamically formed main sequence - neutron star binaries to start mass transfer or appear as bright low-mass X-ray binaries. 	
0008064v1	http://arxiv.org/pdf/cond-mat/0008064v1	2000	Error and attack tolerance of complex networks	Reka Albert|Hawoong Jeong|Albert-Laszlo Barabasi	  Many complex systems, such as communication networks, display a surprising degree of robustness: while key components regularly malfunction, local failures rarely lead to the loss of the global information-carrying ability of the network. The stability of these complex systems is often attributed to the redundant wiring of the functional web defined by the systems' components. In this paper we demonstrate that error tolerance is not shared by all redundant systems, but it is displayed only by a class of inhomogeneously wired networks, called scale-free networks. We find that scale-free networks, describing a number of systems, such as the World Wide Web, Internet, social networks or a cell, display an unexpected degree of robustness, the ability of their nodes to communicate being unaffected by even unrealistically high failure rates. However, error tolerance comes at a high price: these networks are extremely vulnerable to attacks, i.e. to the selection and removal of a few nodes that play the most important role in assuring the network's connectivity. 	
0008171v2	http://arxiv.org/pdf/cond-mat/0008171v2	2000	Scaling behaviour in the fracture of fibrous materials	I. L. Menezes-Sobrinho|J. G. Moreira|A. T. Bernardes	  We study the existence of distinct failure regimes in a model for fracture in fibrous materials. We simulate a bundle of parallel fibers under uniaxial static load and observe two different failure regimes: a catastrophic and a slowly shredding. In the catastrophic regime the initial deformation produces a crack which percolates through the bundle. In the slowly shredding regime the initial deformations will produce small cracks which gradually weaken the bundle. The boundary between the catastrophic and the shredding regimes is studied by means of percolation theory and of finite-size scaling theory. In this boundary, the percolation density $\rho$ scales with the system size $L$, which implies the existence of a second-order phase transition with the same critical exponents as those of usual percolation. 	
0104080v1	http://arxiv.org/pdf/cond-mat/0104080v1	2001	Criticality in a model of banking crises	Giulia Iori|Saqib Jafarey	  An interbank market lets participants pool the risk arising from the combination of illiquid investments and random withdrawals by depositors. But it also creates the potential for one bank's failure to trigger off avalanches of further failures. We simulate a model of interbank lending to study the interplay of these two effects. We show that when banks are similar in size and exposure to risk, avalanche effects are small so that widening the interbank market leads to more stability. But as heterogeneity increases, avalanche effects become more important. By varying the heterogeneity and connectivity across banks, the system enters a critical regime with a power law distribution of avalanche sizes. 	
0106012v2	http://arxiv.org/pdf/cond-mat/0106012v2	2002	The Random Fuse Network as a Dipolar Magnet	Marc Barthelemy|Rava da Silveira|Henri Orland	  We introduce an approximate mapping between the random fuse network (RFN) and a random field dipolar Ising model (RFDIM). The state of the network damage is associated with a metastable spin configuration. A mean-field treatment, numerical solutions, and heuristic arguments support the broad validity of the approximation and yield a generic phase diagram. At low disorder, the growth of a single unstable `crack' leads to an abrupt global failure. Beyond a critical disorder, the conducting network sustains significant damage before the coalescence of cracks results in global failure. 	
0404331v1	http://arxiv.org/pdf/cond-mat/0404331v1	2004	Optimization of Robustness of Complex Networks	G. Paul|T. Tanizawa|S. Havlin|H. E. Stanley	  Networks with a given degree distribution may be very resilient to one type of failure or attack but not to another. The goal of this work is to determine network design guidelines which maximize the robustness of networks to both random failure and intentional attack while keeping the cost of the network (which we take to be the average number of links per node) constant. We find optimal parameters for: (i) scale free networks having degree distributions with a single power-law regime, (ii) networks having degree distributions with two power-law regimes, and (iii) networks described by degree distributions containing two peaks. Of these various kinds of distributions we find that the optimal network design is one in which all but one of the nodes have the same degree, $k_1$ (close to the average number of links per node), and one node is of very large degree, $k_2 \sim N^{2/3}$, where $N$ is the number of nodes in the network. 	
0406450v3	http://arxiv.org/pdf/cond-mat/0406450v3	2005	Failure properties of loaded fiber bundles having a lower cutoff in   fiber threshold distribution	Srutarshi Pradhan|Alex Hansen	  Presence of lower cutoff in fiber threshold distribution may affect the failure properties of a bundle of fibers subjected to external load. We investigate this possibility both in a equal load sharing (ELS) fiber bundle model and in local load sharing (LLS) one. We show analytically that in ELS model, the critical strength gets modified due to the presence of lower cutoff and it becomes bounded by an upper limit. Although the dynamic exponents for the susceptibility and relaxation time remain unchanged, the avalanche size distribution shows a permanent deviation from the mean-fiels power law. In the LLS model, we analytically estimate the upper limit of the lower cutoff above which the bundle fails at one instant. Also the system size variation of bundle's strength and the avalanche statistics show strong dependence on the lower cutoff level. 	
0408580v2	http://arxiv.org/pdf/cond-mat/0408580v2	2005	Universality Class of Fiber Bundle Model on Complex Networks	Dong-Hee Kim|Beom Jun Kim|Hawoong Jeong	  We investigate the failure characteristics of complex networks within the framework of the fiber bundle model subject to the local load sharing rule in which the load of the broken fiber is transferred only to its neighbor fibers. Although the load sharing is strictly local, it is found that the critical behavior belongs to the universality class of the global load sharing where the load is transferred equally to all fibers in the system. From the numerical simulations and the analytical approach applied to the microscopic behavior, it is revealed that the emergence of a single dominant hub cluster of broken fibers causes the global load sharing effect in the failure process. 	
0411529v2	http://arxiv.org/pdf/cond-mat/0411529v2	2005	Regeneration of Stochastic Processes: An Inverse Method	J. Peinke|M. Reza Rahimi Tabar|Muhammad Sahimi|F. Ghasemi	  We propose a novel inverse method that utilizes a set of data to construct a simple equation that governs the stochastic process for which the data have been measured, hence enabling us to reconstruct the stochastic process. As an example, we analyze the stochasticity in the beat-to-beat fluctuations in the heart rates of healthy subjects as well as those with congestive heart failure. The inverse method provides a novel technique for distinguishing the two classes of subjects in terms of a drift and a diffusion coefficients which behave completely differently for the two classes of subjects, hence potentially providing a novel diagnostic tool for distinguishing healthy subjects from those with congestive heart failure, even at the early stages of the disease development. 	
0503473v1	http://arxiv.org/pdf/cond-mat/0503473v1	2005	Breakdown of Heterogeneous Materials	Purusattam Ray	  We discuss the threshold activated extremal dynamics that is prevalent in the breakdown processes in heterogeneous materials. We model such systems by an elastic spring network with random breaking thresholds assigned to the springs. Results are obtained from molecular dynamics simulation of the system under constant stress and constant strain conditions. We find that the distribution $P(m)$ of the avalanches of size $m$, caused by the rupturing of the springs till the failure of the network, decays as a power-law: $P(m) \sim m^{-\alpha}$, where $\alpha$ can be closely approximated to 5/2. The average avalanche size $<m>$ diverges as $<m> \sim (F_c - F)^{-1/2}$ close to the stress $F_c$ at which the total failure of the network occurs. We study the time evolution of the breakdown process: we find that the bonds rupture randomly over the network at initial times but the rupturing becomes highly correlated at late times to give rise to a well-defined macroscopic crack. 	
0508682v2	http://arxiv.org/pdf/cond-mat/0508682v2	2005	Avalanche dynamics driven by adaptive rewirings in complex networks	K. Rho|S. R. Hong|B. Kahng	  We introduce a toy model displaying the avalanche dynamics of failure in scale-free networks. In the model, the network growth is based on the Barab\'asi and Albert model and each node is assigned a capacity or tolerance, which is constant irrespective of node index. The degree of each node increases over time. When the degree of a node exceeds its capacity, it fails and each link connected to it is is rewired to other unconnected nodes by following the preferential attachment rule. Such a rewiring edge may trigger another failure. This dynamic process can occur successively, and it exhibits a self-organized critical behavior in which the avalanche size distribution follows a power law. The associated exponent is $\tau \approx 2.6(1)$. The entire system breaks down when any rewired edges cannot locate target nodes: the time at which this occurs is referred to as the breaking time. We obtain the breaking time as a function of the capacity. Moreover, using extreme value statistics, we determine the distribution function of the breaking time. 	
0602371v2	http://arxiv.org/pdf/cond-mat/0602371v2	2006	Rupture processes in fiber bundle models	Per C. Hemmer|Alex Hansen|Srutarshi Pradhan	  Fiber bundles with statistically distributed thresholds for breakdown of individual fibers are interesting models of the static and dynamics of failures in materials under stress. They can be analyzed to an extent that is not possible for more complex materials. During the rupture process in a fiber bundle avalanches, in which several fibers fail simultaneously, occur. We study by analytic and numerical methods the statistics of such avalanches, and the breakdown process for several models of fiber bundles. The models differ primarily in the way the extra stress caused by a fiber failure is redistributed among the surviving fibers. 	
0607305v1	http://arxiv.org/pdf/cond-mat/0607305v1	2006	Local load sharing fiber bundles with a lower cutoff of strength   disorder	Frank Raischel|Ferenc Kun|Hans J. Herrmann	  We study the failure properties of fiber bundles with a finite lower cutoff of the strength disorder varying the range of interaction between the limiting cases of completely global and completely local load sharing. Computer simulations revealed that at any range of load redistribution there exists a critical cutoff strength where the macroscopic response of the bundle becomes perfectly brittle, i.e. linearly elastic behavior is obtained up to global failure, which occurs catastrophically after the breaking of a small number of fibers. As an extension of recent mean field studies [Phys. Rev. Lett. 95, 125501 (2005)], we demonstrate that approaching the critical cutoff, the size distribution of bursts of breaking fibers shows a crossover to a universal power law form with an exponent 3/2 independent of the range of interaction. 	
0701237v1	http://arxiv.org/pdf/cond-mat/0701237v1	2007	Failure avalanches in fiber bundles for discrete load increase	Per C. Hemmer|Srutarshi Pradhan	  The statistics of burst avalanche sizes $n$ during failure processes in a fiber bundle follows a power law, $D(n)\sim n^{-\xi}$, for large avalanches. The exponent $\xi$ depends upon how the avalanches are provoked. While it is known that when the load on the bundle is increased in a continuous manner, the exponent takes the value $\xi=5/2$, we show that when the external load is increased in discrete and not too small steps, the exponent value $\xi=3$ is relevant. Our analytic treatment applies to bundles with a general probability distribution of the breakdown thresholds for the individual fibers. The pre-asymptotic size distribution of avalanches is also considered. 	
0009049v1	http://arxiv.org/pdf/gr-qc/0009049v1	2000	Does the third law of black hole thermodynamics really have a serious   failure?	Istvan Racz	  The almost perfect correspondence between certain laws of classical black hole mechanics and the ordinary laws of thermodynamics is spoiled by the failure of the conventional back hole analogue of the third law. Our aim here is to contribute to the associated discussion by flashing light on some simple facts of black hole physics. However, no attempt is made to lay to rest the corresponding long lasting debate. Instead, merely some evidence is provided to make it clear that although the borderline between extremal and non-extremal black holes is very thin they are essentially different. Hopefully, a careful investigation of the related issues will end up with an appropriate form of the third law and hence with an unblemished setting of black hole thermodynamics. 	
9407243v1	http://arxiv.org/pdf/hep-ph/9407243v1	1994	CP Violation in Beauty Decays -- the Standard Model Paradigm of Large   Effects	I. I. Bigi	  The Standard Model contains a natural source for CP asymmetries in weak decays, which is described by the KM mechanism. Beyond $\epsilon _K$ it generates only elusive manifestations of CP violation in {\em light-}quark systems. On the other hand it naturally leads to large asymmetries in certain non-leptonic beauty decays. In particular when $B^0-\bar B^0$ oscillations are involved, theoretical uncertainties in the hadronic matrix elements either drop out or can be controlled, and one predicts asymmetries well in excess of 10\% with high parametric reliability. It is briefly described how the KM triangle can be determined experimentally and then subjected to sensitive consistency tests. Any failure would constitute indirect, but unequivocal evidence for the intervention of New Physics; some examples are sketched. Any outcome of a comprehensive program of CP studies in $B$ decays -- short of technical failure -- will provide us with fundamental and unique insights into nature's design. 	
0411206v1	http://arxiv.org/pdf/physics/0411206v1	2004	Leave-one-out prediction error of systolic arterial pressure time series   under paced breathing	N. Ancona|R. Maestri|D. Marinazzo|L. Nitti|M. Pellicoro|G. D. Pinna|S. Stramaglia	  In this paper we show that different physiological states and pathological conditions may be characterized in terms of predictability of time series signals from the underlying biological system. In particular we consider systolic arterial pressure time series from healthy subjects and Chronic Heart Failure patients, undergoing paced respiration. We model time series by the regularized least squares approach and quantify predictability by the leave-one-out error. We find that the entrainment mechanism connected to paced breath, that renders the arterial blood pressure signal more regular, thus more predictable, is less effective in patients, and this effect correlates with the seriousness of the heart failure. The leave-one-out error separates controls from patients and, when all orders of nonlinearity are taken into account, alive patients from patients for which cardiac death occurred. 	
0501014v1	http://arxiv.org/pdf/physics/0501014v1	2005	Numerical study of the temperature and porosity effects on the fracture   propagation in a 2D network of elastic bonds	Harold Auradou|Maria Zei|Elisabeth Bouchaud	  This article reports results concerning the fracture of a 2d triangular lattice of atoms linked by springs. The lattice is submitted to controlled strain tests and the influence of both porosity and temperature on failure is investigated. The porosity is found on one hand to decrease the stiffness of the material but on the other hand it increases the deformation sustained prior to failure. Temperature is shown to control the ductility due to the presence of cavities that grow and merge. The rough surfaces resulting from the propagation of the crack exhibit self-affine properties with a roughness exponent $\zeta = 0.59 \pm 0.07$ over a range of length scales which increases with temperature. Large cavities also have rough walls which are found to be fractal with a dimension, $D$, which evolves with the distance from the crack tip. For large distances, $D$ is found to be close to 1.5, and close to 1.0 for cavities just before their coalescence with the main crack. 	
0101050v1	http://arxiv.org/pdf/quant-ph/0101050v1	2001	New Tests of Macroscopic Local Realism using Continuous Variable   Measurements	M. D. Reid	  We show that quantum mechanics predicts an Einstein-Podolsky-Rosen paradox (EPR), and also a contradiction with local hidden variable theories, for photon number measurements which have limited resolving power, to the point of imposing an uncertainty in the photon number result which is macroscopic in absolute terms. We show how this can be interpreted as a failure of a new, very strong premise, called macroscopic local realism. We link this premise to the Schrodinger-cat paradox. Our proposed experiments ensure all fields incident on each measurement apparatus are macroscopic. We show that an alternative measurement scheme corresponds to balanced homodyne detection of quadrature phase amplitudes. The implication is that where either EPR correlations or failure of local realism is predicted for continuous variable (quadrature phase amplitude) measurements, one can perform a modified experiment which would lead to conclusions about the much stronger premise of macroscopic local realism. 	
0603017v2	http://arxiv.org/pdf/quant-ph/0603017v2	2006	Feats, Features and Failures of the PR-box	Valerio Scarani	  One of the most intriguing features of quantum physics is the non-locality of correlations that can be obtained by measuring entangled particles. Recently, it has been noticed that non-locality can be studied without reference to the Hilbert space formalism. I review here the properties of the basic mathematical tool used for such studies, the so called Popescu-Rohrlich-box, in short PR-box. Among its feats, are the simulation of the correlations of the singlet and of other non-local probability distributions. Among its features, the "anomaly of non-locality" and a great power for information-theoretical tasks. Among its failures, the impossibility of reproducing all multi-partite distributions and the triviality of the allowed dynamics. 	
0801.4701v1	http://arxiv.org/pdf/0801.4701v1	2008	Energy bursts in fiber bundle models of composite materials	Srutarshi Pradhan|Per C. Hemmer	  As a model of composite materials, a bundle of many fibers with stochastically distributed breaking thresholds for the individual fibers is considered. The bundle is loaded until complete failure to capture the failure scenario of composite materials under external load. The fibers are assumed to share the load equally, and to obey Hookean elasticity right up to the breaking point. We determine the distribution of bursts in which an amount of energy $E$ is released. The energy distribution follows asymptotically a universal power law $E^{-5/2}$, for any statistical distribution of fiber strengths. A similar power law dependence is found in some experimental acoustic emission studies of loaded composite materials. 	
0806.3121v1	http://arxiv.org/pdf/0806.3121v1	2008	Algorithmic Based Fault Tolerance Applied to High Performance Computing	George Bosilca|Remi Delmas|Jack Dongarra|Julien Langou	  We present a new approach to fault tolerance for High Performance Computing system. Our approach is based on a careful adaptation of the Algorithmic Based Fault Tolerance technique (Huang and Abraham, 1984) to the need of parallel distributed computation. We obtain a strongly scalable mechanism for fault tolerance. We can also detect and correct errors (bit-flip) on the fly of a computation. To assess the viability of our approach, we have developed a fault tolerant matrix-matrix multiplication subroutine and we propose some models to predict its running time. Our parallel fault-tolerant matrix-matrix multiplication scores 1.4 TFLOPS on 484 processors (cluster jacquard.nersc.gov) and returns a correct result while one process failure has happened. This represents 65% of the machine peak efficiency and less than 12% overhead with respect to the fastest failure-free implementation. We predict (and have observed) that, as we increase the processor count, the overhead of the fault tolerance drops significantly. 	
0812.3303v1	http://arxiv.org/pdf/0812.3303v1	2008	Wetting failure and contact line dynamics in a Couette flow	M. Sbragaglia|K. Sugiyama|L. Biferale	  Liquid-liquid wetting failure is investigated in a two-dimensional Couette system with two immiscible fluids of arbitrary viscosity. The problem is solved exactly using a sharp interface treatment of hydrodynamics (lubrication theory) as a function of the capillary number, viscous ratio and separation of scale, i.e. slip length versus macroscopic scale of the system. The existence of critical velocities, above which no stationary solutions are found, is analyzed in detail in terms of the relevant parameters of the system. Comparisons with existing analysis for other geometries are also carried out. A numerical method of analysis is also presented, based on diffuse interface models obtained from multiphase extensions of the lattice Boltzmann equation (LBE). Sharp interface and diffuse interface models are quantitatively compared face to face indicating the correct limit of applicability of the diffuse interface models. 	
0907.3353v2	http://arxiv.org/pdf/0907.3353v2	2010	Intermittency and roughening in the failure of brittle heterogeneous   materials	D. Bonamy	  Stress enhancement in the vicinity of brittle cracks makes the macro-scale failure properties extremely sensitive to the micro-scale material disorder. Therefore: (i) Fracturing systems often display a jerky dynamics, so-called crackling noise, with seemingly random sudden energy release spanning over a broad range of scales, reminiscent of earthquakes; (ii) Fracture surfaces exhibit roughness at scales much larger than that of material micro-structure. Here, I provide a critical review of experiments and simulations performed in this context, highlighting the existence of universal scaling features, independent of both the material and the loading conditions, reminiscent of critical phenomena. I finally discuss recent stochastic descriptions of crack growth in brittle disordered media that seem to capture qualitatively - and sometimes quantitatively - these scaling features. 	
0910.3972v1	http://arxiv.org/pdf/0910.3972v1	2009	Thermally Induced Local Failures in Quasi-One-Dimensional Systems:   Collapse in Carbon Nanotubes, Necking in Nanowires and Opening of Bubbles in   DNA	Cristiano Nisoli|Douglas Abraham|Turab Lookman|Avadh Saxena	  We present a general framework to explore thermally activated failures in quasi one dimensional systems. We apply it to the collapse of carbon nanotubes, the formation of bottlenecks in nanowires, both of which limit conductance, and the opening of local regions or "bubbles" of base pairs in strands of DNA that are relevant for transcription and danaturation. We predict an exponential behavior for the probability of the opening of bubbles in DNA, the average distance between flattened regions of a nanotube or necking in a nanowire as a monotonically decreasing function of temperature, and compute a temperature below which these events become extremely rare. These findings are difficult to obtain numerically, however, they could be accessible experimentally. 	
1001.1225v2	http://arxiv.org/pdf/1001.1225v2	2010	Percolation on bipartite scale-free networks	Hans Hooyberghs|Bert Van Schaeybroeck|Joseph O. Indekeu	  Recent studies introduced biased (degree-dependent) edge percolation as a model for failures in real-life systems. In this work, such process is applied to networks consisting of two types of nodes with edges running only between nodes of unlike type. Such bipartite graphs appear in many social networks, for instance in affiliation networks and in sexual contact networks in which both types of nodes show the scale-free characteristic for the degree distribution. During the depreciation process, an edge between nodes with degrees k and q is retained with probability proportional to (kq)^(-alpha), where alpha is positive so that links between hubs are more prone to failure. The removal process is studied analytically by introducing a generating functions theory. We deduce exact self-consistent equations describing the system at a macroscopic level and discuss the percolation transition. Critical exponents are obtained by exploiting the Fortuin-Kasteleyn construction which provides a link between our model and a limit of the Potts model. 	
1002.4938v2	http://arxiv.org/pdf/1002.4938v2	2010	Cavity analysis on the robustness of random networks against targeted   attacks: Influences of degree-degree correlations	Yoshifumi Shiraki|Yoshiyuki Kabashima	  We developed a scheme for evaluating the size of the largest connected subnetwork (giant component) in random networks and the percolation threshold when sites (nodes) and/or bonds (edges) are removed from the networks based on the cavity method of statistical mechanics of disordered systems. An advantage of our scheme is the capability of handling targeted attacks on sites/bonds in the presence of degree correlations beyond naive analyses on random failures (crashes) in networks of no degree correlations. We apply our scheme particularly to random networks of bimodal degree distribution (two-peak networks), which have been proposed in earlier studies as robust networks against random failures of site and/or targeted attacks on sites, and show that the correlations among degrees affect a network's robustness against targeted attacks on sites or bonds non-trivially depending on details of network configurations. 	
1004.2322v1	http://arxiv.org/pdf/1004.2322v1	2010	Dynamical Jumping Real-Time Fault-Tolerant Routing Protocol for Wireless   Sensor Networks	Guowei Wu|Chi Lin|Feng Xia|Lin Yao|He Zhang|Bing Liu	  In time-critical wireless sensor network (WSN) applications, a high degree of reliability is commonly required. A dynamical jumping real-time fault-tolerant routing protocol (DMRF) is proposed in this paper. Each node utilizes the remaining transmission time of the data packets and the state of the forwarding candidate node set to dynamically choose the next hop. Once node failure, network congestion or void region occurs, the transmission mode will switch to jumping transmission mode, which can reduce the transmission time delay, guaranteeing the data packets to be sent to the destination node within the specified time limit. By using feedback mechanism, each node dynamically adjusts the jumping probabilities to increase the ratio of successful transmission. Simulation results show that DMRF can not only efficiently reduce the effects of failure nodes, congestion and void region, but also yield higher ratio of successful transmission, smaller transmission delay and reduced number of control packets. 	
1004.4684v2	http://arxiv.org/pdf/1004.4684v2	2010	Deformation and Failure of Amorphous Solidlike Materials	Michael L. Falk|James S. Langer	  Since the 1970's, theories of deformation and failure of amorphous, solidlike materials have started with models in which stress-driven, molecular rearrangements occur at localized flow defects via "shear transformations". This picture is the basis for the modern theory of "shear transformation zones" (STZ's), which is the focus of this review. We begin by describing the structure of the theory in general terms and by showing several applications, specifically: interpretation of stress-strain measurements for a bulk metallic glass, analysis of numerical simulations of shear banding, and the use of the STZ equations of motion in free-boundary calculations. In the second half of this article, we focus for simplicity on what we call an "athermal" model of amorphous plasticity, and use that model to illustrate how the STZ theory emerges within a systematic formulation of nonequilibrium thermodynamics. 	
1011.4135v1	http://arxiv.org/pdf/1011.4135v1	2010	Progressive Decoding for Data Availability and Reliability in   Distributed Networked Storage	Yunghsiang Han|Soji Omiwade|Rong Zheng	  To harness the ever growing capacity and decreasing cost of storage, providing an abstraction of dependable storage in the presence of crash-stop and Byzantine failures is compulsory. We propose a decentralized Reed Solomon coding mechanism with minimum communication overhead. Using a progressive data retrieval scheme, a data collector contacts only the necessary number of storage nodes needed to guarantee data integrity. The scheme gracefully adapts the cost of successful data retrieval to the number of storage node failures. Moreover, by leveraging the Welch-Berlekamp algorithm, it avoids unnecessary computations. Compared to the state-of-the-art decoding scheme, the implementation and evaluation results show that our progressive data retrieval scheme has up to 35 times better computation performance for low Byzantine node rates. Additionally, the communication cost in data retrieval is derived analytically and corroborated by Monte-Carlo simulation results. Our implementation is flexible in that the level of redundancy it provides is independent of the number of data generating nodes, a requirement for distributed storage systems 	
1102.1609v3	http://arxiv.org/pdf/1102.1609v3	2011	Exact Minimum-Repair-Bandwidth Cooperative Regenerating Codes for   Distributed Storage Systems	Kenneth W. Shum|Yuchong Hu	  In order to provide high data reliability, distributed storage systems disperse data with redundancy to multiple storage nodes. Regenerating codes is a new class of erasure codes to introduce redundancy for the purpose of improving the data repair performance in distributed storage. Most of the studies on regenerating codes focus on the single-failure recovery, but it is not uncommon to see two or more node failures at the same time in large storage networks. To exploit the opportunity of repairing multiple failed nodes simultaneously, a cooperative repair mechanism, in the sense that the nodes to be repaired can exchange data among themselves, is investigated. A lower bound on the repair-bandwidth for cooperative repair is derived and a construction of a family of exact cooperative regenerating codes matching this lower bound is presented. 	
1106.3234v3	http://arxiv.org/pdf/1106.3234v3	2013	Towards designing robust coupled networks	Christian M. Schneider|Nuri Yazdani|Nuno A. M. Araujo|Shlomo Havlin|Hans J. Herrmann	  Natural and technological interdependent systems have been shown to be highly vulnerable due to cascading failures and an abrupt collapse of global connectivity under initial failure. Mitigating the risk by partial disconnection endangers their functionality. Here we propose a systematic strategy of selecting a minimum number of autonomous nodes that guarantee a smooth transition in robustness. Our method which is based on betweenness is tested on various examples including the famous 2003 electrical blackout of Italy. We show that, with this strategy, the necessary number of autonomous nodes can be reduced by a factor of five compared to a random choice. We also find that the transition to abrupt collapse follows tricritical scaling characterized by a set of exponents which is independent on the protection strategy. 	
1108.3167v2	http://arxiv.org/pdf/1108.3167v2	2011	Local/global model order reduction strategy for the simulation of   quasi-brittle fracture	Pierre Kerfriden|Jean-Charles Passieux|Stephane Pierre-Alain Bordas	  This paper proposes a novel technique to reduce the computational burden associated with the simulation of localised failure. The proposed methodology affords the simulation of damage initiation and propagation whilst concentrating the computational effort where it is most needed, i.e. in the localisation zones. To do so, a local/global technique is devised where the global (slave) problem (far from the zones undergoing severe damage and cracking) is solved for in a reduced space computed by the classical Proper Orthogonal Decomposition, while the local (master) degrees of freedom (associated with the part of the structure where most of the damage is taking place) are fully resolved. Both domains are coupled through a local/global technique. This method circumvents the difficulties associated with model order reduction for the simulation of highly non-linear mechanical failure and offers an alternative or complementary approach to the development of multiscale fracture simulators. 	
1112.2046v1	http://arxiv.org/pdf/1112.2046v1	2011	Improving TCP Performance over Wireless Network with Frequent   Disconnections	Purvang Dalal|Nikhil Kothari|K. S. Dasgupta	  Presented in this paper is the solution to the problem that arises when the TCP/IP protocol suite is used to provide Internet connectivity through mobile terminals over emerging 802.11 wireless links. Taking into consideration the strong drive towards wireless Internet access through mobile terminals, the problem of frequent disconnections causing serial timeouts is examined and analyzed, with the help of extensive simulations. After a detailed review of wireless link loss recovery mechanism and identification of related problems, a new scheme with modifications at link layer and transport layer is proposed. The proposed modifications which depend on interaction between two layers (i) reduce the idle time before transmission at TCP by preventing timeout occurrences and (ii) decouple the congestion control from recovery of the losses due to link failure. Results of simulation based experiments demonstrate considerable performance improvement with the proposed modifications over the conventional TCP, when a wireless sender is experiencing frequent link failures. 	
1203.6778v1	http://arxiv.org/pdf/1203.6778v1	2012	Systemic losses in banking networks: indirect interaction of nodes via   asset prices	Igor Tsatskis	  A simple banking network model is proposed which features multiple waves of bank defaults and is analytically solvable in the limiting case of an infinitely large homogeneous network. The model is a collection of nodes representing individual banks; associated with each node is a balance sheet consisting of assets and liabilities. Initial node failures are triggered by external correlated shocks applied to the asset sides of the balance sheets. These defaults lead to further reductions in asset values of all nodes which in turn produce additional failures, and so on. This mechanism induces indirect interactions between the nodes and leads to a cascade of defaults. There are no interbank links, and therefore no direct interactions, between the nodes. The resulting probability distribution for the total (direct plus systemic) network loss can be viewed as a modification of the well-known Vasicek distribution. 	
1212.6967v1	http://arxiv.org/pdf/1212.6967v1	2012	Entropic Inference: some pitfalls and paradoxes we can avoid	Ariel Caticha	  The method of maximum entropy has been very successful but there are cases where it has either failed or led to paradoxes that have cast doubt on its general legitimacy. My more optimistic assessment is that such failures and paradoxes provide us with valuable learning opportunities to sharpen our skills in the proper way to deploy entropic methods. The central theme of this paper revolves around the different ways in which constraints are used to capture the information that is relevant to a problem. This leads us to focus on four epistemically different types of constraints. I propose that the failure to recognize the distinctions between them is a prime source of errors. I explicitly discuss two examples. One concerns the dangers involved in replacing expected values with sample averages. The other revolves around misunderstanding ignorance. I discuss the Friedman-Shimony paradox as it is manifested in the three-sided die problem and also in its original thermodynamic formulation. 	
1302.0744v2	http://arxiv.org/pdf/1302.0744v2	2013	Explicit MBR All-Symbol Locality Codes	Govinda M. Kamath|Natalia Silberstein|N. Prakash|Ankit S. Rawat|V. Lalitha|O. Ozan Koyluoglu|P. Vijay Kumar|Sriram Vishwanath	  Node failures are inevitable in distributed storage systems (DSS). To enable efficient repair when faced with such failures, two main techniques are known: Regenerating codes, i.e., codes that minimize the total repair bandwidth; and codes with locality, which minimize the number of nodes participating in the repair process. This paper focuses on regenerating codes with locality, using pre-coding based on Gabidulin codes, and presents constructions that utilize minimum bandwidth regenerating (MBR) local codes. The constructions achieve maximum resilience (i.e., optimal minimum distance) and have maximum capacity (i.e., maximum rate). Finally, the same pre-coding mechanism can be combined with a subclass of fractional-repetition codes to enable maximum resilience and repair-by-transfer simultaneously. 	
1303.4918v1	http://arxiv.org/pdf/1303.4918v1	2013	Non-Markovian Models of Blocking in Concurrent and Countercurrent Flows	Andrea Gabrielli|Julian Talbot|Pascal Viot	  We investigate models in which blocking can interrupt a particulate flow process at any time. Filtration, and flow in micro/nano-channels and traffic flow are examples of such processes. We first consider concurrent flow models where particles enter a channel randomly. If at any time two particles are simultaneously present in the channel, failure occurs. The key quantities are the survival probability and the distribution of the number of particles that pass before failure. We then consider a counterflow model with two opposing Poisson streams. There is no restriction on the number of particles passing in the same direction, but blockage occurs if, at any time, two opposing particles are simultaneously present in the passage. 	
1307.0433v2	http://arxiv.org/pdf/1307.0433v2	2013	'Mutual Watch-dog Networking': Distributed Awareness of Faults and   Critical Events in Petascale/Exascale systems	Roberto Ammendola|Andrea Biagioni|Ottorino Frezza|Francesca Lo Cicero|Alessandro Lonardo|Pier Stanislao Paolucci|Davide Rossetti|Francesco Simula|Laura Tosoratto|Piero Vicini	  Many tile systems require techniques to be applied to increase components resilience and control the FIT (Failures In Time) rate. When scaling to peta- exa-scale systems the FIT rate may become unacceptable due to component numerosity, requiring more systemic countermeasures. Thus, the ability to be fault aware, i.e. to detect and collect information about fault and critical events, is a necessary feature that large scale distributed architectures must provide in order to apply systemic fault tolerance techniques. In this context, the LO|FA|MO approach is a way to obtain systemic fault awareness, by implementing a mutual watchdog mechanism and guaranteeing fault detection in a no-single-point-of-failure fashion. This document contains specification and implementation details about this approach, in the shape of a technical report. 	
1307.1253v2	http://arxiv.org/pdf/1307.1253v2	2014	Network robustness of multiplex networks with interlayer degree   correlations	Byungjoon Min|Su Do Yi|Kyu-Min Lee|K. -I. Goh	  We study the robustness properties of multiplex networks consisting of multiple layers of distinct types of links, focusing on the role of correlations between degrees of a node in different layers. We use generating function formalism to address various notions of the network robustness relevant to multiplex networks such as the resilience of ordinary- and mutual connectivity under random or targeted node removals as well as the biconnectivity. We found that correlated coupling can affect the structural robustness of multiplex networks in diverse fashion. For example, for maximally-correlated duplex networks, all pairs of nodes in the giant component are connected via at least two independent paths and network structure is highly resilient to random failure. In contrast, anti-correlated duplex networks are on one hand robust against targeted attack on high-degree nodes, but on the other hand they can be vulnerable to random failure. 	
1307.1354v3	http://arxiv.org/pdf/1307.1354v3	2014	Modeling and Predicting the Growth and Death of Membership-based   Websites	Bruno Ribeiro	  Driven by outstanding success stories of Internet startups such as Facebook and The Huffington Post, recent studies have thoroughly described their growth. These highly visible online success stories, however, overshadow an untold number of similar ventures that fail. The study of website popularity is ultimately incomplete without general mechanisms that can describe both successes and failures. In this work we present six years of the daily number of users (DAU) of twenty-two membership-based websites - encompassing online social networks, grassroots movements, online forums, and membership-only Internet stores - well balanced between successes and failures. We then propose a combination of reaction-diffusion-decay processes whose resulting equations seem not only to describe well the observed DAU time series but also provide means to roughly predict their evolution. This model allows an approximate automatic DAU-based classification of websites into self-sustainable v.s. unsustainable and whether the startup growth is mostly driven by marketing & media campaigns or word-of-mouth adoptions. 	
1307.6682v1	http://arxiv.org/pdf/1307.6682v1	2013	Phase Model with Feedback Control for Power Grids	Tatsuma Matsuo|Hidetsugu Sakaguchi	  A phase model with feedback control is studied as a dynamical model of power grids. As an example, we study a model network corresponding to the power grid in the Kyushu region. The standard frequency is maintained by the mutual synchronization and the feedback control. Electric failures are induced by an overload. We propose a local feedback method in which the strength of feedback control is proportional to the magnitude of generators. We find that the electric failures do not occur until the utilization ratio is close to 1 under this feedback control. We also find that the temporal response for the time-varying input power is suppressed under this feedback control. We explain the mechanisms using the corresponding global feedback method. 	
1308.4301v1	http://arxiv.org/pdf/1308.4301v1	2013	Crossover Behaviour In Driven Cascades	James Burridge	  We propose a model which explains how power-law crossover behaviour can arise in a system which is capable of experiencing cascading failure. In our model the susceptibility of the system to cascades is described by a single number, the propagation power, which measures the ease with which cascades propagate. Physically, such a number could represent the density of unstable material in a system, its internal connectivity, or the mean susceptibility of its component parts to failure. We assume that the propagation power follows an upward drifting Brownian motion between cascades, and drops discontinuously each time a cascade occurs. Cascades are described by a continuous state branching process with distributional properties determined by the value of the propagation power when they occur. In common with many cascading models, pure power law behaviour is exhibited at a critical level of propagation power, and the mean cascade size diverges. This divergence constrains large systems to the subcritical region. We show that as a result, crossover behaviour appears in the cascade distribution when an average is performed over the distribution of propagation power. We are able to analytically determine the exponents before and after the crossover. 	
1312.3739v1	http://arxiv.org/pdf/1312.3739v1	2013	Semantics of (Resilient) X10	Silvia Crafa|David Cunningham|Vijay Saraswat|Avraham Shinnar|Olivier Tardieu	  We present a formal small-step structural operational semantics for a large fragment of X10, unifying past work. The fragment covers multiple places, mutable objects on the heap, sequencing, \code{try/catch}, \code{async}, \code{finish}, and \code{at} constructs. This model accurately captures the behavior of a large class of concurrent, multi-place X10 programs. Further, we introduce a formal model of resilience in X10. During execution of an X10 program, a place may fail for many reasons. Resilient X10 permits the program to continue executing, losing the data at the failed place, and most of the control state, and repairing the global control state in such a way that key semantic principles hold, the Invariant Happens Before Principle, and the Failure Masking Principle. These principles permit an X10 programmer to write clean code that continues to work in the presence of place failure. The given semantics have additionally been mechanized in Coq. 	
1402.6841v2	http://arxiv.org/pdf/1402.6841v2	2014	On the failure of mean-field theories near a critical point	Navinder Singh	  It is well known that mean-field theories fail to reproduce the experimentally known critical exponents. The traditional argument which explain this failure of mean-field theories near a critical point is the Ginsburg criterion in which diverging fluctuations of the order parameter is the root cause. We argue, contrary to the above mentioned traditional view, that diverging fluctuations in real physical systems near a critical point are genuine consequence of the breakdown of the property of statistical independence, and are faithfully reproduced by the mean-field theory. By looking at the problem from the point of view of "statistical independence" the divergence of fluctuations in real physical systems near criticality becomes immediately apparent as a connection can be established between diverging correlation length and diverging fluctuations. To address the question of why mean-field theories, much successful qualitatively, fail to reproduce the known values of critical indices we argue, using the essential ideas of the Wilsonian renormalization group, that mean-field theories fail to capture the long length scale averages of an order parameter near a critical point. 	
1404.4584v2	http://arxiv.org/pdf/1404.4584v2	2014	Fracture strength: Stress concentration, extreme value statistics and   the fate of the Weibull distribution	Zsolt Bertalan|Ashivni Shekhawat|James P. Sethna|Stefano Zapperi	  The fracture strength distribution of materials is often described in terms of the Weibull law which can be derived by using extreme value statistics if elastic interactions are ignored. Here, we consider explicitly the interplay between elasticity and disorder and test the asymptotic validity of the Weibull distribution through numerical simulations of the two-dimensional random fuse model. Even when the local fracture strength follows the Weibull distribution, the global failure distribution is dictated by stress enhancement at the tip of the cracks and sometimes deviates from the Weibull law. Only in the case of a pre-existing power law distribution of crack widths do we find that the failure strength is Weibull distributed. Contrary to conventional assumptions, even in this case, the Weibull exponent can not be simply inferred from the exponent of the initial crack width distribution. Our results thus raise some concerns on the applicability of the Weibull distribution in most practical cases. 	
1404.7287v2	http://arxiv.org/pdf/1404.7287v2	2014	Disjoint-Path Selection in Internet: What traceroutes tell us?	Sameer Qazi|Tim Moors	  Routing policies used in the Internet can be restrictive, limiting communication between source-destination pairs to one path, when often better alternatives exist. To avoid route flapping, recovery mechanisms may be dampened, making adaptation slow. Unstructured overlays have been proposed to mitigate the issues of path and performance failures in the Internet by routing through an indirect-path via overlay peer(s). Choosing alternate-paths in overlay networks is a challenging issue. Ensuring both availability and performance guarantees on alternate paths requires aggressive monitoring of all overlay paths using active probing; this limits scalability. An alternate technique to select an overlay-path is to bias its selection based on physical disjointness criteria to bypass the failure on the primary-path. Recently, several techniques have emerged which can optimize the selection of a disjoint-path without incurring the high costs associated with probing paths. In this paper, we show that using only commodity approaches, i.e. running infrequent traceroutes between overlay hosts, a lot of information can be revealed about the underlying physical path diversity in the overlay network which can be used to make informed-guesses for alternate-path selection. We test our approach using datasets between real-world hosts in AMP and RIPE networks. 	
1405.2992v1	http://arxiv.org/pdf/1405.2992v1	2014	Correlating power consumption and network traffic for improving data   centers resiliency	Roberto Baldoni|Mario Caruso|Adriano Cerocchi|Claudio Ciccotelli|Luca Montanari|Luca Nicoletti	  The deployment of business critical applications and information infrastructures are moving to the cloud. This means they are hosted in large scale data centers with other business applications and infrastructures with less (or none) mission critical constraints. This mixed and complex environment makes very challenging the process of monitoring critical applications and handling (detecting and recovering) possible failures of servers' data center that could affect responsiveness and/or reliability of mission critical applications. Monitoring mechanisms used in data center are usually intrusive in the sense that they need to install agents on each single server. This has considerable drawbacks: huge usage of human resources to install and patch the system and interference with the critical application because agents share application resources. In order to detect (and possibly predict) failures in data centers the paper does a first attempt in showing the correlation between network traffic and servers' power consumption. This is an important step in deriving non-intrusive monitoring systems, as both network traffic and power consumption can be captured without installing any software at the servers. This will improve in its turn the overall resiliency of the data center and its self-managing capacity. 	
1406.4613v2	http://arxiv.org/pdf/1406.4613v2	2014	Failure of the Generalized Eigenstate Thermalization Hypothesis in   integrable models with multiple particle species	B. Pozsgay	  It has been recently observed for a particular quantum quench in the XXZ spin chain that local observables do not equilibrate to the predictions of the Generalized Gibbs Ensemble (GGE). In this work we argue that the breakdown of the GGE can be attributed to the failure of the Generalized Eigenstate Thermalization Hypothesis (GETH), which has been the main candidate to explain the validity of the GGE. We provide explicit counterexamples to the GETH and argue that generally it does not hold in models with multiple particle species. Therefore there is no reason to assume that the GGE should describe the long time limit of observables in these integrable models. 	
1406.5903v3	http://arxiv.org/pdf/1406.5903v3	2016	Blind Sensor Calibration using Approximate Message Passing	Christophe Schülke|Francesco Caltagirone|Lenka Zdeborová	  The ubiquity of approximately sparse data has led a variety of com- munities to great interest in compressed sensing algorithms. Although these are very successful and well understood for linear measurements with additive noise, applying them on real data can be problematic if imperfect sensing devices introduce deviations from this ideal signal ac- quisition process, caused by sensor decalibration or failure. We propose a message passing algorithm called calibration approximate message passing (Cal-AMP) that can treat a variety of such sensor-induced imperfections. In addition to deriving the general form of the algorithm, we numerically investigate two particular settings. In the first, a fraction of the sensors is faulty, giving readings unrelated to the signal. In the second, sensors are decalibrated and each one introduces a different multiplicative gain to the measures. Cal-AMP shares the scalability of approximate message passing, allowing to treat big sized instances of these problems, and ex- perimentally exhibits a phase transition between domains of success and failure. 	
1407.6910v1	http://arxiv.org/pdf/1407.6910v1	2014	Probabilistic metrology defeats ultimate deterministic bound	J. Calsamiglia|B. Gendra|R. Munoz-Tapia|E. Bagan	  Quantum-enhanced measurements exploit quantum mechanical effects to provide ultra-precise estimates of physical variables for use in advanced technologies, such as frequency calibration of atomic clocks, gravitational waves detection, and biosensing. Quantum metrology studies the fundamental limits in the estimation precision given a certain amount of resources (e.g. the number of probe systems) and restrictions (e.g. limited interaction time, or coping with unavoidable presence of noise). Here we show that, even in the presence of noise, probabilistic measurement strategies (which have a certain probability of failure or abstention) can provide, upon a heralded successful outcome, estimates with a precision that violates the deterministic bounds. This establishes a new ultimate quantum metrology limit. For probe systems subject to local dephasing, we quantify such precision limit as a function of the probability of failure that can be tolerated. We show that the possibility of abstaining can substantially set back the detrimental effects of noise. 	
1408.0485v1	http://arxiv.org/pdf/1408.0485v1	2014	Size dependent crush analysis of lithium orthosilicate pebbles	Ratna Kumar Annabattula|Matthias Kolb|Yixiang Gan|Rolf Rolli|Marc Kamlah	  Crushing strength of the breeder materials (lithium orthosilicate, $\rm{Li_4SiO_4}$ or OSi) in the form of pebbles to be used for EU solid breeder concept is investigated. The pebbles are fabricated using a melt-spray method and hence a size variation in the pebbles produced is expected. The knowledge of the mechanical integrity (crush strength) of the pebbles is important for a successful design of breeder blanket. In this paper, we present the experimental results of the crush (failure) loads for spherical OSi pebbles of different diameters ranging from $250~\mu$m to $800~\mu$m. The ultimate failure load for each size shows a Weibull distribution. Furthermore, the mean crush load increases with increase in pebble diameter. It is also observed that the level of opacity of the pebble influences the crush load significantly. The experimental data presented in this paper and the associated analysis could possibly help us to develop a framework for simulating a crushable polydisperse pebble assembly using discrete element method. 	
1409.5622v1	http://arxiv.org/pdf/1409.5622v1	2014	Instability of Sharing Systems in the Presence of Retransmissions	Predrag R. Jelenković|Evangelia D. Skiani	  Retransmissions represent a primary failure recovery mechanism on all layers of communication network architecture. Similarly, fair sharing, e.g. processor sharing (PS), is a widely accepted approach to resource allocation among multiple users. Recent work has shown that retransmissions in failure-prone, e.g. wireless ad hoc, networks can cause heavy tails and long delays. In this paper, we discover a new phenomenon showing that PS-based scheduling induces complete instability with zero throughput in the presence of retransmissions, regardless of how low the traffic load may be. This phenomenon occurs even when the job sizes are bounded/fragmented, e.g. deterministic. Our analytical results are further validated via simulation experiments. Moreover, our work demonstrates that scheduling one job at a time, such as first-come-first-serve, achieves stability and should be preferred in these systems. 	
1412.1331v1	http://arxiv.org/pdf/1412.1331v1	2014	On analysis of incomplete field failure data	Zhisheng Ye|Hon Keung Tony Ng	  Many commercial products are sold with warranties and indirectly through dealers. The manufacturer-retailer distribution mechanism results in serious missing data problems in field return data, as the sales date for an unreturned unit is generally unknown to the manufacturer. This study considers a general setting for field failure data with unknown sales dates and a warranty limit. A stochastic expectation-maximization (SEM) algorithm is developed to estimate the distributions of the sales lag (time between shipment to a retailer and sale to a customer) and the lifetime of the product under study. Extensive simulations are used to evaluate the performance of the SEM algorithm and to compare with the imputation method proposed by Ghosh [Ann. Appl. Stat. 4 (2010) 1976-1999]. Three real examples illustrate the methodology proposed in this paper. 	
1412.2313v1	http://arxiv.org/pdf/1412.2313v1	2014	Microstructural Evolution of Charged Defects in the Fatigue Process of   Polycrystalline BiFeO3 Thin Films	Qingqing Ke|Amit Kumar|Xiaojie Lou|Yuan Ping Feng|Kaiyang Zeng|Yongqing Cai|John Wang	  Fatigue failure in ferroelectrics has been intensively investigated in the past few decades. Most of the mechanisms discussed for ferroelectric fatigue have been built on the "hypothesis of variation in charged defects", which however are rarely evidenced by experimental observation. Here, using a combination of complex impedance spectra techniques, piezoresponse force microscopy and first-principles theory, we examine the microscopic evolution and redistribution of charged defects during the electrical cycling in BiFeO3 thin films. The dynamic formation and melting behaviors of oxygen vacancy (VO) order are identified during the fatigue process. It reveals that the isolated VO tend to self-order along grain boundaries to form a planar-aligned structure, which blocks the domain reversals. Upon further electrical cycling, migration of VO within vacancy clusters is accommodated with a lower energy barrier (~0.2 eV) and facilitates the formation of nearby-electrode layer incorporated with highly concentrated VO. The interplay between the macroscopic fatigue and microscopic evolution of charged defects clearly demonstrates the role of ordered VO cluster in the fatigue failure of BiFeO3 thin films. 	
1501.07400v1	http://arxiv.org/pdf/1501.07400v1	2015	Resilience for Exascale Enabled Multigrid Methods	Markus Huber|Björn Gmeiner|Ulrich Rüde|Barbara Wohlmuth	  With the increasing number of components and further miniaturization the mean time between faults in supercomputers will decrease. System level fault tolerance techniques are expensive and cost energy, since they are often based on redundancy. Also classical check-point-restart techniques reach their limits when the time for storing the system state to backup memory becomes excessive. Therefore, algorithm-based fault tolerance mechanisms can become an attractive alternative. This article investigates the solution process for elliptic partial differential equations that are discretized by finite elements. Faults that occur in the parallel geometric multigrid solver are studied in various model scenarios. In a standard domain partitioning approach, the impact of a failure of a core or a node will affect one or several subdomains. Different strategies are developed to compensate the effect of such a failure algorithmically. The recovery is achieved by solving a local subproblem with Dirichlet boundary conditions using local multigrid cycling algorithms. Additionally, we propose a superman strategy where extra compute power is employed to minimize the time of the recovery process. 	
1504.08359v2	http://arxiv.org/pdf/1504.08359v2	2015	New insights into the problem with a singular drift term in the complex   Langevin method	Jun Nishimura|Shinji Shimasaki	  The complex Langevin method aims at performing path integral with a complex action numerically based on complexification of the original real dynamical variables. One of the poorly understood issues concerns occasional failure in the presence of logarithmic singularities in the action, which appear, for instance, from the fermion determinant in finite density QCD. We point out that the failure should be attributed to the breakdown of the relation between the complex weight that satisfies the Fokker-Planck equation and the probability distribution associated with the stochastic process. In fact, this problem can occur in general when the stochastic process involves a singular drift term. We show, however, in a simple example that there exists a parameter region in which the method works although the standard reweighting method is hardly applicable. 	
1509.04357v1	http://arxiv.org/pdf/1509.04357v1	2015	Fragmentation properties of two-dimensional Proximity Graphs considering   random failures and targeted attacks	Christoph Norrenbrock|Oliver Melchert|Alexander K. Hartmann	  The pivotal quality of proximity graphs is connectivity, i.e. all nodes in the graph are connected to one another either directly or via intermediate nodes. These types of graphs are robust, i.e., they are able to function well even if they are subject to limited removal of elementary building blocks, as it may occur for random failures or targeted attacks. Here, we study how the structure of these graphs is affected when nodes get removed successively until an extensive fraction is removed such that the graphs fragment. We study different types of proximity graphs for various node removal strategies. We use different types of observables to monitor the fragmentation process, simple ones like number and sizes of connected components, and more complex ones like the hop diameter and the backup capacity, which is needed to make a network N-1 resilient. The actual fragmentation turns out to be described by a second order phase transition. Using finite-size scaling analyses we numerically assess the threshold fraction of removed nodes, which is characteristic for the particular graph type and node deletion scheme, that suffices to decompose the underlying graphs. 	
1602.05370v1	http://arxiv.org/pdf/1602.05370v1	2016	Super-stretchable borophene and its stability under straining	Zhenqian Pang|Xin Qian|Ronggui Yang|Yujie Wei	  Recent success in synthesizing two-dimensional borophene on silver substrate attracts strong interest in exploring its possible extraordinary physical properties. By using the density functional theory calculations, we show that borophene is highly stretchable along the transverse direction. The strain-to-failure in the transverse direction is nearly twice as that along the longitudinal direction. The straining induced flattening and subsequent stretch of the flat borophene are accounted for the large strain-to-failure for tension in the transverse direction. The mechanical properties in the other two directions exhibit strong anisotropy. Phonon dispersions of the strained borophene monolayers suggest that negative frequencies are presented, which indicates the instability of free-standing borophene even under high tensile stress. 	
1602.07657v2	http://arxiv.org/pdf/1602.07657v2	2016	Substrate effect and nanoindentation fracture toughness based on pile up   and failure	Arnab S. Bhattacharyya|R. Praveen Kumar|Rohit Mandal|Nikhil Kumar|N. Rajak|Abhishek Sharma|Shashi Kant	  The effect of substrate was studied using nanoindentation on thin films. Soft films on hard substrate showed more pile up than usual which was attributed to the dislocation pile up at the film substrate interface. The effect of tip blunting on the load depth and hardness plots of nanoindentation was shown. The experimental date of variation of Vickers hardness with film thickness and loads were fitted and new parameters were analyzed. The delaminated area was analyzed using geometrical shapes using optical view of the failure region along with the load displacement Indentation fracture using Nanoindentation using Berkovich indenter has been studied. Indentation fracture toughness (KR) was analyzed based on computational programs. The contact mechanics during nanoindentation was studied with parameters related to indenter shape and tip sharpness. Elastic, plastic and total energies were computationally determined. The energy difference was related to shear stress being generated with elastic to plastic transition. Change in the nature of residual stress was related to film thickness. 	
1603.00154v1	http://arxiv.org/pdf/1603.00154v1	2016	Broadcast Repair for Wireless Distributed Storage Systems	Ping Hu|Chi Wan Sung|Terence H. Chan	  In wireless distributed storage systems, storage nodes are connected by wireless channels, which are broadcast in nature. This paper exploits this unique feature to design an efficient repair mechanism, called broadcast repair, for wireless distributed storage systems with multiple-node failures. Since wireless channels are typically bandwidth limited, we advocate a new measure on repair performance called repair-transmission bandwidth, which measures the average number of packets transmitted by helper nodes per failed node. The fundamental tradeoff between storage amount and repair-transmission bandwidth is obtained. It is shown that broadcast repair outperforms cooperative repair, which is the basic repair method for wired distributed storage systems with multiple-node failures, in terms of storage efficiency and repair-transmission bandwidth, thus yielding a better tradeoff curve. 	
1604.03258v1	http://arxiv.org/pdf/1604.03258v1	2016	Topology and morphology influences on the onset of ductile failure in a   two-phase microstructure	T. W. J. de Geus|R. H. J. Peerlings|M. G. D. Geers	  Multi-phase material are frequently applied in a wide variety of products, as they posses a unique set of properties by combining two or more distinct phases at the level of the microstructure. Although the macroscopic stiffness and hardening are reasonably well understood, questions remain about the dominant failure mechanism(s). We identify the role of the microstructural topology (the distribution of phases) on damage "hot-spot" in the microstructure, by performing a numerical study on a large set of randomly generated topologies. The result identifies a distinct probability distribution of phases around a typical damage "hot-spot". This work is focused on assessing the sensitivity of the result to the assumptions made on the microstructural geometry. 	
1604.04591v1	http://arxiv.org/pdf/1604.04591v1	2016	An Interference-Free Programming Model for Network Objects	Mischael Schill|Christopher M. Poskitt|Bertrand Meyer	  Network objects are a simple and natural abstraction for distributed object-oriented programming. Languages that support network objects, however, often leave synchronization to the user, along with its associated pitfalls, such as data races and the possibility of failure. In this paper, we present D-SCOOP, a distributed programming model that allows for interference-free and transaction-like reasoning on (potentially multiple) network objects, with synchronization handled automatically, and network failures managed by a compensation mechanism. We achieve this by leveraging the runtime semantics of a multi-threaded object-oriented concurrency model, directly generalizing it with a message-based protocol for efficiently coordinating remote objects. We present our pathway to fusing these contrasting but complementary ideas, and evaluate the performance overhead of the automatic synchronization in D-SCOOP, finding that it comes close to---or outperforms---explicit locking-based synchronization in Java RMI. 	
1604.06173v1	http://arxiv.org/pdf/1604.06173v1	2016	Onset of Plasticity in Thin Polystyrene Films	Bekele J. Gurmessa|Andrew B. Croll	  Polymer glasses have numerous advantageous mechanical properties in comparison to other materials. One of the most useful is the high degree of toughness that can be achieved due to significant yield occurring in the material. Remarkably, the onset of plasticity in polymeric materials is very poorly quantified, despite its importance as the ultimate limit of purely elastic behavior. Here we report the results of a novel experiment which is extremely sensitive to the onset of yield and discuss its impact on measurement and elastic theory. In particular, we use an elastic instability to locally bend and impart a \textit{local} tensile stress in a thin, glassy polystyrene film, and directly measure the resulting residual stress caused by the bending. We show that plastic failure is initiated at extremely low strains, of order $10^{-3}$ for polystyrene. Not only is this critical strain found to be small in comparison to bulk measurement, we show that it is influenced by thin film confinement - leading to an increase in the critical strain for plastic failure as film thickness approaches zero. 	
1609.02293v1	http://arxiv.org/pdf/1609.02293v1	2016	Evolution of Strength and Failure of SCC during Early Hydration	Linus K. Mettler|Falk K. Wittel|Robert J. Flatt|Hans J. Herrmann	  The early strength evolution of self-consolidating concrete (SCC) is studied by a set of non-standard mechanical tests for compressive, tensile, shear and bending failure. The results are applicable in an industrial environment for process control, e.g. of slip casting with adaptive molds in robotic fabrication. A procedure for collapsing data to a master evolution curve is presented that allows to distinguish two regimes in the evolution. In the first, the material is capable of undergoing large localized plastic deformation, as expected from thixotropic yield stress fluids. This is followed by a transition to cohesive frictional material behavior dominated by crack growth. The typical differences in tensile and compressive strength of hardened concrete are observed to originate at the transition. Finally, the evolution of a limit surface in principal stress space is constructed and discussed. 	
1611.02717v2	http://arxiv.org/pdf/1611.02717v2	2016	Resilience Design Patterns - A Structured Approach to Resilience at   Extreme Scale (version 1.0)	Saurabh Hukerikar|Christian Engelmann	  In this document, we develop a structured approach to the management of HPC resilience based on the concept of resilience-based design patterns. A design pattern is a general repeatable solution to a commonly occurring problem. We identify the commonly occurring problems and solutions used to deal with faults, errors and failures in HPC systems. The catalog of resilience design patterns provides designers with reusable design elements. We define a design framework that enhances our understanding of the important constraints and opportunities for solutions deployed at various layers of the system stack. The framework may be used to establish mechanisms and interfaces to coordinate flexible fault management across hardware and software components. The framework also enables optimization of the cost-benefit trade-offs among performance, resilience, and power consumption. The overall goal of this work is to enable a systematic methodology for the design and evaluation of resilience technologies in extreme-scale HPC systems that keep scientific applications running to a correct solution in a timely and cost-efficient manner in spite of frequent faults, errors, and failures of various types. 	
1612.01284v1	http://arxiv.org/pdf/1612.01284v1	2016	Modeling Structure and Resilience of the Dark Network	M. De Domenico|A. Arenas	  While the statistical and resilience properties of the Internet are no more changing significantly across time, the Darknet, a network devoted to keep anonymous its traffic, still experiences rapid changes to improve the security of its users. Here, we study the structure of the Darknet and we find that its topology is rather peculiar, being characterized by non-homogenous distribution of connections -- typical of scale-free networks --, very short path lengths and high clustering -- typical of small-world networks -- and lack of a core of highly connected nodes.   We propose a model to reproduce such features, demonstrating that the mechanisms used to improve cyber-security are responsible for the observed topology. Unexpectedly, we reveal that its peculiar structure makes the Darknet much more resilient than the Internet -- used as a benchmark for comparison at a descriptive level -- to random failures, targeted attacks and cascade failures, as a result of adaptive changes in response to the attempts of dismantling the network across time. 	
1705.02245v1	http://arxiv.org/pdf/1705.02245v1	2017	Data Readiness Levels	Neil D. Lawrence	  Application of models to data is fraught. Data-generating collaborators often only have a very basic understanding of the complications of collating, processing and curating data. Challenges include: poor data collection practices, missing values, inconvenient storage mechanisms, intellectual property, security and privacy. All these aspects obstruct the sharing and interconnection of data, and the eventual interpretation of data through machine learning or other approaches. In project reporting, a major challenge is in encapsulating these problems and enabling goals to be built around the processing of data. Project overruns can occur due to failure to account for the amount of time required to curate and collate. But to understand these failures we need to have a common language for assessing the readiness of a particular data set. This position paper proposes the use of data readiness levels: it gives a rough outline of three stages of data preparedness and speculates on how formalisation of these levels into a common language for data readiness could facilitate project management. 	
1707.02422v2	http://arxiv.org/pdf/1707.02422v2	2017	Predictability and Strength of a Heterogeneous System : The Role of   System Size and Disorder	Subhadeep Roy	  In this work I have studied the effect of disorder and system size in fiber bundle model with a certain range of stress redistribution. The strength of the bundle as well as the failure abruptness is observed with varying disorder, stress release range and system sizes. With a local stress concentration, the strength of the bundle is observed to decrease with system size. The behavior of such decrement changes drastically as disorder strength is tuned. At moderate disorder, the critical stress scales with system size in an inverse logarithmic manner. In low disorder, where the brittle response is highly expected, the strength decreases in a scale free manner. With increasing system size and stress release range the model approaches thermodynamic limit and the mean field limit respectively. A detail study expresses different limit in the model and the corresponding modes of failure on the plane of above mentioned parameters. 	
1709.08329v1	http://arxiv.org/pdf/1709.08329v1	2017	Graphene helicoid as novel nanospring	Haifei Zhan|Yingyan Zhang|Chunhui Yang|Gang Zhang|Yuantong Gu	  Advancement of nanotechnology has greatly accelerated the miniaturization of mechanical or electronic devices. This work proposes a new nanoscale spring - a graphene nanoribbon-based helicoid (GH) structure by using large-scale molecular dynamics simulation. It is found that the GH structure not only possesses an extraordinary high tensile deformation capability, but also exhibits unique features not accessible from traditional springs. Specifically, its yield strain increases when its inner radius is enlarged, which can exceed 1000%, and it has three elastic deformation stages including the initial delamination, stable delamination and elastic deformation. Moreover, the failure of the GH is found to be governed by the failure of graphene nanoribbon and the inner edge atoms absorb most of the tensile strain energy. Such fact leads to a constant elastic limit force (corresponding to the yield point) for all GHs. This study has provided a comprehensive understanding of the tensile behaviors of GH, which opens the avenue to design novel nanoscale springs based on 2D nanomaterials. 	
1711.05317v1	http://arxiv.org/pdf/1711.05317v1	2017	Correlated network of networks enhances robustness against catastrophic   failures	Byungjoon Min|Muhua Zheng|Hernán A. Makse	  Networks in nature rarely function in isolation but instead interact with one another with a form of network of networks (NoN). Network of networks with interdependency between distinct networks contains instabilities of abrupt collapse related to the global rule of activation. As a remedy of the collapse instability, here we investigate a model of correlated NoN and find that the collapse instabilities can be removed with a specific pattern of correlated connectivity. We find that when hubs provide majority of interconnections and interconnections are convergent, a system of networks becomes stable systematically. Our study identifies a stable structure of correlated NoN against catastrophic failures. Our result further suggests a plausible way to enhance network robustness by manipulating connection patterns, along with other methods such as controlling the state of node based on local rule. 	
1802.01340v1	http://arxiv.org/pdf/1802.01340v1	2018	Experimental Constraints On The Fatigue of Icy Satellite Lithospheres by   Tidal Forces	Noah P. Hammond|Amy C. Barr|Reid F. Cooper|Tess E. Caswell|Greg Hirth	  Fatigue can cause materials that undergo cyclic loading to experience brittle failure at much lower stresses than under monotonic loading. We propose that the lithospheres of icy satellites could become fatigued and thus weakened by cyclical tidal stresses. To test this hypothesis, we performed a series of laboratory experiments to measure the fatigue of water ice at temperatures of $198$ K and $233$ K and at a loading frequency of $1$ Hz. We find that ice is \textit{not} susceptible to fatigue at our experimental conditions and that the brittle failure stress does not decrease with increasing number of loading cycles. Even though fatigue was not observed at our experimental conditions, colder temperatures, lower loading frequencies, and impurities in the ice shells of icy satellites may increase the likelihood of fatigue crack growth. We also explore other mechanisms that may explain the weak behavior of the lithospheres of some icy satellites. 	
1802.09490v1	http://arxiv.org/pdf/1802.09490v1	2018	Controlling Human Utilization of Failure-Prone Systems via Taxes	Ashish R. Hota|Shreyas Sundaram	  We consider a game-theoretic model where individuals compete over a shared failure-prone system or resource. We investigate the effectiveness of a taxation mechanism in controlling the utilization of the resource at the Nash equilibrium when the decision-makers have behavioral risk preferences, captured by prospect theory. We first observe that heterogeneous prospect-theoretic risk preferences can lead to counter-intuitive outcomes. In particular, for resources that exhibit network effects, utilization can increase under taxation and there may not exist a tax rate that achieves the socially optimal level of utilization. We identify conditions under which utilization is monotone and continuous, and then characterize the range of utilizations that can be achieved by a suitable choice of tax rate. We further show that resource utilization is higher when players are charged differentiated tax rates compared to the case when all players are charged an identical tax rate. 	
0504648v1	http://arxiv.org/pdf/astro-ph/0504648v1	2005	Lithopanspermia in Star Forming Clusters	Fred C. Adams|David N. Spergel	  This paper considers the lithopanspermia hypothesis in star forming groups and clusters, where the chances of biological material spreading from one solar system to another is greatly enhanced (relative to the field) due to the close proximity of the systems and lower relative velocities. These effects more than compensate for the reduced time spent in such crowded environments. This paper uses 300,000 Monte Carlo scattering calculations to determine the cross sections for rocks to be captured by binaries and provides fitting formulae for other applications. We assess the odds of transfer as a function of the ejection speed and number of members in the birth aggregate. The odds of any given ejected meteroid being recaptured by another solar system are relatively low. Because the number of ejected rocks per system can be large, virtually all solar systems are likely to share rocky ejecta with all of the other solar systems in their birth cluster. The number of ejected rocks that carry living microorganisms is much smaller and less certain, but we estimate that several million rocks can be ejected from a biologically active solar system. For typical birth environments, the capture of life bearing rocks is expected to occur 10 -- 16,000 times per cluster (under favorable conditions), depending on the ejection speeds. Only a small fraction of the captured rocks impact the surfaces of terrestrial planets, so that only a few lithopanspermia events are expected (per cluster). 	
1205.6226v1	http://arxiv.org/pdf/1205.6226v1	2012	Seasonal melting and the formation of sedimentary rocks on Mars, with   predictions for the Gale Crater mound	Edwin S. Kite|Itay Halevy|Melinda A. Kahre|Michael J. Wolff|Michael Manga	  A model for the formation and distribution of sedimentary rocks on Mars is proposed. The rate-limiting step is supply of liquid water from seasonal melting of snow or ice. The model is run for a O(10^2) mbar pure CO2 atmosphere, dusty snow, and solar luminosity reduced by 23%. For these conditions snow only melts near the equator, and only when obliquity >40 degrees, eccentricity >0.12, and perihelion occurs near equinox. These requirements for melting are satisfied by 0.01-20% of the probability distribution of Mars' past spin-orbit parameters. Total melt production is sufficient to account for aqueous alteration of the sedimentary rocks. The pattern of seasonal snowmelt is integrated over all spin-orbit parameters and compared to the observed distribution of sedimentary rocks. The global distribution of snowmelt has maxima in Valles Marineris, Meridiani Planum and Gale Crater. These correspond to maxima in the sedimentary-rock distribution. Higher pressures and especially higher temperatures lead to melting over a broader range of spin-orbit parameters. The pattern of sedimentary rocks on Mars is most consistent with a Mars paleoclimate that only rarely produced enough meltwater to precipitate aqueous cements and indurate sediment. The results suggest intermittency of snowmelt and long globally-dry intervals, unfavorable for past life on Mars. This model makes testable predictions for the Mars Science Laboratory rover at Gale Crater. Gale Crater is predicted to be a hemispheric maximum for snowmelt on Mars. 	
1505.01117v1	http://arxiv.org/pdf/1505.01117v1	2015	Hilbert transform based analyses on ship-rocking signals	Wei Huang|Yu-jian Li|Deyong Kang|Zhi Chen	  The ship-rocking is a crucial factor which affects the accuracy of the ocean-based flight vehicle measurement. Here we have analyzed four groups of ship-rocking time series in horizontal and vertical directions utilizing a Hilbert based method from statistical physics. Our method gives a way to construct an analytic signal on the two-dimensional plane from a one-dimensional time series. The analytic signal share the complete property of the original time series. From the analytic signal of a time series, we have found some information of the original time series which are often hidden from the view of the conventional methods. The analytic signals of interest usually evolve very smoothly on the complex plane. In addition, the phase of the analytic signal is usually moves linearly in time. From the auto-correlation and cross-correlation functions of the original signals as well as the instantaneous amplitudes and phase increments of the analytic signals we have found that the ship-rocking in horizontal direction drives the ship-rocking in vertical direction when the ship navigates freely. And when the ship keeps a fixed navigation direction such relation disappears. Based on these resultswe could predict certain amount of future values of the ship-rocking time series based on the current and the previous values. Our predictions are as accurate as the conventional methods from stochastic processes and provide a much wider prediction time range. 	
1703.05686v1	http://arxiv.org/pdf/1703.05686v1	2017	Khristianovich-Geertsma-de Klerk problem with stress contrast	Igor Gladkov|Aleksandr Linkov	  The paper contains an extension of the Khristianovich-Geertsma-de Klerk (KGD) model to the case when the confining rock pressure, which closes a hydraulic fracture, varies in the direction of its propagation. The extension is impelled by the need to simulate fracture hampering (acceleration) when it penetrates into a layer with increased (decreased) rock pressure. The paper presents the problem formulation, an efficient numerical method for its solving, examples of fractures propagating through layers with various stresses and general conclusions. It is established that when the fracture enters a layer with increased rock stresses (positive stress contrast), it actually stops. In this case, the fluid particle velocity drops practically to zero and the velocity near the tip oscillates about a small value until the fluid pressure increases to the level of the increased rock pressure. In the opposite case, when the fracture enters a layer with decreased stresses (negative stress contrast), the fracture front accelerates until the fluid pressure drops to the level of the decreased rock pressure. Physical considerations and numerical results imply that the transition through a boundary of layers with different stresses may be characterized by a simple dimensionless parameter. The latter is defined as the ratio of the average difference between the fluid and rock pressures (at the moment of reaching the contact) to the stress jump at the contact. The parameter is applicable to contacts with positive as well as negative contrasts. The model and the method developed are to serve for improvement of the popular pseudo three-dimensional (P3D) model. 	
0303067v1	http://arxiv.org/pdf/physics/0303067v1	2003	The power law character of off-site power failures	A. John Arul|C. Senthil Kumar|S. Marimuthu|Om Pal Singh	  A study on the behavior of off-site AC power failure recovery times at three nuclear plant sites is presented. It is shown, that power law is appropriate for the representation of failure frequency-duration correlation function of off-site power failure events, based on simple assumptions about component failure and repair rates. It is also found that the annual maxima of power failure duration follow Frechet distribution, which is a type II asymptotic distribution, strengthening our assumption of power law for the parent distribution. The extreme value distributions obtained are used to extrapolate for failure durations beyond the observed range. 	
1502.00821v1	http://arxiv.org/pdf/1502.00821v1	2015	Software that Learns from its Own Failures	Martin Monperrus	  All non-trivial software systems suffer from unanticipated production failures. However, those systems are passive with respect to failures and do not take advantage of them in order to improve their future behavior: they simply wait for them to happen and trigger hard-coded failure recovery strategies. Instead, I propose a new paradigm in which software systems learn from their own failures. By using an advanced monitoring system they have a constant awareness of their own state and health. They are designed in order to automatically explore alternative recovery strategies inferred from past successful and failed executions. Their recovery capabilities are assessed by self-injection of controlled failures; this process produces knowledge in prevision of future unanticipated failures. 	
9902016v2	http://arxiv.org/pdf/hep-ex/9902016v2	1999	Study of the Photonuclear interaction of muons in rock with the MACRO   experiment	E. Scapparone|for the MACRO Collaboration	  We present first results about the measurement of the charged hadrons production by atmopsheric muons in the rock above MACRO. A comparison between the measure rate with the Monte Carlo expectation is presented 	
9902018v2	http://arxiv.org/pdf/hep-ex/9902018v2	1999	Study of the Photonuclear interaction of muons in rock with the MACRO   experiment	E. Scapparone|for the MACRO Collaboration	  We present first results about the measurement of the charged hadrons production by atmopsheric muons in the rock above MACRO. A comparison between the measurerate with the Monte Carlo expectation is presented. 	
0601030v1	http://arxiv.org/pdf/hep-ex/0601030v1	2006	Low energy neutron propagation in MCNPX and GEANT4	R. Lemrani|M. Robinson|V. A. Kudryavtsev|M. De Jesus|G. Gerbier|N. J. C. Spooner	  Simulations of neutron background from rock for underground experiments are presented. Neutron propagation through two types of rock, lead and hydrocarbon material is discussed. The results show a reasonably good agreement between GEANT4, MCNPX and GEANT3 in transporting low-energy neutrons. 	
0805.3665v1	http://arxiv.org/pdf/0805.3665v1	2008	Production of 239 Pu from a natural Uranium disk and "hot" rock using a   neutron howitzer	Joseph Steiner|Aaron Anderson|Michael De Marco	  A neutron howitzer was used to produce 239Np from the targets of natural U and a hot rock. An intrinsic Germanium detector enabled the observations of the gamma rays in the decay of 239Np and a determination of its half life of 2.3 days. This shows that 239Pu had been produced in both targets 	
1311.2853v1	http://arxiv.org/pdf/1311.2853v1	2013	The Solar Orientation of the Lion Rock Complex in Sri Lanka	Amelia Carolina Sparavigna	  This paper discusses the solar orientation of the archaeological complex of Sigiriya, the Lion Rock, in Sri Lanka. We can see that the axis of this complex is oriented with the sunset of the zenithal sun. 	
1607.04243v1	http://arxiv.org/pdf/1607.04243v1	2016	A real-time analysis of rock fragmentation using UAV technology	Thomas Bamford|Kamran Esmaeili|Angela P. Schoellig	  Accurate measurement of blast-induced rock fragmentation is of great importance for many mining operations. The post-blast rock size distribution can significantly influence the efficiency of all the downstream mining and comminution processes. Image analysis methods are one of the most common methods used to measure rock fragment size distribution in mines regardless of criticism for lack of accuracy to measure fine particles and other perceived deficiencies. The current practice of collecting rock fragmentation data for image analysis is highly manual and provides data with low temporal and spatial resolution. Using UAVs for collecting images of rock fragments can not only improve the quality of the image data but also automate the data collection process. Ultimately, real-time acquisition of high temporal- and spatial-resolution data based on UAV technology will provide a broad range of opportunities for both improving blast design without interrupting the production process and reducing the cost of the human operator. This paper presents the results of a series of laboratory-scale rock fragment measurements using a quadrotor UAV equipped with a camera. The goal of this work is to highlight the benefits of aerial fragmentation analysis in terms of both prediction accuracy and time effort. A pile of rock fragments with different fragment sizes was placed in a lab that is equipped with a motion capture camera system for precise UAV localization and control. Such an environment presents optimal conditions for UAV flight and thus, is well-suited for conducting proof-of-concept experiments before testing them in large-scale field experiments. The pile was photographed by a camera attached to the UAV, and the particle size distribution curves were generated in almost real-time. The pile was also manually photographed and the results of the manual method were compared to the UAV method. 	
1008.0471v1	http://arxiv.org/pdf/1008.0471v1	2010	Stress Orientation Confidence Intervals from Focal Mechanism Inversion	Stefan A. Revets	  The determination of confidence intervals of stress orientation is a crucial element in the discussion of homogeneity or heterogeneity of the stress field under study. The error estimates provided by the grid search method Focal Mechanism Stress Inversion of Gephart and Forsyth (1984) have been shown to be too wide but the reasons for this failure have escaped elucidation. Through the use of directional statistics and synthetic focal mechanisms, I show that the grid search methodology does yield appropriate uncertainty estimates. The direct perturbation of the synthetic focal mechanisms introduces bias which leads to confidence intervals which become increasingly too wide as the amount of perturbation increases. The synthetic data also show at what point the method fails to overcome this bias and when confidence intervals will be too wide. The indirect perturbation of the focal mechanisms by perturbing the generating deviatoric stress tensor generates synthetic data devoid of bias. Inversion of these data sets yields correct confidence intervals. The Focal Mechanism Stress Inversion method is vindicated as a highly effective method, and with the use of appropriate directional statistics, its results can be assessed and homogeneity or heterogeneity of the stress field can be discussed with confidence. 	
1209.5291v1	http://arxiv.org/pdf/1209.5291v1	2012	Stress relaxation through crosslink unbinding in cytoskeletal networks	Claus Heussinger	  The mechanical properties of cells are dominated by the cytoskeleton, an interconnected network of long elastic filaments. The connections between the filaments are provided by crosslinking proteins, which constitute, next to the filaments, the second important mechanical element of the network. An important aspect of cytoskeletal assemblies is their dynamic nature, which allows remodeling in response to external cues. The reversible nature of crosslink binding is an important mechanism that underlies these dynamical processes. Here, we develop a theoretical model that provides insight into how the mechanical properties of cytoskeletal networks may depend on their underlying constituting elements. We incorporate three important ingredients: nonaffine filament deformations in response to network strain; interplay between filament and crosslink mechanical properties; reversible crosslink (un)binding in response to imposed stress. With this we are able to self-consistently calculate the nonlinear modulus of the network as a function of deformation amplitude and crosslink as well as filament stiffnesses. During loading crosslink unbinding processes lead to a relaxation of stress and therefore to a reduction of the network modulus and eventually to network failure, when all crosslink are unbound. This softening due to crosslink unbinding generically competes with an inherent stiffening response, which may either be due to filament or crosslink nonlinear elasticity. 	
1401.4287v1	http://arxiv.org/pdf/1401.4287v1	2014	The model of stress distribution in polymer electrolyte membrane	Vadim V. Atrazhev|Tatiana Yu. Astakhova|Dmitry V. Dmitriev|Nikolay S. Erikhman|Vadim I. Sultanov|Timothy Patterson|Sergei F. Burlatsky	  An analytical model of mechanical stress in a polymer electrolyte membrane (PEM) of a hydrogen/air fuel cell with porous Water Transfer Plates (WTP) is developed in this work. The model considers a mechanical stress in the membrane is a result of the cell load cycling under constant oxygen utilization. The load cycling causes the cycling of the inlet gas flow rate, which results in the membrane hydration/dehydration close to the gas inlet. Hydration/dehydration of the membrane leads to membrane swelling/shrinking, which causes mechanical stress in the constrained membrane. Mechanical stress results in through-plane crack formation. Thereby, the mechanical stress in the membrane causes mechanical failure of the membrane, limiting fuel cell lifetime. The model predicts the stress in the membrane as a function of the cell geometry, membrane material properties and operation conditions. The model was applied for stress calculation in GORE-SELECT. 	
1711.04505v1	http://arxiv.org/pdf/1711.04505v1	2017	Tailoring mechanically-tunable strain fields in graphene	M. Goldsche|J. Sonntag|T. Khodkov|G. Verbiest|S. Reichardt|C. Neumann|T. Ouaj|N. von den Driesch|D. Buca|C. Stampfer	  There are a number of theoretical proposals based on strain engineering of graphene and other two-dimensional materials, however purely mechanical control of strain fields in these systems has remained a major challenge. The two approaches mostly used so far either couple the electrical and mechanical properties of the system simultaneously or introduce some unwanted disturbances due to the substrate. Here, we report on silicon micro-machined comb-drive actuators to controllably and reproducibly induce strain in a suspended graphene sheet, in an entirely mechanical way. We use spatially resolved confocal Raman spectroscopy to quantify the induced strain, and we show that different strain fields can be obtained by engineering the clamping geometry, including tunable strain gradients of up to 1.4 %/$\mu$m. Our approach also allows for multiple axis straining and is equally applicable to other two-dimensional materials, opening the door to an investigating their mechanical and electromechanical properties. Our measurements also clearly identify defects at the edges of a graphene sheet as being weak spots responsible for its mechanical failure. 	
1801.04292v1	http://arxiv.org/pdf/1801.04292v1	2018	Mechanical Properties of Phagraphene Membranes: A Fully Atomistic   Molecular Dynamics Investigation	Jose M. de Sousa|Acrisio L. Aguiar|Eduardo C. Girao|Alexandre F. Fonseca|Antonio G. Sousa Filho|Douglas S. Galvao	  Recently, a new 2D carbon allotrope structure, named phagraphene (PG), was proposed. PG has a densely array of penta-hexa-hepta-graphene carbon rings. PG was shown to present low and anisotropic thermal conductivity and it is believed that this anisotropy should be also reflected in its mechanical properties. Although PG mechanical properties have been investigated, a detailed and comprehensive study is still lacking. In the present work we have carried out fully atomistic reactive molecular dynamics simulations using the ReaxFF force field, to investigate the mechanical properties and fracture patterns of PG membranes. The Young's modulus values of the PG membranes were estimated from the stress-strain curves. Our results show that these curves present three distinct regimes: one regime where ripples dominate the structure and mechanical properties of the PG membranes; an elastic regime where the membranes exhibit fully planar configurations; and finally a plastic regime where permanent deformations happened to the PG membrane up to the mechanical failure or fracture. 	
0504073v1	http://arxiv.org/pdf/cs/0504073v1	2005	Rendezvous Regions: A Scalable Architecture for Resource Discovery and   Service Location in Large-Scale Mobile Networks	Karim Seada|Ahmed Helmy	  In large-scale wireless networks such as mobile ad hoc and sensor networks, efficient and robust service discovery and data-access mechanisms are both essential and challenging. Rendezvous-based mechanisms provide a valuable solution for provisioning a wide range of services. In this paper, we describe Rendezvous Regions (RRs) - a novel scalable rendezvous-based architecture for wireless networks. RR is a general architecture proposed for service location and bootstrapping in ad hoc networks, in addition to data-centric storage, configuration, and task assignment in sensor networks. In RR the network topology is divided into geographical regions, where each region is responsible for a set of keys representing the services or data of interest. Each key is mapped to a region based on a hash-table-like mapping scheme. A few elected nodes inside each region are responsible for maintaining the mapped information. The service or data provider stores the information in the corresponding region and the seekers retrieve it from there. We run extensive detailed simulations, and high-level simulations and analysis, to investigate the design space, and study the architecture in various environments including node mobility and failures. We evaluate it against other approaches to identify its merits and limitations. The results show high success rate and low overhead even with dynamics. RR scales to large number of nodes and is highly robust and efficient to node failures. It is also robust to node mobility and location inaccuracy with a significant advantage over point-based rendezvous mechanisms. 	
0803.1625v1	http://arxiv.org/pdf/0803.1625v1	2008	Physicalism versus quantum mechanics	Henry P. Stapp	  In the context of theories of the connection between mind and brain, physicalism is the demand that all is basically purely physical. But the concept of "physical" embodied in this demand is characterized essentially by the properties of the physical that hold in classical physical theories. Certain of these properties contradict the character of the physical in quantum mechanics, which provides a better, more comprehensive, and more fundamental account of phenomena. It is argued that the difficulties that have plaged physicalists for half a century, and that continue to do so, dissolve when the classical idea of the physical is replaced by its quantum successor. The argument is concretized in a way that makes it accessible to non-physicists by exploiting the recent evidence connecting our conscious experiences to macroscopic measurable synchronous oscillations occurring in well-separated parts of the brain. A specific new model of the mind-brain connection that is fundamentally quantum mechanical but that ties conscious experiences to these macroscopic synchronous oscillations is used to illustrate the essential disparities between the classical and quantum notions of the physical, and in particular to demonstrate the failure in the quantum world of the principle of the causal closure of the physical, a failure that goes beyond what is entailed by the randomness in the outcomes of observations, and that accommodates the efficacy in the brain of conscious intent. 	
0808.3228v1	http://arxiv.org/pdf/0808.3228v1	2008	Hydro-Gravitational-Dynamics of Planets and Dark Energy	Carl H. Gibson|Rudolph E. Schild	  Self-gravitational fluid mechanical methods termed hydro-gravitational-dynamics (HGD) predict plasma fragmentation 0.03 Myr after the turbulent big bang to form protosuperclustervoids, turbulent protosuperclusters, and protogalaxies at the 0.3 Myr transition from plasma to gas. Linear protogalaxyclusters fragment at 0.003 Mpc viscous-inertial scales along turbulent vortex lines or in spirals, as observed. The plasma protogalaxies fragment on transition into white-hot planet-mass gas clouds (PFPs) in million-solar-mass clumps (PGCs) that become globular-star-clusters (GCs) from tidal forces or dark matter (PGCs) by freezing and diffusion into 0.3 Mpc halos with 97% of the galaxy mass. The weakly collisional non-baryonic dark matter diffuses to > Mpc scales and frag-ments to form galaxy cluster halos. Stars and larger planets form by binary mergers of the trillion PFPs per PGC on 0.03 Mpc galaxy accretion disks. Star deaths depend on rates of planet accretion and internal star mixing. Moderate accretion rates produce white dwarfs that evaporate surrounding gas planets by spin-radiation to form planetary nebulae before Supernova Ia events, dimming some events to give systematic distance errors misinterpreted as the dark energy hypothesis and overestimates of the universe age. Failures of standard LCDM cosmological models reflect not only obsolete Jeans 1902 fluid mechanical assumptions, but also failures of standard turbulence models that claim the cascade of turbulent kinetic energy is from large scales to small. Because turbulence is always driven at all scales by inertial-vortex forces the turbulence cascade is always from small scales to large. 	
1304.6283v1	http://arxiv.org/pdf/1304.6283v1	2013	Damage mechanisms in the dynamic fracture of nominally brittle polymers	Davy Dalmas|Claudia Guerra|Julien Scheibert|Daniel Bonamy	  Linear Elastic Fracture Mechanics (LEFM) provides a consistent framework to evaluate quantitatively the energy flux released to the tip of a growing crack. Still, the way in which the crack selects its velocity in response to this energy flux remains far from completely understood. To uncover the underlying mechanisms, we experimentally studied damage and dissipation processes that develop during the dynamic failure of polymethylmethacrylate (PMMA), classically considered as the archetype of brittle amorphous materials. We evidenced a well-defined critical velocity along which failure switches from nominally-brittle to quasi-brittle, where crack propagation goes hand in hand with the nucleation and growth of microcracks. Via post-mortem analysis of the fracture surfaces, we were able to reconstruct the complete spatiotemporal microcracking dynamics with micrometer/nanosecond resolution. We demonstrated that the true local propagation speed of individual crack fronts is limited to a fairly low value, which can be much smaller than the apparent speed measured at the continuum-level scale. By coalescing with the main front, microcracks boost the macroscale velocity through an acceleration factor of geometrical origin. We discuss the key role of damage-related internal variables in the selection of macroscale fracture dynamics. 	
1511.02050v1	http://arxiv.org/pdf/1511.02050v1	2015	Finite size effects on crack front pinning at heterogeneous planar   interfaces: Experimental, finite elements and perturbation approaches	Sylvain Patinet|L Alzate|E Barthel|D Dalmas|D Vandembroucq|V Lazarus	  Understanding the role played by the microstructure of materials on their macroscopic failure properties is an important challenge in solid mechanics. Indeed, when a crack propagates at a heterogeneous brittle interface, the front is trapped by tougher regions and deforms. This pinning induces non-linearities in the crack propagation problem, even within Linear Elastic Fracture Mechanics theory, and modifies the overall failure properties of the material. For example crack front pinning by tougher places could increase the fracture resistance of multilayer structures, with interesting technological applications. Analytical perturbation approaches, based on Bueckner-Rice elastic line models, focus on the crack front perturbations, hence allow for a description of these phenomena. Here, they are applied to experiments investigating the propagation of a purely interfacial crack in a simple toughness pattern: a single defect strip surrounded by homogeneous interface. We show that by taking into account the finite size of the body, quantitative agreement with experimental and finite elements results is achieved. In particular this method allows to predict the toughness contrast, i.e. the toughness difference between the single defect strip and its homogeneous surrounding medium. This opens the way to a more accurate use of the perturbation method to study more disordered heterogeneous materials, where the finite elements method is less adequate. From our results, we also propose a simple method to determine the adhesion energy of tough interfaces by measuring the crack front deformation induced by known interface patterns. 	
1701.01193v2	http://arxiv.org/pdf/1701.01193v2	2017	Graphene and its elemental analogue: A molecular dynamics view of   fracture phenomenon	Tawfiqur Rakib|Satyajit Mojumder|Sourav Das|Sourav Saha|Mohammad Motalab	  Graphene and some graphene like two dimensional materials; hexagonal boron nitride (hBN) and silicene have unique mechanical properties which severely limit the suitability of conventional theories used for common brittle and ductile materials to predict the fracture response of these materials. This study revealed the fracture response of graphene, hBN and silicene nanosheets under different tiny crack lengths by molecular dynamics (MD) simulations using LAMMPS. The useful strength of these large area two dimensional materials are determined by their fracture toughness. Our study shows a comparative analysis of mechanical properties among the elemental analogues of graphene and suggested that hBN can be a good substitute for graphene in terms of mechanical properties. We have also found that the pre-cracked sheets fail in brittle manner and their failure is governed by the strength of the atomic bonds at the crack tip. The MD prediction of fracture toughness shows significant difference with the fracture toughness determined by Griffth's theory of brittle failure which restricts the applicability of Griffith's criterion for these materials in case of nano-cracks. Moreover, the strengths measured in armchair and zigzag directions of nanosheets of these materials implied that the bonds in armchair direction has the stronger capability to resist crack propagation compared to zigzag direction. 	
0209129v1	http://arxiv.org/pdf/cond-mat/0209129v1	2002	Fracture of disordered solids in compression as a critical phenomenon:   III. Analysis of the localization transition	Renaud Toussaint|Steven R. Pride	  The properties of the Hamiltonian developed in Paper II are studied showing that at a particular strain level a ``localization'' phase transition occurs characterized by the emergence of conjugate bands of coherently oriented cracks. The functional integration that yields the partition function is then performed analytically using an approximation that employs only a subset of states in the functional neighborhood surrounding the most probable states. Such integration establishes the free energy of the system, and upon taking the derivatives of the free energy, the localization transition is shown to be continuous and to be distinct from peak stress. When the bulk modulus of the grain material is large, localization always occurs in the softening regime following peak stress, while for sufficiently small bulk moduli and at sufficiently low confining pressure, the localization occurs in the hardening regime prior to peak stress.   In the approach to localization, the stress-strain relation for the whole rock remains analytic, as is observed both in experimental data and in simpler models.   The correlation function of the crack fields is also obtained. It has a correlation length characterizing the aspect ratio of the crack clusters that diverges as (\xi \sim (\ep_{c}-\ep)^{-2}) at localization. 	
0510031v1	http://arxiv.org/pdf/cond-mat/0510031v1	2005	New Misfit-Layered Cobalt Oxide (CaOH)1.14CoO2	M. Shizuya|M. Isobe|Y. Baba|T. Nagai|M. Osada|K. Kosuda|S. Takenouchi|Y. Matsui|E. Takayama-Muromachi	  We found a new cobalt oxide (CaOH)1.14CoO2 by utilizing the high-pressure technique. X-ray and electron diffraction studies revealed that the compound has layer structure which consists of CdI2-type CoO2 layers and rock-salt-type double CaOH atomic layers. The two subcells have incommensurate periodicity along the a-axis, resulting in modulated crystal structure due to the inter-subcell interaction. The structural modulation affects carrier conduction through the potential randomness. We found that the two-dimensional (2-D) variable-range hopping (VRH) regime with hole conduction is dominant at low temperature for this compound, and that the conduction mechanism undergoes crossover from the 2-D VRH regime to thermal activation-energy type one with increasing temperature. Based on the experimental results of resistivity, thermoelectric power, magnetic susceptibility and specific heat measurements, we suggested a possible electronic-band structure model to explain these results. The cobalt t2g-derivative band crosses Fermi energy level near the band edge, yielding small finite density of localized states at the Fermi level in the band. The observed resistivity, Seebeck coefficient, large Pauli paramagnetic component in the magnetic susceptibility and comparatively small Sommerfeld constant in the specific heat are principally attributed to the holes in the t2g-derivative band. We estimated the Wilson ratio to be about 2.8, suggesting the strong electron correlation realized in this compound. 	
0606116v3	http://arxiv.org/pdf/cond-mat/0606116v3	2006	Dichotomous Markov noise: Exact results for out-of-equilibrium systems.   A review	Ioana Bena	  Nonequilibrium systems driven by additive or multiplicative dichotomous Markov noise appear in a wide variety of physical and mathematical models. We review here some prototypical examples, with an emphasis on {\em analytically-solvable} situations. In particular, it has escaped attention till recently that the standard results for the long-time properties of such systems cannot be applied when unstable fixed points are crossed in the asymptotic regime. We show how calculations have to be modified to deal with these cases and present a few relevant applications -- the hypersensitive transport, the rocking ratchet, and the stochastic Stokes' drift. These results reinforce the impression that dichotomous noise can be put on a par with Gaussian white noise as far as obtaining analytical results is concerned. They convincingly illustrate the interplay between noise and nonlinearity in generating nontrivial behaviors of nonequilibrium systems and point to various practical applications. 	
0811.0605v2	http://arxiv.org/pdf/0811.0605v2	2009	Evaluation of a permeability-porosity relationship in a low permeability   creeping material using a single transient test	Siavash Ghabezloo|Jean Sulem|Jérémie Saint-Marc	  A method is presented for the evaluation of the permeability-porosity relationship in a low-permeability porous material using the results of a single transient test. This method accounts for both elastic and non-elastic deformations of the sample during the test and is applied to a hardened class G oil well cement paste. An initial hydrostatic undrained loading is applied to the sample. The generated excess pore pressure is then released at one end of the sample while monitoring the pore pressure at the other end and the radial strain in the middle of the sample during the dissipation of the pore pressure. These measurements are back analysed to evaluate the permeability and its evolution with porosity change. The effect of creep of the sample during the test on the measured pore pressure and volume change is taken into account in the analysis. This approach permits to calibrate a power law permeability-porosity relationship for the tested hardened cement paste. The porosity sensitivity exponent of the power-law is evaluated equal to 11 and is shown to be mostly independent of the stress level and of the creep strains. 	
0902.0857v1	http://arxiv.org/pdf/0902.0857v1	2009	Miscible transfer of solute in different types of rough fractures: from   random to multiscale fracture walls heights	Harold Auradou|Alejandro Boschan|Ricardo Chertcoff|Maria Veronica D'Angelo|Jean-Pierre Hulin|Irene Ippolito	  Miscible tracer dispersion measurements in transparent model fractures with different types of wall roughness are reported. The nature (Fickian or not) of dispersion is determined by studying variations of the mixing front as a function of the traveled distance but also as a function of the lateral scale over which the tracer concentration is averaged. The dominant convective dispersion mechanisms (velocity profile in the gap, velocity variations in the fracture plane) are established by comparing measurements using Newtonian and shear thinning fluids. For small monodisperse rugosities, front spreading is diffusive with a dominant geometrical dispersion (dispersion coefficient $D \propto Pe$) at low P\'eclet numbers $Pe$; at higher $Pe$ values one has either $D \propto Pe^2$ ({\it i.e.} Taylor dispersion) for obstacles of height smaller than the gap or $D \propto Pe^{1.35}$ for obstacles bridging the gap. For a self affine multiscale roughness like in actual rocks and a relative shear displacement $\vec{\delta}$ of complementary walls, the aperture field is channelized in the direction perpendicular to $\delta$. For a mean velocity $\vec{U}$ parallel to the channels, the global front geometry reflects the velocity contrast between them and is predicted from the aperture field. For $\vec{U}$ perpendicular to the channels, global front spreading is much reduced. Local spreading of the front thickness remains mostly controlled by Taylor dispersion except in the case of a very strong channelization parallel to $\vec U$. 	
0909.1588v1	http://arxiv.org/pdf/0909.1588v1	2009	Surveying Diffusion in Complex Geometries. An Essay	Denis Grebenkov	  The surrounding world surprises us by the beauty and variety of complex shapes that emerge from nanometric to macroscopic scales. Natural or manufactured materials (sandstones, sedimentary rocks and cement), colloidal solutions (proteins and DNA), biological cells, tissues and organs (lungs, kidneys and placenta), they all present irregularly shaped "scenes" for a fundamental transport "performance", that is, diffusion. Here, the geometrical complexity, entangled with the stochastic character of diffusive motion, results in numerous fascinating and sometimes unexpected effects like diffusion screening or localization. These effects control many diffusion-mediated processes that play an important role in heterogeneous catalysis, biochemical mechanisms, electrochemistry, growth phenomena, oil recovery, or building industry. In spite of a long and rich history of academic and industrial research in this field, it is striking to see how little we know about diffusion in complex geometries, especially the one which occurs in three dimensions.   We present our recent results on restricted diffusion. We look into the role of geometrical complexity at different levels, from boundary microroughness to hierarchical structure and connectivity of the whole diffusion-confining domain. We develop a new approach which consists in combining fast random walk algorithms with spectral tools. The main focus is on studying diffusion in model complex geometries (von Koch boundaries, Kitaoka acinus, etc.), as well as on developing and testing spectral methods. We aim at extending this knowledge and at applying the accomplished arsenal of theoretical and numerical tools to structures found in nature and industry. 	
1101.2295v2	http://arxiv.org/pdf/1101.2295v2	2012	4D imaging of fracturing in organic-rich shales during heating	Maya Kobchenko|Hamed Panahi|Francois Renard|Dag Kristian Dysthe|Anders Malthe-Sorenssen|Adriano Mazzini|Julien Scheibert|Bjorn Jamtveit|Paul Meakin	  To better understand the mechanisms of fracture pattern development and fluid escape in low permeability rocks, we performed time-resolved in situ X-ray tomography imaging to investigate the processes that occur during the slow heating (from 60\degree; to 400\degree;C) of organic-rich Green River shale. At about 350\degree;C cracks nucleated in the sample, and as the temperature continued to increase, these cracks propagated parallel to shale bedding and coalesced, thus cutting across the sample. Thermogravimetry and gas chromatography revealed that the fracturing occurring at ~350\degree;C was associated with significant mass loss and release of light hydrocarbons generated by the decomposition of immature organic matter. Kerogen decomposition is thought to cause an internal pressure build up sufficient to form cracks in the shale, thus providing pathways for the outgoing hydrocarbons. We show that a 2D numerical model based on this idea qualitatively reproduces the experimentally observed dynamics of crack nucleation, growth and coalescence, as well as the irregular outlines of the cracks. Our results provide a new description of fracture pattern formation in low permeability shales. 	
1102.0125v1	http://arxiv.org/pdf/1102.0125v1	2011	Application of the Earth's Natural Electromagnetic Noise to Geophysical   Prospecting and Seraching for Oil	Sergey Yu. Malyshkov|Yury P. Malyshkov|Vasily F. Gordeev|Sergey G. Shtalin|Vitaly I. Polivach|Yury Yu. Bazhanov|Terje Hauan	  When applying the Earth's natural pulse electromagnetic fields to geophysical prospecting one should take into account characteristics of their spatial and temporal variations. ENPEMF is known to include both pulses attributed to atmospheric thunderstorms and pulses generated in the lithosphere by mechanic-to-electric energy conversion in rocks. It is evident that the most valuable information on the geophysical structure of a certain area is obviously contained in pulses originated from this area. This article covers a method of recording spatial variations of the Earth's natural pulse electromagnetic fields which is able to take due account of spatial and temporal variations of EM fields and suits to reveal crustal structural and lithologic heterogeneities including hydrocarbon pools. We use a system of several stations recording the ENPEMF concurrently to erase the temporal variations from ENPEMF records and to sort out the pulses of local and remote origin. Some stations are fixed (reference) and record only temporal variations of EM fields. While the other stations are mobile and measure pulse characteristics related to both spatial and temporal ENPEMF variations along measurement routes crossing the area investigated. Spatial variations of EM fields left after having deleted the temporal variations and pulses generated out of the area investigate show the availability or the lack of geophysical anomalies. 	
1102.2283v1	http://arxiv.org/pdf/1102.2283v1	2011	The role of space in the exploitation of resources	Yun Kang|Nicolas Lanchier	  In order to understand the role of space in ecological communities where each species produces a certain type of resource and has varying abilities to exploit the resources produced by its own species and by the other species, we carry out a comparative study of an interacting particle system and its mean-field approximation. For a wide range of parameter values, we show both analytically and numerically that the spatial model results in predictions that significantly differ from its nonspatial counterpart, indicating that the use of the mean-field approach to describe the evolution of communities in which individuals only interact locally is invalid. In two-species communities, the disagreements between the models appear either when both species compete by producing resources that are more beneficial for their own species or when both species cooperate by producing resources that are more beneficial for the other species. In particular, while both species coexist if and only if they cooperate in the mean-field approximation, the inclusion of space in the form of local interactions may prevent coexistence even in cooperative communities. Introducing additional species, cooperation is no longer the only mechanism that promotes coexistence. We prove that, in three-species communities, coexistence results either from a global cooperative behavior, or from rock-paper-scissors type interactions, or from a mixture of these dynamics, which excludes in particular all cases in which two species compete. 	
1111.6176v1	http://arxiv.org/pdf/1111.6176v1	2011	Granular physics in low-gravity environments using DEM	G. Tancredi|A. Maciel|L. Heredia|P. Richeri|S. Nesmachnow	  Granular materials of different sizes are present on the surface of several atmosphere-less Solar System bodies. The phenomena related to granular materials have been studied in the framework of the discipline called Granular Physics; that has been studied experimentally in the laboratory and, in the last decades, by performing numerical simulations. The Discrete Element Method simulates the mechanical behavior of a media formed by a set of particles which interact through their contact points. The difficulty in reproducing vacuum and low-gravity environments makes numerical simulations the most promising technique in the study of granular media under these conditions. In this work, relevant processes in minor bodies of the Solar System are studied using the Discrete Element Method. Results of simulations of size segregation in low-gravity environments in the cases of the asteroids Eros and Itokawa are presented. The segregation of particles with different densities was analysed, in particular, the case of comet P/Hartley 2. The surface shaking in these different gravity environments could produce the ejection of particles from the surface at very low relative velocities. The shaking causing the above processes is due to: impacts, explosions like the release of energy by the liberation of internal stresses or the re accommodation of material. Simulations of the passage of impact-induced seismic waves through a granular medium were also performed. We present several applications of the Discrete Element Methods for the study of the physical evolution of agglomerates of rocks under low-gravity environments. 	
1202.4286v1	http://arxiv.org/pdf/1202.4286v1	2012	A numerical retro-action model relates rocky coast erosion to   percolation theory	Andrea Baldassarri|Bernard Sapoval|Simon Félix	  We discuss various situations where the formation of rocky coast morphology can be attributed to the retro-action of the coast morphology itself on the erosive power of the sea. Destroying the weaker elements of the coast, erosion can creates irregular seashores. In turn, the geometrical irregularity participates in the damping of sea-waves, decreasing their erosive power. There may then exist a mutual self-stabilization of the wave amplitude together with the irregular morphology of the coast. A simple model of this type of stabilization is discussed. The resulting coastline morphologies are diverse, depending mainly on the morphology/damping coupling. In the limit case of weak coupling, the process spontaneously builds fractal morphologies with a dimension close to 4/3. This provides a direct connection between the coastal erosion problem and the theory of percolation. For strong coupling, rugged but non-fractal coasts may emerge during the erosion process, and we investigate a geometrical characterization in these cases. The model is minimal, but can be extended to take into account heterogeneity in the rock lithology and various initial conditions. This allows to mimic coastline complexity, well beyond simple fractality. Our results suggest that the irregular morphology of coastlines as well as the stochastic nature of erosion are deeply connected with the critical aspects of percolation phenomena. 	
1202.5198v1	http://arxiv.org/pdf/1202.5198v1	2012	Network Theory, Cracking and Frictional Sliding	H. O. Ghaffari|R. P. Young	  We have developed different network approaches to complex patterns of frictional interfaces (contact areas developments). Here, we analyze the dynamics of static friction. We found, under the correlation measure, the fraction of triangles correlates with the detachment fronts. Also, for all types of the loops (such as triangles), there is a universal power law between nodes' degree and motifs where motifs frequency follow a power law. This shows high energy localization is characterized by fast variation of the loops fraction. Also, this proves that the congestion of loops occurs around hubs. Furthermore, the motif distributions and modularity space of networks -in terms of within-module degree and participation coefficient- show universal trends, indicating an in common aspect of energy flow in shear ruptures. Moreover, we confirmed that slow ruptures generally hold small localization, while regular ruptures carry a high level of energy localization. We proposed that assortativity, as an index to correlation of node's degree, can uncover acoustic features of the interfaces. We showed that increasing assortativity induces a nearly silent period of fault's activities. Also, we proposed that slow ruptures resulted from within-module developments rather than extra-modules of the networks. Our approach presents a completely new perspective of the evolution of shear ruptures. 	
1206.5901v1	http://arxiv.org/pdf/1206.5901v1	2012	A nonlocal model for fluid-structure interaction with applications in   hydraulic fracturing	Daniel Z. Turner	  Modeling important engineering problems related to flow-induced damage (in the context of hydraulic fracturing among others) depends critically on characterizing the interaction of porous media and interstitial fluid flow. This work presents a new formulation for incorporating the effects of pore pressure in a nonlocal representation of solid mechanics. The result is a framework for modeling fluid-structure interaction problems with the discontinuity capturing advantages of an integral based formulation. A number of numerical examples are used to show that the proposed formulation can be applied to measure the effect of leak-off during hydraulic fracturing as well as modeling consolidation of fluid saturated rock and surface subsidence caused by fluid extraction from a geologic reservoir. The formulation incorporates the effect of pore pressure in the constitutive description of the porous material in a way that is appropriate for nonlinear materials, easily implemented in existing codes, straightforward in its evaluation (no history dependence), and justifiable from first principles. A mixture theory approach is used (deviating only slightly where necessary) to motivate an alteration to the peridynamic pressure term based on the fluid pore pressure. The resulting formulation has a number of similarities to the effective stress principle developed by Terzaghi and Biot and close correspondence is shown between the proposed method and the classical effective stress principle. 	
1212.0109v2	http://arxiv.org/pdf/1212.0109v2	2014	Quantum Structure in Competing Lizard Communities	Diederik Aerts|Jan Broekaert|Marek Czachor|Maciej Kuna|Barry Sinervo|Sandro Sozzo	  Almost two decades of research on applications of the mathematical formalism of quantum theory as a modeling tool in domains different from the micro-world has given rise to many successful applications in situations related to human behavior and thought, more specifically in cognitive processes of decision-making and the ways concepts are combined into sentences. In this article, we extend this approach to animal behavior, showing that an analysis of an interactive situation involving a mating competition between certain lizard morphs allows to identify a quantum theoretic structure. More in particular, we show that when this lizard competition is analyzed structurally in the light of a compound entity consisting of subentities, the contextuality provided by the presence of an underlying rock-paper-scissors cyclic dynamics leads to a violation of Bell's inequality, which means it is of a non-classical type. We work out an explicit quantum-mechanical representation in Hilbert space for the lizard situation and show that it faithfully models a set of experimental data collected on three throat-colored morphs of a specific lizard species. Furthermore, we investigate the Hilbert space modeling, and show that the states describing the lizard competitions contain entanglement for each one of the considered confrontations of lizards with different competing strategies, which renders it no longer possible to interpret these states of the competing lizards as compositions of states of the individual lizards. 	
1308.3344v1	http://arxiv.org/pdf/1308.3344v1	2013	How Channel Segregates Originates: The Flow of Accumulated Impurity   Clusters in Solidifying Steels	Dianzhong Li|Xing-Qiu Chen|Paixian Fu|Xiaoping Ma|Hongwei Liu|Yun Chen|Yikun Luan|Yiyi Li	  The phenomenon, channel segregates (CS) as a result of gravity-driven flow due to density contrast occurred in the solid-liquid mushy zones1during solidification, often causes the severe destruction of homogeneity and even some fatal damages. Investigation on its mechanism sheds light on the understanding and controlling of the formation of solidifying metals,earth's core, igneous rock and sea ice. Until now, it still remains controversial what composes the density contrasts and, to what extent, how it affects channel segregates. Here, we show that in experimental 500kg and 100 ton commercial cast steel ingots CS originates from oxide Al2O3/MnS impurity clusters (OICs) initially nucleated from the oxide (Al2O3) particles, which induce an extra flow due to sharp density contrast between clusters and melt. The results uncover that, as OICs enrich and grow, their driven flow becomes stronger than the traditionally recognized inter-dendritic thermo-solutal convection, dominating the subsequent opening of the channels. This study extends the classical macrosegregation theory, highlights a significant technological breakthrough to control CS, and could quickly yield practical benefits to the worldwide manufacture of over 50 million tons of ingots, super-thick slab and heavy castings annually, as well as has general implications for the elaboration of other related natural phenomena. 	
1310.2674v1	http://arxiv.org/pdf/1310.2674v1	2013	Charge disproportionation without charge transfer in the rare-earth   nickelates as a possible mechanism for the metal-insulator transition	Steve Johnston|Anamitra Mukherjee|Ilya Elfimov|Mona Berciu|George A. Sawatzky	  We study a model for the metal-insulator (MI) transition in the rare-earth nickelates RNiO$_3$, based upon a negative charge transfer energy and coupling to a rock-salt like lattice distortion of the NiO$_6$ octahedra. Using exact diagonalization and the Hartree-Fock approximation we demonstrate that electrons couple strongly to these distortions. For small distortions the system is metallic, with ground state of predominantly $d^8\ligand$ character, where $\ligand$ denotes a ligand hole. For sufficiently large distortions ($\delta d_{\rm Ni-O} \sim 0.05 - 0.10\AA$), however, a gap opens at the Fermi energy as the system enters a periodically distorted state alternating along the three crystallographic axes, with $(d^8\ligand^2)_{S=0}(d^8)_{S=1}$ character, where $S$ is the total spin. Thus the MI transition may be viewed as being driven by an internal volume "collapse" where the NiO$_6$ octahedra with two ligand holes shrink around their central Ni, while the remaining octahedra expand accordingly, resulting in the ($1/2,1/2,1/2$) superstructure observed in x-ray diffraction in the insulating phase. This insulating state is an example of a new type of charge ordering achieved without any actual movement of the charge. 	
1402.3875v3	http://arxiv.org/pdf/1402.3875v3	2014	Hydrogen peroxide thermochemical oscillator as driver for primordial RNA   replication	Rowena Ball|John Brindley	  This paper presents and tests a previously unrecognised mechanism for driving a replicating molecular system on the prebiotic earth. It is proposed that cell-free RNA replication in the primordial soup may have been driven by self-sustained oscillatory thermochemical reactions. To test this hypothesis a well-characterised hydrogen peroxide oscillator was chosen as the driver and complementary RNA strands with known association and melting kinetics were used as the substrate. An open flow system model for the self-consistent, coupled evolution of the temperature and concentrations in a simple autocatalytic scheme is solved numerically, and it is shown that thermochemical cycling drives replication of the RNA strands. For the (justifiably realistic) values of parameters chosen for the simulated example system, the mean amount of replicant produced at steady state is 6.56 times the input amount, given a constant supply of substrate species. The spontaneous onset of sustained thermochemical oscillations via slowly drifting parameters is demonstrated, and a scheme is given for prebiotic production of complementary RNA strands on rock surfaces. 	
1406.3340v1	http://arxiv.org/pdf/1406.3340v1	2014	From pairwise to group interactions in games of cyclic dominance	Attila Szolnoki|Jeromos Vukov|Matjaz Perc	  We study the rock-paper-scissors game in structured populations, where the invasion rates determine individual payoffs that govern the process of strategy change. The traditional version of the game is recovered if the payoffs for each potential invasion stem from a single pairwise interaction. However, the transformation of invasion rates to payoffs also allows the usage of larger interaction ranges. In addition to the traditional pairwise interaction, we therefore consider simultaneous interactions with all nearest neighbors, as well as with all nearest and next-nearest neighbors, thus effectively going from single pair to group interactions in games of cyclic dominance. We show that differences in the interaction range affect not only the stationary fractions of strategies, but also their relations of dominance. The transition from pairwise to group interactions can thus decelerate and even revert the direction of the invasion between the competing strategies. Like in evolutionary social dilemmas, in games of cyclic dominance too the indirect multipoint interactions that are due to group interactions hence play a pivotal role. Our results indicate that, in addition to the invasion rates, the interaction range is at least as important for the maintenance of biodiversity among cyclically competing strategies. 	
1406.5227v1	http://arxiv.org/pdf/1406.5227v1	2014	Synthesis of SnTe Nanoplates with {100} and {111} Surfaces	Jie Shen|Yeonwoong Jung|Ankit S. Disa|Fred J. Walker|Charles H. Ahn|Judy J. Cha	  SnTe is a topological crystalline insulator that possesses spin-polarized, Dirac-dispersive surface states protected by crystal symmetry. Multiple surface states exist on the {100}, {110}, and {111} surfaces of SnTe, with the band structure of surface states depending on the mirror symmetry of a particular surface. Thus, to access surface states selectively, it is critical to control the morphology of SnTe such that only desired crystallographic surfaces are present. Here, we grow SnTe nanostructures using vapor-liquid-solid and vapor-solid growth mechanisms. Previously, SnTe nanowires and nanocrystals have been grown.1-4 In this report, we demonstrate synthesis of SnTe nanoplates with lateral dimensions spanning tens of microns and thicknesses of a hundred nanometers. The top and bottom surfaces are either (100) or (111), maximizing topological surface states on these surfaces. Magnetotransport on these SnTe nanoplates shows high bulk carrier density, consistent with bulk SnTe crystals arising due to defects such as Sn vacancies. In addition, we observe a structural phase transition in these nanoplates from the high temperature rock salt to low temperature rhombohedral structure. For nanoplates with very high carrier density, we observe a slight upturn in resistance at low temperatures, indicating electron-electron interactions. 	
1411.4245v2	http://arxiv.org/pdf/1411.4245v2	2015	How turbulence regulates biodiversity in systems with cyclic competition	Daniel Groselj|Frank Jenko|Erwin Frey	  Cyclic, nonhierarchical interactions among biological species represent a general mechanism by which ecosystems are able to maintain high levels of biodiversity. However, species coexistence is often possible only in spatially extended systems with a limited range of dispersal, whereas in well-mixed environments models for cyclic competition often lead to a loss of biodiversity. Here we consider the dispersal of biological species in a fluid environment, where mixing is achieved by a combination of advection and diffusion. In particular, we perform a detailed numerical analysis of a model composed of turbulent advection, diffusive transport, and cyclic interactions among biological species in two spatial dimensions and discuss the circumstances under which biodiversity is maintained when external environmental conditions, such as resource supply, are uniform in space. Cyclic interactions are represented by a model with three competitors, resembling the children's game of rock-paper-scissors, whereas the flow field is obtained from a direct numerical simulation of two-dimensional turbulence with hyperviscosity. It is shown that the space-averaged dynamics undergoes bifurcations as the relative strengths of advection and diffusion compared to biological interactions are varied. 	
1504.03276v1	http://arxiv.org/pdf/1504.03276v1	2015	Is the Shroud of Turin in Relation to the Old Jerusalem Historical   Earthquake?	Alberto Carpinteri|Giuseppe Lacidogna|Oscar Borla	  Phillips and Hedges suggested, in the scientific magazine Nature (1989), that neutron radiation could be liable of a wrong radiocarbon dating, while proton radiation could be responsible of the Shroud body image formation. On the other hand, no plausible physical reason has been proposed so far to explain the radiation source origin, and its effects on the linen fibres. However, some recent studies, carried out by the first author and his Team at the Laboratory of Fracture Mechanics of the Politecnico di Torino, found that it is possible to generate neutron emissions from very brittle rock specimens in compression through piezonuclear fission reactions. Analogously, neutron flux increments, in correspondence to seismic activity, should be a result of the same reactions. A group of Russian scientists measured a neutron flux exceeding the background level by three orders of magnitude in correspondence to rather appreciable earthquakes (4th degree in Richter Scale). The authors consider the possibility that neutron emissions by earthquakes could have induced the image formation on Shroud linen fibres, trough thermal neutron capture by Nitrogen nuclei, and provided a wrong radiocarbon dating due to an increment in C(14,6)content. Let us consider that, although the calculated integral flux of 10^13 neutrons per square centimetre is 10 times greater than the cancer therapy dose, nevertheless it is100 times smaller than the lethal dose. 	
1506.04981v2	http://arxiv.org/pdf/1506.04981v2	2015	Analogies between the cracking noise of ethanol-dampened charcoal and   earthquakes	H. V. Ribeiro|L. S. Costa|L. G. A. Alves|P. A. Santoro|S. Picoli|E. K. Lenzi|R. S. Mendes	  We report on an extensive characterization of the cracking noise produced by charcoal samples when dampened with ethanol. We argue that the evaporation of ethanol causes transient and irregularly distributed internal stresses that promote the fragmentation of the samples and mimic some situations found in mining processes. The results show that, in general, the most fundamental seismic laws ruling earthquakes (Gutenberg-Richter law, unified scaling law for the recurrence times, Omori's law, productivity law and Bath's law) hold under the conditions of the experiment. Some discrepancies were also identified (a smaller exponent in Gutenberg-Richter law, a stationary behavior in the aftershock rates for long times and a double power-law relationship in productivity law) and related to the different loading condition. Our results thus corroborate to elucidate the parallel between seismic laws and fracture experiments caused by a more complex loading condition that also occurs in natural and induced seismicity (such as long-term fluid injection and gas-rock outbursts in mining processes). 	
1507.02388v2	http://arxiv.org/pdf/1507.02388v2	2015	A Venus-Mass Planet Orbiting a Brown Dwarf: Missing Link between Planets   and Moons	A. Udalski|Y. K. Jung|C. Han|A. Gould|S. Kozlowski|J. Skowron|R. Poleski|I. Soszyński|P. Pietrukowicz|P. Mróz|M. K. Szymański|Ł. Wyrzykowski|K. Ulaczyk|G. Pietrzyński|Y. Shvartzvald|D. Maoz|S. Kaspi|B. S. Gaudi|K. -H. Hwang|J. -Y. Choi|I. -G. Shin|H. Park|V. Bozza	  The co-planarity of solar-system planets led Kant to suggest that they formed from an accretion disk, and the discovery of hundreds of such disks around young stars as well as hundreds of co-planar planetary systems by the Kepler satellite demonstrate that this formation mechanism is extremely widespread. Many moons in the solar system, such as the Galilean moons of Jupiter, also formed out of the accretion disks that coalesced into the giant planets. We report here the discovery of an intermediate system OGLE-2013-BLG-0723LB/Bb composed of a Venus-mass planet orbiting a brown dwarf, which may be viewed either as a scaled down version of a planet plus star or as a scaled up version of a moon plus planet orbiting a star. The latter analogy can be further extended since they orbit in the potential of a larger, stellar body. For ice-rock companions formed in the outer parts of accretion disks, like Uranus and Callisto, the scaled masses and separations of the three types of systems are similar, leading us to suggest that formation processes of companions within accretion disks around stars, brown dwarfs, and planets are similar. 	
1605.07579v1	http://arxiv.org/pdf/1605.07579v1	2016	Martian zeolites as a source of atmospheric methane	Olivier Mousis|Jean-Marc Simon|Jean-Pierre Bellat|Frédéric Schmidt|Sylvain Bouley|Eric Chassefière|Violaine Sautter|Yoann Quesnel|Sylvain Picaud|Sébastien Lectez	  The origin of the martian methane is still poorly understood. A plausible explanation is that methane could have been produced either by hydrothermal alteration of basaltic crust or by serpentinization of ultramafic rocks producing hydrogen and reducing crustal carbon into methane. Once formed, methane storage on Mars is commonly associated with the presence of hidden clathrate reservoirs. Here, we alternatively suggest that chabazite and clinoptilolite, which belong to the family of zeolites, may form a plausible storage reservoir of methane in the martian subsurface. Because of the existence of many volcanic terrains, zeolites are expected to be widespread on Mars and their Global Equivalent Layer may range up to more than $\sim$1 km, according to the most optimistic estimates. If the martian methane present in chabazite and clinoptilolite is directly sourced from an abiotic source in the subsurface, the destabilization of a localized layer of a few millimeters per year may be sufficient to explain the current observations. The sporadic release of methane from these zeolites requires that they also remained isolated from the atmosphere during its evolution. The methane release over the ages could be due to several mechanisms such as impacts, seismic activity or erosion. If the methane outgassing from excavated chabazite and/or clinoptilolite prevails on Mars, then the presence of these zeolites around Gale Crater could explain the variation of methane level observed by Mars Science Laboratory. 	
1609.01237v1	http://arxiv.org/pdf/1609.01237v1	2016	Blade-shaped (PKN) Hydraulic Fracture Driven By A Turbulent Fluid In An   Impermeable Rock	Navid Zolfaghari|Colin R. Meyer|Andrew P. Bunger	  High flow rate, water-driven hydraulic fractures are more common now than ever in the oil and gas industry. Although the fractures are small, the high injection rate and low viscosity of the water, lead to high Reynolds numbers and potentially turbulence in the fracture. Here we present a semi-analytical solution for a blade-shaped (PKN) geometry hydraulic fracture driven by a turbulent fluid in the limit of zero fluid leak-off to the formation. We model the turbulence in the PKN fracture using the Gaukler-Manning-Strickler parametrization, which relates the the flow rate of the water to the pressure gradient along the fracture. The key parameter in this relation is the Darcy-Weisbach friction factor for the roughness of the crack wall. Coupling this turbulence parametrization with conservation of mass allows us to write a nonlinear pde for the crack width as a function of space and time. By way of a similarity ansatz, we obtain a semi-analytical solution using an orthogonal polynomial series. Embedding the asymptotic behavior near the fracture tip into the polynomial series, we find very rapid convergence: a suitably accurate solution is obtained with two terms of the series. This closed-form solution facilitates clear comparisons between the results and parameters for laminar and turbulent hydraulic fractures. In particular, it resolves one of the well known problems whereby calibration of models to data has difficulty simultaneously matching the hydraulic fracture length and wellbore pressure. 	
1701.01177v2	http://arxiv.org/pdf/1701.01177v2	2017	The Width Distribution of Loops and Strands in the Solar Corona -- Are   we Hitting Rock Bottom ?	Markus J. Aschwanden|Hard Peter	  In this study we analyze {\sl Atmospheric Imaging Assembly (AIA)} and Hi-C images in order to investigate absolute limits for the finest loop strands. We develop a model of the occurrence-size distribution function of coronal loop widths, characterized by a lower limit of widths $w_{min}$, a peak width $w_p$, a peak occurrence number $n_p$, and a power law slope $a$. Our data analysis includes automated tracing of curvi-linear features with the OCCULT-2 code, automated sampling of the cross-sectional widths of coronal loops, and fitting of the theoretical size distribution to the observed distribution. With Monte-Carlo simulations and variable pixel sizes $\Delta x$ we derive a first diagnostic criterion to discriminate whether the loop widths are unresolved $(w_p/\Delta x \approx 2.5\pm0.2)$, or fully resolved (if $w_p/\Delta x > 2.7$). For images with resolved loop widths we can apply a second diagnostic criterion that predicts the lower limit of loop widths, $w_{min} \approx 3 (\Delta x_{crit}-0.37")$ as a function of the critical resolution $\Delta x_{crit}$. We find that the loop widths are marginally resolved in AIA images, but are fully resolved in Hi-C images, where our model predicts a lower limit of loop widths at $w_{min} \approx 100$ km and a most frequent (peak) value at $w_p \approx 300$ km, in agreement with recent results of Brooks et al. This result agrees with the statistics of photospheric granulation sizes and thus supports coronal heating mechanisms operating on the macroscopic scale of photospheric magneto-convection, rather than nanoflare heating models with unresolved microscopic scales. 	
1702.04438v2	http://arxiv.org/pdf/1702.04438v2	2017	A novel procedure for the identification of chaos in complex biological   systems	D. Bazeia|M. B. P. N. Pereira|A. V. Brito|B. F. de Oliveira|J. G. G. S. Ramos	  We demonstrate the presence of chaos in stochastic simulations that are widely used to study biodiversity in nature. The investigation deals with a set of three distinct species that evolve according to the standard rules of mobility, reproduction and predation, with predation following the cyclic rules of the popular rock, paper and scissors game. The study uncovers the possibility to distinguish between time evolutions that start from slightly different initial states, guided by the Hamming distance which heuristically unveils the chaotic behavior. The finding opens up a quantitative approach that relates the correlation length to the average density of maxima of a typical species, and an ensemble of stochastic simulations is implemented to support the procedure. The main result of the work shows how a single and simple experimental realization that counts the density of maxima associated with the chaotic evolution of the species serves to infer its correlation length. We use the result to investigate others distinct complex systems, one dealing with a set of differential equations that can be used to model a diversity of natural and artificial chaotic systems, and another one, focusing on the ocean water level. 	
1705.01296v1	http://arxiv.org/pdf/1705.01296v1	2017	Nonlinear waves in solids with slow dynamics: an internal-variable model	B Berjamin|N Favrie|B Lombard|G Chiavassa	  In heterogeneous solids such as rocks and concrete, the speed of sound diminishes with the strain amplitude of a dynamic loading (softening). This decrease known as "slow dynamics" occurs at time scales larger than the period of the forcing. Also, hysteresis is observed in the steady-state response. The phenomenological model by Vakhnenko et al. is based on a variable that describes the softening of the material [Phys. Rev. E 70-1, 2004]. However, this model is 1D and it is not thermodynamically admissible. In the present article, a 3D model is derived in the framework of the finite strain theory. An internal variable that describes the softening of the material is introduced, as well as an expression of the specific internal energy. A mechanical constitu-tive law is deduced from the Clausius-Duhem inequality. Moreover, a family of evolution equations for the internal variable is proposed. Here, an evolution equation with one relaxation time is chosen. By construction, this new model of continuum is thermodynamically admissible and dissipative (inelas-tic). In the case of small uniaxial deformations, it is shown analytically that the model reproduces qualitatively the main features of real experiments. 	
1706.07610v1	http://arxiv.org/pdf/1706.07610v1	2017	Superconducting (Li, Fe)OHFeSe film of high quality and high critical   parameters	Yulong Huang|Zhongpei Feng|Shunli Ni|Jun Li|Wei Hu|Shaobo Liu|Yiyuan Mao|Huaxue Zhou|Fang Zhou|Kui Jin|Huabing Wang|Jie Yuan|Xiaoli Dong|Zhongxian Zhao	  The superconducting film of (Li1-xFex)OHFeSe is reported for the first time. The thin film exhibits a small in-plane crystal mosaic of 0.22 deg, in terms of the FWHM (full-width-at-half-maximum) of x-ray rocking curve, and an excellent out-of-plane orientation by x-ray phi-scan. Its bulk superconducting transition temperature (Tc) of 42.4 K is characterized by both zero electrical resistance and diamagnetization measurements. The upper critical field (Hc2) is estimated to be 79.5 T and 443 T, respectively, for the magnetic field perpendicular and parallel to the ab plane. Moreover, a large critical current density (Jc) of a value over 0.5 MA/cm2 is achieved at ~20 K. Such a (Li1-xFex)OHFeSe film is therefore not only important to the fundamental research for understanding the high-Tc mechanism, but also promising in the field of high-Tc superconductivity application, especially in high-performance electronic devices and large scientific facilities such as superconducting accelerator. 	
1711.08966v3	http://arxiv.org/pdf/1711.08966v3	2018	Survival behavior in the cyclic Lotka-Volterra model with a randomly   switching reaction rate	Robert West|Mauro Mobilia|Alastair M. Rucklidge	  We study the influence of a randomly switching reproduction-predation rate on the survival behavior of the non-spatial cyclic Lotka-Volterra model, also known as the zero-sum rock-paper-scissors game, used to metaphorically describe the cyclic competition between three species. In large and finite populations, demographic fluctuations (internal noise) drive two species to extinction in a finite time, while the species with the smallest reproduction-predation rate is the most likely to be the surviving one ("law of the weakest"). Here, we model environmental (external) noise by assuming that the reproduction-predation rate of the "strongest species" (the fastest to reproduce/predate) in a given static environment randomly switches between two values corresponding to more and less favorable external conditions. We study the joint effect of environmental and demographic noise on the species survival probabilities and on the mean extinction time. In particular, we investigate whether the survival probabilities follow the law of the weakest and analyze their dependence of the external noise intensity and switching rate. Remarkably, when, on average, there is a finite number of switches prior to extinction, the survival probability of the predator of the species whose reaction rate switches typically varies non-monotonically with the external noise intensity (with optimal survival about a critical noise strength). We also outline the relationship with the case where all reaction rates switch on markedly different time scales. 	
1712.01926v1	http://arxiv.org/pdf/1712.01926v1	2017	Lattice thermal transport in group II-alloyed PbTe	Yi Xia|James M. Hodges|Mercouri G. Kanatzidis|Maria K. Y. Chan	  PbTe, one of the most promising thermoelectric materials, has recently demonstrated thermoelectric figure of merit ($ZT$) of above 2.0 when alloyed with group II elements. The improvements are due mainly to significant reduction of lattice thermal conductivity ($\kappa_{l}$), which was in turn attributed to nanoparticle precipitates. However, a fundamental understanding of various phonon scattering mechanisms within the bulk alloy is still lacking. In this work, we apply the newly-developed density-functional-theory (DFT)-based compressive sensing lattice dynamics (CSLD) approach to model lattice heat transport in PbTe, MTe, and Pb$_{0.94}$M$_{0.06}$Te (M=Mg, Ca, Sr and Ba), compare our results with experimental measurements, with focus on strain effect and mass disorder scattering. We find that (1) CaTe, SrTe and BaTe in the rock-salt structure exhibit much higher $\kappa_{l}$ than PbTe, while MgTe in the same structure shows anomalously low $\kappa_{l}$; (2) lattice heat transport of PbTe is extremely sensitive to static strain induced by alloying atoms in solid solution form; (3) mass disorder scattering plays a major role in reducing $\kappa_{l}$ for Mg/Ca/Sr-alloyed PbTe through strongly suppressing the lifetimes of intermediate- and high-frequency phonons, while for Ba-alloyed PbTe, precipitated nanoparticles are also important. 	
1801.06652v1	http://arxiv.org/pdf/1801.06652v1	2018	Reactive transport under stress: Permeability evolution in deformable   porous media	Roi Roded|Xavier Paredes|Ran Holtzman	  We study reactive transport in a stressed porous media, where dissolution of the solid matrix causes two simultaneous, competing effects: pore enlargement (chemical deformation), and pore compaction due to mechanical weakening. A novel, mechanistic pore-scale model simulates flooding of a sample under fixed confining stress, showing that increasing stress inhibits permeability enhancement, increasing the injected volume required to reach a certain permeability, in agreement with recent experiments. We explain this behavior by stress concentration downstream, in the less dissolved (hence stiffer) region. As this region is also less conductive, even its small compaction has a strong bottleneck effect that curbs the permeability.   Our results also elucidate that the impact of stress depends on the dissolution regime. Under wormholing conditions (slow injection, i.e. high Damkohler, $Da$), the development of a sharp dissolution front and high porosity contrast accentuates the bottleneck effect. This reduces transport heterogeneity, promoting wormhole competition. Once the outlet starts eroding, the extreme focusing of transport and hence dissolution--characteristic of wormholing--becomes dominant, diminishing the bottleneck effect and hence the impact of stress. In contrast, at low $Da$, incomplete reaction upstream allows the reagent to traverse the sample, causing a more uniform dissolution. The continuous dissolution and its partial counteraction by compaction downstream provides a steady, gradual increase in the effect of stress. Consequently, the impact of stress is more pronounced at high $Da$ during early stages (low permeability), and at low $Da$ close to breakthrough. Our work promotes understanding of the hydromechanical property evolution, with important implications for processes ranging from diagenesis and weathering of rocks, to well stimulation and carbon geosequetration. 	
1802.05456v1	http://arxiv.org/pdf/1802.05456v1	2018	Transport of polymer particles in a oil-water flow in porous media:   enhancing oil recovery	Max A. Endo Kokubun|Florin A. Radu|Eirik Keilegavlen|Kundan Kumar|Kristine Spildo	  We study a heuristic, core-scale model for the transport of polymer particles in a two phase (oil and water) porous medium. We are motivated by recent experimental observations which report increased oil recovery when polymers are injected after the initial waterflood. The recovery mechanism is believed to be microscopic diversion of the flow, where injected particles can accumulate in narrow pore throats and clog it, in a process known as a log-jamming effect. The blockage of the narrow pore channels lead to a microscopic diversion of the water flow, causing a redistribution of the local pressure, which again can lead to the mobilization of trapped oil, enhancing its recovery. Our objective herein is to develop a core-scale model that is consistent with the observed production profiles. We show that previously obtained experimental results can be qualitatively explained by a simple two-phase flow model with an additional transport equation for the polymer particles. A key aspect of the formulation is that the microscopic heterogeneity of the rock and a dynamic altering of the permeability must be taken into account in the rate equations. 	
1507.01716v1	http://arxiv.org/pdf/1507.01716v1	2015	Temporal-varying failures of nodes in networks	Georgie Knight|Giampaolo Cristadoro|Eduardo G. Altmann	  We consider networks in which random walkers are removed because of the failure of specific nodes. We interpret the rate of loss as a measure of the importance of nodes, a notion we denote as failure-centrality. We show that the degree of the node is not sufficient to determine this measure and that, in a first approximation, the shortest loops through the node have to be taken into account. We propose approximations of the failure-centrality which are valid for temporal-varying failures and we dwell on the possibility of externally changing the relative importance of nodes in a given network, by exploiting the interference between the loops of a node and the cycles of the temporal pattern of failures. In the limit of long failure cycles we show analytically that the escape in a node is larger than the one estimated from a stochastic failure with the same failure probability. We test our general formalism in two real-world networks (air-transportation and e-mail users) and show how communities lead to deviations from predictions for failures in hubs. 	
1301.1433v1	http://arxiv.org/pdf/1301.1433v1	2013	Deformation mechanisms in a TiNi shape memory alloy during cyclic   loading	Anne-Lise Gloanec|Giovambattista Billota|Michel Gerland	  The deformation mechanisms governing the cyclic stress-strain behaviour of a TiNi shape memory alloy were investigated in this work. To understand the development of these mechanisms during cyclic loading, three low-cycle fatigue tests were performed and stopped at different stages. The first test was stopped after the first cycle; the second one was stopped after 40 cycles, corresponding to the beginning of the stabilisation of the cyclic strain-stress behaviour; and the last one was carried out to failure (3324 cycles). Submitted to fatigue loading, the response of the TiNi shape memory alloy presents a classical pseudoelastic response. Two deformation mechanisms, identified by TEM observations, are highlighted, the first one by twins and the second by dislocation slip and its interaction with precipitates. These two mechanisms evolve without competition during cyclic loading. The nanomechanical properties of the alloy were also examined, and the evolution of the microhardness or indentation modulus was monitored. 	
1505.07524v1	http://arxiv.org/pdf/1505.07524v1	2015	Hydrogen Segregation in Palladium and the Combined Effects of   Temperature and Defects on Mechanical Properties	Hieu H. Pham|A. Amine Benzerga|Tahir Cagin	  Atomistic calculations were carried out to investigate the mechanical properties of Pd crystals as a combined function of structural defects, hydrogen concentration and high temperature. These factors are found to individually induce degradation in the mechanical strength of Pd in a monotonous manner. In addition, defects such as vacancies and grain boundaries could provide a driving force for hydrogen segregation, thus enhance the tendency for their trapping. The simulations show that hydrogen maintains the highest localization at grain boundaries at ambient temperatures. This finding correlates well with the experimental observation that hydrogen embrittlement is more frequently observed around room temperature. The strength-limiting mechanism of mechanical failures induced by hydrogen is also discussed, which supports the hydrogen-enhanced localized plasticity theorem. 	
1509.02879v1	http://arxiv.org/pdf/1509.02879v1	2015	Progressive Collapse Mechanisms of Brittle and Ductile Framed Structures	Enrico Masoero|Falk K. Wittel|Hans J. Herrmann|B. M. Chiaia	  In this paper, we study the progressive collapse of 3D framed structures made of reinforced concrete after the sudden loss of a column. The structures are represented by elasto-plastic Euler Bernoulli beams with elongation-rotation failure threshold. We performed simulations using the Discrete Element Method considering inelastic collisions between the structural elements. The results show what collapse initiation and impact-driven propagation mechanisms are activated in structures with different geometric and mechanical features. Namely, we investigate the influence of the cross sectional size and reinforcement $\alpha$ and of the plastic capacity $\beta$ of the structural elements. We also study the final collapse extent and the fragment size distribution and their relation to $\alpha$, $\beta$ and to the observed collapse mechanisms. Finally, we compare the damage response of structures with symmetric and asymmetric reinforcement in the beams. 	
1512.00665v1	http://arxiv.org/pdf/1512.00665v1	2015	HBTM: A Heartbeat-based Behavior Detection Mechanism for POSIX Threads   and OpenMP Applications	Weidong Wang|Chunhua Liao|Liqiang Wang|Daniel J. Quinlan|Wei Lu	  Extreme-scale computing involves hundreds of millions of threads with multi-level parallelism running on large-scale hierarchical and heterogeneous hardware. In POSIX threads and OpenMP applications, some key behaviors occurring in runtime such as thread failure, busy waiting, and exit need to be accurately and timely detected. However, for the most of these applications, there are lack of unified and efficient detection mechanisms to do this. In this paper, a heartbeat-based behavior detection mechanism for POSIX threads (Pthreads) and OpenMP applications (HBTM) is proposed. In the design, two types of implementations are conducted, centralized and decentralized respectively. In both implementations, unified API has been designed to guarantee the generality of the mechanism. Meanwhile, a ring-based detection algorithm is designed to ease the burden of the centra thread at runtime. To evaluate the mechanism, the NAS Parallel Benchmarks (NPB) are used to test the performance of the HBTM. The experimental results show that the HBTM supports detection of behaviors of POSIX threads and OpenMP applications while acquiring a short latency and near 1% overhead. 	
1602.00295v2	http://arxiv.org/pdf/1602.00295v2	2016	Understanding the Planck Blackbody Spectrum and Landau Diamagnetism   within Classical Electromagnetism	Timothy H. Boyer	  Electromagnetism is a \textit{relativistic} theory and one must exercise care in coupling this theory with \textit{nonrelativistic} classical mechanics and with \textit{nonrelativistic} classical statistical mechanics. Indeed historically, both the blackbody radiation spectrum and diamagnetism within classical theory have been misunderstood because of two crucial failures: 1)the neglect of classical electromagnetic zero-point radiation, and 2) the use of erroneous combinations of nonrelativistic mechanics with relativistic electrodynamics. Here we review the treatment of classical blackbody radiation, and show that use of Lorentz-invariant classical electromagnetic zero-point radiation can be used to explain both the Planck blackbody spectrum and Landau diamagnetism at thermal equilibrium within classical electromagnetic theory. The analysis requires that relativistic electromagnetism is joined appropriately with simple nonrelativistic mechanical systems which can be regarded as the zero-velocity limits of relativistic systems, and that nonrelativistic classical statistical mechanics is applied only in the low-frequency limit when zero-point energy makes no contribution. 	
1710.01124v1	http://arxiv.org/pdf/1710.01124v1	2017	Effect of chain length distribution on mechanical behavior of polymeric   networks	Mohammad Tehrani|Alireza Sarvestani	  The effect of network chain distribution on mechanical behavior of elastomers is one of the long standing problems in rubber mechanics. The classical theory of rubber elasticity is built upon the assumption of entropic elasticity of networks whose constitutive strands are of uniform length. The kinetic theories for vulcanization, computer simulations, and indirect experimental measurements all indicate that the microstructure of vulcanizates is made of polymer strands with a random distribution of length. The polydispersity in strand length is expected to control the mechanical strength of rubber as the overloaded short strands break at small deformations and transfer the load to the longer strands. The purpose of this contribution is to present a simple theory of rubber mechanics which takes into account the length distribution of strands and its effect on the onset of bulk failure. 	
1802.00598v1	http://arxiv.org/pdf/1802.00598v1	2018	Mechanical responses of two-dimensional MoTe2; pristine 2H, 1T and 1T'   and 1T'/2H heterostructure	Bohayra Mortazavi|Golibjon R Berdiyorov|Meysam Makaremi|Timon Rabczuk	  Transition metal dichalcogenides (TMD) are currently among the most interesting two-dimensional (2D) materials due to their outstanding properties. MoTe2 involves attractive polymorphic TMD crystals which can exist in three different 2D atomic lattices of 2H, 1T and 1T', with diverse properties, like semiconducting and metallic electronic characters. Using the polymorphic heteroepitaxy, most recently coplanar semiconductor/metal (2H/1T') few-layer MoTe2 heterostructures were experimentally synthesized, highly promising to build circuit components for next generation nanoelectronics. Motivated by the recent experimental advances, we conducted first-principles calculations to explore the mechanical properties of single-layer MoTe2 structures. We first studied the mechanical responses of pristine and single-layer 2H-, 1T- and 1T'-MoTe2. In these cases we particularly analyzed the possibility of engineering of the electronic properties of these attractive 2D structures using the biaxial or uniaxial tensile loadings. Finally, the mechanical-failure responses of 1T'/2H-MoTe2 heterostructure were explored, which confirms the remarkable strength of this novel 2D system. 	
1703.01595v1	http://arxiv.org/pdf/1703.01595v1	2017	Creep stability of the proposed AIDA mission target 65803 Didymos: I.   Discrete cohesionless granular physics model	Yun Zhang|Derek C. Richardson|Olivier S. Barnouin|Clara Maurel|Patrick Michel|Stephen R. Schwartz|Ronald-Louis Ballouz|Lance A. M. Benner|Shantanu P. Naidu|Junfeng Li	  As the target of the proposed Asteroid Impact & Deflection Assessment (AIDA) mission, the near-Earth binary asteroid 65803 Didymos represents a special class of binary asteroids, those whose primaries are at risk of rotational disruption. To gain a better understanding of these binary systems and to support the AIDA mission, this paper investigates the creep stability of the Didymos primary by representing it as a cohesionless self-gravitating granular aggregate subject to rotational acceleration. To achieve this goal, a soft-sphere discrete element model (SSDEM) capable of simulating granular systems in quasi-static states is implemented and a quasi-static spin-up procedure is carried out. We devise three critical spin limits for the simulated aggregates to indicate their critical states triggered by reshaping and surface shedding, internal structural deformation, and shear failure, respectively. The failure condition and mode, and shear strength of an aggregate can all be inferred from the three critical spin limits. The effects of arrangement and size distribution of constituent particles, bulk density, spin-up path, and interparticle friction are numerically explored. The results show that the shear strength of a spinning self-gravitating aggregate depends strongly on both its internal configuration and material parameters, while its failure mode and mechanism are mainly affected by its internal configuration. Additionally, this study provides some constraints on the possible physical properties of the Didymos primary based on observational data and proposes a plausible formation mechanism for this binary system. With a bulk density consistent with observational uncertainty and close to the maximum density allowed for the asteroid, the Didymos primary in certain configurations can remain geo-statically stable without including cohesion. 	
1707.00788v1	http://arxiv.org/pdf/1707.00788v1	2017	Lifeguard : SWIM-ing with Situational Awareness	Armon Dadgar|James Phillips|Jon Currey	  SWIM is a peer-to-peer group membership protocol that uses randomized probing and gossip to obtain attractive scaling and robustness properties. Sensitivity to slow message processing, due to factors such as CPU exhaustion, network delay or loss, can lead SWIM to declare healthy members faulty. To counter this, SWIM adds a Suspicion mechanism, that trades increased failure detection latency for a lower false positive failure detection rate. However, relatively short lived periods of slow message processing commonly experienced in data centers can still lead to healthy members being marked as failed.   We observe that the Suspicion mechanism still assumes timely processing of some messages. In particular, refutation of a suspicion can only succeed if it is processed by the suspecting member in a timely manner. However, missing expected responses could indicate a member is experiencing slow message processing, and an episode of slow message processing at a given group member is likely to impact multiple of its interactions with other members in a short period of time. Based on these insights, we define a set of extensions to SWIM that allow a member to dynamically adjust its timeouts to mitigate timeliness issues. We call these extensions Lifeguard.   We analyze the effect of Lifeguard using synthetic benchmarks that vary message processing delays in a controlled manner. Across the wide range of cases tested, Lifeguard is able to reduce the false positive rate by a factor of more than 50x, while modestly increasing failure detection latency and message load.   Furthermore, by modifying tuning parameters, Life- guard allows users to reduce median detection latency by 45% while still reducing false positives at healthy members by 3x compared to without Lifeguard. The tuning parameters allow users to choose a suitable trade-off between lower false positives and lower detection latency. 	
9612095v1	http://arxiv.org/pdf/cond-mat/9612095v1	1996	First-Order Transition in the Breakdown of Disordered Media	Stefano Zapperi|Purusattam Ray|H. Eugene Stanley|Alessandro Vespignani	  We study the approach to global breakdown in disordered media driven by increasing external forces. We first analyze the problem by mean-field theory, showing that the failure process can be described as a first-order phase transition, similarly to the case of thermally activated fracture in homogeneous media. Then we quantitatively confirm the predictions of the mean-field theory using numerical simulations of discrete models. Widely distributed avalanches and the corresponding mean-field scaling are explained by the long-range nature of elastic interactions. We discuss the analogy of our results to driven disordered first-order transitions and spinodal nucleation in magnetic systems. 	
9711296v1	http://arxiv.org/pdf/cond-mat/9711296v1	1997	Analytical approaches to CA for traffic flow: Approximations and exact   solutions	Andreas Schadschneider	  Cellular automata have turned out to be important tools for the simulation of traffic flow. They are designed for an efficient impletmentation on the computer, but hard to treat analytically. Here we discuss several approaches for an analytical description of the Nagel-Schreckenberg (NaSch) model and its variants. These methods yield the exact solution for the special case $\vm=1$ of the NaSch model and are good approximations for higher values of the velocity ($\vm > 1$). We discuss the validity of these approximations and the conclusions for the underlying physics that can be drawn from the success or failure of the approximation. 	
9906057v1	http://arxiv.org/pdf/cond-mat/9906057v1	1999	Rate-and-State Theory of Plastic Deformation Near a Circular Hole	J. S. Langer|Alexander E. Lobkovsky	  We show that a simple rate-and-state theory accounts for most features of both time-independent and time-dependent plasticity in a spatially inhomogeneous situation, specifically, a circular hole in a large stressed plate. Those features include linear viscoelastic flow at small applied stresses, strain hardening at larger stresses, and a dynamic transition to viscoplasticity at a yield stress. In the static limit, this theory predicts the existence of a plastic zone near the hole for some but not all ranges of parameters. The rate-and-state theory also predicts dynamic failure modes that we believe may be relevant to fracture mechanics. 	
9908226v2	http://arxiv.org/pdf/cond-mat/9908226v2	2000	Damage in Fiber Bundle Models	Ferenc Kun|Stefano Zapperi|Hans J. Herrmann	  We introduce a continuous damage fiber bundle model that gives rise to macroscopic plasticity and compare its behavior with that of dry fiber bundles. Several interesting constitutive behaviors are found in this model depending on the value of the damage parameter and on the form of the disorder distribution. In addition, we compare the behavior of global load transfer models with local load transfer models and study in detail the damage evolution before failure. We emphasize the analogies between our results and spinodal nucleation in first-order phase transitions. 	
0004398v1	http://arxiv.org/pdf/cond-mat/0004398v1	2000	Fast fluctuating fields as the source of low-frequency conductance   fluctuations in many-electron systems and failure of quantum kinetics	Yu. E. Kuzovlev	  It is shown that in many-electron systems quantum transfer amplitudes and thus transfer probabilities may be strongly influenced by fast fluctuating fields, in particular, caused by simultaneous electron transfers. Corresponding mutual interplay of many electron jumps, arising at the fundamental level of quantum phases, results in long-correlated (1/f type) conductance fluctuations. However, thats could not be theoretically catched if neglect the real discreteness of quantum energy spectra and use the continuous spectrum approximation when building kinetic theory. Basing on first principles, the estimates of low-frequency fluctuations of tunneling conductance are presented. 	
0008208v1	http://arxiv.org/pdf/cond-mat/0008208v1	2000	Effective attraction between like-charged colloids in a 2D plasma	Ning Ma|S. M. Girvin|R. Rajaraman	  The existence of attractions between like-charged colloids immersed in ionic solution have been discovered in recent experiments. This phenomenon contradicts the predictions of DLVO theory and indicates a failure of mean field theory. We study a toy model based on a two dimensional one-component plasma, which is exactly soluble at one particular coupling constant. We show that colloidal interaction results from a competition between ion-ion repulsion and longer ranged ion-void attraction. 	
0011113v2	http://arxiv.org/pdf/cond-mat/0011113v2	2001	Failure regime in (1+1) dimensions in fibrous materials	I. L. Menezes-Sobrinho|A. T. Bernardes|J. G. Moreira	  In this paper, we introduce a model for fracture in fibrous materials that takes into account the rupture height of the fibers, in contrast with previous models. Thus, we obtain the profile of the fracture and calculate its roughness, defined as the variance around the mean height. We investigate the relationship between the fracture roughness and the fracture toughness. 	
0101071v1	http://arxiv.org/pdf/cond-mat/0101071v1	2001	Conditional statistics of temperature fluctuations in turbulent   convection	Emily S. C. Ching|K. L. Chau	  We find that the conditional statistics of temperature difference at fixed values of the locally averaged temperature dissipation rate in turbulent convection become Gaussian in the regime where the mixing dynamics is expected to be driven by buoyancy. Hence, intermittency of the temperature fluctuations in this buoyancy-driven regime can be solely attributed to the variation of the locally averaged temperature dissipation rate. We further obtain the functional behavior of these conditional temperature structure functions. This functional form demonstrates explicitly the failure of dimensional agruments and enhances the understanding of the temperature structure functions. 	
0105274v1	http://arxiv.org/pdf/cond-mat/0105274v1	2001	Failure of random matrix theory to correctly describe quantum dynamics	Tsampikos Kottos|Doron Cohen	  Consider a classically chaotic system which is described by a Hamiltonian H_0. At t=0 the Hamiltonian undergoes a sudden-change H_0 -> H. We consider the quantum-mechanical spreading of the evolving energy distribution, and argue that it cannot be analyzed using a random-matrix theory (RMT) approach. RMT can be trusted only to the extend that it gives trivial results that are implied by first-order perturbation theory. Non-perturbative effects are sensitive to the underlying classical dynamics, and therefore the hbar-->0 behavior for effective RMT models is strikingly different from the correct semiclassical limit. 	
0107597v1	http://arxiv.org/pdf/cond-mat/0107597v1	2001	Memory beyond memory in heart beating: an efficient way to detect   pathological conditions	P. Allegrini|P. Grigolini|P. Hamilton|L. Palatella|G. Raffaelli	  We study the long-range correlations of heartbeat fluctuations with the method of diffusion entropy. We show that this method of analysis yields a scaling parameter $\delta$ that apparently conflicts with the direct evaluation of the distribution of times of sojourn in states with a given heartbeat frequency. The strength of the memory responsible for this discrepancy is given by a parameter $\epsilon^{2}$, which is derived from real data. The distribution of patients in the ($\delta$, $\epsilon^{2}$)-plane yields a neat separation of the healthy from the congestive heart failure subjects. 	
0202330v1	http://arxiv.org/pdf/cond-mat/0202330v1	2002	Optimal design, robustness, and risk aversion	M. E. J. Newman|Michelle Girvan|J. Doyne Farmer	  Highly optimized tolerance is a model of optimization in engineered systems, which gives rise to power-law distributions of failure events in such systems. The archetypal example is the highly optimized forest fire model. Here we give an analytic solution for this model which explains the origin of the power laws. We also generalize the model to incorporate risk aversion, which results in truncation of the tails of the power law so that the probability of disastrously large events is dramatically lowered, giving the system more robustness. 	
0208405v1	http://arxiv.org/pdf/cond-mat/0208405v1	2002	Depinning transitions in discrete reaction-diffusion equations	A. Carpio|L. L. Bonilla	  We consider spatially discrete bistable reaction-diffusion equations that admit wave front solutions. Depending on the parameters involved, such wave fronts appear to be pinned or to glide at a certain speed. We study the transition of traveling waves to steady solutions near threshold and give conditions for front pinning (propagation failure). The critical parameter values are characterized at the depinning transition and an approximation for the front speed just beyond threshold is given. 	
0302307v1	http://arxiv.org/pdf/cond-mat/0302307v1	2003	Lonely adatoms in space	Joachim Krug	  There is a close relation between the problems of second layer nucleation in epitaxial crystal growth and chemical surface reactions, such as hydrogen recombination, on interstellar dust grains. In both cases standard rate equation analysis has been found to fail because the process takes place in a confined geometry. Using scaling arguments developed in the context of second layer nucleation, I present a simple derivation of the hydrogen recombination rate for small and large grains. I clarify the reasons for the failure of rate equations for small grains, and point out a logarithmic correction to the reaction rate when the reaction is limited by the desorption of hydrogen atoms (the second order reaction regime). 	
0307138v2	http://arxiv.org/pdf/cond-mat/0307138v2	2004	Pseudo Random Coins Show More Heads Than Tails	Heiko Bauke|Stephan Mertens	  Tossing a coin is the most elementary Monte Carlo experiment. In a computer the coin is replaced by a pseudo random number generator. It can be shown analytically and by exact enumerations that popular random number generators are not capable of imitating a fair coin: pseudo random coins show more heads than tails. This bias explains the empirically observed failure of some random number generators in random walk experiments. It can be traced down to the special role of the value zero in the algebra of finite fields. 	
0506757v1	http://arxiv.org/pdf/cond-mat/0506757v1	2005	Quantum Non-Locality in Systems with Open Boundaries: Failure of the   Wigner-Function Formalism	Luigi Genovese|David Taj|Fausto Rossi	  We shall revisit the conventional treatment of open quantum devices based on the Wigner-Function formalism. Our analysis will show that the artificial spatial separation between device active region and external reservoirs -properly defined within a semiclassical simulation scheme- is intrinsically incompatible with the non-local character of quantum mechanics. More specifically, by means of an exactly-solvable semiconductor model, we shall show that the application of the conventional boundary-condition scheme to the Wigner transport equation may produce highly non-physical results, like thermal injection of coherent state superpositions and boundary-driven negative probability distributions. 	
0509409v1	http://arxiv.org/pdf/cond-mat/0509409v1	2005	Planar Voronoi cells and the failure of Aboav's law	Hendrik-Jan Hilhorst	  Aboav's law is a quantitative expression of the empirical fact that in planar cellular structures many-sided cells tend to have few-sided neighbors. This law is nonetheless violated in the most widely used model system, {\it viz.} the Poisson-Voronoi tessellation. We obtain the correct law for this model: Given an $n$-sided cell, any of its neighbors has on average $m\_n$ sides where $m\_n=4+3(\pi/n)^{-{1/2}}+...$ in the limit of large $n$. This expression is quite accurate also for nonasymptotic $n$ and we discuss its implications for the analysis of experimental data. 	
0511407v1	http://arxiv.org/pdf/cond-mat/0511407v1	2005	Mode-Coupling Theory (MCT) Lecture Notes	David R. Reichman|Patrick Charbonneau	  In this set of lecture notes we review the mode-coupling theory of the glass transition from several perspectives. First, we derive mode-coupling equations for the description of density fluctuations from microscopic considerations with the use the Mori-Zwanzig projection operator technique. We also derive schematic mode-coupling equations of a similar form from a field-theoretic perspective. We review the successes and failures of mode-coupling theory, and discuss recent advances in the applications of the theory. 	
0601125v1	http://arxiv.org/pdf/cond-mat/0601125v1	2006	On the free energy within the mean-field approximation	R. Agra|F. van Wijland|E. Trizac	  We compare two widespread formulations of the mean-field approximation, based on minimizing an appropriately built mean-field free energy. We use the example of the antiferromagnetic Ising model to show that one of these formulations does not guarantee the existence of an underlying variational principle. This results in a severe failure where straightforward minimization of the corresponding mean-field free energy leads to incorrect results. 	
0209014v1	http://arxiv.org/pdf/cs/0209014v1	2002	Randomized protocols for asynchronous consensus	James Aspnes	  The famous Fischer, Lynch, and Paterson impossibility proof shows that it is impossible to solve the consensus problem in a natural model of an asynchronous distributed system if even a single process can fail. Since its publication, two decades of work on fault-tolerant asynchronous consensus algorithms have evaded this impossibility result by using extended models that provide (a) randomization, (b) additional timing assumptions, (c) failure detectors, or (d) stronger synchronization mechanisms than are available in the basic model. Concentrating on the first of these approaches, we illustrate the history and structure of randomized asynchronous consensus protocols by giving detailed descriptions of several such protocols. 	
0005092v2	http://arxiv.org/pdf/hep-ph/0005092v2	2000	Semi-inclusive hadron-hadron transverse spin asymmetries and their   implication for polarized DIS	M. Boglione|E. Leader	  We discuss a possible explanation of the 25 year old mystery of the large transverse spin asymmetries found in many semi-inclusive hadron-hadron reactions. We obtain the first reliable information about the transverse polarized quark densities Delta_T q(x) and we find surprising implications for the usual, longitudinal, polarized DIS. The plan of the presentation is as follows: 1) A brief reminder about the internal structure of the nucleon. 2) The transverse asymmetries. 3) Why it is so difficult to explain the asymmetries. 4) Failure and then success using a new soft mechanism. 5) implications for polarized DIS. 	
0111351v3	http://arxiv.org/pdf/hep-ph/0111351v3	2002	Brane Cosmology Solutions with Bulk Scalar Fields	Stephen C. Davis	  Brane cosmologies with static, five-dimensional and Z_2 symmetric bulks are analysed. A general solution generating mechanism is outlined. The qualatitive cosmological behaviour of all such solutions is determined. Conditions for avoiding naked bulk singularities are also discussed. The restrictions placed on the solutions by the assumption of such a static bulk are investigated. In particular the requirement of a non-standard energy-momentum conservation law. The failure of such solutions to provide viable quintessence terms in the Friedmann equations is also discussed. 	
9310176v1	http://arxiv.org/pdf/hep-th/9310176v1	1993	Elements of Reality and the Failure of the Product Rule Measurability of   Commuting Observables	Lev Vaidman	  The concept of ``elements of reality" is analyzed within the framework of quantum theory. It is shown that elements of reality fail to fulfill the product rule. This is the core of recent proofs of the impossibility of a Lorentz-invariant interpretation of quantum mechanics. A generalization and extension of the concept of elements of reality is presented. Lorentz-invariance is restored by giving up the product rule. The consequences of giving up the ``and" rule, which must be abandoned together with the product rule, are discussed. 	
9906156v2	http://arxiv.org/pdf/hep-th/9906156v2	1999	Reparametrization Invariance of Path Integrals	H. Kleinert|A. Chervyakov	  We demonstrate the reparametrization invariance of perturbatively defined one-dimensional functional integrals up to the three-loop level for a path integral of a quantum-mechanical point particle in a box. We exhibit the origin of the failure of earlier authors to establish reparametrization invariance which led them to introduce, superfluously, a compensating potential depending on the connection of the coordinate system. We show that problems with invariance are absent by defining path integrals as the epsilon-> 0 -limit of 1+ epsilon -dimensional functional integrals. 	
0011255v2	http://arxiv.org/pdf/hep-th/0011255v2	2000	On Isolated Vacua and Background Independence	T. Banks	  I argue that isolated vacua of M-theory, cannot in any conventional way be said to live in the same theory as other disconnected parts of the moduli space. The usual field theoretic mechanisms, which allow an observer in one disconnected component of a moduli space to verify the existence of other components, fail. The failure is a consequence of robust properties of black holes. When barriers between components are much smaller than the Planck scale, the usual field theoretic picture is approximately valid. 	
0512034v1	http://arxiv.org/pdf/physics/0512034v1	2005	"Long live effrontery!" Albert Einstein and the birth of Quantum Theory	Domenico Giulini	  From its very beginning, Quantum Theory developed contrary to the intentions of its creators. For Max Planck it marks the failure of a long-term research program, in which he tried to understand the 2nd law of thermodynamics deterministically in terms of mechanics and electrodynamics. For Albert Einstein it meant a refutation of his scientific credo. I describe parts of the early stages of this most remarkable development, up to Einstein's light-quantum hypotheis and its unfavourable reception by most other physicists. 	
0312035v2	http://arxiv.org/pdf/quant-ph/0312035v2	2004	Bell's inequality and the coincidence-time loophole	Jan-Ake Larsson|Richard Gill	  This paper analyzes effects of time-dependence in the Bell inequality. A generalized inequality is derived for the case when coincidence and non-coincidence [and hence whether or not a pair contributes to the actual data] is controlled by timing that depends on the detector settings. Needless to say, this inequality is violated by quantum mechanics and could be violated by experimental data provided that the loss of measurement pairs through failure of coincidence is small enough, but the quantitative bound is more restrictive in this case than in the previously analyzed "efficiency loophole." 	
0501081v2	http://arxiv.org/pdf/quant-ph/0501081v2	2005	Quantum perfect correlations	Masanao Ozawa	  The notion of perfect correlations between arbitrary observables, or more generally arbitrary POVMs, is introduced in the standard formulation of quantum mechanics, and characterized by several well-established statistical conditions. The transitivity of perfect correlations is proved to generally hold, and applied to a simple articulation for the failure of Hardy's nonlocality proof for maximally entangled states. The notion of perfect correlations between observables and POVMs is used for defining the notion of a precise measurement of a given observable in a given state. A longstanding misconception on the correlation made by the measuring interaction is resolved in the light of the new theory of quantum perfect correlations. 	
0702233v1	http://arxiv.org/pdf/quant-ph/0702233v1	2007	Quantum Fermi's Golden Rule	Fausto Rossi	  We shall revisit the conventional adiabatic or Markov approximation, showing its intrinsic failure in describing the proper quantum-mechanical evolution of a generic subsystem interacting with its environment. In particular, we shall show that -contrary to the semiclassical case- the Markov limit does not preserve the positive-definite character of the corresponding density matrix, thus leading to highly non-physical results. To overcome this problem, we shall propose an alternative adiabatic procedure which (i) in the semiclassical limit reduces to the standard Fermi's golden rule, and (ii) describes a genuine Limblad evolution, thus providing a reliable/robust treatment of energy-dissipation and dephasing processes. 	
0709.1585v1	http://arxiv.org/pdf/0709.1585v1	2007	Ab initio Study of Misfit Dislocations at the SiC/Si(001) Interface	Giancarlo Cicero|Laurent Pizzagalli|Alessandra Catellani	  The high lattice mismatched SiC/Si(001) interface was investigated by means of combined classical and ab initio molecular dynamics. Among the several configurations analyzed, a dislocation network pinned at the interface was found to be the most efficient mechanism for strain relief. A detailed description of the dislocation core is given, and the related electronic properties are discussed for the most stable geometry: we found interface states localized in the gap that may be a source of failure of electronic devices. 	
0709.4571v1	http://arxiv.org/pdf/0709.4571v1	2007	Einstein's Lost Battles and Neglected Achievements	Petar Grujic	  We analyze some of Einstein's failures to accomplish tasks which he posed to himself, notably deterministic interpretation of Quantum mechanics and formulation of the Unified theory of physical interactions, putting them into broader ontological and epistemological context. We highlight, on the other hand, a number of his contributions that have been unjustly neglected by the physical community, like the semiclassical quantization principle and the concept of antiparticle. Finally, we mention briefly three issues, which are related to Einstein, but are subject to controversy. 	
0711.3935v1	http://arxiv.org/pdf/0711.3935v1	2007	Coding for Network Coding	Andrea Montanari|Ruediger Urbanke	  We consider communication over a noisy network under randomized linear network coding. Possible error mechanism include node- or link- failures, Byzantine behavior of nodes, or an over-estimate of the network min-cut. Building on the work of Koetter and Kschischang, we introduce a probabilistic model for errors. We compute the capacity of this channel and we define an error-correction scheme based on random sparse graphs and a low-complexity decoding algorithm. By optimizing over the code degree profile, we show that this construction achieves the channel capacity in complexity which is jointly quadratic in the number of coded information bits and sublogarithmic in the error probability. 	
0801.0382v2	http://arxiv.org/pdf/0801.0382v2	2008	Some conjectures about the mechanism of poltergeist phenomenon	P. Brovetto|V. Maxia	  Poltergeist accounts concern at least four kinds of strange spontaneous manifestations, such as burning of materials, failures of electric equipments, rapping noises and movements of objects. A simple analysis of phenomenology of these disturbances shows that they might have a common origin, that is, a reduction in strength of molecular bonds due to an enhancement in polarization of vacuum which decreases the actual electron charge. Arguments based on Prigogine' nonequilibrium thermodynamics are proposed, which show how transformations in brain of some pubescent childs or young womans might be the cause of these effects. 	
0802.4010v1	http://arxiv.org/pdf/0802.4010v1	2008	Brain architecture: A design for natural computation	Marcus Kaiser	  Fifty years ago, John von Neumann compared the architecture of the brain with that of computers that he invented and which is still in use today. In those days, the organisation of computers was based on concepts of brain organisation. Here, we give an update on current results on the global organisation of neural systems. For neural systems, we outline how the spatial and topological architecture of neuronal and cortical networks facilitates robustness against failures, fast processing, and balanced network activation. Finally, we discuss mechanisms of self-organization for such architectures. After all, the organization of the brain might again inspire computer architecture. 	
0804.2394v1	http://arxiv.org/pdf/0804.2394v1	2008	Front propagation in A+B -> 2A reaction under subdiffusion	Daniela Froemberg|Hauke Schmidt-Martens|Igor M. Sokolov|Francesc Sagues	  We consider an irreversible autocatalytic conversion reaction A+B -> 2A under subdiffusion described by continuous time random walks. The reactants' transformations take place independently on their motion and are described by constant rates. The analog of this reaction in the case of normal diffusion is described by the Fisher-Kolmogorov-Petrovskii-Piskunov (FKPP) equation leading to the existence of a nonzero minimal front propagation velocity which is really attained by the front in its stable motion. We show that for subdiffusion this minimal propagation velocity is zero, which suggests propagation failure. 	
0808.3271v1	http://arxiv.org/pdf/0808.3271v1	2008	Reply to Comment on "Failure of the work-Hamiltonian connection for   free-energy calculations" by Horowitz and Jarzynski	J. M. G. Vilar|J. M. Rubi	  We show that the Comment [arXiv:0808.1224] by Horowitz and Jarzynski obtains as a main result a general free energy change for a harmonic system that in the macroscopic limit does not recover the textbook expression for the energy change of a Hookean spring. The reason is that Horowitz and Jarzynski improperly identify work with parametric changes of the Hamiltonian instead of with the standard quantity, force times displacement. 	
0811.2776v2	http://arxiv.org/pdf/0811.2776v2	2009	On a damage-plasticity approach to model concrete failure	Peter Grassl	  A damage-plasticity constitutive model for the description of fracture in plain concrete is presented. Two approaches, the local model comprising the adjustment of the softening modulus and the nonlocal model based on spatial averaging of history variables, are applied to the analysis of a concrete bar subjected to uniaxial tension and to a three-point bending test. The influence of mesh size and the decomposition into damage and plasticity components are discussed. It is shown that for the two examples studied, both approaches result in mesh-independent results. However, the nonlocal model, which relies on spatial averaging of history variables, exhibits sensitivity with respect to boundary conditions, which requires further studies. 	
0901.3277v1	http://arxiv.org/pdf/0901.3277v1	2009	Size effects in statistical fracture	Mikko J. Alava|Phani K. V. V. Nukala|Stefano Zapperi	  We review statistical theories and numerical methods employed to consider the sample size dependence of the failure strength distribution of disordered materials. We first overview the analytical predictions of extreme value statistics and fiber bundle models and discuss their limitations. Next, we review energetic and geometric approaches to fracture size effects for specimens with a flaw. Finally, we overview the numerical simulations of lattice models and compare with theoretical models. 	
0906.3832v1	http://arxiv.org/pdf/0906.3832v1	2009	Hardware Trojan by Hot Carrier Injection	Y. Shiyanovskii|F. Wolff|C. Papachristou|D. Weyer|W. Clay	  This paper discusses how hot carrier injection (HCI) can be exploited to create a trojan that will cause hardware failures. The trojan is produced not via additional logic circuitry but by controlled scenarios that maximize and accelerate the HCI effect in transistors. These scenarios range from manipulating the manufacturing process to varying the internal voltage distribution. This new type of trojan is difficult to test due to its gradual hardware degradation mechanism. This paper describes the HCI effect, detection techniques and discusses the possibility for maliciously induced HCI trojans. 	
1001.4103v1	http://arxiv.org/pdf/1001.4103v1	2010	The Failure of the Ergodic Assumption	M. Ignaccolo|M. Latka|B. J. West	  The well established procedure of constructing phenomenological ensemble from a single long time series is investigated. It is determined that a time series generated by a simple Uhlenbeck-Ornstein Langevin equation is mean ergodic. However the probability ensemble average yields a variance that is different from that determined using the phenomenological ensemble (time average). We conclude that the latter ensemble is often neither stationary nor ergodic and consequently the probability ensemble averages can misrepresent the underlying dynamic process. 	
1003.2191v1	http://arxiv.org/pdf/1003.2191v1	2010	Spectral matrix methods for partitioning power grids: Applications to   the Italian and Floridian high-voltage networks	Ibrahim Abou Hamad|Brett Israels|Per Arne Rikvold|Svetlana V. Poroseva	  Intentional islanding is used to limit cascading power failures by isolating highly connected "islands" with local generating capacity. To efficiently isolate an island, one should break as few power lines as possible. This is a graph partitioning problem, and here we give preliminary results on islanding of the Italian and Floridian high-voltage grids by spectral matrix methods. 	
1005.5448v1	http://arxiv.org/pdf/1005.5448v1	2010	Failover in cellular automata	Shailesh Kumar|Shrisha Rao	  A cellular automata (CA) configuration is constructed that exhibits emergent failover. The configuration is based on standard Game of Life rules. Gliders and glider-guns form the core messaging structure in the configuration. The blinker is represented as the basic computational unit, and it is shown how it can be recreated in case of a failure. Stateless failover using primary-backup mechanism is demonstrated. The details of the CA components used in the configuration and its working are described, and a simulation of the complete configuration is also presented. 	
1009.1268v1	http://arxiv.org/pdf/1009.1268v1	2010	Prediction of the collapse point of overloaded materials by monitoring   energy emissions	Srutarshi Pradhan|Per C. Hemmer	  A bundle of many fibers with stochastically distributed breaking thresholds is considered as a model of composite materials. The fibers are assumed to share the load equally, and to obey Hookean elasticity up to the breaking point. The bundle is slightly overloaded, which leads to complete failure. We study the properties of emission bursts in which an amount of energy $E$ is released. The analysis shows that the size of the energy bursts has a minimum when the system is half-way from the collapse point. 	
1009.3429v3	http://arxiv.org/pdf/1009.3429v3	2011	Semantics of Typed Lambda-Calculus with Constructors	Barbara Petit	  We present a Curry-style second-order type system with union and intersection types for the lambda-calculus with constructors of Arbiser, Miquel and Rios, an extension of lambda-calculus with a pattern matching mechanism for variadic constructors. We then prove the strong normalisation and the absence of match failure for a restriction of this system, by adapting the standard reducibility method. 	
1009.4127v1	http://arxiv.org/pdf/1009.4127v1	2010	Piezonuclear Reactions	Fabio Cardone|Roberto Mignani|Andrea Petrucci	  In this paper, we deal with the subject of piezonuclear reactions, namely nuclear reactions (of new type) triggered by pressure waves. We discuss the experimental evidences obtained in the last two decades, which can be summarized essentially as follows: experiments in cavitation of liquids, where transmutation of elements, creation of elements and emission of neutrons have been observed; emission of neutrons in brittle failure of solids subjected to mechanical pressure; alteration of the lifetime of un unstable element (thorium) subjected to cavitation. A theoretical model to explain these facts is proposed. Future perspectives of these experimental and theoretical investigations are also underlined. 	
1103.1288v2	http://arxiv.org/pdf/1103.1288v2	2011	A damage-plasticity model for the dynamic failure of concrete	Peter Grassl|Ulrika Nystrom|Rasmus Rempling|Kent Gylltoft	  A constitutive model based on the combination of damage mechanics and plasticity is developed to analyse concrete structures subjected to dynamic loading. The aim is to obtain a model, which requires input parameters with clear physical meanings. The model should describe the important characteristics of concrete subjected to multiaxial and rate-depending loading. This is achieved by combining an effective stress based plasticity model with an isotropic damage model based on plastic and elastic strain measures. The model response in tension, uni-, bi- and tri-axial compression is compared to experimental results in the literature. 	
1105.0379v1	http://arxiv.org/pdf/1105.0379v1	2011	Self-Repairing Codes for Distributed Storage - A Projective Geometric   Construction	Frederique Oggier|Anwitaman Datta	  Self-Repairing Codes (SRC) are codes designed to suit the need of coding for distributed networked storage: they not only allow stored data to be recovered even in the presence of node failures, they also provide a repair mechanism where as little as two live nodes can be contacted to regenerate the data of a failed node. In this paper, we propose a new instance of self-repairing codes, based on constructions of spreads coming from projective geometry. We study some of their properties to demonstrate the suitability of these codes for distributed networked storage. 	
1106.0386v1	http://arxiv.org/pdf/1106.0386v1	2011	Cubic to hexagonal iron phase transition promoted by interstitial   hydrogen	A. Castedo|J. Sanchez|J. Fullea|M. C. Andrade|P. L. de Andres	  Using ab-initio density functional theory we study the role of interstitial hydrogen on the energetics of the phase transformation of iron from bcc to hcp along Bain's pathway. The impurity creates an internal stress field that can be released through a tetragonal distortion of the lattice, promoting the bcc (ferromagnetic) $\rightarrow$ fcc (frustrated antiferromagnetic) $\rightarrow$ hcp (ferromagnetic) transition. The transformation between crystal systems is accompanied by a drastic magnetic reorganization and sudden variations of the unit cell volume, that can be one of the reasons for embrittlement and mechanical failure of iron upon hydrogen adsorption. 	
1111.1091v1	http://arxiv.org/pdf/1111.1091v1	2011	On the merit of a Central Limit Theorem-based approximation in   statistical physics	Bruno Leggio|Oleg Lychkovskiy|Antonino Messina	  The applicability conditions of a recently reported Central Limit Theorem-based approximation method in statistical physics are investigated and rigorously determined. The failure of this method at low and intermediate temperature is proved as well as its inadequacy to disclose quantum criticalities at fixed temperatures. Its high temperature predictions are in addition shown to coincide with those stemming from straightforward appropriate expansions up to (k_B T)^(-2). Our results are clearly illustrated by comparing the exact and approximate temperature dependence of the free energy of some exemplary physical systems. 	
1111.1826v1	http://arxiv.org/pdf/1111.1826v1	2011	Monitoring Software Reliability using Statistical Process control: An   MMLE approach	R. Satya Prasad|Bandla Sreenivasa Rao|R. R. L. Kantam	  This paper consider an MMLE (Modified Maximum Likelihood Estimation) based scheme to estimate software reliability using exponential distribution. The MMLE is one of the generalized frameworks of software reliability models of Non Homogeneous Poisson Processes (NHPPs). The MMLE gives analytical estimators rather than an iterative approximation to estimate the parameters. In this paper we proposed SPC (Statistical Process Control) Charts mechanism to determine the software quality using inter failure times data. The Control charts can be used to measure whether the software process is statistically under control or not. 	
1204.2183v2	http://arxiv.org/pdf/1204.2183v2	2012	Atomic Mechanism of Flow in Simple Liquids under Shear	Takuya Iwashita|Takeshi Egami	  Atomic correlations in a simple liquid in steady-state flow under shear stress were studied by molecular dynamics simulation. The local atomic level strain was determined through the anisotropic pair-density function (PDF). The atomic level strain has a limited spatial extension whose range is dependent on the strain rate and extrapolates to zero at the critical strain rate. A failure event is identified with altering the local topology of atomic connectivity by exchanging bonds among neighboring atoms. 	
1204.6098v1	http://arxiv.org/pdf/1204.6098v1	2012	On Locality in Distributed Storage Systems	Ankit Singh Rawat|Sriram Vishwanath	  This paper studies the design of codes for distributed storage systems (DSS) that enable local repair in the event of node failure. This paper presents locally repairable codes based on low degree multivariate polynomials. Its code construction mechanism extends work on Noisy Interpolating Set by Dvir et al. \cite{dvir2011}. The paper presents two classes of codes that allow node repair to be performed by contacting 2 and 3 surviving nodes respectively. It further shows that both classes are good in terms of their rate and minimum distance, and allow their rate to be bartered for greater flexibility in the repair process. 	
1205.4922v2	http://arxiv.org/pdf/1205.4922v2	2013	On "Novel attractive forces" between ions in quantum plasmas -- failure   of linearized quantum hydrodynamics	M. Bonitz|E. Pehlke|T. Schoof	  In a recent letter [P.K. Shukla and B. Eliasson, Phys. Rev. Lett. 108, 165007 (2012)] the discovery of a new attractive force between protons in a hydrogen plasma was reported that would be responsible for the formation of molecules and of a proton lattice. Here we show, based on ab initio density functional theory calculations, that these predictions are wrong and caused by using linearized quantum hydrodynamics beyond the limits of its applicability. 	
1210.7982v1	http://arxiv.org/pdf/1210.7982v1	2012	Derivation of the Johnson-Samwer $T^{(2/3)}$ Temperature Dependence of   the Yield Strain in Metallic Glasses	Ratul Dasgupta|Ashwin Joy|H. G. E. Hentschel|Itamar Procaccia	  Metallic Glasses are prone to fail mechanically via a shear-banding instability. In a remarkable paper Johnson and Samwer demonstrated that this failure enjoys a high degree of universality in the sense that a large group of metallic glasses appears to possess a yield-strain that decreases with temperature following a $-T^{2/3}$ law up to logarithmic corrections. In this Letter we offer a theoretical derivation of this law. We show that our formula fits very well simulational data on typical amorphous solids. 	
1212.5292v2	http://arxiv.org/pdf/1212.5292v2	2015	Low-Temperature Magnetization Dynamics of Magnetic Molecular Solids in a   Swept Field	Erik Lenferink|Avinash Vijayaraghavan|Anupam Garg	  The swept-field experiments on magnetic molecular solids such as \Fe8 are studied using Monte Carlo simulations. A kinetic equation is developed to understand the phenomenon. It is found that the simulations provide a quantitatively accurate account of the experiments. The kinetic equation provides a similarly accurate account except at very low sweep velocities, where it fails modestly. This failure is due to the neglect of short-range correlations between the dipolar magnetic fields seen by the molecular spins. Both the simulations and the kinetic equation provide a good understanding of the distribution of these dipolar fields. 	
1301.0605v1	http://arxiv.org/pdf/1301.0605v1	2012	Loopy Belief Propogation and Gibbs Measures	Sekhar Tatikonda|Michael I. Jordan	  We address the question of convergence in the loopy belief propagation (LBP) algorithm. Specifically, we relate convergence of LBP to the existence of a weak limit for a sequence of Gibbs measures defined on the LBP s associated computation tree.Using tools FROM the theory OF Gibbs measures we develop easily testable sufficient conditions FOR convergence.The failure OF convergence OF LBP implies the existence OF multiple phases FOR the associated Gibbs specification.These results give new insight INTO the mechanics OF the algorithm. 	
1301.6331v1	http://arxiv.org/pdf/1301.6331v1	2013	Optimal Locally Repairable Codes via Rank-Metric Codes	Natalia Silberstein|Ankit Singh Rawat|O. Ozan Koyluoglu|Sriram Vishwanath	  This paper presents a new explicit construction for locally repairable codes (LRCs) for distributed storage systems which possess all-symbols locality and maximal possible minimum distance, or equivalently, can tolerate the maximal number of node failures. This construction, based on maximum rank distance (MRD) Gabidulin codes, provides new optimal vector and scalar LRCs. In addition, the paper also discusses mechanisms by which codes obtained using this construction can be used to construct LRCs with efficient repair of failed nodes by combination of LRC with regenerating codes. 	
1302.0345v1	http://arxiv.org/pdf/1302.0345v1	2013	On the single-electron theory of quantum spin Hall effect in two   dimensional topological insulators	Yi-Dong Wu	  Recently we wrote a paper on the theory of the quantum spin Hall effect(QSHE) in two dimensional(2D) topological insulators(TIs)1 which have been considered as do not add much new insight to the exhaustively studied topic of TI within a single-electron picture by the referees. In this paper we review the papers on the mechanism of the QSHE which have significant influence on understanding of the subject. By illustrating the failures of the previous works we show our paper do contribute a different point of view to this topic, which we believe is not only a new but also the correct way to approach the problem at the single-electron level. 	
1308.2519v1	http://arxiv.org/pdf/1308.2519v1	2013	Damage accumulation in quasi-brittle fracture	Claudio Manzato|Mikko J. Alava|Stefano Zapperi	  The strength of quasi-brittle materials depends on the ensemble of defects inside the sample and on the way damage accumulates before failure. Using large scale numerical simulations of the random fuse model, we investigate the evolution of the microcrack distribution that is directly related to the strength distribution and its size effects. We show that the broadening of the distribution tail originates from the dominating microcracks in each sample and is related to a tendency of crack coalescence that increases with system size. We study how the observed behavior depends on the disorder present in the sample. 	
1404.0944v1	http://arxiv.org/pdf/1404.0944v1	2014	Negative Ion Sources: Magnetron and Penning	D. C. Faircloth	  The history of the magnetron and Penning electrode geometry is briefly outlined. Plasma generation by electrical discharge-driven electron impact ionization is described and the basic physics of plasma and electrodes relevant to magnetron and Penning discharges are explained. Negative ions and their applications are introduced, along with their production mechanisms. Caesium and surface production of negative ions are detailed. Technical details of how to build magnetron and Penning surface plasma sources are given, along with examples of specific sources from around the world. Failure modes are listed and lifetimes compared. 	
1408.4052v1	http://arxiv.org/pdf/1408.4052v1	2014	QCD. What else is needed for the Proton Structure Function?	Y. S. Kim	  While QCD can provide corrections to the parton distribution function, it cannot produce the distribution. Where is then the starting point for the proton structure function? The only known source is the quark-model wave function for the proton at rest. The harmonic oscillator is used for the trial wave function. When Lorentz-boosted, this wave function exhibits all the peculiarities of Feynman's parton picture. The time-separation between the quarks plays the key role in the boosting process. This variable is hidden in the present form of quantum mechanics, and the failure to measure it leads to an increase in entropy. This leads to a picture of boiling quarks which become partons in their plasma state. 	
1502.01237v1	http://arxiv.org/pdf/1502.01237v1	2015	Running Identical Threads in C-Slow Retiming based Designs for   Functional Failure Detection	Tobias Strauch	  This paper shows the usage of C-Slow Retiming (CSR) in safety critical and low power applications. CSR generates C copies of a design by reusing the given logic resources in a time sliced fashion. When all C design copies are stimulated with the same input values, then all C design copies should behave the same way and will therefore create a redundant system. The paper shows that this special method of using CSR offers great benefits when used in safety critical and low power applications. Additional optimization techniques towards reducing register count are shown and an on-the-fly recovery mechanism is discussed. 	
1504.06274v1	http://arxiv.org/pdf/1504.06274v1	2015	A new approach for physiological time series	Dong Mao|Yang Wang|Qiang Wu	  We developed a new approach for the analysis of physiological time series. An iterative convolution filter is used to decompose the time series into various components. Statistics of these components are extracted as features to characterize the mechanisms underlying the time series. Motivated by the studies that show many normal physiological systems involve irregularity while the decrease of irregularity usually implies the abnormality, the statistics for "outliers" in the components are used as features measuring irregularity. Support vector machines are used to select the most relevant features that are able to differentiate the time series from normal and abnormal systems. This new approach is successfully used in the study of congestive heart failure by heart beat interval time series. 	
1506.01508v1	http://arxiv.org/pdf/1506.01508v1	2015	Escaping the Tragedy of the Commons through Targeted Punishment	Samuel Johnson	  Failures of cooperation cause many of society's gravest problems. It is well known that cooperation among many players faced with a social dilemma can be maintained thanks to the possibility of punishment, but achieving the initial state of widespread cooperation is often much more difficult. We show here that there exist strategies of `targeted punishment' whereby a small number of punishers can shift a population of defectors into a state of global cooperation. The heterogeneity of players, often regarded as an obstacle, can in fact boost the mechanism's effectivity. We conclude by outlining how the international community could use a strategy of this kind to combat climate change. 	
1509.01670v1	http://arxiv.org/pdf/1509.01670v1	2015	Mesoscopic description of random walks on combs	Vicenc Mendez|Alexander Iomin|Daniel Campos|Werner Horsthemke	  Combs are a simple caricature of various types of natural branched structures, which belong to the category of loopless graphs and consist of a backbone and branches. We study continuous time random walks on combs and present a generic method to obtain their transport properties. The random walk along the branches may be biased, and we account for the effect of the branches by renormalizing the waiting time probability distribution function for the motion along the backbone. We analyze the overall diffusion properties along the backbone and find normal diffusion, anomalous diffusion, and stochastic localization (diffusion failure), respectively, depending on the characteristics of the continuous time random walk along the branches. 	
1510.03718v1	http://arxiv.org/pdf/1510.03718v1	2015	Elastic interactions between 2D geometric defects	Michael Moshe|Eran Sharon|Raz Kupferman	  In this paper, we introduce a methodology applicable to a wide range of localized two-dimensional sources of stress. This methodology is based on a geometric formulation of elasticity. Localized sources of stress are viewed as singular defects---point charges of the curvature associated with a reference metric. The stress field in the presence of defects can be solved using a scalar stress function that generalizes the classical Airy stress function to the case of materials with nontrivial geometry. This approach allows the calculation of interaction energies between various types of defects. We apply our methodology to two physical systems: shear-induced failure of amorphous materials and the mechanical interaction between contracting cells. 	
1510.07148v1	http://arxiv.org/pdf/1510.07148v1	2015	Mobility and Energy Conscious Clustering Protocol for Wireless Networks	Abhinav Singh|Awadhesh Kumar Singh	  In this paper we present a distributed clustering protocol for mobile wireless sensor networks. A large majority of research in clustering and routing algorithms for WSNs assume a static network and hence are rendered inefficient in cases of highly mobile sensor networks, which is an aspect addressed here. MECP is an energy efficient, mobility aware protocol and utilizes information about movement of sensor nodes and residual energy as attributes in network formation. It also provides a mechanism for fault tolerance to decrease packet data loss in case of cluster head failures. 	
1511.05705v1	http://arxiv.org/pdf/1511.05705v1	2015	Continuous Wire Reinforcement for Jammed Granular Architecture	Matthias Fauconneau|Falk K. Wittel|Hans J. Herrmann	  The mechanical behavior of continuous fiber reinforced granular columns is simulated by means of a Discrete Element Model. Spherical particles are randomly deposited simultaneously with a wire, that is deployed following different patterns inside of a flexible cylinder for triaxial compression testing. We quantify the effect of three different fiber deployment patterns on the failure envelope, represented by Mohr-Coulomb cones, and derive suggestions for improved deployment strategies. 	
1601.05167v1	http://arxiv.org/pdf/1601.05167v1	2016	Breakdown of the Isobaric Multiplet Mass Equation as An Effect of the   Isospin-Symmetry Breaking	J. M. Dong|W. Zuo|J. Z. Gu	  The breakdown of the quadratic form of isobaric multiplet mass equation (IMME), presents a long-standing challenge to the existing theoretical models. In particular, recent high-precision nuclear mass measurements have indicated a dramatic failure of the IMME for several isobaric multiplets. We propose a new mechanism that the isospin-projection $T_z$ dependence of the 1st-order symmetry energy coefficient (SEC) drives a significant breakdown of the IMME, where the 1st-order SEC is primarily induced by the isospin-symmetry breaking (ISB) of strong nuclear force. Completely different from the existing knowledge, the deviation from the IMME cannot be measured simply by the high-order terms such as cubic term $dT_{z}^3$. 	
1602.01240v1	http://arxiv.org/pdf/1602.01240v1	2016	On the apparent failure of the topological theory of phase transitions	Matteo Gori|Roberto Franzosi|Marco Pettini	  The topological theory of phase transitions has its strong point in two theorems proving that, for a wide class of physical systems, phase transitions necessarily stem from topological changes of some submanifolds of configuration space. It has been recently argued that the $2D$ lattice $\phi^4$-model provides a counterexample that falsifies this theory. It is here shown that this is not the case: the phase transition of this model stems from an asymptotic ($N\to\infty$) change of topology of the energy level sets, in spite of the absence of critical points of the potential in correspondence of the transition energy. 	
1603.08898v2	http://arxiv.org/pdf/1603.08898v2	2016	Fracture initiation in multi-phase materials: a statistical   characterization of microstructural damage sites	T. W. J. de Geus|J. E. P. van Duuren|R. H. J. Peerlings|M. G. D. Geers	  Understanding the microstructural influence on the failure mechanisms in multi-phase materials calls for the identification of the worst-case scenario. This necessitates a statistical approach. By performing simulations directly based on micrographs, such an approach becomes feasible. This is applied here to extract the average microstructure around damage sites. 	
1605.07734v1	http://arxiv.org/pdf/1605.07734v1	2016	Recursive SDN for Carrier Networks	James McCauley|Zhi Liu|Aurojit Panda|Teemu Koponen|Barath Raghavan|Jennifer Rexford|Scott Shenker	  Control planes for global carrier networks should be programmable (so that new functionality can be easily introduced) and scalable (so they can handle the numerical scale and geographic scope of these networks). Neither traditional control planes nor new SDN-based control planes meet both of these goals. In this paper, we propose a framework for recursive routing computations that combines the best of SDN (programmability) and traditional networks (scalability through hierarchy) to achieve these two desired properties. Through simulation on graphs of up to 10,000 nodes, we evaluate our design's ability to support a variety of routing and traffic engineering solutions, while incorporating a fast failure recovery mechanism. 	
1606.00111v2	http://arxiv.org/pdf/1606.00111v2	2016	It's Time: OS Mechanisms for Enforcing Asymmetric Temporal Integrity	Anna Lyons|Gernot Heiser	  Mixed-criticality systems combine real-time components of different levels of criticality, i.e. severity of failure, on the same processor, in order to obtain good resource utilisation. They must guarantee deadlines of highly-critical tasks at the expense of lower-criticality ones in the case of overload. Present operating systems provide inadequate support for this kind of system, which is of growing importance in avionics and other verticals. We present an approach that provides the required asymmetric integrity and its implementation in the high-assurance seL4 microkernel. 	
1612.08554v1	http://arxiv.org/pdf/1612.08554v1	2016	A Fidelity Susceptibility Approach to Quantum Annealing of NP-hard   problems	Jun Takahashi|Koji Hukushima	  The computational complexity conjecture of NP $\nsubseteq$ BQP implies that there should be an exponentially small energy gap for Quantum Annealing (QA) of NP-hard problems. We aim to verify how this computation originated gapless point could be understood based on physics, using the quantum Monte Carlo method. As a result, we found a phase transition detectable only by the divergence of fidelity susceptibility. The exponentially small gapless points of each instance are all located in the phase found in this study, which suggests that this phase transition is the physical cause of the failure of QA for NP-hard problems. 	
1701.01234v4	http://arxiv.org/pdf/1701.01234v4	2017	Failure of deterministic and stochastic thermostats to control   temperature of molecular systems	Hiroshi Watanabe	  We investigate the ergodicity and "hot solvent/cold solute" problems in molecular dynamics simulations. While the kinetic moments and the stimulated Nos\'e--Hoover methods improve the ergodicity of a harmonic-oscillator system, both methods exhibit the "hot solvent/cold solute" problem in a binary liquid system. These results show that the devices to improve the ergodicity do not resolve the "hot solvent/cold solute" problem. 	
1712.07662v1	http://arxiv.org/pdf/1712.07662v1	2017	Bosonization in Non-Relativistic CFTs	Carl Turner	  We demonstrate explicitly the correspondence between all protected operators in a 2+1 dimensional non-supersymmetric bosonization duality in the non-relativistic limit. Roughly speaking we consider $SU(N)$ Chern-Simons field theory at level $k$ with $N_f$ flavours of fundamental boson, and match its chiral sector to that of a $SU(k)$ theory at level $N$ with $N_f$ fundamental fermions. We present the matching at the level of indices and individual operators, seeing the mechanism of failure for $N_f > N$, and point out that the non-relativistic setting is a particularly friendly setting for studying interesting questions about such dualities. 	
1802.01083v2	http://arxiv.org/pdf/1802.01083v2	2018	Mesoscopic Description of the Equal Load Sharing Fiber Bundle Model	Martin Hendrick|Srutarshi Pradhan|Alex Hansen	  One aim of the equal load sharing fiber bundle model is to describe the critical behavior of failure events. One way of accomplishing this, is through a discrete recursive dynamics. We introduce a continuous mesoscopic equation catching the critical behavior found through recursive dynamics. It allows us to formulate the model using the unifying framework of absorbing phase transitions traditionally used in the study of non-equilibrium phase transitions. Consequently, this work is a first step towards a field theory for fiber bundle models. 	
1803.01110v1	http://arxiv.org/pdf/1803.01110v1	2018	Nickel Titanium Alloy failure analysis under thermal cycling and   mechanical Loading: A Preliminary Study	Mahdi Mohajeri|Behrouz Haghgouyan|Homero Castaneda|Dimitris C. Lagoudas	  The electrochemical frequency modulation (EFM) technique can consider as a new tool for electrochemical corrosion monitoring. The calculation of corrosion rate with a non-destructive and rapid technique is a necessity to study corrosion behavior of metals under loading and thermal cycling. NiTi shape memory alloy (SMA) is characterized by differential scanning calorimetry (DSC) and uniaxial tensile testing. The corrosion behavior and reliability of technique have been examined for NiTi sample in artificial physiological solution. The results show the sensitivity of EFM technique to temperature and base frequencies. 	
0001425v1	http://arxiv.org/pdf/cond-mat/0001425v1	2000	The critical earthquake concept applied to mine rockbursts with   time-to-failure analysis	G. Ouillon|D. Sornette	  We report new tests of the critical earthquake concepts performed on rockbursts in deep South African mines. We extend the concept of an optimal time and space correlation region and test it on the eight main shocks of our catalog provided by ISSI. In a first test, we use the simplest signature of criticality in terms of a power law time-to-failure formula. Notwithstanding the fact that the search for the optimal correlation size is performed with this simple power law, we find evidence both for accelerated seismicity and for the presence of logperiodic behavior with a prefered scaling factor close to 2. We then propose a new algorithm based on a space and time smoothing procedure, which is also intended to account for the finite range and time mechanical interactions between events. This new algorithm provides a much more robust and efficient construction of the optimal correlation region, which allows us the use of the logperiodic formula directly in the search process. In this preliminary work, we have only tested the new algorithm on the largest event on the catalog. The result is of remarkable good quality with a dramatic improvement in accuracy and robustness. This confirms the potential importance of logperiodic signals. Our study opens the road for an efficient implemention of a systematic testing procedure of real-time predictions. 	
0402557v2	http://arxiv.org/pdf/cond-mat/0402557v2	2004	A Biased Resistor Network Model for Electromigration Failure and Related   Phenomena in Metallic Lines	C. Pennetta|E. Alfinito|L. Reggiani|F. Fantini|I. DeMunari|A. Scorzoni	  Electromigration phenomena in metallic lines are studied by using a biased resistor network model. The void formation induced by the electron wind is simulated by a stochastic process of resistor breaking, while the growth of mechanical stress inside the line is described by an antagonist process of recovery of the broken resistors. The model accounts for the existence of temperature gradients due to current crowding and Joule heating. Alloying effects are also accounted for. Monte Carlo simulations allow the study within a unified theoretical framework of a variety of relevant features related to the electromigration. The predictions of the model are in excellent agreement with the experiments and in particular with the degradation towards electrical breakdown of stressed Al-Cu thin metallic lines. Detailed investigations refer to the damage pattern, the distribution of the times to failure (TTFs), the generalized Black's law, the time evolution of the resistance, including the early-stage change due to alloying effects and the electromigration saturation appearing at low current densities or for short line lengths. The dependence of the TTFs on the length and width of the metallic line is also well reproduced. Finally, the model successfully describes the resistance noise properties under steady state conditions. 	
1006.3724v1	http://arxiv.org/pdf/1006.3724v1	2010	A Peer-to-Peer Middleware Framework for Resilient Persistent Programming	Alan Dearle|Graham Kirby|Stuart Norcross|Andrew McCarthy	  The persistent programming systems of the 1980s offered a programming model that integrated computation and long-term storage. In these systems, reliable applications could be engineered without requiring the programmer to write translation code to manage the transfer of data to and from non-volatile storage. More importantly, it simplified the programmer's conceptual model of an application, and avoided the many coherency problems that result from multiple cached copies of the same information. Although technically innovative, persistent languages were not widely adopted, perhaps due in part to their closed-world model. Each persistent store was located on a single host, and there were no flexible mechanisms for communication or transfer of data between separate stores. Here we re-open the work on persistence and combine it with modern peer-to-peer techniques in order to provide support for orthogonal persistence in resilient and potentially long-running distributed applications. Our vision is of an infrastructure within which an application can be developed and distributed with minimal modification, whereupon the application becomes resilient to certain failure modes. If a node, or the connection to it, fails during execution of the application, the objects are re-instantiated from distributed replicas, without their reference holders being aware of the failure. Furthermore, we believe that this can be achieved within a spectrum of application programmer intervention, ranging from minimal to totally prescriptive, as desired. The same mechanisms encompass an orthogonally persistent programming model. We outline our approach to implementing this vision, and describe current progress. 	
1109.3561v1	http://arxiv.org/pdf/1109.3561v1	2011	Universal adaptive self-stabilizing traversal scheme: random walk and   reloading wave	Thibault Bernard|Alain Bui|Devan Sohier	  In this paper, we investigate random walk based token circulation in dynamic environments subject to failures. We describe hypotheses on the dynamic environment that allow random walks to meet the important property that the token visits any node infinitely often. The randomness of this scheme allows it to work on any topology, and require no adaptation after a topological change, which is a desirable property for applications to dynamic systems. For random walks to be a traversal scheme and to answer the concurrence problem, one needs to guarantee that exactly one token circulates in the system. In the presence of transient failures, configurations with multiple tokens or with no token can occur. The meeting property of random walks solves the cases with multiple tokens. The reloading wave mechanism we propose, together with timeouts, allows to detect and solve cases with no token. This traversal scheme is self-stabilizing, and universal, meaning that it needs no assumption on the system topology. We describe conditions on the dynamicity (with a local detection criterion) under which the algorithm is tolerant to dynamic reconfigurations. We conclude by a study on the time between two visits of the token to a node, which we use to tune the parameters of the reloading wave mechanism according to some system characteristics. 	
1305.0989v1	http://arxiv.org/pdf/1305.0989v1	2013	The Dynamics of Rapid Fracture: Instabilities, Nonlinearities and Length   Scales	Eran Bouchbinder|Tamar Goldman|Jay Fineberg	  The failure of materials and interfaces is mediated by cracks, nearly singular dissipative structures that propagate at velocities approaching the speed of sound. Crack initiation and subsequent propagation -- the dynamic process of fracture -- couples a wide range of time and length scales. Crack dynamics challenge our understanding of the fundamental physics processes that take place in the extreme conditions within the nearly singular region where material failure occurs. Here, we first briefly review the classic approach to dynamic fracture, "Linear Elastic Fracture Mechanics" (LEFM), and discuss its successes and limitations. We show how, on the one hand, recent experiments performed on straight cracks propagating in soft brittle materials have quantitatively confirmed the predictions of this theory to an unprecedented degree. On the other hand, these experiments show how LEFM breaks down as the singular region at the tip of a crack is approached. This breakdown naturally leads to a new theoretical framework coined "Weakly Nonlinear Fracture Mechanics", where weak elastic nonlinearities are incorporated. The stronger singularity predicted by this theory gives rise to a new and intrinsic length scale, $\ell_{nl}$. These predictions are verified in detail through direct measurements. We then theoretically and experimentally review how the emergence of $\ell_{nl}$ is linked to a new equation for crack motion, which predicts the existence of a high-speed oscillatory crack instability whose wave-length is determined by $\ell_{nl}$. We conclude by delineating outstanding challenges in the field. 	
1406.2087v1	http://arxiv.org/pdf/1406.2087v1	2014	Mechanical Properties and Plasticity of a Model Glass Loaded Under   Stress Control	Vladimir Dailidonis|Valery Ilyin|Pankaj Mishra|Itamar Procaccia	  Much of the progress achieved in understanding plasticity and failure in amorphous solids had been achieved using experiments and simulations in which the materials were loaded using strain control. There is paucity of results under stress control. Here we present a new method that was carefully geared to allow loading under stress control either at $T=0$ or at any other temperature, using Monte-Carlo techniques. The method is applied to a model perfect crystalline solid, to a crystalline solid contaminated with topological defects, and to a generic glass. The highest yield stress belongs to the crystal, the lowest to the crystal with a few defects, with the glass in between. Although the glass is more disordered than the crystal with a few defects, it yields stress is much higher than that of the latter. We explain this fact   by considering the actual microscopic interactions that are typical to glass forming materials, pointing out the reasons for the higher cohesive nature of the glass. The main conclusion of this paper is that the instabilities encountered in stress-control condition are the identical saddle-node bifurcation seen in strain-control. Accordingly one can use the latter condition to infer about the former. Finally we discuss temperature effects and comment on the time needed to see a stress controlled material failure. 	
1603.01168v1	http://arxiv.org/pdf/1603.01168v1	2016	Numerical simulation of wave propagation and snow failure from explosive   loading	Rolf Sidler|Stephan Simioni|Jürg Dual|Jürg Schweizer	  Avalanche control by explosion is a widely applied method to minimize the avalanche risk to infrastructure in snow-covered mountain areas. However, the mechanisms involved leading from an explosion to the release of an avalanche are not well understood. Here we test the hypothesis that weak layers fail due to the stress caused by propagating acoustic waves. The underlying mechanism is that the stress induced by the acoustic waves exceeds the strength of the snow layers. We compare field measurements to a numerical simulation of acoustic wave propagation in a porous material. The simulation consists of an acoustic domain for the air above the snowpack and a poroelastic domain for the dry snowpack. The two domains are connected by a wave field decomposition and open pore boundary conditions. Empirical relations are used to derive a porous model of the snowpack from density profiles of the field experiment. Biot's equations are solved in the poroelastic domain to obtain simulated accelerations in the snowpack and a time dependent stress field. Locations of snow failure were identified by comparing the principal normal and shear stress fields to snow strength which is assumed to be a function of snow porosity. One air pressure measurement above the snowpack was used to calibrate the pressure amplitude of the source in the simulation. Additional field measurements of air pressure and acceleration measurements inside the snowpack were compared to individual field variables of the simulation. The acceleration of the air flowing inside the pore space of the snowpack was identified to have the highest correlation to the acceleration measurements in the snowpack. 	
1603.08910v2	http://arxiv.org/pdf/1603.08910v2	2016	Fracture in multi-phase materials: why some microstructures are more   critical than others	T. W. J. de Geus|R. H. J. Peerlings|M. G. D. Geers	  Our goal is to unravel the mechanisms that lead to failure of a ductile two-phase material - that consists of a ductile soft phase and a relatively brittle hard phase. An idealized microstructural model is used to study damage propagation systematically and transparently. The analysis uncovers distinct microstructural features around early voids, whereby regions of the hard phase are aligned with the tensile axis and regions of the soft phase are aligned with the shear directions. These features are consistently found in regions exhibiting damage propagation, whereby the damage remains initiation driven, i.e. voids nucleate independently of each other. Upon localization, damage is controlled on a longer length-scale relying on a critical relative position of 'initiation hot-spots'. The damage rapidly increases in bands of the soft phase wherein several voids are aligned with the shear directions. The relative arrangement of the voids determines whether the microstructure fails early, or at a substantially higher strain. Although much research is needed to refine these findings for real or more realistic microstructures, in particular in three-dimensions, this paper opens a route to a deeper understanding of the ultimate failure of multi-phase materials. 	
1702.01963v1	http://arxiv.org/pdf/1702.01963v1	2017	Seamless Handover in IP over ICN Networks: a Coding Approach	Mohammed Al-Khalidi|Nikolaos Thomos|Martin J. Reed|Mays F. AL-Naday|Dirk Trossen	  Seamless connectivity plays a key role in realising QoS-based delivery in mobile networks. However, current handover mechanisms hinder the ability to meet this target, due to the high ratio of handover failures, packet loss and service interruption. These challenges are further magnified in Heterogeneous Cellular Networks (HCN) such as Advanced Long Term Evolution (LTE-Advanced) and LTE in unlicensed spectrum (LTE-LAA), due to the variation in handover requirements. Although mechanisms, such as Fast Handover for Proxy Mobile IPv6 (PFMIPv6), attempt to tackle these issues; they come at a high cost with sub-optimal outcomes. This primarily stems from various limitations of existing IP core networks. In this paper we propose a novel handover solution for mobile networks, exploiting the advantages of a revolutionary IP over Information-Centric Networking (IP-over-ICN) architecture in supporting flexible service provisioning through anycast and multicast, combined with the advantages of random linear coding techniques in eliminating the need for retransmissions. Our solution allows coded traffic to be disseminated in a multicast fashion during handover phase from source directly to the destination(s), without the need for an intermediate anchor as in exiting solutions; thereby, overcoming packet loss and handover failures, while reducing overall delivery cost. We evaluate our approach with an analytical and simulation model showing significant cost reduction compared to PFMIPv6. 	
1707.08928v2	http://arxiv.org/pdf/1707.08928v2	2017	Localizing softness and stress along loops in three-dimensional   topological metamaterials	Guido Baardink|Anton Souslov|Jayson Paulose|Vincenzo Vitelli	  Topological states can be used to control the mechanical properties of a material along an edge or around a localized defect. The surface rigidity of elastic networks is characterized by a bulk topological invariant called the polarization; materials with a well-defined uniform polarization display a dramatic range of edge softnesses depending on the orientation of the polarization relative to the terminating surface. However, in all three-dimensional mechanical metamaterials proposed to date, the topological edge modes are mixed with bulk soft modes and so-called Weyl loops. Here, we report the design of a gapped 3D topological metamaterial with a uniform polarization that displays a corresponding asymmetry between the number of soft modes on opposing surfaces and, in addition, no bulk soft modes. We then use this construction to localize topological soft modes in interior regions of the material by including defect structures---dislocation loops---that are unique to three dimensions. We derive a general formula that relates the difference in the number of soft modes and states of self-stress localized along the dislocation loop to the handedness of the vector triad formed by the lattice polarization, Burgers vector, and dislocation-line direction. Our findings suggest a novel strategy for pre-programming failure and softness localized along lines in 3D, while avoiding extended periodic failure modes associated with Weyl loops. 	
1708.07233v1	http://arxiv.org/pdf/1708.07233v1	2017	Reliability and Fault-Tolerance by Choreographic Design	Ian Cassar|Adrian Francalanza|Claudio Antares Mezzina|Emilio Tuosto	  Distributed programs are hard to get right because they are required to be open, scalable, long-running, and tolerant to faults. In particular, the recent approaches to distributed software based on (micro-)services where different services are developed independently by disparate teams exacerbate the problem. In fact, services are meant to be composed together and run in open context where unpredictable behaviours can emerge. This makes it necessary to adopt suitable strategies for monitoring the execution and incorporate recovery and adaptation mechanisms so to make distributed programs more flexible and robust. The typical approach that is currently adopted is to embed such mechanisms in the program logic, which makes it hard to extract, compare and debug. We propose an approach that employs formal abstractions for specifying failure recovery and adaptation strategies. Although implementation agnostic, these abstractions would be amenable to algorithmic synthesis of code, monitoring and tests. We consider message-passing programs (a la Erlang, Go, or MPI) that are gaining momentum both in academia and industry. Our research agenda consists of (1) the definition of formal behavioural models encompassing failures, (2) the specification of the relevant properties of adaptation and recovery strategy, (3) the automatic generation of monitoring, recovery, and adaptation logic in target languages of interest. 	
1802.02258v1	http://arxiv.org/pdf/1802.02258v1	2018	A computational framework for microstructural modelling of   polycrystalline materials with damage and failure	Vincenzo Gulizzi	  In the present thesis, a computational framework for the analysis of the deformation and damage phenomena occurring at the micro-scale of polycrystalline materials is presented.   Micro-mechanics studies are commonly performed using the Finite Element Method (FEM) for its versatility and robustness. However, finite element formulations usually lead to an extremely high number of degrees of freedom of the considered micro-structures, thus making alternative formulations of great engineering interest. Among the others, the Boundary Element Method (BEM) represents a viable alternative to FEM approaches as it allows to express the problem in terms of boundary values only, thus reducing the total number of degrees of freedom.   The computational framework developed in this thesis is based on a non-linear multi-domain BEM approach for generally anisotropic materials and is devoted to the analysis of three-dimensional polycrystalline microstructures. Different theoretical and numerical aspects of the polycrystalline problem using the boundary element method are investigated: first, being the formulation based on a integral representation of the governing equations, a novel and more compact expression of the integration kernels capable of representing the multi-field behaviour of generally anisotropic materials is presented; second, the sources of the high computational cost of polycrystalline analyses are identified and suitably treated by means of different strategies including an ad-hoc grain boundary meshing technique developed to tackle the large statistical variability of polycrystalline micro-morphologies; third, non-linear deformation and failure mechanisms such as inter-granular and trans-granular cracking and generally anisotropic crystal plasticity are studied and the numerical results presented throughout the thesis demonstrate the potential of the developed framework. 	
9908478v1	http://arxiv.org/pdf/cond-mat/9908478v1	1999	Quantitative Analysis of Experimental and Synthetic Microstructures for   Sedimentary Rock	B. Biswal|C. Manwart|R. Hilfer|S. Bakke|P. E. Oren	  A quantitative comparison between the experimental microstructure of a sedimentary rock and three theoretical models for the same rock is presented. The microstructure of the rock sample (Fontainebleau sandstone) was obtained by microtomography. Two of the models are stochastic models based on correlation function reconstruction, and one model is based on sedimentation, compaction and diagenesis combined with input from petrographic analysis. The porosity of all models closely match that of the experimental sample and two models have also the same two point correlation function as the experimental sample. We compute quantitative differences and similarities between the various microstructures by a method based on local porosity theory. Differences are found in the degree of anisotropy, and in fluctuations of porosity and connectivity. The stochastic models differ strongly from the real sandstone in their connectivity properties, and hence need further refinement when used to model transport. 	
9908025v2	http://arxiv.org/pdf/physics/9908025v2	1999	On the Support that the Special and General Theories of Relativity   Provide for Rock's Argument Concerning Induced Self-Motion	D. M. Snyder	  Though Einstein and other physicists recognized the importance of an observer being at rest in an inertial reference frame for the special theory of relativity, the supporting psychological structures were not discussed much by physicists. On the other hand, Rock, a psychologist, wrote of the factors involved in the perception of motion, including one's own motion. Rock thus came to discuss issues of significance to relativity theory, apparently without any significant understanding of how his theory might be related to relativity theory. In this paper, connections between Rock's theory on the perception of one's own motion, as well as empirical work supporting it, and relativity theory are explored. 	
1301.6530v2	http://arxiv.org/pdf/1301.6530v2	2013	Search for magnetic monopoles in polar volcanic rocks	K. Bendtz|D. Milstead|H. -P. Hächler|A. M. Hirt|P. Mermod|P. Michael|T. Sloan|C. Tegner|S. B. Thorarinsson	  For a broad range of values of magnetic monopole mass and charge, the abundance of monopoles trapped inside the Earth would be expected to be enhanced in the mantle beneath the geomagnetic poles. A search for magnetic monopoles was conducted using the signature of an induced persistent current following the passage of igneous rock samples through a SQUID-based magnetometer. A total of 24.6 kg of rocks from various selected sites, among which 23.4 kg are mantle-derived rocks from the Arctic and Antarctic areas, was analysed. No monopoles were found and a 90% confidence level upper limit of $9.8\cdot 10^{-5}$/gram is set on the monopole density in the search samples. 	
1406.2007v2	http://arxiv.org/pdf/1406.2007v2	2014	Geochemistry of silicate-rich rocks can curtail spreading of carbon   dioxide in subsurface aquifers	Silvana S. S. Cardoso|Jeanne T. H. Andres	  Pools of carbon dioxide are found in natural geological accumulations and in engineered storage in saline aquifers. It has been thought that once this CO2 dissolves in the formation water, making it denser, convection streams will transport it efficiently to depth, but this may not be so. Here, we assess theoretically and experimentally the impact of natural chemical reactions between the dissolved CO2 and the rock formation on the convection streams in the subsurface. We show that, while in carbonate rocks the streaming of dissolved carbon dioxide persists, the chemical interactions in silicate-rich rocks may curb this transport drastically and even inhibit it altogether. These results challenge our view of carbon sequestration and dissolution rates in the subsurface, suggesting that pooled carbon dioxide may remain in the shallower regions of the formation for hundreds to thousands of years. The deeper regions of the reservoir can remain virtually carbon free. 	
1509.06879v2	http://arxiv.org/pdf/1509.06879v2	2016	Estimation of low energy neutron flux ($E_n\leq15$ MeV) in India-based   Neutrino Observatory cavern using Monte Carlo techniques	N. Dokania|V. Singh|S. Mathimalar|A. Garai|V. Nanal|R. G. Pillay|K. G. Bhushan	  The neutron flux at low energy ($E_n\leq15$ MeV) resulting from the radioactivity of the rock in the underground cavern of the India-based Neutrino Observatory is estimated using Geant4-based Monte Carlo simulations. The neutron production rate due to the spontaneous fission of $^{235, 238}$U, $^{232}$Th and ($\alpha, n$) interactions in the rock is determined employing the actual rock composition. It is been shown that the total flux is equivalent to a finite size cylindrical rock ($D=L=140$ cm) element. The energy integrated neutron flux thus obtained at the centre of the underground tunnel is 2.76 (0.47) $\times 10^{-6}\rm~n ~cm^{-2}~s^{-1}$. The estimated neutron flux is of the same order ($\sim10^{-6}\rm~n ~cm^{-2}~s^{-1}$)~as measured in other underground laboratories. 	
1511.08004v2	http://arxiv.org/pdf/1511.08004v2	2016	RoCK blocks, wreath products and KLR algebras	Anton Evseev	  We consider RoCK (or Rouquier) blocks of symmetric groups and Hecke algebras at roots of unity. We prove a conjecture of Turner asserting that a certain idempotent truncation of a RoCK block of weight $d$ of a symmetric group $\mathfrak S_n$ defined over a field $F$ of characteristic $e$ is Morita equivalent to the principal block of the wreath product $\mathfrak S_e \wr \mathfrak S_d$. This generalises a theorem of Chuang and Kessar that applies to RoCK blocks with abelian defect groups. Our proof relies crucially on an isomorphism between $F\mathfrak S_n$ and a cyclotomic Khovanov-Lauda-Rouquier algebra, and the Morita equivalence we produce is that of graded algebras. We also prove the analogous result for an Iwahori-Hecke algebra at a root of unity defined over an arbitrary field. 	
1604.00972v1	http://arxiv.org/pdf/1604.00972v1	2016	On signatures of sonic wavepackets in time-resolved X-ray diffractometry   of metal single crystals absorbing pulse from ultrafast laser	Oleg Korovyanko|Oleksandra Korovyanko	  Copper (Cu), gold (Au) (111) crystals were illuminated with 120 fs pulses and probed by 600 fs X-ray pulses. Rocking curves were measured versus 267 nm (UV) pump- 0.154 nm probe delay time. Curve width broadening, peak diffracted intensity and angular shift were recorded for the range of UV excitation intensities of 2-8 mJ/cm2 . Observed oscillations in time delay dependences of shift, width and of peak intensity of rocking curves are correlated, as described by acoustic pulse bouncing between crystal surfaces. In (111) Au, acoustic pulse originates from pump absorption layer, and peak intensity drops by a factor of two (at delay time at which sonic wave reaches Au-substrate boundary) of its initial value prior to absorption of laser pulse. In Cu, correlated rocking curves dynamics is also observed, however buildup of rocking curve broadening is faster than one can expect from sonic wavepacket originating in pump absorption layer. This effect is attributed to acoustic phonon generation outside of pump absorption layer, it is much stronger in Cu than in Au. Phonon generation in Cu is distributed throughout the whole bulk of crystal due to delocalization of mobile hot electrons diffusing from pump absorption layer. 	
1610.02748v1	http://arxiv.org/pdf/1610.02748v1	2016	Mars sedimentary rock erosion rates constrained using crater counts,   with applications to organic matter preservation and to the global dust cycle	Edwin S. Kite|David P. Mayer	  Small-crater counts on Mars light-toned sedimentary rock are often inconsistent with any isochron; these data are usually plotted then ignored. We show (using an 18-HiRISE-image, >10^4 crater dataset) that these non-isochron crater counts are often well-fit by a model where crater production is balanced by crater obliteration via steady exhumation. For these regions, we fit erosion rates. We infer that Mars light-toned sedimentary rocks typically erode at ~10^2 nm/yr, when averaged over 10 km^2 scales and 10^7-10^8 yr timescales. Crater-based erosion-rate determination is consistent with independent techniques, but can be applied to nearly all light-toned sedimentary rocks on Mars. Erosion is swift enough that radiolysis cannot destroy complex organic matter at some locations (e.g. paleolake deposits at SW Melas), but radiolysis is a severe problem at other locations (e.g. Oxia Planum). The data suggest that the relief of the Valles Marineris mounds is currently being reduced by wind erosion, and that dust production on Mars <3 Gya greatly exceeds the modern reservoir of mobile dust. 	
1710.05608v1	http://arxiv.org/pdf/1710.05608v1	2017	Correlated microtiming deviations in jazz and rock music	Mathias Sogorski|Theo Geisel|Viola Priesemann	  Musical rhythms performed by humans typically show temporal fluctuations. While they have been characterized in simple rhythmic tasks, it is an open question what is the nature of temporal fluctuations, when several musicians perform music jointly in all its natural complexity. To study such fluctuations in over 100 original jazz and rock/pop recordings played with and without metronome we developed a semi-automated workflow allowing the extraction of cymbal beat onsets with millisecond precision. Analyzing the inter-beat interval (IBI) time series revealed evidence for two long-range correlated processes characterized by power laws in the IBI power spectral densities. One process dominates on short timescales ($t < 8$ beats) and reflects microtiming variability in the generation of single beats. The other dominates on longer timescales and reflects slow tempo variations. Whereas the latter did not show differences between musical genres (jazz vs. rock/pop), the process on short timescales showed higher variability for jazz recordings, indicating that jazz makes stronger use of microtiming fluctuations within a measure than rock/pop. Our results elucidate principles of rhythmic performance and can inspire algorithms for artificial music generation. By studying microtiming fluctuations in original music recordings, we bridge the gap between minimalistic tapping paradigms and expressive rhythmic performances. 	
1801.01100v1	http://arxiv.org/pdf/1801.01100v1	2018	Effects of friction and plastic deformation in shock-comminuted damaged   rocks on impact heating	Kosuke Kurosawa|Hidenori Genda	  Hypervelocity impacts cause significant heating of planetary bodies. Such events are recorded by a reset of 40Ar-36Ar ages and/or impact melts. Here, we investigate the influence of friction and plastic deformation in shock-generated comminuted rocks on the degree of impact heating using the iSALE shock-physics code. We demonstrate that conversion from kinetic to internal energy in the targets with strength occurs during pressure release, and additional heating becomes significant for low-velocity impacts (<10 km/s). This additional heat reduces the impact-velocity thresholds required to heat the targets with the 0.1 projectile mass to temperatures for the onset of Ar loss and melting from 8 and 10 km/s, respectively, for strengthless rocks to 2 and 6 km/s for typical rocks. Our results suggest that the impact conditions required to produce the unique features caused by impact heating span a much wider range than previously thought. 	
1508.03000v1	http://arxiv.org/pdf/1508.03000v1	2015	Few common failure cases in mobile robots	Ramviyas Parasuraman	  A mobile robot deployed for remote inspection, surveying or rescue missions can fail due to various possibilities and can be hardware or software related. These failure scenarios necessitate manual recovery (self-rescue) of the robot from the environment. It would bring unforeseen challenges to recover the mobile robot if the environment where it was deployed had hazardous or harmful conditions (e.g. ionizing radiations). While it is not fully possible to predict all the failures in the robot, failures can be reduced by employing certain design/usage considerations. Few example failure cases based on real experiences are presented in this short article along with generic suggestions on overcoming the illustrated failure situations. 	
0702028v2	http://arxiv.org/pdf/physics/0702028v2	2008	Rocking and rolling: a can that appears to rock might actually roll	Manoj Srinivasan|Andy Ruina	  A beer bottle or soda can on a table, when slightly tipped and released, falls to an upright position and then rocks up to a somewhat opposite tilt. Superficially this rocking motion involves a collision when the flat circular base of the container slaps the table before rocking up to the opposite tilt. A keen eye notices that the after-slap rising tilt is not generally just diametrically opposite the initial tilt but is veered to one side or the other. Cushman and Duistermaat (2006) recently noticed such veering when a flat disk with rolling boundary conditions is dropped nearly flat. Here, we generalize these rolling disk results to arbitrary axi-symmetric bodies and to frictionless sliding. More specifically, we study motions that almost but do not quite involve a face-down collision of the round container's bottom with the table-top. These motions involve a sudden rapid motion of the contact point around the circular base. Surprisingly, like for the rolling disk, the net angle of motion of this contact point is nearly independent of initial conditions. This angle of turn depends simply on the geometry and mass distribution but not on the moment of inertia about the symmetry axis. We derive simple asymptotic formulas for this "angle of turn" of the contact point and check the result with numerics and with simple experiments. For tall containers (height much bigger than radius) the angle of turn is just over $\pi$ and the sudden rolling motion superficially appears as a nearly symmetric collision leading to leaning on an almost diametrically opposite point on the bottom rim. 	
1110.0532v1	http://arxiv.org/pdf/1110.0532v1	2011	Strange Beta: An Assistance System for Indoor Rock Climbing Route   Setting Using Chaotic Variations and Machine Learning	Caleb Phillips|Lee Becker|Elizabeth Bradley	  This paper applies machine learning and the mathematics of chaos to the task of designing indoor rock-climbing routes. Chaotic variation has been used to great advantage on music and dance, but the challenges here are quite different, beginning with the representation. We present a formalized system for transcribing rock climbing problems, then describe a variation generator that is designed to support human route-setters in designing new and interesting climbing problems. This variation generator, termed Strange Beta, combines chaos and machine learning, using the former to introduce novelty and the latter to smooth transitions in a manner that is consistent with the style of the climbs This entails parsing the domain-specific natural language that rock climbers use to describe routes and movement and then learning the patterns in the results. We validated this approach with a pilot study in a small university rock climbing gym, followed by a large blinded study in a commercial climbing gym, in cooperation with experienced climbers and expert route setters. The results show that {\sc Strange Beta} can help a human setter produce routes that are at least as good as, and in some cases better than, those produced in the traditional manner. 	
1210.5280v2	http://arxiv.org/pdf/1210.5280v2	2013	Stability of Ice/Rock Mixtures with Application to a Partially   Differentiated Titan	Joseph G. O'Rourke|David J. Stevenson	  Titan's moment of inertia, calculated assuming hydrostatic equilibrium from gravity field data obtained during the Cassini-Huygens mission, implies an internal mass distribution that may be incompatible with complete differentiation. This suggests that Titan may have a mixed ice/rock core, possibly consistent with slow accretion in a gas-starved disk, which may initially spare Titan from widespread ice melting and subsequent differentiation. A partially differentiated Titan, however, must still efficiently remove radiogenic heat over geologic time. We argue that compositional heterogeneity in the major Saturnian satellites indicates that Titan formed from planetesimals with disparate densities. The resulting compositional anomalies would quickly redistribute to form a vertical density gradient that would oppose thermal convection. We use elements of the theory of double-diffusive convection to create a parameterized model for the thermal evolution of ice/rock mixtures with a stabilizing compositional gradient. Simulations are performed for a wide range of initial conditions to account for large uncertainties in material properties and accretionary processes. Ultimately, for realistic density gradients, double-diffusive convection in the ice/rock interior can delay, but not prevent, ice melting and differentiation, even if a substantial fraction of potassium is leached from the rock component. Consequently, Titan is not partially differentiated. 	
1304.6048v3	http://arxiv.org/pdf/1304.6048v3	2014	Rock physics and geophysics for unconventional resource, multi-component   seismic, quantitative interpretation	Michael E. Glinsky|Andrea Cortis|Jinsong Chen|Doug Sassen|Howard Rael	  An extension of a previously developed rock physics model is made that quantifies the relationship between the ductile fraction of a brittle/ductile binary mixture and the isotropic seismic reflection response. By making a weak scattering (Born) approximation and plane wave (eikonal) approximation, with a subsequent ordering according to the angle of incidence, singular value decomposition analysis are done to understand the stack weightings, number of stacks, and the type of stacks that will optimally estimate the two fundamental rock physics parameters. Through this angle ordering, it is found that effective wavelets can be used for the stacks up to second order. Finally, it is concluded that the full PP stack and the "full" PS stack are the two optimal stacks needed to estimate the two rock physics parameters. They dominate over both the second order AVO "gradient" stack and the higher order (4th order) PP stack (even at large angles of incidence). Using this result and model based Bayesian inversion, the detectability of the ductile fraction (shown by others to be the important quantity for the geomechanical response of unconventional reservoir fracking) is demonstrated on a model characteristic of the Marcellus shale play. 	
0105277v1	http://arxiv.org/pdf/cond-mat/0105277v1	2001	Mechanical properties and formation mechanisms of a wire of single gold   atoms	G. Rubio-Bollinger|S. R. Bahn|N. Agrait|K. W. Jacobsen|S. Vieira	  A scanning tunneling microscope (STM) supplemented with a force sensor is used to study the mechanical properties of a novel metallic nanostructure: a freely suspended chain of single gold atoms. We find that the bond strength of the nanowire is about twice that of a bulk metallic bond. We perform ab initio calculations of the force at chain fracture and compare quantitatively with experimental measurements. The observed mechanical failure and nanoelastic processes involved during atomic wire fabrication are investigated using molecular dynamics (MD) simulations, and we find that the total effective stiffness of the nanostructure is strongly affected by the detailed local atomic arrangement at the chain bases. 	
0510009v1	http://arxiv.org/pdf/cond-mat/0510009v1	2005	Mechanical Properties of Viral Capsids	Roya Zandi|David Reguera	  Viruses are known to tolerate wide ranges of pH and salt conditions and to withstand internal pressures as high as 100 atmospheres. In this paper we investigate the mechanical properties of viral capsids, calling explicit attention to the inhomogeneity of the shells that is inherent to their discrete and polyhedral nature. We calculate the distribution of stress in these capsids and analyze their response to isotropic internal pressure (arising, for instance, from genome confinement and/or osmotic activity). We compare our results with appropriate generalizations of classical (i.e., continuum) elasticity theory. We also examine competing mechanisms for viral shell failure, e.g., in-plane crack formation versus radial bursting. The biological consequences of the special stabilities and stress distributions of viral capsids are also discussed. 	
0805.3184v1	http://arxiv.org/pdf/0805.3184v1	2008	The clouds of physics and Einstein's last query: Can quantum mechanics   be derived from general relativity?	Friedwardt Winterberg	  Towards the end of the 19th century, Kelvin pronounced as the "clouds of physics" 1) the failure of the Michelson-Morely experiment to detect an ether wind, 2) the violation of the classical mechanical equipartition theorem in statistical thermodynamics. And he believed that the removal of these clouds would bring physics to an end. But as we know, the removal of these clouds led to the two great breakthoughts of modern physics: 1) The theory of relativity, and 2) to quantum mechanics. Towards the end of the 20th century more clouds of physics became apparent. They are 1) the riddle of quantum gravity, 2) the superluminal quantum correlations, 3) the small cosmological constant. Furthermore, there is the riddle of dark energy making up 70% of the physical universe, the non-baryonic cold dark matter making up 26% and the very small initial entropy of the universe. An attempt is made to explain the importance of these clouds for the future of physics. Conjectures for a possible solution are presented. they have to do with Einstein's last query: "Can quantum mechanics be derived general relativity", and with the question is there an ether? 	
1012.2516v1	http://arxiv.org/pdf/1012.2516v1	2010	An Efficient Security Mechanism for High-Integrity Wireless Sensor   Networks	Jaydip Sen	  Wireless sensor networks (WSNs) have recently attracted a lot of interest in the research community due their wide range of applications. Unfortunately, these networks are vulnerable to numerous security threats that can adversely affect their proper functioning. This problem is more critical if the network is deployed for some mission-critical applications such as in a tactical battlefield. Random failure of nodes and intentional compromise of nodes by an insider attack in a WSN pose particularly difficult challenges to security engineers as these attacks cannot be defended by traditional cryptography-based mechanisms. In this paper, a security solution is proposed for detecting compromised and faulty nodes in a WSN. The mechanism also isolates a compromised node from the network so that it cannot participate in any network activity. The proposed mechanism is based on misbehavior classification, behaviour monitoring and trust management. It involves minimum computation and communication overhead and is ideally suited for a resource-constrained, high-integrity WSN. 	
1111.0380v2	http://arxiv.org/pdf/1111.0380v2	2012	An Efficient Security Mechanism for High-Integrity Wireless Sensor   Networks	Jaydip Sen|Sripad Krishna	  Wireless sensor networks (WSNs) have recently attracted a lot of interest in the research community due their wide range of applications. Unfortunately, these networks are vulnerable to numerous security threats that can adversely affect their proper functioning. This problem is more critical if the network is deployed for some mission-critical applications such as in a tactical battlefield. Random failure of nodes and intentional compromise of nodes by an insider attack in a WSN pose particularly difficult challenges to security engineers as these attacks cannot be defended by traditional cryptography-based mechanisms. In this paper, a security solution is proposed for detecting compromised and faulty nodes in a WSN. The mechanism also isolates a compromised node from the network so that it cannot participate in any network activity. The proposed mechanism is based on misbehavior classification, behaviour monitoring and trust management. It involves minimum computation and communication overhead and is ideally suited for a resource-constrained, high-integrity WSN. 	
1211.0355v3	http://arxiv.org/pdf/1211.0355v3	2013	Ideal Strength of Doped Graphene	S. J. Woo|Young-Woo Son	  While the mechanical distortions change the electronic properties of graphene significantly, the effects of electronic manipulation on its mechanical properties have not been known. Using first-principles calculation methods, we show that, when graphene expands isotropically under equibiaxial strain, both the electron and hole doping can maintain or improve its ideal strength slightly and enhance the critical breaking strain dramatically. Contrary to the isotropic expansions, the electron doping decreases the ideal strength as well as critical strain of uniaxially strained graphene while the hole doping increases the both. Distinct failure mechanisms depending on type of strains are shown to be origins of the different doping induced mechanical stabilities. Our findings may resolve a contradiction between recent experimental and theoretical results on the strength of graphene. 	
1402.1006v1	http://arxiv.org/pdf/1402.1006v1	2014	Role of defects and geometry in the strength of polycrystalline graphene	Zhigong Song|Jian Wu|Zhiping Xu	  Defects in solid commonly limit mechanical performance of the material. However, recent measurements reported that the extraordinarily high strength of graphene is almost retained with the presence of grain boundaries. We clarify in this work that lattice defects in the grain boundaries and distorted geometry thus induced define the mechanical properties characterized under specific loading conditions. Atomistic simulations and theoretical analysis show that tensile tests measure in-plane strength that is governed by defect-induced stress buildup, while nanoindentation probes local strength under the indenter tip and bears additional geometrical effects from warping. These findings elucidate the failure mechanisms of graphene under realistic loading conditions and assess the feasibility of abovementioned techniques in quantifying the strength of graphene, and suggest that mechanical properties of low-dimensional materials could be tuned by implanting defects and geometrical distortion they leads to. 	
1407.0382v2	http://arxiv.org/pdf/1407.0382v2	2014	Reversible mechanical and electrical properties of ripped graphene	J. Henry Hinnefeld|Stephen T. Gill|Shuze Zhu|William J. Swanson|Teng Li|Nadya Mason	  We examine the mechanical properties of graphene devices stretched on flexible elastomer substrates. Using atomic force microscopy, transport measurements, and mechanics simulations, we show that micro-rips form in the graphene during the initial application of tensile strain; however subsequent applications of the same tensile strain elastically open and close the existing rips. Correspondingly, while the initial tensile strain degrades the devices' transport properties, subsequent strain-relaxation cycles affect transport only moderately, and in a largely reversible fashion, yielding robust electrical transport even after partial mechanical failure. 	
1410.0454v1	http://arxiv.org/pdf/1410.0454v1	2014	Strain-enhanced tunneling magnetoresistance in MgO magnetic tunnel   junctions	Li Ming Loong|Xuepeng Qiu|Zhi Peng Neo|Praveen Deorani|Yang Wu|Charanjit S. Bhatia|Mark Saeys|Hyunsoo Yang	  While the effects of lattice mismatch-induced strain, mechanical strain, as well as the intrinsic strain of thin films are sometimes detrimental, resulting in mechanical deformation and failure, strain can also be usefully harnessed for applications such as data storage, transistors, solar cells, and strain gauges, among other things. Here, we demonstrate that quantum transport across magnetic tunnel junctions (MTJs) can be significantly affected by the introduction of controllable mechanical strain, achieving an enhancement factor of ~2 in the experimental tunneling magnetoresistance (TMR) ratio. We further correlate this strain-enhanced TMR with coherent spin tunneling through the MgO barrier. Moreover, the strain-enhanced TMR is analyzed using non-equilibrium Green's function (NEGF) quantum transport calculations. Our results help elucidate the TMR mechanism at the atomic level and can provide a new way to enhance, as well as tune, the quantum properties in nanoscale materials and devices. 	
1506.07503v1	http://arxiv.org/pdf/1506.07503v1	2015	Attention-Based Models for Speech Recognition	Jan Chorowski|Dzmitry Bahdanau|Dmitriy Serdyuk|Kyunghyun Cho|Yoshua Bengio	  Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks in- cluding machine translation, handwriting synthesis and image caption gen- eration. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in reaches a competitive 18.7% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the at- tention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6% level. 	
1601.08043v1	http://arxiv.org/pdf/1601.08043v1	2016	Mechanism for the stabilization of protein clusters above the solubility   curve: the role of non-ideal chemical reactions	James F. Lutsko	  Dense protein clusters are known to play an important role in nucleation of protein crystals from dilute solutions. While these have generally been thought to be formed from a metastable phase, the observation of similar, if not identical, clusters above the critical point for the dilute-solution/strong-solution phase transition has thrown this into doubt. Furthermore, the observed clusters are stable for relatively long times. Because protein aggregation plays an important role in some pathologies, understanding the nature of such clusters is an important problem. One mechanism for the stabilization of such structures was proposed by Pan, Vekilov and Lubchenko and was investigated using a DDFT model which confirmed the viability of the model. Here, we revisit that model and incorporate additional physics in the form of state-dependent reaction rates. We show by a combination of numerical results and general arguments that the state-dependent rates disrupt the stability mechanism. Finally, we argue that the state-depedent reactions correct unphysical aspects of the model with ideal (state-independent) reactions and that this necessarily leads to the failure of the proposed mechanism. 	
1602.00456v2	http://arxiv.org/pdf/1602.00456v2	2016	The Ideal Tensile Strength and Phonon Instability of Borophene	Haifeng Wang|Qingfang Li|Yan Gao|F. Miao|Xiang-Feng Zhou|X. G. Wan	  Very recently, two-dimensional(2D) boron sheets (borophene) with rectangular structure has been grown successfully on single crystal Ag(111) substrates.The fabricated boroprene is predicted to have unusual mechanical properties. We performed first-principle calculations to investigate the mechanical properties of the monolayer borophene, including ideal tensile strength and critical strain. It was found that monolayer borophene can withstand stress up to 20.26 N/m and 12.98 N/m in a and b directions, respectively.However, its critical strain was found to be small. In a direction, the critical value is only 8%, which, to the best of our knowledge, is the lowest among all studied 2D materials.Our numerical results show that the tensile strain applied in b direction enhances the bucking height of borophene resulting in an out-of-plane negative Poisson's ratio, which makes the boron sheet show superior mechanical flexibility along b direction.The failure mechanism and phonon instability of monolayer borophene were also explored. 	
1607.03141v1	http://arxiv.org/pdf/1607.03141v1	2016	Competing Mechanisms between Dislocation and Phase Transformation in   Plastic Deformation of Single Crystalline Yttria-Stabilized Tetragonal   Zirconia Nanopillars	Ning Zhang|Mohsen Asle Zaeem	  Molecular dynamics (MD) is employed to investigate the plastic deformation mechanisms of single crystalline yttria-stabilized tetragonal zirconia (YSTZ) nanopillars under uniaxial compression. Simulation results show that the nanoscale plastic deformation of YSTZ is strongly dependent on the crystallographic orientation of zirconia nanopillars. For the first time, the experimental explored tetragonal to monoclinic phase transformation is reproduced by MD simulations in some particular loading directions. Three distinct mechanisms of dislocation, phase transformation, and a combination of dislocation and phase transformation are identified when applying compressive loading along different directions. The strength of zirconia nanopillars exhibits a sensitive behavior depending on the failure mechanisms, such that the dislocation-mediated deformation leads to the lowest strength, while the phase transformation-dominated deformation results in the highest strength. 	
1609.08123v1	http://arxiv.org/pdf/1609.08123v1	2016	Strong Equivalence Principle in Polymer Quantum Mechanics and deformed   Heisenberg Algebra	Nirmalya Kajuri	  The Strong equivalence Principle (SEP) states that the description of a physical system in a gravitational field is indistinguishable from the description of the same system at rest in an accelerating frame. While this statement holds true in both General Relativity and ordinary Quantum Mechanics, one expects it to fail when quantum gravity corrections are taken into account. In this paper we investigate the possible failure of the SEP in two Quantum Gravity inspired modifications of Quantum Mechanics - Polymer Quantum Mechanics and deformed Heisenberg Algebra. We find that the SEP fails to hold in both these theories. We estimate the deviation from SEP and find in both cases that it is too small to be measured in present day experiments. 	
1609.08338v1	http://arxiv.org/pdf/1609.08338v1	2016	Mechanical stability of particle-stabilized droplets under micropipette   aspiration	Niveditha Samudrala|Jin Nam|Raphael Sarfati|Robert W. Style|Eric R. Dufresne	  We investigate the mechanical behavior of particle-stabilized droplets using micropipette aspiration. We observe that droplets stabilized with amphiphilic dumbbell-shaped particles exhibit a two-stage response to increasing suction pressure. Droplets first drip, then wrinkle and buckle like an elastic shell. While particles have a dramatic impact on the mechanism of failure, the mechanical strength of the droplets is only modestly increased. On the other hand, droplets coated with the molecular surfactant Sodium Dodecyl Sulfate are even weaker than bare droplets. In all cases, the magnitude of the critical pressure for the onset of instabilities is set by the fluid surface tension. 	
1706.01428v3	http://arxiv.org/pdf/1706.01428v3	2017	A correspondence between thermodynamics and inference	Colin H. LaMont|Paul A. Wiggins	  We systematically explore a natural analogy between Bayesian statistics and thermal physics in which sample size corresponds to inverse temperature. We discover that some canonical thermodynamic quantities already correspond to well-established statistical quantities. Motivated by physical insight into thermal physics, we define two novel statistical quantities: a learning capacity and Gibbs entropy. The definition of the learning capacity leads to a critical insight: The well-known mechanism of failure of the equipartition theorem in statistical mechanics is the mechanism for anomalously-predictive or sloppy models in statistics. This correspondence between the learning and heat capacities provides new insight into the mechanism of machine learning. The correspondence also suggests a solution to a long-standing difficulty in Bayesian statistics: the definition of an objective prior. We propose that the Gibbs entropy provides a natural generalization of the principle of indifference that defines objectivity. This approach unifies the disparate Bayesian, frequentist and information-based paradigms of statistics by achieving coherent inference between these competing formulations. 	
1709.08412v1	http://arxiv.org/pdf/1709.08412v1	2017	Non-local plasticity effects on notch fracture mechanics	Emilio Martínez-Pañeda|Susana del Busto|Covadonga Betegón	  We investigate the influence of gradient-enhanced dislocation hardening on the mechanics of notch-induced failure. The role of geometrically necessary dislocations (GNDs) in enhancing cracking is assessed by means of a mechanism-based strain gradient plasticity theory. Both stationary and propagating cracks from notch-like defects are investigated through the finite element method. A cohesive zone formulation incorporating monotonic and cyclic damage contributions is employed to address both loading conditions. Computations are performed for a very wide range of length scale parameters and numerous geometries are addressed, covering the main types of notches. Results reveal a strong influence of the plastic strain gradients in all the scenarios considered. Transitional combinations of notch angle, radius and length scale parameter are identified that establish the regimes of GNDs-relevance, laying the foundations for the rational application of gradient plasticity models in damage assessment of notched components. 	
1710.01117v2	http://arxiv.org/pdf/1710.01117v2	2017	A control mechanism for intramural periarterial drainage via astrocytes:   How neuronal activity could improve waste clearance from the brain	Alexandra K. Diem|Roxana O. Carare|Neil. W. Bressloff	  The mechanisms behind waste clearance from deep within the parenchyma of the brain remain unclear to this date. Experimental evidence has shown that one pathway for waste clearance, termed intramural periarterial drainage (IPAD), is the rapid drainage of interstitial fluid (ISF) via basement membranes (BM) of the smooth muscle cells (SMC) of cerebral arteries and its failure is closely associated with the pathology of Alzheimer's disease (AD). We have previously shown that arterial pulsations from the heart beat are not strong enough to drive waste clearance. Here we demonstrate computational evidence for a mechanism for cerebral waste clearance that is driven by functional hyperaemia, that is, the dilation of cerebral arteries as a consequence of increased neuronal demand. This mechanism is based on our model for fluid flow through the vascular basement membrane. It accounts for waste clearance rates observed in mouse experiments and aligns with pathological observations as well as recommendations to lower the individual risk of AD, such as keeping mentally and physically active. 	
1801.05639v1	http://arxiv.org/pdf/1801.05639v1	2018	Mechanical Properties of Schwarzites - A Fully Atomistic Reactive   Molecular Dynamics Investigation	Cristiano F. Woellner|Tiago Botari|Eric Perim|Douglas S. Galvao	  Schwarzites are crystalline, 3D porous structures with stable negative curvature formed of sp2-hybridized carbon atoms. These structures present topologies with tunable porous size and shape and unusual mechanical properties. In this work, we have investigated the mechanical behavior under compressive strains and energy absorption of four different Schwarzites, through reactive molecular dynamics simulations, using the ReaxFF force field as available in the LAMMPS code. We considered two Schwarzites families, the so-called Gyroid and Primitive and two structures from each family. Our results also show they exhibit remarkable resilience under mechanical compression. They can be reduced to half of their original size before structural failure (fracture) occurs. 	
1801.04015v1	http://arxiv.org/pdf/1801.04015v1	2018	Spatio-Temporal Pricing for Ridesharing Platforms	Hongyao Ma|Fei Fang|David C. Parkes	  Ridesharing platforms match drivers and riders to trips, using dynamic prices to balance supply and demand. A challenge is to set prices that are appropriately smooth in space and time, in the sense that drivers will choose to accept their dispatched trips, rather than drive to another area or wait for higher prices or a better trip. We introduce the Spatio-Temporal Pricing (STP) mechanism. The mechanism is incentive-aligned, in that it is a subgame-perfect equilibrium for drivers to accept their dispatches, and the mechanism is welfare-optimal, envy-free, individually rational and budget balanced from any history onward. We work in a complete information, discrete time, multi-period, multi-location model, and prove the existence of anonymous, origin-destination, competitive equilibrium (CE) prices. The STP mechanism employs driver-pessimal CE prices, and the proof of incentive alignment makes use of the $M^\natural$ concavity of min-cost flow objectives. The same connection to min-cost flow problems provides an efficient algorithm to compute an optimal matching and prices. We also give an impossibility result, that there can be no dominant-strategy mechanism with the same economic properties. An empirical analysis conducted in simulation suggests that the STP mechanism can achieve significantly higher social welfare than a myopic pricing mechanism, and highlights the failure of incentive alignment due to non-smooth prices in myopic mechanisms. 	
9905161v1	http://arxiv.org/pdf/cond-mat/9905161v1	1999	Failure time and microcrack nucleation	A. Guarino|S. Ciliberto|A. Garcimartin	  The failure time of samples of heterogeneous materials (wood, fiberglass) is studied as a function of the applied stress. It is shown that in these materials the failure time is predicted with a good accuracy by a model of microcrack nucleation proposed by Pomeau. It is also shown that the crack growth process presents critical features when the failure time is approached. 	
0012009v1	http://arxiv.org/pdf/cs/0012009v1	2000	Finding Failure Causes through Automated Testing	Holger Cleve|Andreas Zeller	  A program fails. Under which circumstances does this failure occur? One single algorithm, the delta debugging algorithm, suffices to determine these failure-inducing circumstances. Delta debugging tests a program systematically and automatically to isolate failure-inducing circumstances such as the program input, changes to the program code, or executed statements. 	
0410051v1	http://arxiv.org/pdf/cs/0410051v1	2004	Turing Machine with Faults, Failures and Recovery	Alex Vinokur	  A Turing machine with faults, failures and recovery (TMF) is described. TMF is (weakly) non-deterministic Turing machine consisting of five semi-infinite tapes (Master Tape, Synchro Tape, Backup Tape, Backup Synchro Tape, User Tape) and four controlling components (Program, Daemon, Apparatus, User). Computational process consists of three phases (Program Phase, Failure Phase, Repair Phase). C++ Simulator of a Turing machine with faults, failures and recovery has been developed. 	
0709.1875v1	http://arxiv.org/pdf/0709.1875v1	2007	Failure Analysis and Field Failures: a Real Shortcut to Reliability   Improvement	G. Mura|G. Cassanelli	  Starting from two case histories, where only after thorough Failure Analysis the suddenly appearance of a failure was linked to much earlier events, the possibility of improving the reliability and of adjusting the reliability prediction tools are discussed. 	
1203.6404v1	http://arxiv.org/pdf/1203.6404v1	2012	Definition, Detection, and Recovery of Single-Page Failures, a Fourth   Class of Database Failures	Goetz Graefe|Harumi Kuno	  The three traditional failure classes are system, media, and transaction failures. Sometimes, however, modern storage exhibits failures that differ from all of those. In order to capture and describe such cases, single-page failures are introduced as a fourth failure class. This class encompasses all failures to read a data page correctly and with plausible contents despite all correction attempts in lower system levels. Efficient recovery seems to require a new data structure called the page recovery index. Its transactional maintenance can be accomplished writing the same number of log records as today's efficient implementations of logging and recovery. Detection and recovery of a single-page failure can be sufficiently fast that the affected data access is merely delayed, without the need to abort the transaction. 	
1509.04557v1	http://arxiv.org/pdf/1509.04557v1	2015	Spatio-temporal propagation of cascading overload failures	Jichang Zhao|Daqing Li|Hillel Sanhedrai|Reuven Cohen|Shlomo Havlin	  Different from the direct contact in epidemics spread, overload failures propagate through hidden functional dependencies. Many studies focused on the critical conditions and catastrophic consequences of cascading failures. However, to understand the network vulnerability and mitigate the cascading overload failures, the knowledge of how the failures propagate in time and space is essential but still missing. Here we study the spatio-temporal propagation behavior of cascading overload failures analytically and numerically. The cascading overload failures are found to spread radially from the center of the initial failure with an approximately constant velocity. The propagation velocity decreases with increasing tolerance, and can be well predicted by our theoretical framework with one single correction for all the tolerance values. This propagation velocity is found similar in various model networks and real network structures. Our findings may help to predict and mitigate the dynamics of cascading overload failures in realistic systems. 	
1707.05428v1	http://arxiv.org/pdf/1707.05428v1	2017	Coordination and Control of Distributed Discrete-event Systems subject   to Sensor and Actuator Failures	Jin Dai|Hai Lin	  We study the coordination and control problem of distributed discrete-event systems with synchronous communication, in the presence of subsystems whose sensors and/or actuators may be affected by unexpected failures. We model sensor failures as permanent loss of observability of certain sensor events that belong to a subsystem, while characterize actuator failures as loss of controllability of the subsystems' actuator events. The failure tolerance property requires that the distributed discrete-event systems satisfy a global specification prior to as well as after occurrences of potential failures. To prevent the failure-pruned subsystems from jeopardizing the fulfillment of the specification, we propose automaton-theoretic frameworks corresponding to the enforcement of sensor and actuator failure tolerance by incorporating learning-based supervisor synthesis and coordination approaches with appropriate post-failure control reconfiguration schemes. The effectiveness of the proposed frameworks is demonstrated by an illustrative example. 	
1710.09722v2	http://arxiv.org/pdf/1710.09722v2	2018	Exhaustive Exploration of the Failure-oblivious Computing Search Space	Thomas Durieux|Youssef Hamadi|Zhongxing Yu|Martin Monperrus	  High-availability of software systems requires automated handling of crashes in presence of errors. Failure-oblivious computing is one technique that aims to achieve high availability. We note that failure-obliviousness has not been studied in depth yet, and there is very few study that helps understand why failure-oblivious techniques work. In order to make failure-oblivious computing to have an impact in practice, we need to deeply understand failure-oblivious behaviors in software. In this paper, we study, design and perform an experiment that analyzes the size and the diversity of the failure-oblivious behaviors. Our experiment consists of exhaustively computing the search space of 16 field failures of large-scale open-source Java software. The outcome of this experiment is a much better understanding of what really happens when failure-oblivious computing is used, and this opens new promising research directions. 	
1711.04491v1	http://arxiv.org/pdf/1711.04491v1	2017	The impact of a network split on cascading failure processes	Fiona Sloothaak|Sem C. Borst|Bert Zwart	  Cascading failure models are typically used to capture the phenomenon where failures possibly trigger further failures in succession, causing knock-on effects. In many networks this ultimately leads to a disintegrated network where the failure propagation continues independently across the various components. In order to gain insight in the impact of network splitting on cascading failure processes, we extend a well-established cascading failure model for which the number of failures obeys a power-law distribution. We assume that a single line failure immediately splits the network in two components, and examine its effect on the power-law exponent. The results provide valuable qualitative insights that are crucial first steps towards understanding more complex network splitting scenarios. 	
1802.07455v1	http://arxiv.org/pdf/1802.07455v1	2018	Asymptotic efficiency of restart and checkpointing	Antonio Sodre	  Many tasks are subject to failure before completion. Two of the most common failure recovery strategies are restart and checkpointing. Under restart, once a failure occurs, it is restarted from the beginning. Under checkpointing, the task is resumed from the preceding checkpoint after the failure. We study asymptotic efficiency of restart for an infinite sequence of tasks, whose sizes form a stationary sequence. We define asymptotic efficiency as the limit of the ratio of the total time to completion in the absence of failures over the total time to completion when failures take place. Whether the asymptotic efficiency is positive or not depends on the comparison of the tail of the distributions of the task size and the random variables governing failures. Our framework allows for variations in the failure rates and dependencies between task sizes. We also study a similar notion of asymptotic efficiency for checkpointing when the task is infinite a.s. and the inter-checkpoint times are i.i.d.. Moreover, in checkpointing, when the failures are exponentially distributed, we prove the existence of an infinite sequence of universal checkpoints, which are always used whenever the system starts from any checkpoint that precedes them. 	
1010.1830v1	http://arxiv.org/pdf/1010.1830v1	2010	An elastoplastic framework for granular materials becoming cohesive   through mechanical densification. Part II - the formulation of elastoplastic   coupling at large strain	Andrea Piccolroaz|Davide Bigoni|Alessandro Gajo	  The two key phenomena occurring in the process of ceramic powder compaction are the progressive gain in cohesion and the increase of elastic stiffness, both related to the development of plastic deformation. The latter effect is an example of `elastoplastic coupling', in which the plastic flow affects the elastic properties of the material, and has been so far considered only within the framework of small strain assumption (mainly to describe elastic degradation in rock-like materials), so that it remains completely unexplored for large strain. Therefore, a new finite strain generalization of elastoplastic coupling theory is given to describe the mechanical behaviour of materials evolving from a granular to a dense state. The correct account of elastoplastic coupling and of the specific characteristics of materials evolving from a loose to a dense state (for instance, nonlinear --or linear-- dependence of the elastic part of the deformation on the forming pressure in the granular --or dense-- state) makes the use of existing large strain formulations awkward, if even possible. Therfore, first, we have resorted to a very general setting allowing general transformations between work-conjugate stress and strain measures; second, we have introduced the multiplicative decomposition of the deformation gradient and, third, employing isotropy and hyperelasticity of elastic response, we have obtained a relation between the Biot stress and its `total' and `plastic' work-conjugate strain measure. This is a key result, since it allows an immediate achievement of the rate elastoplastic constitutive equations. Knowing the general form of these equations, all the specific laws governing the behaviour of ceramic powders are finally introduced as generalizations of the small strain counterparts given in Part I of this paper. 	
1506.03793v2	http://arxiv.org/pdf/1506.03793v2	2016	Nuclear fusion in the deuterated cores of inflated hot Jupiters	Rachid Ouyed|Prashanth Jaikumar	  Ouyed et al. (1998) proposed Deuterium (DD) fusion at the core-mantle interface of giant planets as a mechanism to explain their observed heat excess. But rather high interior temperatures (~10^5 K) and a stratified D layer are needed, making such a scenario unlikely. In this paper, we re-examine DD fusion, with the addition of screening effects pertinent to a deuterated core containing ice and some heavy elements. This alleviates the extreme temperature constraint and removes the requirement of a stratified D layer. As an application, we propose that, if their core temperatures are a few times 10^4 K and core composition is chemically inhomogeneous, the observed inflated size of some giant exoplanets ("hot Jupiters") may be linked to screened DD fusion occurring deep in the interior. Application of an analytic evolution model suggests that the amount of inflation from this effect can be important if there is sufficient rock-ice in the core, making DD fusion an effective extra internal energy source for radius inflation. The mechanism of screened DD fusion, operating in the above temperature range, is generally consistent with the trend in radius anomaly with planetary equilibrium temperature $T_{\rm eq}$, and also depends on planetary mass. Although we do not consider the effect of incident stellar flux, we expect that a minimum level of irradiation is necessary to trigger core erosion and subsequent DD fusion inside the planet. Since DD fusion is quite sensitive to the screening potential inferred from laboratory experiments, observations of inflated hot Jupiters may help constrain screening effects in the cores of giant planets. 	
0506078v2	http://arxiv.org/pdf/quant-ph/0506078v2	2005	Generalization of Classical Statistical Mechanics to Quantum Mechanics   and Stable Property of Condensed Matter	Y. C. Huang|F. C. Ma|N. Zhang	  Classical statistical average values are generally generalized to average values of quantum mechanics, it is discovered that quantum mechanics is direct generalization of classical statistical mechanics, and we generally deduce both a new general continuous eigenvalue equation and a general discrete eigenvalue equation in quantum mechanics, and discover that a eigenvalue of quantum mechanics is just an extreme value of an operator in possibility distribution, the eigenvalue f is just classical observable quantity. A general classical statistical uncertain relation is further given, the general classical statistical uncertain relation is generally generalized to quantum uncertainty principle, the two lost conditions in classical uncertain relation and quantum uncertainty principle, respectively, are found. We generally expound the relations among uncertainty principle, singularity and condensed matter stability, discover that quantum uncertainty principle prevents from the appearance of singularity of the electromagnetic potential between nucleus and electrons, and give the failure conditions of quantum uncertainty principle. Finally, we discover that the classical limit of quantum mechanics is classical statistical mechanics, the classical statistical mechanics may further be degenerated to classical mechanics, and we discover that only saying that the classical limit of quantum mechanics is classical mechanics is mistake. As application examples, we deduce both Shrodinger equation and state superposition principle, deduce that there exist decoherent factor from a general mathematical representation of state superposition principle, and the consistent difficulty between statistical interpretation of quantum mechanics and determinant property of classical mechanics is overcome. 	
0711.0952v1	http://arxiv.org/pdf/0711.0952v1	2007	On the Luttinger theorem concerning number of particles in the ground   states of systems of interacting fermions	Behnam Farid	  We analyze the original proof by Luttinger and Ward of the Luttinger theorem, according to which for uniform ground states of systems of (interacting) fermions, which may be metallic or insulating, the number of k points corresponding to non-negative values of G_s(k;mu) is equal to the total number of particles with spin index s in these ground states. Here G_s(k;mu) is the single-particle Green function of particles with spin index s at the chemical potential mu. For the cases where the two-body interaction potential is short-range, and in particular for lattice models, we explicitly demonstrate that this theorem is unconditionally valid, irrespective of the strength of the bare interaction potential. We arrive at this conclusion by amongst other things demonstrating that the perturbation series expansion for self-energy in terms of skeleton diagrams, as encountered in the proof of the Luttinger-Ward identity, is uniformly convergent for almost all momenta and energies. We further investigate the mechanisms underlying some reported instances of failure of the Luttinger theorem. With one exception, for all the cases considered in this paper, we show that the apparent failures of the Luttinger theorem can be attributed either to shortcomings of the employed single-particle Green functions or to misapplication of this theorem. The one exceptional case brings to light the possibility of a genuine failure of the Luttinger theorem for insulating ground states, which we show to be brought about by a false limit that in principle can be reached on taking the zero-temperature limit without the value of mu coinciding with the zero-temperature limit of the chemical potential satisfying the equation of state at finite temperatures; no such ambiguity can arise for metallic states. 	
0901.3806v1	http://arxiv.org/pdf/0901.3806v1	2009	Modeling long-term longitudinal HIV dynamics with application to an AIDS   clinical study	Yangxin Huang|Tao Lu	  A virologic marker, the number of HIV RNA copies or viral load, is currently used to evaluate antiretroviral (ARV) therapies in AIDS clinical trials. This marker can be used to assess the ARV potency of therapies, but is easily affected by drug exposures, drug resistance and other factors during the long-term treatment evaluation process. HIV dynamic studies have significantly contributed to the understanding of HIV pathogenesis and ARV treatment strategies. However, the models of these studies are used to quantify short-term HIV dynamics ($<$ 1 month), and are not applicable to describe long-term virological response to ARV treatment due to the difficulty of establishing a relationship of antiviral response with multiple treatment factors such as drug exposure and drug susceptibility during long-term treatment. Long-term therapy with ARV agents in HIV-infected patients often results in failure to suppress the viral load. Pharmacokinetics (PK), drug resistance and imperfect adherence to prescribed antiviral drugs are important factors explaining the resurgence of virus. To better understand the factors responsible for the virological failure, this paper develops the mechanism-based nonlinear differential equation models for characterizing long-term viral dynamics with ARV therapy. The models directly incorporate drug concentration, adherence and drug susceptibility into a function of treatment efficacy and, hence, fully integrate virologic, PK, drug adherence and resistance from an AIDS clinical trial into the analysis. A Bayesian nonlinear mixed-effects modeling approach in conjunction with the rescaled version of dynamic differential equations is investigated to estimate dynamic parameters and make inference. In addition, the correlations of baseline factors with estimated dynamic parameters are explored and some biologically meaningful correlation results are presented. Further, the estimated dynamic parameters in patients with virologic success were compared to those in patients with virologic failure and significantly important findings were summarized. These results suggest that viral dynamic parameters may play an important role in understanding HIV pathogenesis, designing new treatment strategies for long-term care of AIDS patients. 	
1410.6836v3	http://arxiv.org/pdf/1410.6836v3	2015	Reducing Cascading Failure Risk by Increasing Infrastructure Network   Interdependency	Mert Korkali|Jason G. Veneman|Brian F. Tivnan|Paul D. H. Hines	  Increased coupling between critical infrastructure networks, such as power and communication systems, will have important implications for the reliability and security of these systems. To understand the effects of power-communication coupling, several have studied interdependent network models and reported that increased coupling can increase system vulnerability. However, these results come from models that have substantially different mechanisms of cascading, relative to those found in actual power and communication networks. This paper reports on two sets of experiments that compare the network vulnerability implications resulting from simple topological models and models that more accurately capture the dynamics of cascading in power systems. First, we compare a simple model of topological contagion to a model of cascading in power systems and find that the power grid shows a much higher level of vulnerability, relative to the contagion model. Second, we compare a model of topological cascades in coupled networks to three different physics-based models of power grids coupled to communication networks. Again, the more accurate models suggest very different conclusions. In all but the most extreme case, the physics-based power grid models indicate that increased power-communication coupling decreases vulnerability. This is opposite from what one would conclude from the coupled topological model, in which zero coupling is optimal. Finally, an extreme case in which communication failures immediately cause grid failures, suggests that if systems are poorly designed, increased coupling can be harmful. Together these results suggest design strategies for reducing the risk of cascades in interdependent infrastructure systems. 	
0207393v3	http://arxiv.org/pdf/cond-mat/0207393v3	2003	Phase transition in fiber bundle models with recursive dynamics	Pratip Bhattacharyya|Srutarshi Pradhan|Bikas K. Chakrabarti	  We study the phase transition in a class of fiber bundle models in which the fiber strengths are distributed randomly within a finite interval and global load sharing is assumed. The dynamics is expressed as recursion relations for the redistribution of the applied stress and the evolution of the surviving fraction of fibers. We show that an irreversible phase transition of second-order occurs, from a phase of partial failure to a phase of total failure, when the initial applied stress just exceeds a critical value. The phase transition is characterised by static and dynamic critical properties. We calculate exactly the critical value of the initial stress for three models of this kind, each with a different distribution of fiber strengths. We derive the exact expressions for the order parameter, the susceptibility to changes in the initial applied sress and the critical relaxation of the surviving fraction of fibers for all the three models. The static and dynamic critical exponents obtained from these expressions are found to be universal. 	
0301076v1	http://arxiv.org/pdf/cond-mat/0301076v1	2003	Fracture of Notched Single Crystal Silicon	Nicholas P. Bailey|James P. Sethna	  We study atomistically the fracture of single crystal silicon at atomically sharp notches with opening angles of 0 degrees (a crack), 70.53 degrees, 90 degrees and 125.3 degrees. Such notches occur in silicon that has been formed by etching into microelectromechanical structures and tend to be the initiation sites for failure by fracture of these structures. Analogous to the stress intensity factor of traditional linear elastic fracture mechanics which characterizes the stress state in the limiting case of a crack, there exists a similar parameter K for the case of the notch. In the case of silicon, a brittle material, this characterization appears to be particularly valid. We use three interatomic potentials: a modified Stillinger-Weber potential, the Environment-Dependent Interatomic Potential (EDIP), and the modified embedded atom method (MEAM). Of these, MEAM gives critical K-values closest to experiment. In particular the EDIP potential leads to unphysical ductile failure in most geometries. Because the units of K depend on the notch angle, the shape of the K versus angle plot depends on the units used. In particular when an atomic length unit is used the plot is almost flat, showing--in principle from macroscopic observations alone--the association of an atomic length scale to the fracture process. 	
0409524v2	http://arxiv.org/pdf/cond-mat/0409524v2	2005	Statistical Physics of Rupture in Heterogeneous Media	D. Sornette	  The damage and fracture of materials are technologically of enormous interest due to their economic and human cost. They cover a wide range of phenomena like e.g. cracking of glass, aging of concrete, the failure of fiber networks in the formation of paper and the breaking of a metal bar subject to an external load. Failure of composite systems is of utmost importance in naval, aeronautics and space industry. By the term composite, we refer to materials with heterogeneous microscopic structures and also to assemblages of macroscopic elements forming a super-structure. Chemical and nuclear plants suffer from cracking due to corrosion either of chemical or radioactive origin, aided by thermal and/or mechanical stress. Despite the large amount of experimental data and the considerable effort that has been undertaken by material scientists, many questions about fracture have not been answered yet. There is no comprehensive understanding of rupture phenomena but only a partial classification in restricted and relatively simple situations. This lack of fundamental understanding is indeed reflected in the absence of reliable prediction methods for rupture, based on a suitable monitoring of the stressed system. Not only is there a lack of non-empirical understanding of the reliability of a system, but also the empirical laws themselves have often limited value. The difficulties stem from the complex interplay between heterogeneities and modes of damage and the possible existence of a hierarchy of characteristic scales (static and dynamic).   The paper presents a review of recent efforts from the statistical physics community to address these points. 	
0508424v1	http://arxiv.org/pdf/cond-mat/0508424v1	2005	Predicting Failure using Conditioning on Damage History: Demonstration   on Percolation and Hierarchical Fiber Bundles	J. Andersen|D. Sornette	  We formulate the problem of probabilistic predictions of global failure in the simplest possible model based on site percolation and on one of the simplest model of time-dependent rupture, a hierarchical fiber bundle model. We show that conditioning the predictions on the knowledge of the current degree of damage (occupancy density $p$ or number and size of cracks) and on some information on the largest cluster improves significantly the prediction accuracy, in particular by allowing to identify those realizations which have anomalously low or large clusters (cracks). We quantify the prediction gains using two measures, the relative specific information gain (which is the variation of entropy obtained by adding new information) and the root-mean-square of the prediction errors over a large ensemble of realizations. The bulk of our simulations have been obtained with the two-dimensional site percolation model on a lattice of size $L \times L=20 \times 20$ and hold true for other lattice sizes. For the hierarchical fiber bundle model, conditioning the measures of damage on the information of the location and size of the largest crack extends significantly the critical region and the prediction skills. These examples illustrate how on-going damage can be used as a revelation of both the realization-dependent pre-existing heterogeneity and the damage scenario undertaken by each specific sample. 	
0610399v1	http://arxiv.org/pdf/cond-mat/0610399v1	2006	Equilibrium and transport properties of constrained systems	Debasish Chaudhuri	  Systems under external confinement and constraints often show interesting properties. In this thesis, we study some systems under external confinement. We begin by finding out the probability distribution of end-to-end separation of a Worm Like Chain (WLC) polymer whose ends are positionally (and orientationally) constrained. We use Monte-Carlo simulations (MC) and a theoretical mapping of the WLC to a quantum particle moving on the surface of an unit sphere to find multimodality in Helmholtz ensemble as a generic signature of semi-flexibility. Secondly, we study Laser Induced Freezing using a Kosterlitz-Thouless type renormalization group calculation and a restricted MC simulation to obtain phase diagrams for Hard Disk, Soft Disk and DLVO potentials. They show very good agreement with phase diagrams simulated by other groups. Lastly, we study the strain response and failure mechanism of a two-dimensional solid confined within a hard wall channel using MC and molecular dynamics simulations. We find a reversible plastic failure through solid-smectic coexistence and observe layering transitions. Mean field calculations can capture some of these features. We study the heat transport in this system thorugh nonequilibrium molecular dynamics simulations and find strong signatures of the transitions. We propose a simple free volume calculation that reproduces some qualitative features of the strain response of heat current for small strains. 	
0411006v2	http://arxiv.org/pdf/q-bio/0411006v2	2006	Lethality and synthetic lethality in the genome-wide metabolic network   of Escherichia coli	C. -M. Ghim|K. -I. Goh|B. Kahng	  Recent genomic analyses on the cellular metabolic network show that reaction flux across enzymes are diverse and exhibit power-law behavior in its distribution. While one may guess that the reactions with larger fluxes are more likely to be lethal under the blockade of its catalyzing gene products or gene knockouts, we find, by in silico flux analysis, that the lethality rarely has correlations with the flux level owing to the widespread backup pathways innate in the genome-wide metabolism of \textit{Escherichia coli}. Lethal reactions, of which the deletion generates cascading failure of following reactions up to the biomass reaction, are identified in terms of the Boolean network scheme as well as the flux balance analysis. The avalanche size of a reaction, defined as the number of subsequently blocked reactions after its removal, turns out to be a useful measure of lethality. As a means to elucidate phenotypic robustness to a single deletion, we investigate synthetic lethality in reaction level, where simultaneous deletion of a pair of nonlethal reactions leads to the failure of the biomass reaction. Synthetic lethals identified via flux balance and Boolean scheme are consistently shown to act in parallel pathways, working in such a way that the backup machinery is compromised. 	
0711.0399v1	http://arxiv.org/pdf/0711.0399v1	2007	Slow failure of quasi-brittle solids	Leonid S. Metlov	  A new mesoscopic non-equilibrium thermodynamic approach is developed. The approach is based on the thermodynamic identity associated the first and second law of thermodynamics. In the framework of the approach different internal dissipative channels of energy are taken in account in an explicit form, namely, the thermal channel and channels of defect subsystems. The identity has a perfect differential form what permits to introduce an extended non-equilibrium state and use the good developed mathematical formalism of equilibrium and non-equilibrium thermodynamics. The evolution of non-equilibrium variables of a physical system are described by a Landau-based equation set expressed through internal or different kinds of free energy connected by means of the Legendre transforms. The accordance between the different kinds of energy is possible owing to introduction of some trends into the equation subset described the defect subsystems and having a nature of structural viscosity. The possibilities of the approach are illustrated on the example of quasibrittle solid damage and failure. Taking into account only one type of defects (viz., microcracks) and mechanical parameters in an expansion of free energy down to third powers in relative to average energy per microcrack, the description of destruction of quasi-brittle solids during long-term loading is considered. The consideration allows to find equilibrium and non-equilibrium values of the free energy. A qualitative behavior of the system on parameters of the theory is analyzed. The destruction of material is described from the uniform positions, both at uniform tension and uniaxial compression. Origins of the high stability of mine workings at small depths and their instability at large depths are explained. 	
0806.1775v1	http://arxiv.org/pdf/0806.1775v1	2008	Particle size effect on strength, failure and shock behavior in   Polytetrafluoroethylene-Al-W granular composites	E. B. Herbold|V. F. Nesterenko|D. J. Benson|J. Cai|K. S. Vecchio|F. Jiang|J. W. Addiss|S. M. Walley|W. G. Proud	  The variation of metallic particle size and sample porosity significantly alters the dynamic mechanical properties of high density granular composites processed using a cold isostatically pressed mixture of polytetrafluoroethylene (PTFE), aluminum (Al) and tungsten (W) powders. Quasi-static and dynamic experiments are performed with identical constituent mass fractions with variations in the size of the W particles and pressing conditions. The relatively weak polymer matrix allows the strength and fracture modes of this material to be governed by the granular type behavior of agglomerated metal particles. A higher ultimate compressive strength was observed in relatively high porosity samples with small W particles compared to those with coarse W particles in all experiments. Mesoscale granular force chains comprised of the metallic particles explain this unusual phenomenon as observed in a hydrocode simulation of a drop-weight test. Macrocracks forming below the critical failure strain for the matrix and unusual behavior due to a competition between densification and fracture in dynamic tests of porous samples were also observed. Shock loading of this granular composite resulted in higher fraction of total internal energy deposition in the soft PTFE matrix, specifically thermal energy, which can be tailored by the W particle size distribution. 	
0907.4290v1	http://arxiv.org/pdf/0907.4290v1	2009	Dragon-Kings, Black Swans and the Prediction of Crises	Didier Sornette	  We develop the concept of ``dragon-kings'' corresponding to meaningful outliers, which are found to coexist with power laws in the distributions of event sizes under a broad range of conditions in a large variety of systems. These dragon-kings reveal the existence of mechanisms of self-organization that are not apparent otherwise from the distribution of their smaller siblings. We present a generic phase diagram to explain the generation of dragon-kings and document their presence in six different examples (distribution of city sizes, distribution of acoustic emissions associated with material failure, distribution of velocity increments in hydrodynamic turbulence, distribution of financial drawdowns, distribution of the energies of epileptic seizures in humans and in model animals, distribution of the earthquake energies). We emphasize the importance of understanding dragon-kings as being often associated with a neighborhood of what can be called equivalently a phase transition, a bifurcation, a catastrophe (in the sense of Rene Thom), or a tipping point. The presence of a phase transition is crucial to learn how to diagnose in advance the symptoms associated with a coming dragon-king. Several examples of predictions using the derived log-periodic power law method are discussed, including material failure predictions and the forecasts of the end of financial bubbles. 	
1005.3011v1	http://arxiv.org/pdf/1005.3011v1	2010	On the relevance of avoided crossings away from quantum critical point   to the complexity of quantum adiabatic algorithm	S. Knysh|V. Smelyanskiy	  Two recent preprints [B. Altshuler, H. Krovi, and J. Roland, "Quantum adiabatic optimization fails for random instances of NP-complete problems", arXiv:0908.2782 and "Anderson localization casts clouds over adiabatic quantum optimization", arXiv:0912.0746] argue that random 4th order perturbative corrections to the energies of local minima of random instances of NP-complete problem lead to avoided crossings that cause the failure of quantum adiabatic algorithm (due to exponentially small gap) close to the end, for very small transverse field that scales as an inverse power of instance size N. The theoretical portion of this work does not to take into account the exponential degeneracy of the ground and excited states at zero field. A corrected analysis shows that unlike those in the middle of the spectrum, avoided crossings at the edge would require high [O(1)] transverse fields, at which point the perturbation theory may become divergent due to quantum phase transition. This effect manifests itself only in large instances [exp(0.02 N) >> 1], which might be the reason it had not been observed in the authors' numerical work. While we dispute the proposed mechanism of failure of quantum adiabatic algorithm, we cannot draw any conclusions on its ultimate complexity. 	
1007.0656v5	http://arxiv.org/pdf/1007.0656v5	2010	Failure of the fluctuation-dissipation relation to ensure equilibrium	A Bhattacharyay	  Fluctuation-dissipation relation ensures thermodynamic equilibrium of a particle immersed in a heat bath. We will show that, under certain circumstances, the fluctuation-dissipation relation fails to ensure equilibrium between the immersed system and the heat bath. We consider a symmetry broken dimer, constrained to move in one dimension, is in compliance with the requirements of fluctuation-dissipation relation. An exact analytic result shows a nonzero average velocity of the center of mass of the dimer indicating that the state of the system is a nonequilibrium one. Based on this new physical observation, we propose an alternative paradigm for a Brownian motor which would extract useful energy directly from the heat bath unlike the ones based on Brownian Ratchet principle. 	
1104.3479v2	http://arxiv.org/pdf/1104.3479v2	2017	Reliability-based design optimization of shells with uncertain geometry   using adaptive Kriging metamodels	V. Dubourg|J. -M. Bourinet|B. Sudret	  Optimal design under uncertainty has gained much attention in the past ten years due to the ever increasing need for manufacturers to build robust systems at the lowest cost. Reliability-based design optimization (RBDO) allows the analyst to minimize some cost function while ensuring some minimal performances cast as admissible failure probabilities for a set of performance functions. In order to address real-world engineering problems in which the performance is assessed through computational models (e.g., finite element models in structural mechanics) metamodeling techniques have been developed in the past decade. This paper introduces adaptive Kriging surrogate models to solve the RBDO problem. The latter is cast in an augmented space that "sums up" the range of the design space and the aleatory uncertainty in the design parameters and the environmental conditions. The surrogate model is used (i) for evaluating robust estimates of the failure probabilities (and for enhancing the computational experimental design by adaptive sampling) in order to achieve the requested accuracy and (ii) for applying a gradient-based optimization algorithm to get optimal values of the design parameters. The approach is applied to the optimal design of ring-stiffened cylindrical shells used in submarine engineering under uncertain geometric imperfections. For this application the performance of the structure is related to buckling which is addressed here by means of a finite element solution based on the asymptotic numerical method. 	
1105.0562v2	http://arxiv.org/pdf/1105.0562v2	2011	Metamodel-based importance sampling for structural reliability analysis	V. Dubourg|F. Deheeger|B. Sudret	  Structural reliability methods aim at computing the probability of failure of systems with respect to some prescribed performance functions. In modern engineering such functions usually resort to running an expensive-to-evaluate computational model (e.g. a finite element model). In this respect simulation methods, which may require $10^{3-6}$ runs cannot be used directly. Surrogate models such as quadratic response surfaces, polynomial chaos expansions or kriging (which are built from a limited number of runs of the original model) are then introduced as a substitute of the original model to cope with the computational cost. In practice it is almost impossible to quantify the error made by this substitution though. In this paper we propose to use a kriging surrogate of the performance function as a means to build a quasi-optimal importance sampling density. The probability of failure is eventually obtained as the product of an augmented probability computed by substituting the meta-model for the original performance function and a correction term which ensures that there is no bias in the estimation even if the meta-model is not fully accurate. The approach is applied to analytical and finite element reliability problems and proves efficient up to 100 random variables. 	
1107.4785v1	http://arxiv.org/pdf/1107.4785v1	2011	A Novel Cyber-Insurance for Internet Security	Ranjan Pal|Leana Golubchik|Konstantinos Psounis	  Internet users such as individuals and organizations are subject to different types of epidemic risks such as worms, viruses, and botnets. To reduce the probability of risk, an Internet user generally invests in self-defense mechanisms like antivirus and antispam software. However, such software does not completely eliminate risk. Recent works have considered the problem of residual risk elimination by proposing the idea of cyber-insurance. In reality, an Internet user faces risks due to security attacks as well as risks due to non-security related failures (e.g., reliability faults in the form of hardware crash, buffer overflow, etc.) . These risk types are often indistinguishable by a naive user. However, a cyber-insurance agency would most likely insure risks only due to security attacks. In this case, it becomes a challenge for an Internet user to choose the right type of cyber-insurance contract as standard optimal contracts, i.e., contracts under security attacks only, might prove to be sub-optimal for himself. In this paper, we address the problem of analyzing cyber-insurance solutions when a user faces risks due to both, security as well as non-security related failures. We propose \emph{Aegis}, a novel cyber-insurance model in which the user accepts a fraction \emph{(strictly positive)} of loss recovery on himself and transfers rest of the loss recovery on the cyber-insurance agency. We mathematically show that given an option, Internet users would prefer Aegis contracts to traditional cyber-insurance contracts, under all premium types. This result firmly establishes the non-existence of traditional cyber-insurance markets when Aegis contracts are offered to users. 	
1201.0916v1	http://arxiv.org/pdf/1201.0916v1	2012	Device and method for investigation of mechanical properties of the   materials under high-strain rate tensile load	Sergey Lopatnikov|Nikolas Shevchenko|John W. Gillespie Jr	  A new apparatus and method is proposed for the investigation of material behavior under high-strain-rate tensile loads. We refer this apparatus as a Split Flying Bar. The method is based on using the inertia of a working mass attached to a specimen. The specimen is placed between the working mass (front part of the bar) and backing part of the bar, which is captured in flight by special brake. When the back part of the flying split bar is stopped, the working mass continues the flight by inertia, creating specimen tension with strain rate depending on the length of specimen and velocity of flying bar. Properly choosing the working mass and the speed of the flying bar, one can tune maximal stress and strain rate over a wide range. The method is highly scalable and can be used for investigation of specimens from single filament up to reasonable macroscopic size. Contrary to tensile SHPB, the method provides a way to investigate materials with practically arbitrary strain to failure. For example, polyuria, whose strain to failure reaches hundreds of percent. Also, the method can be used for high-strain rate pull-out tests or to measure the quality of adhesion layers under high-stress rates, e.t.c. In this paper I introduce the basics of the method and interpretation of the data. 	
1201.2439v1	http://arxiv.org/pdf/1201.2439v1	2012	Material point method simulations of fragmenting cylinders	Biswajit Banerjee	  Most research on the simulation of deformation and failure of metals has been and continues to be performed using the finite element method. However, the issues of mesh entanglement under large deformation, considerable complexity in handling contact, and difficulties encountered while solving large deformation fluid-structure interaction problems have led to the exploration of alternative approaches. The material point method uses Lagrangian solid particles embedded in an Eulerian grid. Particles interact via the grid with other particles in the same body, with other solid bodies, and with fluids. Thus, the three issues mentioned in the context of finite element analysis are circumvented.   In this paper, we present simulations of cylinders which fragment due to explosively expanding gases generated by reactions in a high energy material contained inside. The material point method is the numerical method chosen for these simulations discussed in this paper. The plastic deformation of metals is simulated using a hypoelastic-plastic stress update with radial return that assumes an additive decomposition of the rate of deformation tensor. Various plastic strain, plastic strain rate, and temperature dependent flow rules and yield conditions are investigated. Failure at individual material points is determined using porosity, damage and bifurcation conditions. Our models are validated using data from high strain rate impact experiments. It is concluded that the material point method possesses great potential for simulating high strain-rate, large deformation fluid-structure interaction problems. 	
1208.6116v1	http://arxiv.org/pdf/1208.6116v1	2012	A computational toy model for shallow landslides: Molecular Dynamics   approach	Gianluca Martelloni|Franco Bagnoli|Emanuele Massaro	  The aim of this paper is to propose a 2D computational algorithm for modeling of the trigger and the propagation of shallow landslides caused by rainfall. We used a Molecular Dynamics (MD) inspired model, similar to discrete element method (DEM), that is suitable to model granular material and to observe the trajectory of single particle, so to identify its dynamical properties. We consider that the triggering of shallow landslides is caused by the decrease of the static friction along the sliding surface due to water infiltration by rainfall. Thence the triggering is caused by two following conditions: (a) a threshold speed of the particles and (b) a condition on the static friction, between particles and slope surface, based on the Mohr-Coulomb failure criterion. The latter static condition is used in the geotechnical model to estimate the possibility of landslide triggering. Finally the interaction force between particles is defined trough a potential that, in the absence of experimental data, we have modeled as the Lennard-Jones 2-1 potential. In the model the viscosity is also introduced and for a large range of values of the model's parameters, we observe a characteristic velocity pattern, with acceleration increments, typical of real landslides. The results of simulations are quite promising: the energy and the time triggering distributions of local avalanches shows a power law distribution, analogous to the observed Gutenberg-Richter and Omori power law distributions for earthquakes. Finally it is possible to apply the method of the inverse surface displacement velocity [Fukuzono 1985] for predicting the failure time. 	
1402.4700v2	http://arxiv.org/pdf/1402.4700v2	2014	Slow slip and the transition from fast to slow fronts in the rupture of   frictional interfaces	Jørgen Kjoshagen Trømborg|Henrik Andersen Sveinsson|Julien Scheibert|Kjetil Thøgersen|David Skålid Amundsen|Anders Malthe-Sørenssen	  The failure of the population of micro-junctions forming the frictional interface between two solids is central to fields ranging from biomechanics to seismology. This failure is mediated by the propagation along the interface of various types of rupture fronts, covering a wide range of velocities. Among them are so-called slow fronts, which are recently discovered fronts much slower than the materials' sound speeds. Despite intense modelling activity, the mechanisms underlying slow fronts remain elusive. Here, we introduce a multi-scale model capable of reproducing both the transition from fast to slow fronts in a single rupture event and the short-time slip dynamics observed in recent experiments. We identify slow slip immediately following the arrest of a fast front as a phenomenon sufficient for the front to propagate further at a much slower pace. Whether slow fronts are actually observed is controlled both by the interfacial stresses and by the width of the local distribution of forces among micro-junctions. Our results show that slow fronts are qualitatively different from faster fronts. Since the transition from fast to slow fronts is potentially as generic as slow slip, we anticipate that it might occur in the wide range of systems in which slow slip has been reported, including seismic faults. 	
1404.2815v2	http://arxiv.org/pdf/1404.2815v2	2014	Usage leading to an abrupt collapse of connectivity	D. V. Stäger|N. A. M. Araújo|H. J. Herrmann	  Network infrastructures are essential for the distribution of resources such as electricity and water. Typical strategies to assess their resilience focus on the impact of a sequence of random or targeted failures of network nodes or links. Here we consider a more realistic scenario, where elements fail based on their usage. We propose a dynamic model of transport based on the Bak-Tang-Wiesenfeld sandpile model where links fail after they have transported more than an amount $\mu$ (threshold) of the resource and we investigate it on the square lattice. As we deal with a new model, we provide insight on its fundamental behavior and dependence on parameters. We observe that for low values of the threshold due to a positive feedback of link failure, an avalanche develops that leads to an abrupt collapse of the lattice. By contrast, for high thresholds the lattice breaks down in an uncorrelated fashion. We determine the critical threshold $\mu^*$ separating these two regimes and show how it depends on the toppling threshold of the nodes and the mass increment added stepwise to the system. We find that the time of major disconnection is well described with a linear dependence on $\mu$. Furthermore, we propose a lower bound for $\mu^*$ by measuring the strength of the dynamics leading to abrupt collapses. 	
1406.3053v2	http://arxiv.org/pdf/1406.3053v2	2014	Successes and failures of Hubbard-corrected density functional theory:   The case of Mg doped LiCoO$_2$	Juan A. Santana|Jeongnim Kim|P. R. C. Kent|Fernando A. Reboredo	  We have evaluated the successes and failures of the Hubbard-corrected density functional theory (DFT+U) approach to study Mg doping of LiCoO$_2$. We computed the effect of the U parameter on the energetic, geometric and electronic properties of two possible doping mechanisms: (1) substitution of Mg onto a Co (or Li) site with an associated impurity state and, (2) formation of impurity-state-free complexes of substitutional Mg and point defects in LiCoO$_2$. We find that formation of impurity states results in changes on the valency of Co in LiCoO$_2$. Variation of the Co U shifts the energy of the impurity state, resulting in energetic, geometric and electronic properties that depend significantly on the specific value of U. In contrast, the properties of the impurity-state-free complexes are insensitive to U. These results identify reasons for the strong dependence on the doping properties on the chosen value of U and for the overall difficulty of achieving agreement with the experimentally known energetic and electronic properties of doped transition metal oxides such as LiCoO$_2$. 	
1502.04848v2	http://arxiv.org/pdf/1502.04848v2	2015	Failing softly: A fracture theory of highly-deformable materials	Tamar Goldman Boué|Roi Harpaz|Jay Fineberg|Eran Bouchbinder	  Highly-deformable materials, from synthetic hydrogels to biological tissues, are becoming increasingly important from both fundamental and practical perspectives. Their mechanical behaviors, in particular the dynamics of crack propagation during failure, are not yet fully understood. Here we propose a theoretical framework for the dynamic fracture of highly-deformable materials, in which the effects of a dynamic crack are treated with respect to the nonlinearly deformed (pre-stressed/strained), non-cracked, state of the material. Within this framework, we derive analytic and semi-analytic solutions for the near-tip deformation fields and energy release rates of dynamic cracks propagating in incompressible neo-Hookean solids under biaxial and uniaxial loading. We show that moderately large pre-stressing has a marked effect on the stress fields surrounding a crack's tip. We verify these predictions by performing extensive experiments on the fracture of soft brittle elastomers over a range of loading levels and propagation velocities, showing that the newly developed framework offers significantly better approximations to the measurements than standard approaches at moderately large levels of external loadings and high propagation velocities. This framework should be relevant to the failure analysis of soft and tough, yet brittle, materials. 	
1505.05259v4	http://arxiv.org/pdf/1505.05259v4	2016	SAF: Stochastic Adaptive Forwarding in Named Data Networking	Daniel Posch|Benjamin Rainer|Hermann Hellwagner	  Forwarding decisions in classical IP-based networks are predetermined by routing. This is necessary to avoid loops, inhibiting opportunities to implement an adaptive and intelligent forwarding plane. Consequently, content distribution efficiency is reduced due to a lack of inherent multi-path transmission. In Named Data Networking (NDN) instead, routing shall hold a supporting role to forwarding, providing sufficient potential to enhance content dissemination at the forwarding plane. In this paper we design, implement, and evaluate a novel probability-based forwarding strategy, called Stochastic Adaptive Forwarding (SAF) for NDN. SAF imitates a self-adjusting water pipe system, intelligently guiding and distributing Interests through network crossings circumventing link failures and bottlenecks. Just as real pipe systems, SAF employs overpressure valves enabling congested nodes to lower pressure autonomously. Through an implicit feedback mechanism it is ensured that the fraction of the traffic forwarded via congested nodes decreases. By conducting simulations we show that our approach outperforms existing forwarding strategies in terms of the Interest satisfaction ratio in the majority of the evaluated scenarios. This is achieved by extensive utilization of NDN's multipath and content-lookup capabilities without relying on the routing plane. SAF explores the local environment by redirecting requests that are likely to be dropped anyway. This enables SAF to identify new paths to the content origin or to cached replicas, circumventing link failures and resource shortages without relying on routing updates. 	
1508.06854v1	http://arxiv.org/pdf/1508.06854v1	2015	How Evolution Learns to Generalise: Principles of under-fitting,   over-fitting and induction in the evolution of developmental organisation	Kostas Kouvaris|Jeff Clune|Louis Kounios|Markus Brede|Richard A. Watson	  One of the most intriguing questions in evolution is how organisms exhibit suitable phenotypic variation to rapidly adapt in novel selective environments which is crucial for evolvability. Recent work showed that when selective environments vary in a systematic manner, it is possible that development can constrain the phenotypic space in regions that are evolutionarily more advantageous. Yet, the underlying mechanism that enables the spontaneous emergence of such adaptive developmental constraints is poorly understood. How can natural selection, given its myopic and conservative nature, favour developmental organisations that facilitate adaptive evolution in future previously unseen environments? Such capacity suggests a form of \textit{foresight} facilitated by the ability of evolution to accumulate and exploit information not only about the particular phenotypes selected in the past, but regularities in the environment that are also relevant to future environments. Here we argue that the ability of evolution to discover such regularities is analogous to the ability of learning systems to generalise from past experience. Conversely, the canalisation of evolved developmental processes to past selective environments and failure of natural selection to enhance evolvability in future selective environments is directly analogous to the problem of over-fitting and failure to generalise in machine learning. We show that this analogy arises from an underlying mechanistic equivalence by showing that conditions corresponding to those that alleviate over-fitting in machine learning enhance the evolution of generalised developmental organisations under natural selection. This equivalence provides access to a well-developed theoretical framework that enables us to characterise the conditions where natural selection will find general rather than particular solutions to environmental conditions. 	
1601.02709v1	http://arxiv.org/pdf/1601.02709v1	2016	Dynamic signal tracking in a simple V1 spiking model	Guillaume Lajoie|Lai-Sang Young	  This work is part of an effort to understand the neural basis for our visual system's ability, or failure, to accurately track moving visual signals. We consider here a ring model of spiking neurons, intended as a simplified computational model of a single hypercolumn of the primary visual cortex. Signals that consist of edges with time-varying orientations localized in space are considered. Our model is calibrated to produce spontaneous and driven firing rates roughly consistent with experiments, and our two main findings, for which we offer dynamical explanation on the level of neuronal interactions, are the following: (1) We have documented consistent transient overshoots in signal perception following signal switches due to emergent interactions of the E- and I-populations, and (2) for continuously moving signals, we have found that accuracy is considerably lower at reversals of orientation than when continuing in the same direction (as when the signal is a rotating bar). To measure performance, we use two metrics, called fidelity and reliability, to compare signals reconstructed by the system to the ones presented, and to assess trial-to-trial variability. We propose that the same population mechanisms responsible for orientation selectivity also impose constraints on dynamic signal tracking that manifest in perception failures consistent with psychophysical observations. 	
1603.08635v1	http://arxiv.org/pdf/1603.08635v1	2016	Development and Validation of Functional Model of a Cruise Control   System	Avinash Visagan Varadarajan|Marcel Romijn|Bart Oosthoek|Joanna van de Mortel-Fronczak|Jos Beijer	  Modern automobiles can be considered as a collection of many subsystems working with each other to realize safe transportation of the occupants. Innovative technologies that make transportation easier are increasingly incorporated into the automobile in the form of functionalities. These new functionalities in turn increase the complexity of the system framework present and traceability is lost or becomes very tricky in the process. This hugely impacts the development phase of an automobile, in which, the safety and reliability of the automobile design should be ensured. Hence, there is a need to ensure operational safety of the vehicles while adding new functionalities to the vehicle. To address this issue, functional models of such systems are created and analysed. The main purpose of developing a functional model is to improve the traceability and reusability of a system which reduces development time and cost. Operational safety of the system is ensured by analysing the system with respect to random and systematic failures and including safety mechanism to prevent such failures. This paper discusses the development and validation of a functional model of a conventional cruise control system in a passenger vehicle based on the ISO 26262 Road Vehicles - Functional Safety standard. A methodology for creating functional architectures and an architecture of a cruise control system developed using the methodology are presented. 	
1604.02858v1	http://arxiv.org/pdf/1604.02858v1	2016	Microstructural topology effects on the onset of ductile failure in   multi-phase materials - a systematic computational approach	T. W. J. de Geus|R. H. J. Peerlings|M. G. D. Geers	  Multi-phase materials are key for modern engineering applications. They are generally characterized by a high strength and ductility. Many of these materials fail by ductile fracture of the, generally softer, matrix phase. In this work we systematically study the influence of the arrangement of the phases by correlating the microstructure of a two-phase material to the onset of ductile failure. A single topological feature is identified in which critical levels of damage are consistently indicated. It consists of a small region of the matrix phase with particles of the hard phase on both sides in a direction that depends on the applied deformation. Due to this configuration, a large tensile hydrostatic stress and plastic strain is observed inside the matrix, indicating high damage. This topological feature has, to some extent, been recognized before for certain multi-phase materials. This study however provides insight in the mechanics involved, including the influence of the loading conditions and the arrangement of the phases in the material surrounding the feature. Furthermore, a parameter study is performed to explore the influence of volume fraction and hardness of the inclusion phase. For the same macroscopic hardening response, the ductility is predicted to increase if the volume fraction of the hard phase increases while at the same time its hardness decreases. 	
1611.07012v3	http://arxiv.org/pdf/1611.07012v3	2017	GRAM: Graph-based Attention Model for Healthcare Representation Learning	Edward Choi|Mohammad Taha Bahadori|Le Song|Walter F. Stewart|Jimeng Sun	  Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two important challenges remain: -Data insufficiency:Often in healthcare predictive modeling, the sample size is insufficient for deep learning methods to achieve satisfactory results. -Interpretation:The representations learned by deep learning methods should align with medical knowledge. To address these challenges, we propose a GRaph-based Attention Model, GRAM that supplements electronic health records (EHR) with hierarchical information inherent to medical ontologies. Based on the data volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in the ontology via an attention mechanism. We compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart failure prediction task. Compared to the basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely observed in the training data and 3% improved area under the ROC curve for predicting heart failure using an order of magnitude less training data. Additionally, unlike other methods, the medical concept representations learned by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts when facing data insufficiency at the lower level concepts. 	
1707.01549v2	http://arxiv.org/pdf/1707.01549v2	2017	Topology determines force distributions in one-dimensional random spring   networks	Knut M. Heidemann|Andrew O. Sageman-Furnas|Abhinav Sharma|Florian Rehfeldt|Christoph F. Schmidt|Max Wardetzky	  Networks of elastic fibers are ubiquitous in biological systems and often provide mechanical stability to cells and tissues. Fiber reinforced materials are also common in technology. An important characteristic of such materials is their resistance to failure under load. Rupture occurs when fibers break under excessive force and when that failure propagates. Therefore it is crucial to understand force distributions. Force distributions within such networks are typically highly inhomogeneous and are not well understood. Here we construct a simple one-dimensional model system with periodic boundary conditions by randomly placing linear springs on a circle. We consider ensembles of such networks that consist of $N$ nodes and have an average degree of connectivity $z$, but vary in topology. Using a graph-theoretical approach that accounts for the full topology of each network in the ensemble, we show that, surprisingly, the force distributions can be fully characterized in terms of the parameters $(N,z)$. Despite the universal properties of such $(N,z)$-ensembles, our analysis further reveals that a classical mean-field approach fails to capture force distributions correctly. We demonstrate that network topology is a crucial determinant of force distributions in elastic spring networks. 	
1707.01996v1	http://arxiv.org/pdf/1707.01996v1	2017	Capacity of Wireless Distributed Storage Systems with Broadcast Repair	Ping Hu|Chi Wan Sung|Terence H. Chan	  In wireless distributed storage systems, storage nodes are connected by wireless channels, which are broadcast in nature. This paper exploits this unique feature to design an efficient repair mechanism, called broadcast repair, for wireless distributed storage systems in the presence of multiple-node failures. Due to the broadcast nature of wireless transmission, we advocate a new measure on repair performance called repair-transmission bandwidth. In contrast to repair bandwidth, which measures the average number of packets downloaded by a newcomer to replace a failed node, repair-transmission bandwidth measures the average number of packets transmitted by helper nodes per failed node. A fundamental study on the storage capacity of wireless distributed storage systems with broadcast repair is conducted by modeling the storage system as a multicast network and analyzing the minimum cut of the corresponding information flow graph. The fundamental tradeoff between storage efficiency and repair-transmission bandwidth is also obtained for functional repair. The performance of broadcast repair is compared both analytically and numerically with that of cooperative repair, the basic repair method for wired distributed storage systems with multiple-node failures. While cooperative repair is based on the idea of allowing newcomers to exchange packets, broadcast repair is based on the idea of allowing a helper to broadcast packets to all newcomers simultaneously. We show that broadcast repair outperforms cooperative repair, offering a better tradeoff between storage efficiency and repair-transmission bandwidth. 	
1708.08286v2	http://arxiv.org/pdf/1708.08286v2	2018	A Scalable and Extensible Checkpointing Scheme for Massively Parallel   Simulations	Nils Kohl|Johannes Hötzer|Florian Schornbaum|Martin Bauer|Christian Godenschwager|Harald Köstler|Britta Nestler|Ulrich Rüde	  Realistic simulations in engineering or in the materials sciences can consume enormous computing resources and thus require the use of massively parallel supercomputers. The probability of a failure increases both with the runtime and with the number of system components. For future exascale systems it is therefore considered critical that strategies are developed to make software resilient against failures. In this article, we present a scalable, distributed, diskless, and resilient checkpointing scheme that can create and recover snapshots of a partitioned simulation domain. We demonstrate the efficiency and scalability of the checkpoint strategy for simulations with up to $40$ billion computational cells executing on more than $400$ billion floating point values. A checkpoint creation is shown to require only a few seconds and the new checkpointing scheme scales almost perfectly up to more than $260\,000$ ($2^{18}$) processes. To recover from a diskless checkpoint during runtime, we realize the recovery algorithms using ULFM MPI. The checkpointing mechanism is fully integrated in a state-of-the-art high-performance multi-physics simulation framework. We demonstrate the efficiency and robustness of the method with a realistic phase-field simulation originating in the material sciences and with a lattice Boltzmann method implementation. 	
1710.00940v1	http://arxiv.org/pdf/1710.00940v1	2017	The "weak" interdependence of infrastructure systems produces mixed   percolation transitions in multilayer networks	Run-Ran Liu|Daniel A. Eisenberg|Thomas P. Seager|Ying-Cheng Lai	  In this work, we propose an interdependent, multilayer network model and percolation process that matches infrastructures better than previous models by allowing some nodes to survive when their interdependent neighbors fail. We consider a node-to-link failure propagation mechanism and establish "weak" interdependence across layers via a tolerance parameter $\alpha$ which quantifies the likelihood that a node survives when one of its interdependent neighbors fails. We measure the robustness of any individual layer by the final size of its giant component. Analytical and numerical results show that weak interdependence produces a striking phenomenon: layers at different positions within the multilayer system experience distinct percolation transitions. Especially, layers with high super degree values percolate in an abrupt manner, while those with low super degree values exhibit both continuous and abrupt transitions. This novel phenomenon we call \emph{mixed percolation transitions} has significant implications for network robustness. Previous results that do not consider cascade tolerance and layer super degree may be under- or over-estimating the vulnerability of real systems. Moreover, since $\alpha$ represents a generic measure of various risk management strategies used to buffer infrastructure assets from cascades, our model reveals how nodal protection activities influence failure dynamics in interdependent, multilayer systems. 	
1710.02345v1	http://arxiv.org/pdf/1710.02345v1	2017	Atomistic simulations on ductile-brittle transition in <111> BCC Fe   nanowires	G. Sainath|B. K. Choudhary	  Molecular dynamics simulations have been performed to understand the influence of temperature on the tensile deformation and fracture behavior of $<$111$>$ BCC Fe nanowires. The simulations have been carried out at different temperatures in the range 10-1000 K employing a constant strain rate of $1\times$ $10^8$ $s^{-1}$. The results indicate that at low temperatures (10-375 K), the nanowires yield through the nucleation of a sharp crack and fails in brittle manner. On the other hand, nucleation of multiple 1/2$<$111$>$ dislocations at yielding followed by significant plastic deformation leading to ductile failure has been observed at high temperatures in the range 450-1000 K. At the intermediate temperature of 400 K, the nanowire yields through nucleation of crack associated with many mobile 1/2$<$111$>$ and immobile $<$100$>$ dislocations at the crack tip and fails in ductile manner. The ductile-brittle transition observed in $<$111$>$ BCC Fe nanowires is appropriately reflected in the stress-strain behavior and plastic strain at failure. The ductile-brittle transition increases with increasing nanowire size. The change in fracture behavior has been discussed in terms of the relative variations in yield and fracture stresses and change in slip behavior with respect to temperature. Further, the dislocation multiplication mechanism assisted by the kink nucleation from the nanowire surface observed at high temperatures has been presented. 	
1712.05532v1	http://arxiv.org/pdf/1712.05532v1	2017	Soft modes and strain redistribution in continuous models of amorphous   plasticity: the Eshelby paradigm, and beyond?	Xiangyu Cao|Alexandre Nicolas|Denny Trimcev|Alberto Rosso	  The deformation of disordered solids relies on swift and localised rearrangements of particles. The inspection of soft vibrational modes can help predict the locations of these rearrangements, while the strain that they actually redistribute mediates collective effects. Here, we study soft modes and strain redistribution in a two-dimensional continuous mesoscopic model based on a Ginzburg-Landau free energy for perfect solids, supplemented with a plastic disorder potential that accounts for shear softening and rearrangements. Regardless of the disorder strength, our numerical simulations show soft modes that are always sharply peaked at the softest point of the material (unlike what happens for the depinning of an elastic interface). Contrary to widespread views, the deformation halo around this peak does not always have a quadrupolar (Eshelby-like) shape. Instead, for finite and narrowly-distributed disorder, it looks like a fracture, with a strain field that concentrates along some easy directions. These findings are rationalised with analytical calculations in the case where the plastic disorder is confined to a point-like `impurity'. In this case, we unveil a continuous family of elastic propagators, which are identical for the soft modes and for the equilibrium configurations. This family interpolates between the standard quadrupolar propagator and the fracture-like one as the anisotropy of the elastic medium is increased. Therefore, we expect to see a fracture-like propagator when extended regions on the brink of failure have already softened along the shear direction and thus rendered the material anisotropic, but not failed yet. We speculate that this might be the case in carefully aged glasses just before macroscopic failure. 	
1802.00245v3	http://arxiv.org/pdf/1802.00245v3	2018	Towards Reliable (and Efficient) Job Executions in a Practical   Geo-distributed Data Analytics System	Xiaoda Zhang|Zhuzhong Qian|Sheng Zhang|Yize Li|Xiangbo Li|Xiaoliang Wang|Sanglu Lu	  Geo-distributed data analytics are increasingly common to derive useful information in large organisations. Naive extension of existing cluster-scale data analytics systems to the scale of geo-distributed data centers faces unique challenges including WAN bandwidth limits, regulatory constraints, changeable/unreliable runtime environment, and monetary costs. Our goal in this work is to develop a practical geo-distribued data analytics system that (1) employs an intelligent mechanism for jobs to efficiently utilize (adjust to) the resources (changeable environment) across data centers; (2) guarantees the reliability of jobs due to the possible failures; and (3) is generic and flexible enough to run a wide range of data analytics jobs without requiring any changes.   To this end, we present a new, general geo-distributed data analytics system, HOUTU, that is composed of multiple autonomous systems, each operating in a sovereign data center. HOUTU maintains a job manager (JM) for a geo-distributed job in each data center, so that these replicated JMs could individually and cooperatively manage resources and assign tasks. Our experiments on the prototype of HOUTU running across four Alibaba Cloud regions show that HOUTU provides nearly efficient job performance as in the existing centralized architecture, and guarantees reliable job executions when facing failures. 	
1803.02695v1	http://arxiv.org/pdf/1803.02695v1	2018	The Altes Family of Log-Periodic Chirplets and the Hyperbolic Chirplet   Transform	Donnacha Daly|Didier Sornette	  This work revisits a class of biomimetically inspired log-periodic waveforms first introduced by R.A. Altes in the 1970s for generalized target description. It was later observed that there is a close connection between such sonar techniques and wavelet decomposition for multiresolution analysis. Motivated by this, we formalize the original Altes waveforms as a family of hyperbolic chirplets suitable for the detection of accelerating time-series oscillations. The formalism results in a remarkably flexible set of wavelets with desirable properties of admissibility, regularity, vanishing moments, and time-frequency localization. These "Altes wavelets" also facilitate efficient implementation of the scale invariant hyperbolic chirplet transform (HCT).   From a practical perspective, log-periodic oscillations with an acceleration towards criticality can serve as indicators of an incipient bifurcation. Such signals abound in nature, often as precursors to phase transitions in the non-linear dynamics of complex systems. For example, the authors' interest lies in automatic detection of the well documented phenomenon of log-periodic price dynamics during financial bubbles and preceding market crashes. However, the methodology presented here is more widely applicable in such diverse domains as prediction of critical failures in mechanical systems, and fault detection in electrical networks. Examples beyond failure diagnostics include animal species identification via call recordings, commercial \& military radar, and there are many more. A synthetic application is presented in this report for illustrative purposes. 	
0106047v1	http://arxiv.org/pdf/cond-mat/0106047v1	2001	Oscillatory Finite-Time Singularities in Finance, Population and Rupture	K. D. Ide|D. Sornette	  We present a simple two-dimensional dynamical system where two nonlinear terms, exerting respectively positive feedback and reversal, compete to create a singularity in finite time decorated by accelerating oscillations. The power law singularity results from the increasing growth rate. The oscillations result from the restoring mechanism. As a function of the order of the nonlinearity of the growth rate and of the restoring term, a rich variety of behavior is documented analytically and numerically. The dynamical behavior is traced back fundamentally to the self-similar spiral structure of trajectories in phase space unfolding around an unstable spiral point at the origin. The interplay between the restoring mechanism and the nonlinear growth rate leads to approximately log-periodic oscillations with remarkable scaling properties. Three domains of applications are discussed: (1) the stock market with a competition between nonlinear trend-followers and nonlinear value investors; (2) the world human population with a competition between a population-dependent growth rate and a nonlinear dependence on a finite carrying capacity; (3) the failure of a material subjected to a time-varying stress with a competition between positive geometrical feedback on the damage variable and nonlinear healing. 	
0106096v1	http://arxiv.org/pdf/cond-mat/0106096v1	2001	Statistical mechanics of complex networks	Reka Albert|Albert-Laszlo Barabasi	  Complex networks describe a wide range of systems in nature and society, much quoted examples including the cell, a network of chemicals linked by chemical reactions, or the Internet, a network of routers and computers connected by physical links. While traditionally these systems were modeled as random graphs, it is increasingly recognized that the topology and evolution of real networks is governed by robust organizing principles. Here we review the recent advances in the field of complex networks, focusing on the statistical mechanics of network topology and dynamics. After reviewing the empirical data that motivated the recent interest in networks, we discuss the main models and analytical tools, covering random graphs, small-world and scale-free networks, as well as the interplay between topology and the network's robustness against failures and attacks. 	
0608730v1	http://arxiv.org/pdf/cond-mat/0608730v1	2006	Nanoscale damage during fracture in silica glass	Daniel Bonamy|Silke Prades|Cindy Rountree|Laurent Ponson|Davy Dalmas|Elisabeth Bouchaud|K. Ravi-Chandar|Claude Guillot	  We report here atomic force microscopy experiments designed to uncover the nature of failure mechanisms occuring within the process zone at the tip of a crack propagating into a silica glass specimen under stress corrosion. The crack propagates through the growth and coalescence of nanoscale damage spots. This cavitation process is shown to be the key mechanism responsible for damage spreading within the process zone. The possible origin of the nucleation of cavities, as well as the implications on the selection of both the cavity size at coalescence and the process zone extension are finally discussed. 	
0206061v2	http://arxiv.org/pdf/physics/0206061v2	2002	Fundamental Disagreement of Wave Mechanics with Relativity	Ezzat G. Bakhoum	  A number of well-known difficulties in physics resulted from merging the theory of relativity with the Compton-de Broglie wave mechanics. Two such problems were the failure of Dirac's relativistic wave equation to predict the correct velocity of the electron, and the fact that the measured yield from nuclear fission was found to be substantially less than the theoretical yield. It is shown that the origin of these and other problems stem from the inconsistency of the relativistic mass-energy equivalence principle with the fundamental assumptions of wave mechanics. An alternative view of the concept of mass-energy equivalence that results in a very good agreement between theory and experiment is demonstrated. The conclusions of this paper will be quite important for ongoing research, such as the current problem of the neutrino's mass. 	
0509061v3	http://arxiv.org/pdf/quant-ph/0509061v3	2006	From Einstein's Theorem to Bell's Theorem: A History of Quantum   Nonlocality	H. M. Wiseman	  In this Einstein Year of Physics it seems appropriate to look at an important aspect of Einstein's work that is often down-played: his contribution to the debate on the interpretation of quantum mechanics. Contrary to popular opinion, Bohr had no defence against Einstein's 1935 attack (the EPR paper) on the claimed completeness of orthodox quantum mechanics. I suggest that Einstein's argument, as stated most clearly in 1946, could justly be called Einstein's reality-locality-completeness theorem, since it proves that one of these three must be false. Einstein's instinct was that completeness of orthodox quantum mechanics was the falsehood, but he failed in his quest to find a more complete theory that respected reality and locality. Einstein's theorem, and possibly Einstein's failure, inspired John Bell in 1964 to prove his reality-locality theorem. This strengthened Einstein's theorem (but showed the futility of his quest) by demonstrating that either reality or locality is a falsehood. This revealed the full nonlocality of the quantum world for the first time. 	
0711.2993v1	http://arxiv.org/pdf/0711.2993v1	2007	Fragmentation processes in impact of spheres	H. A. Carmona|F. K. Wittel|F. Kun|H. J. Herrmann	  We study the brittle fragmentation of spheres by using a three-dimensional Discrete Element Model. Large scale computer simulations are performed with a model that consists of agglomerates of many particles, interconnected by beam-truss elements. We focus on the detailed development of the fragmentation process and study several fragmentation mechanisms. The evolution of meridional cracks is studied in detail. These cracks are found to initiate in the inside of the specimen with quasi-periodic angular distribution. The fragments that are formed when these cracks penetrate the specimen surface give a broad peak in the fragment mass distribution for large fragments that can be fitted by a two-parameter Weibull distribution. This mechanism can only be observed in 3D models or experiments. The results prove to be independent of the degree of disorder in the model. Our results significantly improve the understanding of the fragmentation process for impact fracture since besides reproducing the experimental observations of fragment shapes, impact energy dependence and mass distribution, we also have full access to the failure conditions and evolution. 	
0712.3918v1	http://arxiv.org/pdf/0712.3918v1	2007	Digital Image Mechanical Identification (DIMI)	François Hild|Stéphane Roux	  A continuous pathway from digital images acquired during a mechanical test to quantitative identification of a constitutive law is presented herein based on displacement field analysis. From images, displacement fields are directly estimated within a finite element framework. From the latter, the application of the equilibrium gap method provides the means for rigidity field evaluation. In the present case, a reconditioned formulation is proposed for a better stability. Last, postulating a specific form of a damage law, a linear system is formed that gives a direct access to the (non-linear) damage growth law in one step. The two last procedures are presented, validated on an artificial case, and applied to the case of a biaxial tension of a composite sample driven up to failure. A quantitative estimate of the quality of the determination is proposed, and in the last application, it is shown that no more than 7% of the displacement field fluctuations are not accounted for by the determined damage law. 	
1011.2287v2	http://arxiv.org/pdf/1011.2287v2	2012	Causal Symmetry and the Transactional Interpretation	Peter W. Evans	  Cramer's (1986) transactional interpretation of quantum mechanics posits retrocausal influences in quantum processes in an attempt to alleviate some of the interpretational difficulties of the Copenhagen interpretation. In response to Cramer's theory, Maudlin (2002) has levelled a significant objection against any retrocausal model of quantum mechanics. I present here an examination of the transactional interpretation of quantum mechanics and an analysis of Maudlin's critique. I claim that, although Maudlin correctly isolates the weaknesses of Cramer's theory, his justification for this weakness is off the mark. The cardinal vice of the transactional interpretation is its failure to provide a sufficient causal structure to constrain uniquely the behaviour of quantum systems and I contend that this is due to a lack of causal symmetry in the theory. In contrast, Maudlin attributes this shortcoming to retrocausality itself and emphasises an apparently fundamental incongruence between retrocausality and his own metaphysical picture of reality. I conclude by arguing that the problematic aspect of this incongruence is Maudlin's assumptions about what is appropriate for such a metaphysical picture. 	
1102.0468v2	http://arxiv.org/pdf/1102.0468v2	2011	Gauge invariant accounts of the Higgs mechanism	Ward Struyve	  The Higgs mechanism gives mass to Yang-Mills gauge bosons. According to the conventional wisdom, this happens through the spontaneous breaking of gauge symmetry. Yet, gauge symmetries merely reflect a redundancy in the state description and therefore the spontaneous breaking can not be an essential ingredient. Indeed, as already shown by Higgs and Kibble, the mechanism can be explained in terms of gauge invariant variables, without invoking spontaneous symmetry breaking. In this paper, we present a general discussion of such gauge invariant treatments for the case of the Abelian Higgs model, in the context of classical field theory. We thereby distinguish between two different notions of gauge: one that takes all local transformations to be gauge and one that relates gauge to a failure of determinism. 	
1107.1952v1	http://arxiv.org/pdf/1107.1952v1	2011	Impairment of double exchange mechanism in electron transport of iron   pnictides	Lei Hao|Chi-Cheng Lee|T. K. Lee	  Double exchange mechanism is believed to favor transport along ferromagnetic directions, the failure of which in explaining the unusual resistivity anisotropy in iron pnictides is investigated. Several factors intrinsic to the microscopic mechanism of transport in iron pnictides are identified and analyzed, including the moderate Hund's coupling, low local moment, and presence of two anisotropic degenerate orbitals xz and yz. In particular, the substantial second neighbor hoppings are found to be decisive in giving results opposite to the double exchange picture. In high temperature nonmagnetic phase, orbital ordering is shown to give the right trend of resistivity anisotropy as observed experimentally, advocating its essential role in electron transport of iron pnictides. 	
1203.0083v1	http://arxiv.org/pdf/1203.0083v1	2012	Molecular Dynamics Study of the Mechanical Behavior of Few Layer   Graphene	Young In Jhon|Myung S. Jhon	  Atomistic simulation was performed to study the mechanical properties of few layer graphene (FLG) in conjunction with monlayer graphene (MLG) under uniaxial elongation by systematically increasing the layer number from one to six. We found that the ultimate tensile strength and strain increased in these FLGs for both zigzag and armchair-directional elongations when compared with the results of MLG. We also found that the largest increments were obtained in bi- or tri-layer graphene for all the FLG systems we studied. Using atomic stress distribution analysis, it is observed that the width of the distribution became narrower, thus the maximum stress decreased in FLG compared to MLG at respective stages of identical tensile stress. It indicates that locally-driven highly elevated atomic stress of FLG has been effectively relaxed to the atoms in other layers through cooperative interlayer interaction. This effect explains the reason for synergetic mechanical strengthening of FLG since tensile failure is critically influenced by maximum atomic stress. Furthermore, the Young's moduli were slightly smaller for all FLGs compared to MLG. 	
1204.6010v1	http://arxiv.org/pdf/1204.6010v1	2012	Correlations between mechanical, structural, and dynamical properties of   polymer nanocomposites	Kutvonen Aki|Rossi Giulia|Ala-Nissila Tapio	  We study the structural and dynamical mechanisms of reinforcement of a polymer nanocomposite (PNC) via coarse-grained molecular dynamics simulations. In a regime of strong polymer-filler interactions, the stress at failure of the PNC is clearly correlated to structural quantities, such as the filler loading, the surface area of the polymer-filler interface, and the network structure. Additionally, we find that small fillers, of the size of the polymer monomers, are the most effective at reinforcing the matrix by surrounding the polymer chains and maximizing the number of strong polymer-filler interactions. Such a structural configuration is correlated to a dynamical feature, namely, the minimization of the relative mobility of the fillers with respect to the polymer matrix. 	
1207.3591v3	http://arxiv.org/pdf/1207.3591v3	2012	Microscopic Mechanism of Shear Bands in Amorphous Solids	Ratul Dasgupta|H. George E. Hentschel|Itamar Procaccia	  The fundamental instability responsible for the shear localization which results in shear bands in amorphous solids remains unknown despite enormous amount of research, both experimental and theoretical. As this is the main mechanism for the failure of metallic glasses, understanding the instability is invaluable in finding how to stabilize such materials against the tendency to shear localize. In this Letter we explain the mechanism for shear localization under shear, which is the appearance of highly correlated lines of Eshelby-like quadrupolar singularities which organize the non-affine plastic flow of the amorphous solid into a shear band. We prove analytically that such highly correlated solutions in which $\C N$ quadrupoles are aligned with equal orientations are minimum energy states when the strain is high enough. The line lies at 45 degrees to the compressive stress. 	
1208.1184v1	http://arxiv.org/pdf/1208.1184v1	2012	Payment Rules through Discriminant-Based Classifiers	Paul Duetting|Felix Fischer|Pitchayut Jirapinyo|John K. Lai|Benjamin Lubin|David C. Parkes	  In mechanism design it is typical to impose incentive compatibility and then derive an optimal mechanism subject to this constraint. By replacing the incentive compatibility requirement with the goal of minimizing expected ex post regret, we are able to adapt statistical machine learning techniques to the design of payment rules. This computational approach to mechanism design is applicable to domains with multi-dimensional types and situations where computational efficiency is a concern. Specifically, given an outcome rule and access to a type distribution, we train a support vector machine with a special discriminant function structure such that it implicitly establishes a payment rule with desirable incentive properties. We discuss applications to a multi-minded combinatorial auction with a greedy winner-determination algorithm and to an assignment problem with egalitarian outcome rule. Experimental results demonstrate both that the construction produces payment rules with low ex post regret, and that penalizing classification errors is effective in preventing failures of ex post individual rationality. 	
1303.6260v1	http://arxiv.org/pdf/1303.6260v1	2013	E-HORM: An Energy-efficient Hole Removing Mechanism in Wireless Senor   Networks	M. B. Rasheed|N. Javaid|Z. A. Khan|U. Qasim|M. Ishfaq	  Cluster based routing protocols forWireless Sensor Networks (WSNs) have been widely used for better performance in terms of energy efficiency. Efficient use of energy is challenging task of designing these protocols. Energy holes are created due to quickly drain the energy of a few nodes due to nonuniform node distribution in the network. Normally, energy holes make the data routing failure when nodes transmit data back to the sink. We propose Energy-efficientHOle Removing Mechanism (E-HORM) technique to remove energy holes. In this technique, we use sleep and awake mechanism for sensor nodes to save energy. This approach finds the maximum distance nodes to calculate the maximum energy for data transmission. We consider it as a threshold energy Eth. Every node first checks its energy level for data transmission. If the energy level of node is less than Eth, it cannot transmit data. 	
1305.4032v1	http://arxiv.org/pdf/1305.4032v1	2013	Radiative Mechanisms in GRB prompt emission	Asaf Pe'er	  Motivated by the Fermi gamma-ray space telescope results, in recent years immense efforts were given to understanding the mechanism that leads to the prompt emission observed. The failure of the optically thin emission models (synchrotron and synchrotron self Compton) increased interest in alternative models. Optically thick models, while having several advantages, also face difficulty in capturing several key observables. Theoretical efforts are focused in two main directions: (1) mechanisms that act to broaden the Planck spectrum; and (2) combining the optically thin and optically thick models to a hybrid model that could explain the key observables. 	
1306.1448v1	http://arxiv.org/pdf/1306.1448v1	2013	I am 4 vho: new approach to improve seamless vertical hanover in   heterogeneous wireless networks	Omar Khattab|Omar Alani	  Two mechanisms have been proposed independently by IEEE and 3GPP; namely, Media Independent Handover (MIH) and Access Network Discovery and Selection Function (ANDSF), respectively. These mechanisms enable a seamless Vertical Handover (VHO) between the different types of technologies (3GPP and non-3GPP), such as GSM (Global System for Mobile Communication), Wireless Fidelity (Wi- Fi), Worldwide Interoperability for Microwave Access (WiMAX), Universal Mobile Telecommunications System (UMTS) and Long Term Evolution (LTE). In this paper, we overview these mechanisms and show their components, benefits and drawbacks. Then we present our Imperative Alternative MIH for Vertical Handover (I AM 4 VHO) approach based on the approaches that have been studied in the literature with better performance (packet loss and latency), less connection failure (probability of reject sessions), less complexity and more exhaustive for enhancing VHO heterogeneous wireless networks environment. 	
1308.1888v1	http://arxiv.org/pdf/1308.1888v1	2013	Strand-Based Approach to Patch Security Protocols	Dieter Hutter|Raul Monroy	  In this paper, we introduce a mechanism that aims to speed up the development cycle of security protocols, by adding automated aid for diagnosis and repair. Our mechanism relies on existing verification tools analyzing intermediate protocols and synthesizing potential attacks if the protocol is flawed. The analysis of these attacks (including type flaw attacks) pinpoints the source of the failure and controls the synthesis of appropriate patches to the protocol. Using strand spaces, we have developed general guidelines for protocol repair, and captured them into formal requirements on (sets of) protocol steps. For each requirement, there is a collection of rules that transform a set of protocol steps violating the requirement into a set conforming it. We have implemented our mechanism into a tool, called SHRIMP. We have successfully tested SHRIMP on numerous faulty protocols, all of which were successfully repaired, fully automatically. 	
1309.4990v1	http://arxiv.org/pdf/1309.4990v1	2013	"Superluminal paradox" in wavepacket propagation and its quantum   mechanical resolution	D. Sokolovski|E. Akhmatskaya	  We analyse in detail the reshaping mechanism leading to apparently "superluminal" advancement of a wave packet traversing a classically forbidden region. In the coordinate representation, a barrier is shown to act as an effective beamsplitter, recombining envelopes of the freely propagating pulse with various spacial shifts. Causality ensures that none of the constituent envelopes are advanced with respect to free propagation, yet the resulting pulse is advanced due to a peculiar interference effect, similar to the one responsible for "anomalous" values which occur in Aharonov's "weak measurements". In the momentum space, the effect is understood as a bandwidth phenomenon, where the incident pulse probes local, rather than global, analytical properties of the transmission amplitude T (p). The advancement is achieved when T (p) mimics locally an exponential behaviour, similar to the one occurring in Berry's "superoscillations". Seen in a broader quantum mechanical context, the "paradox" is but a consequence of an attempt to obtain "which way?" information without destroying the interference between the pathways of interest. This explains, to a large extent, the failure to adequately describe tunnelling in terms of a single "tunnelling time". 	
1407.1606v1	http://arxiv.org/pdf/1407.1606v1	2014	Elasticity and Plasticity in Stiff and Flexible Oligomeric Glasses	Oleg Gendelman|H. George E. Hentschel|Pankaj K. Mishra|Itamar Procaccia|Jacques Zylberg	  In this paper we focus on the mechanical properties of oligomeric glasses (waxes), employing a microscopic model that provides, via numerical simulations, information about the shear modulus of such materials, the failure mechanism via plastic instabilities and about the geometric responses of the oligomers themselves to a mechanical load. We present a microscopic theory that explains the numerically observed phenomena, including an exact theory of the shear modulus and of the plastic instabilities, both local and system spanning. In addition we present a model to explain the geometric changes in the oligomeric chains under increasing strains. 	
1407.5927v1	http://arxiv.org/pdf/1407.5927v1	2014	Strain localization in a nanocrystalline metal: Atomic mechanisms and   the effect of testing conditions	Timothy J. Rupert	  Molecular dynamics simulations are used to investigate strain localization in a model nanocrystalline metal. The atomic mechanisms of such catastrophic failure are first studied for two grain sizes of interest. Detailed analysis shows that the formation of a strain path across the sample width is crucial, and can be achieved entirely through grain boundary deformation or through a combination of grain boundary sliding and grain boundary dislocation emission. Pronounced mechanically-induced grain growth is also found within the strain localization region. The effects of testing conditions on strain localization are also highlighted, to understand the conditions that promote shear banding and compare these observations to metallic glass behavior. We observed that, while strain localization occurs at low temperatures and slow strain rates, a shift to more uniform plastic flow is observed when either strain rate or temperature is increased. We also explore how external sample dimensions influence strain localization, but find no size effect for the grain sizes and samples sizes studied here. 	
1408.4831v1	http://arxiv.org/pdf/1408.4831v1	2014	Self-replicating cracks: a collaborative fracture mode in thin films	Joel Marthelot|Benoit Roman|Jose Bico|Jeremie Teisseire|Davy Dalmas|Francisco Melo	  Straight cracks are observed in thin coatings under residual tensile stress, resulting into the classical network pattern observed in china crockery, old paintings or dry mud. Here, we present a novel fracture mechanism where delamination and propagation occur simultaneously, leading to the spontaneous self-replication of an initial template. Surprisingly, this mechanism is active below the standard critical tensile load for channel cracks and selects a robust interaction length scale on the order of 30 times the film thickness. Depending on triggering mechanisms, crescent alleys, spirals or long bands are generated over a wide range of experimental parameters. We describe with a simple physical model the selection of the fracture path and provide a configuration diagram displaying the different failure modes. 	
1411.5258v2	http://arxiv.org/pdf/1411.5258v2	2015	Neuronal Response Impedance Mechanism Implementing Cooperative Networks   with Low Firing Rates and Microseconds Precision	Roni Vardi|Amir Goldental|Hagar Marmari|Haya Brama|Edward Stern|Shira Sardi|Pinhas Sabo|Ido Kanter	  Realizations of low firing rates in neural networks usually require globally balanced distributions among excitatory and inhibitory links, while feasibility of temporal coding is limited by neuronal millisecond precision. We show that cooperation, governing global network features, emerges through nodal properties, as opposed to link distributions. Using in vitro and in vivo experiments we demonstrate microsecond precision of neuronal response timings under low stimulation frequencies, whereas moderate frequencies result in a chaotic neuronal phase characterized by degraded precision. Above a critical stimulation frequency, which varies among neurons, response failures were found to emerge stochastically such that the neuron functions as a low pass filter, saturating the average inter-spike-interval. This intrinsic neuronal response impedance mechanism leads to cooperation on a network level, such that firing rates are suppressed towards the lowest neuronal critical frequency simultaneously with neuronal microsecond precision. Our findings open up opportunities of controlling global features of network dynamics through few nodes with extreme properties. 	
1501.00360v1	http://arxiv.org/pdf/1501.00360v1	2015	Critical length limiting super-low friction	Ming Ma|Andrea Benassi|Andrea Vanossi|Michael Urbakh	  Since the demonstration of super-low friction (superlubricity) in graphite at nanoscale, one of the main challenges in the field of nano- and micro-mechanics was to scale this phenomenon up. A key question to be addressed is to what extent superlubricity could persist, and what mechanisms could lead to its failure. Here, using an edge-driven Frenkel-Kontorova model, we establish a connection between the critical length above which superlubricity disappears and both intrinsic material properties and experimental parameters. A striking boost in dissipated energy with chain length emerges abruptly due to a high-friction stick-slip mechanism caused by deformation of the slider leading to a local commensuration with the substrate lattice. We derived a parameter-free analytical model for the critical length that is in excellent agreement with our numerical simulations. Our results provide a new perspective on friction and nano-manipulation and can serve as a theoretical basis for designing nano-devices with super-low friction, such as carbon nanotubes. 	
1507.03779v3	http://arxiv.org/pdf/1507.03779v3	2016	Failure of the Volume Function in Granular Statistical Mechanics and an   Alternative Formulation	Raphael Blumenfeld|Shahar Amitai|Joe F. Jordan|Rebecca Hihinashvili	  We first show that the currently accepted statistical mechanics for granular matter is flawed. The reason is that it is based on the volume function, which depends only on a minute fraction of all the structural degrees of freedom and is unaffected by most of the configurational microstates. Consequently, the commonly used partition function underestimates the entropy severely. We then propose a new formulation, replacing the volume function with a ${\it connectivity}$ function that depends on all the structural degrees of freedom and accounts correctly for the entire entropy. We discuss the advantages of the new formalism and derive explicit results for two- and three-dimensional systems. We test the formalism by calculating the entropy of an experimental two-dimensional system, as a function of system size, and showing that it is an extensive variable. 	
1509.01950v1	http://arxiv.org/pdf/1509.01950v1	2015	Hierarchical structures for a robustness-oriented capacity design	Enrico Masoero|Falk K. Wittel|Hans J. Herrmann|B. M. Chiaia	  In this paper, we study the response of 2D framed structures made of rectangular cells, to the sudden removal of columns. We employ a simulation algorithm based on the Discrete Element Method, where the structural elements are represented by elasto-plastic Euler Bernoulli beams with elongation-rotation failure threshold. The effect of structural cell slenderness and of topological hierarchy on the dynamic residual strength after damage $\ROne$ is investigated. Topologically \textit{hierarchical} frames have a primary structure made of few massive elements, while \textit{homogeneous} frames are made of many thin elements. We also show how $\ROne$ depends on the activated collapse mechanisms, which are determined by the mechanical hierarchy between beams and columns, i.e. by their relative strength and stiffness. Finally, principles of robustness-oriented capacity design which seem to be in contrast to the conventional anti-seismic capacity design are addressed. 	
1512.04562v1	http://arxiv.org/pdf/1512.04562v1	2015	Multiscale modelling of tumour growth induced by circadian rhythm   disruption in epithelial tissue	D. A. Bratsun|D. V. Merkuriev|A. P. Zakharov|L. M. Pismen	  We propose a multiscale chemo-mechanical model of cancer tumour development in an epithelial tissue. The model is based on transformation of normal cells into the cancerous state triggered by a local failure of spatial synchronisation of the circadian rhythm. The model includes mechanical interactions and chemical signal exchange between neighbouring cells, as well as division of cells and intercalation, and allows for modification of the respective parameters following transformation into the cancerous state. The numerical simulations reproduce different dephasing patterns - spiral waves and quasistationary clustering, with the latter being conducive to cancer formation. Modification of mechanical properties reproduces distinct behaviour of invasive and localised carcinoma. 	
1602.06139v1	http://arxiv.org/pdf/1602.06139v1	2016	Toward a virtual material for lifetime prediction of CMCs	Martin Genet|Pierre Ladevèze|Gilles Lubineau|Emmanuel Baranger|A Mouret	  A first version of a multi-scale, multi-physic hybrid model --called virtual material-- for predictions on Self-Healing Ceramic Matrix Composite's (CMCs) lifetime is presented. The model has a mechanical and a chemical part, which are presented here in their actual state of development. The mechanical part provides precise data for the chemical models through an hybrid --melting continuum damage macro model discrete crack surfaces-- representation of the morphology of the crack network at yarn scale. The chemical part should provide predictions on the structure's lifetime using a model of the self-healing process, not yet achieved then not presented here, and a model of fiber sub-critical failure under mechanical and chemical load. 	
1608.04788v2	http://arxiv.org/pdf/1608.04788v2	2016	High anisotropy of fully hydrogenated borophene	Zhiqiang Wang|Tie-Yu Lü|Hui-Qiong Wang|Yuan-Ping Feng|Jin-Cheng Zheng	  We have studied the mechanical properties and phonon dispersions of fully hydrogenated borophene (borophane) under strains by first principles calculations. Uniaxial tensile strains along the a- and b-direction, respectively, and biaxial tensile strain have been considered. Our results show that the mechanical properties and phonon stability of borophane are both highly anisotropic. The ultimate tensile strain along the a-direction is only 0.12, but it can be as large as 0.30 along the b-direction. Compared to borophene and other 2D materials (graphene, graphane, silicene, silicane, h-BN, phosphorene and MoS2), borophane presents the most remarkable anisotropy in in-plane ultimate strain, which is very important for strain engineering. Furthermore, the phonon dispersions under the three applied strains indicate that borophane can withstand up to 5% and 15% uniaxial tensile strain along the a- and b-direction, respectively, and 9% biaxial tensile strain, indicating that mechanical failure in borophane is likely to originate from phonon instability. 	
1611.04189v3	http://arxiv.org/pdf/1611.04189v3	2016	Emergent gravity from Eguchi-Kawai reduction	Edgar Shaghoulian	  Holographic theories with a local gravitational dual have a number of striking features. Here I argue that many of these features are controlled by the Eguchi-Kawai mechanism, which is proposed to be a hallmark of such holographic theories. Higher-spin holographic duality is presented as a failure of the Eguchi-Kawai mechanism, and its restoration illustrates the deformation of higher-spin theory into a proper string theory with a local gravitational limit. AdS/CFT is used to provide a calculable extension of the Eguchi-Kawai mechanism to field theories on curved manifolds and thereby introduce "topological volume independence." Finally, I discuss implications for a general understanding of the extensivity of the Bekenstein-Hawking-Wald entropy. 	
1707.07473v3	http://arxiv.org/pdf/1707.07473v3	2017	Verifying Policy Enforcers	Oliviero Riganelli|Daniela Micucci|Leonardo Mariani|Yliès Falcone	  Policy enforcers are sophisticated runtime components that can prevent failures by enforcing the correct behavior of the software. While a single enforcer can be easily designed focusing only on the behavior of the application that must be monitored, the effect of multiple enforcers that enforce different policies might be hard to predict. So far, mechanisms to resolve interferences between enforcers have been based on priority mechanisms and heuristics. Although these methods provide a mechanism to take decisions when multiple enforcers try to affect the execution at a same time, they do not guarantee the lack of interference on the global behavior of the system. In this paper we present a verification strategy that can be exploited to discover interferences between sets of enforcers and thus safely identify a-priori the enforcers that can co-exist at run-time. In our evaluation, we experimented our verification method with several policy enforcers for Android and discovered some incompatibilities. 	
0001023v1	http://arxiv.org/pdf/physics/0001023v1	2000	Measurement of mechanical vibrations excited in aluminium resonators by   0.6 GeV electrons	G. D. van Albada|E. Coccia|V. Fafone|H. van der Graaf|G. Heijboer|J. W. van Holten|W. J. Kasdorp|J. B. van der Laan|L. Lapikas|G. Mazzitelli|G. J. L. Nooren|C. W. J. Noteboom|J. E. J. Oberski|G. Pallottino|H. Z. Peek|F. Ronga|A. Schimmel|T. G. B. W. Sluijk|P. Steman|J. Venema|P. K. A. de Witt Huberts	  We present measurements of mechanical vibrations induced by 0.6 GeV electrons impinging on cylindrical and spherical aluminium resonators. To monitor the amplitude of the resonator's vibrational modes we used piezoelectric ceramic sensors, calibrated by standard accelerometers. Calculations using the thermo-acoustic conversion model, agree well with the experimental data, as demonstrated by the specific variation of the excitation strengths with the absorbed energy, and with the traversing particles' track positions. For the first longitudinal mode of the cylindrical resonator we measured a conversion factor of 7.4 +- 1.4 nm/J, confirming the model value of 10 nm/J. Also, for the spherical resonator, we found the model values for the L=2 and L=1 mode amplitudes to be consistent with our measurement. We thus have confirmed the applicability of the model, and we note that calculations based on the model have shown that next generation resonant mass gravitational wave detectors can only be expected to reach their intended ultra high sensitivity if they will be shielded by an appreciable amount of rock, where a veto detector can reduce the background of remaining impinging cosmic rays effectively. 	
0706.0403v1	http://arxiv.org/pdf/0706.0403v1	2007	Asymptotic Behavior of Total Times For Jobs That Must Start Over If a   Failure Occurs	Soeren Asmussen|Pierre Fiorini|Lester Lipsky|Tomasz Rolski|Robert Sheahan	  Many processes must complete in the presence of failures. Different systems respond to task failure in different ways. The system may resume a failed task from the failure point (or a saved checkpoint shortly before the failure point), it may give up on the task and select a replacement task from the ready queue, or it may restart the task. The behavior of systems under the first two scenarios is well documented, but the third ({\em RESTART}) has resisted detailed analysis. In this paper we derive tight asymptotic relations between the distribution of {\em task times} without failures to the {\em total time} when including failures, for any failure distribution. In particular, we show that if the task time distribution has an unbounded support then the total time distribution $H$ is always heavy-tailed. Asymptotic expressions are given for the tail of $H$ in various scenarios. The key ingredients of the analysis are the Cram\'er--Lundberg asymptotics for geometric sums and integral asymptotics, that in some cases are obtained via Tauberian theorems and in some cases by bare-hand calculations. 	
0810.3438v1	http://arxiv.org/pdf/0810.3438v1	2008	Efficient Algorithms and Routing Protocols for Handling Transient Single   Node Failures	Amit M Bhosle|Teofilo F Gonzalez	  Single node failures represent more than 85% of all node failures in the today's large communication networks such as the Internet. Also, these node failures are usually transient. Consequently, having the routing paths globally recomputed does not pay off since the failed nodes recover fairly quickly, and the recomputed routing paths need to be discarded. Instead, we develop algorithms and protocols for dealing with such transient single node failures by suppressing the failure (instead of advertising it across the network), and routing messages to the destination via alternate paths that do not use the failed node. We compare our solution to that of Ref. [11] wherein the authors have presented a "Failure Insensitive Routing" protocol as a proactive recovery scheme for handling transient node failures. We show that our algorithms are faster by an order of magnitude while our paths are equally good. We show via simulation results that our paths are usually within 15% of the optimal for randomly generated graph with 100-1000 nodes. 	
1012.4025v1	http://arxiv.org/pdf/1012.4025v1	2010	Optimal adaptive control of cascading power grid failures	Daniel Bienstock	  We present theoretical results and experiments with parallel algorithms for computing an adaptive, online control with the objective of attenuating a power grid cascading failure. 	
1108.3831v1	http://arxiv.org/pdf/1108.3831v1	2011	A remark on the failure of multiplicity one for GSp(4)	Daniel File|Ramin Takloo-Bighash	  We revisit a classical result of Howe and Pitatski-Shapiro on the failure of strong multiplicity one for $\GSp(4)$. 	
1407.3286v1	http://arxiv.org/pdf/1407.3286v1	2014	Solvability-Based Comparison of Failure Detectors	Srikanth Sastry|Josef Widder	  Failure detectors are oracles that have been introduced to provide processes in asynchronous systems with information about faults. This information can then be used to solve problems otherwise unsolvable in asynchronous systems. A natural question is on the "minimum amount of information" a failure detector has to provide for a given problem. This question is classically addressed using a relation that states that a failure detector D is stronger (that is, provides "more, or better, information") than a failure detector D' if D can be used to implement D'. It has recently been shown that this classic implementability relation has some drawbacks. To overcome this, different relations have been defined, one of which states that a failure detector D is stronger than D' if D can solve all the time-free problems solvable by D'. In this paper we compare the implementability-based hierarchy of failure detectors to the hierarchy based on solvability. This is done by introducing a new proof technique for establishing the solvability relation. We apply this technique to known failure detectors from the literature and demonstrate significant differences between the hierarchies. 	
1409.2223v1	http://arxiv.org/pdf/1409.2223v1	2014	Assessment of classification techniques on predicting success or failure   of Software reusability	Nahid Hajizadeh|Manijeh Keshtgari|Marzieh Ahmadzadeh	  Assessment of classification techniques on predicting success or failure of Software reusability 	
1708.09494v1	http://arxiv.org/pdf/1708.09494v1	2017	An Exploratory Study of Field Failures	Luca Gazzola|Leonardo Mariani|Fabrizio Pastore|Mauro Pezz`e	  Field failures, that is, failures caused by faults that escape the testing phase leading to failures in the field, are unavoidable. Improving verification and validation activities before deployment can identify and timely remove many but not all faults, and users may still experience a number of annoying problems while using their software systems. This paper investigates the nature of field failures, to understand to what extent further improving in-house verification and validation activities can reduce the number of failures in the field, and frames the need of new approaches that operate in the field. We report the results of the analysis of the bug reports of five applications belonging to three different ecosystems, propose a taxonomy of field failures, and discuss the reasons why failures belonging to the identified classes cannot be detected at design time but shall be addressed at runtime. We observe that many faults (70%) are intrinsically hard to detect at design-time. 	
1803.00384v1	http://arxiv.org/pdf/1803.00384v1	2018	Fibres of Failure: Classifying errors in predictive processes	Leo Carlsson|Gunnar Carlsson|Mikael Vejdemo-Johansson	  We describe Fibres of Failure (FiFa), a method to classify failure modes of predictive processes using the Mapper algorithm from Topological Data Analysis.   Our method uses Mapper to build a graph model of input data stratified by prediction error.   Groupings found in high-error regions of the Mapper model then provide distinct failure modes of the predictive process.   We demonstrate FiFa on misclassifications of MNIST images with added noise, and demonstrate two ways to use the failure mode classification: either to produce a correction layer that adjusts predictions by similarity to the failure modes; or to inspect members of the failure modes to illustrate and investigate what characterizes each failure mode. 	
0612141v1	http://arxiv.org/pdf/cs/0612141v1	2006	Exact Failure Frequency Calculations for Extended Systems	Annie Druault-Vicard|Christian Tanguy	  This paper shows how the steady-state availability and failure frequency can be calculated in a single pass for very large systems, when the availability is expressed as a product of matrices. We apply the general procedure to $k$-out-of-$n$:G and linear consecutive $k$-out-of-$n$:F systems, and to a simple ladder network in which each edge and node may fail. We also give the associated generating functions when the components have identical availabilities and failure rates. For large systems, the failure rate of the whole system is asymptotically proportional to its size. This paves the way to ready-to-use formulae for various architectures, as well as proof that the differential operator approach to failure frequency calculations is very useful and straightforward. 	
0906.1328v1	http://arxiv.org/pdf/0906.1328v1	2009	Multidimensional Analysis of System Logs in Large-scale Cluster Systems	Wei Zhou|Jianfeng Zhan|Dan Meng	  It is effective to improve the reliability and availability of large-scale cluster systems through the analysis of failures. Existed failure analysis methods understand and analyze failures from one or few dimension. The analysis results are partial and with less precision because of the limitation of data source. This paper presents multidimensional analysis based on graph mining to analyze multi-source system logs, which is a promising failure analysis method to get more complete and precise failure knowledge. 	
1012.2411v1	http://arxiv.org/pdf/1012.2411v1	2010	Measurement of Reciprocity Failure in Near Infrared Detectors	T. Biesiadzinski|W. Lorenzon|R. Newman|M. Schubnell|G. Tarle|C. Weaverdyck	  Flux dependent non-linearity (reciprocity failure) in HgCdTe near infrared detectors can severely impact an instrument's performance, in particular with respect to precision photometric measurements. The cause of this effect is presently not understood. To investigate reciprocity failure, a dedicated test system was built. For flux levels between 1 and 50,000 photons/s, a sensitivity to reciprocity failure of approximately 0.1%/decade was achieved. A wavelength independent non-linearity due to reciprocity failure of about 0.35%/decade was measured in a 1.7 micron HgCdTe detector. 	
1106.1090v1	http://arxiv.org/pdf/1106.1090v1	2011	Reciprocity Failure in HgCdTe Detectors: Measurements and Mitigation	T. Biesiadzinski|W. Lorenzon|R. Newman|M. Schubnell|G. Tarle|C. Weaverdyck	  A detailed study of reciprocity failure in four 1.7 micron cutoff HgCdTe near-infrared detectors is presented. The sensitivity to reciprocity failure is approximately 0.1%\decade over up to five orders of magnitude in illumination intensity. The four detectors, which represent three successive production runs with modified growth recipes, show large differences in amount and spatial structure of reciprocity failure. Reciprocity failure could be reduced to negligible levels by cooling the detectors to about 110 K. No wavelength dependence was observed. The observed spatial structure appears to be weakly correlated with image persistence. 	
1208.5029v1	http://arxiv.org/pdf/1208.5029v1	2012	Probability of Failure in Hypersonic Engines Using Large Deviations	George Papanicolaou|Nicholas West|Tzu-Wei Yang	  We consider a reduced order model of an air-breathing hypersonic engine with a time-dependent stochastic inflow that may cause the failure of the engine. The probability of failure is analyzed by the Freidlin-Wentzell theory, the large deviation principle for finite dimensional stochastic differential equations. We compute the asymptotic failure probability by numerically solving the constrained optimization related to the large deviation problem. A large-deviation-based importance sampling suggested by the most probable inflow perturbation is also implemented to compute the probability of failure of the engine. The numerical simulations show that the importance sampling method is much more efficient than the basic Monte Carlo method. 	
1402.5899v1	http://arxiv.org/pdf/1402.5899v1	2014	Structural failure of two-density-layer cohesionless biaxial ellipsoids	Masatoshi Hirabayashi	  This paper quantitatively evaluates structural failure of biaxial cohesionless ellipsoids that have a two-density-layer distribution. The internal density layer is modeled as a sphere, while the external density layer is the rest of the part. The density is supposed to be constant in each layer. The present study derives averaged stresses over the whole volume of these bodies and uses limit analysis to determine their global failure. The upper bound condition of global failure is considered in terms of the size of the internal layer and the aspect ratio of the shape. The result shows that the two-density-layer causes the body to have different strength against structural failure. 	
1404.7565v2	http://arxiv.org/pdf/1404.7565v2	2014	Investigating SCADA Failures in Interdependent Critical Infrastructure   Systems	Razgar Ebrahimy	  This paper is based on the initial ideas of a PhD proposal which will investigate SCADA failures in physical infrastructure systems. The results will be used to develop a new notation to help risk assessment using dependable computing concepts. SCADA systems are widely used within critical infrastructures to perform system controls and deliver services to linked and dependent systems. Failures in SCADA systems will be investigated to help us understand and prevent cascading failures in future. 	
1709.10166v1	http://arxiv.org/pdf/1709.10166v1	2017	Emergent failures and cascades in power grids: a statistical physics   perspective	Tommaso Nesti|Alessandro Zocca|Bert Zwart	  We consider complex networks where line failures occur indirectly as line flows are influenced by fluctuating input at nodes, a prime example being a power grid where power is generated by renewable sources. We examine the propagation of such emergent failures in the small noise regime, combining concepts from statistical physics and the physics of power flow. In particular we characterize rigorously and explicitly the configuration of inputs responsible for failures and cascades, and analyze the propagation of failures, which often is not of nearest-neighbor type. 	
1605.09350v1	http://arxiv.org/pdf/1605.09350v1	2016	Computing backup forwarding rules in Software-Defined Networks	Niels L. M. van Adrichem|Farabi Iqbal|Fernando A. Kuipers	  The past century of telecommunications has shown that failures in networks are prevalent. Although much has been done to prevent failures, network nodes and links are bound to fail eventually. Failure recovery processes are therefore needed. Failure recovery is mainly influenced by (1) detection of the failure, and (2) circumvention of the detected failure. However, especially in SDNs where controllers recompute network state reactively, this leads to high delays. Hence, next to primary rules, backup rules should be installed in the switches to quickly detour traffic once a failure occurs. In this work, we propose algorithms for computing an all-to-all primary and backup network forwarding configuration that is capable of circumventing link and node failures. Omitting the high delay invoked by controller recomputation through preconfiguration, our proposal's recovery delay is close to the detection time which is significantly below the 50 ms rule of thumb. After initial recovery, we recompute network configuration to guarantee protection from future failures. Our algorithms use packet-labeling to guarantee correct and shortest detour forwarding. The algorithms and labeling technique allow packets to return to the primary path and are able to discriminate between link and node failures. The computational complexity of our solution is comparable to that of all-to-all-shortest paths computations. Our experimental evaluation on both real and generated networks shows that network configuration complexity highly decreases compared to classic disjoint paths computations. Finally, we provide a proof-of-concept OpenFlow controller in which our proposed configuration is implemented, demonstrating that it readily can be applied in production networks. 	
1608.04002v2	http://arxiv.org/pdf/1608.04002v2	2016	Survivable Cloud Network Design Against Multiple Failures Through   Protecting Spanning Trees	Zhili Zhou|Tachun Lin|Krishnaiyan Thulasiraman	  Survivable design of cross-layer networks, such as the cloud computing infrastructure, lies in its resource deployment and allocation and mapping of the logical (virtual datacenter/IP) network into the physical infrastructure (cloud backbone/WDM) such that link or node failure(s) in the physical infrastructure would not result in cascading failures in the logical network. Most of the prior approaches for survivable cross-layer network design aim at single-link failure scenario, which are not applicable to the more challenging multi-failure scenarios. Also, as many of these approaches use the cross-layer cut concept, enumeration of all cuts in the network is required and thus introducing exponential number of constraints. To overcome these difficulties, we investigate in this paper survivable mapping approaches against multiple physical link failures and its special case, Shared Risk Link Group (SRLG) failure. We present the necessary and sufficient conditions based on both cross-layer spanning trees and cutsets to guarantee a survivable mapping when multiple physical link failures occur. Based on the necessary and sufficient conditions, we propose to solve the problem through (1) mixed-integer linear programs which avoid enumerating all combinations of link failures, and (2) an algorithm which generates/adds logical spanning trees sequentially. Our simulation results show that the proposed approaches can produce survivable mappings effectively against both $k$- and SRLG-failures. 	
0902.2121v1	http://arxiv.org/pdf/0902.2121v1	2009	The 1/r singularity in weakly nonlinear fracture mechanics	Eran Bouchbinder|Ariel Livne|Jay Fineberg	  Material failure by crack propagation essentially involves a concentration of large displacement-gradients near a crack's tip, even at scales where no irreversible deformation and energy dissipation occurs. This physical situation provides the motivation for a systematic gradient expansion of general nonlinear elastic constitutive laws that goes beyond the first order displacement-gradient expansion that is the basis for linear elastic fracture mechanics (LEFM). A weakly nonlinear fracture mechanics theory was recently developed by considering displacement-gradients up to second order. The theory predicts that, at scales within a dynamic lengthscale $\ell$ from a crack's tip, significant $\log{r}$ displacements and $1/r$ displacement-gradient contributions arise. Whereas in LEFM the $1/r$ singularity generates an unbalanced force and must be discarded, we show that this singularity not only exists but is {\em necessary} in the weakly nonlinear theory. The theory generates no spurious forces and is consistent with the notion of the autonomy of the near-tip nonlinear region. The J-integral in the weakly nonlinear theory is also shown to be path-independent, taking the same value as the linear elastic J-integral. Thus, the weakly nonlinear theory retains the key tenets of fracture mechanics, while providing excellent quantitative agreement with measurements near the tip of single propagating cracks. As $\ell$ is consistent with lengthscales that appear in crack tip instabilities, we suggest that this theory may serve as a promising starting point for resolving open questions in fracture dynamics. 	
1301.1614v1	http://arxiv.org/pdf/1301.1614v1	2013	Polycrystal model of the mechanical behavior of a Mo-TiC30vol.%   metal-ceramic composite using a 3D microstructure map obtained by a dual beam   FIB-SEM	Denis Cédat|Olivier Fandeur|Colette Rey|Dierk Raabe	  The mechanical behavior of a Mo-TiC30 vol.% ceramic-metal composite was investigated over a large temperature range (25^{\circ}C to 700^{\circ}C). High-energy X-ray tomography was used to reveal the percolation of the hard titanium carbide phase through the composite. Using a polycrystal approach for a two-phase material, finite element simulations were performed on a real 3D aggregate of the material. The 3D microstructure, used as starting configuration for the predictions, was obtained by serial-sectioning in a dual beam Focused Ion Beam (FIB)-Scanning Electron Microscope (SEM) coupled to an Electron Back Scattering Diffraction system (3D EBSD, EBSD tomography). The 3D aggregate consists of a molybdenum matrix and a percolating TiC skeleton. As most BCC metals, the molybdenum matrix phase is characterized by a change in the plasticity mechanisms with temperature. We used a polycrystal model for the BCC material, which was extended to two phases (TiC and Mo). The model parameters of the matrix were determined from experiments on pure molydenum. For all temperatures investigated, the TiC particles were considered as brittle. Gradual damage of the TiC particles was treated, based on an accumulative failure law that is approximated by an evolution of the apparent particle elastic stiffness. The model enabled us to determine the evolution of the local mechanical fields with deformation and temperature. We showed that a 3D aggregate representing the actual microstructure of the composite is required to understand the local and global mechanical properties of the studied composite. 	
1308.4696v1	http://arxiv.org/pdf/1308.4696v1	2013	Mechanical properties of hydrogen functionalized graphene allotropes	Yinfeng Li|Dibakar Datta|Zhonghua Li|Vivek B. Shenoy	  Molecular dynamics (MD) simulations have been performed to investigate the mechanical properties of hydrogen functionalized graphene allotropes (GAs) for H-coverage spanning the entire range (0-100%). Four allotropes (graphyne, cyclic graphene, octagonal graphene, and biphenylene) with larger unit lattice size than graphene are considered. The effect of the degree of functionalization and molecular structure on the Young's modulus and strength are investigated, and the failure processes of some new GAs are reported for the first time. We show that the mechanical properties of the hydrogenated GAs deteriorate drastically with increasing H-coverage within the sensitive threshold, beyond which the mechanical properties remain insensitive to the increase in H-coverage. This drastic deterioration arises both from the conversion of sp2 to sp3 bonding and easy rotation of unsupported sp3 bonds. Allotropes with different lattice structures correspond to different sensitive thresholds. The Young's moduli deterioration of fully hydrogenated allotropes can be up to 70% smaller than that of the corresponding pristine structure. Moreover the tensile strength shows an even larger drop of about 90% and higher sensitivity to H-coverage even if it is small. Our results suggest that the unique coverage-dependent deterioration of the mechanical properties must be taken into account when analyzing the performance characteristics of nanodevices fabricated from functionalized GAs. 	
1506.07792v1	http://arxiv.org/pdf/1506.07792v1	2015	Strain-controlled criticality governs the nonlinear mechanics of fibre   networks	A. Sharma|A. J. Licup|R. Rens|M. Sheinman|K. A. Jansen|G. H. Koenderink|F. C. MacKintosh	  Disordered fibrous networks are ubiquitous in nature as major structural components of living cells and tissues. The mechanical stability of networks generally depends on the degree of connectivity: only when the average number of connections between nodes exceeds the isostatic threshold are networks stable (Maxwell, J. C., Philosophical Magazine 27, 294 (1864)). Upon increasing the connectivity through this point, such networks undergo a mechanical phase transition from a floppy to a rigid phase. However, even sub-isostatic networks become rigid when subjected to sufficiently large deformations. To study this strain-controlled transition, we perform a combination of computational modeling of fibre networks and experiments on networks of type I collagen fibers, which are crucial for the integrity of biological tissues. We show theoretically that the development of rigidity is characterized by a strain-controlled continuous phase transition with signatures of criticality. Our experiments demonstrate mechanical properties consistent with our model, including the predicted critical exponents. We show that the nonlinear mechanics of collagen networks can be quantitatively captured by the predictions of scaling theory for the strain-controlled critical behavior over a wide range of network concentrations and strains up to failure of the material. 	
1511.03477v1	http://arxiv.org/pdf/1511.03477v1	2015	Two-dimensional Graphene Heterojunctions: the Tunable Mechanical   Properties	Kang Xia|Haifei Zhan|Yuantong Gu	  We report the mechanical properties of different two-dimensional carbon heterojunctions (HJs) made from graphene and various stable graphene allotropes, including {\alpha}-, {\beta}-, {\gamma}- and 6612-graphyne (GY), and graphdiyne (GDY). It is found that all HJs exhibit a brittle behaviour except the one with {\alpha}-GY, which however shows a hardening process due to the formation of triple carbon rings. Such hardening process has greatly deferred the failure of the structure. The yielding of the HJs is usually initiated at the interface between graphene and graphene allotropes, and monoatomic carbon rings are normally formed after yielding. By varying the locations of graphene (either in the middle or at the two ends of the HJs), similar mechanical properties have been obtained, suggesting insignificant impacts from location of graphene allotropes. Whereas, changing the types and percentages of the graphene allotropes, the HJs exhibit vastly different mechanical properties. In general, with the increasing graphene percentage, the yield strain decreases and the effective Young's modulus increases. Meanwhile, the yield stress appears irrelevant with the graphene percentage. This study provides a fundamental understanding of the tensile properties of the heterojunctions that are crucial for the design and engineering of their mechanical properties, in order to facilitate their emerging future applications in nanoscale devices, such as flexible/stretchable electronics. 	
1603.02862v1	http://arxiv.org/pdf/1603.02862v1	2016	Is the World Local or Nonlocal? Towards an Emergent Quantum Mechanics in   the 21st Century	Jan Walleczek|Gerhard Groessing	  What defines an emergent quantum mechanics (EmQM)? Can new insight be advanced into the nature of quantum nonlocality by seeking new links between quantum and emergent phenomena as described by self-organization, complexity, or emergence theory? Could the development of a future EmQM lead to a unified, relational image of the cosmos? One key motivation for adopting the concept of emergence in relation to quantum theory concerns the persistent failure in standard physics to unify the two pillars in the foundations of physics: quantum theory and general relativity theory (GRT). The total contradiction in the foundational, metaphysical assumptions that define orthodox quantum theory versus GRT might render inter-theoretic unification impossible. On the one hand, indeterminism and non-causality define orthodox quantum mechanics, and, on the other hand, GRT is governed by causality and determinism. How could these two metaphysically-contradictory theories ever be reconciled? The present work argues that metaphysical contradiction necessarily implies physical contradiction. The contradictions are essentially responsible also for the measurement problem in quantum mechanics. A common foundation may be needed for overcoming the contradictions between the two foundational theories. The concept of emergence, and the development of an EmQM, might help advance a common foundation - physical and metaphysical - as required for successful inter-theory unification. 	
1605.06719v3	http://arxiv.org/pdf/1605.06719v3	2017	(Modular) Effect Algebras are Equivalent to (Frobenius) Antispecial   Algebras	Dusko Pavlovic|Peter-Michael Seidel	  Effect algebras are one of the generalizations of Boolean algebras proposed in the quest for a quantum logic. Frobenius algebras are a tool of categorical quantum mechanics, used to present various families of observables in abstract, often nonstandard frameworks. Both effect algebras and Frobenius algebras capture their respective fragments of quantum mechanics by elegant and succinct axioms; and both come with their conceptual mysteries. A particularly elegant and mysterious constraint, imposed on Frobenius algebras to characterize a class of tripartite entangled states, is the antispecial law. A particularly contentious issue on the quantum logic side is the modularity law, proposed by von Neumann to mitigate the failure of distributivity of quantum logical connectives. We show that, if quantum logic and categorical quantum mechanics are formalized in the same framework, then the antispecial law of categorical quantum mechanics corresponds to the natural requirement of effect algebras that the units are each other's unique complements; and that the modularity law corresponds to the Frobenius condition. These correspondences lead to the equivalence announced in the title. Aligning the two formalisms, at the very least, sheds new light on the concepts that are more clearly displayed on one side than on the other (such as e.g. the orthogonality). Beyond that, it may also open up new approaches to deep and important problems of quantum mechanics (such as the classification of complementary observables). 	
1703.00113v2	http://arxiv.org/pdf/1703.00113v2	2017	Modeling of internal mechanical failure of all-solid-state batteries   during electrochemical cycling, and implications for battery design	Giovanna Bucci|Tushar Swamy|Yet-Ming Chiang|W. Craig Carter	  This is the first quantitative analysis of mechanical reliability of all-solid state batteries. Mechanical degradation of the solid electrolyte (SE) is caused by intercalation-induced expansion of the electrode particles, within the constrain of a dense microstructure. A coupled electro-chemo-mechanical model was implemented to quantify the material properties that cause a SE to fracture. The treatment of microstructural details is essential to the understanding of stress-localization phenomena and fracture. A cohesive zone model is employed to simulate the evolution of damage. In the numerical tests, fracture is prevented only if electrode-particle's expansion is lower than 7.5% and the solid-electrolyte's fracture energy higher than $G_c = 4$ J m$^{-2}$. Perhaps counter-intuitively, the analyses show that compliant solid electrolytes (with Young's modulus in the order of E$_{SE} = 15$ GPa) are more prone to micro-cracking. This result, captured by our non-linear kinematics model, contradicts the speculations that sulfide SEs are more suitable for the design of bulk-type batteries than oxide SEs. Mechanical degradation is linked to the battery power-density. Fracture in solid Li-ion conductors represents a barrier for Li transport, and accelerates the decay of rate performance. 	
1704.08605v1	http://arxiv.org/pdf/1704.08605v1	2017	Failsafe Mechanism Design of Multicopters Based on Supervisory Control   Theory	Quan Quan|Zhiyao Zhao|Liyong Lin|Peng Wang|Walter Murray Wonham|Kai-Yuan Cai	  In order to handle undesirable failures of a multicopter which occur in either the pre-flight process or the in-flight process, a failsafe mechanism design method based on supervisory control theory is proposed for the semi-autonomous control mode. Failsafe mechanism is a control logic that guides what subsequent actions the multicopter should take, by taking account of real-time information from guidance, attitude control, diagnosis, and other low-level subsystems. In order to design a failsafe mechanism for multicopters, safety issues of multicopters are introduced. Then, user requirements including functional requirements and safety requirements are textually described, where function requirements determine a general multicopter plant, and safety requirements cover the failsafe measures dealing with the presented safety issues. In order to model the user requirements by discrete-event systems, several multicopter modes and events are defined. On this basis, the multicopter plant and control specifications are modeled by automata. Then, a supervisor is synthesized by monolithic supervisory control theory. In addition, we present three examples to demonstrate the potential blocking phenomenon due to inappropriate design of control specifications. Also, we discuss the meaning of correctness and the properties of the obtained supervisor. This makes the failsafe mechanism convincingly correct and effective. Finally, based on the obtained supervisory controller generated by TCT software, an implementation method suitable for multicopters is presented, in which the supervisory controller is transformed into decision-making codes. 	
9512025v1	http://arxiv.org/pdf/cond-mat/9512025v1	1995	Quantitative Characterization of Permeability Fluctuations in Sandstone	Hernán A. Makse|Glenn W. Davies|Shlomo Havlin|Plamen Ch. Ivanov|Peter R. King|H. E. Stanley	  Sedimentary rocks have complicated permeability fluctuations arising from the geological processes that formed them. These permeability fluctuations significantly affect the flow of fluids through the rocks. We analyze data on two sandstone samples from different geological environments, and find that the permeability fluctuations display long-range power-law correlations characterized by an exponent $H$. For both samples, we find $H \approx 0.82-0.90$. 	
9708100v1	http://arxiv.org/pdf/cond-mat/9708100v1	1997	A Growth Model for Porous Sedimentary Rocks	Sujata Tarafdar|Shashwati Roy	  A growth model for porous sedimentary rocks is proposed, using a simple computer simulation algorithm. We generate the structure by ballistic deposition of particles with a bimodal size distribution. Porosity and specific surface area are calculated varying the proportion of small and larger particles. Permeability and its variation with porosity are studied. The fractal nature of the pore space is also discussed. 	
0004061v3	http://arxiv.org/pdf/cond-mat/0004061v3	2000	Ratchet effect in dc SQUIDs	S. Weiss|D. Koelle|J. Mueller|K. Barthel|R. Gross	  We analyzed voltage rectification for dc SQUIDs biased with ac current with zero mean value. We demonstrate that the reflection symmetry in the 2-dimensional SQUID potential is broken by an applied flux and with appropriate asymmetries in the dc SQUID. Depending on the type of asymmetry, we obtain a rocking or a simultaneously rocking and flashing ratchet, the latter showing multiple sign reversals in the mean voltage with increasing amplitude of the ac current. Our experimental results are in agreement with numerical solutions of the Langevin equations for the asymmetric dc SQUID. 	
0312543v1	http://arxiv.org/pdf/cond-mat/0312543v1	2003	Simultaneous Measurement of Rock Permeability and Effective Porosity   using Laser-Polarized Noble Gas NMR	R. Wang|R. W. Mair|M. S. Rosen|D. G. Cory|R. L. Walsworth	  We report simultaneous measurements of the permeability and effective porosity of oil-reservoir rock cores using one-dimensional NMR imaging of the penetrating flow of laser-polarized xenon gas. The permeability result agrees well with industry standard techniques, whereas effective porosity is not easily determined by other methods. This novel NMR technique may have applications to the characterization of fluid flow in a wide variety of porous and granular media. 	
0507017v2	http://arxiv.org/pdf/cond-mat/0507017v2	2005	3-junction SQUID rocking ratchet	A. Sterck|R. Kleiner|D. Koelle	  We investigate 3-junction SQUIDs which show voltage rectification if biased with an ac current drive with zero mean value. The Josephson phase across the SQUID experiences an effective ratchet potential, and the device acts as an efficient rocking ratchet, as demonstrated experimentally for adiabatic and nonadiabatic drive frequencies. For high-frequency drives the rectified voltage is quantized due to synchronization of the phase dynamics with the external drive. The experimental data are in excellent agreement with numerical simulations including thermal fluctuations. 	
9809006v1	http://arxiv.org/pdf/hep-ex/9809006v1	1998	Study of Photo-Nuclear Interaction of muons in rock with the MACRO   experiment	G. Battistoni	  We present first results about the measurement of the characteristics of charged hadrons production by atmospheric muons in the rock above MACRO. Selection criteria which allow to discriminate hadron cascades from e.m. showers generated by muons are described. A comparison between the measured rate with that expected from a Monte Carlo simulation which treats the process as dominated by photo-nuclear interaction is presented. These data can be used to validate such models aiming to the evaluation of hadron background from cosmic muons in different experimental environments. 	
0003007v1	http://arxiv.org/pdf/hep-ex/0003007v1	2000	Ratio of Electron Scattering from Nuclei to Deuterium at low x and low   Momentum Transfer	Stephen Rock|Peter Bosted	  We have extracted ratios of cross sections for scattering electrons from high mass targets compared to low mass targets in the region of x ~ 0.02 and Q^2 < 1 (GeV/c)^2 from SLAC experiments performed over the past three decades. Additional analysis was needed for radiative corrections, target end caps and calibration runs. We observe no significant difference in the nuclear ratio for low Q^2 compared to results at Q^2 > 1. 	
0106084v1	http://arxiv.org/pdf/hep-ex/0106084v1	2001	Measuring the Phases of G_E and G_M of the Nucleon	Stephen Rock	  The nucleon electromagnetic form factors G_E and G_M are complex quantities in the time-like region. The absolute values can be determined by measuring the angular distribution of the nucleons in e^+ e^- --> N N-bar. The complex phase can only be determined by measuring one or more polarizations of the initial or final state. For PEP-N, we can use unpolarized e+ and e- and measure the polarization of one of the outgoing nucleons. 	
0201017v1	http://arxiv.org/pdf/hep-ex/0201017v1	2002	Measurement of the ratio double/single muon events as a function of rock   depth with MACRO	M. Sioli|for the MACRO Collaboration	  We report the measurement by the MACRO experiment of the ratio of double muon events over single muon events as a function of rock depth. Particular attention has been devoted to the analysis of high zenith angle events. Results are compared to the expectation of a detailed simulation performed with HEMAS-DPM Monte Carlo. No deviations with respect to ``standard physics'' predictions have been found. 	
9706436v1	http://arxiv.org/pdf/hep-ph/9706436v1	1997	High Momentum Transfer HERA Events and pQCD at High x	Stephen Rock|Peter Bosted	  We compare data on the proton structure function from SLAC experiments in the range of x between 0.7 and 0.97 in and near the resonance region with previous empirical fits to Deep Inelastic Scattering and with calculations from parton distribution functions. The data is in rough agreement with the empirical fits, but is an order of magnitude higher than the pQCD calculations at the highest x. This compares with the two orders of magnitude increase in the quark distributions at high x which seem to be necessary to explain the HERA high momentum transfer events. 	
0004073v3	http://arxiv.org/pdf/hep-ph/0004073v3	2001	Charm Production by Cosmic Muons	Francesco Vissani	  Narrow muon bundles in underground detectors permit to study muoproduction reactions that take place in the surrounding rock. We analyze the relevance of a QED+QCD reaction, muoproduction of "open charm". The contribution to double muon events is estimated to be 4-8 % of the one due to QED "trident" process, for an ideal detector located under a rock depth of 3 km water equivalent, and an observation threshold of 1 GeV. 	
0412377v1	http://arxiv.org/pdf/hep-ph/0412377v1	2004	Charged lepton-nucleus inelastic scattering at high energies	K. S. Kuzmin|K. S. Lokhtin|S. I. Sinegovsky	  The composite model is constructed to describe inelastic high-energy scattering of muons and taus in standard rock. It involves photonuclear interactions at low $Q^2$ as well as moderate $Q^2$ processes and the deep inelastic scattering (DIS). In the DIS region the neutral current contribution is taken into consideration. Approximation formulas both for the muons and tau energy loss in standard rock are presented for wide energy range. 	
0610020v1	http://arxiv.org/pdf/q-bio/0610020v1	2006	Quartet consistency count method for reconstructing phylogenetic trees	Jin-Hwan Cho|Dosang Joe|Young Rock Kim	  Among the distance based algorithms in phylogenetic tree reconstruction, the neighbor-joining algorithm has been a widely used and effective method. We propose a new algorithm which counts the number of consistent quartets for cherry picking with tie breaking. We show that the success rate of the new algorithm is almost equal to that of neighbor-joining. This gives an explanation of the qualitative nature of neighbor-joining and that of dissimilarity maps from DNA sequence data. Moreover, the new algorithm always reconstructs correct trees from quartet consistent dissimilarity maps. 	
0611015v1	http://arxiv.org/pdf/q-bio/0611015v1	2006	Phylogenetic tree constructing algorithms fit for grid computing with   SVD	Young Rock Kim|Oh-In Kwon|Seong-Hun Paeng|Chun-Jae Park	  Erikkson showed that singular value decomposition(SVD) of flattenings determined a partition of a phylogenetic tree to be a split. In this paper, based on his work, we develop new statistically consistent algorithms fit for grid computing to construct a phylogenetic tree by computing SVD of flattenings with the small fixed number of rows. 	
0705.3699v1	http://arxiv.org/pdf/0705.3699v1	2007	The Cascades Proposal for the Deep Underground Science and Engineering   Laboratory	W. C. Haxton|J. F. Wilkerson	  One of the options for creating a Deep Underground Science and Engineering Laboratory (DUSEL) is a site in the Mt. Stuart batholith, a granodiorite and tonalite rock mass in the Cascade mountain range in Washington State. The batholith's 100-year history in hard-rock tunneling includes the construction of the longest and deepest tunnels in the U.S., the parallel Cascade and Pioneer tunnels. The laboratory plan would utilize these two tunnels to produce a laboratory that has many desirable features, including dedicated, clean, horizontal access, container-module transport, and low operations costs. Various aspects of the site help to reduce geotechnical, environmental, and safety risks. 	
0804.4314v1	http://arxiv.org/pdf/0804.4314v1	2008	Optimal driving waveform for overdamped, adiabatic rocking ratchets	Steven J. Lade	  As a first step in the project of ratchet optimisation, the optimal driving waveform among a wide class of admissible functions for an overdamped, adiabatic rocking ratchet is shown to be dichotomous. `Optimum' is defined as that which achieves the maximum (or minimum negative) average particle velocity. Implications for the design of ratchets, for example in nanotechnological transport, may follow. The main result is applicable to a general class of adiabatic responses. 	
0810.4635v1	http://arxiv.org/pdf/0810.4635v1	2008	Muon simulation codes MUSIC and MUSUN for underground physics	V. A. Kudryavtsev	  The paper describes two Monte Carlo codes dedicated to muon simulations: MUSIC (MUon SImulation Code) and MUSUN (MUon Simulations UNderground). MUSIC is a package for muon transport through matter. It is particularly useful for propagating muons through large thickness of rock or water, for instance from the surface down to underground/underwater laboratory. MUSUN is designed to use the results of muon transport through rock/water to generate muons in or around underground laboratory taking into account their energy spectrum and angular distribution. 	
1102.0626v1	http://arxiv.org/pdf/1102.0626v1	2011	High pressure synthesis of FeO-ZnO solid solutions with rock salt   structure: in situ X-ray diffraction studies	P. S. Sokolov|A. N. Baranov|C. Lathe|V. L. Solozhenko	  X-ray diffraction with synchrotron radiation has been used for the first time to study chemical interaction in the FeO-ZnO system at 4.8 GPa and temperatures up to 1300 K. Above 750 K, the chemical reaction between FeO and ZnO has been observed that resulted in the formation of rock salt (rs) Fe1-xZnxO solid solutions (0.3 \leq x \leq 0.85). The lattice parameters of these solid solutions have been in situ measured as a function of temperature under pressure, and corresponding thermal expansion coefficients have been calculated. 	
1107.0074v1	http://arxiv.org/pdf/1107.0074v1	2011	Testing Machine for Expansive Mortar	Romulo Augusto Ventura Silva	  The correct evaluation of a material property is fundamental to, on their application; they met all expectations that were designed for. In development of an expansive cement for ornamental rocks purpose, was denoted the absence of methodologies and equipments to evaluate the expansive pressure and temperature of expansive cement during their expansive process, having that data collected in a static state of the specimen. In that paper, is described equipment designed for evaluation of pressure and temperature of expansive cements applied to ornamental rocks. 	
1111.6309v1	http://arxiv.org/pdf/1111.6309v1	2011	Rocky core solubility in Jupiter and giant exoplanets	Hugh F. Wilson|Burkhard Militzer	  Gas giants are believed to form by the accretion of hydrogen-helium gas around an initial protocore of rock and ice. The question of whether the rocky parts of the core dissolve into the fluid H-He layers following formation has significant implications for planetary structure and evolution. Here we use ab initio calculations to study rock solubility in fluid hydrogen, choosing MgO as a representative example of planetary rocky materials, and find MgO to be highly soluble in H for temperatures in excess of approximately 10000 K, implying significant redistribution of rocky core material in Jupiter and larger exoplanets. 	
1112.5068v1	http://arxiv.org/pdf/1112.5068v1	2011	Experimental and theoretical evidence for pressure-induced metallization   in FeO with the rock-salt type structure	Kenji Ohta|R. E. Cohen|Kei Hirose|Kristjan Haule|Katsuya Shimizu|Yasuo Ohishi	  Electrical conductivity of FeO was measured up to 141 GPa and 2480 K in a laserheated diamond-anvil cell. The results show that rock-salt (B1) type structured FeO metallizes at around 70 GPa and 1900 K without any structural phase transition. We computed fully self-consistently the electronic structure and the electrical conductivity of B1 FeO as a function of pressure and temperature, and found that although insulating as expected at ambient condition, B1 FeO metallizes at high temperatures, consistent with experiments. The observed metallization is related to spin crossover. 	
1507.08145v2	http://arxiv.org/pdf/1507.08145v2	2015	From coin-tossing to rock-paper-scissors and beyond: A log-exp gap   theorem for selecting a leader	Michael Fuchs|Hsien-Kuei Hwang|Yoshiaki Itoh	  A class of games for finding a leader among a group of candidates is studied in detail. This class covers games based on coin-tossing and rock-paper-scissors as special cases and its complexity exhibits similar stochastic behaviors: either of logarithmic mean and bounded variance or of exponential mean and exponential variance. Many applications are also discussed. 	
1511.00351v1	http://arxiv.org/pdf/1511.00351v1	2015	Cosmic Ray Induced EM Showers in the NO$ν$A Detectors	Hongyue Duyang	  The NO$\nu$A experiment is an electron neutrino appearance neutrino oscillation experiment at Fermilab. Electron neutrino events are identified by the electromagnetic (EM) showers induced by electrons in the final state of neutrino interactions. EM showers induced by cosmic muons or rock muons, are abundant in NO$\nu$A detectors. We use a Muon-Removal Technique to get pure EM shower samples from cosmic and rock muon data. Those samples can be used to characterize the EM signature and provide valuable checks of the MC simulation, reconstruction, PID algorithms, and calibration across the NO$\nu$A detectors. 	
1601.07235v1	http://arxiv.org/pdf/1601.07235v1	2016	Interval Estimation for the 'Net Promoter Score'	Brendan Rocks	  The Net Promoter Score (NPS) is a novel summary statistic used by thousands of companies as a key performance indicator of customer loyalty. While adoption of the statistic has grown rapidly over the last decade, there has been little published on its statistical properties. Common interval estimation techniques are adapted for use with the NPS, and performance assessed on the largest available database of companies' Net Promoter Scores. Variations on the Adjusted Wald, and an iterative Score test are found to have superior performance. 	
1606.04141v1	http://arxiv.org/pdf/1606.04141v1	2016	A Lecture on Integration by Parts	John A. Rock	  Integration by parts (IBP) has acquired a bad reputation. While it allows us to compute a wide variety of integrals when other methods fall short, its implementation is often seen as plodding and confusing. Readers familiar with tabular IBP understand that, in particular cases, it has the capacity to significantly streamline and shorten computation. In this paper, we establish a tabular approach to IBP that is completely general and, more importantly, a powerful tool that promotes exploration and creativity. 	
1609.04141v1	http://arxiv.org/pdf/1609.04141v1	2016	Spirals and heteroclinic cycles in a spatially extended   Rock-Paper-Scissors model of cyclic dominance	C. M. Postlethwaite|A. M. Rucklidge	  Spatially extended versions of the cyclic-dominance Rock-Paper-Scissors model have traveling wave (in one dimension) and spiral (in two dimensions) behavior. The far field of the spirals behave like traveling waves, which themselves have profiles reminiscent of heteroclinic cycles. We compute numerically a nonlinear dispersion relation between the wavelength and wavespeed of the traveling waves, and, together with insight from heteroclinic bifurcation theory and further numerical results from 2D simulations, we are able to make predictions about the overall structure and stability of spiral waves in 2D cyclic dominance models. 	
1711.10528v1	http://arxiv.org/pdf/1711.10528v1	2017	Venus Topography and Boundary Conditions in 3D General Circulation   Modeling	M. J. Way|June Wang	  We explain how GCM boundary condition choices such as ocean-lake coverage-depth, rotation rate, atmospheric constituents and other factors influence surface conditions in ROCKE-3D paleo-Venus simulations. Studies such as these should also be considered when examining liquid water habitability in similar exoplanet experiments. We also describe how one ingests 3D topographic data from NASAs Venus Magellan Spacecraft radar observations into the ROCKE-3D Planetary General Circulation Model. 	
9801204v1	http://arxiv.org/pdf/cond-mat/9801204v1	1998	How Sandcastles Fall	Thomas C. Halsey|Alex J. Levine	  Capillary forces significantly affect the stability of sandpiles. We analyze the stability of sandpiles with such forces, and find that the critical angle is unchanged in the limit of an infinitely large system; however, this angle is increased for finite-sized systems. The failure occurs in the bulk of the sandpile rather than at the surface. This is related to a standard result in soil mechanics. The increase in the critical angle is determined by the surface roughness of the particles, and exhibits three regimes as a function of the added-fluid volume. Our theory is in qualitative agreement with the recent experimental results of Hornbaker et al., although not with the interpretation they make of these results. 	
9912162v2	http://arxiv.org/pdf/cond-mat/9912162v2	1999	Partitioning of a polymer chain between two confining cavities: the role   of electrostatic interactions	S. Tsonchev|R. D. Coalson|A. Duncan	  A recently developed lattice field theory approach to the statistical mechanics of charged polymers in electrolyte solutions [S. Tsonchev, R. D. Coalson, and A. Duncan, Phys. Rev. E {\bf{60}}, 4257, (1999)] is generalized to the case where ground-state dominance in the polymer's Green's function does not apply. The full mean-field equations for the system are derived and are shown to possess a unique solution. The approach is applied to the problem of a charged Gaussian polymer chain confined to move within the region defined by two fused spheres. The failure of the notion of ground-state dominance under certain conditions even in the limit of large number of monomers is demonstrated. 	
0004022v2	http://arxiv.org/pdf/cond-mat/0004022v2	2000	Quantum-Mechanical Non-Perturbative Response of Driven Chaotic   Mesoscopic Systems	Doron Cohen|Tsampikos Kottos	  Consider a time-dependent Hamiltonian $H(Q,P;x(t))$ with periodic driving $x(t)=A\sin(\Omega t)$. It is assumed that the classical dynamics is chaotic, and that its power-spectrum extends over some frequency range $|\omega|<\omega_{cl}$. Both classical and quantum-mechanical (QM) linear response theory (LRT) predict a relatively large response for $\Omega<\omega_{cl}$, and a relatively small response otherwise, independently of the driving amplitude $A$. We define a non-perturbative regime in the $(\Omega,A)$ space, where LRT fails, and demonstrate this failure numerically. For $A>A_{prt}$, where $A_{prt}\propto\hbar$, the system may have a relatively strong response for $\Omega>\omega_{cl}$, and the shape of the response function becomes $A$ dependent. 	
0301350v2	http://arxiv.org/pdf/cond-mat/0301350v2	2003	Magnetic Polarons in the 1D FM Kondo Model	Winfried Koller|Alexander Prüll|Hans Gerd Evertz|Wolfgang von der Linden	  The ferromagnetic Kondo model with classical corespins is studied via unbiased Monte-Carlo simulations. We show that with realistic parameters for the manganites and at low temperatures, the double-exchange mechanism does not lead to phase separation in one-dimensional chains but rather stabilizes individual ferromagnetic polarons. Within the ferromagnetic polaron picture, the pseudogap in the one-particle spectral function A_k(\omega) can easily be explained. Ferromagnetic polarons also clear up a seeming failure of the double-exchange mechanism in explaining the comparable bandwidths in the ferromagnetic and paramagnetic phase. For our analysis, we extend a simplified model, the finite temperature uniform hopping approach (UHA), to include polarons. It can easily be evaluated numerically and provides a simple quantitative understanding of the physical features of the ferromagnetic Kondo model. 	
0304365v2	http://arxiv.org/pdf/cond-mat/0304365v2	2003	Error Diagrams and Temporal Correlations in a Fracture Model with   Characteristic and Power-Law Distributed Avalanches	Yamir Moreno|Miguel Vazquez-Prada|Javier B. Gomez|Amalio F. Pacheco	  Forecasting failure events is one of the most important problems in fracture mechanics and related sciences. In this paper, we use the Molchan scheme to investigate the error diagrams in a fracture model which has the notable advantage of displaying two completely different regimes according to the heterogeneity of the system. In one regime, a characteristic event is observed while for the second regime a power-law spectrum of avalanches is obtained reminiscent of self-organized criticality. We find that both regimes are different when predicting large avalanches and that, in the second regime, there are non-trivial temporal correlations associated to clustering of large events. Finally, we extend the discussion to seismology, where both kinds of avalanche size distributions can be seen. 	
0310701v1	http://arxiv.org/pdf/cond-mat/0310701v1	2003	Non trivial behavior of the linear response function in phase ordering   kinetics	Federico Corberi|Nicola Fusco|Eugenio Lippiello|Marco Zannetti	  Drawing from exact, approximate and numerical results an overview of the properties of the out of equilibrium response function in phase ordering kinetics is presented. Focusing on the zero field cooled magnetization, emphasis is on those features of this quantity which display non trivial behavior when relaxation proceeds by coarsening. Prominent among these is the dimensionality dependence of the scaling exponent $a_{\chi}$ which leads to failure of the connection between static and dynamic properties at the lower dimensionality $d_L$, where $a_{\chi}=0$. We also analyse the mean spherical model as an explicit example of a stochastic unstable system, for which the connection between statics and dynamics fails at all dimensionalities. 	
0311123v2	http://arxiv.org/pdf/cond-mat/0311123v2	2004	Statistical Mechanical Approach to Error Exponents of Lossy Data   Compression	Tadaaki Hosaka|Yoshiyuki Kabashima	  We present herein a scheme by which to accurately evaluate the error exponents of a lossy data compression problem, which characterize average probabilities over a code ensemble of compression failure and success above or below a critical compression rate, respectively, utilizing the replica method (RM). Although the existing method used in information theory (IT) is, in practice, limited to ensembles of randomly constructed codes, the proposed RM-based approach can be applied to a wider class of ensembles. This approach reproduces the optimal expressions of the error exponents achieved by the random code ensembles, which are known in IT. In addition, the proposed framework is used to show that codes composed of non-monotonic perceptrons of a specific type can provide the optimal exponents in most cases, which is supported by numerical experiments. 	
0411556v1	http://arxiv.org/pdf/cond-mat/0411556v1	2004	Quantized Failure Criteria and Indirect Observation for Predicting the   Nanoscale Strength of Materials: The Example of the Ultra Nano Crystalline   Diamond	Nicola M. Pugno	  In this paper theoretical and statistical/experimental criteria for determining the nanoscale strength of materials are proposed. In particular, quantized criteria in fracture mechanics, dynamic fracture mechanics and fatigue, as well as an experimental indirect observation of the nanoscale strength, are proposed. The increasing of the dynamic resistance and the role of a fractal crack surface formation are also rationalized. The analysis shows that materials can be sensitive to flaws also at nanoscale (as demonstrated for carbon nanotubes), in contrast to the conclusion of a recently published paper, and that the surfaces are weaker than the inner parts of a solid by a factor of about 10%. In addition, the proposed statistical/experimental procedure is applied for predicting the nanoscale strength of the ultrananocrystalline diamond (UNCD), an innovative material only recently developed. 	
0412233v1	http://arxiv.org/pdf/cond-mat/0412233v1	2004	On nonextensive thermo-statistics: systematization, clarification of   scope and interpretation, and illustrations	Roberto Luzzi|Áurea R. Vasconcellos|J. Galvão Ramos	  When dealing with certain kind of complex phenomena the theoretician may face some difficulties -- typically a failure to have access to information for properly characterize the system -- for applying the full power of the standard approach to the well established, physically and logically sound, Boltzmann-Gibbs statistics. To circumvent such difficulties, in order to make predictions on properties of the system and looking for an understanding of the physics involved (for example in analyzing the technological characteristics of fractal-structured devices) can be introduced large families of auxiliary statistics. We present here a systematization of these different styles in what can be termed as Unconventional Statistical Mechanics, accompanied of an analysis of the construction and a clarification of its scope and interpretation. As illustrations are derived heterotypical Bose-Einstein, Fermi-Dirac and Maxwell-Boltzmann distributions, and several applications including studies of experimental works are briefly described. 	
0607452v1	http://arxiv.org/pdf/cond-mat/0607452v1	2006	Dielectric breakdown in underoxidized magnetic tunnel junctions:   Dependence on oxidation time and area	J. Ventura|R. Ferreira|J. B. Sousa|P. P. Freitas	  Magnetic tunnel junctions (MTJs) with partially oxidized 9 \AA AlO$_x$-barriers were recently shown to have the necessary characteristics to be used as magnetoresistive sensors in high-density storage devices. Here we study dielectric breakdown in such underoxidized magnetic tunnel junctions, focusing on its dependence on tunnel junction area and oxidation time. A clear relation between breakdown mechanism and junction area is observed for the MTJs with the highest studied oxidation time: samples with large areas fail usually due to extrinsic causes (characterized by a smooth resistance decrease at dielectric breakdown). Small area junctions fail mainly through an intrinsic mechanism (sharp resistance decrease at breakdown). However, this dependence changes for lower oxidation times, with extrinsic breakdown becoming dominant. In fact, in the extremely underoxidized magnetic tunnel junctions, failure is exclusively related with extrinsic causes, independently of MTJ-area. These results are related with the presence of defects in the barrier (weak spots that lead to intrinsic breakdown) and of metallic unoxidized Al nanoconstrictions (leading to extrinsic breakdown). 	
0107070v1	http://arxiv.org/pdf/nlin/0107070v1	2001	Spatiotemporal dynamics of pacing in models of anatomical reentry	Sitabhra Sinha|David J. Christini	  Reentry around non-conducting ventricular scar tissue, which often causes lethal arrhythmias, is typically treated by rapid stimulation from an implantable defibrillator. However, the mechanisms of termination (success and failure) are poorly understood. To elucidate such mechanisms, we studied pacing of anatomical reentry in 1-D and 2-D excitable cardiac media models. Our results suggest that the existence of inhomogeneity in the reentry circuit is essential for pacing termination of tachycardia to be successful. Considering the role of such inhomogeneities may lead to more effective pacing algorithms. 	
0406046v1	http://arxiv.org/pdf/nlin/0406046v1	2004	Terminating ventricular tachycardia by pacing induced dynamical   inhomogeneities in the reentry circuit	Sitabhra Sinha|Johannes Breuer	  Formation of feedback loops of excitation waves (reentrant circuit) around non-conducting ventricular scar tissue is a common cause of lethal cardiac arrhythmias, such as ventricular tachycardia. This is typically treated by rapid stimulation from an implantable device (ICD). However, the mechanisms of reentry termination success and, more importantly, failure, are poorly understood. To study such mechanisms, we simulated pacing of anatomical reentry in an ionic model of cardiac tissue, having significant restitution and dispersion properties. Our results show that rapid pacing causes inhomogeneous conduction in the reentrant circuit, leading to successful pacing termination of tachycardia. The study suggests that more effective pacing algorithms can be designed by taking into account the role of such dynamical inhomogeneities. 	
0301044v3	http://arxiv.org/pdf/quant-ph/0301044v3	2003	Mixing quantum and classical mechanics and uniqueness of Planck's   constant	Debendranath Sahoo	  Observables of quantum or classical mechanics form algebras called quantum or classical Hamilton algebras respectively (Grgin E and Petersen A (1974) {\it J Math Phys} {\bf 15} 764\cite{grginpetersen}, Sahoo D (1977) {\it Pramana} {\bf 8} 545\cite{sahoo}). We show that the tensor-product of two quantum Hamilton algebras, each characterized by a different Planck's constant is an algebra of the same type characterized by yet another Planck's constant. The algebraic structure of mixed quantum and classical systems is then analyzed by taking the limit of vanishing Planck's constant in one of the component algebras. This approach provides new insight into failures of various formalisms dealing with mixed quantum-classical systems. It shows that in the interacting mixed quantum-classical description, there can be no back-reaction of the quantum system on the classical. A natural algebraic requirement involving restriction of the tensor product of two quantum Hamilton algebras to their components proves that Planck's constant is unique. 	
0305137v5	http://arxiv.org/pdf/quant-ph/0305137v5	2003	An Explanation of Spin Based on Classical Mechanics and Electrodynamics	O. Chavoya-Aceves	  It is proved that, according to Classical Mechanics and Electrodynamics, the trajectory of the center of mass of a neutral system of electrical charges can be deflected by an inhomogeneous magnetic field, even if its internal angular momentum is zero. This challenges the common view about the function of the Stern-Gerlach apparatus, as resolving the eigen-states of an intrinsic angular momentum. Doubts are cast also on the supposed failure of Schrodinger's theory to explain the properties of atoms in presence of magnetic fields without introducing spin variables. 	
0308114v1	http://arxiv.org/pdf/quant-ph/0308114v1	2003	The Bell-Kochen-Specker Theorem	D. M. Appleby	  Meyer, Kent and Clifton (MKC) claim to have nullified the Bell-Kochen-Specker (Bell-KS) theorem. It is true that they invalidate KS's account of the theorem's physical implications. However, they do not invalidate Bell's point, that quantum mechanics is inconsistent with the classical assumption, that a measurement tells us about a property previously possessed by the system. This failure of classical ideas about measurement is, perhaps, the single most important implication of quantum mechanics. In a conventional colouring there are some remaining patches of white. MKC fill in these patches, but only at the price of introducing patches where the colouring becomes ''pathologically'' discontinuous. The discontinuities mean that the colours in these patches are empirically unknowable. We prove a general theorem which shows that their extent is at least as great as the patches of white in a conventional approach. The theorem applies, not only to the MKC colourings, but also to any other such attempt to circumvent the Bell-KS theorem (Pitowsky's colourings, for example). We go on to discuss the implications. MKC do not nullify the Bell-KS theorem. They do, however, show that we did not, hitherto, properly understand the theorem. For that reason their results (and Pitowsky's earlier results) are of major importance. 	
0406075v1	http://arxiv.org/pdf/quant-ph/0406075v1	2004	Symmetric Triple Well with Non-Equivalent Vacua: Simple Quantum   Mechanical Approach	H. A. Alhendi|E. I. Lashin	  The structure of the energy levels in a deep triple well is analyzed using simple quantum mechanical considerations. The resultant spectra of the first three energy levels are found to be composed of a ground state localized at the central well and the two other states are distributed only among the left and right well in anti-symmetric and symmetric way respectively. Due to the tunneling effects the energy eigenvalue of the ground state is approximately equal to the ground state energy for a harmonic oscillator localized at the central well, while the two others are nearly degenerate with approximate values equal to the ground state energy of a harmonic oscillator localized at the left or right well. The resulting pattern of the spectra are confirmed numerically. The failure of the instantonic approach recently applied for predicting the correct spectra is commented 	
0509048v1	http://arxiv.org/pdf/quant-ph/0509048v1	2005	The Grammar of Teleportation	Christopher G. Timpson	  Whilst a straightforward consequence of the formalism of non-relativistic quantum mechanics, the phenomenon of quantum teleportation has given rise to considerable puzzlement. In this paper, the teleportation protocol is reviewed and these puzzles dispelled. It is suggested that they arise from two primary sources: 1) the familiar error of hypostatizing an abstract noun (in this case, `information') and 2) failure to differentiate interpretation dependent from interpretation independent features of quantum mechanics. A subsidiary source of error, the simulation fallacy, is also identified. The resolution presented of the puzzles of teleportation illustrates the benefits of paying due attention to the logical status of `information' as an abstract noun. 	
0704.0761v2	http://arxiv.org/pdf/0704.0761v2	2007	Failure of the work-Hamiltonian connection for free energy calculations	Jose M. G. Vilar|J. Miguel Rubi	  Extensions of statistical mechanics are routinely being used to infer free energies from the work performed over single-molecule nonequilibrium trajectories. A key element of this approach is the ubiquitous expression dW/dt=\partial H(x,t)/ \partial t which connects the microscopic work W performed by a time-dependent force on the coordinate x with the corresponding Hamiltonian H(x,t) at time t. Here we show that this connection, as pivotal as it is, cannot be used to estimate free energy changes. We discuss the implications of this result for single-molecule experiments and atomistic molecular simulations and point out possible avenues to overcome these limitations. 	
0707.1333v2	http://arxiv.org/pdf/0707.1333v2	2007	Disproof of Bell's Theorem: Further Consolidations	Joy Christian	  The failure of Bell's theorem for Clifford algebra valued local variables is further consolidated by proving that the conditions of remote parameter independence and remote outcome independence are duly respected within the recently constructed exact, local realistic model for the EPR-Bohm correlations. Since the conjunction of these two conditions is equivalent to the locality condition of Bell, this provides an independent geometric proof of the local causality of the model, at the level of microstates. In addition to local causality, the model respects at least seven other conceptual and operational requirements, arising either from the predictions of quantum mechanics or the premises of Bell's theorem, including the Malus's law for sequential spin measurements. Since the agreement between the predictions of the model and those of quantum mechanics is quantitatively precise in all respects, the ensemble interpretation of the entangled singlet state becomes amenable. 	
0709.2172v1	http://arxiv.org/pdf/0709.2172v1	2007	The influence of metallic particle size on the mechanical properties of   PTFE-Al-W powder composites	J. Cai|V. F. Nesterenko|K. S. Vecchio|E. B. Herbold|D. J. Benson|F. Jiang|J. W. Addiss|S. M. Walley|W. G. Proud	  The dynamic mechanical properties of reactive materials (e.g., high density mixtures of polytetraflouroethylene (PTFE), aluminum (Al) and tungsten (W) powders) can be tailored by changing the morphology of the particles and porosity. Cold isostatically pressed PTFE-Al-W powder composites with fine metallic particles and a higher porosity exhibited higher ultimate compressive strength than less porous composites having equivalent mass ratios with coarse W particles. The mesoscale force chains between the fine metallic particles are responsible for this unusual phenomenon. We observed macrocracks below the critical failure strain for the matrix and a competition between densification and fracture in some porous samples in dynamic tests. 	
0803.2362v1	http://arxiv.org/pdf/0803.2362v1	2008	Failure of feedback as a putative common mechanism of spreading   depolarizations in migraine and stroke	Markus A. Dahlem|Felix M. Schneider|Eckehard Schoell	  The stability of cortical function depends critically on proper regulation. Under conditions of migraine and stroke a breakdown of transmembrane chemical gradients can spread through cortical tissue. A concomitant component of this emergent spatio-temporal pattern is a depolarization of cells detected as slow voltage variations. The velocity of ~3 mm/min indicates a contribution of diffusion. We propose a mechanism for spreading depolarizations (SD) that rests upon a nonlocal or non-instantaneous feedback in a reaction-diffusion system. Depending upon the characteristic space and time scales of the feedback, the propagation of cortical SD can be suppressed by shifting the bifurcation line, which separates the parameter regime of pulse propagation from the regime where a local disturbance dies out. The optimisation of this feedback is elaborated for different control schemes and ranges of control parameters. 	
0809.4616v1	http://arxiv.org/pdf/0809.4616v1	2008	Low-resolution measurements induced classicality	R. M. Angelo	  The classical limit of quantum mechanics is discussed for closed quantum systems in terms of observational aspects. Initially, the failure of the limit h->0 is explicitly demonstrated in a model of two quantum mechanically interacting oscillators by showing that neither quantum expectations reduce to Newtonian trajectories nor entanglement vanishes. This result suggests that the quantum-to-classical transition occurs only at an approximative level, which is regulated by the low accuracy of the measurements. In order to verify the consistence of these ideas we take into account the experimental resolution of physical measurements by introducing a discretized formulation for the quantum structure of wave functions. As a result, in the low-resolution limit the quasi-determinism is recovered and hence the quantum-to-classical transition is shown to occur adequately. Other puzzling problems, such as the classical limit of quantum superpositions and nonlocal correlations, are naturally address as well. 	
0812.4162v1	http://arxiv.org/pdf/0812.4162v1	2008	Kinetic roughening in a realistic model of non-conserved interface   growth	Matteo Nicoli|Mario Castro|Rodolfo Cuerno	  We provide a quantitative picture of non-conserved interface growth from a diffusive field making special emphasis on two main issues, the range of validity of the effective small-slopes (interfacial) theories and the interplay between the emergence of morphologically instabilities in the aggregate dynamics, and its kinetic roughening properties. Taking for definiteness electrochemical deposition as our experimental field of reference, our theoretical approach makes use of two complementary approaches: interfacial effective equations and a phase-field formulation of the electrodeposition process. Both descriptions allow us to establish a close quantitative connection between theory and experiments. Moreover, we are able to correlate the anomalous scaling properties seen in some experiments with the failure of the small slope approximation, and to assess the effective re-emergence of standard kinetic roughening properties at very long times under appropriate experimental conditions. 	
0901.4407v2	http://arxiv.org/pdf/0901.4407v2	2010	A dynamic model of time-dependent complex networks	Scott A. Hill|Dan Braha	  The characterization of the "most connected" nodes in static or slowly evolving complex networks has helped in understanding and predicting the behavior of social, biological, and technological networked systems, including their robustness against failures, vulnerability to deliberate attacks, and diffusion properties. However, recent empirical research of large dynamic networks (characterized by connections that are irregular and evolve rapidly) has demonstrated that there is little continuity in degree centrality of nodes over time, even when their degree distributions follow a power law. This unexpected dynamic centrality suggests that the connections in these systems are not driven by preferential attachment or other known mechanisms. We present a novel approach to explain real-world dynamic networks and qualitatively reproduce these dynamic centrality phenomena. This approach is based on a dynamic preferential attachment mechanism, which exhibits a sharp transition from a base pure random walk scheme. 	
0905.1829v1	http://arxiv.org/pdf/0905.1829v1	2009	An experimental test of volume-equilibration between granular systems	Frederic Lechenault|Karen E. Daniels	  Understanding granular and other athermal systems requires the identification of state variables which consistently predict their bulk properties. A promising approach has been to draw on the techniques of equilibrium statistical mechanics, but to consider alternate conserved quantities in place of energy. The Edwards ensemble, based on volume conservation, provides a temperaturelike intensive parameter called compactivity. We present experiments which demonstrate the failure of compactivity to equilibrate (via volume-exchange) between a pair of externally-agitated granular subsystems with different material properties. Nonetheless, we identify a material-independent relationship between the mean and fluctuations of the local packing fraction which forms the basis for an equation of state. This relationship defines an intensive parameter that decouples from the volume statistics. 	
0910.3850v1	http://arxiv.org/pdf/0910.3850v1	2009	General principles in the interpretation of quantum mechanics	Casey Blood	  The three major theoretical principles of quantum mechanics relevant to its interpretation are: (T1), linearity; (T2), invariance under certain groups; and (T3) the orthogonality and isolation of the different branches of the state vector. These three imply the particle-like properties of mass, energy, momentum, spin, charge, and locality are actually properties of the state vector; and this in turn implies there is no evidence for the existence of particles. Experimentally there is no evidence for collapse (E1) and theoretically linearity prohibits collapse. One also has the experimentally verified probability law (E2), which is found to rule out the many-worlds interpretation. The failure of these three major interpretation, particles, collapse, and many-worlds, apparently implies an acceptable interpretation must be based on perception. Rather than being a separate principle, probability follows in this interpretation from a weak assumption on perception plus the combinatorics when an experiment is run many times. This suggests a relatively simple experimental test of the perception interpretation. 	
1003.5023v1	http://arxiv.org/pdf/1003.5023v1	2010	Collective Helping and Bystander Effects in Coevolving Helping Networks	Hang-Hyun Jo|Hyun Keun Lee|Hyunggyu Park	  We study collective helping behavior and bystander effects in a coevolving helping network model. A node and a link of the network represents an agent who renders or receives help and a friendly relation between agents, respectively. A helping trial of an agent depends on relations with other involved agents and its result (success or failure) updates the relation between the helper and the recipient. We study the network link dynamics and its steady states analytically and numerically. The full phase diagram is presented with various kinds of active and inactive phases and the nature of phase transitions are explored. We find various interesting bystander effects, consistent with the field study results, of which the underlying mechanism is proposed. 	
1011.1529v1	http://arxiv.org/pdf/1011.1529v1	2010	A Survey on Wireless Sensor Network Security	Jaydip Sen	  Wireless sensor networks (WSNs) have recently attracted a lot of interest in the research community due their wide range of applications. Due to distributed nature of these networks and their deployment in remote areas, these networks are vulnerable to numerous security threats that can adversely affect their proper functioning. This problem is more critical if the network is deployed for some mission-critical applications such as in a tactical battlefield. Random failure of nodes is also very likely in real-life deployment scenarios. Due to resource constraints in the sensor nodes, traditional security mechanisms with large overhead of computation and communication are infeasible in WSNs. Security in sensor networks is, therefore, a particularly challenging task. This paper discusses the current state of the art in security mechanisms for WSNs. Various types of attacks are discussed and their countermeasures presented. A brief discussion on the future direction of research in WSN security is also included. 	
1011.6048v1	http://arxiv.org/pdf/1011.6048v1	2010	Noisy-threshold control of cell death	Jose M. G. Vilar	  Cellular responses to death-promoting stimuli typically proceed through a differentiated multistage process, involving a lag phase, extensive death, and potential adaptation. Deregulation of this chain of events is at the root of many diseases. Improper adaptation is particularly important because it allows cell sub-populations to survive even in the continuous presence of death conditions, which results, among others, in the eventual failure of many targeted anticancer therapies. Here, I show that these typical responses arise naturally from the interplay of intracellular variability with a threshold-based control mechanism that detects cellular changes in addition to just the cellular state itself. Implementation of this mechanism in a quantitative model for T-cell apoptosis, a prototypical example of programmed cell death, captures with exceptional accuracy experimental observations for different expression levels of the oncogene Bcl-xL and directly links adaptation with noise in an ATP threshold below which cells die. These results indicate that oncogenes like Bcl-xL, besides regulating absolute death values, can have a novel role as active controllers of cell-cell variability and the extent of adaptation. 	
1101.0114v2	http://arxiv.org/pdf/1101.0114v2	2013	Behavioral subtyping through typed assertions	Herbert Toth	  This paper presents a critical discussion of popular approaches to ensure the Liskov substitution principle in class hierarchies (e.g. Design by Contract(TM), specification inheritance). It will be shown that they have some deficiencies which are due to the way how effective constraints are calculated for subclass methods. A new mechanism, called client conformance, is introduced that takes the client's view on the program state into account more properly: The client's static type determines the context in which reasoning about program state is to be done. This is the context to which the runtime assertion checking (RAC) of server methods must be adapted appropriately. In a stepwise argumentation we show the improvements for RAC that can be reached following this approach in a natural way, preserving the percolation pattern mechanism: Clients will neither be confronted with unsafe or surprising executions, nor with surprising failures of server methods. 	
1104.0802v1	http://arxiv.org/pdf/1104.0802v1	2011	Dielectric insulation and high-voltage issues	D. Tommasini	  Electrical faults are in most cases dramatic events for magnets, due to the large stored energy which is potentially available to be dissipated at the fault location. After a reminder of the principles of electrostatics in Section 1, the basic mechanisms of conduction and breakdown in dielectrics are summarized in Section 2. Section 3 introduces the types and function of the electrical insulation in magnets, and Section 4 its relevant failure mechanisms. Section 5 deals with ageing and, finally, Section 6 gives some principles for testing. Though the School specifically dealt with warm magnets, for completeness some principles of dielectric insulation for superconducting accelerator magnets are briefly summarized in a dedicated appendix. 	
1202.2551v1	http://arxiv.org/pdf/1202.2551v1	2012	A Simulation Model for Evaluating Distributed Systems Dependability	Ciprian Dobre|Florin Pop|Valentin Cristea	  In this paper we present a new simulation model designed to evaluate the dependability in distributed systems. This model extends the MONARC simulation model with new capabilities for capturing reliability, safety, availability, security, and maintainability requirements. The model has been implemented as an extension of the multithreaded, process oriented simulator MONARC, which allows the realistic simulation of a wide-range of distributed system technologies, with respect to their specific components and characteristics. The extended simulation model includes the necessary components to inject various failure events, and provides the mechanisms to evaluate different strategies for replication, redundancy procedures, and security enforcement mechanisms, as well. The results obtained in simulation experiments presented in this paper probe that the use of discrete-event simulators, such as MONARC, in the design and development of distributed systems is appealing due to their efficiency and scalability. 	
1203.0242v1	http://arxiv.org/pdf/1203.0242v1	2012	Network Physiology reveals relations between network topology and   physiological function	Amir Bashan|Ronny P. Bartsch|Jan W. Kantelhardt|Shlomo Havlin|Plamen Ch. Ivanov	  The human organism is an integrated network where complex physiologic systems, each with its own regulatory mechanisms, continuously interact, and where failure of one system can trigger a breakdown of the entire network. Identifying and quantifying dynamical networks of diverse systems with different types of interactions is a challenge. Here, we develop a framework to probe interactions among diverse systems, and we identify a physiologic network. We find that each physiologic state is characterized by a specific network structure, demonstrating a robust interplay between network topology and function. Across physiologic states the network undergoes topological transitions associated with fast reorganization of physiologic interactions on time scales of a few minutes, indicating high network flexibility in response to perturbations. The proposed system-wide integrative approach may facilitate the development of a new field, Network Physiology. 	
1203.1374v2	http://arxiv.org/pdf/1203.1374v2	2012	Steady Periodic Shear Flow is Stable in Two Space Dimensions .   Nonequilibrium Molecular Dynamics vs Navier-Stokes-Fourier Stability Theory   -- A Comment on two Arxiv Contributions	Wm. G. Hoover|Carol G. Hoover	  Dufty, Lee, Lutsko, Montanero, and Santos have carried out stability analyses of steady stationary shear flows. Their approach is based on the compressible and heat conducting Navier-Stokes-Fourier model. It predicts the unstable exponential growth of long-wavelength transverse perturbations for both two- and three-dimensional fluids. We point out that the patently-stable two-dimensional periodic shear flows studied earlier by Petravic, Posch, and ourselves contradict these predicted instabilities. The stable steady-state shear flows are based on nonequilibrium molecular dynamics with simple thermostats maintaining nonequilibrium stationary states in two space dimensions. The failure of the stability analyses remains unexplained. 	
1209.1078v2	http://arxiv.org/pdf/1209.1078v2	2013	Electromagnetic potentials and Aharonov-Bohm effect	Alexander Ershkovich	  Hamilton-Jacobi equation which governs classical mechanics and electrodynamics explicitly depends on the electromagnetic potentials (A,{\phi}), similar to Schroedinger equation. We derived the Aharonov-Bohm effect from Hamilton-Jacobi equation thereby having proved that this effect is of classical origin. These facts enable us to arrive at the following conclusions: a) the very idea of special role of potentials (A,{\phi}) in quantum mechanics (different from their role in classical physics) lost the ground, and becomes dubious, as this idea is based on the Aharonov-Bohm effect, b) failure to find any signs of a special role of these potentials in the appropriate experiments (Feinberg, 1963) is thereby explained, and c) discovery of classical analogues of the Aharonov-Bohm effect (Berry et al., 1980) is also explained by a classical nature of this effect. Elucidation of the "unlocal" interaction problem was made. 	
1209.3410v1	http://arxiv.org/pdf/1209.3410v1	2012	Work hardening behavior in a steel with multiple TRIP mechanisms	M. C. McGrath|D. C. Van Aken|N. I. Medvedeva|J. E. Medvedeva	  Transformation induced plasticity (TRIP) behavior was studied in steel with composition Fe-0.07C-2.85Si-15.3Mn-2.4Al-0.017N that exhibited two TRIP mechanisms. The initial microstructure consisted of both {\epsilon}- and {\alpha}-martensites with 27% retained austenite. TRIP behavior in the first 5% strain was predominately austenite transforming to {\epsilon}-martensite (Stage I), but upon saturation of Stage I, the {\epsilon}-martensite transformed to {\alpha}-martensite (Stage II). Alloy segregation also affected the TRIP behavior with alloy rich regions producing TRIP just prior to necking. This behavior was explained by first principle calculations that revealed aluminum significantly affected the stacking fault energy in Fe-Mn-Al-C steels by decreasing the unstable stacking fault energy and promoting easy nucleation of {\epsilon}-martensite. The addition of aluminum also raised the intrinsic stacking fault energy and caused the {\epsilon}-martensite to be unstable and transform to {\alpha}-martensite under further deformation. The two stage TRIP behavior produced a high strain hardening exponent of 1.4 and led to ultimate tensile strength of 1165 MPa and elongation to failure of 35%. 	
1210.1505v2	http://arxiv.org/pdf/1210.1505v2	2014	A Comparative Study of SIP Overload Control Algorithms	Yang Hong|Changcheng Huang|James Yan	  Recent collapses of SIP servers in the carrier networks indicates two potential problems of SIP: (1) the current SIP design does not easily scale up to large network sizes, and (2) the built-in SIP overload control mechanism cannot handle overload conditions effectively. In order to help carriers prevent widespread SIP network failure effectively, this chapter presents a systematic investigation of current state-of-the-art overload control algorithms. To achieve this goal, this chapter first reviews two basic mechanisms of SIP, and summarizes numerous experiment results reported in the literatures which demonstrate the impact of overload on SIP networks. After surveying the approaches for modeling the dynamic behaviour of SIP networks experiencing overload, the chapter presents a comparison and assessment of different types of SIP overload control solutions. Finally it outlines some research opportunities for managing SIP overload control. 	
1212.4335v1	http://arxiv.org/pdf/1212.4335v1	2012	Influence of nanoparticle size, loading, and shape on the mechanical   properties of polymer nanocomposites	Aki Kutvonen|Giulia Rossi|Sakari R. Puisto|Niko K. J. Rostedt|Tapio Ala-Nissila	  We study the influence of spherical, triangular, and rod-like nanoparticles on the mechanical properties of a polymer nanocomposite (PNC), via coarse-grained molecular dynamics simulations. We focus on how the nanoparticle size, loading, mass, and shape influence the PNC's elastic modulus, stress at failure and resistance against cavity formation and growth, under external stress. We find that in the regime of strong polymer-nanoparticle interactions, the formation of a polymer network via temporary polymer-nanoparticle crosslinks has a predominant role on the PNC reinforcement. Spherical nanoparticles, whose size is comparable to that of the polymer monomers, are more effective at toughening the PNC than larger spherical particles. When comparing particles of spherical, triangular, and rod-like geometries, the rod-like nanoparticles emerge as the best PNC toughening agents. 	
1212.6060v1	http://arxiv.org/pdf/1212.6060v1	2012	Analytic prognostic for petrochemical pipelines	Abdo Abou Jaoude|Seifedine Kadry|Khaled El-Tawil|Hassan Noura|Mustapha Ouladsine	  Pipelines tubes are part of vital mechanical systems largely used in petrochemical industries. They serve to transport natural gases or liquids. They are cylindrical tubes and are submitted to the risks of corrosion due to high PH concentrations of the transported liquids in addition to fatigue cracks due to the alternation of pressure-depression of gas along the time, initiating therefore in the tubes body micro-cracks that can propagate abruptly to lead to failure. The development of the prognostic process for such systems increases largely their performance and their availability, as well decreases the global cost of their missions. Therefore, this paper deals with a new prognostic approach to improve the performance of these pipelines. Only the first mode of crack, that is, the opening mode, is considered. 	
1303.5365v2	http://arxiv.org/pdf/1303.5365v2	2013	Improving Network Efficiency by Removing Energy Holes in WSNs	M. B. Rasheedl|N. Javaid|A. Javaid|M. A. Khan|S. H. Bouk|Z. A. Khan	  Cluster based Wireless Sensor Networks (WSNs) have been widely used for better performance in terms of energy efficiency. Efficient use of energy is challenging task of designing these protocols. Energy holedare created due to quickly drain the energy of a few nodes due to non-uniform distribution in the network. Normally, energy holes make the data routing failure when nodes transmit data back to the base station. We proposedEnergy-efficient HOleRemoving Mechanism (E-HORM) technique to remove energy holes. In this technique, we use sleep and awake mechanism for sensor nodes to save energy. This approach finds the maximum distance node to calculate the maximum energy for data transmission. We considered it as a threshold energy Eth. Every node first checks its energy level for data transmission. If the energy level is less than Eth, it cannot transmit data. We also explain mathematically the energy consumption and average energy saving of sensor nodes in each round. Extensive simulations showed that when use this approach for WSNs significantly helps to extend the network lifetime and stability period. 	
1304.3675v2	http://arxiv.org/pdf/1304.3675v2	2014	Self-assembly of colloidal polymers via depletion-mediated lock and key   binding	Douglas J. Ashton|Robert L. Jack|Nigel B. Wilding	  We study the depletion-induced self-assembly of indented colloids. Using state-of-the-art Monte Carlo simulation techniques that treat the depletant particles explicitly, we demonstrate that colloids assemble by a lock-and-key mechanism, leading to colloidal polymerization. The morphology of the chains that are formed depends sensitively on the size of the colloidal indentation, with smaller values additionally permitting chain branching. In contrast to the case of spheres with attractive patches, Wertheim's thermodynamic perturbation theory fails to provide a fully quantitative description of the polymerization transition. We trace this failure to a neglect of packing effects and we introduce a modified theory that accounts better for the shape of the colloids, yielding improved agreement with simulation. 	
1306.4575v1	http://arxiv.org/pdf/1306.4575v1	2013	A Mathematical Model for Predicting the Life of PEM Fuel Cell Membranes   Subjected to Hydration Cycling	S. F. Burlatsky|M. Gummalla|J. O'Neill|V. V. Atrazhev|A. N. Varyukhin|D. V. Dmitriev|N. S. Erikhman	  Under typical PEM fuel cell operating conditions, part of membrane electrode assembly is subjected to humidity cycling due to variation of inlet gas RH and/or flow rate. Cyclic membrane hydration/dehydration would cause cyclic swelling/shrinking of the unconstrained membrane. In a constrained membrane, it causes cyclic stress resulting in mechanical failure in the area adjacent to the gas inlet. A mathematical modeling framework for prediction of the lifetime of a PEM FC membrane subjected to hydration cycling is developed in this paper. The model predicts membrane lifetime as a function of RH cycling amplitude and membrane mechanical properties. The modeling framework consists of three model components: a fuel cell RH distribution model, a hydration/dehydration induced stress model that predicts stress distribution in the membrane, and a damage accrual model that predicts membrane life-time. Short descriptions of the model components along with overall framework are presented in the paper. The model was used for lifetime prediction of a GORE-SELECT membrane. 	
1307.4541v1	http://arxiv.org/pdf/1307.4541v1	2013	The resilience of interdependent transportation networks under targeted   attack	Peng Zhang|Baisong Cheng|Zhuang Zhao|Daqing Li|Guangquan Lu|Yunpeng Wang|Jinghua Xiao	  Modern world builds on the resilience of interdependent infrastructures characterized as complex networks. Recently, a framework for analysis of interdependent networks has been developed to explain the mechanism of resilience in interdependent networks. Here we extend this interdependent network model by considering flows in the networks and study the system's resilience under different attack strategies. In our model, nodes may fail due to either overload or loss of interdependency. Under the interaction between these two failure mechanisms, it is shown that interdependent scale-free networks show extreme vulnerability. The resilience of interdependent SF networks is found in our simulation much smaller than single SF network or interdependent SF networks without flows. 	
1309.6000v1	http://arxiv.org/pdf/1309.6000v1	2013	A New Sentinel Approach for Energy Efficient and Hole Aware Wireless   Sensor Networks	Dame Diongue|Ousmane Thiare	  Recent advances in micro-sensor and communication technology have enabled the emergence of a new technology, Wireless Sensor Networks (WSN). WSN have emerging recently as a key solution to monitor remote or hostile environments and concern a wide range of applications. These networks are faced with many challenges such as energy efficiency usage, topology maintenance, network lifetime maximization, etc. Experience shows that sensing and communications tasks consume energy, therefore judicious power management can effectively extend network lifetime. Moreover, the low cost of sensor devices will allows deployment of huge number nodes that can permit a high redundancy degree. In this paper, we focus on the problem of energy efficiency and topology maintenance in a densely deployed network context. Hence we propose an energy aware sleep scheduling and rapid topology healing scheme for long life wireless sensor networks. Our scheme is a strong node scheduling based mechanism for lifetime maximization in wireless sensor networks and has a fast maintenance method to cover nodes failure. Our sentinel scheme is based on a probabilistic model which provides a distributed sleep scheduling and topology control algorithm. Simulations and experimental results are presented to verify our approach and the performance of our mechanism. 	
1311.0318v1	http://arxiv.org/pdf/1311.0318v1	2013	Creep dynamics of viscoelastic interfaces	E. A. Jagla	  The movement of a purely elastic interface driven on a disordered energy potential is characterized by a depinning transition: when the pulling force S is larger than some critical value S_1 the system is in a flowing regime and moves at a finite velocity. If S < S_1 the interface remains pinned and its velocity is zero. We show that for a one-dimensional interface, the inclusion of viscoelastic relaxation produces the appearance of an intervening regime between the pinned and the flowing phases in a well defined stress interval S_0<S<S_1, in which the interface evolves through a sequence of avalanches that give rise to a creep process. As S --> S_0 the creep velocity vanishes in an universal way that is governed by a directed percolation process. As S --> S_1 the creep velocity increases as a power law due to the increase of the typical size of the avalanches. The present observations may serve to improve the understanding of fatigue failure mechanisms. 	
1401.3381v1	http://arxiv.org/pdf/1401.3381v1	2014	Promises, Impositions, and other Directionals	Jan A. Bergstra|Mark Burgess	  Promises, impositions, proposals, predictions, and suggestions are categorized as voluntary co-operational methods. The class of voluntary co-operational methods is included in the class of so-called directionals. Directionals are mechanisms supporting the mutual coordination of autonomous agents.   Notations are provided capable of expressing residual fragments of directionals. An extensive example, involving promises about the suitability of programs for tasks imposed on the promisee is presented. The example illustrates the dynamics of promises and more specifically the corresponding mechanism of trust updating and credibility updating. Trust levels and credibility levels then determine the way certain promises and impositions are handled.   The ubiquity of promises and impositions is further demonstrated with two extensive examples involving human behaviour: an artificial example about an agent planning a purchase, and a realistic example describing technology mediated interaction concerning the solution of pay station failure related problems arising for an agent intending to leave the parking area. 	
1402.1217v2	http://arxiv.org/pdf/1402.1217v2	2014	Entanglement, scaling, and the meaning of the wave function in   protective measurement	Maximilian Schlosshauer|Tangereen V. B. Claringbold	  We examine the entanglement and state disturbance arising in a protective measurement and argue that these inescapable effects doom the claim that protective measurement establishes the reality of the wave function. An additional challenge to this claim results from the exponential number of protective measurements required to reconstruct multi-qubit states. We suggest that the failure of protective measurement to settle the question of the meaning of the wave function is entirely expected, for protective measurement is but an application of the standard quantum formalism, and none of the hard foundational questions can ever be settled in this way. 	
1404.5539v2	http://arxiv.org/pdf/1404.5539v2	2014	Electricity Pooling Markets with Strategic Producers Possessing   Asymmetric Information II: Inelastic Demand	Mohammad Rasouli|Demosthenis Teneketzis	  In the restructured electricity industry, electricity pooling markets are an oligopoly with strategic producers possessing private information (private production cost function). We focus on pooling markets where aggregate demand is represented by a non-strategic agent.   Inelasticity of demand is a main difficulty in electricity markets which can potentially result in market failure and high prices. We consider demand to be inelastic.   We propose a market mechanism that has the following features. (F1) It is individually rational. (F2) It is budget balanced. (F3) It is price efficient, that is, at equilibrium the price of electricity is equal to the marginal cost of production. (F4) The energy production profile corresponding to every non-zero Nash equilibrium of the game induced by the mechanism is a solution of the corresponding centralized problem where the objective is the maximization of the sum of the producers' and consumers' utilities.   We identify some open problems associated with our approach to electricity pooling markets. 	
1406.3526v5	http://arxiv.org/pdf/1406.3526v5	2017	Quantum Logic as Classical Logic	Simon Kramer	  We propose a semantic representation of the standard quantum logic QL within a classical, normal modal logic, and this via a lattice-embedding of orthomodular lattices into Boolean algebras with one modal operator. Thus our classical logic is a completion of the quantum logic QL. In other words, we refute Birkhoff and von Neumann's classic thesis that the logic (the formal character) of Quantum Mechanics would be non-classical as well as Putnam's thesis that quantum logic (of his kind) would be the correct logic for propositional inference in general. The propositional logic of Quantum Mechanics is modal but classical, and the correct logic for propositional inference need not have an extroverted quantum character. One normal necessity modality suffices to capture the subjectivity of observation in quantum experiments, and this thanks to its failure to distribute over classical disjunction. The key to our result is the translation of quantum negation as classical negation of observability. 	
1411.5710v1	http://arxiv.org/pdf/1411.5710v1	2014	Quantum annealing: the fastest route to quantum computation?	C. R. Laumann|R. Moessner|A. Scardicchio|S. L. Sondhi	  In this review we consider the performance of the quantum adiabatic algorithm for the solution of decision problems. We divide the possible failure mechanisms into two sets: small gaps due to quantum phase transitions and small gaps due to avoided crossings inside a phase. We argue that the thermodynamic order of the phase transitions is not predictive of the scaling of the gap with the system size. On the contrary, we also argue that, if the phase surrounding the problem Hamiltonian is a Many-Body Localized (MBL) phase, the gaps are going to be typically exponentially small and that this follows naturally from the existence of local integrals of motion in the MBL phase. 	
1412.1020v2	http://arxiv.org/pdf/1412.1020v2	2015	A new failure mechanism in thin film by collaborative fracture and   delamination: interacting duos of cracks	Joel Marthelot|Jose Bico|Francisco Melo|Benoit Roman	  When a thin film moderately adherent to a substrate is subjected to residual stress, the cooperation between fracture and delamination leads to unusual fracture patterns, such as spirals, alleys of crescents and various types of strips, all characterized by a robust characteristic length scale. We focus on the propagation of a duo of cracks: two fractures in the film connected by a delamination front and progressively detaching a strip. We show experimentally that the system selects an equilibrium width on the order of 25 times the thickness of the coating and independent of both fracture and adhesion energies. We investigate numerically the selection of the width and the condition for propagation by considering Griffith's criterion and the principle of local symmetry. In addition, we propose a simplified model based on the criterion of maximum of energy release rate, which provides insights of the physical mechanisms leading to these regular patterns, and predicts the effect of material properties on the selected width of the detaching strip. 	
1501.06590v1	http://arxiv.org/pdf/1501.06590v1	2015	Fluctuating Nonlinear Spring Model of Mechanical Deformation of   Biological Particles	Olga Kononova|Joost Snijder|Kenneth A. Marx|Gijs J. L. Wuite|Wouter H. Roos|Valeri Barsegov	  We present a new theory for modeling forced indentation spectral lineshapes of biological particles, which considers non-linear Hertzian deformation due to an indenter-particle physical contact and bending deformations of curved beams modeling the particle structure. The bending of beams beyond the critical point triggers the particle dynamic transition to the collapsed state, an extreme event leading to the catastrophic force drop as observed in the force (F)-deformation (X) spectra. The theory interprets fine features of the spectra: the slope of the FX curves and the position of force-peak signal, in terms of mechanical characteristics --- the Young's moduli for Hertzian and bending deformations E_H and E_b, and the probability distribution of the maximum strength with the strength of the strongest beam F_b^* and the beams' failure rate m. The theory is applied to successfully characterize the $FX$ curves for spherical virus particles --- CCMV, TrV, and AdV. 	
1505.05163v3	http://arxiv.org/pdf/1505.05163v3	2015	Inflexibility and independence: Phase transitions in the majority-rule   model	Nuno Crokidakis|Paulo Murilo Castro de Oliveira	  In this work we study opinion formation in a population participating of a public debate with two distinct choices. We considered three distinct mechanisms of social interactions and individuals' behavior: conformity, nonconformity and inflexibility. The conformity is ruled by the majority-rule dynamics, whereas the nonconformity is introduced in the population as an independent behavior, implying the failure to attempted group influence. Finally, the inflexible agents are introduced in the population with a given density. These individuals present a singular behavior, in a way that their stubbornness makes them reluctant to change their opinions. We consider these effects separately and all together, with the aim to analyze the critical behavior of the system. We performed numerical simulations in some lattice structures and for distinct population sizes, and our results suggest that the different formulations of the model undergo order-disorder phase transitions in the same universality class of the Ising model. Some of our results are complemented by analytical calculations. 	
1509.00979v1	http://arxiv.org/pdf/1509.00979v1	2015	Mechanisms in Impact Fragmentation	Falk K. Wittel|Humberto A. Carmona|Ferenc Kun|Hans J. Herrmann	  The brittle fragmentation of spheres is studied numerically by a 3D Discrete Element Model. Large scale computer simulations are performed with models that consist of agglomerates of many spherical particles, interconnected by beam-truss elements. We focus on a detailed description of the fragmentation process and study several fragmentation mechanisms involved. The evolution of meridional cracks is studied in detail. These cracks are found to initiate in the inside of the specimen with quasi-periodic angular distribution and give a broad peak in the fragment mass distribution for large fragments that can be fitted by a two-parameter Weibull distribution. The results prove to be independent of the degree of disorder in the model, but mean fragment sizes scale with velocity. Our results reproduce many experimental observations of fragment shapes, impact energy dependence or mass distribution, and significantly improve the understanding of the fragmentation process for impact fracture since we have full access to the failure conditions and evolution. 	
1510.00374v1	http://arxiv.org/pdf/1510.00374v1	2015	Study on the fragmentation of shells	Falk K. Wittel|Ferenc Kun|Hans J. Herrmann|Bernd H. Kröplin	  Fragmentation can be observed in nature and in everyday life on a wide range of length scales and for all kinds of technical applications. Most studies on dynamic failure focus on the behaviour of bulk systems in one, two and three dimensions under impact and explosive loading, showing universal power law behaviour of fragment size distribution. However, hardly any studies have been devoted to fragmentation of shells. We present a detailed experimental and theoretical study on the fragmentation of closed thin shells of various materials, due to an excess load inside the system and impact with a hard wall. Characteristic fragmentation mechanisms are identified by means of a high speed camera and fragment shapes and mass distributions are evaluated. Theoretical rationalisation is given by means of stochastic break-up models and large-scale discrete element simulations with spherical shell systems under different extreme loading situations. By this we ex-plain fragment shapes and distributions and prove a power law for the fragment mass distribution. Satisfactory agreement between experimental findings and nu-merical predictions of the exponents of the power laws for the fragment shapes is obtained. 	
1511.04562v1	http://arxiv.org/pdf/1511.04562v1	2015	The breakdown of superlubricity by driving-induced commensurate   dislocations	Andrea Benassi|Ming Ma|Michael Urbakh|Andrea Vanossi	  In the framework of a Frenkel-Kontorova-like model, we address the robustness of the superlubricity phenomenon in an edge-driven system at large scales, highlighting the dynamical mechanisms leading to its failure due to the slider elasticity. The results of the numerical simulations perfectly match the length critical size derived from a parameter-free analytical model. By considering different driving and commensurability interface configurations, we explore the distinctive nature of the transition from superlubric to high-friction sliding states which occurs above the critical size, discovering the occurrence of previously undetected multiple dissipative jumps in the friction force as a function of the slider length. These driving-induced commensurate dislocations in the slider are then characterized in relation to their spatial localization and width, depending on the system parameters. Setting the ground to scale superlubricity up, this investigation provides a novel perspective on friction and nanomanipulation experiments and can serve as a theoretical basis for designing high-tech devices with specific superlow frictional features. 	
1601.00787v1	http://arxiv.org/pdf/1601.00787v1	2016	Intrinsic strength and failure behaviors of ultra-small single-walled   carbon nanotubes	Nguyen Tuan Hung|Do Van Truong|Vuong Van Thanh|Riichiro Saito	  The intrinsic mechanical strength of single-walled carbon nanotubes (SWNTs) within the diameter range of 0.3-0.8 nm has been studied based on ab initio density functional theory calculations. In contrast to predicting "smaller is stronger and more elastic" in nanomaterials, the strength of the SWNTs is significantly reduced when decreasing the tube diameter. The results obtained show that the Young`s modulus E significantly reduced in the ultra-small SWNTs with the diameter less than 0.4 nm originates from their very large curvature effect, while it is a constant of about 1.0 TPa, and independent of the diameter and chiral index for the large tube. We find that the Poisson`s ratio, ideal strength and ideal strain are dependent on the diameter and chiral index. Furthermore, the relations between E and ideal strength indicate that Griffith`s estimate of brittle fracture could break down in the smallest (2, 2) nanotube, with the breaking strength of 15% of E. Our results provide important insights into intrinsic mechanical behavior of ultra-small SWNTs under their curvature effect. 	
1604.08244v2	http://arxiv.org/pdf/1604.08244v2	2016	Cosmological Galaxy Evolution with Superbubble Feedback II: The Limits   of Supernovae	B. W. Keller|J. Wadsley|H. M. P Couchman	  We explore when supernovae can (and cannot) regulate the star formation and bulge growth in galaxies based on a sample of 18 simulated galaxies. The simulations include key physics such as evaporation and conduction, neglected in prior work, and required to correctly model superbubbles resulting from stellar feedback. We show that for galaxies with virial masses $>10^{12}\;M_\odot$, supernovae alone cannot prevent excessive star formation. This failure occurs due to a shutdown of galactic winds, with wind mass loadings falling from $\eta\sim10$ to $\eta<1$. In more massive systems, this transfer of baryons to the circumgalactic medium falters earlier on and the galaxies diverge significantly from observed galaxy scaling relations and morphologies. The decreasing efficiency is simply due to a deepening potential well preventing gas escape. This implies that non-supernova feedback mechanisms must become dominant for galaxies with stellar masses greater than $\sim4\times10^{10}\;M_\odot$. The runaway growth of the central stellar bulge, strongly linked to black hole growth, suggests that feedback from active galactic nuclei is the probable mechanism. Below this mass, supernovae alone are able to produce a realistic stellar mass fraction, star formation history and disc morphology. 	
1605.01944v2	http://arxiv.org/pdf/1605.01944v2	2016	SDNsec: Forwarding Accountability for the SDN Data Plane	Takayuki Sasaki|Christos Pappas|Taeho Lee|Torsten Hoefler|Adrian Perrig	  SDN promises to make networks more flexible, programmable, and easier to manage. Inherent security problems in SDN today, however, pose a threat to the promised benefits. First, the network operator lacks tools to proactively ensure that policies will be followed or to reactively inspect the behavior of the network. Second, the distributed nature of state updates at the data plane leads to inconsistent network behavior during reconfigurations. Third, the large flow space makes the data plane susceptible to state exhaustion attacks.   This paper presents SDNsec, an SDN security extension that provides forwarding accountability for the SDN data plane. Forwarding rules are encoded in the packet, ensuring consistent network behavior during reconfigurations and limiting state exhaustion attacks due to table lookups. Symmetric-key cryptography is used to protect the integrity of the forwarding rules and enforce them at each switch. A complementary path validation mechanism allows the controller to reactively examine the actual path taken by the packets. Furthermore, we present mechanisms for secure link-failure recovery and multicast/broadcast forwarding. 	
1606.03474v2	http://arxiv.org/pdf/1606.03474v2	2016	On degeneracy control in overcomplete ICA	Jesse A. Livezey|Alejandro F. Bujan|Friedrich T. Sommer	  Understanding the effects of degeneracy control mechanisms when learning overcomplete representations is crucial for applying Independent Components Analysis (ICA) in machine learning and theoretical neuroscience. A number of approaches to degeneracy control have been proposed which can learn non-degenerate complete representations, however some of these methods can fall into bad local minima when extended to overcomplete ICA. Furthermore, they may have unintended side-effects on the distribution of learned basis elements, which may lead to a biased exploration of the data manifold. In this work, we identify and theoretically analyze the cause of these failures and propose a framework that can be used to evaluate arbitrary degeneracy control mechanisms. We evaluate different methods for degeneracy control in overcomplete ICA and suggest two novel approaches, one of which can learn highly orthonormal bases. Finally, we compare all methods on the task of estimating an overcomplete basis on natural images. 	
1608.00198v2	http://arxiv.org/pdf/1608.00198v2	2016	Existence of localizing solutions in plasticity via geometric singular   perturbation theory	Min-Gi Lee|Athanasios Tzavaras	  Shear bands are narrow zones of intense shear observed during plastic deformations of metals at high strain rates. Because they often precede rupture, their study attracted attention as a mechanism of material failure. Here, we aim to reveal the onset of localization into shear bands using a simple model developed from viscoplasticity. We exploit the properties of scale invariance of the model to construct a family of self-similar focusing solutions that capture the nonlinear mechanism of shear band formation. The key step is to de-singularize a reduced system of singular ordinary differential equations and reduce the problem into the construction of a heteroclinic orbit for an autonomous system of three first-order equations. The associated dynamical system has fast and slow time scales, forming a singularly perturbed problem. The geometric singular perturbation theory is applied to this problem to achieve an invariant surface. The flow on the invariant surface is analyzed via the Poincar\'{e}-Bendixson theorem to construct a heteroclinic orbit. 	
1611.04748v3	http://arxiv.org/pdf/1611.04748v3	2017	Improved Handover Through Dual Connectivity in 5G mmWave Mobile Networks	Michele Polese|Marco Giordani|Marco Mezzavilla|Sundeep Rangan|Michele Zorzi	  The millimeter wave (mmWave) bands offer the possibility of orders of magnitude greater throughput for fifth generation (5G) cellular systems. However, since mmWave signals are highly susceptible to blockage, channel quality on any one mmWave link can be extremely intermittent. This paper implements a novel dual connectivity protocol that enables mobile user equipment (UE) devices to maintain physical layer connections to 4G and 5G cells simultaneously. A novel uplink control signaling system combined with a local coordinator enables rapid path switching in the event of failures on any one link. This paper provides the first comprehensive end-to-end evaluation of handover mechanisms in mmWave cellular systems. The simulation framework includes detailed measurement-based channel models to realistically capture spatial dynamics of blocking events, as well as the full details of MAC, RLC and transport protocols. Compared to conventional handover mechanisms, the study reveals significant benefits of the proposed method under several metrics. 	
1612.09230v2	http://arxiv.org/pdf/1612.09230v2	2017	Ideal strength of two-dimensional stanene may reach or exceed Griffith   strength estimate	Zhe Shi|Chandra Veer Singh	  The ideal strength is the maximum stress a material can withstand, and it is an important intrinsic property for structural applications. Griffith strength limit ~E/9 is the best known upper bound of this property for a material loaded in tension. Here we report that stanene, a recently fabricated two-dimensional material, could approach and possibly exceed this limit from a theoretical perspective. Utilizing first-principles density functional theory, we investigated the nonlinear elastic behavior of stanene and found that its strength could reach ~E/7.4 under uniaxial tension in both armchair and zigzag directions without incurring phonon instability or mechanical failure. The unique mechanical properties of stanene are further appreciated by comparisons with two other Group-IV 2D materials, graphene and silicene. 	
1702.07783v1	http://arxiv.org/pdf/1702.07783v1	2017	Generalized Knudsen Number for Unsteady Fluid Flow	Vural Kara|Victor Yakhot|Kamil L. Ekinci	  We explore the scaling behavior of an unsteady flow that is generated by an oscillating body of finite size in a gas. If the gas is gradually rarefied, the Navier-Stokes equations begin to fail and a kinetic description of the flow becomes more appropriate. The failure of the Navier-Stokes equations can be thought to take place via two different physical mechanisms: either the continuum hypothesis breaks down as a result of a finite size effect; or local equilibrium is violated due to the high rate of strain. By independently tuning the relevant linear dimension and the frequency of the oscillating body, we can experimentally observe these two different physical mechanisms. All the experimental data, however, can be collapsed using a single dimensionless scaling parameter that combines the relevant linear dimension and the frequency of the body. This proposed Knudsen number for an unsteady flow is rooted in a fundamental symmetry principle, namely Galilean invariance. 	
1703.02245v2	http://arxiv.org/pdf/1703.02245v2	2017	Design of the Artificial: lessons from the biological roots of general   intelligence	Nima Dehghani	  Our desire and fascination with intelligent machines dates back to the antiquity's mythical automaton Talos, Aristotle's mode of mechanical thought (syllogism) and Heron of Alexandria's mechanical machines and automata. However, the quest for Artificial General Intelligence (AGI) is troubled with repeated failures of strategies and approaches throughout the history. This decade has seen a shift in interest towards bio-inspired software and hardware, with the assumption that such mimicry entails intelligence. Though these steps are fruitful in certain directions and have advanced automation, their singular design focus renders them highly inefficient in achieving AGI. Which set of requirements have to be met in the design of AGI? What are the limits in the design of the artificial? Here, a careful examination of computation in biological systems hints that evolutionary tinkering of contextual processing of information enabled by a hierarchical architecture is the key to build AGI. 	
1703.10045v1	http://arxiv.org/pdf/1703.10045v1	2017	Machining of Spherical Component Fabricated by Selected Laser Melting,   Part II: Application of Ti in Biomedical	AmirMahyar Khorasani	  Ti and Ti-Based alloys have unique properties such as high strength, low density and excellent corrosion resistance. These properties are essential for the manufacture of lightweight and high strength components for biomedical applications. In this paper, Ti properties such as metallurgy, mechanical properties, surface modification, corrosion resistance, biocompatibility and osseointegration in biomedical applications have been discussed. This paper also analyses the advantages and disadvantages of various Ti manufacturing processes for biomedical applications such as casting, powder metallurgy, cold and hot working, machining, laser engineering net shaping, superplastic forming, forging and ring rolling. The contributions of this research are twofold, firstly scrutinizing the behaviour of Ti and Ti-Based alloys in-vivo and in-vitro experiments in biomedical applications to determine the factors leading to failure, and secondly strategies to achieve desired properties essential to improving the quality of patient outcomes after receiving surgical implants. Future research will be directed toward manufacturing of Ti for medical applications by improving the production process, for example using optimal design approaches in additive manufacturing and investigating alloys containing other materials in order to obtain better medical and mechanical characteristics. 	
1704.00617v1	http://arxiv.org/pdf/1704.00617v1	2017	$α$Check: A mechanized metatheory model-checker	James Cheney|Alberto Momigliano	  The problem of mechanically formalizing and proving metatheoretic properties of programming language calculi, type systems, operational semantics, and related formal systems has received considerable attention recently. However, the dual problem of searching for errors in such formalizations has attracted comparatively little attention. In this article, we present $\alpha$Check, a bounded model-checker for metatheoretic properties of formal systems specified using nominal logic. In contrast to the current state of the art for metatheory verification, our approach is fully automatic, does not require expertise in theorem proving on the part of the user, and produces counterexamples in the case that a flaw is detected. We present two implementations of this technique, one based on negation-as-failure and one based on negation elimination, along with experimental results showing that these techniques are fast enough to be used interactively to debug systems as they are developed. 	
1706.00536v2	http://arxiv.org/pdf/1706.00536v2	2017	Modeling Latent Attention Within Neural Networks	Christopher Grimm|Dilip Arumugam|Siddharth Karamcheti|David Abel|Lawson L. S. Wong|Michael L. Littman	  Deep neural networks are able to solve tasks across a variety of domains and modalities of data. Despite many empirical successes, we lack the ability to clearly understand and interpret the learned internal mechanisms that contribute to such effective behaviors or, more critically, failure modes. In this work, we present a general method for visualizing an arbitrary neural network's inner mechanisms and their power and limitations. Our dataset-centric method produces visualizations of how a trained network attends to components of its inputs. The computed "attention masks" support improved interpretability by highlighting which input attributes are critical in determining output. We demonstrate the effectiveness of our framework on a variety of deep neural network architectures in domains from computer vision, natural language processing, and reinforcement learning. The primary contribution of our approach is an interpretable visualization of attention that provides unique insights into the network's underlying decision-making process irrespective of the data modality. 	
1706.02703v1	http://arxiv.org/pdf/1706.02703v1	2017	Topological resilience in non-normal networked systems	Malbor Asllani|Timoteo Carletti	  The network of interactions in complex systems, strongly influences their resilience, the system capability to resist to external perturbations or structural damages and to promptly recover thereafter. The phenomenon manifests itself in different domains, e.g. cascade failures in computer networks or parasitic species invasion in ecosystems. Understanding the networks topological features that affect the resilience phenomenon remains a challenging goal of the design of robust complex systems. We prove that the non-normality character of the network of interactions amplifies the response of the system to exogenous disturbances and can drastically change the global dynamics. We provide an illustrative application to ecology by proposing a mechanism to mute the Allee effect and eventually a new theory of patterns formation involving a single diffusing species. 	
1709.03286v1	http://arxiv.org/pdf/1709.03286v1	2017	Strand plasticity governs fatigue in colloidal gels	Jan Maarten van Doorn|Joanne E. Verweij|Joris Sprakel|Jasper van der Gucht	  Repeated loading of a solid leads to microstructural damage that ultimately results in catastrophic material failure. While posing a major threat to the stability of virtually all materials, the microscopic origins of fatigue, especially for soft solids, remain elusive. Here we explore fatigue in colloidal gels as prototypical inhomogeneous soft solids by combining experiments and computer simulations. Our results reveal how mechanical loading leads to irreversible strand stretching, which builds slack into the network that softens the solid at small strains and causes strain hardening at larger deformations. We thus find that microscopic plasticity governs fatigue at much larger scales. This gives rise to a new picture of fatigue in soft thermal solids and calls for new theoretical descriptions of soft gel mechanics in which local plasticity is taken into account. 	
1710.01846v1	http://arxiv.org/pdf/1710.01846v1	2017	Revisiting the Deformation-Induced Damage in Filled Elastomers: Effect   of Network Polydispersity	Mohammad Tehrani|Mohammad Hossein Moshaei|Alireza Sarvestani	  A priori assumption in micromechanical analysis of polymeric networks is that the constitutive polymer strands are of equal length. Monodisperse distribution of strands, however, is merely a simplifying assumption. In this paper, we relax this assumption and consider a vulcanized network with a broad distribution of strand length. In the light of this model, we predict the damage initiation and stress-stretch dependency in a filled polymer network with random internal structures. The degradation of network mechanical behavior is assumed to be controlled by the adhesive failure of the strands adsorbed to the filler surface. We show that the short adsorbed strands are the culprits for damage initiation and their finite extensibility is a key determinant of mechanical strength. 	
1712.05896v1	http://arxiv.org/pdf/1712.05896v1	2017	Impression Network for Video Object Detection	Congrui Hetang|Hongwei Qin|Shaohui Liu|Junjie Yan	  Video object detection is more challenging compared to image object detection. Previous works proved that applying object detector frame by frame is not only slow but also inaccurate. Visual clues get weakened by defocus and motion blur, causing failure on corresponding frames. Multi-frame feature fusion methods proved effective in improving the accuracy, but they dramatically sacrifice the speed. Feature propagation based methods proved effective in improving the speed, but they sacrifice the accuracy. So is it possible to improve speed and performance simultaneously?   Inspired by how human utilize impression to recognize objects from blurry frames, we propose Impression Network that embodies a natural and efficient feature aggregation mechanism. In our framework, an impression feature is established by iteratively absorbing sparsely extracted frame features. The impression feature is propagated all the way down the video, helping enhance features of low-quality frames. This impression mechanism makes it possible to perform long-range multi-frame feature fusion among sparse keyframes with minimal overhead. It significantly improves per-frame detection baseline on ImageNet VID while being 3 times faster (20 fps). We hope Impression Network can provide a new perspective on video feature enhancement. Code will be made available. 	
1801.02447v1	http://arxiv.org/pdf/1801.02447v1	2018	Synchronized oscillations and acoustic fluidization in confined granular   materials	F. Giacco|L. de Arcangelis|M. Pica Ciamarra|E. Lippiello	  According to the acoustic fluidization hypothesis, elastic waves at a characteristic frequency form inside seismic faults even in the absence of an external perturbation. These waves are able to generate a normal stress which contrasts the confining pressure and promotes failure. Here, we study the mechanisms responsible for this wave activation via numerical simulations of a granular fault model. We observe the particles belonging to the percolating backbone, which sustains the stress, to perform synchronized oscillations over ellipticlike trajectories in the fault plane. These oscillations occur at the characteristic frequency of acoustic fluidization. As the applied shear stress increases, these oscillations become perpendicular to the fault plane just before the system fails, opposing the confining pressure, consistently with the acoustic fluidization scenario. The same change of orientation can be induced by external perturbations at the acoustic fluidization frequency. 	
1802.03921v1	http://arxiv.org/pdf/1802.03921v1	2018	Test Agents: Adaptive, Autonomous and Intelligent Test Cases	Eduard Enoiu|Mirgita Frasheri	  Growth of software size, lack of resources to perform regression testing, and failure to detect bugs faster have seen increased reliance on continuous integration and test automation. Even with greater hardware and software resources dedicated to test automation, software testing is faced with enormous challenges, resulting in increased dependence on complex mechanisms for automated test case selection and prioritization as part of a continuous integration framework. These mechanisms are currently using simple entities called test cases that are concretely realized as executable scripts. Our key idea is to provide test cases with more reasoning, adaptive behavior and learning capabilities by using the concepts of intelligent software agents. We refer to such test cases as test agents. The model that underlie a test agent is capable of flexible and autonomous actions in order to meet overall testing objectives. Our goal is to increase the decentralization of regression testing by letting test agents to know for themselves when they should be executing, how they should update their purpose, and when they should interact with each other. In this paper, we envision software test agents that display such adaptive autonomous behavior. Emerging developments and challenges regarding the use of test agents are explored-in particular, new research that seeks to use adaptive autonomous agents in software testing. 	
1802.06246v1	http://arxiv.org/pdf/1802.06246v1	2018	Backlash Identification in Two-Mass Systems by Delayed Relay Feedback	Michael Ruderman|Shota Yamada|Hiroshi Fujimoto	  Backlash, also known as mechanical play, is a piecewise differentiable nonlinearity which exists in several actuated systems, comprising, e.g., rack-and-pinion drives, shaft couplings, toothed gears, and other elements. Generally, the backlash is nested between the moving elements of a complex dynamic system, which handicaps its proper detection and identification. A classical example is the two-mass system which can approximate numerous mechanisms connected by a shaft (or link) with relatively high stiffness and backlash in series. Information about the presence and extent of the backlash is seldom exactly known and is rather conditional upon factors such as wear, fatigue and incipient failures in components. This paper proposes a novel backlash identification method using one-side sensing of a twomass system. The method is based on the delayed relay operator in feedback that allows stable and controllable limit cycles to be induced, operating within the unknown backlash gap. The system model, with structural transformations required for the one-side backlash measurements, is given, along with the analysis of the delayed relay in velocity feedback. Experimental evaluations are shown for a two-inertia motor bench with gear coupling, with a low backlash gap of about one degree. 	
0512118v2	http://arxiv.org/pdf/physics/0512118v2	2007	Komatiites: From Earth's Geological Settings to Planetary and   Astrobiological Contexts	Delphine Nna-Mvondo|Jesus Martinez-Frias	  Komatiites are fascinating volcanic rocks. They are among the most ancient lavas of the Earth following the 3.8 Ga pillow basalts at Isua and they represent some of the oldest ultramafic magmatic rocks preserved in the Earth's crust at 3.5 Ga. This fact, linked to their particular features (high magnesium content, high melting temperatures, low dynamic viscosities, etc.), has attracted the community of geoscientists since their discovery in the early sixties, who have tried to determine their origin and understand their meaning in the context of the terrestrial mantle evolution. In addition, it has been proposed that komatiites are not restricted to our planet, but they could be found in other extraterrestrial setting in our Solar System (particularly in Mars and Io). It is important to note that komatiites may be extremely significant in the study of the origins and evolution of Life on Earth. They not only preserve essential geochemical clues of the interaction between the pristine Earth rocks and atmosphere, but also may have been potential suitable sites for biological processes to develop. Thus, besides reviewing the main geodynamic, petrological and geochemical characteristics of komatiites, this paper also aims to widen their investigation beyond the classical geological prospect, calling attention to them as attracting rocks for research in planetology and astrobiology. 	
0708.4379v1	http://arxiv.org/pdf/0708.4379v1	2007	A system for the simulation of simultaneous moves between two   noncolocational players	Marisa Debowsky|Adrian Riskin	  We describe a new system for the simulation of simultaneous moves between noncolocational players. This has applications in the burgeoning Rock-Paper-Scissors by mail movement. 	
1106.3827v1	http://arxiv.org/pdf/1106.3827v1	2011	Giant impacts in the Saturnian System: a possible origin of diversity in   the inner mid-sized satellites	Yasuhito Sekine|Hidenori Genda	  It is widely accepted that Titan and the mid-sized regular satellites around Saturn were formed in the circum-Saturn disk. Thus, if these mid-sized satellites were simply accreted by collisions of similar ice-rock satellitesimals in the disk, the observed wide diversity in density (i.e., the rock fraction) of the Saturnian mid-sized satellites is enigmatic. A recent circumplanetary disk model suggests satellite growth in an actively supplied circumplanetary disk, in which Titan-sized satellites migrate inward by interaction with the gas and are eventually lost to the gas planet. Here we report numerical simulations of giant impacts between Titan-sized migrating satellites and smaller satellites in the inner region of the Saturnian disk. Our results suggest that in a giant impact with impact velocity > 1.4 times the escape velocity and impact angle of ~45 degree, a smaller satellite is destroyed, forming multiple mid-sized satellites with a very wide diversity in satellite density (the rock fraction = 0-92 wt%). Our results of the relationship between the mass and rock fraction of the satellites resulting from giant impacts reproduce the observations of the Saturnian mid-sized satellites. Giant impacts also lead to internal melting of the formed mid-sized satellites, which would initiate strong tidal dissipation and geological activity, such as those observed on Enceladus today and Tethys in the past. Our findings also imply that giant impacts might have affected the fundamental physical property of the Saturnian mid-sized satellites as well as those of the terrestrial planets in the solar system and beyond. 	
1311.0553v1	http://arxiv.org/pdf/1311.0553v1	2013	The density of mid-sized Kuiper belt object 2002 UX25 and the formation   of the dwarf planets	Michael E. Brown	  The formation of the largest objects in the Kuiper belt, with measured densities of ~1.5 g cm-3 and higher, from the coagulation of small bodies, with measured densities below 1 g cm-3 is difficult to explain without invoking significant porosity in the smallest objects. If such porosity does occur, measured densities should begin to increase at the size at which significant porosity is no longer supported. Among the asteroids, this transition occurs for diameters larger than ~350 km. In the Kuiper belt, no density measurements have been made between ~350 km and ~850 km, the diameter range where porosities might first begin to drop. Objects in this range could provide key tests of the rock fraction of small Kuiper belt objects. Here we report the orbital characterization, mass, and density determination of the 2002 UX25 system in the Kuiper belt. For this object, with a diameter of ~650 km, we find a density of 0.82+/-0.11 g cm-3, making it the largest solid known object in the solar system with a measured density below that of pure water ice. We argue that the porosity of this object is unlikely to be above ~20%, suggesting a low rock fraction. If the currently measured densities of Kuiper belt objects are a fair representation of the sample as a whole, creating ~1000 km and larger Kuiper belt objects with rock mass fractions of 70% and higher from coagulation of small objects with rock fractions as low as those inferred from 2002 UX25 is difficult. 	
1601.06078v3	http://arxiv.org/pdf/1601.06078v3	2016	Measurements of high-frequency acoustic scattering from glacially-eroded   rock outcrops	Derek R. Olson|Anthony P. Lyons|Torstein Sæbø	  Measurements of acoustic backscattering from glacially-eroded rock outcrops were made off the coast of Sandefjord, Norway using a high-frequency synthetic aperture sonar (SAS) system. A method by which scattering strength can be estimated from data collected by a SAS system is detailed, as well as a method to estimate an effective calibration parameter for the system. Scattering strength measurements from very smooth areas of the rock outcrops agree with predictions from both the small-slope approximation and perturbation theory, and range between -33 and -26 dB at 20$^\circ$ grazing angle. Scattering strength measurements from very rough areas of the rock outcrops agree with the sine-squared shape of the empirical Lambertian model and fall between -30 and -20 dB at 20$^\circ$ grazing angle. Both perturbation theory and the small-slope approximation are expected to be inaccurate for the very rough area, and overestimate scattering strength by 8 dB or more for all measurements of very rough surfaces. Supporting characterization of the environment was performed in the form of geoacoustic and roughness parameter estimates. 	
1606.05089v1	http://arxiv.org/pdf/1606.05089v1	2016	Ultra-high Sensitivity Moment Magnetometry of Geological Samples Using   Magnetic Microscopy	Eduardo A. Lima|Benjamin P. Weiss	  Paleomagnetically useful information is expected to be recorded by samples with moments up to three orders of magnitude below the detection limit of standard superconducting rock magnetometers. Such samples are now detectable using recently developed magnetic microscopes, which map the magnetic fields above room-temperature samples with unprecedented spatial resolutions and field sensitivities. However, realizing this potential requires the development of techniques for retrieving sample moments from magnetic microscopy data. With this goal, we developed a technique for uniquely obtaining the net magnetic moment of geological samples from magnetic microscopy maps of unresolved or nearly unresolved magnetization. This technique is particularly powerful for analyzing small, weakly magnetized samples such as meteoritic chondrules and terrestrial silicate crystals like zircons. We validated this technique by applying it to field maps generated from synthetic sources and also to field maps measured using a superconducting quantum interference device (SQUID) microscope above geological samples with moments down to 10^-15 Am2. For the most magnetic rock samples, the net moments estimated from the SQUID microscope data are within error of independent moment measurements acquired using lower sensitivity standard rock magnetometers. In addition to its superior moment sensitivity, SQUID microscope net moment magnetometry also enables the identification and isolation of magnetic contamination and background sources, which is critical for improving accuracy in paleomagnetic studies of weakly magnetic rocks. 	
1608.03794v1	http://arxiv.org/pdf/1608.03794v1	2016	On the theory of solitons of fluid pressure and solute density in   geologic porous media, with applications to shale, clay and sandstone	A. Caserta|R. Kanivetsky|E. Salusti	  In this paper we propose the application of a new model of transients of pore pressure p and solute density \r{ho} in geologic porous media. This model is rooted in the non-linear waves theory, the focus of which is advection and effect of large pressure jumps on strain (due to large p in a non-linear version of the Hooke law). It strictly relates p and \r{ho} evolving under the effect of a strong external stress. As a result, the presence of quick and sharp transients in low permeability rocks is unveiled, i.e. the non-linear Burgers solitons. We therefore propose that the actual transport process in porous rocks for large signals is not the linear diffusion, but could be governed by solitons. A test of an eventual presence of solitons in a rock is here proposed, and then applied to Pierre Shale, Bearpaw Shale, Boom Clay and Oznam-Mugu silt and clay. A quick analysis showing the presence of solitons for nuclear waste disposal and salty water intrusions is also analyzed. Finally, in a kind of "theoretical experiment" we show that solitons could also be present in Jordan and St. Peter sandstones, thus suggesting the occurrence of osmosis in these rocks. 	
1703.06407v1	http://arxiv.org/pdf/1703.06407v1	2017	Neutron Production by Cosmic-Ray Muons in Various Materials	K. V. Manukovsky|O. G. Ryazhskaya|N. M. Sobolevsky|A. V. Yudin	  The results obtained by studying the background of neutrons produced by cosmic-ray muons in underground experimental facilities intended for rare-event searches and in surrounding rock are presented. The types of this rock may include granite, sedimentary rock, gypsum, and rock salt. Neutron production and transfer were simulated using the Geant4 and SHIELD transport codes. These codes were tuned via a comparison of the results of calculations with experimental data: in particular, with data of the Artemovsk research station of the Institute for Nuclear Research (INR, Moscow, Russia), as well as via an intercomparison of results of calculations with the Geant4 and SHIELD codes. It turns out that the atomic-number dependence of the production and yield of neutrons has an irregular character and does not allow a description in terms of a universal function of the atomic number. The parameters of this dependence are different for two groups of nuclei-nuclei consisting of alpha particles and all of the remaining nuclei. Moreover, there are manifest exceptions from a power-law dependence, for example, argon. This may entail important consequences both for the existing underground experimental facilities and for those under construction. Investigation of cosmic-ray-induced neutron production in various materials is of paramount importance for the interpretation of experiments conducted at large depths under the Earth's surface. 	
1712.09016v1	http://arxiv.org/pdf/1712.09016v1	2017	Geochemical discrimination and characteristics of magmatic tectonic   settings; a machine learning-based approach	Kenta Ueki|Hideitsu Hino|Tatsu Kuwatani	  Geochemically discriminating between magmatism in different tectonic settings remains a fundamental part of understanding the processes of magma generation within the Earth's mantle. Here, we present an approach where machine-learning (ML) methods are used for quantitative tectonic discrimination and feature selection using global geochemical datasets containing data for volcanic rocks generated in eight different tectonic settings. This study uses support vector machine, random forest, and sparse multinomial regression (SMR) approaches. All these ML methods with data for 20 elements and 5 isotopic ratios allowed the successful geochemical discrimination between igneous rocks formed in eight different tectonic settings with a discriminant ratio better than 83% for all settings barring oceanic plateaus and back-arc basins. SMR is a particularly powerful and interpretable ML method because it quantitatively identifies geochemical signatures that characterize the tectonic settings of interest and the characteristics of each sample as a probability of the membership of the sample for each setting. We also present the most representative basalt composition for each tectonic setting. The new data provide reference points for future geochemical discussions. Our results indicate that at least 17 elements and isotopic ratios are required to characterize each tectonic setting, suggesting that geochemical tectonic discrimination cannot be achieved using only a small number of elemental compositions and/or isotopic ratios. The results show that volcanic rocks formed in different tectonic settings have unique geochemical signatures, indicating that both volcanic rock geochemistry and magma generation processes are closely connected to the tectonic setting. 	
0606009v2	http://arxiv.org/pdf/astro-ph/0606009v2	2007	The Eccentricity-Mass Distribution of Exoplanets: Signatures of   Different Formation Mechanisms?	Ignasi Ribas|Jordi Miralda-Escude	  We examine the distributions of eccentricity and host star metallicity of exoplanets as a function of their mass. Planets with M sin i >~ 4 M_J have an eccentricity distribution consistent with that of binary stars, while planets with M sin i <~ 4 M_J are less eccentric than binary stars and more massive planets. In addition, host star metallicities decrease with planet mass. The statistical significance of both of these trends is only marginal with the present sample of exoplanets. To account for these trends, we hypothesize that there are two populations of gaseous planets: the low-mass population forms by gas accretion onto a rock-ice core in a circumstellar disk and is more abundant at high metalliticities, and the high-mass population forms directly by fragmentation of a pre-stellar cloud. Planets of the first population form in initially circular orbits and grow their eccentricities later, and may have a mass upper limit from the total mass of the disk that can be accreted by the core. The second population may have a mass lower limit resulting from opacity-limited fragmentation. This would roughly divide the two populations in mass, although they would likely overlap over some mass range. If most objects in the second population form before the pre-stellar cloud becomes highly opaque, they would have to be initially located in orbits larger than ~30 AU, and would need to migrate to the much smaller orbits in which they are observed. The higher mean orbital eccentricity of the second population might be caused by the larger required intervals of radial migration, and the brown dwarf desert might be due to the inability of high-mass brown dwarfs to migrate inwards sufficiently in radius. 	
9809420v3	http://arxiv.org/pdf/hep-ph/9809420v3	1999	Slow Magnetic Monopole: Interaction with Matter and New Possibility of   Their Detection	I. V. Kolokolov|P. V. Vorob'ev|V. V. Ianovski	  The possibility of existence of a magnetic monopole has been surveyed by P. Dirac even in 1931, and then from the point of view of the modern theory by A.M. Polyakov and G.~ 'tHooft in 1974. Numerous and unsuccessful attempts of experimental search for monopole in cosmic rays and on accelerators in high energy particle collisions have been done. Also the searches have been carried out in mica for monopole tracks as well as for relict monopoles, entrapped by ferromagnetic inclusions in iron-ores, moon rock and meteorites. These entrapped monopoles, when released, would have the lowest velocities $\beta<10^{-6}$ and do not yield ionization at all, and are hard to detect. Therefore it is necessary to examine thoroughly the mechanisms of slow heavy monopole interaction with matter and their scale of energy loss.   We discuss here the interaction of a massive slow magnetic monopole with magnetically ordered matter, with conductors, superconductors and with condensed matter in general. Our results indicate that the energy loss of a slow supermassive monopole reach $10^{8} eV/cm$ and more if we take into consideration the Cherenkov radiation of magnons or phonons and conductivity of the media. A new method of search for cosmic and relict monopoles by magnetically ordered film is considered too. This approach resembles the traditional method of nuclear emulsion chamber. Apparently the proposed method is particularly attractive for detection of relict monopoles, released from melting iron ore. 	
0704.0357v3	http://arxiv.org/pdf/0704.0357v3	2008	Evolutionary games on minimally structured populations	Gergely J Szollosi|Imre Derenyi	  Population structure induced by both spatial embedding and more general networks of interaction, such as model social networks, have been shown to have a fundamental effect on the dynamics and outcome of evolutionary games. These effects have, however, proved to be sensitive to the details of the underlying topology and dynamics. Here we introduce a minimal population structure that is described by two distinct hierarchical levels of interaction. We believe this model is able to identify effects of spatial structure that do not depend on the details of the topology. We derive the dynamics governing the evolution of a system starting from fundamental individual level stochastic processes through two successive meanfield approximations. In our model of population structure the topology of interactions is described by only two parameters: the effective population size at the local scale and the relative strength of local dynamics to global mixing. We demonstrate, for example, the existence of a continuous transition leading to the dominance of cooperation in populations with hierarchical levels of unstructured mixing as the benefit to cost ratio becomes smaller then the local population size. Applying our model of spatial structure to the repeated prisoner's dilemma we uncover a novel and counterintuitive mechanism by which the constant influx of defectors sustains cooperation. Further exploring the phase space of the repeated prisoner's dilemma and also of the "rock-paper-scissor" game we find indications of rich structure and are able to reproduce several effects observed in other models with explicit spatial embedding, such as the maintenance of biodiversity and the emergence of global oscillations. 	
0809.3316v5	http://arxiv.org/pdf/0809.3316v5	2010	Magnetism, FeS colloids, and Origins of Life	Gargi Mitra-Delmotte|A. N. Mitra	  A number of features of living systems: reversible interactions and weak bonds underlying motor-dynamics; gel-sol transitions; cellular connected fractal organization; asymmetry in interactions and organization; quantum coherent phenomena; to name some, can have a natural accounting via $physical$ interactions, which we therefore seek to incorporate by expanding the horizons of `chemistry-only' approaches to the origins of life. It is suggested that the magnetic 'face' of the minerals from the inorganic world, recognized to have played a pivotal role in initiating Life, may throw light on some of these issues. A magnetic environment in the form of rocks in the Hadean Ocean could have enabled the accretion and therefore an ordered confinement of super-paramagnetic colloids within a structured phase. A moderate H-field can help magnetic nano-particles to not only overcome thermal fluctuations but also harness them. Such controlled dynamics brings in the possibility of accessing quantum effects, which together with frustrations in magnetic ordering and hysteresis (a natural mechanism for a primitive memory) could throw light on the birth of biological information which, as Abel argues, requires a combination of order and complexity. This scenario gains strength from observations of scale-free framboidal forms of the greigite mineral, with a magnetic basis of assembly. And greigite's metabolic potential plays a key role in the mound scenario of Russell and coworkers-an expansion of which is suggested for including magnetism. 	
1111.1674v2	http://arxiv.org/pdf/1111.1674v2	2012	On the relationship between cyclic and hierarchical three-species   predator-prey systems and the two-species Lotka-Volterra model	Qian He|Uwe C. Tauber|R. K. P. Zia	  We aim to clarify the relationship between interacting three-species models and the two-species Lotka-Volterra (LV) model. We utilize mean-field theory and Monte Carlo simulations on two-dimensional square lattices to explore the temporal evolution characteristics of two different interacting three-species predator-prey systems: (1) a cyclic rock-paper-scissors (RPS) model with conserved total particle number but strongly asymmetric reaction rates that lets the system evolve towards one corner of configuration space; (2) a hierarchical food chain where an additional intermediate species is inserted between the predator and prey in the LV model. For model variant (1), we demonstrate that the evolutionary properties of both minority species in the steady state of this stochastic spatial three-species corner RPS model are well approximated by the LV system, with its emerging characteristic features of localized population clustering, persistent oscillatory dynamics, correlated spatio-temporal patterns, and fitness enhancement through quenched spatial disorder in the predation rates. In contrast, we could not identify any regime where the hierarchical model (2) would reduce to the two-species LV system. In the presence of pair exchange processes, the system remains essentially well-mixed, and we generally find the Monte Carlo simulation results for the spatially extended model (2) to be consistent with the predictions from the corresponding mean-field rate equations. If spreading occurs only through nearest-neighbor hopping, small population clusters emerge; yet the requirement of an intermediate species cluster obviously disrupts spatio-temporal correlations between predator and prey, and correspondingly eliminates many of the intriguing fluctuation phenomena that characterize the stochastic spatial LV system. 	
1301.3831v4	http://arxiv.org/pdf/1301.3831v4	2016	Perspectives on effectively constraining the location of a massive   trans-Plutonian object with the New Horizons spacecraft: a sensitivity   analysis	Lorenzo Iorio	  The radio tracking apparatus of the New Horizons spacecraft, currently traveling to the Pluto system where its arrival is scheduled for July 2015, should be able to reach an accuracy of 10 m (range) and 0.1 mm s^-1 (range-rate) over distances up to 50 au. This should allow to effectively constrain the location of a putative trans-Plutonian massive object, dubbed Planet X (PX) hereafter, whose existence has recently been postulated for a variety of reasons connected with, e.g., the architecture of the Kuiper belt and the cometary flux from the Oort cloud. Traditional scenarios involve a rock-ice planetoid with mX = 0.7mE at some 100 - 200 au, or a Jovian body with mX = 5mJ at about 10,000 - 20,000 au; as a result of our preliminary sensitivity analysis, they should be detectable by New Horizons since they would impact its range at a km level or so over a time span six years long. Conversely, range residuals statistically compatible with zero having an amplitude of 10 m would imply that PX, if it exists, could not be located at less than about 4,500 au (mX = 0.7mE) or 60,000 au (mX = 5mJ), thus making a direct detection quite demanding with the present-day technologies. As a consequence, it would be appropriate to rename such a remote body as Telisto. Also fundamental physics would benefit from this analysis since certain subtle effects predicted by MOND for the deep Newtonian regions of our Solar System are just equivalent to those of a distant pointlike mass. 	
1310.4689v1	http://arxiv.org/pdf/1310.4689v1	2013	Morphological analysis of stylolites for paleostress estimation in   limestones surrounding the Andra Underground Research Laboratory site	Alexandra Rolland|Renaud Toussaint|Patrick Baud|Nathalie Conil|Philippe Landrein	  We develop and test a methodology to infer paleostress from the morphology of stylolites within borehole cores. This non-destructive method is based on the analysis of the stylolite trace along the outer cylindrical surface of the cores. It relies on an automatic digitization of high-resolution photographs and on the spatial Fourier spectrum analysis of the stylolite traces. We test and show, on both synthetic and natural examples, that the information from this outer cylindrical surface is equivalent to the one obtained from the destructive planar sections traditionally used. The assessment of paleostress from the stylolite morphology analysis is made using a recent theoretical model, which links the morphological properties to the physical processes acting during stylolite evolution. This model shows that two scaling regimes are to be expected for the stylolite height power spectrum, separated by a cross-over length that depends on the magnitude of the paleostress during formation. We develop a non linear fit method to automatically extract the cross-over lengths from the digitized stylolite profiles. Results on cores from boreholes drilled in the surroundings of the Andra Underground Research Laboratory located at Bure, France, show that different groups of sedimentary stylolites can be distinguished, and correspond to different estimated vertical paleostress values. For the Oxfordian formation, one group of stylolites indicate a paleostress of around 10 MPa, while another group yields 15 MPa. For the Dogger formation, two stylolites indicate a paleostress of around 10 MPa, while others appear to have stopped growing at paleostresses between 30 and 22 MPa, starting at an erosion phase that initiated in the late Cretaceous and continues today. This method has a high potential for further applications on reservoirs or other geological contexts where stylolites are present. 	
1403.3792v1	http://arxiv.org/pdf/1403.3792v1	2014	Globally synchronized oscillations in complex cyclic games	Charlotte Rulquin|Jeferson J. Arenzon	  The Rock-Paper-Scissors (RPS) game and its generalizations with ${\cal S}>3$ species are well studied models for cyclically interacting populations. Four is, however, the minimum number of species that, by allowing other interactions beyond the single, cyclic loop, breaks both the full intransitivity of the food graph and the one predator, one prey symmetry. L\"utz {\it et al} (J. Theor. Biol. {\bf 317} (2013) 286) have shown the existence, on a square lattice, of two distinct phases, with either four or three coexisting species. In both phases, each agent is eventually replaced by one of its predators but these strategy oscillations remain localized as long as the interactions are short ranged. Distant regions may be either out of phase or cycling through different food web subloops (if any). Here we show that upon replacing a minimum fraction $Q_c$ of the short range interactions by long range ones, there is a Hopf bifurcation and global oscillations become stable. Surprisingly, to build such long distance, global synchronization, the four species coexistence phase requires less long range interactions than the three species phase, while one would naively expect the contrary. Moreover, deviations from highly homogeneous conditions ($\chi=0$ or 1) increase $Q_c$ and the more heterogeneous is the food web, the harder the synchronization is. By further increasing $Q$, while the three species phase remains stable, the four species one has a transition to an absorbing, single species state. The existence of a phase with global oscillations for ${\cal S}>3$, when the interaction graph has multiple subloops and several possible local cycles, lead to the conjecture that global oscillations are a general characteristic, even for large, realistic food webs. 	
1506.09157v4	http://arxiv.org/pdf/1506.09157v4	2015	Tidal Evolution of Asteroidal Binaries. Ruled by Viscosity. Ignorant of   Rigidity	Michael Efroimsky	  The rate of tidal evolution of asteroidal binaries is defined by the dynamical Love numbers divided by quality factors. Common is the (often illegitimate) approximation of the dynamical Love numbers with their static counterparts. As the static Love numbers are, approximately, proportional to the inverse rigidity, this renders a popular fallacy that the tidal evolution rate is determined by the product of the rigidity by the quality factor: $\,k_l/Q\propto 1/(\mu Q)\,$. In reality, the dynamical Love numbers depend on the tidal frequency and all rheological parameters of the tidally perturbed body (not just rigidity). We demonstrate that in asteroidal binaries the rigidity of their components plays virtually no role in tidal friction and tidal lagging, and thereby has almost no influence on the intensity of tidal interactions (tidal torques, tidal dissipation, tidally induced changes of the orbit). A key quantity that determines the tidal evolution is a product of the effective viscosity $\,\eta\,$ by the tidal frequency $\,\chi\,$. The functional form of the torque's dependence on this product depends on who wins in the competition between viscosity and self-gravitation. Hence a quantitative criterion, to distinguish between two regimes. For higher values of $\,\eta\chi\,$ we get $\,k_l/Q\propto 1/(\eta\chi)\;$; $\,$while for lower values we obtain $\,k_l/Q\propto \eta\chi\,$. Our study rests on an assumption that asteroids can be treated as Maxwell bodies. Applicable to rigid rocks at low frequencies, this approximation is used here also for rubble piles, due to the lack of a better model. In the future, as we learn more about mechanics of granular mixtures in a weak gravity field, we may have to amend the tidal theory with other rheological parameters, ones that do not show up in the description of viscoelastic bodies. 	
1507.07577v3	http://arxiv.org/pdf/1507.07577v3	2016	Study of cosmic ray events with high muon multiplicity using the ALICE   detector at the CERN Large Hadron Collider	 ALICE Collaboration	  ALICE is one of four large experiments at the CERN Large Hadron Collider near Geneva, specially designed to study particle production in ultra-relativistic heavy-ion collisions. Located 52 meters underground with 28 meters of overburden rock, it has also been used to detect muons produced by cosmic ray interactions in the upper atmosphere. In this paper, we present the multiplicity distribution of these atmospheric muons and its comparison with Monte Carlo simulations. This analysis exploits the large size and excellent tracking capability of the ALICE Time Projection Chamber. A special emphasis is given to the study of high multiplicity events containing more than 100 reconstructed muons and corresponding to a muon areal density $\rho_{\mu} > 5.9~$m$^{-2}$. Similar events have been studied in previous underground experiments such as ALEPH and DELPHI at LEP. While these experiments were able to reproduce the measured muon multiplicity distribution with Monte Carlo simulations at low and intermediate multiplicities, their simulations failed to describe the frequency of the highest multiplicity events. In this work we show that the high multiplicity events observed in ALICE stem from primary cosmic rays with energies above $10^{16}$ eV and that the frequency of these events can be successfully described by assuming a heavy mass composition of primary cosmic rays in this energy range. The development of the resulting air showers was simulated using the latest version of QGSJET to model hadronic interactions. This observation places significant constraints on alternative, more exotic, production mechanisms for these events. 	
1508.07375v1	http://arxiv.org/pdf/1508.07375v1	2015	Incomplete cooling down of Saturn's A ring at solar equinox: Implication   for seasonal thermal inertia and internal structure of ring particles	Ryuji Morishima|Linda Spilker|Shawn Brooks|Estelle Deau|Stu Pilorz	  At the solar equinox in August 2009, the Composite Infrared Spectrometer (CIRS) onboard Cassini showed the lowest Saturn's ring temperatures ever observed. Detailed radiative transfer models show that the observed equinox temperatures of Saturn's A ring are much higher than model predictions as long as only the flux from Saturn is taken into account. This indicates that the A ring was not completely cooled down at the equinox. We develop a simple seasonal model for ring temperatures and first assume that the internal density and the thermal inertia of a ring particle are uniform with depth. The particle size is estimated to be 1-2 m. The seasonal thermal inertia is found to be 30-50 Jm$^{-2}$K$^{-1}$s$^{-1/2}$ in the middle A ring whereas it is $\sim$ 10 Jm$^{-2}$K$^{-1}$s$^{-1/2}$ or as low as the diurnal thermal inertia in the inner and outermost regions of the A ring. An additional internal structure model, in which a particle has a high density core surrounded by a fluffy regolith mantle, shows that the core radius relative to the particle radius is about 0.9 for the middle A ring and is much less for the inner and outer regions of the A ring. This means that the radial variation of the internal density of ring particles exists across the A ring. Some mechanisms may be confining dense particles in the middle A ring against viscous diffusion. Alternatively, the (middle) A ring might have recently formed ($<$ 10$^{8}$ yr) by destruction of an icy satellite, so that dense particles have not yet diffused over the A ring and regolith mantles of particles have not grown thick. Our model results also indicate that the composition of the core is predominantly water ice, not rock. 	
1512.00769v2	http://arxiv.org/pdf/1512.00769v2	2016	Three is much more than two in coarsening dynamics of cyclic   competitions	Namiko Mitarai|Ivar Gunnarson|Buster Niels Pedersen|Christian Anker Rosiek|Kim Sneppen	  The classical game of rock-paper-scissors have inspired experiments and spatial model systems that address robustness of biological diversity. In particular the game nicely illustrates that cyclic interactions allow multiple strategies to coexist for long time intervals. When formulated in terms of a one-dimensional cellular automata, the spatial distribution of strategies exhibits coarsening with algebraically growing domain size over time, while the two-dimensional version allows domains to break and thereby opens for long-time coexistence. We here consider a quasi-one-dimensional implementation of the cyclic competition, and study the long-term dynamics as a function of rare invasions between parallel linear ecosystems. We find that increasing the complexity from two to three parallel subsystems allows a transition from complete coarsening to an active steady state where the domain size stays finite. We further find that this transition happens irrespective of whether the update is done in parallel for all sites simultaneously, or done randomly in sequential order. In both cases the active state is characterized by localized bursts of dislocations, followed by longer periods of coarsening. In the case of the parallel dynamics, we find that there is another phase transition between the active steady state and the coarsening state within the three-line system when the invasion rate between the subsystems is varied. We identify the critical parameter for this transition, and show that the density of active boundaries have critical exponents that are consistent with the directed percolation universality class. On the other hand, numerical simulations with the random sequential dynamics suggest that the system may exhibit an active steady state as long as the invasion rate is finite. 	
1610.10067v5	http://arxiv.org/pdf/1610.10067v5	2017	Vital Signs: Seismology of ocean worlds	Steven D. Vance|Sharon Kedar|Mark P. Panning|Simon C. Staehler|Bruce G. Bills|Ralph D. Lorenz|Hsin-Hua Huang|William T. Pike|Julie C. Castillo|Philippe Lognonne|Victor C. Tsai|Alyssa R. Rhoden	  Ice-covered ocean worlds possess diverse energy sources and associated mechanisms that are capable of driving significant seismic activity, but to date no measurements of their seismic activity have been obtained. Such investigations could probe their transport properties and radial structures, with possibilities for locating and characterizing trapped liquids that may host life and yielding critical constraints on redox fluxes, and thus on habitability. Modeling efforts have examined seismic sources from tectonic fracturing and impacts. Here, we describe other possible seismic sources, their associations with science questions constraining habitability, and the feasibility of implementing such investigations. We argue, by analogy with the Moon, that detectable seismic activity on tidally flexed ocean worlds should occur frequently. Their ices fracture more easily than rocks, and dissipate more tidal energy than the <1 GW of the Moon and Mars. Icy ocean worlds also should create less thermal noise for a due to their greater distance and consequently smaller diurnal temperature variations. They also lack substantial atmospheres (except in the case of Titan) that would create additional noise. Thus, seismic experiments could be less complex and less susceptible to noise than prior or planned planetary seismology investigations of the Moon or Mars. 	
1709.07943v2	http://arxiv.org/pdf/1709.07943v2	2017	Cascaded Region-based Densely Connected Network for Event Detection: A   Seismic Application	Yue Wu|Youzuo Lin|Zheng Zhou|David Chas Bolton|Ji Liu|Paul Johnson	  Automatic event detection from time series signals has wide applications, such as abnormal event detection in video surveillance and event detection in geophysical data. Traditional detection methods detect events primarily by the use of similarity and correlation in data. Those methods can be inefficient and yield low accuracy. In recent years, because of the significantly increased computational power, machine learning techniques have revolutionized many science and engineering domains. In this study, we apply a deep-learning-based method to the detection of events from time series seismic signals. However, a direct adaptation of the similar ideas from 2D object detection to our problem faces two challenges. The first challenge is that the duration of earthquake event varies significantly; The other is that the proposals generated are temporally correlated. To address these challenges, we propose a novel cascaded region-based convolutional neural network to capture earthquake events in different sizes, while incorporating contextual information to enrich features for each individual proposal. To achieve a better generalization performance, we use densely connected blocks as the backbone of our network. Because of the fact that some positive events are not correctly annotated, we further formulate the detection problem as a learning-from-noise problem. To verify the performance of our detection methods, we employ our methods to seismic data generated from a bi-axial "earthquake machine" located at Rock Mechanics Laboratory, and we acquire labels with the help of experts. Through our numerical tests, we show that our novel detection techniques yield high accuracy. Therefore, our novel deep-learning-based detection methods can potentially be powerful tools for locating events from time series data in various applications. 	
1802.01731v1	http://arxiv.org/pdf/1802.01731v1	2018	On-Orbit Performance of the Helioseismic and Magnetic Imager Instrument   onboard the Solar Dynamics Observatory	J. Todd Hoeksema|Charles S. Baldner|Rock I. Bush|Jesper Schou|Philip H. Scherrer	  The Helioseismic and Magnetic Imager (HMI) instrument is a major component of NASA's Solar Dynamics Observatory (SDO) spacecraft. Since beginning normal science operations on 1 May 2010, HMI has operated with remarkable continuity, e.g. during the more than five years of the SDO prime mission that ended 30 September 2015, HMI collected 98.4% of all possible 45-second velocity maps; minimizing gaps in these full-disk Dopplergrams is crucial for helioseismology. HMI velocity, intensity, and magnetic-field measurements are used in numerous investigations, so understanding the quality of the data is important. We describe the calibration measurements used to track HMI performance and detail trends in important instrument parameters during the mission. Regular calibration sequences provide information used to improve and update the HMI data calibration. The set-point temperature of the instrument front window and optical bench is adjusted regularly to maintain instrument focus, and changes in the temperature-control scheme have been made to improve stability in the observable quantities. The exposure time has been changed to compensate for a 15% decrease in instrument throughput. Measurements of the performance of the shutter and tuning mechanisms show that they are aging as expected and continue to perform according to specification. Parameters of the tunable-optical-filter elements are regularly adjusted to account for drifts in the central wavelength. Frequent measurements of changing CCD-camera characteristics, such as gain and flat field, are used to calibrate the observations. Infrequent expected events, such as eclipses, transits, and spacecraft off-points, interrupt regular instrument operations and provide the opportunity to perform additional calibration. Onboard instrument anomalies are rare and seem to occur quite uniformly in time. The instrument continues to perform very well. 	
0502510v1	http://arxiv.org/pdf/cond-mat/0502510v1	2005	Quantum Monte Carlo calculations of the structural properties and the   B1-B2 phase transition of MgO	D. Alfè|M. Alfredsson|J. Brodholt|M. J. Gillan M. D. Towler|R. J. Needs	  We report diffusion Monte Carlo (DMC) calculations on MgO in the rock-salt and CsCl structures. The calculations are based on Hartree-Fock pseudopotentials, with the single-particle orbitals entering the correlated wave function being represented by a systematically convergeable cubic-spline basis. Systematic tests are presented on system-size errors using periodically repeating cells of up to over 600 atoms. The equilibrium lattice parameter of the rock-salt structure obtained within DMC is almost identical to the Hartree-Fock result, which is close to the experimental value. The DMC result for the bulk modulus is also in good agreement with the experimental value. The B1-B2 transition pressure (between the rock-salt and CsCl structures) is predicted to be just below 600 GPa, which is beyond the experimentally accessible range, in accord with other predictions based on Hartree-Fock and density functional theories. 	
0212099v1	http://arxiv.org/pdf/physics/0212099v1	2002	Gamma-ray measurements of naturally occurring radioactive samples from   Cyprus characteristic geological rocks	M. Tzortzis|H. Tsertos|S. Christofides|G. Christodoulides	  Using high-resolution gamma-ray spectroscopy, the terrestrial gamma radiation in all the predominant types of geological rock formations appearing in Cyprus was measured. Soil samples were collected from each rock type, sealed in 1-litre plastic Marinelli beakers, and measured in the laboratory for 24 hours each. From the measured gamma-ray spectra, activity concentrations were determined for Th-232 (range from 1.3 to 52.8 Bq/kg), U-238 (from 0.9 to 90.3 Bq/kg) and K-40 (from 13 to 894 Bq/kg). Elemental concentrations mean values of (2.8 +- 0.7) ppm, (1.3 +- 0.3) ppm and (0.6 +- 0.1) % were extracted, for thorium, uranium and potassium, respectively. Absorbed dose rates in air outdoors were calculated to be in the range of 0.1-50 nGy/h, depending on the geological features, with an overall mean value of (14.7 +- 7.3) nGy/h. The corresponding effective dose rates per person outdoors were estimated to be between 0.1 and 61.4 microSv/y, assuming a 20% occupancy factor. 	
0409079v1	http://arxiv.org/pdf/physics/0409079v1	2004	Natural radioelement concentration in the Troodos Ophiolite Complex of   Cyprus	Michalis Tzortzis|Haralabos Tsertos	  High-resolution gamma-ray spectrometry was exploited to determine naturally occurring thorium (Th), uranium (U) and potassium (K) elemental concentrations in the whole area covered by the Troodos Ophiolite Complex of Cyprus. For that purpose, a total of 59 samples from surface soils and 10 from the main rock formations of the region of interest were analysed. Elemental concentrations were determined for Th (range from 2.5x10^-3 ppm to 2.0 ppm), U (from 8.1x10^-4 ppm to 0.6 ppm), and K (from 1.3x10^-4 % to 1.0 %). The average values (A.M +- S.D.) derived are (0.24 +- 0.34) ppm, (0.10 +- 0.10) ppm and (0.21 +- 0.24) %, for Th, U, and K, respectively, in the soils, and (0.52 +- 0.17) ppm, (0.17 +- 0.11) ppm and (0.49 +- 0.87) % in the rocks. From these values, a radioactivity (radioelement) loss of nearly 50% is estimated in the underlying surface soils due to bleaching and eluviation during weathering of the rocks. The measured Th/U ratio exhibits values between 2 and 4, whereas the K/Th ratio is highly variable ranging between 1.5x10^3 and 3.0x10^4. 	
0510137v1	http://arxiv.org/pdf/physics/0510137v1	2005	Instrumentation For Geological Fieldwork on the Moon	D. L. Talboys|G. W. Fraser|R. M. Ambrosi|N. Nelms|N. P. Bannister|M. R. Sims|D. Pullan|J. Holt	  A human return to the Moon will require that astronauts are well equipped with instrumentation to aid their investigations during geological field work. Two instruments are described in detail. The first is a portable X-ray Spectrometer, which can provide rapid geochemical analyses of rocks and soils, identify lunar resources and aid selection of samples for return to Earth. The second instrument is the Geological and Radiation environment package (GEORAD). This is an instrument package, mounted on a rover, to perform in-situ measurements on the lunar surface. It can be used for bulk geochemical measurements of rocks and soils (particularly identifying KREEP-enriched rocks), prospect for ice in shadowed areas of craters at the poles and characterise the lunar radiation environment. 	
0811.0628v1	http://arxiv.org/pdf/0811.0628v1	2008	Local Structure of Thermoelectric Ca3Co4O9	T. A. Tyson|Z. Chen|Q. Jie|Q. Li|J. J. Tu	  We have combined temperature dependent local structural measurements with first principles density functional calculations to develop a three dimensional local structure model of the misfit system [Ca2CoO3][CoO2]1.61 (referred to as Ca3Co4O9) which has a rock salt structure stacked incommensurately on a hexagonal CoO2 lattice. The local structural measurements reveal a low coordination of Co(2)-O bonds in the rock salt layer with large static structural disorder. The temperature dependence of the Co(1)-Co(1) bond correlations in the CoO2 layer are found to be normal above ~75K and with a very small static disorder component. An anomalous enhancement in the Co(1)-Co(1) correlations occurs at the onset of long-range magnetic order. Density functional computations suggest that the reduction of the coordination of Co(2) is due to the formation of chains of Co(2)Ox in the a-b plane linked to the Ca-O layers by c-axis Co(2)-O bonds. The reduced dimensionality introduced by the chain-like structure in the rock salt layer and high atomic order in the C 	
1005.3439v4	http://arxiv.org/pdf/1005.3439v4	2011	Small World Property of a Rock Joint(Complexity of Frictional   Interfaces: A Complex Network Perspective)	Hamed O. Ghaffari|M. Sharifzadeh|E. Evgin	  The shear strength and stick-slip behavior of a rough rock joint are analyzed using the complex network approach. We develop a network approach on correlation patterns of void spaces of an evolvable rough fracture (crack type II). Correlation among networks properties with the hydro -mechanical attributes (obtained from experimental tests) of fracture before and after slip is the direct result of the revealed non-contacts networks. Joint distribution of locally and globally filtered correlation gives a close relation to the contact zones attachment-detachment sequences through the evolution of shear strength of the rock joint. Especially spread of node's degree rate to spread of clustering coefficient rate yielded possible stick and slip sequences during the displacements. Our method can be developed to investigate the complexity of stick-slip behavior of faults as well as energy /stress localization on crumpled shells/sheets in which ridge networks are controlling the energy distribution. 	
1009.4753v1	http://arxiv.org/pdf/1009.4753v1	2010	Astronomical Symbolism in Australian Aboriginal Rock Art	Ray P. Norris|Duane W. Hamacher	  Traditional Aboriginal Australian cultures include a significant astronomical component, perpetuated through oral tradition and ceremony. This knowledge has practical navigational and calendrical functions, and sometimes extends to a deep understanding of the motion of objects in the sky. Here we explore whether this astronomical tradition is reflected in the rock art of Aboriginal Australians. We find several plausible examples of depictions of astronomical figures and symbols, and also evidence that astronomical observations were used to set out stone arrangements. However, we recognise that the case is not yet strong enough to make an unequivocal statement, and describe our plans for further research. 	
1102.0391v1	http://arxiv.org/pdf/1102.0391v1	2011	Synthesis of rock-salt MeO-ZnO solid solutions (Me = Ni2+, Co2+, Fe2+,   Mn2+) at high pressure and high temperature	A. N. Baranov|P. S. Sokolov|O. O. Kurakevych|V. A. Tafeenko|D. Trots|V. L. Solozhenko	  Series of metastable Me1-xZnxO solid solutions (Me = Ni2+, Co2+, Fe2+, Mn2+) with the rocksalt (rs) crystal structure have been synthesized from the binary oxides by quenching from 7.7 GPa and 1450-1650 K. Phase composition of the samples, as well as structural properties and stoichiometry of synthesized solid solutions have been studied by X-ray powder diffraction, both conventional and with synchrotron radiation. The widest (0.3 \leq x \leq 0.8) composition range of the existence of individual rock-salt solid solution has been established for the NiO-ZnO system. The bulk rs-Co1-xZnxO, rs-Fe1-xZnxO and rs-Mn1-xZnxO solid solutions may be quenched down to ambient conditions only with twice lower ZnO content, i.e. x \leq 0.5, 0.5 and 0.4, respectively; while formation of rock-salt solid solutions in the CdO-ZnO system has not been observed in the whole concentration range. 	
1106.1510v1	http://arxiv.org/pdf/1106.1510v1	2011	Towards OWL-based Knowledge Representation in Petrology	Alex Shkotin|Vladimir Ryakhovsky|Dmitry Kudryavtsev	  This paper presents our work on development of OWL-driven systems for formal representation and reasoning about terminological knowledge and facts in petrology. The long-term aim of our project is to provide solid foundations for a large-scale integration of various kinds of knowledge, including basic terms, rock classification algorithms, findings and reports. We describe three steps we have taken towards that goal here. First, we develop a semi-automated procedure for transforming a database of igneous rock samples to texts in a controlled natural language (CNL), and then a collection of OWL ontologies. Second, we create an OWL ontology of important petrology terms currently described in natural language thesauri. We describe a prototype of a tool for collecting definitions from domain experts. Third, we present an approach to formalization of current industrial standards for classification of rock samples, which requires linear equations in OWL 2. In conclusion, we discuss a range of opportunities arising from the use of semantic technologies in petrology and outline the future work in this area. 	
1201.4036v2	http://arxiv.org/pdf/1201.4036v2	2012	Clonal selection prevents tragedy of the commons when neighbors compete   in a rock-paper-scissors game	Jeppe Juul|Kim Sneppen|Joachim Mathiesen	  The rock-paper-scissors game is a model example of the on-going cyclic turnover typical of many ecosystems, ranging from the terrestrial and aquatic to the microbial. Here we explore the evolution of a rock-paper-scissors system where three species compete for space. The species are allowed to mutate and change the speed by which they invade one another. In the case when all species have similar mutation rates, we observe a perpetual arms race where no single species prevails. When only two species mutate, their aggressions increase indefinitely until the ecosystem collapses and only the non-mutating species survives. Finally we show that when only one species mutates, group selection removes individual predators with the fastest growth rates, causing the growth rate of the species to stabilize. We explain this group selection quantitatively. 	
1211.5029v1	http://arxiv.org/pdf/1211.5029v1	2012	Il Santo Sepolcro, orientamento astronomico della basilica e le omelie   di san Cirillo di Gerusalemme	Costantino Sigismondi	  On the spur of Calvary the Shroud appears. On this rock the Scriptures are fulfilled, and this rock, located exactly to the West of the Temple of Jerusalem, becomes mystically the new orient, place of the Resurrection of the Lord. The alignements between the three holy places for the three monotheistic religions, the Anastasis, the Temple, now Dome of the Rock, and the Chapel of the Ascension with the East-West axis are here measured and commented both from an astronomical and topographic point of view, enlighted by the urbanistic roman concepts of Vitruvius and the catechesis of saint Cyril bishop of Jerusalem 13 years after the inauguration of the Constantinian buildings at the Holy Sepulcre. 	
1301.2365v1	http://arxiv.org/pdf/1301.2365v1	2013	Body-rock or lift-off in flow	Frank T. Smith|Phillip L. Wilson	  Conditions are investigated under which a body lying at rest or rocking on a solid horizontal surface can be removed from the surface by hydrodynamic forces or instead continues rocking. The investigation is motivated by recent observations on Martian dust movement as well as other small- and large-scale applications. The nonlinear theory of fluid-body interaction here has unsteady motion of an inviscid fluid interacting with a moving thin body. Various shapes of body are addressed together with a range of initial conditions. The relevant parameter space is found to be subtle as evolution and shape play substantial roles coupled with scaled mass and gravity effects. Lift-off of the body from the surface generally cannot occur without fluid flow but it can occur either immediately or within a finite time once the fluid flow starts up: parameters for this are found and comparisons are made with Martian observations. 	
1304.7931v1	http://arxiv.org/pdf/1304.7931v1	2013	Pressure-induced Topological Phase Transitions in Rock-salt   Chalcogenides	P. Barone|T. Rauch|D. Di Sante|J. Henk|I. Mertig|S. Picozzi	  By means of a comprehensive theoretical investigation, we show that external pressure can induce topological phase transitions in IV-VI semiconducting chalcogenides with rock-salt structure. These materials satisfy mirror symmetries that are needed to sustain topologically protected surface states, at variance with time-reversal symmetry responsible for gapless edge states in $\mathcal{Z}_{2}$ topological insulators. The band inversions at high-symmetry points in the Brillouin zone that are related by mirror symmetry, are brought about by an "asymmetric" hybridization between cation and anion $sp$ orbitals. By working out the microscopic conditions to be fulfilled in order to maximize this hybridization, we identify materials in the rock-salt chalcogenide class that are prone to undergo a topological phase transition induced by pressure and/or alloying. Our model analysis is fully comfirmed by complementary advanced \textit{first-principles} calculations and \textit{ab initio}-based tight-binding simulations. 	
1305.5088v1	http://arxiv.org/pdf/1305.5088v1	2013	Occurrence Probability of Large Solar Energetic Particle Events:   Assessment from Data on Cosmogenic Radionuclides in Lunar Rocks	Gennady A. Kovaltsov|Ilya G. Usoskin	  We revisited assessments of the occurrence probability distribution of large events in solar energetic particles (SEP), based on measurements of cosmogenic radionuclides in lunar rocks. We present a combined cumulative occurrence probability distribution of SEP events based on three time scales: directly measured SEP fluences for the last 60 years; estimates based on terrestrial cosmogenic radionuclides 10Be and 14C for the multi-millennial (Holocene) time scale; and cosmogenic radionuclides measured in lunar rocks on the time scale of up to 1 Myr. All the three time scales yield a consistent distribution. The data suggest a strong rollover of the occurrence probability so that SEP events with the fluence of protons with energy >30 MeV greater than 10^{11} (protons /cm2/yr) are not expected at the Myr time scale. 	
1405.4482v1	http://arxiv.org/pdf/1405.4482v1	2014	Pre-earthquake Magnetic Pulses	John Scoville|Jorge Heraud|Friedemann Freund	  A semiconductor model of rocks is shown to describe unipolar magnetic pulses, a phenomenon that has been observed prior to earthquakes. These pulses are observable because their extremely long wavelength allows them to pass through the Earth's crust. Interestingly, the source of these pulses may be triangulated to pinpoint locations where stress is building deep within the crust. We couple a semiconductor drift-diffusion model to a magnetic field in order to describe the electromagnetic effects associated with electrical currents flowing within rocks. The resulting system of equations is solved numerically and it is seen that a volume of rock may act as a diode that produces transient currents when it switches bias. These unidirectional currents are expected to produce transient unipolar magnetic pulses similar in form, amplitude, and duration to those observed before earthquakes, and this suggests that the pulses could be the result of geophysical semiconductor processes. 	
1501.05389v1	http://arxiv.org/pdf/1501.05389v1	2015	Grain-scale thermoelastic stresses and spatiotemporal temperature   gradients on airless bodies, implications for rock breakdown	Jamie L. Molaro|Shane Byrne|Steve A. Langer	  Thermomechanical processes such as fatigue and shock have been suggested to cause and contribute to rock breakdown on Earth, and on other planetary bodies, particularly airless bodies in the inner solar system. In this study, we modeled grain-scale stresses induced by diurnal temperature variations on simple microstructures made of pyroxene and plagioclase on various solar system bodies. We found that a heterogeneous microstructure on the Moon experiences peak tensile stresses on the order of 100 MPa. The stresses induced are controlled by the coefficient of thermal expansion and Young's modulus of the mineral constituents, and the average stress within the microstructure is determined by relative volume of each mineral. Amplification of stresses occurs at surface-parallel boundaries between adjacent mineral grains and at the tips of pore spaces. We also found that microscopic spatial and temporal surface temperature gradients do not correlate with high stresses, making them inappropriate proxies for investigating microcrack propagation. Although these results provide very strong evidence for the significance of thermomechanical processes on airless bodies, more work is needed to quantify crack propagation and rock breakdown rates. 	
1505.00559v2	http://arxiv.org/pdf/1505.00559v2	2015	Melt-preferred orientation, anisotropic permeability, and melt-band   formation in a deforming, partially molten aggregate	Jesse Taylor-West|Richard F. Katz	  Shear deformation of partially molten rock in laboratory experiments causes the emergence of melt-enriched sheets (bands in cross-section) that are aligned at about 15-20 degrees to the shear plane. Deformation and deviatoric stress also cause the coherent alignment of pores at the grain scale. This leads to a melt-preferred orientation that may, in turn, give rise to an anisotropic permeability. Here we develop a simple, general model of anisotropic permeability in partially molten rocks. We use linearised analysis and nonlinear numerical solutions to investigate its behaviour under simple-shear deformation. In particular, we consider implications of the model for the emergence and angle of melt-rich bands. Anisotropic permeability affects the angle of bands and, in a certain parameter regime, it can give rise to low angles consistent with experiments. However, the conditions required for this regime have a narrow range and seem unlikely to be entirely met by experiments. Anisotropic permeability may nonetheless affect melt transport and the behaviour of partially molten rocks in Earth's mantle. 	
1510.06604v1	http://arxiv.org/pdf/1510.06604v1	2015	Internal structure of Pluto and Charon with an iron core	A. Aitta	  Pluto has been observed by the New Horizons space probe to have some relatively fresh ice on the old ices covering most of the surface. Pluto was thought to consist of only a rocky core below the ice. Here I show that Pluto can have an iron core, as can also its companion Charon, which has recently been modelled to have one. The presence of an iron core means the giant impact origin calculations should be redone to include iron and thus higher temperatures. An iron core leads to the possibility of a different geology. An originally molten core becomes solid later, with contraction and a release of latent heat. The space vacated allows the upper rock layers to flow downwards at some locations at the surface of the core, and some of the ice above the rock to descend, filling the spaces left by the rock motion downwards. These phenomena can lead to the forces recently deforming the icy surface of Pluto, and in a lesser way, of Charon. 	
1511.04295v1	http://arxiv.org/pdf/1511.04295v1	2015	Changes in porosity, permeability and surface area during rock   dissolution: effects of mineralogical heterogeneity	Ting Min|Yimin Gao|Li Chen|Qinjun Kang|Wen-Quan Tao	  Effects of heterogeneity of mineral distribution and reaction rate on the rock dissolution process are investigated using a pore-scale reactive transport model based on the lattice Boltzmann method. Coupled fluid flow, species transport, chemical reaction and solid structure alternation due to dissolution are simulated. Effects of mineral distributions and chemical heterogeneity on the dissolution behaviors and evolutions of hydrologic properties are studied under different reactive transport conditions. Simulation results show that the coupling between advection, diffusion and reaction as well as the mineralogical heterogeneity leads to complex reactive transport behaviors and complicated temporal evolutions of hydrologic properties including porosity, permeability and reactive surface. Diverse relationships between surface area and volume are predicted, which cannot be described by simple models such as the spherical-grain model. Porosity-permeability relationships also differ under different mineral distributions and reactive transport conditions. Simulation results indicate that it is extremely challenging to propose general relationships for hydrologic properties for dissolution of rocks with mineralogical heterogeneity, due to the complicated interactions between reactive transport and mineralogical heterogeneity. 	
1603.05973v1	http://arxiv.org/pdf/1603.05973v1	2016	Producing Virtually Defect Free Nanoscale Ripples by Ion Bombardment of   Rocked Solid Surfaces	Matt Harrison|R. Mark Bradley	  Bombardment of a solid surface with a broad, obliquely-incident ion beam frequently produces nanoscale surface ripples. The primary obstacle that prevents the adoption of ion bombardment as a nano-fabrication tool is the high density of defects in the patterns that are typically formed. Our simulations indicate that ion bombardment can produce nearly defect free ripples on the surface of an elemental solid if the sample is concurrently and periodically rocked about an axis orthogonal to the surface normal and the incident beam direction. We also investigate the conditions necessary for rocking to produce highly ordered ripples and discuss how the results of our simulations can be reproduced experimentally. 	
1607.02113v2	http://arxiv.org/pdf/1607.02113v2	2016	Pattern formation in a complex Swift-Hohenberg equation with phase   bistability	Manuel Martínez-Quesada|Germán J. de Valcárcel	  We study pattern formation in a complex Swift Hohenberg equation with phase-sensitive (parametric) gain. Such an equation serves as a universal order parameter equation describing the onset of spontaneous oscillations in extended systems submitted to a kind of forcing dubbed rocking when the instability is towards long wavelengths. Applications include two-level lasers and photorefractive oscillators. Under rocking, the original continuous phase symmetry of the system is replaced by a discrete one, so that phase bistability emerges. This leads to the spontaneous formation of phase-locked spatial structures like phase domains and dark-ring (phase-) cavity solitons. Stability of the homogeneous solutions is studied and numerical simulations are made covering all the dynamical regimes of the model, which turn out to be very rich. Formal derivations of the rocked complex Swift-Hohenberg equation, using multiple scale techniques, are given for the two-level laser and the photorefractive oscillator. 	
1608.08001v1	http://arxiv.org/pdf/1608.08001v1	2016	Influence of pore pressure to the development of a hydraulic fracture in   poroelastic medium	Sergey V. Golovin|Alexey N. Baykin	  In this paper we demonstrate the influence of the pore pressure to the development of a hydraulically-driven fracture in a poroelastic medium. We present a novel numerical model for propagation of a planar hydraulic fracture and prove its correctness by demonstration of the numerical convergence and by comparison with known solutions. The advantage of the algorithm is that it does not require the distinguishing of the fracture's tips and reconstruction of the numerical mesh according to the fracture propagation. Next, we perform a thorough analysis of the interplay of fluid filtration and redistribution of stresses near the fracture. We demonstrate that the fracture length decreases with the increase of the Biot's number (the parameter that determines the contribution of the pore pressure to the stress) and explain this effect by analysing the near-fracture pore pressure, rock deformation and stresses. We conclude, that the correct account for the fluid exchange between the fracture and the rock should be based not only on physical parameters of the rock and fluid, but also on the analysis of stresses near the fracture. 	
1611.00596v1	http://arxiv.org/pdf/1611.00596v1	2016	An Insight in Explaining the Stress Distribution in and around EGS	Mahmood Arshad|Masami Nakagawa|Kamran Jahanbakhsh|Lucila Dunnington	  Developing an Enhanced Geothermal System, otherwise known as EGS, is a complex process and is dependent on range of geological and operating variables. Stresses in and around EGS are believed to be either sound and non-harming or violent and catastrophic among different groups involved, directly or indirectly, in EGS. Pros and cons of EGS have been under discussion and research for a while now. This paper addresses issues associated with stress redistribution during and after working cycle of EGS and gives a new insight in understanding the behavior of stresses redistribution in and around basement rock. As the basement rock is thermo-elastically connected to the country rock, newly generated stresses interact with the existing in-situ stresses under prevailing conditions of geological, design and operating variables. Variables dictating the continuous, safe and efficient working of EGS are also outlined in detailed sections. Guidelines for future of the research related to stress redistribution in EGS are also part of this paper. 	
1803.00758v1	http://arxiv.org/pdf/1803.00758v1	2018	Driving Digital Rock towards Machine Learning: predicting permeability   with Gradient Boosting and Deep Neural Networks	Oleg Sudakov|Evgeny Burnaev|Dmitry Koroteev	  We present a research study aimed at testing of applicability of machine learning techniques for prediction of permeability of digitized rock samples. We prepare a training set containing 3D images of sandstone samples imaged with X-ray microtomography and corresponding permeability values simulated with Pore Network approach. We also use Minkowski functionals and Deep Learning-based descriptors of 3D images and 2D slices as input features for predictive model training and prediction. We compare predictive power of various feature sets and methods. The later include Gradient Boosting and various architectures of Deep Neural Networks (DNN). The results demonstrate applicability of machine learning for image-based permeability prediction and open a new area of Digital Rock research. 	
1409.0540v2	http://arxiv.org/pdf/1409.0540v2	2015	The Landscape of the Neutrino Mechanism of Core-Collapse Supernovae:   Neutron Star and Black Hole Mass Functions, Explosion Energies and Nickel   Yields	Ondrej Pejcha|Todd A. Thompson	  If the neutrino luminosity from the proto-neutron star formed during a massive star core collapse exceeds a critical threshold, a supernova (SN) results. Using spherical quasi-static evolutionary sequences for hundreds of progenitors over a range of metallicities, we study how the explosion threshold maps onto observables, including the fraction of successful explosions, the neutron star (NS) and black hole (BH) mass functions, the explosion energies (E_SN) and nickel yields (M_Ni), and their mutual correlations. Successful explosions are intertwined with failures in a complex pattern that is not simply related to initial progenitor mass or compactness. We predict that progenitors with initial masses of 15 +/- 1, 19 +/- 1, and 21-26 M_Sun are most likely to form BHs, that the BH formation probability is non-zero at solar-metallicity and increases significantly at low metallicity, and that low luminosity, low Ni-yield SNe come from progenitors close to success/failure interfaces. We qualitatively reproduce the observed E_SN-M_Ni correlation, we predict a correlation between the mean and width of the NS mass and E_SN distributions, and that the means of the NS and BH mass distributions are correlated. We show that the observed mean NS mass of ~1.33 M_Sun implies that the successful explosion fraction is higher than 0.35. Overall, we show that the neutrino mechanism can in principle explain the observed properties of SNe and their compact objects. We argue that the rugged landscape of progenitors and outcomes mandates that SN theory should focus on reproducing the wide ranging distributions of observed SN properties. 	
0306217v4	http://arxiv.org/pdf/cond-mat/0306217v4	2004	Competing Styles of Statistical Mechanics: I. Systematization and   Clarification in a General Theory	Roberto Luzzi|Áurea R. Vasconcellos|J. Galvão Ramos	  Competing styles of Statistical Mechanics have been introduced as practical succedaneous to the conventional well established Boltzmann-Gibbs statistical mechanics, when in the use of the latter the researcher is impaired in his/her capacity for satisfying the Criteria of Efficiency and/or Sufficiency in statistics [Fisher, 1922], that is, a failure in the characterization (presence of fractality, scaling, etc.) of the system related to some aspect relevant to the given physical situation. To patch this limitation on the part of the observer, in order to make predictions on the values of observables and response functions, are introduced unconventional approaches. We present a detailed description of their construction and a clarification of its scope and interpretation. Also, resorting to the use of the particular case of Renyi's unconventional statistics is built a nonequilibrium ensemble formalism. The unconventional distribution functions of fermions and bosons are obtained, and in a follow-up article [cond-mat/0306247] we describe applications to the study of experimental results in semiconductor physics and in electro-chemistry involving nanometric scales and fractal-like structures, and some additional theoretical analysis is added. PACS: 05.70.Ln, 82.20.Mj, 82.20.Db Keywords: Nonequilibrium Ensemble Formalism; Generalized Informational Entropies; Generalized Statistics; Nonextensive Statistics; Renyi Statistics; Escort Probability. 	
nonequilibrium ensemble formalism, generalized informational entropies,
generalized statistics, nonextensive statistics, renyi statistics, escort
probability 

0607103v1	http://arxiv.org/pdf/cs/0607103v1	2006	Ideas by Statistical Mechanics (ISM)	Lester Ingber	  Ideas by Statistical Mechanics (ISM) is a generic program to model evolution and propagation of ideas/patterns throughout populations subjected to endogenous and exogenous interactions. The program is based on the author's work in Statistical Mechanics of Neocortical Interactions (SMNI), and uses the author's Adaptive Simulated Annealing (ASA) code for optimizations of training sets, as well as for importance-sampling to apply the author's copula financial risk-management codes, Trading in Risk Dimensions (TRD), for assessments of risk and uncertainty. This product can be used for decision support for projects ranging from diplomatic, information, military, and economic (DIME) factors of propagation/evolution of ideas, to commercial sales, trading indicators across sectors of financial markets, advertising and political campaigns, etc. A statistical mechanical model of neocortical interactions, developed by the author and tested successfully in describing short-term memory and EEG indicators, is the proposed model. Parameters with a given subset of macrocolumns will be fit using ASA to patterns representing ideas. Parameters of external and inter-regional interactions will be determined that promote or inhibit the spread of these ideas. Tools of financial risk management, developed by the author to process correlated multivariate systems with differing non-Gaussian distributions using modern copula analysis, importance-sampled using ASA, will enable bona fide correlations and uncertainties of success and failure to be calculated. Marginal distributions will be evolved to determine their expected duration and stability using algorithms developed by the author, i.e., PATHTREE and PATHINT codes. 	
0501011v2	http://arxiv.org/pdf/quant-ph/0501011v2	2005	Contribution from stochastic electrodynamics to the understanding of   quantum mechanics	L. de la Pena|A. M. Cetto	  During the last decades there has been a relatively extensive attempt to develop the theory of stochastic electrodynamics (SED) with a view to establishing it as the foundation for quantum mechanics. The theory had several important successes, but failed when applied to the study of particles subject to nonlinear forces. An analysis of the failure showed that its reasons are not to be ascribed to the principles of SED, but to the methods used to construct the theory, particularly the use of a Fokker-Planck approximation and perturbation theory. A new, non perturbative approach has been developed, called linear stochastic electrodynamics (LSED), of which a clean form is presented here. After introducing the fundamentals of SED, we discuss in detail the principles on which LSED is constructed. We pay attention to the fundamental issue of the mechanism that leads to the quantum behaviour of field and matter, and demonstrate that indeed LSED is a natural way to the quantum formalism by demanding its solutions to comply with a limited number of principles, each one with a clear physical meaning. As a further application of the principles of LSED we derive also the Planck distribution. In a final section we revisit some of the most tantalizing quandaries of quantum mechanics from the point of view offered by the present theory, and show that it offers a clear physical answer to them. 	
1302.3022v1	http://arxiv.org/pdf/1302.3022v1	2013	Characterising the Anisotropic Mechanical Properties of Excised Human   Skin	Aisling Ni Annaidh|Karine Bruyere|Michel Destrade|Michael D. Gilchrist|Melanie Ottenio	  The mechanical properties of skin are important for a number of applications including surgery, dermatology, impact biomechanics and forensic science. In this study we have investigated the influence of location and orientation on the deformation characteristics of 56 samples of excised human skin. Uniaxial tensile tests were carried out at a strain rate of 0.012s$^{-1}$ on skin from the back. Digital Image Correlation was used for 2D strain measurement and a histological examination of the dermis was also performed. The mean ultimate tensile strength (UTS) was 21.6$\pm$8.4MPa, the mean failure strain 54$\pm$17%, the mean initial slope 1.18$\pm$0.88MPa, the mean elastic modulus 83.3$\pm$34.9MPa and the mean strain energy was 3.6$\pm$1.6MJ/m$^3$. A multivariate analysis of variance has shown that these mechanical properties of skin are dependent upon the orientation of Langer lines (P$<$0.0001-P=0.046). The location of specimens on the back was also found to have a significant effect on the UTS (P =0.0002), the elastic modulus (P=0.001) and the strain energy (P=0.0052). The histological investigation concluded that there is a definite correlation between the orientation of Langer Lines and the preferred orientation of collagen fibres in the dermis (P$<$0.001). The data obtained in this study will provide essential information for those wishing to model the skin using a structural constitutive model. 	
1303.6520v1	http://arxiv.org/pdf/1303.6520v1	2013	Activation and radiation damage in the environment of hadron   accelerators	Daniela Kiselev	  A component which suffers radiation damage usually also becomes radioactive, since the source of activation and radiation damage is the interaction of the material with particles from an accelerator or with reaction products. However, the underlying mechanisms of the two phenomena are different. These mechanisms are described here. Activation and radiation damage can have far-reaching consequences. Components such as targets, collimators, and beam dumps are the first candidates for failure as a result of radiation damage. This means that they have to be replaced or repaired. This takes time, during which personnel accumulate dose. If the dose to personnel at work would exceed permitted limits, remote handling becomes necessary. The remaining material has to be disposed of as radioactive waste, for which an elaborate procedure acceptable to the authorities is required. One of the requirements of the authorities is a complete nuclide inventory. The methods used for calculation of such inventories are presented, and the results are compared with measured data. In the second part of the paper, the effect of radiation damage on material properties is described. The mechanism of damage to a material due to irradiation is described. The amount of radiation damage is quantified in terms of displacements per atom. Its calculation and deficiencies in explaining and predicting the changes in mechanical and thermal material properties are discussed, and examples are given. 	
1408.2093v2	http://arxiv.org/pdf/1408.2093v2	2014	Why Current Interpretations of Quantum Mechanics are Deficient	Elliott Tammaro	  Quantum mechanics under the Copenhagen interpretation is one of the most experimentally well verified formalisms. However, it is known that the interpretation makes explicit reference to external observation or "measurement." One says that the Copenhagen interpretation suffers from the measurement problem. This deficiency of the interpretation excludes it as a viable fundamental formalism and prevents the use of standard quantum mechanics in discussions of quantum cosmology. Numerous alternative interpretations have been developed with the goals of reproducing its predictive success while obviating the measurement problem. While several interpretations make distinct, falsifiable, predictions, many claim to precisely reproduce the results of standard quantum mechanics. The sheer number of interpretations raises several issues. If the experimental predictions are identical, how are they to be assessed? On what grounds can an interpretation be said to trump another? Without recourse to experimental findings, one may continue to assess an interpretation on its logical structure, self-consistency, and simplicity (number and plausibility of its assumptions). We argue, and where possible, demonstrate, that all common interpretations have unresolved deficiencies. Among these deficiencies are failures to resolve the measurement problem, fine-tuning problems, logical/mathematical inconsistencies, disagreement with experiment, and others. Shortcomings as severe as these call into question the viability of any of the common interpretations. When appropriate, we indicate where future work may resolve some of these issues. 	
1508.02137v1	http://arxiv.org/pdf/1508.02137v1	2015	Stochastic modeling and survival analysis of marginally trapped neutrons   for a magnetic trapping neutron lifetime experiment	K. J. Coakley|M. S. Dewey|M. G. Huber|P. R. Huffman|C. R. Huffer|D. E. Marley|H. P. Mumm|C. M. O'Shaughnessy|K. W. Schelhammer|A. K. Thompson|A. T. Yue	  In a variety of neutron lifetime experiments, in addition to $\beta-$decay, neutrons can be lost by other mechanisms including wall losses. Failure to account for these other loss mechanisms produces systematic measurement error and associated systematic uncertainties in neutron lifetime measurements. In this work, we develop a physical model for neutron wall losses and construct a competing risks survival analysis model to account for losses due to the joint effect of $\beta-$decay losses, wall losses of marginally trapped neutrons, and an additional absorption mechanism. We determine the survival probability function associated with the wall loss mechanism by a Monte Carlo method. Based on a fit of the competing risks model to a subset of the NIST experimental data, we determine the mean lifetime of trapped neutrons to be approximately 700 s -- considerably less than the current best estimate of (880.1 $\pm$ 1.1) s promulgated by the Particle Data Group [1]. Currently, experimental studies are underway to determine if this discrepancy can be explained by neutron capture by ${}^3$He impurities in the trapping volume. Analysis of the full NIST data will be presented in a later publication. 	
1604.06485v1	http://arxiv.org/pdf/1604.06485v1	2016	A review of the effects of chemical and phase segregation on the   mechanical behaviour of multi-phase steels	Bernard Ennis	  In the drive towards higher strength alloys, a diverse range of alloying elements is employed to enhance their strength and ductility. Limited solid solubility of these elements in steel leads to segregation during casting which affects the entire down-stream processing and eventually the mechanical properties of the finished product. Although it is thought that the presence of continuous bands lead to premature failure, it has not been possible to verify this link. This poses as increasingly greater risk for higher alloyed, higher strength steels which are prone to centre-line segregation: it is thus vital to be able to predict the mechanical behaviour of multi-phase (MP) steels under loading.   This review covers the microstructure and properties of galvanised advanced high strength steels with particular emphasis to their use in automotive applications. In order to understand the origins of banding, the origins of segregation of alloying elements during casting and partitioning in the solid state will be discussed along with the effects on the mechanical behaviour and damage evolution under (tensile) loading. Attention will also be paid to the application of microstructural models in tailoring the production process to enable suppression of the effects of segregation upon banding. Finally, the theory and application of the experimental techniques used in this work to elucidate the structure and properties will be examined. 	
1702.02145v1	http://arxiv.org/pdf/1702.02145v1	2017	Structure--property relationships of cell clusters in biotissues: 2D   analysis	Xiaohua Zhou|Erhu Zhang|Minggang Xia|Jianlin Liu|Shengli Zhang	  To insight the relationships between the self-organizing structures of cells, such as the cell clusters, and the properties of biotissues is helpful in revealing the function and designing biomaterial. Traditional random foam model neglects several important details of the frameworks of cell clusters, in this study we use a more complete model, cell adhesion model, to investigate the mechanical and morphological properties of the two-dimensional (2D) dry foams composed by cells. Supposing these structures are formed due to adhesion between cells, the equilibrium formations result from the minimum of the free energy. The equilibrium shape equations for high symmetrical structures without the volume constraint are derived, and the analytical results of the corresponding mechanical parameters, such as the Young's modulus, bulk modulus and failure strength, are obtained. Numerical simulation method is applied to study the complex shapes with the volume constraint and several stable multicellular structures are obtained. Symmetry-breaking due to the volume change is founded and typical periodic shapes and the corresponding phase transformations are explored. Our study provides a potential method to connect the microstructure with the macro-mechanical parameters of biotissues. The results also are helpful to understand the physical mechanism of how the structures of biotissues are formed. 	
1712.02074v1	http://arxiv.org/pdf/1712.02074v1	2017	Tensile rupture of medial arterial tissue studied by X-ray   micro-tomography on stained samples	Clémentine Helfenstein-Didier|Damien Taïnoff|Julien Viville|Jérôme Adrien|Éric Maire|Pierre Badel	  Detailed characterization of damage and rupture mechanics of arteries is one the current challenges in vascular biomechanics, which requires developing suitable experimental approaches. This paper introduces an approach using in situ tensile tests in an X-ray micro-tomography setup to observe mechanisms of damage initiation and progression in medial layers of porcine aortic samples. The technique requires the use of sodium polytungstate as a contrast agent, of which the conditions for use are detailed in this paper. Immersion of the samples during 24 hours in a 15 g.L-1 concentrated solution provided the best compromise for viewing musculo-elastic units in this tissue. The process of damage initiation, delamination and rupture of medial tissue under tensile loading was observed and can be described as an elementary process repeating several times until complete failure. This elementary process initiates with a sudden mode I fracture of a group of musculo-elastic units, followed by an elastic recoil of these units, causing mode II separation of these, hence a delamination plane. The presented experimental approach constitutes a basis for observation of other constituents, or for investigations on other tissues and damage mechanisms. 	
1801.04269v1	http://arxiv.org/pdf/1801.04269v1	2018	Mechanical Properties of Pentagraphene-based Nanotubes: A Molecular   Dynamics Study	Jose M. de Sousa|Acrisio L. Aguiar|Eduardo C. Girão|Alexandre F. Fonseca|Antonio G. Sousa Filho|Douglas S. Galvao	  The study of the mechanical properties of nanostructured systems has gained importance in theoretical and experimental research in recent years. Carbon nanotubes (CNTs) are one of the strongest nanomaterials found in nature, with Young's Modulus (YM) in the order 1.25 TPa. One interesting question is about the possibility of generating new nanostructures with 1D symmetry and with similar and/or superior CNT properties. In this work, we present a study on the dynamical, structural, mechanical properties, fracture patterns and YM values for one class of these structures, the so-called pentagraphene nanotubes (PGNTs). These tubes are formed rolling up pentagraphene membranes (which are quasi-bidimensional structures formed by densely compacted pentagons of carbon atoms in sp3 and sp2 hybridized states) in the same form that CNTs are formed from rolling up graphene membranes. We carried out fully atomistic molecular dynamics simulations using the ReaxFF force field. We have considered zigzag-like and armchair-like PGNTs of different diameters. Our results show that PGNTs present YM ~ 800 GPa with distinct elastic behavior in relation to CNTs, mainly associated with mechanical failure, chirality dependent fracture patterns and extensive structural reconstructions. 	
0901.4591v1	http://arxiv.org/pdf/0901.4591v1	2009	Network Coding-Based Protection Strategy Against Node Failures	Salah A. Aly|Ahmed E. Kamal	  The enormous increase in the usage of communication networks has made protection against node and link failures essential in the deployment of reliable networks. To prevent loss of data due to node failures, a network protection strategy is proposed that aims to withstand such failures. Particularly, a protection strategy against any single node failure is designed for a given network with a set of $n$ disjoint paths between senders and receivers. Network coding and reduced capacity are deployed in this strategy without adding extra working paths to the readily available connection paths. This strategy is based on protection against node failures as protection against multiple link failures. In addition, the encoding and decoding operational aspects of the premeditated protection strategy are demonstrated. 	
0907.1182v1	http://arxiv.org/pdf/0907.1182v1	2009	Catastrophic cascade of failures in interdependent networks	Sergey V. Buldyrev|Roni Parshani|Gerald Paul|H. Eugene Stanley|Shlomo Havlin	  Many systems, ranging from engineering to medical to societal, can only be properly characterized by multiple interdependent networks whose normal functioning depends on one another. Failure of a fraction of nodes in one network may lead to a failure in another network. This in turn may cause further malfunction of additional nodes in the first network and so on. Such a cascade of failures, triggered by a failure of a small faction of nodes in only one network, may lead to the complete fragmentation of all networks. We introduce a model and an analytical framework for studying interdependent networks. We obtain interesting and surprising results that should significantly effect the design of robust real-world networks. For two interdependent Erdos-Renyi (ER) networks, we find that the critical average degree below which both networks collapse is <k_c>=2.445, compared to <k_c>=1 for a single ER network. Furthermore, while for a single network a broader degree distribution of the network nodes results in higher robustness to random failure, for interdependent networks, the broader the distribution is, the more vulnerable the networks become to random failure. 	
1003.0951v2	http://arxiv.org/pdf/1003.0951v2	2013	LogMaster: Mining Event Correlations in Logs of Large scale Cluster   Systems	Rui Ren|Xiaoyu Fu|Jianfeng Zhan|Wei Zhou	  This paper presents a methodology and a system, named LogMaster, for mining correlations of events that have multiple attributions, i.e., node ID, application ID, event type, and event severity, in logs of large-scale cluster systems. Different from traditional transactional data, e.g., supermarket purchases, system logs have their unique characteristic, and hence we propose several innovative approaches to mine their correlations. We present a simple metrics to measure correlations of events that may happen interleavedly. On the basis of the measurement of correlations, we propose two approaches to mine event correlations; meanwhile, we propose an innovative abstraction: event correlation graphs (ECGs) to represent event correlations, and present an ECGs based algorithm for predicting events. For two system logs of a production Hadoop-based cloud computing system at Research Institution of China Mobile and a production HPC cluster system at Los Alamos National Lab (LANL), we evaluate our approaches in three scenarios: (a) predicting all events on the basis of both failure and non-failure events; (b) predicting only failure events on the basis of both failure and non-failure events; (c) predicting failure events after removing non-failure events. 	
1003.3880v2	http://arxiv.org/pdf/1003.3880v2	2010	Lessons from the Failure and Subsequent Success of a Complex Healthcare   Sector IT Project	David Greenwood|Ali Khajeh-Hosseini|Ian Sommerville	  This paper argues that IT failures diagnosed as errors at the technical or project management level are often mistakenly pointing to symptoms of failure rather than a project's underlying socio-complexity (complexity resulting from the interactions of people and groups) which is usually the actual source of failure. We propose a novel method, Stakeholder Impact Analysis, that can be used to identify risks associated with socio-complexity as it is grounded in insights from the social sciences, psychology and management science. This paper demonstrates the effectiveness of Stakeholder Impact Analysis by using the 1992 London Ambulance Service Computer Aided Dispatch project as a case study, and shows that had our method been used to identify the risks and had they been mitigated, it would have reduced the risk of project failure. This paper's original contribution comprises expanding upon existing accounts of failure by examining failures at a level of granularity not seen elsewhere that enables the underlying socio-complexity sources of risk to be identified. 	
1011.4535v2	http://arxiv.org/pdf/1011.4535v2	2010	Cascading Link Failure in the Power Grid: A Percolation-Based Analysis	Hongda Xiao|Edmund Yeh	  Large-scale power blackouts caused by cascading failure are inflicting enormous socioeconomic costs. We study the problem of cascading link failures in power networks modelled by random geometric graphs from a percolation-based viewpoint. To reflect the fact that links fail according to the amount of power flow going through them, we introduce a model where links fail according to a probability which depends on the number of neighboring links. We devise a mapping which maps links in a random geometric graph to nodes in a corresponding dual covering graph. This mapping enables us to obtain the first-known analytical conditions on the existence and non-existence of a large component of operational links after degree-dependent link failures. Next, we present a simple but descriptive model for cascading link failure, and use the degree-dependent link failure results to obtain the first-known analytical conditions on the existence and non-existence of cascading link failures. 	
1105.5903v3	http://arxiv.org/pdf/1105.5903v3	2011	Probabilistic Analysis of the Network Reliability Problem on a Random   Graph Ensemble	Akiyuki Yano|Tadashi Wadayama	  In the field of computer science, the network reliability problem for evaluating the network failure probability has been extensively investigated. For a given undirected graph $G$, the network failure probability is the probability that edge failures (i.e., edge erasures) make $G$ unconnected. Edge failures are assumed to occur independently with the same probability. The main contributions of the present paper are the upper and lower bounds on the expected network failure probability. We herein assume a simple random graph ensemble that is closely related to the Erd\H{o}s-R\'{e}nyi random graph ensemble. These upper and lower bounds exhibit the typical behavior of the network failure probability. The proof is based on the fact that the cut-set space of $G$ is a linear space over $\Bbb F_2$ spanned by the incident matrix of $G$. The present study shows a close relationship between the ensemble analysis of the network failure probability and the ensemble analysis of the error detection probability of LDGM codes with column weight 2. 	
1106.1652v1	http://arxiv.org/pdf/1106.1652v1	2011	Distributed Storage Codes through Hadamard Designs	Dimitris S. Papailiopoulos|Alexandros G. Dimakis	  In distributed storage systems that employ erasure coding, the issue of minimizing the total {\it repair bandwidth} required to exactly regenerate a storage node after a failure arises. This repair bandwidth depends on the structure of the storage code and the repair strategies used to restore the lost data. Minimizing it requires that undesired data during a repair align in the smallest possible spaces, using the concept of interference alignment (IA). Here, a points-on-a-lattice representation of the symbol extension IA of Cadambe {\it et al.} provides cues to perfect IA instances which we combine with fundamental properties of Hadamard matrices to construct a new storage code with favorable repair properties. Specifically, we build an explicit $(k+2,k)$ storage code over $\mathbb{GF}(3)$, whose single systematic node failures can be repaired with bandwidth that matches exactly the theoretical minimum. Moreover, the repair of single parity node failures generates at most the same repair bandwidth as any systematic node failure. Our code can tolerate any single node failure and any pair of failures that involves at most one systematic failure. 	
1409.0624v1	http://arxiv.org/pdf/1409.0624v1	2014	A random shock model with mixed effect, including competing soft and   sudden failures, and dependence	Sophie Mercier|H. H. Pham	  A system is considered, which is subject to external and possibly fatal shocks, with dependence between the fatality of a shock and the system age. Apart from these shocks, the system suffers from competing soft and sudden failures, where soft failures refer to the reaching of a given thresh-old for the degradation level, and sudden failures to accidental failures, characterized by a failure rate. A non-fatal shock increases both degradation level and failure rate of a random amount, with possible dependence between the two increments. The system reliability is calculated by four different methods. Conditions under which the system lifetime is New Better than Used are proposed. The in uence of various parameters of the shocks environment on the system lifetime is studied. 	
1701.00898v2	http://arxiv.org/pdf/1701.00898v2	2017	Double Link Failure Protection using a Single P-cycle	Pallavi Athe|Yatindra Nath Singh	  In this letter, we investigate survivability in optical networks for protection from two simultaneous link failures. Failure probability of two links with overlapping protection can be high if these links are geographically close. In a network with deterministic single link protection, simultaneous failure of two links may lead to partial or full loss of traffic on the failed links. Two link failure protection will make the network more resilient by protecting double failures having overlapping protection. A method for achieving double fault tolerance is double cycle method (DB); it uses two pre-configured cycles (p-cycles) to protect a link. Single p-cycle (SG) method, which uses one p-cycle to protect a link from two simultaneous link failure is introduced in this letter. Integer linear programs (ILP) are formulated for the SG method as well as DB method. It has been observed that the SG method provides a solution to bigger networks with lesser computational resources as compared to the DB method. 	
1702.00298v2	http://arxiv.org/pdf/1702.00298v2	2018	Cascading Failures in Interdependent Systems: Impact of Degree   Variability and Dependence	Richard J. La	  We study cascading failures in a system comprising interdependent networks/systems, in which nodes rely on other nodes both in the same system and in other systems to perform their function. The (inter-)dependence among nodes is modeled using a dependence graph, where the degree vector of a node determines the number of other nodes it can potentially cause to fail in each system through aforementioned dependency. In particular, we examine the impact of the variability and dependence properties of node degrees on the probability of cascading failures. We show that larger variability in node degrees hampers widespread failures in the system, starting with random failures. Similarly, positive correlations in node degrees make it harder to set off an epidemic of failures, thereby rendering the system more robust against random failures. 	
1704.06917v1	http://arxiv.org/pdf/1704.06917v1	2017	Identify Critical Branches with Cascading Failure Chain Statistics and   Hypertext-Induced Topic Search Algorithm	Chao Luo|Jun Yang	  An effective way to suppress the cascading failure risk is the branch capacity upgrade, whose optimal decision making, however, may incur high computational burden. A practical way is to find out some critical branches as the candidates in advance. This paper proposes a simulation data oriented approach to identify the critical branches with higher importance in cascading failure propagation. First, a concept of cascading failure chain (CFC) is introduced and numerous samples of CFC are generated with an AC power flow based cascading failure simulator. Then, a directed weighted graph is constructed, whose edges denotes the severities of branch interactions. Third, the weighted hypertext-induced topic search (HITS) algorithm is used to rate and rank this graph's vertices,through which the critical branches can be identified accordingly. Validations on IEEE 118bus and RTS96 systems show that the proposed approach can identify critical branches whose capacity upgrades suppress cascading failure risk more greatly. Moreover, it is also shown that structural importance of a branch does not agree with its importance in cascading failure, which indicates the effectiveness of the proposed approach compared with structure vulnerabilities based identifying methods. 	
1706.10127v1	http://arxiv.org/pdf/1706.10127v1	2017	Studying Cascading Overload Failures under High Penetration of Wind   Generation	Mir Hadi Athari|Zhifang Wang	  While power systems are reliable infrastructures, their complex interconnectivities allow for propagation of disturbances through cascading failures which causes blackouts. Meanwhile the ever increasing penetration level of renewable generation into power grids introduces a massive amount of uncertainty to the grid that might have a severe impact on grid vulnerability to overload cascading failures. There are numerous studies in the literature that focus on modeling cascading failures with different approaches. However, there is a need for studies that simulate cascading failure considering the uncertainty coming from high penetration of renewable generation. In this study, the impacts of wind generation in terms of its penetration and uncertainty levels on grid vulnerability to cascading overload failures are studied. The simulation results on IEEE 300 bus system show that uncertainty coming from wind energy have severe impact on grid vulnerability to cascading overload failures. Results also suggest that higher penetration levels of wind energy if not managed appropriately will add to this severity due to injection of higher uncertainties into the grid. 	
1101.3859v1	http://arxiv.org/pdf/1101.3859v1	2011	OSPF Weight Setting Optimization for Single Link Failures	Mohammed H. Sqalli|Sadiq M. Sait|Syed Asadullah	  In operational networks, nodes are connected via multiple links for load sharing and redundancy. This is done to make sure that a failure of a link does not disconnect or isolate some parts of the network. However, link failures have an effect on routing, as the routers find alternate paths for the traffic originally flowing through the link which has failed. This effect is severe in case of failure of a critical link in the network, such as backbone links or the links carrying higher traffic loads. When routing is done using the Open Shortest Path First (OSPF) routing protocol, the original weight selection for the normal state topology may not be as efficient for the failure state. In this paper, we investigate the single link failure issue with an objective to find a weight setting which results in efficient routing in normal and failure states. We engineer Tabu Search Iterative heuristic using two different implementation strategies to solve the OSPF weight setting problem for link failure scenarios. We evaluate these heuristics and show through experimental results that both heuristics efficiently handle weight setting for the failure state. A comparison of both strategies is also presented. 	
1207.0485v1	http://arxiv.org/pdf/1207.0485v1	2012	Spatial heterogeneity promotes coexistence of rock-paper-scissor   metacommunities	Sebastian J. Schreiber|Timothy P. Killingback	  The rock-paper-scissor game -- which is characterized by three strategies R,P,S, satisfying the non-transitive relations S excludes P, P excludes R, and R excludes S -- serves as a simple prototype for studying more complex non-transitive systems. For well-mixed systems where interactions result in fitness reductions of the losers exceeding fitness gains of the winners, classical theory predicts that two strategies go extinct. The effects of spatial heterogeneity and dispersal rates on this outcome are analyzed using a general framework for evolutionary games in patchy landscapes. The analysis reveals that coexistence is determined by the rates at which dominant strategies invade a landscape occupied by the subordinate strategy (e.g. rock invades a landscape occupied by scissors) and the rates at which subordinate strategies get excluded in a landscape occupied by the dominant strategy (e.g. scissor gets excluded in a landscape occupied by rock). These invasion and exclusion rates correspond to eigenvalues of the linearized dynamics near single strategy equilibria. Coexistence occurs when the product of the invasion rates exceeds the product of the exclusion rates. Provided there is sufficient spatial variation in payoffs, the analysis identifies a critical dispersal rate $d^*$ required for regional persistence. For dispersal rates below $d^*$, the product of the invasion rates exceed the product of the exclusion rates and the rock-paper-scissor metacommunities persist regionally despite being extinction prone locally. For dispersal rates above $d^*$, the product of the exclusion rates exceed the product of the invasion rates and the strategies are extinction prone. These results highlight the delicate interplay between spatial heterogeneity and dispersal in mediating long-term outcomes for evolutionary games. 	
1411.6585v1	http://arxiv.org/pdf/1411.6585v1	2014	Characterizing the nonlinear interaction of S- and P-waves in a rock   sample	Thomas Gallot|Alison Malcolm|Thomas L. Szabo|Stephen Brown|Daniel Burns|Michael Fehler	  The nonlinear elastic response of rocks is known to be caused by the rocks' microstructure, particularly cracks and fluids. This paper presents a method for characterizing the nonlinearity of rocks in a laboratory scale experiment with a unique configuration. This configuration has been designed to open up the possibility the nonlinear characterization of rocks as an imaging tool in a field scenario. The nonlinear interaction of two traveling waves: a low-amplitude 500 kHz P-wave probe and a high-amplitude 50 kHz S-wave pump has been studied on a room-dry 15 x 15x 3 cm slab of Berea sandstone. Changes in the arrival time of the P-wave probe as it passes through the perturbation created by the traveling S-wave pump were recorded. Waveforms were time gated to simulate a semi-infinite medium. The shear wave phase relative to the P-wave probe signal was varied with resultant changes in the P-wave probe arrival time of up to 100 ns, corresponding to a change in elastic properties of 0.2%. In order to estimate the strain in our sample, ae also measured the particle velocity at the sample surface to scale a finite difference linear elastic simulation to estimate the complex strain field in the sample, on the order of $10^{-6}$, induced by the S-wave pump. We derived a fourth order elastic model to relate the changes in elasticity to the pump strain components. We recover quadratic and cubic nonlinear parameters: $\tilde{\beta}=-872$, $\tilde{\delta}=-1.1\times10^{10}$, respectively, at room-temperature and when particle motions of the pump and probe waves are aligned. Temperature fluctuations are correlated to changes in the recovered values of $\tilde{\beta}$ and $\tilde{\delta}$ and we find that the nonlinear parameter changes when the particle motions are orthogonal. No evidence of slow dynamics was seen in our measurements. 	
1509.07436v3	http://arxiv.org/pdf/1509.07436v3	2016	Subterranean production of neutrons, $^{39}$Ar and $^{21}$Ne: Rates and   uncertainties	Ondřej Šrámek|Lauren Stevens|William F. McDonough|Sujoy Mukhopadhyay|R. J. Peterson	  Accurate understanding of the subsurface production rate of the radionuclide $^{39}$Ar is necessary for argon dating techniques and noble gas geochemistry of the shallow and the deep Earth, and is also of interest to the WIMP dark matter experimental particle physics community. Our new calculations of subsurface production of neutrons, $^{21}$Ne, and $^{39}$Ar take advantage of the state-of-the-art reliable tools of nuclear physics to obtain reaction cross sections and spectra (TALYS) and to evaluate neutron propagation in rock (MCNP6). We discuss our method and results in relation to previous studies and show the relative importance of various neutron, $^{21}$Ne, and $^{39}$Ar nucleogenic production channels. Uncertainty in nuclear reaction cross sections, which is the major contributor to overall calculation uncertainty, is estimated from variability in existing experimental and library data. Depending on selected rock composition, on the order of $10^7$-$10^{10}$ {\alpha} particles are produced in one kilogram of rock per year (order of 1-$10^3$ kg$^{-1}$ s$^{-1}$); the number of produced neutrons is lower by $\sim6$ orders of magnitude, $^{21}$Ne production rate drops by an additional factor of 15-20, and another one order of magnitude or more is dropped in production of $^{39}$Ar. Our calculation yields a nucleogenic $^{21}$Ne/$^4$He production ratio of $(4.6\pm0.6) \times 10^{-8}$ in Continental Crust and $(4.2\pm0.5) \times 10^{-8}$ in Oceanic Crust and Depleted Mantle. Calculated $^{39}$Ar production rates span a great range from $29\pm9$ atoms kg-rock$^{-1}$ yr$^{-1}$ in the K-Th-U-enriched Upper Continental Crust to $(2.6\pm0.8) \times 10^{-4}$ atoms kg-rock$^{-1}$ yr$^{-1}$ in Depleted Upper Mantle. Nucleogenic $^{39}$Ar production exceeds the cosmogenic production below $\sim700$ meters depth and thus, affects radiometric ages of groundwater. 	
9908329v1	http://arxiv.org/pdf/cond-mat/9908329v1	1999	Material failure time and the fiber bundle model with thermal noise	A. Guarino|R. Scorretti|S. Ciliberto	  The statistical properties of failure are studied in a fiber bundle model with thermal noise. We find that in agreement with recent experiments the macroscopic failure is produced by a thermal activation of microcracks. Most importantly the effective temperature of the system is amplified by the spatial disorder (heterogeneity) of the fiber bundle. 	
1105.0296v1	http://arxiv.org/pdf/1105.0296v1	2011	A Formal Model of Anonymous Systems	Yang D. Li	  We put forward a formal model of anonymous systems. And we concentrate on the anonymous failure detectors in our model. In particular, we give three examples of anonymous failure detectors and show that they can be used to solve the consensus problem and that they are equivalent to their classic counterparts. Moreover, we show some relationship among them and provide a simple classification of anonymous failure detectors. 	
1510.09119v2	http://arxiv.org/pdf/1510.09119v2	2016	From Byzantine Failures to Crash Failures in Message-Passing Systems: a   BG Simulation-based approach	Damien Imbs|Michel Raynal|Julien Stainer	  The BG-simulation is a powerful reduction algorithm designed for asynchronous read/write crash-prone systems. It allows a set of $(t+1)$ asynchronous sequential processes to wait-free simulate (i.e., despite the crash of up to $t$ of them) an arbitrary number $n$ of processes under the assumption that at most $t$ of them may crash. The BG simulation shows that, in read/write systems, the crucial parameter is not the number $n$ of processes, but the upper bound $t$ on the number of process crashes.   The paper extends the concept of BG simulation to asynchronous message-passing systems prone to Byzantine failures. Byzantine failures are the most general type of failure: a faulty process can exhibit any arbitrary behavior. Because of this, they are also the most difficult to analyze and to handle algorithmically. The main contribution of the paper is a signature-free reduction of Byzantine failures to crash failures. Assuming $t<\min(n',n/3)$, the paper presents an algorithm that simulates a system of $n'$ processes where up to $t$ may crash, on top of a basic system of $n$ processes where up to $t$ may be Byzantine. While topological techniques have been used to relate the computability of Byzantine failure-prone systems to that of crash failure-prone ones, this simulation is the first, to our knowledge, that establishes this relation directly, in an algorithmic way.   In addition to extending the basic BG simulation to message-passing systems and failures more severe than process crashes, being modular and direct, this simulation provides us with a deeper insight in the nature and understanding of crash and Byzantine failures in the context of asynchronous message-passing systems. Moreover, it also allows crash-tolerant algorithms, designed for asynchronous read/write systems, to be executed on top of asynchronous message-passing systems prone to Byzantine failures. 	
1609.02956v1	http://arxiv.org/pdf/1609.02956v1	2016	Puzzles in modern biology. I. Male sterility, failure reveals design	Steven A. Frank	  Many human males produce dysfunctional sperm. Various plants frequently abort pollen. Hybrid matings often produce sterile males. Widespread male sterility is puzzling. Natural selection prunes reproductive failure. Puzzling failure implies something that we do not understand about how organisms are designed. Solving the puzzle reveals the hidden processes of design. 	
1702.05849v1	http://arxiv.org/pdf/1702.05849v1	2017	A Platform for Automating Chaos Experiments	Ali Basiri|Aaron Blohowiak|Lorin Hochstein|Casey Rosenthal	  The Netflix video streaming system is composed of many interacting services. In such a large system, failures in individual services are not uncommon. This paper describes the Chaos Automation Platform, a system for running failure injection experiments on the production system to verify that failures in non-critical services do not result in system outages. 	
1708.07379v1	http://arxiv.org/pdf/1708.07379v1	2017	Results of the Survey: Failures in Robotics and Intelligent Systems	Johannes Wienke|Sebastian Wrede	  In January 2015 we distributed an online survey about failures in robotics and intelligent systems across robotics researchers. The aim of this survey was to find out which types of failures currently exist, what their origins are, and how systems are monitored and debugged - with a special focus on performance bugs. This report summarizes the findings of the survey. 	
1710.06832v1	http://arxiv.org/pdf/1710.06832v1	2017	The Origins of Computational Mechanics: A Brief Intellectual History and   Several Clarifications	James P. Crutchfield	  The principle goal of computational mechanics is to define pattern and structure so that the organization of complex systems can be detected and quantified. Computational mechanics developed from efforts in the 1970s and early 1980s to identify strange attractors as the mechanism driving weak fluid turbulence via the method of reconstructing attractor geometry from measurement time series and in the mid-1980s to estimate equations of motion directly from complex time series. In providing a mathematical and operational definition of structure it addressed weaknesses of these early approaches to discovering patterns in natural systems.   Since then, computational mechanics has led to a range of results from theoretical physics and nonlinear mathematics to diverse applications---from closed-form analysis of Markov and non-Markov stochastic processes that are ergodic or nonergodic and their measures of information and intrinsic computation to complex materials and deterministic chaos and intelligence in Maxwellian demons to quantum compression of classical processes and the evolution of computation and language.   This brief review clarifies several misunderstandings and addresses concerns recently raised regarding early works in the field (1980s). We show that misguided evaluations of the contributions of computational mechanics are groundless and stem from a lack of familiarity with its basic goals and from a failure to consider its historical context. For all practical purposes, its modern methods and results largely supersede the early works. This not only renders recent criticism moot and shows the solid ground on which computational mechanics stands but, most importantly, shows the significant progress achieved over three decades and points to the many intriguing and outstanding challenges in understanding the computational nature of complex dynamic systems. 	
0306247v4	http://arxiv.org/pdf/cond-mat/0306247v4	2004	Competing Styles of Statistical Mechanics: II. Comparison of Theory and   Experiment and Further Illustrations	Áurea R. Vasconcellos|J. Galvão Ramos|Roberto Luzzi	  In the present follow-up article of a previous one [1] we illustrate the use of the Unconventional Statistical Mechanics described and discussed in the latter. This is done via the analysis, resorting to Renyi approach, of experimental results in the case of so-called "anomalous" luminescence in nanometric quantum wells in semiconductor heterostructures, and the so-called "anomalous" cyclic voltammetry in fractal-like electrodes in microbatteries. Also a purely theoretical analysis is done in the cases of an ideal gas and of radiation comparing the conventional and unconventional approaches. In all of these situations it is discussed which is the failure to satisfy the Criteria of Efficiency and/or Sufficiency thus requiring to resort to the unconventional approach, and what determines the value of the infoentropic index in each case, and its dependence on the system characteristics. Moreover, on the basis of the results we obtain, it is conjectured that the infoentropic index may satisfy what we call a law defining a "path to sufficiency". PACS: 05.70.Ln; 82.20.Mj; 82.20.Db Keywords: Renyi Statistics; Escort Probability; Fractal Structured Systems; Power Law Properties. 	 renyi statistics, escort probability, fractal structured
systems, power law properties 

0403005v2	http://arxiv.org/pdf/quant-ph/0403005v2	2005	Path integrals from classical momentum paths	John Hegseth	  The path integral formulation of quantum mechanics constructs the propagator by evaluating the action S for all classical paths in coordinate space. A corresponding momentum path integral may also be defined through Fourier transforms in the endpoints. Although these momentum path integrals are especially simple for several special cases, no one has, to my knowledge, ever formally constructed them from all classical paths in momentum space. I show that this is possible because there exists another classical mechanics based on an alternate classical action R. Hamilton's Canonical equations result from a variational principle in both S and R. S uses fixed beginning and ending spatial points while R uses fixed beginning and ending momentum points. This alternative action's classical mechanics also includes a Hamilton-Jacobi equation. I also present some important points concerning the beginning and ending conditions on the action necessary to apply a Canonical transformation. These properties explain the failure of the Canonical transformation in the phase space path integral. It follows that a path integral may be constructed from classical position paths using S in the coordinate representation or from classical momentum paths using R in the momentum representation. Several example calculations are presented that illustrate the simplifications and practical advantages made possible by this broader view of the path integral. In particular, the normalized amplitude for a free particle is found without using the Schrodinger equation, the internal spin degree of freedom is simply and naturally derived, and the simple harmonic oscillator is calculated. 	
1102.0113v1	http://arxiv.org/pdf/1102.0113v1	2011	Drug transport mechanism of P-glycoprotein monitored by single molecule   fluorescence resonance energy transfer	Stefan Ernst|Brandy Verhalen|Nawid Zarrabi|Stephan Wilkens|Michael Boersch	  In this work we monitor the catalytic mechanism of P-glycoprotein (Pgp) using single-molecule fluorescence resonance energy transfer (FRET). Pgp, a member of the ATP binding cassette family of transport proteins, is found in the plasma membrane of animal cells where it is involved in the ATP hydrolysis driven export of hydrophobic molecules. When expressed in the plasma membrane of cancer cells, the transport activity of Pgp can lead to the failure of chemotherapy by excluding the mostly hydrophobic drugs from the interior of the cell. Despite ongoing effort, the catalytic mechanism by which Pgp couples MgATP binding and hydrolysis to translocation of drug molecules across the lipid bilayer is poorly understood. Using site directed mutagenesis, we have introduced cysteine residues for fluorescence labeling into different regions of the nucleotide binding domains (NBDs) of Pgp. Double-labeled single Pgp molecules showed fluctuating FRET efficiencies during drug stimulated ATP hydrolysis suggesting that the NBDs undergo significant movements during catalysis. Duty cycle-optimized alternating laser excitation (DCO-ALEX) is applied to minimize FRET artifacts and to select the appropriate molecules. The data show that Pgp is a highly dynamic enzyme that appears to fluctuate between at least two major conformations during steady state turnover. 	
1202.4796v2	http://arxiv.org/pdf/1202.4796v2	2013	Realistic time-scale fully atomistic simulations of surface nucleation   of dislocations in pristine nanopillars	Pratyush Tiwary|Axel van de Walle	  We use our recently proposed accelerated dynamics algorithm (Tiwary & van de Walle, 2011) to calculate temperature and stress dependence of activation free energy for surface nucleation of dislocations in pristine Gold nanopillars under realistic loads. While maintaining fully atomistic resolution, we achieve the fraction of a second time-scale regime. We find that the activation free energy depends significantly on the driving force (stress or strain) and temperature, leading to very high activation entropies. We also perform compression tests on Gold nanopillars for strain rates varying between 7 orders of magnitudes, reaching as low as 10^3/s. Our calculations show the quantitative effects on the yield point of unrealistic strain-rate Molecular Dynamics calculations: we find that while the failure mechanism for <001> compression of Gold nanopillars remains the same across the entire strain-rate range, the elastic limit (defined as stress for nucleation of the first dislocation) depends significantly on the strain-rate. We also propose a new methodology that overcomes some of the limits in our original accelerated dynamics scheme (and accelerated dynamics methods in general). We lay out our methods in sufficient details so as to be used for understanding and predicting deformation mechanism under realistic driving forces for various problems. 	
1207.1064v2	http://arxiv.org/pdf/1207.1064v2	2012	Fluctuations and differential contraction during regeneration of Hydra   vulgaris tissue toroids	Michael Krahe|Iris Wenzel|Kao-Nung Lin|Julia Fischer|Joseph Goldmann|Markus Kästner|Claus Fütterer	  We studied regenerating bilayered tissue toroids dissected from Hydra vulgaris polyps and relate our macroscopic observations to the dynamics of force-generating mesoscopic cytoskeletal structures. Tissue fragments undergo a specific toroid-spheroid folding process leading to complete regeneration towards a new organism. The time scale of folding is too fast for biochemical signalling or morphogenetic gradients which forced us to assume purely mechanical self-organization. The initial pattern selection dynamics was studied by embedding toroids into hydro-gels allowing us to observe the deformation modes over longer periods of time. We found increasing mechanical fluctuations which break the toroidal symmetry and discuss the evolution of their power spectra for various gel stiffnesses. Our observations are related to single cell studies which explain the mechanical feasibility of the folding process. In addition, we observed switching of cells from a tissue bound to a migrating state after folding failure as well as in tissue injury.   We found a supra-cellular actin ring assembled along the toroid's inner edge. Its contraction can lead to the observed folding dynamics as we could confirm by finite element simulations. This actin ring in the inner cell layer is assembled by myosin- driven length fluctuations of supra-cellular {\alpha}-actin structures (myonemes) in the outer cell-layer. 	
1312.7835v1	http://arxiv.org/pdf/1312.7835v1	2013	Can we advance macroscopic quantum systems outside the framework of   complex decoherence theory?	Mark E. Brezinski|Maria Rupnick	  Macroscopic quantum systems (MQS) are macroscopic systems driven by quantum rather than classical mechanics, a long studied area with minimal success till recently. Harnessing the benefits of quantum mechanics on a macroscopic level would revolutionize fields ranging from telecommunication to biology, the latter focused on here for reasons discussed. Contrary to misconceptions, there are no known physical laws that prevent the development of MQS. Instead, they are generally believed universally lost in complex systems from environmental entanglements (decoherence). But we argue success is achievable MQS with decoherence compensation developed, naturally or artificially, from top-down rather current reductionist approaches. This paper advances the MQS field by a complex systems approach to decoherence. First, why complex system decoherence approaches (top-down) are needed is discussed. Specifically, complex adaptive systems (CAS) are not amenable to reductionist models (and their master equations) because of emergent behavior, approximation failures, not accounting for quantum compensator mechanisms, ignoring path integrals, and the subentity problem. In addition, since MQS must exist within the context of the classical world, rapid decoherence and prolonged coherence are both needed. Nature has already demonstrated this for quantum subsystems such as photosynthesis and magnetoreception. Second, we perform a preliminary study that illustrates a top-down approach to potential MQS. In summary, reductionist arguments against MQS are not justifiable. It is more likely they are not easily detectable in large intact classical systems or has been destroyed by reductionist experimental set-ups. This complex systems decoherence approach, using top down investigations, is critical to paradigm shifts in MQS research both in biological and non-biological systems. 	
1503.04422v1	http://arxiv.org/pdf/1503.04422v1	2015	Making Availability as a Service in the Clouds	Pengfei Chen|Yong Qi|Peipei Wang|Li Su|Xinyi Li	  Cloud computing has achieved great success in modern IT industry as an excellent computing paradigm due to its flexible management and elastic resource sharing. To date, cloud computing takes an irrepalceable position in our socioeconomic system and influences almost every aspect of our daily life. However, it is still in its infancy, many problems still exist.Besides the hotly-debated security problem, availability is also an urgent issue.With the limited power of availability mechanisms provided in present cloud platform, we can hardly get detailed availability information of current applications such as the root causes of availability problem,mean time to failure, etc. Thus a new mechanism based on deep avaliability analysis is neccessary and benificial.Following the prevalent terminology 'XaaS',this paper proposes a new win-win concept for cloud users and providers in term of 'Availability as a Service' (abbreviated as 'AaaS').The aim of 'AaaS' is to provide comprehensive and aimspecific runtime avaliabilty analysis services for cloud users by integrating plent of data-driven and modeldriven approaches. To illustrate this concept, we realize a prototype named 'EagleEye' with all features of 'AaaS'. By subscribing corresponding services in 'EagleEye', cloud users could get specific availability information of their applications deployed in cloud platform. We envision this new kind of service will be merged into the cloud management mechanism in the near future. 	
1509.07801v3	http://arxiv.org/pdf/1509.07801v3	2016	Analysis of the Behavior of Ultra High Performance Concrete at Early Age	Lin Wan|Roman Wendner|Benliang Liang|Gianluca Cusatis	  Ultra high performance concretes (UHPCs) are cementitious composite materials with high level of perfor- mance characterized by high compressive strength, high tensile strength and superior durability, reached by low water-to-binder ratio, optimized aggregate size distribution, thermal activation, and fiber reinforcement. In the past couple of decades, more and more UHPCs have been developed and found their ways into practice. Thus, the demand for computational models capable of describing and predicting relevant aging phenomena to assist design and planning is increasing. This paper presents the early age experimental characterization as well as the results of subsequent simulations of a typical UHPC matrix. Performed and simulated tests include unconfined compression, splitting (Brazilian), and three-point-bending tests. The computational framework is formulated by coupling a hygro-thermo-chemical (HTC) theory and a comprehensive mesoscale discrete model with formulated aging functions. The HTC component allows taking into account various types of curing conditions with varying temperature and relative humidity and predicting the level of concrete aging. The mechanical component, the Lattice Discrete Particle Model (LDPM), permits the simulation of the failure behavior of concrete at the length scale of major heterogeneities. The aging functions relate the mesoscale LDPM mechanical properties in terms of aging degree, defined in this work as the ratio between the quasi-static elastic modulus at a certain age and its asymptotic value. The obtained results provide insights in both UHPC early age mechanisms and a computational model for the analysis of aging UHPC structures. 	
1703.07864v1	http://arxiv.org/pdf/1703.07864v1	2017	Quantifying the structural integrity of nanorod arrays	Florian Thöle|Longjian Xue|Claudia Heß|Reinald Hillebrand|Stanislav N. Gorb|Martin Steinhart	  Arrays of aligned nanorods oriented perpendicular to a support, which are accessible by top-down lithography or by means of shape-defining hard templates, have received increasing interest as sensor components, components for nanophotonics and nanoelectronics, substrates for tissue engineering, sur-faces having specific adhesive or antiadhesive properties and as surfaces with customized wettability. Agglomeration of the nanorods deteriorates the performance of components based on nanorod arrays. A comprehensive body of literature deals with mechanical failure mechanisms of nanorods and design criteria for mechanically stable nanorod arrays. However, the structural integrity of nanorod arrays is commonly evaluated only visually and qualitatively. We use real-space analysis of microscopic images to quantify the fraction of condensed nanorods in nanorod arrays. We suggest the number of array elements apparent in the micrographs divided by the number of array elements a defect-free array would contain in the same area, referred to as integrity fraction, as a measure of structural array integrity. Reproducible procedures to determine the imaged number of array elements are introduced. Thus, quantitative comparisons of different nanorod arrays, or of one nanorod array at different stages of its use, are possible. Structural integrities of identical nanorod arrays differing only in the length of the nanorods are exemplarily analyzed. 	
0310222v1	http://arxiv.org/pdf/astro-ph/0310222v1	2003	Cooling Flows or Heating Flows?	James Binney	  It is now clear that AGN heat cooling flows, largely by driving winds. The winds may contain a relativistic component that generates powerful synchrotron radiation, but it is not clear that all winds do so. The spatial and temporal stability of the AGN/cooling flow interaction are discussed. Collimation of the winds probably provides spatial stability. Temporal stability may be possible only for black holes with masses above a critical value. Both the failure of cooling flows to have adiabatic cores and the existence of X-ray cavities confirm the importance of collimated outflows. I quantify the scale of the convective flow that the AGN Hydra would need to drive if it balanced radiative inward flow by outward flow parallel to the jets. At least in Virgo any such flow must be confined to r<~20 kpc. Hydrodynamical simulations suggest that AGN outbursts cannot last longer than ~25 Myr. Data for four clusters with well studied X-ray cavities suggests that heating associated with cavity formation approximately balances radiative cooling. The role of cosmic infall and the mechanism of filament formation are briefly touched on. 	
9707013v1	http://arxiv.org/pdf/cond-mat/9707013v1	1997	Multicanonical Methods vs. Molecular Dynamics vs. Monte Carlo:   Comparison for Lennard-Jones Glasses	Kamal K. Bhattacharya|James P. Sethna	  We applied a multicanonical algorithm (entropic sampling) to a two-dimensional and a three-dimensional Lennard-Jones system with quasicrystalline and glassy ground states. Focusing on the ability of the algorithm to locate low lying energy states, we compared the results of the multicanonical simulations with standard Monte Carlo simulated annealing and molecular dynamics methods. We find slight benefits to using entropic sampling in small systems (less than 80 particles), which disappear with larger systems. This is disappointing as the multicanonical methods are designed to surmount energy barriers to relaxation. We analyze this failure theoretically, and show (1) the multicanonical method is reduced in the thermodynamic limit (large systems) to an effective Monte Carlo simulated annealing with a random temperature vs. time, and (2) the multicanonical method gets trapped by unphysical entropy barriers in the same metastable states whose energy barriers trap the traditional quenches. The performance of Monte Carlo and molecular dynamics quenches were remarkably similar. 	
9707114v1	http://arxiv.org/pdf/cond-mat/9707114v1	1997	Scaling with respect to disorder in time-to-failure	D. Sornette|J. V. Andersen	  We revisit a simple dynamical model of rupture in random media with long-range elasticity to test whether rupture can be seen as a first-order or a critical transition. We find a clear scaling of the macroscopic modulus as a function of time-to-rupture and of the amplitude of the disorder, which allows us to collapse neatly the numerical simulations over more than five decades in time and more than one decade in disorder amplitude onto a single master curve. We thus conclude that, at least in this model, dynamical rupture in systems with long-range elasticity is a genuine critical phenomenon occurring as soon as the disorder is non-vanishing. 	
9708220v1	http://arxiv.org/pdf/cond-mat/9708220v1	1997	Fine structure and complex exponents in power law distributions from   random maps	Per Jögi|Didier Sornette|Michael Blank	  Discrete scale invariance (DSI) has recently been documented in time-to-failure rupture, earthquake processes and financial crashes, in the fractal geometry of growth processes and in random systems. The main signature of DSI is the presence of log-periodic oscillations correcting the usual power laws, corresponding to complex exponents. Log-periodic structures are important because they reveal the presence of preferred scaling ratios of the underlying physical processes. Here, we present new evidence of log-periodicity overlaying the leading power law behavior of probability density distributions of affine random maps with parametric noise. The log-periodicity is due to intermittent amplifying multiplicative events. We quantify precisely the progressive smoothing of the log-periodic structures as the randomness increases and find a large robustness. Our results provide useful markers for the search of log-periodicity in numerical and experimental data. 	
9709327v1	http://arxiv.org/pdf/cond-mat/9709327v1	1997	Comment on "Tricritical Behavior in Rupture Induced by Disorder"	Rava da Silveira	  In their letter, Andersen, Sornette, and Leung [Phys. Rev. Lett. 78, 2140 (1997)] describe possible behaviors for rupture in disordered media, based on the mean field-like democratic fiber bundle model. In this model, fibers are pulled with a force which is distributed uniformly. A fiber breaks if the stress on it exceeds a threshold chosen from a probability distribution, and the force is then redistributed over the intact fibers. Andersen et al. claim the existence of a tricritical point, separating a "first-order" regime, characterized by a sudden global failure, from a "second-order" regime, characterized by a divergence in the breaking rate. We show that a first-order transition is an artifact of a (large enough) discontinuity put by hand in the disorder distribution. Thus, in generic physical cases, a first-order regime is not present. This result is obtained from a graphical method, which, unlike Andersen at al.'s analytical solution, enables us to distinguish the various classes of qualitatively different behaviors of the model. 	
9803191v1	http://arxiv.org/pdf/cond-mat/9803191v1	1998	Evidence of discrete scale invariance in DLA and time-to-failure by   canonical averaging	A. Johansen|D. Sornette	  Discrete scale invariance, which corresponds to a partial breaking of the scaling symmetry, is reflected in the existence of a hierarchy of characteristic scales l0, c l0, c^2 l0,... where c is a preferred scaling ratio and l0 a microscopic cut-off. Signatures of discrete scale invariance have recently been found in a variety of systems ranging from rupture, earthquakes, Laplacian growth phenomena, ``animals'' in percolation to financial market crashes. We believe it to be a quite general, albeit subtle phenomenon. Indeed, the practical problem in uncovering an underlying discrete scale invariance is that standard ensemble averaging procedures destroy it as if it was pure noise. This is due to the fact, that while c only depends on the underlying physics, l0 on the contrary is realisation-dependent. Here, we adapt and implement a novel so-called ``canonical'' averaging scheme which re-sets the l0 of different realizations to approximately the same value. The method is based on the determination of a realization-dependent effective critical point obtained from, e.g., a maximum susceptibility criterion. We demonstrate the method on diffusion limited aggregation and a model of rupture. 	
9808140v1	http://arxiv.org/pdf/cond-mat/9808140v1	1998	Evidence for the droplet/scaling picture of spin glasses	M. A. Moore|Hemant Bokil|Barbara Drossel	  We have studied the Parisi overlap distribution for the three dimensional Ising spin glass in the Migdal-Kadanoff approximation. For temperatures T around 0.7Tc and system sizes upto L=32, we found a P(q) as expected for the full Parisi replica symmetry breaking, just as was also observed in recent Monte Carlo simulations on a cubic lattice. However, for lower temperatures our data agree with predictions from the droplet or scaling picture. The failure to see droplet model behaviour in Monte Carlo simulations is due to the fact that all existing simulations have been done at temperatures too close to the transition temperature so that sytem sizes larger than the correlation length have not been achieved. 	
9905329v1	http://arxiv.org/pdf/cond-mat/9905329v1	1999	Multifractality in Human Heartbeat Dynamics	Plamen Ch. Ivanov|Luís A. Nunes Amaral|Ary L. Goldberger|Shlomo Havlin|Michael G. Rosenblum|Zbigniew Struzik|H. Eugene Stanley	  Recent evidence suggests that physiological signals under healthy conditions may have a fractal temporal structure. We investigate the possibility that time series generated by certain physiological control systems may be members of a special class of complex processes, termed multifractal, which require a large number of exponents to characterize their scaling properties. We report on evidence for multifractality in a biological dynamical system --- the healthy human heartbeat. Further, we show that the multifractal character and nonlinear properties of the healthy heart rate are encoded in the Fourier phases. We uncover a loss of multifractality for a life-threatening condition, congestive heart failure. 	
9910211v2	http://arxiv.org/pdf/cond-mat/9910211v2	1999	Dynamics of the 2D two-component plasma near the Kosterlitz-Thouless   transition	Dierk Bormann|Hans Beck|Oliver Gallus|Massimiliano Capezzali	  We study the dynamics of a classical, two-component plasma in two dimensions, in the vicinity of the Kosterlitz-Thouless (KT) transition where the system passes from a dielectric low-temperature phase (consisting of bound pairs) to a conducting phase. We use two ``complementary'' analytical approaches and compare to simulations. The conventional, ``intuitive'' approach is built on the KT picture of independently relaxing, bound pairs. A more formal approach, working with Mori projected dynamic correlation functions, avoids to assume the pair picture from the start. We discuss successes and failures of both approaches, and suggest a way to combine the advantages of both. 	
0003131v1	http://arxiv.org/pdf/cond-mat/0003131v1	2000	Scaling of interfaces in brittle fracture and perfect plasticity	E. T. Seppala|V. I. Raisanen|M. J. Alava	  The roughness properties of two-dimensional fracture surfaces as created by the slow failure of random fuse networks are considered and compared to yield surfaces of perfect plasticity with similar disorder. By studying systems up to a linear size L=350 it is found that in the cases studied the fracture surfaces exhibit self-affine scaling with a roughness exponent close to 2/3, which is asymptotically exactly true for plasticity though finite-size effects are evident for both. The overlap of yield or minimum energy and fracture surfaces with exactly the same disorder configuration is shown to be a decreasing function of the system size and to be of a rather large magnitude for all cases studied. The typical ``overlap cluster'' length between pairs of such interfaces converges to a constant with $L$ increasing. 	
0005284v2	http://arxiv.org/pdf/cond-mat/0005284v2	2000	Scale Invariance in the Nonstationarity of Physiological Signals	Pedro Bernaola-Galvan|Plamen Ch. Ivanov|Luis A. Nunes Amaral|Ary L. Goldberger|H. Eugene Stanley	  We introduce a segmentation algorithm to probe temporal organization of heterogeneities in human heartbeat interval time series. We find that the lengths of segments with different local values of heart rates follow a power-law distribution. This scale-invariant structure is not a simple consequence of the long-range correlations present in the data. We also find that the differences in mean heart rates between consecutive segments display a common functional form, but with different parameters for healthy individuals and for patients with heart failure. This finding may provide information into the way heart rate variability is reduced in cardiac disease. 	
0007300v2	http://arxiv.org/pdf/cond-mat/0007300v2	2000	Network robustness and fragility: Percolation on random graphs	D. S. Callaway|M. E. J. Newman|S. H. Strogatz|D. J. Watts	  Recent work on the internet, social networks, and the power grid has addressed the resilience of these networks to either random or targeted deletion of network nodes. Such deletions include, for example, the failure of internet routers or power transmission lines. Percolation models on random graphs provide a simple representation of this process, but have typically been limited to graphs with Poisson degree distribution at their vertices. Such graphs are quite unlike real world networks, which often possess power-law or other highly skewed degree distributions. In this paper we study percolation on graphs with completely general degree distribution, giving exact solutions for a variety of cases, including site percolation, bond percolation, and models in which occupation probabilities depend on vertex degree. We discuss the application of our theory to the understanding of network resilience. 	
0012007v1	http://arxiv.org/pdf/cond-mat/0012007v1	2000	Dilute Bose gas: short-range particle correlations and ultraviolet   divergence	A. Yu. Cherny|A. A. Shanenko	  The modified Bogoliubov model where the primordial interaction is replaced by the t matrix is reinvestigated. It is shown to provide a negative value of the kinetic energy for a strongly interacting dilute Bose gas, contrary to the original Bogoliubov model. To clear up the origin of this failure, the correct values of the kinetic and interaction energies of a dilute Bose gas are calculated. It is demonstrated that both the problem of the negative kinetic energy and the ultraviolet divergence, dating back to the well-known paper of Lee, Yang and Huang, is connected with an inadequate picture of the short-range boson correlations. These correlations are reconsidered within the thermodynamically consistent model proposed earlier by the present authors. Found results are in absolute agreement with the data of the Monte-Carlo calculations for the hard-sphere Bose gas. 	
0012311v1	http://arxiv.org/pdf/cond-mat/0012311v1	2000	Indivisibility of electron bubbles in helium	Veit Elser	  A recent proposal by Maris[1], that single electron bubbles in helium might fission into separate, particle-like entities, does not properly take into account the failure of the adiabatic approximation when, due to tunneling, there is a long electronic time scale. The point along the fission pathway of a photoexcited p-state bubble, where the adiabatic approximation first breaks down, occurs well before the bubble waist has pinched down forming two cavities. In the connected two-lobed geometry, the p- and s-states are strongly mixed by an antisymmetric vibrational mode, and the excitation decays by the mechanism where one lobe collapses while the other expands into the spherical s-state geometry. The extreme pressure jump in a photoexcited bubble leads to shock formation that may halt the elongation even before adiabaticity is compromised. In this case, the photoexcited bubble decays radiatively from the relaxed p-state geometry.   [1] H.J. Maris, On the fission of elementary particles and the evidence for fractional electrons in liquid helium, J. Low Temp. Phys. 120, 173 (2000) 	
0101378v2	http://arxiv.org/pdf/cond-mat/0101378v2	2001	Nonlinear reactive systems viewed as Boolean dynamical systems	E. Abad|P. Grosfils|G. Nicolis	  We present a stochastic, time-discrete boolean model which mimics the mesoscopic dynamics of the desorption reactions $A+A\to A+S$ and $A+A\to S+S$ in a 1D lattice. In the continuous-time limit, we derive a hierarchy of dynamical equations for the subset of moments involving contiguous lattice sites. The solution of the hierarchy allows to compute the exact dynamics of the mean coverage for both microscopic and coarse-grained initial conditions, which turn out to be different from the mean field predictions. The evolution equations for the mean coverage and the second order moments are shown to be equivalent to those provided by a time-continuous Master equation. The important role of higher order fluctuations is brought out by the failure of a truncation scheme retaining only two-particle fluctuation correlations. 	
0102351v2	http://arxiv.org/pdf/cond-mat/0102351v2	2001	Roughness exponent in the fracture of fibrous materials	I. L. Menezes-Sobrinho	  In this paper, a computational model in (2+1)-dimensions which simulates the rupture process of a fibrous material submitted to a constant force $F$, is analyzed. The roughness exponent $\zeta$ at the boundary that separates two failure regimes, catastrophic and slowly shredding, is evaluated. In the catastrophic (dynamic) regime the initial strain creates a crack which percolates rapidly through the material. In the slowly shredding (quasi-static) regime several cracks of small size appear in all parts of the material, the rupture process is slow and any single crack percolates the sample. At the boundary between these two regimes, we obtained a value $\zeta\simeq 0.42\pm 0.02$ for the roughness exponent, in agreement with results provided by other simulations in three dimension. Also, at this boundary we observed a power law behavior on the number of cracks versus its size. 	
0102397v1	http://arxiv.org/pdf/cond-mat/0102397v1	2001	Simulation of pedestrian dynamics using a 2-dimensional cellular   automaton	C. Burstedde|K. Klauck|A. Schadschneider|J. Zittartz	  We propose a 2-dimensional cellular automaton model to simulate pedestrian traffic. It is a vmax=1 model with exclusion statistics and parallel dynamics. Long-range interactions between the pedestrians are mediated by a so called floor field which modifies the transition rates to neighbouring cells. This field, which can be discrete or continuous, is subject to diffusion and decay. Furthermore it can be modified by the motion of the pedestrians. Therefore the model uses an idea similar to chemotaxis, but with pedestrians following a virtual rather than a chemical trace. Our main goal is to show that the introduction of such a floor field is sufficient to model collective effects and self-organization encountered in pedestrian dynamics, e.g. lane formation in counterflow through a large corridor. As an application we also present simulations of the evacuation of a large room with reduced visibility, e.g. due to failure of lights or smoke. 	
0103232v1	http://arxiv.org/pdf/cond-mat/0103232v1	2001	Creep rupture of viscoelastic fiber bundles	Raul Cruz Hidalgo|Ferenc Kun|Hans. J. Herrmann	  We study the creep rupture of bundles of viscoelastic fibers occurring under uniaxial constant tensile loading. A novel fiber bundle model is introduced which combines the viscoelastic constitutive behaviour and the strain controlled breaking of fibers. Analytical and numerical calculations showed that above a critical external load the deformation of the system monotonically increases in time resulting in global failure at a finite time $t_f$, while below the critical load the deformation tends to a constant value giving rise to an infinite lifetime. Our studies revealed that the nature of the transition between the two regimes, i.e. the behaviour of $t_f$ at the critical load $sigma_c$, strongly depends on the range of load sharing: for global load sharing $t_f$ has a power law divergence at $\sigma_c$ with a universal exponent of 0.5, however, for local load sharing the transition becomes abrupt: at the critical load $t_f$ jumps to a finite value, analogous to second and first order phase transitions, respectively. The acoustic response of the bundle during creep is also studied. 	
0106054v1	http://arxiv.org/pdf/cond-mat/0106054v1	2001	Theory of self-similar oscillatory finite-time singularities in Finance,   Population and Rupture	D. Sornette|K. Ide	  This is a short letter summarizing the long paper cond-mat/0106047 in which we present a simple two-dimensional dynamical system reaching a singularity in finite time decorated by accelerating oscillations due to the interplay between nonlinear positive feedback and reversal in the inertia. This provides a fundamental equation for the dynamics of (1) stock market prices in the presence of nonlinear trend-followers and nonlinear value investors, (2) the world human population with a competition between a population-dependent growth rate and a nonlinear dependence on a finite carrying capacity and (3) the failure of a material subject to a time-varying stress with a competition between positive geometrical feedback on the damage variable and nonlinear healing. The rich fractal scaling properties of the dynamics are traced back to the self-similar spiral structure in phase space unfolding around an unstable spiral point at the origin. 	
0106136v2	http://arxiv.org/pdf/cond-mat/0106136v2	2002	Instability of scale-free networks under node-breaking avalanches	Y. Moreno|J. B. Gómez|A. F. Pacheco	  The instability introduced in a large scale-free network by the triggering of node-breaking avalanches is analyzed using the fiber-bundle model as conceptual framework. We found, by measuring the size of the giant component, the avalanche size distribution and other quantities, the existence of an abrupt transition. This test of strength for complex networks like Internet is more stringent than others recently considered like the random removal of nodes, analyzed within the framework of percolation theory. Finally, we discuss the possible implications of our results and their relevance in forecasting cascading failures in scale-free networks. 	
0106485v3	http://arxiv.org/pdf/cond-mat/0106485v3	2003	The Stable Random Matrix ensembles	M. Tierz	  We address the construction of stable random matrix ensembles as the generalization of the stable random variables (Levy distributions). With a simple method we derive the Cauchy case, which is known to have remarkable properties. These properties allow for such an intuitive method -that relies on taking traces- to hold. Approximate but general results regarding the other distributions are derived as well. Some of the special properties of these ensembles are evidenced by showing partial failure of mean-field approaches. To conclude, we compute the confining potential that gives a Gaussian density of states in the limit of large matrices. The result is an hypergeometric function, in contrast with the simplicity of the Cauchy case. 	
0106569v1	http://arxiv.org/pdf/cond-mat/0106569v1	2001	Density functional theory in the canonical ensemble I General formalism	J. A. Hernando	  Density functional theory stems from the Hohenberg-Kohn-Sham-Mermin (HKSM) theorem in the grand canonical ensemble (GCE). However, as recent work shows, although its extension to the canonical ensemble (CE) is not straightforward, work in nanopore systems could certainly benefit from a mesoscopic DFT in the CE. The stumbling block is the fixed $N$ constraint which is responsible for the failure in proving the interchangeability of density profiles and external potentials as independent variables. Here we prove that, if in the CE the correlation functions are stripped off of their asymptotic behaviour (which is not in the form of a properly irreducible $n$-body function), the HKSM theorem can be extended to the CE. In proving that, we generate a new {\it hierarchy} of $N$-modified distribution and correlation functions which have the same formal structure that the more conventional ones have (but with the proper irreducible $n$-body behaviour) and show that, if they are employed, either a modified external field or the density profiles can indistinctly be used as independent variables. We also write down the $N$-modified free energy functional and prove that the thermodynamic potential is minimized by the equilibrium values of the new hierarchy. 	
0107467v1	http://arxiv.org/pdf/cond-mat/0107467v1	2001	The Heumann-Hotzel model for aging revisited	Nazareno G. F. de Medeiros|Roberto N. Onody	  Since its proposition in 1995, the Heumann-Hotzel model has remained as an obscure model of biological aging. The main arguments used against it were its apparent inability to describe populations with many age intervals and its failure to prevent a population extinction when only deleterious mutations are present. We find that with a simple and minor change in the model these difficulties can be surmounted. Our numerical simulations show a plethora of interesting features: the catastrophic senescence, the Gompertz law and that postponing the reproduction increases the survival probability, as has already been experimentally confirmed for the Drosophila fly. 	
0107476v3	http://arxiv.org/pdf/cond-mat/0107476v3	2003	New Results for the Nonlocal Kardar-Parisi-Zhang Equation	Eytan Katzav	  In this paper various predictions for the scaling exponents of the Nonlocal Kardar-Parisi-Zhang (NKPZ) equation are discussed. I use the Self-Consistent Expansion (SCE), and obtain results that are quite different from result obtained in the past, using Dynamic Renormalization Group analysis (DRG), a Scaling Approach (SA) and a self-consistent Mode Coupling approach (MC). It is shown that the results obtained using SCE recover an exact result for a subfamily of the NKPZ models in one dimension, while all the other methods fail to do so. It is also shown that the SCE result is the only one that is compatible with simple observations on the dependence of the dynamic exponent $z$ in the NKPZ model on the exponent $\rho$ characterizing the decay of the nonlinear interaction. The reasons for the failure of other methods to deal with NKPZ are also discussed. 	
0108258v1	http://arxiv.org/pdf/cond-mat/0108258v1	2001	Large-scale simulation of adhesion dynamics for end-graphed polymers	Scott W. Sides|Gary S. Grest|Mark J. Stevens	  The adhesion between a polymer melt and substrate is studied in the presence of chemically attached chains on the substrate surface. Extensive molecular dynamics simulations have been carried out to study the effect of temperature, tethered chain areal density ($\Sigma$), tethered chain length ($N_{t}$), chain bending energy ($k_{\theta}$) and tensile pull velocity ($v$) on the adhesive failure mechanisms of pullout and/or scission of the tethered chains. We observe a crossover from pure chain pullout to chain scission as $N_{t}$ is increased. Below the glass transition, the value of $N_{t}$ for which this crossover begins approaches the bulk entanglement length $N_{e}$. For the values of $N_{t}$ and $\Sigma$ used here, no crossover to crazing is observed. 	
0109119v1	http://arxiv.org/pdf/cond-mat/0109119v1	2001	A simple model of bank bankruptcies	A. Aleksiejuk|J. A. Holyst	  Interbank deposits (loans and credits) are quite common in banking system all over the world. Such interbank co-operation is profitable for banks but it can also lead to collective financial failures. In this paper we introduce a new model of directed percolation as a simple representation for contagion process and mass bankruptcies in banking systems. Directed connections that are randomly distributed between junctions of bank lattice simulate flows of money in our model. Critical values of a mean density of interbank connections as well as static and dynamic scaling laws for the statistic of avalange bankruptcies are found. Results of computer simulations for the universal profile of bankruptcies spreading are in a qualitative agreement with the third wave of bank suspensions during The Great Depression in the USA. 	
0111586v2	http://arxiv.org/pdf/cond-mat/0111586v2	2001	Self-organized criticality in a model of collective bank bankruptcies	Agata Aleksiejuk|Janusz A. Holyst|Gueorgi Kossinets	  The question we address here is of whether phenomena of collective bankruptcies are related to self-organized criticality. In order to answer it we propose a simple model of banking networks based on the random directed percolation. We study effects of one bank failure on the nucleation of contagion phase in a financial market. We recognize the power law distribution of contagion sizes in 3d- and 4d-networks as an indicator of SOC behavior. The SOC dynamics was not detected in 2d-lattices. The difference between 2d- and 3d- or 4d-systems is explained due to the percolation theory. 	
0202231v1	http://arxiv.org/pdf/cond-mat/0202231v1	2002	Regular binary thermal lattice-gases	Ronald Blaak|David Dubbeldam	  We analyze the power spectrum of a regular binary thermal lattice gas in two dimensions and derive a Landau-Placzek formula, describing the power spectrum in the low-wavelength, low frequency domain, for both the full mixture and a single component in the binary mixture. The theoretical results are compared with simulations performed on this model and show a perfect agreement. The power spectrums are found to be similar in structure as the ones obtained for the continuous theory, in which the central peak is a complicated superposition of entropy and concentration contributions, due to the coupling of the fluctuations in these quantities. Spectra based on the relative difference between both components have in general additional Brillouin peaks as a consequence of the equipartition failure. 	
0207311v1	http://arxiv.org/pdf/cond-mat/0207311v1	2002	A model of large-scale proteome evolution	Ricard V. Sole|Romualdo Pastor-Satorras|Eric Smith|Thomas B. Kepler	  The next step in the understanding of the genome organization, after the determination of complete sequences, involves proteomics. The proteome includes the whole set of protein-protein interactions, and two recent independent studies have shown that its topology displays a number of surprising features shared by other complex networks, both natural and artificial. In order to understand the origins of this topology and its evolutionary implications, we present a simple model of proteome evolution that is able to reproduce many of the observed statistical regularities reported from the analysis of the yeast proteome. Our results suggest that the observed patterns can be explained by a process of gene duplication and diversification that would evolve proteome networks under a selection pressure, favoring robustness against failure of its individual components. 	
0207402v1	http://arxiv.org/pdf/cond-mat/0207402v1	2002	The process of irreversible nucleation in multilayer growth. I. Failure   of the mean-field approach	Paolo Politi|Claudio Castellano	  The formation of stable dimers on top of terraces during epitaxial growth is investigated in detail. In this paper we focus on mean-field theory, the standard approach to study nucleation. Such theory is shown to be unsuitable for the present problem, because it is equivalent to considering adatoms as independent diffusing particles. This leads to an overestimate of the correct nucleation rate by a factor N, which has a direct physical meaning: in average, a visited lattice site is visited N times by a diffusing adatom. The dependence of N on the size of the terrace and on the strength of step-edge barriers is derived from well known results for random walks. The spatial distribution of nucleation events is shown to be different from the mean-field prediction, for the same physical reason. In the following paper we develop an exact treatment of the problem. 	
0210119v1	http://arxiv.org/pdf/cond-mat/0210119v1	2002	Time evolution of damage under variable ranges of load transfer	Oluwole E. Yewande|Yamir Moreno|Ferenc Kun|Raul Cruz Hidalgo|Hans J. Herrmann	  We study the time evolution of damage in a fiber bundle model in which the range of interaction of fibers varies through an adjustable stress transfer function recently introduced. We find that the lifetime of the material exhibits a crossover from mean field to short range behavior as in the static case. Numerical calculations showed that the value at which the transition takes place depends on the system's disorder. Finally, we have performed a microscopic analysis of the failure process. Our results confirm that the growth dynamics of the largest crack is radically different in the two limiting regimes of load transfer during the first stages of breaking. 	
0211331v1	http://arxiv.org/pdf/cond-mat/0211331v1	2002	Universality classes in creep rupture	Ferenc Kun|Yamir Moreno|Raul Cruz Hidalgo|Hans. J. Herrmann	  We study the creep response of solids to a constant external load in the framework of a novel fiber bundle model introduced. Analytical and numerical calculations showed that increasing the external load on a specimen a transition takes place from a partially failed state of infinite lifetime to a state where global failure occurs at a finite time. Two universality classes of creep rupture were identified depending on the range of interaction of fibers: in the mean field limit the transition between the two states is continuous characterized by power law divergences, while for local interactions it becomes abrupt with no scaling. Varying the range of interaction a sharp transition is revealed between the mean field and short range regimes. The creeping system evolves into a macroscopic stationary state accompanied by the emergence of a power law distribution of inter-event times of the microscopic relaxation process, which indicates self organized criticality in creep. 	
0212187v1	http://arxiv.org/pdf/cond-mat/0212187v1	2002	Risk and Utility in Portfolio Optimization	Morrel H. Cohen|Vincent D. Natoli	  Modern portfolio theory(MPT) addresses the problem of determining the optimum allocation of investment resources among a set of candidate assets. In the original mean-variance approach of Markowitz, volatility is taken as a proxy for risk, conflating uncertainty with risk. There have been many subsequent attempts to alleviate that weakness which, typically, combine utility and risk. We present here a modification of MPT based on the inclusion of separate risk and utility criteria. We define risk as the probability of failure to meet a pre-established investment goal. We define utility as the expectation of a utility function with positive and decreasing marginal value as a function of yield. The emphasis throughout is on long investment horizons for which risk-free assets do not exist. Analytic results are presented for a Gaussian probability distribution. Risk-utility relations are explored via empirical stock-price data, and an illustrative portfolio is optimized using the empirical data. 	
0301086v1	http://arxiv.org/pdf/cond-mat/0301086v1	2003	Cascade-based attacks on complex networks	Adilson E. Motter|Ying-Cheng Lai	  We live in a modern world supported by large, complex networks. Examples range from financial markets to communication and transportation systems. In many realistic situations the flow of physical quantities in the network, as characterized by the loads on nodes, is important. We show that for such networks where loads can redistribute among the nodes, intentional attacks can lead to a cascade of overload failures, which can in turn cause the entire or a substantial part of the network to collapse. This is relevant for real-world networks that possess a highly heterogeneous distribution of loads, such as the Internet and power grids. We demonstrate that the heterogeneity of these networks makes them particularly vulnerable to attacks in that a large-scale cascade may be triggered by disabling a single key node. This brings obvious concerns on the security of such systems. 	
0301576v1	http://arxiv.org/pdf/cond-mat/0301576v1	2003	Orientation Dependence of Step Stiffness: Failure of SOS and Ising   Models to Describe Experimental Data	Sabine Dieluweit|Harald Ibach|Margret Giesen|T. L. Einstein	  We have investigated the step stiffness on Cu(001) surfaces as a function of step orientation by two independent methods at several temperatures near 300 K. Both sets of data agree well and show a substantial dependence of the stiffness on the angle of orientation. With the exception of steps oriented along $<110>$, the experimental stiffness is significantly larger than the stiffness calculated within the solid-on-solid (SOS) model and the Ising-model, even if next nearest-neighbor interactions are taken into account. Our results have considerable consequences for the understanding and for the theoretical modeling of equilibrium and growth phenomena, such as step meandering instabilities. 	
0307029v1	http://arxiv.org/pdf/cond-mat/0307029v1	2003	Assessing Interaction Networks with Applications to Catastrophe Dynamics   and Disaster Management	Dirk Helbing|Christian Kuehnert	  In this paper we present a versatile method for the investigation of interaction networks and show how to use it to assess effects of indirect interactions and feedback loops. The method allows to evaluate the impact of optimization measures or failures on the system. Here, we will apply it to the investigation of catastrophes, in particular to the temporal development of disasters (catastrophe dynamics). The mathematical methods are related to the master equation, which allows the application of well-known solution methods. We will also indicate connections of disaster management with excitable media and supply networks. This facilitates to study the effects of measures taken by the emergency management or the local operation units. With a fictious, but more or less realistic example of a spreading epidemic disease or a wave of influenza, we illustrate how this method can, in principle, provide decision support to the emergency management during such a disaster. Similar considerations may help to assess measures to fight the SARS epidemics, although immunization is presently not possible. 	
0308610v1	http://arxiv.org/pdf/cond-mat/0308610v1	2003	Bond valence calculation for several perovskites and the evidences for a   valence charge transfer process in these compounds	Hoang Nam Nhat	  This paper presents the bond valence calculation for several perovskite systems and describes the evidences for a valence charge transfer process in these compounds. The reviewing of the crystal structures of La1-xPbxMnO3 (x=0.1-0.5), La0.6Sr0.4-xTixMnO3 (x=0.0-0.25) and La1-xSrxCoO3 (x=0.1-0.5) is also presented. On the basis of testing samples, the distribution of valence charge has been evaluated which showed the failure of elastic bonding mechanism on all studied systems and revealed the general deficit of valence charge in the unit cell. This deficit was not equally localized on all coordination spheres but proved asymmetrically distributed between the spheres. As the content of substitution increased, the charge deficit declined systematically from balanced level, signifying the continuous transfer of valence charge from the B-O6 to A-O12 spheres. The transfered charge varied from system to system, depending on the valence deviation of spheres and was not small. The total valence deviation reached near 2electron/unit cell in the studied systems. The local deviation may be more larger than this average value. The possible impact of the limitted accuracy of the available structural data on the bond valence results has been considered. 	
0309449v1	http://arxiv.org/pdf/cond-mat/0309449v1	2003	First passage and arrival time densities for Lévy flights and the   failure of the method of images	Aleksei V. Chechkin|Ralf Metzler|Vsevolod Y. Gonchar|Joseph Klafter|Leonid V. Tanatarov	  We discuss the first passage time problem in the semi-infinite interval, for homogeneous stochastic Markov processes with L{\'e}vy stable jump length distributions $\lambda(x)\sim\ell^{\alpha}/|x|^{1+\alpha}$ ($|x|\gg\ell$), namely, L{\'e}vy flights (LFs). In particular, we demonstrate that the method of images leads to a result, which violates a theorem due to Sparre Andersen, according to which an arbitrary continuous and symmetric jump length distribution produces a first passage time density (FPTD) governed by the universal long-time decay $\sim t^{-3/2}$. Conversely, we show that for LFs the direct definition known from Gaussian processes in fact defines the probability density of first arrival, which for LFs differs from the FPTD. Our findings are corroborated by numerical results. 	
0311046v1	http://arxiv.org/pdf/cond-mat/0311046v1	2003	Generic features of the fluctuation dissipation relation in coarsening   systems	Federico Corberi|Claudio Castellano|Eugenio Lippiello|Marco Zannetti	  The integrated response function in phase-ordering systems with scalar, vector, conserved and non conserved order parameter is studied at various space dimensionalities. Assuming scaling of the aging contribution $\chi_{ag} (t,t_w)= t_w ^{-a_\chi} \hat \chi (t/t_w)$ we obtain, by numerical simulations and analytical arguments, the phenomenological formula describing the dimensionality dependence of $a_\chi$ in all cases considered. The primary result is that $a_\chi$ vanishes continuously as $d$ approaches the lower critical dimensionality $d_L$. This implies that i) the existence of a non trivial fluctuation dissipation relation and ii) the failure of the connection between statics and dynamics are generic features of phase ordering at $d_L$. 	
0311284v2	http://arxiv.org/pdf/cond-mat/0311284v2	2004	Percolation and localization in the random fuse model	Phani Kumar V. V. Nukala|Srdan Simunovic|Stefano Zapperi	  We analyze damage nucleation and localization in the random fuse model with strong disorder using numerical simulations. In the initial stages of the fracture process, damage evolves in an uncorrelated manner, resembling percolation. Subsequently, as the damage starts to accumulate, current enhancement at the tips of the microcracks leads eventually to catastrophic failure. We study this behavior quantifying the deviations from percolation and discussing alternative scaling laws for damage. The analysis of damage profiles confirms that localization occurs abruptly starting from an uniform damage landscape. Finally, we show that the cumulative damage distribution follows the normal distribution, suggesting that damage is uncorrelated on large length scales. 	
0401017v1	http://arxiv.org/pdf/cond-mat/0401017v1	2004	A Predator Prey Approach to Diversity Based Defenses in Heterogeneous   Networks	Sean P. Gorman|Rajendra G. Kulkarni|Laurie A. Schintler|Roger R. Stough	  In light of the rise of malicious attacks on the Internet, and the various networks and applications attached to it, new approaches towards modeling predatory activity in networks could be useful. Past research has simulated networks assuming that all vertices are homogenously susceptible to attack or infection. Often times in real world networks only subsets of vertices are susceptible to attack or infection in a heterogeneous population of vertices. One approach to examining a heterogeneous network susceptible to attack is modeling a network as a predator prey landscape. If each type of vulnerable device is considered a heterogeneous species what level of species diversification is needed to keep a malicious attack from a causing a catastrophic failure to the entire network. This paper explores the predator prey analogy for the Internet and presents findings on how different levels of species diversification effects network resilience. The paper will also discuss the connection between diversification, competition, anti-trust, and national security. 	
0401592v1	http://arxiv.org/pdf/cond-mat/0401592v1	2004	Damage Growth in Random Fuse Networks	F. Reurings|M. J. Alava	  The correlations among elements that break in random fuse network fracture are studied, for disorder strong enough to allow for volume damage before final failure. The growth of microfractures is found to be uncorrelated above a lengthscale, that increases as the the final breakdown is approached. Since the fuse network strength decreases with sample size, asymptotically the process resembles more and more mean-field-like (``democratic fiber bundle'') fracture. This is found from the microscopic dynamics of avalanches or microfractures, from a study of damage localization via entropy, and from the final damage profile. In particular, the last one is statistically constant, except exactly at the final crack zone (in contrast to recent results by Hansen et al., Phys. Rev. Lett. 90, 045504 (2003)), in spite of the fact that the fracture surfaces are self-affine. 	
0404226v1	http://arxiv.org/pdf/cond-mat/0404226v1	2004	Network-Induced Oscillatory Behavior in Material Flow Networks	Dirk Helbing|Ulrich Witt|Stefan Laemmer|Thomas Brenner	  Network theory is rapidly changing our understanding of complex systems, but the relevance of topological features for the dynamic behavior of metabolic networks, food webs, production systems, information networks, or cascade failures of power grids remains to be explored. Based on a simple model of supply networks, we offer an interpretation of instabilities and oscillations observed in biological, ecological, economic, and engineering systems. We find that most supply networks display damped oscillations, even when their units - and linear chains of these units - behave in a non-oscillatory way. Moreover, networks of damped oscillators tend to produce growing oscillations. This surprising behavior offers, for example, a new interpretation of business cycles and of oscillating or pulsating processes. The network structure of material flows itself turns out to be a source of instability, and cyclical variations are an inherent feature of decentralized adjustments. 	
0406567v1	http://arxiv.org/pdf/cond-mat/0406567v1	2004	Optimization of Network Robustness to Waves of Targeted and Random   Attack	T. Tanizawa|G. Paul|R. Cohen|S. Havlin|H. E. Stanley	  We study the robustness of complex networks to multiple waves of simultaneous (i) targeted attacks in which the highest degree nodes are removed and (ii) random attacks (or failures) in which fractions $p_t$ and $p_r$ respectively of the nodes are removed until the network collapses. We find that the network design which optimizes network robustness has a bimodal degree distribution, with a fraction $r$ of the nodes having degree $k_2= (\kav - 1 +r)/r$ and the remainder of the nodes having degree $k_1=1$, where $\kav$ is the average degree of all the nodes. We find that the optimal value of $r$ is of the order of $p_t/p_r$ for $p_t/p_r\ll 1$. 	
0407568v1	http://arxiv.org/pdf/cond-mat/0407568v1	2004	Crack roughness and avalanche precursors in the random fuse model	Stefano Zapperi|Phani Kumar V. V. Nukala|Srdan Simunovic	  We analyze the scaling of the crack roughness and of avalanche precursors in the two dimensional random fuse model by numerical simulations, employing large system sizes and extensive sample averaging. We find that the crack roughness exhibits anomalous scaling, as recently observed in experiments. The roughness exponents ($\zeta$, $\zeta_{loc}$) and the global width distributions are found to be universal with respect to the lattice geometry. Failure is preceded by avalanche precursors whose distribution follows a power law up to a cutoff size. While the characteristic avalanche size scales as $s_0 \sim L^D$, with a universal fractal dimension $D$, the distribution exponent $\tau$ differs slightly for triangular and diamond lattices and, in both cases, it is larger than the mean-field (fiber bundle) value $\tau=5/2$. 	
0408064v2	http://arxiv.org/pdf/cond-mat/0408064v2	2004	Pair Contact Process with Diffusion: Failure of Master Equation Field   Theory	Hans-Karl Janssen|Frederic van Wijland|Olivier Deloubriere|Uwe C. Tauber	  We demonstrate that the `microscopic' field theory representation, directly derived from the corresponding master equation, fails to adequately capture the continuous nonequilibrium phase transition of the Pair Contact Process with Diffusion (PCPD). The ensuing renormalization group (RG) flow equations do not allow for a stable fixed point in the parameter region that is accessible by the physical initial conditions. There exists a stable RG fixed point outside this regime, but the resulting scaling exponents, in conjunction with the predicted particle anticorrelations at the critical point, would be in contradiction with the positivity of the equal-time mean-square particle number fluctuations. We conclude that a more coarse-grained effective field theory approach is required to elucidate the critical properties of the PCPD. 	
0410684v2	http://arxiv.org/pdf/cond-mat/0410684v2	2006	Robustness of the avalanche dynamics in data packet transport on   scale-free networks	E. J. Lee|K. -I. Goh|B. Kahng|D. Kim	  We study the avalanche dynamics in the data packet transport on scale-free networks through a simple model. In the model, each vertex is assigned a capacity proportional to the load with a proportionality constant $1+a$. When the system is perturbed by a single vertex removal, the load of each vertex is redistributed, followed by subsequent failures of overloaded vertices. The avalanche size depends on the parameter $a$ as well as which vertex triggers it. We find that there exists a critical value $a_c$ at which the avalanche size distribution follows a power law. The critical exponent associated with it appears to be robust as long as the degree exponent is between 2 and 3, and is close in value to that of the distribution of the diameter changes by single vertex removal. 	
0412038v2	http://arxiv.org/pdf/cond-mat/0412038v2	2007	Stress relaxation in a perfect nanocrystal by coherent ejection of   lattice layers	Abhishek Chaudhuri|Surajit Sengupta|Madan Rao	  We show that a small crystal trapped within a potential well and in contact with its own fluid, responds to large compressive stresses by a novel mechanism -- the transfer of complete lattice layers across the solid-fluid interface. Further, when the solid is impacted by a momentum impulse set up in the fluid, a coherently ejected lattice layer carries away a definite quantity of energy and momentum, resulting in a sharp peak in the calculated phonon absorption spectrum. Apart from its relevance to studies of stability and failure of small sized solids, such coherent nanospallation may be used to make atomic wires or monolayer films. 	
0412395v1	http://arxiv.org/pdf/cond-mat/0412395v1	2004	The failure of the master equation for the reactive systems	Maria K. Koleva	  Two crucial for the breakdown of the master equation arguments are put forward. The first one is related to the violence of a fundamental requirement to the notion of state (thermodynamical) variable, namely: a state variable is defined provided it is insensitive to the particularities of the spatio-temporal configurations upon which the averaging over the dynamical variables proceeds. The second one is related to a ubiquitous divergence of the scattering length in the low-energy limit. In turn, it makes the rates of all the elementary processes divergent as well. Though radically novel viewpoints to the low-energy limit and to the evolution ensure the boundedness of the rates and hold the notion of a state variable available, the master equation remains inappropriate. 	
0501420v2	http://arxiv.org/pdf/cond-mat/0501420v2	2005	Spatial small-world networks: A wiring-cost perspective	Thomas Petermann|Paolo De Los Rios	  Supplementing a lattice with long-range connections effectively models small-world networks characterized by a high local and global interconnectedness observed in systems ranging from society to the brain. If the links have a wiring cost associated to their length l, the corresponding distribution q(l) plays a crucial role. Uniform length distributions have received most attention despite indications that q(l) ~ l^{-\alpha} exist, e.g. for integrated circuits, the Internet and cortical networks. Here we discuss for such systems the emergence of small-world topology, its relationship to the wiring costs, the distribution of flows as well as the robustness with respect to random failures and overload. The main finding is that the choice of such a distribution leads to favorable attributes in most of the investigated properties. 	
0501737v1	http://arxiv.org/pdf/cond-mat/0501737v1	2005	Langevin Simulation of the Chirally Decomposed Sine-Gordon Model	L. Moriconi|M. Moriconi	  A large class of quantum and statistical field theoretical models, encompassing relevant condensed matter and non-abelian gauge systems, are defined in terms of complex actions. As the ordinary Monte-Carlo methods are useless in dealing with these models, alternative computational strategies have been proposed along the years. The Langevin technique, in particular, is known to be frequently plagued with difficulties such as strong numerical instabilities or subtle ergodic behavior. Regarding the chirally decomposed version of the sine-Gordon model as a prototypical case for the failure of the Langevin approach, we devise a truncation prescription in the stochastic differential equations which yields numerical stability and is assumed not to spoil the Berezinskii-Kosterlitz-Thouless transition. This conjecture is supported by a finite size scaling analysis, whereby a massive phase ending at a line of critical points is clearly observed for the truncated stochastic model. 	
0504212v1	http://arxiv.org/pdf/cond-mat/0504212v1	2005	Possibility of Prediction of Avalanches in Power Law Systems	Rumi De|G. Ananthakrishna	  We consider a modified Burridge-Knopoff model with a view to understand results of acoustic emission (AE) relevant to earthquakes by adding a dissipative term which mimics bursts of acoustic signals. Interestingly, we find a precursor effect in the cumulative energy dissipated which allows identification of a large slip event. Further, the AE activity for several large slip events follows a universal stretched exponential behavior with corrections in terms of time-to-failure. We find that many features of the statistics of AE signals such as their amplitudes, durations and the intervals between successive AE bursts obey power laws consistent with recent experimental results. Large magnitude events have different power law from that of the small ones, the latter being sensitive to the pulling speed. 	
0601310v2	http://arxiv.org/pdf/cond-mat/0601310v2	2006	Heat conduction in a confined solid strip: Response to external strain	Debasish Chaudhuri|Abhishek Dhar	  We study heat conduction in a system of hard disks confined to a narrow two dimensional channel. The system is initially in a high density solid-like phase. We study, through nonequilibrium molecular dynamics simulations, the dependence of the heat current on an externally applied elongational strain. The strain leads to deformation and failure of the solid and we find that the changes in internal structure can lead to very sharp changes in the heat current. A simple free-volume type calculation of the heat current in a finite hard-disk system is proposed. This reproduces some qualitative features of the current-strain graph for small strains. 	
0604473v3	http://arxiv.org/pdf/cond-mat/0604473v3	2007	Critical packing in granular shear bands	S. Fazekas|J. Török|J. Kertész	  In a realistic three-dimensional setup, we simulate the slow deformation of idealized granular media composed of spheres undergoing an axisymmetric triaxial shear test. We follow the self-organization of the spontaneous strain localization process leading to a shear band and demonstrate the existence of a critical packing density inside this failure zone. The asymptotic criticality arising from the dynamic equilibrium of dilation and compaction is found to be restricted to the shear band, while the density outside of it keeps the memory of the initial packing. The critical density of the shear band depends on friction (and grain geometry) and in the limit of infinite friction it defines a specific packing state, namely the \emph{dynamic random loose packing}. 	
0611313v2	http://arxiv.org/pdf/cond-mat/0611313v2	2007	Statistical Neurodynamics for sequence processing neural networks with   finite dilution	Pan Zhang|Yong Chen	  We extend the statistical neurodynamics to study transient dynamics of sequence processing neural networks with finite dilution, and the theoretical results is supported by the extensive numerical simulations. It is found that the order parameter equations are completely equivalent to those of the Generating Functional Method, which means that crosstalk noise is normal distribution even in the case of failure in retrieval process. In order to verify the gaussian assumption of crosstalk noise, we numerically obtain the cumulants of crosstalk noise, and third- and fourth-order cumulants are found to be indeed zero even in non-retrieval case. 	
0701707v1	http://arxiv.org/pdf/cond-mat/0701707v1	2007	Relaxation dynamics in strained fiber bundles	Srutarshi Pradhan|Per C. Hemmer	  Under an applied external load the global load-sharing fiber bundle model, with individual fiber strength thresholds sampled randomly from a probability distribution, will relax to an equilibrium state, or to complete bundle breakdown. The relaxation can be viewed as taking place in a sequence of steps. In the first step all fibers weaker than the applied stress fail. As the total load is redistributed on the surviving fibers, a group of secondary fiber failures occur, etc. For a bundle with a finite number of fibers the process stops after a finite number of steps, $t$. By simulation and theoretical estimates, it is determined how $t$ depends upon the stress, the initial load per fiber, both for subcritical and supercritical stress. The two-sided critical divergence is characterized by an exponent -1/2, independent of the probability distribution of the fiber thresholds. 	
0703420v2	http://arxiv.org/pdf/cond-mat/0703420v2	2007	Use and Abuse of a Fractional Fokker-Planck Dynamics for Time-Dependent   Driving	E. Heinsalu|M. Patriarca|I. Goychuk|P. Hänggi	  We investigate a subdiffusive, fractional Fokker-Planck dynamics occurring in time-varying potential landscapes and thereby disclose the failure of the fractional Fokker-Planck equation (FFPE) in its commonly used form when generalized in an {\it ad hoc} manner to time-dependent forces. A modified FFPE (MFFPE) is rigorously derived, being valid for a family of dichotomously alternating force-fields. This MFFPE is numerically validated for a rectangular time-dependent force with zero average bias. For this case subdiffusion is shown to become enhanced as compared to the force free case. We question, however, the existence of any physically valid FFPE for arbitrary varying time-dependent fields that differ from this dichotomous varying family. 	
9604102v1	http://arxiv.org/pdf/cs/9604102v1	1996	Practical Methods for Proving Termination of General Logic Programs	E. Marchiori	  Termination of logic programs with negated body atoms (here called general logic programs) is an important topic. One reason is that many computational mechanisms used to process negated atoms, like Clark's negation as failure and Chan's constructive negation, are based on termination conditions. This paper introduces a methodology for proving termination of general logic programs w.r.t. the Prolog selection rule. The idea is to distinguish parts of the program depending on whether or not their termination depends on the selection rule. To this end, the notions of low-, weakly up-, and up-acceptable program are introduced. We use these notions to develop a methodology for proving termination of general logic programs, and show how interesting problems in non-monotonic reasoning can be formalized and implemented by means of terminating general logic programs. 	
0403042v2	http://arxiv.org/pdf/cs/0403042v2	2004	Protecting Public-Access Sites Against Distributed Denial-of-Service   Attacks	Katerina J. Argyraki|David R. Cheriton	  A distributed denial-of-service (DDoS) attack can flood a victim site with malicious traffic, causing service disruption or even complete failure. Public-access sites like amazon or ebay are particularly vulnerable to such attacks, because they have no way of a priori blocking unauthorized traffic.   We present Active Internet Traffic Filtering (AITF), a mechanism that protects public-access sites from highly distributed attacks by causing undesired traffic to be blocked as close as possible to its sources. We identify filters as a scarce resource and show that AITF protects a significant amount of the victim's bandwidth, while requiring from each participating router a number of filters that can be accommodated by today's routers. AITF is incrementally deployable, because it offers a substantial benefit even to the first sites that deploy it. 	
9703019v1	http://arxiv.org/pdf/dg-ga/9703019v1	1997	Can We Look at The Quantisation Rules as Constraints?	Ennio Gozzi	  In this paper we explore the idea of looking at the Dirac quantisation conditions as $\hbar$-dependent constraints on the tangent bundle to phase-space. Starting from the path-integral version of classical mechanics and using the natural Poisson brackets structure present in the cotangent bundle to the tangent bundle of phase- space, we handle the above constraints using the standard theory of Dirac for constrained systems. The hope is to obtain, as total Hamiltonian, the Moyal operator of time-evolution and as Dirac brackets the Moyal ones. Unfortunately the program fails indicating that something is missing. We put forward at the end some ideas for future work which may overcome this failure. 	
9801003v3	http://arxiv.org/pdf/gr-qc/9801003v3	1998	Nonholonomic Mapping Principle for Classical Mechanics in Spaces with   Curvature and Torsion. New Covariant Conservation Law for Energy-Momentum   Tensor	Hagen Kleinert	  The lecture explains the geometric basis for the recently-discovered nonholonomic mapping principle which specifies certain laws of nature in spacetimes with curvature and torsion from those in flat spacetime, thus replacing and extending Einstein's equivalence principle. An important consequence is a new action principle for determining the equation of motion of a free spinless point particle in such spacetimes. Surprisingly, this equation contains a torsion force, although the action involves only the metric. This force changes geodesic into autoparallel trajectories, which are a direct manifestation of inertia. The geometric origin of the torsion force is a closure failure of parallelograms. The torsion force changes the covariant conservation law of the energy-momentum tensor whose new form is derived. 	
0203029v1	http://arxiv.org/pdf/gr-qc/0203029v1	2002	Nonholonomic Mapping Principle for Classical and Quantum Mechanics in   Spaces with Curvature and Torsion	Hagen Kleinert	  I explain the geometric basis for the recently-discovered nonholonomic mapping principle which permits deriving laws of nature in spacetimes with curvature and torsion from those in flat spacetime, thus replacing and extending Einstein's equivalence principle. As an important consequence, it yields a new action principle for determining the equation of motion of a free spinless point particle in such spacetimes. Surprisingly, this equation contains a torsion force, although the action involves only the metric. This force makes trajectories autoparallel rather than geodesic. Its geometric origin is the closure failure of parallelograms in the presence of torsion, A simple generalization of the mapping principle transforms path integrals from flat spacetimes to those with curvature and torsion, thus playing the role of a quantum equivalence principle, applicable at present only to spaces with gradient torsion. 	
0410036v1	http://arxiv.org/pdf/hep-lat/0410036v1	2004	Failure of Mean Field Theory at Large N	Shailesh Chandrasekharan|Costas G. Strouthos	  We study strongly coupled lattice QCD with $N$ colors of staggered fermions in 3+1 dimensions. While mean field theory describes the low temperature behavior of this theory at large $N$, it fails in the scaling region close to the finite temperature second order chiral phase transition. The universal critical region close to the phase transition belongs to the 3d XY universality class even when $N$ becomes large. This is in contrast to Gross-Neveu models where the critical region shrinks as $N$ (the number of flavors) increases and mean field theory is expected to describe the phase transition exactly in the limit of infinite $N$. Our work demonstrates that close to second order phase transitions infrared fluctuations can sometimes be important even when $N$ is strictly infinite. 	
9607230v1	http://arxiv.org/pdf/hep-ph/9607230v1	1996	End-Point Behavior of Exclusive Processes: The Twilight Regime of   Perturbative QCD	N. G. Stefanis	  A selected set of topics along the borderline between perturbative and nonperturbative QCD in exclusive reactions are studied. Specific problems, related to different mechanisms of momentum transfer to an intact hadron, are discussed. Calculations of the space-like form factors of the pion and the nucleon are reviewed within a convolution scheme of short-distance (hard) and large-distance (soft) contributions which takes into account soft gluon emission and the intrinsic transverse hadron size. The failure of this scheme to reproduce the existing experimental data signals sizeable higher-order perturbative corrections (a K-factor of order two) and/or higher-twist contributions. 	
9702385v1	http://arxiv.org/pdf/hep-ph/9702385v1	1997	Charmonium Production at ELFE Energies	Paul Hoyer	  I discuss issues related to charmonium production, in view of physics possibilities at a 15 ... 30 GeV continuous beam electron facility. High energy photo- and hadroproduction of heavy quarkonia presents several challenges to QCD models concerning cross sections, polarization and nuclear target dependence. Theoretical approaches based on color evaporation as well as on color singlet and color octet mechanisms have met with both successes and failures, indicating that charmonium production is a sensitive probe of color dynamics. Experiments close to charm kinematic threshold will be sensitive also to target substructure since only unusual, compact target configurations contribute. In particular, subthreshold production on nuclei should identify nuclear hot spots of high energy density. At low energies, charmonium will form inside the target nucleus, allowing a determination of c cbar bound state interactions in nuclear matter. 	
9708314v1	http://arxiv.org/pdf/hep-ph/9708314v1	1997	Massiveness of Glueballs as Origin of the OZI Rule	Wei-Shu Hou|Cheng-Ying Ko	  The heaviness of the glueball mass scale is suggested as the source of the OZI rule at low energy. The $J/\psi \to \rho\pi$ decay "anomaly" implies the vector glueball $O$ has mass $m_O \approx m_{J/\psi}$. Such a heavy mass is supported by other glueball studies. Glueball-meson matrix elements turn out to be not suppressed at all at the 1 GeV scale, and a simple and intuitive picture emerges which is consistent with the Gell-Mann-Okubo mass formula as well as the measured sign of $\phi$-$\omega$ mixing. The suppression of glueball mediated $\bar q_iq_i \longleftrightarrow \bar q_jq_j$ transitions and the cancellation mechanism in two-step meson rescatterings are viewed as related by duality. Extensions to the $2^{++}$, $3^{--}$ meson sectors, and failure for $0^{\pm +}$ mesons are also discussed. 	
9806424v2	http://arxiv.org/pdf/hep-ph/9806424v2	1998	Quarkonium Production through Hard Comover Scattering	Paul Hoyer|Stephane Peigne	  We propose a qualitatively new mechanism for quarkonium production, motivated by the global features of the experimental data and by the successes/failures of existing models. In QCD, heavy quarks are created in conjunction with a bremsstrahlung color field emitted by the colliding partons. We study the effects of perturbative gluon exchange between the quark pair and a comoving color field. Such scattering can flip the spin and color of the quarks to create a non-vanishing overlap with the wave function of physical quarkonium. Several observed features that are difficult to understand in current models find simple explanations. Transverse gluon exchange produces unpolarized J/psi's, the chi_c1 and chi_c2 states are produced at similar rates, and the anomalous dependence of the J/psi cross section on the nuclear target size can be qualitatively understood. 	
9908382v2	http://arxiv.org/pdf/hep-ph/9908382v2	1999	Probing Supersymmetric Flavor Models with $ε'/ε$	G. Eyal|A. Masiero|Y. Nir|L. Silvestrini	  We discuss the supersymmetric contribution to $\epsilon'/\epsilon$ in various supersymmetric flavor models. We find that in alignment models the supersymmetric contribution could be significant while in heavy squark models it is expected to be small. The situation is particularly interesting in models that solve the flavor problems by either of the above mechanisms and the remaining CP problems by means of approximate CP, that is, all CP violating phases are small. In such models, the standard model contributions cannot account for $\epsilon'/\epsilon$ and a failure of the supersymmetric contributions to do so would exclude the model. In models of alignment and approximate CP, the supersymmetric contributions can account for $\epsilon'/\epsilon$ only if both the supersymmetric model parameters and the hadronic parameters assume rather extreme values. Such models are then strongly disfavored by the $\epsilon'/\epsilon$ measurements. Models of heavy squarks and approximate CP are excluded. 	
9909332v1	http://arxiv.org/pdf/hep-ph/9909332v1	1999	Coherence of neutrino flavor mixing in quantum field theory	Christian Y. Cardall	  In the simplistic quantum mechanical picture of flavor mixing, conditions on the maximum size and minimum coherence time of the source and detector regions for the observation of interference---as well as the very viability of the approach---can only be argued in an ad hoc way from principles external to the formalism itself. To examine these conditions in a more fundamental way, the quantum field theoretical $S$-matrix approach is employed in this paper, without the unrealistic assumption of microscopic stationarity. The fully normalized, time-dependent neutrino flavor mixing event rates presented here automatically reveal the coherence conditions in a natural, self-contained, and physically unambiguous way, while quantitatively describing the transition to their failure. 	
0004227v4	http://arxiv.org/pdf/hep-ph/0004227v4	2003	Neutrino Oscillations v.s. Leptogenesis in SO(10) Models	Emmanuel Nezri|Jean Orloff	  We study the link between neutrino oscillations and leptogenesis in the minimal framework assuming an SO(10) see-saw mechanism with 3 families. Dirac neutrino masses being fixed, the solar and atmospheric data then generically induce a large mass-hierarchy and a small mixing between the lightest right-handed neutrinos, which fails to produce sufficient lepton asymmetry by 5 orders of magnitudes at least. This failure can be attenuated for a very specific value of the mixing sin^2(2\theta_{e3})=0.1, which interestingly lies at the boundary of the CHOOZ exclusion region, but will be accessible to future long baseline experiments. 	
0012336v1	http://arxiv.org/pdf/hep-ph/0012336v1	2000	A Few Aspects of Heavy Quark Expansion	Nikolai Uraltsev	  Two topics in heavy quark expansion are discussed. The heavy quark potential in perturbation theory is reviewed in connection to the problem of the heavy quark mass. The nontrivial reason behind the failure of the "potential subtracted" mass in higher orders is elucidated. The heavy quark sum rules are the second subject. The physics behind the new exact sum rules is described and a simple quantum mechanical derivation is given. The question of saturation of sum rules is discussed. A comment on the nonstandard possibility which would affect analysis of BR_sl(B) vs. n_c is made. 	
0505222v1	http://arxiv.org/pdf/hep-ph/0505222v1	2005	Split Fermions Baryogenesis from the Kobayashi-Maskawa Phase	Gilad Perez|Tomer Volansky	  A new scenario of baryogenesis is presented, within the split fermions framework. Our model employs a first order phase transition of the localizer field. The standard model (SM), Kobayashi-Maskawa phase induces a sizable CP asymmetry. The usual suppression of CP violation which arises in the SM baryogenesis is absent due to the existence of order one Yukawa couplings before the fermions are localized in the extra dimension. Models of the above type naturally contain B-L violating operators, allowed by the SM symmetries, which induce the baryon asymmetry. Our mechanism demonstrates the following concept: the flavor puzzle and the SM failure to create the baryon asymmetry are linked and may have a common resolution which does not rely on introduction of new CP violating sources. 	
9808151v1	http://arxiv.org/pdf/hep-th/9808151v1	1998	BRST Inner Product Spaces and the Gribov Obstruction	Norbert Duechting|Sergei V. Shabanov|Thomas Strobl	  A global extension of the Batalin-Marnelius proposal for a BRST inner product to gauge theories with topologically nontrivial gauge orbits is discussed. It is shown that their (appropriately adapted) method is applicable to a large class of mechanical models with a semisimple gauge group in the adjoint and fundamental representation. This includes cases where the Faddeev-Popov method fails. Simple models are found also, however, which do not allow for a well-defined global extension of the Batalin-Marnelius inner product due to a Gribov obstruction. Reasons for the partial success and failure are worked out and possible ways to circumvent the problem are briefly discussed. 	
0103197v2	http://arxiv.org/pdf/hep-th/0103197v2	2001	Modular transformation and boundary states in logarithmic conformal   field theory	Shinsuke Kawai|John F. Wheater	  We study the $c=-2$ model of logarithmic conformal field theory in the presence of a boundary using symplectic fermions. We find boundary states with consistent modular properties. A peculiar feature of this model is that the vacuum representation corresponding to the identity operator is a sub-representation of a ``reducible but indecomposable'' larger representation. This leads to unusual properties, such as the failure of the Verlinde formula. Despite such complexities in the structure of modules, our results suggest that logarithmic conformal field theories admit bona fide boundary states. 	
0302197v2	http://arxiv.org/pdf/hep-th/0302197v2	2003	SO(2,1) conformal anomaly: Beyond contact interactions	Gino N. J. Ananos|Horacio E. Camblong|Carlos R. Ordonez	  The existence of anomalous symmetry-breaking solutions of the SO(2,1) commutator algebra is explicitly extended beyond the case of scale-invariant contact interactions. In particular, the failure of the conservation laws of the dilation and special conformal charges is displayed for the two-dimensional inverse square potential. As a consequence, this anomaly appears to be a generic feature of conformal quantum mechanics and not merely an artifact of contact interactions. Moreover, a renormalization procedure traces the emergence of this conformal anomaly to the ultraviolet sector of the theory, within which lies the apparent singularity. 	
0303166v2	http://arxiv.org/pdf/hep-th/0303166v2	2003	Anomaly in conformal quantum mechanics: From molecular physics to black   holes	Horacio E. Camblong|Carlos R. Ordonez	  A number of physical systems exhibit a particular form of asymptotic conformal invariance: within a particular range of distances, they are characterized by a long-range conformal interaction (inverse square potential), the absence of dimensional scales, and an SO(2,1) symmetry algebra. Examples from molecular physics to black holes are provided and discussed within a unified treatment. When such systems are physically realized in the appropriate strong-coupling regime,the occurrence of quantum symmetry breaking is possible. This anomaly is revealed by the failure of the symmetry generators to close the algebra in a manner shown to be independent of the renormalization procedure. 	
0512023v1	http://arxiv.org/pdf/hep-th/0512023v1	2005	Infrared properties of boundaries in 1-d quantum systems	Daniel Friedan|Anatoly Konechny	  We present some partial results on the general infrared behavior of bulk-critical 1-d quantum systems with boundary. We investigate whether the boundary entropy, s(T), is always bounded below as the temperature T decreases towards 0, and whether the boundary always becomes critical in the IR limit. We show that failure of these properties is equivalent to certain seemingly pathological behaviors far from the boundary. One of our approaches uses real time methods, in which locality at the boundary is expressed by analyticity in the frequency. As a preliminary, we use real time methods to prove again that the boundary beta-function is the gradient of the boundary entropy, which implies that s(T) decreases with T. The metric on the space of boundary couplings is interpreted as the renormalized susceptibility matrix of the boundary, made finite by a natural subtraction. 	
0311225v1	http://arxiv.org/pdf/math/0311225v1	2003	Compactness in the d-bar Neumann problem, magnetic Schrodinger   operators, and the Aharonov-Bohm effect	Michael Christ|Siqi Fu	  Compactness of the d-bar Neumann operator is studied for weakly pseudoconvex bounded Hartogs domains in two dimensions. A nonsmooth example is constructed in which condition (P) fails to hold, yet the Neumann operator is compact. The main result, in contrast, is that for smoothly bounded Hartogs domains, condition (P) of Catlin and Sibony is equivalent to compactness.   The analyses of both compactness and condition (P) boil down to properties of the lowest eigenvalues of certain sequences of Schrodinger operators, with and without magnetic fields, parametrized by a Fourier variable resulting from the Hartogs symmetry. The nonsmooth counterexample is based on the Aharonov-Bohm phenomenon of quantum mechanics. For smooth domains, we prove that there always exists an exceptional sequence of Fourier variables for which the Aharonov-Bohm effect is quite weak. This sequence can be quite sparse, so that the failure of compactness is due to a rather subtle effect. 	
0011006v1	http://arxiv.org/pdf/nlin/0011006v1	2000	Spatiotemporal Patterns in Arrays of Coupled Nonlinear Oscillators	M. Lakshmanan|P. Muruganandam	  Nonlinear reaction-diffusion systems admit a wide variety of spatiotemporal patterns or structures. In this lecture, we point out that there is certain advantage in studying discrete arrays, namely cellular neural/nonlinear networks (CNNs), over continuous systems. Then, to illustrate these ideas, the dynamics of diffusively coupled one and two dimensional cellular nonlinear networks (CNNs), involving Murali-Lakshmanan-Chua circuit as the basic element, is considered. Propagation failure in the case of uniform diffusion and propagation blocking in the case of defects are pointed out. The mechanism behind these phenomena in terms of loss of stability is explained. Various spatiotemporal patterns arising from diffusion driven instability such as hexagons, rhombous and rolls are considered when external forces are absent. Existence of penta-hepta defects and removal of them due to external forcing is discussed. The transition from hexagonal to roll structure and breathing oscillations in the presence of external forcing is also demonstrated. Further spatiotemporal chaos, synchronization and size instability in the coupled chaotic systems are elucidated. 	
0512248v2	http://arxiv.org/pdf/physics/0512248v2	2006	A simple mechanism for controlling vortex breakdown in a closed flow	C. Cabeza|Gustavo Sarasua|Arturo C. Marti|Italo Bove	  This work is focused to study the development and control of the laminar vortex breakdown of a flow enclosed in a cylinder. We show that vortex breakdown can be controlled by the introduction of a small fixed rod in the axis of the cylinder. Our method to control the onset of vortex breakdown is simpler than those previously proposed, since it does not require any auxiliary device system. The effect of the fixed rods may be understood using a simple model based on the failure of the quasi-cylindrical approximation. We report experimental results of the critical Reynolds number for the appearance of vortex breakdown for different radius of the fixed rods and different aspect ratios of the system. Good agreement is found between the theoretical and experimental results. 	
0701290v1	http://arxiv.org/pdf/physics/0701290v1	2007	The rich-club phenomenon across complex network hierarchies	Julian J. McAuley|Luciano da Fontoura Costa|Tiberio S. Caetano	  The so-called rich-club phenomenon in a complex network is characterized when nodes of higher degree (hubs) are better connected among themselves than are nodes with smaller degree. The presence of the rich-club phenomenon may be an indicator of several interesting high-level network properties, such as tolerance to hub failures. Here we investigate the existence of the rich-club phenomenon across the hierarchical degrees of a number of real-world networks. Our simulations reveal that the phenomenon may appear in some hierarchies but not in others and, moreover, that it may appear and disappear as we move across hierarchies. This reveals the interesting possibility of non-monotonic behavior of the phenomenon; the possible implications of our findings are discussed. 	
0401016v1	http://arxiv.org/pdf/q-bio/0401016v1	2004	Extinction times for birth-death processes: exact results, continuum   asymptotics, and the failure of the Fokker-Planck approximation	Charles R. Doering|Khachik V. Sargsyan|Leonard M. Sander	  We consider extinction times for a class of birth-death processes commonly found in applications, where there is a control parameter which determines whether the population quickly becomes extinct, or rather persists for a long time. We give an exact expression for the discrete case and its asymptotic expansion for large values of the population. We have results below the threshold, at the threshold, and above the threshold (where there is a quasi-stationary state and the extinction time is very long.) We show that the Fokker-Planck approximation is valid only quite near the threshold. We compare our analytical results to numerical simulations for the SIS epidemic model, which is in the class that we treat. This is an interesting example of the delicate relationship between discrete and continuum treatments of the same problem. 	
0403015v1	http://arxiv.org/pdf/q-bio/0403015v1	2004	Edge vulnerability in neural and metabolic networks	Marcus Kaiser|Claus C. Hilgetag	  Biological networks, such as cellular metabolic pathways or networks of corticocortical connections in the brain, are intricately organized, yet remarkably robust toward structural damage. Whereas many studies have investigated specific aspects of robustness, such as molecular mechanisms of repair, this article focuses more generally on how local structural features in networks may give rise to their global stability. In many networks the failure of single connections may be more likely than the extinction of entire nodes, yet no analysis of edge importance (edge vulnerability) has been provided so far for biological networks. We tested several measures for identifying vulnerable edges and compared their prediction performance in biological and artificial networks. Among the tested measures, edge frequency in all shortest paths of a network yielded a particularly high correlation with vulnerability, and identified inter-cluster connections in biological but not in random and scale-free benchmark networks. We discuss different local and global network patterns and the edge vulnerability resulting from them. 	
0503011v1	http://arxiv.org/pdf/q-bio/0503011v1	2005	On the Statistical Law of Life	N. M. Pugno	  In this paper we derive a statistical law of Life. It governs the probability of death, or complementary of survival, of the living organisms. We have deduced such a law coupling the widely used Weibull statistics, developed for describing the distribution of the strength of solids, with the universal model for ontogenetic growth only recently proposed by West and co-authors. The main idea presented in this paper is that cracks can propagate in solids and cause their failure as sick cells in living organisms can cause their death. Making a rough analogy, living organisms are found to behave as growing mechanical components under cyclic, i.e., fatigue, loadings and composed by a dynamic evolutionary material that, as an ineluctable fate, deteriorates. The implications on biological scaling laws are discussed. As an example of application, we apply such a statistical law to large data collections on human deaths due to cancer of various types recorded in Italy: a relevant agreement is observed. 	
0601094v1	http://arxiv.org/pdf/quant-ph/0601094v1	2006	Casimir effect for curved geometries: PFA validity limits	Holger Gies|Klaus Klingmuller	  We compute Casimir interaction energies for the sphere-plate and cylinder-plate configuration induced by scalar-field fluctuations with Dirichlet boundary conditions. Based on a high-precision calculation using worldline numerics, we quantitatively determine the validity bounds of the proximity force approximation (PFA) on which the comparison between all corresponding experiments and theory are based. We observe the quantitative failure of the PFA on the 1% level for a curvature parameter a/R > 0.00755. Even qualitatively, the PFA fails to predict reliably the correct sign of genuine Casimir curvature effects. We conclude that data analysis of future experiments aiming at a precision of 0.1% must no longer be based on the PFA. 	
0603171v1	http://arxiv.org/pdf/quant-ph/0603171v1	2006	Hardy's criterion of nonlocality for mixed states	GianCarlo Ghirardi|Luca Marinatto	  We generalize Hardy's proof of nonlocality to the case of bipartite mixed statistical operators, and we exhibit a necessary condition which has to be satisfied by any given mixed state $\sigma$ in order that a local and realistic hidden variable model exists which accounts for the quantum mechanical predictions implied by $\sigma$. Failure of this condition will imply both the impossibility of any local explanation of certain joint probability distributions in terms of hidden variables and the nonseparability of the considered mixed statistical operator. Our result can be also used to determine the maximum amount of noise, arising from imperfect experimental implementations of the original Hardy's proof of nonlocality, in presence of which it is still possible to put into evidence the nonlocal features of certain mixed states. 	
0706.0170v1	http://arxiv.org/pdf/0706.0170v1	2007	On the origin of the $λ$-transition in liquid Sulphur	Tullio Scopigno|Spyros Yannopoulos|Filippo Scarponi|Kostas Andrikopoulos|Daniele Fioretto|Giancarlo Ruocco	  Developing a novel experimental technique, we applied photon correlation spectroscopy using infrared radiation in liquid Sulphur around $T_\lambda$, i.e. in the temperature range where an abrupt increase in viscosity by four orders of magnitude is observed upon heating within few degrees. This allowed us - overcoming photo-induced and absorption effects at visible wavelengths - to reveal a chain relaxation process with characteristic time in the ms range. These results do rehabilitate the validity of the Maxwell relation in Sulphur from an apparent failure, allowing rationalizing the mechanical and thermodynamic behavior of this system within a viscoelastic scenario. 	
0708.4296v1	http://arxiv.org/pdf/0708.4296v1	2007	Nuclear physics with spherically symmetric supernova models	M. Liebendoerfer|T. Fischer|C. Fröhlich|F. -K. Thielemann|S. Whitehouse	  Few years ago, Boltzmann neutrino transport led to a new and reliable generation of spherically symmetric models of stellar core collapse and postbounce evolution. After the failure to prove the principles of the supernova explosion mechanism, these sophisticated models continue to illuminate the close interaction between high-density matter under extreme conditions and the transport of leptons and energy in general relativistically curved space-time. We emphasize that very different input physics is likely to be relevant for the different evolutionary phases, e.g. nuclear structure for weak rates in collapse, the equation of state of bulk nuclear matter during bounce, multidimensional plasma dynamics in the postbounce evolution, and neutrino cross sections in the explosive nucleosynthesis. We illustrate the complexity of the dynamics using preliminary 3D MHD high-resolution simulations based on parameterized deleptonization. With established spherically symmetric models we show that typical features of the different phases are reflected in the predicted neutrino signal and that a consistent neutrino flux leads to electron fractions larger than 0.5 in neutrino-driven supernova ejecta. 	
0710.1535v1	http://arxiv.org/pdf/0710.1535v1	2007	Charmonium dynamics in heavy ion collisions	O. Linnyk|E. L. Bratkovskaya|W. Cassing|H. Stoecker	  Applying the HSD transport approach to charmonium dynamics within the 'hadronic comover model' and the 'QGP melting scenario', we show that the suppression pattern seen at RHIC cannot be explained by the interaction with baryons, comoving mesons and/or by color screening mechanism. The interaction with hadrons in the late stages of the collision (when the energy density falls below the critical) gives a sizable contribution to the suppression. On the other hand, it does not account for the observed additional charmonium dissociation and its dependence on rapidity. Together with the failure of the hadron-string models to reproduce high v2 of open charm mesons, this suggests strong pre-hadronic interaction of c-cbar with the medium at high energy densities. 	
0710.1917v3	http://arxiv.org/pdf/0710.1917v3	2007	The heating of the cooling flow (The feedback effervescent heating   model)	Nasser Mohamed Ahmed	  The standard cooling flow model has predicted a large amount of cool gas in the clusters of galaxies. The failure of the Chandra and XXM-Newton telescopes to detect cooling gas (below 1-2 keV) in clusters of galaxies has suggested that some heating process must work to suppress the cooling. The most likely heating source is the heating by AGNs. There are many heating mechanisms, but we will adopt the effervescent heating model which is a result of the interaction of the bubbles inflated by AGN with the intra-cluster medium(ICM).   Using the FLASH code, we have carried out time dependent simulations to investigate the effect of the heating on the suppression of the cooling in cooling flow clusters. We have found that the effervescent heating model can not balance the radiative cooling and it is an artificial model. Furthermore, the effervescent heating is a function of the ICM pressure gradient but the cooling is proportional to the gas density square and square root of the gas temperature. 	
0710.4008v1	http://arxiv.org/pdf/0710.4008v1	2007	Scale-free networks resistant to intentional attacks	Lazaros K. Gallos|Panos Argyrakis	  We study the detailed mechanism of the failure of scale-free networks under intentional attacks. Although it is generally accepted that such networks are very sensitive to targeted attacks, we show that for a particular type of structure such networks surprisingly remain very robust even under removal of a large fraction of their nodes, which in some cases can be up to 70%. The degree distribution $P(k)$ of these structures is such that for small values of the degree $k$ the distribution is constant with $k$, up to a critical value $k_c$, and thereafter it decays with $k$ with the usual power law. We describe in detail a model for such a scale-free network with this modified degree distribution, and we show both analytically and via simulations, that this model can adequately describe all the features and breakdown characteristics of these attacks. We have found several experimental networks with such features, such as for example the IMDB actors collaboration network or the citations network, whose resilience to attacks can be accurately described by our model. 	
0711.1191v1	http://arxiv.org/pdf/0711.1191v1	2007	High Strain and Strain-Rate Behaviour of PTFE/Aluminium/Tungsten   Mixtures	John Addiss|Jing Cai|Stephen Walley|William Proud|Vitali F. Nesterenko	  Conventional drop-weight techniques were modified to accommodate low-amplitude force transducer signals from low-strength, cold isostatically pressed 'heavy' composites of polytetrafluoroethylene, aluminum and tungsten. The failure strength, strain and the post-critical behavior of failed samples were measured for samples of different porosity and tungsten grain size. Unusual phenomenon of significantly higher strength (55 MPa) of porous composites (density 5.9 g/cc) with small W particles (less than 1 micron) in comparison with strength (32 MPa) of dense composites (7.1 g/cc) with larger W particles (44 microns) at the same volume content of components was observed. This is attributed to force chains created by a network of small W particles. Interrupted tests at different levels of strain revealed the mechanisms of fracture under dynamic compression. 	
0801.1927v1	http://arxiv.org/pdf/0801.1927v1	2008	Asynchronous Remote Medical Consultation for Ghana	Rowena Luk|Melissa Ho|Paul M. Aoki	  Computer-mediated communication systems can be used to bridge the gap between doctors in underserved regions with local shortages of medical expertise and medical specialists worldwide. To this end, we describe the design of a prototype remote consultation system intended to provide the social, institutional and infrastructural context for sustained, self-organizing growth of a globally-distributed Ghanaian medical community. The design is grounded in an iterative design process that included two rounds of extended design fieldwork throughout Ghana and draws on three key design principles (social networks as a framework on which to build incentives within a self-organizing network; optional and incremental integration with existing referral mechanisms; and a weakly-connected, distributed architecture that allows for a highly interactive, responsive system despite failures in connectivity). We discuss initial experiences from an ongoing trial deployment in southern Ghana. 	
0801.2468v1	http://arxiv.org/pdf/0801.2468v1	2008	The Construction of the CMS Silicon Strip Tracker	Giacomo Sguazzoni	  The CMS Silicon Strip tracker is a very large scale tracker entirely based on silicon strip detectors technology. The integration of modules, electronics, mechanics and services has been completed within the last eighteen months; first large standalone sub-structures (shells, disks, rods, petals depending on the tracker subdetector) have been integrated and verified; then they have been brought together into the final configuration. The CMS silicon tracker design and its construction is reviewed with particular emphasis on the procedures and quality checks deployed to successfully assembly several silicon strip modules and all ancillary components into these large sub-structures. An overview of the results and the lesson learned from the tracker integration are given, also in terms of failure and damage rates. 	
0802.3003v1	http://arxiv.org/pdf/0802.3003v1	2008	Discrete Fracture Model with Anisotropic Load Sharing	R. C. Hidalgo|S. Zapperi|H. J. Herrmann	  A two-dimensional fracture model where the interaction among elements is modeled by an anisotropic stress-transfer function is presented. The influence of anisotropy on the macroscopic properties of the samples is clarified, by interpolating between several limiting cases of load sharing. Furthermore, the critical stress and the distribution of failure avalanches are obtained numerically for different values of the anisotropy parameter $\alpha$ and as a function of the interaction exponent $\gamma$. From numerical results, one can certainly conclude that the anisotropy does not change the crossover point $\gamma_c=2$ in 2D. Hence, in the limit of infinite system size, the crossover value $\gamma_c=2$ between local and global load sharing is the same as the one obtained in the isotropic case. In the case of finite systems, however, for $\gamma\le2$, the global load sharing behavior is approached very slowly. 	
0803.1913v1	http://arxiv.org/pdf/0803.1913v1	2008	Rupture sismique des fondations par perte de capacité portante: Le cas   des semelles circulaires	Charisis Chatzigogos|Alain Pecker|J. Salençon	  Within the context of earthquake-resistant design of shallow foundations, the present study is concerned with the determination of the seismic bearing capacity of a circular footing resting on the surface of a heterogene-ous purely cohesive semi-infinite soil layer. In the first part of the paper, a database, containing case histories of civil engineering structures that sustained a foundation seismic bearing capacity failure, is briefly pre-sented, aiming at a better understanding of the studied phenomenon and offering a number of case studies useful for validation of theoretical computations. In the second part of the paper, the aforementioned problem is addressed using the kinematic approach of the Yield Design theory, thus establishing optimal upper bounds for the ultimate seismic loads supported by the soil-footing system. The results lead to the establishment of some very simple guidelines that extend the existing formulae for the seismic bearing capacity contained in the European norms (proposed for strip footings on homogeneous soils) to the case of circular footings and to that of heterogeneous cohesive soils. 	
0808.0709v1	http://arxiv.org/pdf/0808.0709v1	2008	Fine Structure of Avalanches in the Abelian Sandpile Model	Amir Abdolvand|Afshin Montakhab	  We study the two-dimensional Abelian Sandpile Model on a square lattice of linear size L. We introduce the notion of avalanche's fine structure and compare the behavior of avalanches and waves of toppling. We show that according to the degree of complexity in the fine structure of avalanches, which is a direct consequence of the intricate superposition of the boundaries of successive waves, avalanches fall into two different categories. We propose scaling ans\"{a}tz for these avalanche types and verify them numerically. We find that while the first type of avalanches has a simple scaling behavior, the second (complex) type is characterized by an avalanche-size dependent scaling exponent. This provides a framework within which one can understand the failure of a consistent scaling behavior in this model. 	
0809.2844v1	http://arxiv.org/pdf/0809.2844v1	2008	Interplay of local hydrogen-bonding and long-ranged dipolar forces in   simulations of confined water	Jocelyn M. Rodgers|John D. Weeks	  Spherical truncations of Coulomb interactions in standard models for water permit efficient molecular simulations and can give remarkably accurate results for the structure of the uniform liquid. However truncations are known to produce significant errors in nonuniform systems, particularly for electrostatic properties. Local molecular field (LMF) theory corrects such truncations by use of an effective or restructured electrostatic potential that accounts for effects of the remaining long-ranged interactions through a density-weighted mean field average and satisfies a modified Poisson's equation defined with a Gaussian-smoothed charge density. We apply LMF theory to three simple molecular systems that exhibit different aspects of the failure of a naive application of spherical truncations -- water confined between hydrophobic walls, water confined between atomically-corrugated hydrophilic walls, and water confined between hydrophobic walls with an applied electric field. Spherical truncations of 1/r fail spectacularly for the final system in particular, and LMF theory corrects the failings for all three. Further, LMF theory provides a more intuitive way to understand the balance between local hydrogen bonding and longer-ranged electrostatics in molecular simulations involving water. 	
0810.2055v1	http://arxiv.org/pdf/0810.2055v1	2008	The performance of Minima Hopping and Evolutionary Algorithms for   cluster structure prediction	Sandro E. Schoenborn|Stefan Goedecker|Shantanu Roy|Artem R. Oganov	  We compare Evolutionary Algorithms with Minima Hopping for global optimization in the field of cluster structure prediction. We introduce a new {\em average offspring} recombination operator and compare it with previously used operators. Minima Hopping is improved with a {\em softening} method and a stronger feedback mechanism. Test systems are atomic clusters with Lennard-Jones interaction as well as silicon and gold clusters described by force fields. The improved Minima Hopping is found to be well-suited to all these homoatomic problems. The evolutionary algorithm is more efficient for systems with compact and symmetric ground states, including LJ$_{150}$, but it fails for systems with very complex energy landscapes and asymmetric ground states, such as LJ$_{75}$ and silicon clusters with more than 30 atoms. Both successes and failures of the evolutionary algorithm suggest ways for its improvement. 	
0811.3611v1	http://arxiv.org/pdf/0811.3611v1	2008	Nucleation of interfacial shear cracks in thin films on disordered   substrates	Michael Zaiser|Paolo Moretti|Avraam Konstantinidis|Elias C Aifantis	  We formulate a theoretical model of the shear failure of a thin film tethered to a rigid substrate. The interface between film and substrate is modeled as a cohesive layer with randomly fluctuating shear strength/fracture energy. We demonstrate that, on scales large compared with the film thickness, the internal shear stresses acting on the interface can be approximated by a second-order gradient of the shear displacement across the interface. The model is used to study one-dimensional shear cracks, for which we evaluate the stress-dependent probability of nucleation of a critical crack. This is used to determine the interfacial shear strength as a function of film geometry and statistical properties of the interface. 	
0901.3759v2	http://arxiv.org/pdf/0901.3759v2	2009	On the relationship between structure and dynamics in a supercooled   liquid	Asaph Widmer-Cooper|Peter Harrowell	  We present the dynamic propensity distribution as an explicit measure of the degree to which the dynamics in a liquid over the time scale of structural relaxation is determined by the initial configuration. We then examine, for a binary mixture of soft discs in two dimensions, the correlation between the spatial distribution of propensity and that of two localmeasures of configuration structure: the local composition and local free volume. While the small particles dominate the high propensity population,we find no strong correlation between either the local composition or the local free volume and the propensity. It is argued that this is a generic failure of purely local structural measures to capture the inherently non-local character of collective behaviour. 	
0901.4692v1	http://arxiv.org/pdf/0901.4692v1	2009	Effectiveness of Ninth-Grade Physics in Maine: Conceptual Understanding	Michael O'Brien|John Thompson	  The Physics First movement - teaching a true physics course to ninth grade students - is gaining popularity in high schools. There are several different rhetorical arguments for and against this movement, and it is quite controversial in physics education. However, there is no actual evidence to assess the success, or failure, of this substantial shift in the science teaching sequence. We have undertaken a comparison study of physics classes taught in ninth- and 12th grade classes in Maine. Comparisons of student understanding and gains with respect to mechanics concepts were made with excerpts from well-known multiple-choice surveys and individual student interviews. Results indicate that both populations begin physics courses with similar content knowledge and specific difficulties, but that in the learning of the concepts ninth graders are more sensitive to the instructional method used. 	
0903.3279v1	http://arxiv.org/pdf/0903.3279v1	2009	Generalisation of the fractal Einstein law relating conduction and   diffusion on networks	Anthony P. Roberts|Christophe P. Haynes	  In the 1980s an important goal of the emergent field of fractals was to determine the relationships between their physical and geometrical properties. The fractal-Einstein and Alexander-Orbach laws, which interrelate electrical, diffusive and fractal properties, are two key theories of this type. Here we settle a long standing controversy about their exactness by showing that the properties of a class of fractal trees violate both laws. A new formula is derived which unifies the two classical results by proving that if one holds, then so must the other, and resolves a puzzling discrepancy in the properties of Eden trees and diffusion limited aggregates. The failure of the classical laws is attributed to anisotropic exploration of the network by a random walker. The occurrence of this newly revealed behaviour means that numerous theories, such as recent first passage time results, are restricted to a narrower range of networks than previously thought. 	
0904.1611v1	http://arxiv.org/pdf/0904.1611v1	2009	Shear Unzipping of DNA	Buddhapriya Chakrabarti|David R. Nelson	  We study theoretically the mechanical failure of a simple model of double stranded DNA under an applied shear. Starting from a more microscopic Hamiltonian that describes a sheared DNA, we arrive at a nonlinear generalization of a ladder model of shear unzipping proposed earlier by deGennes [deGennes P. G. C. R. Acad. Sci., Ser. IV; Phys., Astrophys. 2001, 1505]. Using this model and a combination of analytical and numerical methods, we study the DNA "unzipping" transition when the shearing force exceeds a critical threshold at zero temperature. We also explore the effects of sequence heterogeneity and finite temperature and discuss possible applications to determine the strength of colloidal nanoparticle assemblies functionalized by DNA. 	
0904.1986v1	http://arxiv.org/pdf/0904.1986v1	2009	The Breaking Strain of Neutron Star Crust and Gravitational Waves	C. J. Horowitz|Kai Kadau	  Mountains on rapidly rotating neutron stars efficiently radiate gravitational waves. The maximum possible size of these mountains depends on the breaking strain of neutron star crust. With multi-million ion molecular dynamics simulations of Coulomb solids representing the crust, we show that the breaking strain of pure single crystals is very large and that impurities, defects, and grain boundaries only modestly reduce the breaking strain to around 0.1. Due to the collective behavior of the ions during failure found in our simulations, the neutron star crust is likely very strong and can support mountains large enough so that their gravitational wave radiation could limit the spin periods of some stars and might be detectable in large scale interferometers. Furthermore, our microscopic modeling of neutron star crust material can help analyze mechanisms relevant in magnetar giant and micro flares. 	
0906.1013v1	http://arxiv.org/pdf/0906.1013v1	2009	Dielectrophoretic Assembly of High-Density Arrays of Individual Graphene   Devices for Rapid Screening	Aravind Vijayaraghavan|Calogero Sciascia|Simone Dehm|Antonio Lombardo|Alessandro Bonetti|Andrea C. Ferrari|Ralph Krupke	  We establish the use of dielectrophoresis for the directed parallel assembly of individual flakes and nanoribbons of few-layer graphene into electronic devices. This is a bottom-up approach where source and drain electrodes are prefabricated and the flakes are deposited from a solution using an alternating electric field applied between the electrodes. These devices are characterized by scanning electron microscopy, atomic force microscopy, Raman spectroscopy and electron transport measurements. They are shown to be electrically active and their current carrying capacity and subsequent failure mechanism is revealed. Akin to carbon nanotubes, we show that the dielectrophoretic deposition is self-limiting to one flake per device and is scalable to ultra-large-scale integration densities, thereby enabling the rapid screening of a large number of devices. 	
0908.0424v1	http://arxiv.org/pdf/0908.0424v1	2009	The work value of information	Oscar C. O. Dahlsten|Renato Renner|Elisabeth Rieper|Vlatko Vedral	  We present quantitative relations between work and information that are valid both for finite sized and internally correlated systems as well in the thermodynamical limit. We suggest work extraction should be viewed as a game where the amount of work an agent can extract depends on how well it can guess the micro-state of the system. In general it depends both on the agent's knowledge and risk-tolerance, because the agent can bet on facts that are not certain and thereby risk failure of the work extraction. We derive strikingly simple expressions for the extractable work in the extreme cases of effectively zero- and arbitrary risk tolerance respectively, thereby enveloping all cases. Our derivation makes a connection between heat engines and the smooth entropy approach. The latter has recently extended Shannon theory to encompass finite sized and internally correlated bit strings, and our analysis points the way to an analogous extension of statistical mechanics. 	
0909.0685v1	http://arxiv.org/pdf/0909.0685v1	2009	In-Network Outlier Detection in Wireless Sensor Networks	Joel W. Branch|Chris Giannella|Boleslaw Szymanski|Ran Wolff|Hillol Kargupta	  To address the problem of unsupervised outlier detection in wireless sensor networks, we develop an approach that (1) is flexible with respect to the outlier definition, (2) computes the result in-network to reduce both bandwidth and energy usage,(3) only uses single hop communication thus permitting very simple node failure detection and message reliability assurance mechanisms (e.g., carrier-sense), and (4) seamlessly accommodates dynamic updates to data. We examine performance using simulation with real sensor data streams. Our results demonstrate that our approach is accurate and imposes a reasonable communication load and level of power consumption. 	
0909.3482v1	http://arxiv.org/pdf/0909.3482v1	2009	Schumpeterian economic dynamics as a quantifiable minimum model of   evolution	Stefan Thurner|Peter Klimek|Rudolf Hanel	  We propose a simple quantitative model of Schumpeterian economic dynamics. New goods and services are endogenously produced through combinations of existing goods. As soon as new goods enter the market they may compete against already existing goods, in other words new products can have destructive effects on existing goods. As a result of this competition mechanism existing goods may be driven out from the market - often causing cascades of secondary defects (Schumpeterian gales of destruction). The model leads to a generic dynamics characterized by phases of relative economic stability followed by phases of massive restructuring of markets - which could be interpreted as Schumpeterian business `cycles'. Model timeseries of product diversity and productivity reproduce several stylized facts of economics timeseries on long timescales such as GDP or business failures, including non-Gaussian fat tailed distributions, volatility clustering etc. The model is phrased in an open, non-equilibrium setup which can be understood as a self organized critical system. Its diversity dynamics can be understood by the time-varying topology of the active production networks. 	
0909.3701v1	http://arxiv.org/pdf/0909.3701v1	2009	Anharmonicity and quasi-localization of the excess low-frequency   vibrations in jammed solids	Ning Xu|Vincenzo Vitelli|Andrea J. Liu|Sidney R. Nagel	  We compare the harmonic and anharmonic properties of the vibrational modes in 3-dimensional jammed packings of frictionless spheres interacting via repulsive, finite range potentials. A crossover frequency is apparent in the density of states, the diffusivity and the participation ratio of the modes. At this frequency, which shifts to zero at the jamming threshold, the vibrational modes have a very small participation ratio implying that the modes are quasi-localized. The most anharmonic modes occur at low frequency which is opposite to what is normally found in crystals. The lowest frequency modes have the strongest response to the pressure and the lowest energy barriers to mechanical failure. 	
0911.2010v1	http://arxiv.org/pdf/0911.2010v1	2009	Critical Kondo destruction and the violation of the quantum-to-classical   mapping of quantum criticality	Stefan Kirchner|Qimiao Si	  Antiferromagnetic heavy fermion metals close to their quantum critical points display a richness in their physical properties unanticipated by the traditional approach to quantum criticality, which describes the critical properties solely in terms of fluctuations of the order parameter. This has led to the question as to how the Kondo effect gets destroyed as the system undergoes a phase change. In one approach to the problem, Kondo lattice systems are studied through a self-consistent Bose-Fermi Kondo model within the Extended Dynamical Mean Field Theory. The quantum phase transition of the Kondo lattice is thus mapped onto that of a sub-Ohmic Bose-Fermi Kondo model. In the present article we address some aspects of the failure of the standard order-parameter functional for the the Kondo-destroying quantum critical point of the Bose-Fermi Kondo model. 	
0912.0549v1	http://arxiv.org/pdf/0912.0549v1	2009	Modular Workflow Engine for Distributed Services using Lightweight Java   Clients	R. -M. Vetter|W. Lennartz|J. -V. Peetz	  In this article we introduce the concept and the first implementation of a lightweight client-server-framework as middleware for distributed computing. On the client side an installation without administrative rights or privileged ports can turn any computer into a worker node. Only a Java runtime environment and the JAR files comprising the workflow client are needed. To connect all clients to the engine one open server port is sufficient. The engine submits data to the clients and orchestrates their work by workflow descriptions from a central database. Clients request new task descriptions periodically, thus the system is robust against network failures. In the basic set-up, data up- and downloads are handled via HTTP communication with the server. The performance of the modular system could additionally be improved using dedicated file servers or distributed network file systems.   We demonstrate the design features of the proposed engine in real-world applications from mechanical engineering. We have used this system on a compute cluster in design-of-experiment studies, parameter optimisations and robustness validations of finite element structures. 	
1002.0392v1	http://arxiv.org/pdf/1002.0392v1	2010	A novel and precise time domain description of MOSFET low frequency   noise due to random telegraph signals	Roberto da Silva|Gilson Wirth|Lucas Brusamarello	  Nowadays, random telegraph signals play an important role in integrated circuit performance variability, leading for instance to failures in memory circuits. This problem is related to the successive captures and emissions of electrons at the many traps stochastically distributed at the silicon-oxide (Si-SiO2) interface of MOS transistors. In this paper we propose a novel analytical and numerical approach to statistically describe the fluctuations of current due to random telegraph signal in time domain. Our results include two distinct situations: when the density of interface trap density is uniform in energy, and when it is an u-shape curve as prescribed in literature, here described as simple quadratic function. We establish formulas for relative error as function of the parameters related to capture and emission probabilities. For a complete analysis experimental u-shape curves are used and compared with the theoretical aproach. 	
1003.5206v1	http://arxiv.org/pdf/1003.5206v1	2010	Chaos and Thermalization in the one-dimensional Bose-Hubbard model in   the classical-field approximation	Amy C. Cassidy	  In this thesis, we present a comprehensive study of chaos and thermalization of the one-dimensional Bose-Hubbard Model (BHM) within the classical field approximation. Two quantitative measures are compared: the ensemble-averaged Finite-time Maximal Lyapunov exponent, a measures of chaos and the normalized spectral entropy, a measure of the distance between the numerical time-averaged momentum distribution and the one predicted by thermodynamics. A threshold for chaos is found, which depends on two parameters, the nonlinearity and the total energy-per-particle. Below the threshold, the dynamics are regular, while far above the threshold, complete thermalization is observed, as measured by the normalized spectral entropy. We study individual resonances in the Bose-Hubbard model to determine the criterion for chaos. The criterion based on Chirikov's method of overlapping resonances diverges in the thermodynamic limit, in contrast to the criterion parameters inferred from numerical calculations, signifying the failure of the standard Chirikov's approach. The Ablowitz-Ladik lattice is one of several integrable models that are close to the BHM. We outline the method of Inverse Scattering Transform and generate the integrals of motion of the Ablowitz-Ladik lattice. Furthermore, we discuss the possible role of these quantities in the relaxation dynamics of the BHM. 	
1005.0796v1	http://arxiv.org/pdf/1005.0796v1	2010	Structure-Sensitive Mechanism of Nanographene Failure	Elena F. Sheka|Nadezhda A. Popova|Vera A. Popova|Ekaterina A. Nikitina|Landysh H. Shaymardanova	  The response of a nanographene sheet to external stresses is considered in terms of a mechanochemical reaction. The quantum chemical realization of the approach is based on a coordinate-of-reaction concept for the purpose of introducing a mechanochemical internal coordinate (MIC) that specifies a deformational mode. The related force of response is calculated as the energy gradient along the MIC, while the atomic configuration is optimized over all of the other coordinates under the MIC constant-pitch elongation. The approach is applied to the benzene molecule and (5, 5) nanographene. A drastic anisotropy in the microscopic behavior of both objects under elongation along a MIC has been observed when the MIC is oriented either along or normally to the C-C bonds chain. Both the anisotropy and high stiffness of the nanographene originate at the response of the benzenoid unit to stress. 	
1005.1119v1	http://arxiv.org/pdf/1005.1119v1	2010	Coherent Manipulation of Multilevel Atoms for Quantum Information   Processing	Juan D. Serna	  In quantum information processing, quantum cavities play an important role by providing the mechanisms to transfer information between atom qubits and photon qubits, or to couple single atoms with the optical modes of the cavity field. We explore numerically the population transfer in an atom + cavity system by using the $\pi$-pulse and adiabatic passage methods. While the first method is very efficient transferring the atomic population for no radiative decay of the intermediate level, the second method shows very interesting nonadiabatic, resonance-like properties that can be used to achieve very large transfer efficiencies without needing very large Rabi frequencies or very long interaction times. We introduce a simple analytical model to explore the origin of these properties and describe "qualitatively" the power-law dependence of the failure probability on the product of the pulse amplitude and the interaction time. We also examine numerically the transfer of interatomic coherence in a two-atom + cavity system by using adiabatic methods. For some specific symmetry conditions, we show that the dynamics of the original system can be studied as the individual evolution of a symmetric and an antisymmetric system, interacting separately with the classical field and the cavity mode, but mutually exchanging the atomic coherence. 	
1005.4932v2	http://arxiv.org/pdf/1005.4932v2	2010	Failure of Bell's Theorem and the Local Causality of the Entangled   Photons	Joy Christian	  A counterexample to Bell's theorem is presented which uses a pair of photons instead of spin-1/2 particles used in our previous counterexamples. A locally causal protocol is provided for Alice and Bob, which allows them to simulate observing photon polarizations at various angles, and record their results as A=+/-1 in S^3 and B=+/-1 in S^3, respectively. When these results are compared, the correlations are seen to be exactly those predicted by quantum mechanics; namely cos 2(alpha - beta), where alpha and beta are the angles of polarizers. The key ingredient in our counterexample is the topology of 3-sphere, which remains closed under multiplication, thus preserving the locality condition of Bell. 	
1006.3521v1	http://arxiv.org/pdf/1006.3521v1	2010	Business fluctuations in a credit-network economy	Domenico Delli Gatti|Mauro Gallegati|Bruce Greenwald|Alberto Russo|Joseph E. Stiglitz	  We model a network economy with three sectors: downstream firms, upstream firms, and banks. Agents are linked by productive and credit relationships so that the behavior of one agent influences the behavior of the others through network connections. Credit interlinkages among agents are a source of bankruptcy diffusion: in fact, failure of fulfilling debt commitments would lead to bankruptcy chains. All in all, the bankruptcy in one sector can diffuse to other sectors through linkages creating a vicious cycle and bankruptcy avalanches in the network economy. Our analysis show how the choices of credit supply by both banks and firms are interrelated. While the initial impact of monetary policy is on bank behaviour, we show the interactive play between the choices made by banks, the choices made by firms in their role as providers of credit, and the choices made by firms in their role as producers. 	
1007.1780v1	http://arxiv.org/pdf/1007.1780v1	2010	A subluminous Schroedinger equation	Philip Rosenau|Zeev Schuss	  The standard derivation of Schroedinger's equation from a Lorentz-invariant Feynman path integral consists in taking first the limit of infinite speed of light and then the limit of short time slice. In this order of limits the light cone of the path integral disappears, giving rise to an instantaneous spread of the wave function to the entire space. We ascribe the failure of Schroedinger's equation to retain the light cone of the path integral to the very nature of the limiting process: it is a regular expansion of a singular approximation problem, because the boundary conditions of the path integral on the light cone are lost in this limit. We propose a distinguished limit, which produces an intermediate model between non-relativistic and relativistic quantum mechanics: it produces Schroedinger's equation and preserves the zero boundary conditions on and outside the original light cone of the path integral. These boundary conditions relieve the Schroedinger equation of several annoying, seemingly unrelated unphysical artifacts, including non-analytic wave functions, spontaneous appearance of discontinuities, non-existence of moments when the initial wave function has a jump discontinuity (e.g., a collapsed wave function after a measurement), the EPR paradox, and so on. The practical implications of the present formulation are yet to be seen. 	
1007.2155v3	http://arxiv.org/pdf/1007.2155v3	2011	Evidence for Anthropogenic Surface Loading as Trigger Mechanism of the   2008 Wenchuan Earthquake	Christian D. Klose	  Two and a half years prior to China's M7.9 Wenchuan earthquake of May 2008, at least 300 million metric tons of water accumulated with additional seasonal water level changes in the Minjiang River Valley at the eastern margin of the Longmen Shan. This article shows that static surface loading in the Zipingpu water reservoir induced Coulomb failure stresses on the nearby Beichuan thrust fault system at <17km depth. Triggering stresses exceeded levels of daily lunar and solar tides and perturbed a fault area measuring 416+/-96km^2. These stress perturbations, in turn, likely advanced the clock of the mainshock and directed the initial rupture propagation upward towards the reservoir on the "Coulomb-like" Beichuan fault with rate-and-state dependent frictional behavior. Static triggering perturbations produced up to 60 years (0.6%) of equivalent tectonic loading, and show strong correlations to the coseismic slip. Moreover, correlations between clock advancement and coseismic slip, observed during the mainshock beneath the reservoir, are strongest for a longer seismic cycle (10kyr) of M>7 earthquakes. Finally, the daily event rate of the micro-seismicity (M>0.5) correlates well with the static stress perturbations, indicating destabilization. 	
1008.0464v1	http://arxiv.org/pdf/1008.0464v1	2010	Backward Causation in Complex Action Model --- Superdeterminism and   Transactional Interpretations	Holger B. Nielsen|Masao Ninomiya	  It is shown that the transactional interpretation of quantum mechanics being referred back to Feynman-Wheeler's time reversal symmetric radiation theory has reminiscences to our complex action model. In this complex action model the initial conditions are in principle even calculable. Thus it philosophically points towards superdeterminism, but really the Bell theorem problem is solved in our model of complex action by removing the significance of signals running slower than by light velocity.   Our model as earlier published predicts that LHC should have some failure before reaching to have produced as many Higgs-particles as would have been produced the SSC accelerator. In the present article, we point out that a cardgame involving whether to restrict LHC-running as we have proposed to test our model will under all circumstances be a success. 	
1008.2236v2	http://arxiv.org/pdf/1008.2236v2	2010	Efficient energy transfer in light-harvesting systems, I: optimal   temperature, reorganization energy, and spatial-temporal correlations	Jianlan Wu|Fan Liu|Young Shen|Jianshu Cao|Robert J. Silbey	  Understanding the mechanisms of efficient and robust energy transfer in light-harvesting systems provides new insights for the optimal design of artificial systems. In this paper, we use the Fenna-Matthews-Olson (FMO) protein complex and phycocyanin 645 (PC 645) to explore the general dependence on physical parameters that help maximize the efficiency and maintain its stability. With the Haken-Strobl model, the maximal energy transfer efficiency (ETE) is achieved under an intermediate optimal value of dephasing rate. To avoid the infinite temperature assumption in the Haken-Strobl model and the failure of the Redfield equation in predicting the Forster rate behavior, we use the generalized Bloch-Redfield (GBR) equation approach to correctly describe dissipative exciton dynamics and find that maximal ETE can be achieved under various physical conditions, including temperature, reorganization energy, and spatial-temporal correlations in noise. We also identify regimes of reorganization energy where the ETE changes monotonically with temperature or spatial correlation and therefore cannot be optimized with respect to these two variables. 	
1009.2556v2	http://arxiv.org/pdf/1009.2556v2	2011	Securing Dynamic Distributed Storage Systems against Eavesdropping and   Adversarial Attacks	Sameer Pawar|Salim El Rouayheb|Kannan Ramchandran	  We address the problem of securing distributed storage systems against eavesdropping and adversarial attacks. An important aspect of these systems is node failures over time, necessitating, thus, a repair mechanism in order to maintain a desired high system reliability. In such dynamic settings, an important security problem is to safeguard the system from an intruder who may come at different time instances during the lifetime of the storage system to observe and possibly alter the data stored on some nodes. In this scenario, we give upper bounds on the maximum amount of information that can be stored safely on the system. For an important operating regime of the distributed storage system, which we call the 'bandwidth-limited regime', we show that our upper bounds are tight and provide explicit code constructions. Moreover, we provide a way to short list the malicious nodes and expurgate the system. 	
1009.5454v1	http://arxiv.org/pdf/1009.5454v1	2010	Study of gold induced heavy ion collisions using isospin dependent QMD   model	Bhawna Sharma|Sanjeev Kumar|Suneel Kumar|Rajeev K. Puri	  We have studied the fragment production mechanism in the set of four reactions 197Au79+12C6, 197Au79+26Al13, 197Au79+63Cu29 and 197Au79+208Pb82. The reactions are simulated at an energy 600 MeV/nucleon and collision geometry is varied from central to peripheral (= b/bmax = 0 to 1). A theoretical investigation has been carried out on the study of mass dependence of intermediate mass fragments (5\leqA\leqAtot/6) and other fragments. It is observed that multiplicity shows a good agreement for low Zbound, but it fails for high Zbound. This failure is due the method of analysis MST which we had used in our analysis, because MST method gives one heavy cluster at the time of high density. The discrepancy between theory and experiments can be removed by using reduced isospin dependent NN cross section and sophisticated clustrization algorithm SACA. 	
1011.0063v1	http://arxiv.org/pdf/1011.0063v1	2010	Multiscales and cascade in isotropic turbulence	Zheng Ran	  The central problem of fully developed turbulence is the energy cascading process. It has revisited all attempts at a full physical understanding or mathematical formulation. The main reason for this failure are related to the large hierarchy of scales involved, the highly nonlinear character inherent in the Navier-Stokes equations, and the spatial intermittency of the dynamically active regions. Richardson has described the interplay between large and small scales and the phenomena so described are known as the Richardson cascade. This local interplay also forms the basis of a theory by Kolmogorov. In this letter, we use the explicit map method to analyze the nonlinear dynamical behavior for cascade in isotropic turbulence. This deductive scale analysis is shown to provide the first visual evidence of the celebrated Richardson cascade, and reveals in particular its multiscale character. The results also indicate that the energy cascading process has remarkable similarities with the deterministic construction rules of the logistic map. Cascade of period-doubling bifurcations have been seen in this isotropic turbulent systems that exhibit chaotic behavior. The `cascade' appears as an infinite sequence of period-doubling bifurcations. 	
1012.0203v1	http://arxiv.org/pdf/1012.0203v1	2010	Enhancing synchronization by directionality in complex networks	An Zeng|Seung-Woo Son|Chi Ho Yeung|Ying Fan|Zengru Di	  We proposed a method called residual edge-betweenness gradient (REBG) to enhance synchronizability of networks by assignment of link direction while keeping network topology and link weight unchanged. Direction assignment has been shown to improve the synchronizability of undirected networks in general, but we find that in some cases incommunicable components emerge and networks fail to synchronize. We show that the REBG method can effectively avoid the synchronization failure ($R=\lambda_{2}^{r}/\lambda_{N}^{r}=0$) which occurs in the residual degree gradient (RDG) method proposed in Phys. Rev. Lett. 103, 228702 (2009). Further experiments show that REBG method enhance synchronizability in networks with community structure as compared with the RDG method. 	
1012.4632v2	http://arxiv.org/pdf/1012.4632v2	2011	Quantum theory of fermion production after inflation	J. Berges|D. Gelfand|J. Pruschke	  We show that quantum effects dramatically enhance the production of fermions following preheating after inflation in the early Universe in the presence of high excitations of bosonic quanta. As a consequence fermions rapidly approach a quasistationary distribution with a thermal occupancy in the infrared, while the inflaton enters a turbulent scaling regime. The failure of standard semiclassical descriptions based on the Dirac equation with a homogeneous background field is caused by nonperturbatively high boson occupation numbers. During preheating the inflaton occupation number increases, thus leading to a dynamical mechanism for the enhanced production of fermions from the rescattering of the inflaton quanta. We comment on related phenomena in heavy-ion collisions for the production of quark matter fields from highly occupied gauge bosons. 	
1101.2272v1	http://arxiv.org/pdf/1101.2272v1	2011	Logical Consensus for Distributed and Robust Intrusion Detection	Adriano Fagiolini|Antonio Bicchi	  In this paper we introduce a novel consensus mech- anism where agents of a network are able to share logical values, or Booleans, representing their local opinions on e.g. the presence of an intruder or of a fire within an indoor environment. We first formulate the logical consensus problem, and then we review relevant results in the literature on cellular automata and convergence of finite-state iteration maps. Under suitable joint conditions on the visibility of agents and their communication capability, we provide an algorithm for generating a logical linear consensus system that is globally stable. The solution is optimal in terms of the number of messages to be exchanged and the time needed to reach a consensus. Moreover, to cope with possible sensor failure, we propose a second design approach that produces robust logical nonlinear consensus systems tolerating a given maximum number of faults. Finally, we show applicability of the agreement mechanism to a case study consisting of a distributed Intrusion Detection System (IDS). 	
1101.5564v1	http://arxiv.org/pdf/1101.5564v1	2011	Generating Functions and Stability Study of Multivariate Self-Excited   Epidemic Processes	A. Saichev|D. Sornette	  We present a stability study of the class of multivariate self-excited Hawkes point processes, that can model natural and social systems, including earthquakes, epileptic seizures and the dynamics of neuron assemblies, bursts of exchanges in social communities, interactions between Internet bloggers, bank network fragility and cascading of failures, national sovereign default contagion, and so on. We present the general theory of multivariate generating functions to derive the number of events over all generations of various types that are triggered by a mother event of a given type. We obtain the stability domains of various systems, as a function of the topological structure of the mutual excitations across different event types. We find that mutual triggering tends to provide a significant extension of the stability (or subcritical) domain compared with the case where event types are decoupled, that is, when an event of a given type can only trigger events of the same type. 	
1102.0141v2	http://arxiv.org/pdf/1102.0141v2	2011	Dynamics of the directed Ising chain	Claude Godreche	  The study by Glauber of the time-dependent statistics of the Ising chain is extended to the case where each spin is influenced unequally by its nearest neighbours. The asymmetry of the dynamics implies the failure of the detailed balance condition. The functional form of the rate at which an individual spin changes its state is constrained by the global balance condition with respect to the equilibrium measure of the Ising chain. The local magnetization, the equal-time and two-time correlation functions and the linear response to an external magnetic field obey linear equations which are solved explicitly. The behaviour of these quantities and the relation between the correlation and response functions are analyzed both in the stationary state and in the zero-temperature scaling regime. In the stationary state, a transition between two behaviours of the correlation function occurs when the amplitude of the asymmetry crosses a critical value, with the consequence that the limit fluctuation-dissipation ratio decays continuously from the value 1, for the equilibrium state in the absence of asymmetry, to 0 for this critical value. At zero temperature, under asymmetric dynamics, the system loses its critical character, yet keeping many of the characteristic features of a coarsening system. 	
1103.1207v1	http://arxiv.org/pdf/1103.1207v1	2011	Framework to Solve Load Balancing Problem in Heterogeneous Web Servers	Ms. Deepti Sharma|Ms. Archana B. Saxena	  For popular websites most important concern is to handle incoming load dynamically among web servers, so that they can respond to their client without any wait or failure. Different websites use different strategies to distribute load among web servers but most of the schemes concentrate on only one factor that is number of requests, but none of the schemes consider the point that different type of requests will require different level of processing efforts to answer, status record of all the web servers that are associated with one domain name and mechanism to handle a situation when one of the servers is not working. Therefore, there is a fundamental need to develop strategy for dynamic load allocation on web side. In this paper, an effort has been made to introduce a cluster based frame work to solve load distribution problem. This framework aims to distribute load among clusters on the basis of their operational capabilities. Moreover, the experimental results are shown with the help of example, algorithm and analysis of the algorithm. 	
1103.1897v1	http://arxiv.org/pdf/1103.1897v1	2011	On the Properties of Hydrogen Terminated Diamond as a Photocathode	Jonathan Rameau|John Smedley|Eric Muller|Tim Kidd|Peter Johnson	  Electron emission from the negative electron affinity (NEA) surface of hydrogen terminated, boron doped diamond in the [100] orientation is investigated using angle resolved photoemission spectroscopy (ARPES). ARPES measurements using 16 eV synchrotron and 6 eV laser light are compared and found to show a catastrophic failure of the sudden approximation. While the high energy photoemission is found to yield little information regarding the NEA, low energy laser ARPES reveals for the first time that the NEA results from a novel Franck-Condon mechanism coupling electrons in the conduction band to the vacuum. The result opens the door to development of a new class of NEA electron emitters based on this effect. 	
1104.0121v1	http://arxiv.org/pdf/1104.0121v1	2011	Complex network analysis of water distribution systems	A. Yazdani|P. Jeffrey	  This paper explores a variety of strategies for understanding the formation, structure, efficiency and vulnerability of water distribution networks. Water supply systems are studied as spatially organized networks for which the practical applications of abstract evaluation methods are critically evaluated. Empirical data from benchmark networks are used to study the interplay between network structure and operational efficiency, reliability and robustness. Structural measurements are undertaken to quantify properties such as redundancy and optimal-connectivity, herein proposed as constraints in network design optimization problems. The role of the supply-demand structure towards system efficiency is studied and an assessment of the vulnerability to failures based on the disconnection of nodes from the source(s) is undertaken. The absence of conventional degree-based hubs (observed through uncorrelated non-heterogeneous sparse topologies) prompts an alternative approach to studying structural vulnerability based on the identification of network cut-sets and optimal connectivity invariants. A discussion on the scope, limitations and possible future directions of this research is provided. 	
1104.4209v2	http://arxiv.org/pdf/1104.4209v2	2012	Modeling the clustering in citation networks	Fu-Xin Ren|Xue-Qi Cheng|Hua-Wei Shen	  For the study of citation networks, a challenging problem is modeling the high clustering. Existing studies indicate that the promising way to model the high clustering is a copying strategy, i.e., a paper copies the references of its neighbour as its own references. However, the line of models highly underestimates the number of abundant triangles observed in real citation networks and thus cannot well model the high clustering. In this paper, we point out that the failure of existing models lies in that they do not capture the connecting patterns among existing papers. By leveraging the knowledge indicated by such connecting patterns, we further propose a new model for the high clustering in citation networks. Experiments on two real world citation networks, respectively from a special research area and a multidisciplinary research area, demonstrate that our model can reproduce not only the power-law degree distribution as traditional models but also the number of triangles, the high clustering coefficient and the size distribution of co-citation clusters as observed in these real networks. 	
1106.2275v1	http://arxiv.org/pdf/1106.2275v1	2011	Byzantine Fault Tolerance of Regenerating Codes	Frédérique Oggier|Anwitaman Datta	  Recent years have witnessed a slew of coding techniques custom designed for networked storage systems. Network coding inspired regenerating codes are the most prolifically studied among these new age storage centric codes. A lot of effort has been invested in understanding the fundamental achievable trade-offs of storage and bandwidth usage to maintain redundancy in presence of different models of failures, showcasing the efficacy of regenerating codes with respect to traditional erasure coding techniques. For practical usability in open and adversarial environments, as is typical in peer-to-peer systems, we need however not only resilience against erasures, but also from (adversarial) errors. In this paper, we study the resilience of generalized regenerating codes (supporting multi-repairs, using collaboration among newcomers) in the presence of two classes of Byzantine nodes, relatively benign selfish (non-cooperating) nodes, as well as under more active, malicious polluting nodes. We give upper bounds on the resilience capacity of regenerating codes, and show that the advantages of collaborative repair can turn to be detrimental in the presence of Byzantine nodes. We further exhibit that system mechanisms can be combined with regenerating codes to mitigate the effect of rogue nodes. 	
1106.4090v1	http://arxiv.org/pdf/1106.4090v1	2011	Discovery of Invariants through Automated Theory Formation	Maria Teresa Llano|Andrew Ireland|Alison Pease	  Refinement is a powerful mechanism for mastering the complexities that arise when formally modelling systems. Refinement also brings with it additional proof obligations -- requiring a developer to discover properties relating to their design decisions. With the goal of reducing this burden, we have investigated how a general purpose theory formation tool, HR, can be used to automate the discovery of such properties within the context of Event-B. Here we develop a heuristic approach to the automatic discovery of invariants and report upon a series of experiments that we undertook in order to evaluate our approach. The set of heuristics developed provides systematic guidance in tailoring HR for a given Event-B development. These heuristics are based upon proof-failure analysis, and have given rise to some promising results. 	
1108.0579v1	http://arxiv.org/pdf/1108.0579v1	2011	Damage of Cross-Linked Rubbers as the Scission of Polymer Chains:   Modeling and Tensile Experiments	Alexei Y. Melnikov|A. I. Leonov	  This paper develops a damage model for unfilled cross-linked rubbers based on the concept of scission of polymer chains. The model is built up on the well-known Gent elastic potential complemented by a kinetic equation describing effects of polymer chain scission. The macroscopic parameters in the damage model are evaluated through the parameters for undamaged elastomer. Qualitative analysis of changing molecular parameters of rubbers under scission of polymer chains resulted in easy scaling modeling the dependences of these parameters on the damage factor. It makes possible to predict the rubber failure in molecular terms as mechanical de-vulcanization. The model was tested in tensile quasi-static experiments with both the monotonous loading and repeated loading-unloading. 	
1108.1545v1	http://arxiv.org/pdf/1108.1545v1	2011	Damage of Cross-Linked Rubbers as the Scission of Polymer Chains:   Modeling and Tensile Experiments, Report 1	Alexei Y. Melnikov|A. I. Leonov	  This paper develops a damage model for unfilled cross-linked rubbers based on the concept of scission of polymer chains. The model is built up on the well-known Gent elastic potential complemented by a kinetic equation describing effects of polymer chain scission. The macroscopic parameters in the damage model are evaluated through the parameters for undamaged elastomer. Qualitative analysis of changing molecular parameters of rubbers under scission of polymer chains resulted in easy scaling modeling the dependences of these parameters on the damage factor. It makes possible to predict the rubber failure in molecular terms as mechanical de-vulcanization. The model was tested in tensile quasi-static experiments with both the monotonous loading and repeated loading-unloading. 	
1108.2570v1	http://arxiv.org/pdf/1108.2570v1	2011	Hardness of T-carbon: Density functional theory calculations	Xing-Qiu Chen|Haiyang Niu|Cesare Franchini|Dianzhong Li|Yiyi Li	  We revisit and interpret the mechanical properties of the recently proposed allotrope of carbon, T-carbon [Sheng \emph{et al.}, Phys. Rev. Lett., \textbf{106}, 155703 (2011)], using density functional theory in combination with different empirical hardness models. In contrast with the early estimation based on the Gao's model, which attributes to T-carbon an high Vickers hardness of 61 GPa comparable to that of superhard cubic boron nitride (\emph{c}-BN), we find that T-carbon is not a superhard material, since its Vickers hardenss does not exceed 10 GPa. Besides providing clear evidence for the absence of superhardenss in T-carbon, we discuss the physical reasons behind the failure of Gao's and \v{S}im$\rm\mathring{u}$nek and Vack\'a\v{r}'s (SV) models in predicting the hardness of T-carbon, residing on their improper treatment of the highly anisotropic distribution of quasi-\emph{sp}$^3$-like C-C hybrids. A possible remedy to the Gao and SV models based on the concept of superatom is suggest, which indeed yields a Vickers hardness of about 8 GPa. 	
1109.0839v1	http://arxiv.org/pdf/1109.0839v1	2011	Percolation on correlated random networks	Elena Agliari|Claudia Cioli|Enore Guadagnini	  We consider a class of random, weighted networks, obtained through a redefinition of patterns in an Hopfield-like model and, by performing percolation processes, we get information about topology and resilience properties of the networks themselves. Given the weighted nature of the graphs, different kinds of bond percolation can be studied: stochastic (deleting links randomly) and deterministic (deleting links based on rank weights), each mimicking a different physical process. The evolution of the network is accordingly different, as evidenced by the behavior of the largest component size and of the distribution of cluster sizes. In particular, we can derive that weak ties are crucial in order to maintain the graph connected and that, when they are the most prone to failure, the giant component typically shrinks without abruptly breaking apart; these results have been recently evidenced in several kinds of social networks. 	
1110.1449v1	http://arxiv.org/pdf/1110.1449v1	2011	Teleportation of the one-qubit state with environment-disturbed recovery   operations	Ming-Liang Hu	  We study standard protocol $\mathcal{P}_0$ for teleporting the one-qubit state with both the transmission process of the two qubits constitute the quantum channel and the recovery operations performed by Bob disturbed by the decohering environment. The results revealed that Bob's imperfect operations do not eliminate the possibility of nonclassical teleportation fidelity provided he shares an ideal channel state with Alice, while the transmission process is constrained by a critical time $t_{0,c}$ longer than which will result in failure of $\mathcal{P}_0$ if the two qubits are corrupted by the decohering environment. Moreover, we found that under the condition of the same decoherence rate $\gamma$, the teleportation protocol is significantly more fragile when it is executed under the influence of the noisy environment than those under the influence of the dissipative and dephasing environments. 	
1110.3832v1	http://arxiv.org/pdf/1110.3832v1	2011	Distributed flow optimization and cascading effects in weighted complex   networks	Andrea Asztalos|Sameet Sreenivasan|Boleslaw K. Szymanski|G. Korniss	  We investigate the effect of a specific edge weighting scheme $\sim (k_i k_j)^{\beta}$ on distributed flow efficiency and robustness to cascading failures in scale-free networks. In particular, we analyze a simple, yet fundamental distributed flow model: current flow in random resistor networks. By the tuning of control parameter $\beta$ and by considering two general cases of relative node processing capabilities as well as the effect of bandwidth, we show the dependence of transport efficiency upon the correlations between the topology and weights. By studying the severity of cascades for different control parameter $\beta$, we find that network resilience to cascading overloads and network throughput is optimal for the same value of $\beta$ over the range of node capacities and available bandwidth. 	
1110.5246v1	http://arxiv.org/pdf/1110.5246v1	2011	Fluctuation-induced traffic congestion in heterogeneous networks	A. S. Stepanenko|I. V. Yurkevich|C. C. Constantinou|I. V. Lerner	  In studies of complex heterogeneous networks, particularly of the Internet, significant attention was paid to analyzing network failures caused by hardware faults or overload, where the network reaction was modeled as rerouting of traffic away from failed or congested elements. Here we model another type of the network reaction to congestion -- a sharp reduction of the input traffic rate through congested routes which occurs on much shorter time scales. We consider the onset of congestion in the Internet where local mismatch between demand and capacity results in traffic losses and show that it can be described as a phase transition characterized by strong non-Gaussian loss fluctuations at a mesoscopic time scale. The fluctuations, caused by noise in input traffic, are exacerbated by the heterogeneous nature of the network manifested in a scale-free load distribution. They result in the network strongly overreacting to the first signs of congestion by significantly reducing input traffic along the communication paths where congestion is utterly negligible. 	
1111.2452v1	http://arxiv.org/pdf/1111.2452v1	2011	A nonlinear symmetry breaking effect in shear cracks	Roi Harpaz|Eran Bouchbinder	  Shear cracks propagation is a basic dynamical process that mediates interfacial failure. We develop a general weakly nonlinear elastic theory of shear cracks and show that these experience tensile-mode crack tip deformation, including possibly opening displacements, in agreement with Stephenson's prediction. We quantify this nonlinear symmetry breaking effect, under two-dimensional deformation conditions, by an explicit inequality in terms of the first and second order elastic constants in the quasi-static regime and semi-analytic calculations in the fully dynamic regime. Our general results are applied to various materials. Finally, we discuss available works in the literature and note the potential relevance of elastic nonlinearities for frictional cracks. 	
1112.0387v2	http://arxiv.org/pdf/1112.0387v2	2012	Sandpiles on multiplex networks	Kyu-Min Lee|K. -I. Goh|I. -M. Kim	  We introduce the sandpile model on multiplex networks with more than one type of edge and investigate its scaling and dynamical behaviors. We find that the introduction of multiplexity does not alter the scaling behavior of avalanche dynamics; the system is critical with an asymptotic power-law avalanche size distribution with an exponent $\tau = 3/2$ on duplex random networks. The detailed cascade dynamics, however, is affected by the multiplex coupling. For example, higher-degree nodes such as hubs in scale-free networks fail more often in the multiplex dynamics than in the simplex network counterpart in which different types of edges are simply aggregated. Our results suggest that multiplex modeling would be necessary in order to gain a better understanding of cascading failure phenomena of real-world multiplex complex systems, such as the global economic crisis. 	
1112.2067v1	http://arxiv.org/pdf/1112.2067v1	2011	Ontology-Based Emergency Management System in a Social Cloud	Bhuvaneswari. A|Dr. G. R. Karpagam	  The need for Emergency Management continually grows as the population and exposure to catastrophic failures increase. The ability to offer appropriate services at these emergency situations can be tackled through group communication mechanisms. The entities involved in the group communication include people, organizations, events, locations and essential services. Cloud computing is a "as a service" style of computing that enables on-demand network access to a shared pool of resources. So this work focuses on proposing a social cloud constituting group communication entities using an open source platform, Eucalyptus. The services are exposed as semantic web services, since the availability of machine-readable metadata (Ontology) will enable the access of these services more intelligently. The objective of this paper is to propose an Ontology-based Emergency Management System in a social cloud and demonstrate the same using emergency healthcare domain. 	
1112.4231v2	http://arxiv.org/pdf/1112.4231v2	2012	Student Understanding of Taylor Series Expansions in Statistical   Mechanics	Trevor I. Smith|John R. Thompson|Donald B. Mountcastle	  One goal of physics instruction is to have students learn to make physical meaning of specific mathematical ideas, concepts, and procedures in different physical settings. As part of research investigating student learning in statistical physics, we are developing curriculum materials that guide students through a derivation of the Boltzmann factor, using a Taylor series expansion of entropy. Using results from written surveys, classroom observations, and both individual think-aloud and teaching interviews, we present evidence that many students can recognize and interpret series expansions, but they often lack fluency with the Taylor series despite previous exposures in both calculus and physics courses. We present students' successes and failures both using and interpreting Taylor series expansions in a variety of contexts. 	
1112.5667v1	http://arxiv.org/pdf/1112.5667v1	2011	Arithmetic of Potts model hypersurfaces	Matilde Marcolli|Jessica Su	  We consider Potts model hypersurfaces defined by the multivariate Tutte polynomial of graphs (Potts model partition function). We focus on the behavior of the number of points over finite fields for these hypersurfaces, in comparison with the graph hypersurfaces of perturbative quantum field theory defined by the Kirchhoff graph polynomial. We give a very simple example of the failure of the "fibration condition" in the dependence of the Grothendieck class on the number of spin states and of the polynomial countability condition for these Potts model hypersurfaces. We then show that a period computation, formally similar to the parametric Feynman integrals of quantum field theory, arises by considering certain thermodynamic averages. One can show that these evaluate to combinations of multiple zeta values for Potts models on polygon polymer chains, while silicate tetrahedral chains provide a candidate for a possible occurrence of non-mixed Tate periods. 	
1202.6049v1	http://arxiv.org/pdf/1202.6049v1	2012	Attack Detection and Identification in Cyber-Physical Systems -- Part   II: Centralized and Distributed Monitor Design	Fabio Pasqualetti|Florian Dörfler|Francesco Bullo	  Cyber-physical systems integrate computation, communication, and physical capabilities to interact with the physical world and humans. Besides failures of components, cyber-physical systems are prone to malicious attacks so that specific analysis tools and monitoring mechanisms need to be developed to enforce system security and reliability. This paper builds upon the results presented in our companion paper [1] and proposes centralized and distributed monitors for attack detection and identification. First, we design optimal centralized attack detection and identification monitors. Optimality refers to the ability of detecting (respectively identifying) every detectable (respectively identifiable) attack. Second, we design an optimal distributed attack detection filter based upon a waveform relaxation technique. Third, we show that the attack identification problem is computationally hard, and we design a sub-optimal distributed attack identification procedure with performance guarantees. Finally, we illustrate the robustness of our monitors to system noise and unmodeled dynamics through a simulation study. 	
1202.6144v2	http://arxiv.org/pdf/1202.6144v2	2012	Attack Detection and Identification in Cyber-Physical Systems -- Part I:   Models and Fundamental Limitations	Fabio Pasqualetti|Florian Dörfler|Francesco Bullo	  Cyber-physical systems integrate computation, communication, and physical capabilities to interact with the physical world and humans. Besides failures of components, cyber-physical systems are prone to malignant attacks, and specific analysis tools as well as monitoring mechanisms need to be developed to enforce system security and reliability. This paper proposes a unified framework to analyze the resilience of cyber-physical systems against attacks cast by an omniscient adversary. We model cyber-physical systems as linear descriptor systems, and attacks as exogenous unknown inputs. Despite its simplicity, our model captures various real-world cyber-physical systems, and it includes and generalizes many prototypical attacks, including stealth, (dynamic) false-data injection and replay attacks. First, we characterize fundamental limitations of static, dynamic, and active monitors for attack detection and identification. Second, we provide constructive algebraic conditions to cast undetectable and unidentifiable attacks. Third, by using the system interconnection structure, we describe graph-theoretic conditions for the existence of undetectable and unidentifiable attacks. Finally, we validate our findings through some illustrative examples with different cyber-physical systems, such as a municipal water supply network and two electrical power grids. 	
1203.1979v2	http://arxiv.org/pdf/1203.1979v2	2012	Icebergs in the Clouds: the Other Risks of Cloud Computing	Bryan Ford	  Cloud computing is appealing from management and efficiency perspectives, but brings risks both known and unknown. Well-known and hotly-debated information security risks, due to software vulnerabilities, insider attacks, and side-channels for example, may be only the "tip of the iceberg." As diverse, independently developed cloud services share ever more fluidly and aggressively multiplexed hardware resource pools, unpredictable interactions between load-balancing and other reactive mechanisms could lead to dynamic instabilities or "meltdowns." Non-transparent layering structures, where alternative cloud services may appear independent but share deep, hidden resource dependencies, may create unexpected and potentially catastrophic failure correlations, reminiscent of financial industry crashes. Finally, cloud computing exacerbates already-difficult digital preservation challenges, because only the provider of a cloud-based application or service can archive a "live," functional copy of a cloud artifact and its data for long-term cultural preservation. This paper explores these largely unrecognized risks, making the case that we should study them before our socioeconomic fabric becomes inextricably dependent on a convenient but potentially unstable computing model. 	
1203.2062v1	http://arxiv.org/pdf/1203.2062v1	2012	Meta-models for structural reliability and uncertainty quantification	Bruno Sudret	  A meta-model (or a surrogate model) is the modern name for what was traditionally called a response surface. It is intended to mimic the behaviour of a computational model M (e.g. a finite element model in mechanics) while being inexpensive to evaluate, in contrast to the original model which may take hours or even days of computer processing time. In this paper various types of meta-models that have been used in the last decade in the context of structural reliability are reviewed. More specifically classical polynomial response surfaces, polynomial chaos expansions and kriging are addressed. It is shown how the need for error estimates and adaptivity in their construction has brought this type of approaches to a high level of efficiency. A new technique that solves the problem of the potential biasedness in the estimation of a probability of failure through the use of meta-models is finally presented. 	
1203.6154v1	http://arxiv.org/pdf/1203.6154v1	2012	Finite element modelling of shock-induced damages on ceramic hip   prostheses	Juliana Uribe|Jérôme Hausselle|Jean Geringer	  The aim of this work was to simulate the behaviour of hip prostheses under mechanical shocks. When hip joint is replaced by prosthesis, during the swing phase of the leg, a microseparation between the prosthetic head and the cup could occur. Two different sizes of femoral heads were studied: 28 and 32 mm diameter, made, respectively, in alumina and zirconia. The shock-induced stress was determined numerically using finite element analysis (FEA), Abaqus software. The influence of inclination, force, material, and microseparation was studied. In addition, an algorithm was developed from a probabilistic model, Todinov's approach, to predict lifetime of head and cup. Simulations showed maximum tensile stresses were reached on the cup's surfaces near to rim. The worst case was the cup-head mounted at 30^{\circ}. All simulations and tests showed bulk zirconia had a greater resistance to shocks than bulk alumina. The probability of failure could be bigger than 0.9 when a porosity greater than 0.7% vol. is present in the material. Simulating results showed good agreement with experimental results. The tests and simulations are promising for predicting the lifetime of ceramic prostheses. 	
1204.3888v1	http://arxiv.org/pdf/1204.3888v1	2012	Sustainable institutionalized punishment requires elimination of   second-order free-riders	Matjaz Perc	  Although empirical and theoretical studies affirm that punishment can elevate collaborative efforts, its emergence and stability remain elusive. By peer-punishment the sanctioning is something an individual elects to do depending on the strategies in its neighborhood. The consequences of unsustainable efforts are therefore local. By pool-punishment, on the other hand, where resources for sanctioning are committed in advance and at large, the notion of sustainability has greater significance. In a population with free-riders, punishers must be strong in numbers to keep the "punishment pool" from emptying. Failure to do so renders the concept of institutionalized sanctioning futile. We show that pool-punishment in structured populations is sustainable, but only if second-order free-riders are sanctioned as well, and to a such degree that they cannot prevail. A discontinuous phase transition leads to an outbreak of sustainability when punishers subvert second-order free-riders in the competition against defectors. 	
1204.4822v2	http://arxiv.org/pdf/1204.4822v2	2012	Entropy production from stochastic dynamics in discrete full phase space	Ian J. Ford|Richard E. Spinney	  The stochastic entropy generated during the evolution of a system interacting with an environment may be separated into three components, but only two of these have a non-negative mean. The third component of entropy production is associated with the relaxation of the system probability distribution towards a stationary state and with nonequilibrium constraints within the dynamics that break detailed balance. It exists when at least some of the coordinates of the system phase space change sign under time reversal, and when the stationary state is asymmetric in these coordinates. We illustrate the various components of entropy production, both in detail for particular trajectories and in the mean, using simple systems defined on a discrete phase space of spatial and velocity coordinates. These models capture features of the drift and diffusion of a particle in a physical system, including the processes of injection and removal and the effect of a temperature gradient. The examples demonstrate how entropy production in stochastic thermodynamics depends on the detail that is included in a model of the dynamics of a process. Entropy production from such a perspective is a measure of the failure of such models to meet Loschmidt's expectation of dynamic reversibility. 	
1205.1428v1	http://arxiv.org/pdf/1205.1428v1	2012	High Velocity Penetration/Perforation Using Coupled Smooth Particle   Hydrodynamics-Finite Element Method	S. Swaddiwudhipong|M. J. Islam|Z. S. Liu	  Finite element method (FEM) suffers from a serious mesh distortion problem when used for high velocity impact analyses. The smooth particle hydrodynamics (SPH) method is appropriate for this class of problems involving severe damages but at considerable computational cost. It is beneficial if the latter is adopted only in severely distorted regions and FEM further away. The coupled smooth particle hydrodynamics - finite element method (SFM) has been adopted in a commercial hydrocode LS-DYNA to study the perforation of Weldox 460E steel and AA5083-H116 aluminum plates with varying thicknesses and various projectile nose geometries including blunt, conical and ogival noses. Effects of the SPH domain size and particle density are studied considering the friction effect between the projectile and the target materials. The simulated residual velocities and the ballistic limit velocities from the SFM agree well with the published experimental data. The study shows that SFM is able to emulate the same failure mechanisms of the steel and aluminum plates as observed in various experimental investigations for initial impact velocity of 170 m/s and higher. 	
1205.1826v1	http://arxiv.org/pdf/1205.1826v1	2012	Ion irradiation tolerance of graphene as studied by atomistic   simulations	E. H. Åhlgren|J. Kotakoski|O. Lehtinen|A. V. Krasheninnikov	  As impermeable to gas molecules and at the same time transparent to high-energy ions, graphene has been suggested as a window material for separating a high-vacuum ion beam system from targets kept at ambient conditions. However, accumulation of irradiation-induced damage in the graphene membrane may give rise to its mechanical failure. Using atomistic simulations, we demonstrate that irradiated graphene even with a high vacancy concentration does not show signs of such instability, indicating a considerable robustness of graphene windows. We further show that upper and lower estimates for the irradiation damage in graphene can be set using a simple model. 	
1205.6440v1	http://arxiv.org/pdf/1205.6440v1	2012	Monitoring Software Reliability using Statistical Process Control An   Ordered Statistics Approach	Bandla Srinivasa Rao|R. Satya Prasad|R. R. L. Kantham	  The nature and complexity of software have changed significantly in the last few decades. With the easy availability of computing power, deeper and broader applications are made. It has been extremely necessary to produce good quality software with high precession of reliability right in the first place. Olden day's software errors and bugs were fixed at a later stage in the software development. Today to produce high quality reliable software and to keep a specific time schedule is a big challenge. To cope up the challenge many concepts, methodology and practices of software engineering have been evolved for developing reliable software. Better methods of controlling the process of software production are underway. One of such methods to assess the software reliability is using control charts. In this paper we proposed an NHPP based control mechanism by using order statistics with cumulative quantity between observations of failure data using mean value function of exponential distribution. 	
1206.0103v1	http://arxiv.org/pdf/1206.0103v1	2012	Cooperation in Carrier Sense Based Wireless Ad Hoc Networks - Part I:   Reactive Schemes	Andrea Munari|Marco Levorato|Michele Zorzi	  Cooperative techniques have been shown to significantly improve the performance of wireless systems. Despite being a mature technology in single communication link scenarios, their implementation in wider, and practical, networks poses several challenges which have not been fully identified and understood so far. In this two-part paper, the implementation of cooperative communications in non-centralized ad hoc networks with sensing-based channel access is extensively discussed. Both analysis and simulation are employed to provide a clear understanding of the mutual influence between the link layer contention mechanism and collaborative protocols. Part I of this work focuses on reactive cooperation, in which relaying is triggered by packet delivery failure events, while Part II addresses proactive approaches, preemptively initiated by the source based on channel state information. Results show that sensing-based channel access significantly hampers the effectiveness of cooperation by biasing the spatial distribution of available relays, and by inducing a level of spatial and temporal correlation of the interference that diminishes the diversity improvement on which cooperative gains are founded. Moreover, the efficiency reduction entailed by several practical protocol issues related to carrier sense multiple access which are typically neglected in the literature is thoroughly investigated. 	
1206.1918v1	http://arxiv.org/pdf/1206.1918v1	2012	Routing Protocols for Mobile and Vehicular Ad-Hoc Networks: A   Comparative Analysis	Preetida Vinayakray-Jani|Sugata Sanyal	  We present comparative analysis of MANET (Mobile Ad-Hoc Network) and VANET (Vehicular Ad-Hoc Network) routing protocols, in this paper. The analysis is based on various design factors. The traditional routing protocols of AODV (Ad hoc On-Demand Distance Vector), DSR (Dynamic Source Routing), and DSDV (Destination-Sequenced Distance-Vector) of MANET are utilizing node centric routing which leads to frequent breaking of routes, causing instability in routing. Usage of these protocols in high mobility environment like VANET may eventually cause many packets to drop. Route repairs and failures notification overheads increase significantly leading to low throughput and long delays. Such phenomenon is not suitable for Vehicular Ad hoc Networks (VANET) due to high mobility of nodes where network can be dense or sparse. Researchers have proposed various routing algorithms or mechanism for MANET and VANET. This paper describes the relevant protocols, associated algorithm and the strength and weakness of these routing protocols. 	
1206.2187v1	http://arxiv.org/pdf/1206.2187v1	2012	An Empirical Study of the Repair Performance of Novel Coding Schemes for   Networked Distributed Storage Systems	Lluis Pamies-Juarez|Frédérique Oggier|Anwitaman Datta	  Erasure coding techniques are getting integrated in networked distributed storage systems as a way to provide fault-tolerance at the cost of less storage overhead than traditional replication. Redundancy is maintained over time through repair mechanisms, which may entail large network resource overheads. In recent years, several novel codes tailor-made for distributed storage have been proposed to optimize storage overhead and repair, such as Regenerating Codes that minimize the per repair traffic, or Self-Repairing Codes which minimize the number of nodes contacted per repair. Existing studies of these coding techniques are however predominantly theoretical, under the simplifying assumption that only one object is stored. They ignore many practical issues that real systems must address, such as data placement, de/correlation of multiple stored objects, or the competition for limited network resources when multiple objects are repaired simultaneously. This paper empirically studies the repair performance of these novel storage centric codes with respect to classical erasure codes by simulating realistic scenarios and exploring the interplay of code parameters, failure characteristics and data placement with respect to the trade-offs of bandwidth usage and speed of repairs. 	
1207.3822v1	http://arxiv.org/pdf/1207.3822v1	2012	Mechanical design of ceramic beam tube braze joints for NOvA kicker   magnets	C. R. Ader|R. E. Reilly|J. H. Wilson	  The NO{\nu}A Experiment will construct a detector optimized for electron neutrino detection in the existing NuMI neutrino beam. The NuMI beam line is capable of operating at 400 kW of primary beam power and the upgrade will allow up to 700 kW. Ceramic beam tubes are utilized in numerous kicker magnets in different accelerator rings at Fermi National Accelerator Laboratory. Kovar flanges are brazed onto each beam tube end, since kovar and high alumina ceramic have similar expansion curves. The tube, kovar flange, end piece, and braze foil alloy brazing material are stacked in the furnace and then brazed. The most challenging aspect of fabricating kicker magnets in recent years have been making hermetic vacuum seals on the braze joints between the ceramic and flange. Numerous process variables can influence the robustness of conventional metal/ceramic brazing processes. The ceramic-filler metal interface is normally the weak layer when failure does not occur within the ceramic. Differences between active brazing filler metal and the moly-manganese process will be discussed along with the applicable results of these techniques used for Fermilab production kicker tubes. 	
1208.4172v1	http://arxiv.org/pdf/1208.4172v1	2012	Transaction Log Based Application Error Recovery and Point In-Time Query	Tomas Talius|Robin Dhamankar|Andrei Dumitrache|Hanuma Kodavalla	  Database backups have traditionally been used as the primary mechanism to recover from hardware and user errors. High availability solutions maintain redundant copies of data that can be used to recover from most failures except user or application errors. Database backups are neither space nor time efficient for recovering from user errors which typically occur in the recent past and affect a small portion of the database. Moreover periodic full backups impact user workload and increase storage costs. In this paper we present a scheme that can be used for both user and application error recovery starting from the current state and rewinding the database back in time using the transaction log. While we provide a consistent view of the entire database as of a point in time in the past, the actual prior versions are produced only for data that is accessed. We make the as of data accessible to arbitrary point in time queries by integrating with the database snapshot feature in Microsoft SQL Server. 	
1210.5913v1	http://arxiv.org/pdf/1210.5913v1	2012	Bio-Thentic Card: Authentication concept for RFID Card	Ikuesan R. Adeyemi|Norafida Bt Ithnin	  Radio frequency identification (RFID) is a technology that employs basic identifier of an object embedded in a chip, transmitted via radio wave, for identification. An RFID Card responds to query or interrogation irrespective of "Who" holds the Card; like a key to a door. Since an attacker can possess the card, access to such object can therefore be easily compromised. This security breach is classified as an unauthorized use of Card, and it forms the bedrock for RFID Card compromise especially in access control. As an on-card authentication mechanism, this research proposed a concept termed Bio-Thentic Card, which can be adopted to prevent this single point of failure of RFID Card. The Bio-Thentic Card was fabricated, tested and assessed in line with the known threats, and attacks; and it was observed to proffer substantive solution to unauthorized use of RFID Card vulnerability 	
1210.8372v1	http://arxiv.org/pdf/1210.8372v1	2012	Electronic Strengthening of Graphene by Charge Doping	Chen Si|Wenhui Duan|Zheng Liu|Feng Liu	  Graphene is known as the strongest 2D material in nature, yet we show that moderate charge doping of either electrons or holes can further enhance its ideal strength by up to ~17%, based on first principles calculations. This unusual electronic enhancement, versus conventional structural enhancement, of material's strength is achieved by an intriguing physical mechanism of charge doping counteracting on strain induced enhancement of Kohn anomaly, which leads to an overall stiffening of zone boundary K1 phonon mode whose softening under strain is responsible for graphene failure. Electrons and holes work in the same way due to the high electron-hole symmetry around the Dirac point of graphene, while over doping may weaken the graphene by softening other phonon modes. Our findings uncover another fascinating property of graphene with broad implications in graphene-based electromechanical devices. 	
1211.6796v1	http://arxiv.org/pdf/1211.6796v1	2012	Polymer Welding: Strength Through Entanglements	Ting Ge|Flint Pierce|Dvora Perahia|Gary S. Grest|Mark O. Robbins	  Large-scale simulations of thermal welding of polymers are performed to investigate the rise of mechanical strength at the polymer-polymer interface with the welding time. The welding process is in the core of integrating polymeric elements into devices as well as in thermal induced healing of polymers; processes that require development of interfacial strength equal to that of the bulk. Our simulations show that the interfacial strength saturates at the bulk shear strength much before polymers diffuse by their radius of gyration. Along with the strength increase, the dominant failure mode changes from chain pullout at the interface to chain scission as in the bulk. Formation of sufficient entanglements across the interface, which we track using a Primitive Path Analysis is required to arrest catastrophic chain pullout at the interface. The bulk response is not fully recovered until the density of entanglements at the interface reaches the bulk value. Moreover, the increase of interfacial strength before saturation is proportional to the number of interfacial entanglements between chains from opposite sides. 	
1212.1551v2	http://arxiv.org/pdf/1212.1551v2	2013	From microstructural features to effective toughness in disordered   brittle solids	Vincent Démery|Laurent Ponson|Alberto Rosso	  The relevant parameters at the microstructure scale that govern the macroscopic toughness of disordered brittle materials are investigated theoretically. We focus on planar crack propagation and describe the front evolution as the propagation of a long-range elastic line within a plane with random distribution of toughness. Our study reveals two regimes: in the collective pinning regime, the macroscopic toughness can be expressed as a function of a few parameters only, namely the average and the standard deviation of the local toughness distribution and the correlation lengths of the heterogeneous toughness field; in the individual pinning regime, the passage from micro to macroscale is more subtle and the full distribution of local toughness is required to be predictive. Beyond the failure of brittle solids, our findings illustrate the complex filtering process of microscale quantities towards the larger scales into play in a broad range of systems governed by the propagation of an elastic interface in a disordered medium. 	
1301.6115v1	http://arxiv.org/pdf/1301.6115v1	2013	DebtRank-transparency: Controlling systemic risk in financial networks	Stefan Thurner|Sebastian Poledna	  Banks in the interbank network can not assess the true risks associated with lending to other banks in the network, unless they have full information on the riskiness of all the other banks. These risks can be estimated by using network metrics (for example DebtRank) of the interbank liability network which is available to Central Banks. With a simple agent based model we show that by increasing transparency by making the DebtRank of individual nodes (banks) visible to all nodes, and by imposing a simple incentive scheme, that reduces interbank borrowing from systemically risky nodes, the systemic risk in the financial network can be drastically reduced. This incentive scheme is an effective regulation mechanism, that does not reduce the efficiency of the financial network, but fosters a more homogeneous distribution of risk within the system in a self-organized critical way. We show that the reduction of systemic risk is to a large extent due to the massive reduction of cascading failures in the transparent system. An implementation of this minimal regulation scheme in real financial networks should be feasible from a technical point of view. 	
1301.6375v3	http://arxiv.org/pdf/1301.6375v3	2013	Increased Network Interdependency Leads to Aging	Dervis Can Vural|Greg Morrison|L. Mahadevan	  Although species longevity is subject to a diverse range of selective forces, the mortality curves of a wide variety of organisms are rather similar. We argue that aging and its universal characteristics may have evolved by means of a gradual increase in the systemic interdependence between a large collection of biochemical or mechanical components. Modeling the organism as a dependency network which we create using a constructive evolutionary process, we age it by allowing nodes to be broken or repaired according to a probabilistic algorithm that accounts for random failures/repairs and dependencies. Our simulations show that the network slowly accumulates damage and then catastrophically collapses. We use our simulations to fit experimental data for the time dependent mortality rates of a variety of multicellular organisms and even complex machines such as automobiles. Our study suggests that aging is an emergent finite-size effect in networks with dynamical dependencies and that the qualitative and quantitative features of aging are not sensitively dependent on the details of system structure. 	
1302.2418v1	http://arxiv.org/pdf/1302.2418v1	2013	Effective Mean Field Approach to Kinetic Monte Carlo Simulations in   Limit Cycle Dynamics with Reactive and Diffusive Rewiring	E. Panagakou|G. C. Boulougouris|A. Provata	  The dynamics of complex reactive schemes is known to deviate from the Mean Field (MF) theory when restricted on low dimensional spatial supports. This failure has been attributed to the limited number of species-neighbours which are available for interactions. In the current study, we introduce effective reactive parameters, which depend on the type of the spatial support and which allow for an effective MF description. As working example the Lattice Limit Cycle dynamics is used, restricted on a 2D square lattice with nearest neighbour interactions. We show that the MF steady state results are recovered when the kinetic rates are replaced with their effective values. The same conclusion holds when reactive stochastic rewiring is introduced in the system via long distance reactive coupling. Instead, when the stochastic coupling becomes diffusive the effective parameters no longer predict the steady state. This is attributed to the diffusion process which is an additional factor introduced into the dynamics and is not accounted for, in the kinetic MF scheme. 	
1303.0965v1	http://arxiv.org/pdf/1303.0965v1	2013	On the validity of the method of reduction of dimensionality: area of   contact, average interfacial separation and contact stiffness	I. A. Lyashenko|Lars Pastewka|Bo N. J. Persson	  It has recently been suggested that many contact mechanics problems between solids can be accurately studied by mapping the problem on an effective one dimensional (1D) elastic foundation model. Using this 1D mapping we calculate the contact area and the average interfacial separation between elastic solids with nominally flat but randomly rough surfaces. We show, by comparison to exact numerical results, that the 1D mapping method fails even qualitatively. We also calculate the normal interfacial stiffness $K$ and compare it with the result of an analytical study. We attribute the failure of the elastic foundation model to the neglect of the long-range elastic coupling between the asperity contact regions. 	
1304.2362v1	http://arxiv.org/pdf/1304.2362v1	2013	A Comparison of Decision Analysis and Expert Rules for Sequential   Diagnosis	Jayant Kalagnanam|Max Henrion	  There has long been debate about the relative merits of decision theoretic methods and heuristic rule-based approaches for reasoning under uncertainty. We report an experimental comparison of the performance of the two approaches to troubleshooting, specifically to test selection for fault diagnosis. We use as experimental testbed the problem of diagnosing motorcycle engines. The first approach employs heuristic test selection rules obtained from expert mechanics. We compare it with the optimal decision analytic algorithm for test selection which employs estimated component failure probabilities and test costs. The decision analytic algorithm was found to reduce the expected cost (i.e. time) to arrive at a diagnosis by an average of 14% relative to the expert rules. Sensitivity analysis shows the results are quite robust to inaccuracy in the probability and cost estimates. This difference suggests some interesting implications for knowledge acquisition. 	
1305.5525v1	http://arxiv.org/pdf/1305.5525v1	2013	Time in Quantum Mechanics	Curt A. Moyer	  The failure of conventional quantum theory to recognize time as an observable and to admit time operators is addressed. Instead of focusing on the existence of a time operator for a given Hamiltonian, we emphasize the role of the Hamiltonian as the generator of translations in time to construct time states. Taken together, these states constitute what we call a timeline, or quantum history, that is adequate for the representation of any physical state of the system. Such timelines appear to exist even for the semi-bounded and discrete Hamiltonian systems ruled out by Pauli's theorem. However, the step from a timeline to a valid time operator requires additional assumptions that are not always met. Still, this approach illuminates the crucial issue surrounding the construction of time operators, and establishes quantum histories as legitimate alternatives to the familiar coordinate and momentum bases of standard quantum theory. 	
1306.3704v1	http://arxiv.org/pdf/1306.3704v1	2013	How interbank lending amplifies overlapping portfolio contagion: A case   study of the Austrian banking network	Fabio Caccioli|J. Doyne Farmer|Nick Foti|Daniel Rockmore	  In spite of the growing theoretical literature on cascades of failures in interbank lending networks, empirical results seem to suggest that networks of direct exposures are not the major channel of financial contagion. In this paper we show that networks of interbank exposures can however significantly amplify contagion due to overlapping portfolios. To illustrate this point, we consider the case of the Austrian interbank network and perform stress tests on it according to different protocols. We consider in particular contagion due to (i) counterparty loss; (ii) roll-over risk; and (iii) overlapping portfolios. We find that the average number of bankruptcies caused by counterparty loss and roll-over risk is fairly small if these contagion mechanisms are considered in isolation. Once portfolio overlaps are also accounted for, however, we observe that the network of direct interbank exposures significantly contributes to systemic risk. 	
1306.5448v2	http://arxiv.org/pdf/1306.5448v2	2013	On the efficiency at maximum cooling power	Yann Apertet|Henni Ouerdane|Aurelie Michot|Christophe Goupil|Philippe Lecoeur	  The efficiency at maximum power (EMP) of heat engines operating as generators is one corner stone of finite-time thermodynamics, the Curzon-Ahlborn efficiency $\eta_{\rm CA}$ being considered as a universal upper bound. Yet, no valid counterpart to $\eta_{\rm CA}$ has been derived for the efficiency at maximum cooling power (EMCP) for heat engines operating as refrigerators. In this Letter we analyse the reasons of the failure to obtain such a bound and we demonstrate that, despite the introduction of several optimisation criteria, the maximum cooling power condition should be considered as the genuine equivalent of maximum power condition in the finite-time thermodynamics frame. We then propose and discuss an analytic expression for the EMCP in the specific case of exoreversible refrigerators. 	
1307.2466v2	http://arxiv.org/pdf/1307.2466v2	2013	Tearing of Free-Standing Graphene	Maria J. B. Moura|Michael Marder	  We examine the fracture mechanics of tearing graphene. We present a molecular dynamics simulation of the propagation of cracks in clamped, free-standing graphene as a function of the out-of-plane force. The geometry is motivated by experimental configurations that expose graphene sheets to out-of-plane forces, such as back-gate voltage. We establish the geometry and basic energetics of failure, and obtain approximate analytical expressions for critical crack lengths and forces. We also propose a method to obtain graphene's toughness. We observe that the cracks' path and the edge structure produced are dependent on the initial crack length. This work may help avoid the tearing of graphene sheets and aid the production of samples with specific edge structures. 	
1308.4302v2	http://arxiv.org/pdf/1308.4302v2	2013	Shocks Generate Crossover Behaviour In Lattice Avalanches	James Burridge	  A spatial avalanche model is introduced, in which avalanches increase stability in the regions where they occur. Instability is driven globally by a driving process that contains shocks. The system is typically subcritical, but the shocks occasionally lift it into a near or super critical state from which it rapidly retreats due to large avalanches. These shocks leave behind a signature -- a distinct power--law crossover in the avalanche size distribution. The model is inspired by landslide field data, but the principles may be applied to any system that experiences stabilizing failures, possesses a critical point, and is subject to an ongoing process of destabilization which includes occasional dramatic destabilizing events. 	
1309.3660v1	http://arxiv.org/pdf/1309.3660v1	2013	(Failure of the) Wisdom of the crowds in an endogenous opinion dynamics   model with multiply biased agents	Steffen Eger	  We study an endogenous opinion (or, belief) dynamics model where we endogenize the social network that models the link (`trust') weights between agents. Our network adjustment mechanism is simple: an agent increases her weight for another agent if that agent has been close to truth (whence, our adjustment criterion is `past performance'). Moreover, we consider multiply biased agents that do not learn in a fully rational manner but are subject to persuasion bias - they learn in a DeGroot manner, via a simple `rule of thumb' - and that have biased initial beliefs. In addition, we also study this setup under conformity, opposition, and homophily - which are recently suggested variants of DeGroot learning in social networks - thereby taking into account further biases agents are susceptible to. Our main focus is on crowd wisdom, that is, on the question whether the so biased agents can adequately aggregate dispersed information and, consequently, learn the true states of the topics they communicate about. In particular, we present several conditions under which wisdom fails. 	
1309.5960v4	http://arxiv.org/pdf/1309.5960v4	2014	Warm dark matter does not do better than cold dark matter in solving   small-scale inconsistencies	Aurel Schneider|Donnino Anderhalden|Andrea Maccio|Juerg Diemand	  Over the last decade, warm dark matter (WDM) has been repeatedly proposed as an alternative scenario to the standard cold dark matter (CDM) one, potentially resolving several disagreements between the CDM model and observations on small scales. Here, we reconsider the most important CDM small-scale discrepancies in the light of recent observational constraints on WDM. As a result, we find that a conventional thermal (or thermal-like) WDM cosmology with a particle mass in agreement with Lyman-$\alpha$ is nearly indistinguishable from CDM on the relevant scales and therefore fails to alleviate any of the small-scale problems. The reason for this failure is that the power spectrum of conventional WDM falls off too rapidly. To maintain WDM as a significantly different alternative to CDM, more evolved production mechanisms leading to multiple dark matter components or a gradually decreasing small-scale power spectrum have to be considered. 	
1309.7795v2	http://arxiv.org/pdf/1309.7795v2	2015	Condensation phenomena in fat-tailed distributions: a characterization   by means of an order parameter	Mario Filiasi|Elia Zarinelli|Erik Vesselli|Matteo Marsili	  Condensation phenomena are ubiquitous in nature and are found in condensed matter, disordered systems, networks, finance, etc. In the present work we investigate one of the best frameworks in which condensation phenomena take place, namely, the sum of independent and fat-tailed distributed random variables. For large deviations of the sum, this system undergoes a phase transition and shifts from a democratic phase to a condensed phase, where a single variable (the condensate) carries a finite fraction of the sum. This phenomenon yields the failure of the standard results of the Large Deviation Theory. In this work we exploit the Density Functional Method to overcome the limitation of the Large Deviation Theory and characterize the condensation transition in terms of an order parameter, i.e. the Inverse Participation Ratio (IPR). This procedure leads us to investigate the system in the large-deviation regime where both the sum and the IPR are constrained, observing new phase transitions. As a sample application, the case of condensation phenomena in financial time-series is briefly discussed. 	
1310.0996v1	http://arxiv.org/pdf/1310.0996v1	2013	Spatially localized attacks on interdependent networks: the existence of   a finite critical attack size	Yehiel Berezin|Amir Bashan|Michael M. Danziger|Daqing Li|Shlomo Havlin	  Many real world complex systems such as infrastructure, communication and transportation networks are embedded in space, where entities of one system may depend on entities of other systems. These systems are subject to geographically localized failures due to malicious attacks or natural disasters. Here we study the resilience of a system composed of two interdependent spatially embedded networks to localized geographical attacks. We find that if an attack is larger than a finite (zero fraction of the system) critical size, it will spread through the entire system and lead to its complete collapse. If the attack is below the critical size, it will remain localized. In contrast, under random attack a finite fraction of the system needs to be removed to initiate system collapse. We present both numerical simulations and a theoretical approach to analyze and predict the effect of local attacks and the critical attack size. Our results demonstrate the high risk of local attacks on interdependent spatially embedded infrastructures and can be useful for designing more resilient systems. 	
1310.2702v3	http://arxiv.org/pdf/1310.2702v3	2014	Emergent irreversibility and entanglement spectrum statistics	Claudio Chamon|Alioscia Hamma|Eduardo R. Mucciolo	  We study the problem of irreversibility when the dynamical evolution of a many-body system is described by a stochastic quantum circuit. Such evolution is more general than a Hamiltonian one, and since energy levels are not well defined, the well-established connection between the statistical fluctuations of the energy spectrum and irreversibility cannot be made. We show that the entanglement spectrum provides a more general connection. Irreversibility is marked by a failure of a disentangling algorithm and is preceded by the appearance of Wigner-Dyson statistical fluctuations in the entanglement spectrum. This analysis can be done at the wave-function level and offers an alternative route to study quantum chaos and quantum integrability. 	
1310.3882v1	http://arxiv.org/pdf/1310.3882v1	2013	Structure and Strength at Immiscible Polymer Interfaces	Ting Ge|Gary S. Grest|Mark O. Robbins	  Thermal welding of polymer-polymer interfaces is important for integrating polymeric elements into devices. When two different polymers are joined, the strength of the weld depends critically on the degree of immiscibility. We perform large-scale molecular dynamics simulations of the structure-strength relation at immiscible polymer interfaces. Our simulations show that immiscibility arrests interdiffusion and limits the equilibrium interfacial width. Even for weakly immiscible films, the narrow interface is unable to transfer stress upon deformation as effectively as the bulk material, and chain pullout at the interface becomes the dominant failure mechanism. This greatly reduces the interfacial strength. The weak response of immiscible interfaces is shown to arise from an insufficient density of entanglements across the interface. We demonstrate that there is a threshold interfacial width below which no significant entanglements can form between opposite sides to strengthen the interface. 	
1310.8293v1	http://arxiv.org/pdf/1310.8293v1	2013	Dimensions, Structures and Security of Networks	Angsheng Li|Wei Zhang|Yicheng Pan	  One of the main issues in modern network science is the phenomenon of cascading failures of a small number of attacks. Here we define the dimension of a network to be the maximal number of functions or features of nodes of the network. It was shown that there exist linear networks which are provably secure, where a network is linear, if it has dimension one, that the high dimensions of networks are the mechanisms of overlapping communities, that overlapping communities are obstacles for network security, and that there exists an algorithm to reduce high dimensional networks to low dimensional ones which simultaneously preserves all the network properties and significantly amplifies security of networks. Our results explore that dimension is a fundamental measure of networks, that there exist linear networks which are provably secure, that high dimensional networks are insecure, and that security of networks can be amplified by reducing dimensions. 	
1312.7530v2	http://arxiv.org/pdf/1312.7530v2	2014	Heisenberg Uncertainty Relation Revisited	Kazuo Fujikawa	  It is shown that all the known uncertainty relations are the secondary consequences of Robertson's relation. The basic idea is to use the Heisenberg picture so that the time development of quantum mechanical operators incorporate the effects of the measurement interaction. A suitable use of triangle inequalities then gives rise to various forms of uncertainty relations. The assumptions of unbiased measurement and unbiased disturbance are important to simplify the resulting uncertainty relations and to give the familiar uncertainty relations such as a naive Heisenberg error-disturbance relation. These simplified uncertainty relations are however valid only conditionally. Quite independently of uncertainty relations, it is shown that the notion of precise measurement is incompatible with the assumptions of unbiased measurement and unbiased disturbance. We can thus naturally understand the failure of the naive Heisenberg's error-disturbance relation, as was demonstrated by the recent spin-measurement by J. Erhart, et al.. 	
1401.0592v1	http://arxiv.org/pdf/1401.0592v1	2014	The influence of van der Waals forces on the waveguide deformation and   power limit of nanoscale optomechanical systems	Fei Xu|Bi-cai Zheng|WEi Luo|Yan-qing Lu	  The ultra-short range force, van der Waals force (VWF), will rise rapidly when one nanoscale waveguide is close to another one, and be stronger than the external transverse gradient force (TGF). We theoretically investigate the giant influence of the VWF on the device performance in a typical optomechanical system consisting of a suspended silicon waveguide and a silica substrate including waveguide deformation stiction and failure mechanism. The device shows unique optically-activated plastic/elastic behaviors and stiction due to the VWF. When the input optical power is above the critical power, the waveguide is sticking to the substrate and the deformation is plastic and unrecoverable, even though the total force is less than the yield strength of the waveguide material. This is important and helpful for the design and applications of optomechanical devices. 	
1401.3281v1	http://arxiv.org/pdf/1401.3281v1	2014	A Creepy World	Didier Sornette|Peter Cauwels	  Using the mechanics of creep in material sciences as a metaphor, we present a general framework to understand the evolution of financial, economic and social systems and to construct scenarios for the future. In a nutshell, highly non-linear out-of-equilibrium systems subjected to exogenous perturbations tend to exhibit a long phase of slow apparent stable evolution, which are nothing but slow maturations towards instabilities, failures and changes of regimes. With examples from history where a small event had a cataclysmic consequence, we propose a novel view of the current state of the world via the logical scenarios that derive, avoiding the traps of an illusionary stability and simple linear extrapolation. The endogenous scenarios are "muddling along", "managing through" and "blood red abyss". The exogenous scenarios are "painful adjustment" and "golden east". 	
1401.6807v1	http://arxiv.org/pdf/1401.6807v1	2014	Nonconvex bundle method with application to a delamination problem	M. N. Dao|J. Gwinner|D. Noll|N. Ovcharova	  Delamination is a typical failure mode of composite materials caused by weak bonding. It arises when a crack initiates and propagates under a destructive loading. Given the physical law characterizing the properties of the interlayer adhesive between the bonded bodies, we consider the problem of computing the propagation of the crack front and the stress field along the contact boundary. This leads to a hemivariational inequality, which after discretization by finite elements we solve by a nonconvex bundle method, where upper-$C^1$ criteria have to be minimized. As this is in contrast with other classes of mechanical problems with non-monotone friction laws and in other applied fields, where criteria are typically lower-$C^1$, we propose a bundle method suited for both types of nonsmoothness. We prove its global convergence in the sense of subsequences and test it on a typical delamination problem of material sciences. 	
1401.8131v1	http://arxiv.org/pdf/1401.8131v1	2014	Failure Detection and Recovery in Hierarchical Network Using FTN   Approach	Bhagvan Krishna Gupta|Ankit Mundra|Nitin Rakesh	  In current scenario several commercial and social organizations are using computer networks for their business and management purposes. In order to meet the business requirements networks are also grow. The growth of network also promotes the handling capability of large networks because it counter raises the possibilities of various faults in the network. A fault in network degrades its performance by affecting parameters like throughput, delay, latency, reliability etc. In hierarchical network models any possibility of fault may collapse entire network. If a fault occurrence disables a device in hierarchical network then it may distresses all the devices underneath. Thus it affects entire networks performance. In this paper we propose Fault Tolerable hierarchical Network (FTN) approach as a solution to the problems of hierarchical networks. The proposed approach firstly detects possibilities of fault in the network and accordingly provides specific recovery mechanism. We have evaluated the performance of FTN approach in terms of delay and throughput of network. 	
1402.2057v2	http://arxiv.org/pdf/1402.2057v2	2014	Robustness of complex many-body networks: Novel perspective in 2D   metal-insulator transition	Chung-Pin Chou	  We present a novel theoretical framework established by complex network analysis for understanding the phase transition beyond the Landau symmetry breaking paradigm. In this paper we take a two-dimensional metal-insulator transition driven by electron correlations for example. Passing through the transition point, we find a hidden symmetry broken in the network space, which is invisible in real space. This symmetry is nothing but a kind of robustness of the network to random failures. We then show that a network quantity, small-worldness, is capable of identifying the phase transition with/without any symmetry breaking in the real space and behaving as a new order parameter in the network space. We demonstrate that whether or not the symmetry is broken in real space a variety of phase transitions in condensed matters can be characterized by the hidden symmetry breaking in the weighted network, that is to say, a decline in network robustness. 	
1402.5342v2	http://arxiv.org/pdf/1402.5342v2	2014	Scattering nonlocality in quantum charge transport: Application to   semiconductor nanostructures	Roberto Rosati|Fausto Rossi	  Our primary goal is to provide a rigorous treatment of scattering nonlocality in semiconductor nanostructures. On the one hand, starting from the conventional density-matrix formulation and employing as ideal instrument for the study of the semiclassical limit the well-known Wigner-function picture, we shall perform a fully quantum-mechanical derivation of the space-dependent Boltzmann equation. On the other hand, we shall examine the validity limits of such semiclassical framework, pointing out, in particular, regimes where scattering-nonlocality effects may play a relevant role; to this end we shall supplement our analytical investigation with a number of simulated experiments, discussing and further expanding preliminary studies of scattering-induced quantum diffusion in GaN-based nanomaterials. As for the case of carrier-carrier relaxation in photoexcited semiconductors, our analysis will show the failure of simplified dephasing models in describing phonon-induced scattering nonlocality, pointing out that such limitation is particularly severe for the case of quasielastic dissipation processes. 	
1402.6228v2	http://arxiv.org/pdf/1402.6228v2	2014	Numerical evidence for nucleated self-assembly of DNA brick structures	Aleks Reinhardt|Daan Frenkel	  The observation by Ke et al. [Science 338, 1177 (2012)] that large numbers of short, pre-designed DNA strands can assemble into three-dimensional target structures came as a great surprise, as no colloidal self-assembling system has ever achieved the same degree of complexity. That failure seemed easy to rationalise: the larger the number of distinct building blocks, the higher the expected error rate for self-assembly. The experiments of Ke et al. have disproved this argument. Here, we report Monte Carlo simulations of the self-assembly of a DNA brick cube, comprising approximately 1000 types of DNA strand, using a simple model. We model the DNA strands as lattice tetrahedra with attractive patches, the interaction strengths of which are computed using a standard thermodynamic model. We find that, within a narrow temperature window, the target structure assembles with high probability. Our simulations suggest that mis-assembly is disfavoured because of a slow nucleation step. As our model incorporates no aspect of DNA other than its binding properties, these simulations suggest that, with proper design of the building blocks, other systems, such as colloids, may also assemble into truly complex structures. 	
1403.1887v2	http://arxiv.org/pdf/1403.1887v2	2015	Ultra-ductile and low friction epoxy matrix composites	Anderson O. Okonkwo|Pravin Jagadale|John E. García Herrera|Viktor G. Hadjiev|Juan Muñoz Saldaña|Alberto Taglafierro|Francisco C. Robles Hernandez	  We present the results of an effective reinforcement of epoxy resin matrix with fullerene carbon soot. The optimal carbon soot addition of 1 wt. % results in a toughness improvement of almost 20 times. The optimized soot-epoxy composites also show an increase in tensile elongation of more than 13 %, thus indicating a change of the failure mechanism in tension from brittle to ductile. Additionally, the coefficient of friction is reduced from its 0.91 value in plain epoxy resin to 0.15 in the optimized composite. In the optimized composite, the lateral forces during nanoscratching decrease as much as 80 % with enhancement of the elastic modulus and hardness by 43 % and 94%, respectively. The optimized epoxy resin fullerene soot composite can be a strong candidate for coating applications where toughness, low friction, ductility and light weight are important. 	
1403.2599v2	http://arxiv.org/pdf/1403.2599v2	2014	Anderson localization and momentum-space entanglement	Eric C. Andrade|Mark Steudtner|Matthias Vojta	  We consider Anderson localization and the associated metal-insulator transition for non-interacting fermions in D = 1, 2 space dimensions in the presence of spatially correlated on-site random potentials. To assess the nature of the wavefunction, we follow a recent proposal to study momentum-space entanglement. For a D = 1 model with long-range disorder correlations, both the entanglement spectrum and the entanglement entropy allow us to clearly distinguish between extended and localized states based upon a single realization of disorder. However, for other models including the D = 2 case with long-range correlated disorder, we find that the method is not similarly successful. We analyze the reasons for its failure, concluding that the much desired generalization to higher dimensions may be problematic. 	
1404.6790v2	http://arxiv.org/pdf/1404.6790v2	2014	Assessing T cell clonal size distribution: a non-parametric approach	O. V. Bolkhovskaya|D. Yu. Zorin|M. V. Ivanchenko	  Clonal structure of the human peripheral T-cell repertoire is shaped by a number of homeostatic mechanisms, including antigen presentation, cytokine and cell regulation. Its accurate tuning leads to a remarkable ability to combat pathogens in all their variety, while systemic failures may lead to severe consequences like autoimmune diseases. Here we develop and make use of a non-parametric statistical approach to assess T cell clonal size distributions from recent next generation sequencing data. For 41 healthy individuals and a patient with ankylosing spondylitis, who undergone treatment, we invariably find power law scaling over several decades and for the first time calculate quantitatively meaningful values of decay exponent. It has proved to be much the same among healthy donors, significantly different for an autoimmune patient before the therapy, and converging towards a typical value afterwards. We discuss implications of the findings for theoretical understanding and mathematical modeling of adaptive immunity. 	
1405.0483v2	http://arxiv.org/pdf/1405.0483v2	2014	Percolation on sparse networks	Brian Karrer|M. E. J. Newman|Lenka Zdeborová	  We study percolation on networks, which is used as a model of the resilience of networked systems such as the Internet to attack or failure and as a simple model of the spread of disease over human contact networks. We reformulate percolation as a message passing process and demonstrate how the resulting equations can be used to calculate, among other things, the size of the percolating cluster and the average cluster size. The calculations are exact for sparse networks when the number of short loops in the network is small, but even on networks with many short loops we find them to be highly accurate when compared with direct numerical simulations. By considering the fixed points of the message passing process, we also show that the percolation threshold on a network with few loops is given by the inverse of the leading eigenvalue of the so-called non-backtracking matrix. 	
1405.0528v2	http://arxiv.org/pdf/1405.0528v2	2014	Brightest Cluster Galaxies in Cosmological Simulations with Adaptive   Mesh Refinement: Successes and Failures	Davide Martizzi| Jimmy|Romain Teyssier|Ben Moore	  A large sample of cosmological hydrodynamical zoom-in simulations with Adaptive Mesh Refinement (AMR) is analysed to study the properties of simulated Brightest Cluster Galaxies (BCGs). Following the formation and evolution of BCGs requires modeling an entire galaxy cluster, because the BCG properties are largely influenced by the state of the gas in the cluster and by interactions and mergers with satellites. BCG evolution is also deeply influenced by the presence of gas heating sources such as Active Galactic Nuclei (AGNs) that prevent catastrophic cooling of large amounts of gas. We show that AGN feedback is one of the most important mechanisms in shaping the properties of BCGs at low redshift by analysing our statistical sample of simulations with and without AGN feedback. When AGN feedback is included BCG masses, sizes, star formation rates and kinematic properties are closer to those of the observed systems. Some small discrepancies are observed only for the most massive BCGs and in the fraction of star-forming BCGs, effects that might be due to physical processes that are not included in our model. 	
1405.0932v1	http://arxiv.org/pdf/1405.0932v1	2014	Experimental investigation of the elastoplastic response of aluminum   silicate spray dried powder during cold compaction	F. Bosi|A. Piccolroaz|M. Gei|F. Dal Corso|A. Cocquio|D. Bigoni	  Mechanical experiments have been designed and performed to investigate the elasto-plastic behaviour of green bodies formed from an aluminum silicate spray dried powder used for tiles production. Experiments have been executed on samples obtained from cold compaction into a cylindrical mould and include: uniaxial strain, equi-biaxial flexure and high-pressure triaxial compression/extension tests. Two types of powders have been used to realize the green body samples, differing in the values of water content, which have been taken equal to those usually employed in the industrial forming of traditional ceramics. Yielding of the green body during compaction has been characterized in terms of yield surface shape, failure envelope, and evolution of cohesion and void ratio with the forming pressure, confirming the validity of previously proposed constitutive models for dense materials obtained through cold compaction of granulates. 	
1405.1385v1	http://arxiv.org/pdf/1405.1385v1	2014	Quasi Steady-State Model for Power System Stability: Limitations,   Analysis and a Remedy	Xiaozhe Wang|Hsiao-Dong Chiang	  The quasi steady-state (QSS) model tries to reach a good compromise between accuracy and efficiency in long-term stability analysis. However, the QSS model is unable to provide correct approximations and stability assessment for the long-term stability model consistently. In this paper, some numerical examples in which the QSS model was stable while the long-term stability model underwent instabilities are presented with analysis in nonlinear system framework. At the same time, a hybrid model which serves as a remedy to the QSS model is proposed according to causes for failure of the QSS model and dynamic mechanisms of long-term instabilities. Numerical examples are given to show that the developed hybrid model can successfully capture unstable behaviors of the long-term stability model while the QSS model fails. 	
1405.2843v2	http://arxiv.org/pdf/1405.2843v2	2014	Correlations after quantum quenches in the XXZ spin chain: Failure of   the Generalized Gibbs Ensemble	B. Pozsgay|M. Mestyán|M. A. Werner|M. Kormos|G. Zaránd|G. Takács	  We study the nonequilibrium time evolution of the spin-1/2 anisotropic Heisenberg (XXZ) spin chain, with a choice of dimer product and Neel states as initial states. We investigate numerically various short-ranged spin correlators in the long-time limit and find that they deviate significantly from predictions based on the generalized Gibbs ensemble (GGE) hypotheses. By computing the asymptotic spin correlators within the recently proposed quench-action formalism [Phys. Rev. Lett. 110, 257203 (2013)], however, we find excellent agreement with the numerical data. We, therefore, conclude that the GGE cannot give a complete description even of local observables, while the quench-action formalism correctly captures the steady state in this case. 	
1407.4419v2	http://arxiv.org/pdf/1407.4419v2	2014	Irreversibility and Entanglement Spectrum Statistics in Quantum Circuits	Daniel Shaffer|Claudio Chamon|Alioscia Hamma|Eduardo R. Mucciolo	  We show that in a quantum system evolving unitarily under a stochastic quantum circuit the notions of irreversibility, universality of computation, and entanglement are closely related. As the state evolves from an initial product state, it gets asymptotically maximally entangled. We define irreversibility as the failure of searching for a disentangling circuit using a Metropolis-like algorithm. We show that irreversibility corresponds to Wigner-Dyson statistics in the level spacing of the entanglement eigenvalues, and that this is obtained from a quantum circuit made from a set of universal gates for quantum computation. If, on the other hand, the system is evolved with a non-universal set of gates, the statistics of the entanglement level spacing deviates from Wigner-Dyson and the disentangling algorithm succeeds. These results open a new way to characterize irreversibility in quantum systems. 	
1407.7899v1	http://arxiv.org/pdf/1407.7899v1	2014	Failure of steady state thermodynamics in lattice gases under nonuniform   drive	Ronald Dickman	  To be useful, steady state thermodynamics (SST) must be self-consistent and have predictive value. Although consistency of SST was recently verified for driven lattice gases under global weak exchange, I show here that it does not predict the coexisting densities in athermal stochastic lattice gases under a nonuniform drive. I consider the lattice gas with nearest-neighbor exclusion on the square lattice, with nearest-neighbor hopping (NNE dynamics), and with hopping to both nearest and next-nearest neighbors (NNE2 dynamics). Part of the system is subject to a drive $D$ that favors hopping along one direction, while the other part is free of the drive. Thus the steady state represents coexistence between two subsystems, one far from equilibrium and the other in equilibrium, which exchange particles along the interfaces. The dimensionless chemical potential $\mu^*(\rho,D) = \mu/k_B T$ for the lattice gas with density $\rho$ is readily determined in studies of a uniform system. Under the nonuniform drive, however, equating the chemical potentials of the coexisting subsystems does not yield the coexisting densities. The steady state chemical potential is, moreover, different in the coexisting bulk regions, contrary to the basic principles of thermodynamics. These results cast serious doubt on the predictive value of SST. 	
1408.1183v1	http://arxiv.org/pdf/1408.1183v1	2014	Network Robustness: Detecting Topological Quantum Phases	Chung-Pin Chou	  Can the topology of a network that consists of many particles interacting with each other change in complexity when a phase transition occurs? The answer to this question is particularly interesting to understand the nature of phase transitions if the distinct phases do not break any symmetry, such as topological phase transitions. Here we present a novel theoretical framework established by complex network analysis for demonstrating that across a transition point of the topological superconductors, the network space experiences a homogeneous-heterogeneous transition invisible in real space. This transition is nothing but related to the robustness of a network to random failures. We suggest that the idea of the network robustness can be applied to characterizing various phase transitions whether or not the symmetry is broken. 	
1409.2298v1	http://arxiv.org/pdf/1409.2298v1	2014	Fragmentation of colliding planetesimals with water content	Thomas I. Maindl|Rudolf Dvorak|Christoph Schäfer|Roland Speith	  We investigate the outcome of collisions of Ceres-sized planetesimals composed of a rocky core and a shell of water ice. These collisions are not only relevant for explaining the formation of planetary embryos in early planetary systems, but also provide insight into the formation of asteroid families and possible water transport via colliding small bodies. Earlier studies show characteristic collision velocities exceeding the bodies' mutual escape velocity which - along with the distribution of the impact angles - cover the collision outcome regimes 'partial accretion', 'erosion', and 'hit-and-run' leading to different expected fragmentation scenarios. Existing collision simulations use bodies composed of strengthless material; we study the distribution of fragments and their water contents considering the full elasto-plastic continuum mechanics equations also including brittle failure and fragmentation. 	
1409.2680v2	http://arxiv.org/pdf/1409.2680v2	2015	Anomalous transport of impurities in inelastic Maxwell gases	Vicente Garzó|Nagi Khalil|Emmanuel Trizac	  A mixture of dissipative hard grains generically exhibits a breakdown of kinetic energy equipartition. The undriven and thus freely cooling binary problem, in the tracer limit where the density of one species becomes minute, may exhibit an extreme form of this breakdown, with the minority species carrying a finite fraction of the total kinetic energy of the system. We investigate the fingerprint of this non-equilibrium phase transition, akin to an ordering process, on transport properties. The analysis, performed by solving the Boltzmann kinetic equation from a combination of analytical and Monte Carlo techniques, hints at the possible failure of hydrodynamics in the ordered region. As a relevant byproduct of the study, the behaviour of the second and fourth-degree velocity moments is also worked out. 	
1409.5539v2	http://arxiv.org/pdf/1409.5539v2	2015	Uninformed Hawking Radiation	I. Sakalli|A. Ovgun	  We show in detail that the Parikh-Wilczek tunneling method (PWTM), which was designed for resolving the information loss problem in Hawking radiation (HR)fails whenever the radiation occurs from an isothermal process. The PWTM aims to produce a non-thermal HR which adumbrates the resolution of the problem of unitarity in quantum mechanics (QM), and consequently the entropy (or information) conservation problem. The effectiveness of the method has been satisfactorily tested on numerous black holes (BHs). However, it has been shown that the isothermal HR, which results from the emission of the uncharged particles of the linear dilaton BH (LDBH) described in the Einstein-Maxwell-Dilaton (EMD) theory, the PWTM has vulnerability in having non-thermal radiation. In particular, we consider Painlev\'e-Gullstrand coordinates (PGCs) and isotropic coordinates (ICs) in order to prove the aformentioned failure in the PWTM. While carrying out calculations in the ICs, we also highlight the effect of the refractive index on the null geodesics. 	
1410.0138v1	http://arxiv.org/pdf/1410.0138v1	2014	Characterization of Single-Walled Carbon Nanotubes with Nodal Structural   Defects	Young I. Jhon|Woonjo Cho|Seok Lee|Young Min Jhon	  Recently experiments showed that nodal structural defects are readily formed in the synthesis of single-walled carbon nanotubes (SWNTs) and consequently, SWNTs are likely to deviate from well-defined seamless tubular structures. Here, using graphene-helix growth model, we describe structural details of feasible nodal defects in SWNTs and investigate how mechanical and electronic properties of SWNTs would change in the presence of them using computational methods. Surprisingly atomistic simulations of SWNTs with nodal defects show excellent agreement with previous structural, tensile, and ball-milling experiments whose results cannot be explained using conventional models. The tensile failure of SWNTs with nodal defects requires about four- or six-fold lower strength than pristine ones and these SWNTs are comparatively prone to damage under a lateral compressive biting. We reveal that electronic band-gap of SWNT(12,8) would be remarkably reduced in the presence of nodal defects. This study strongly indicates universality of nodal defects in SWNTs requesting new theoretical framework in SWNT modelling for proper characteristics prediction. 	
1410.8616v1	http://arxiv.org/pdf/1410.8616v1	2014	Data Driven Prognosis: A multi-physics approach verified via balloon   burst experiment	Abhijit Chandra|Oliva Kar	  A multi-physics formulation for Data Driven Prognosis (DDP) is developed. Unlike traditional predictive strategies that require controlled off-line measurements or training for determination of constitutive parameters to derive the transitional statistics, the proposed DDP algorithm relies solely on in situ measurements. It utilizes a deterministic mechanics framework, but the stochastic nature of the solution arises naturally from the underlying assumptions regarding the order of the conservation potential as well as the number of dimensions involved. The proposed DDP scheme is capable of predicting onset of instabilities. Since the need for off-line testing (or training) is obviated, it can be easily implemented for systems where such a priori testing is difficult or even impossible to conduct. The prognosis capability is demonstrated here via a balloon burst experiment where the instability is predicted utilizing only on-line visual observations. The DDP scheme never failed to predict the incipient failure, and no false positives were issued. The DDP algorithm is applicable to others types of datasets. Time horizons of DDP predictions can be adjusted by using memory over different time windows. Thus, a big dataset can be parsed in time to make a range of predictions over varying time horizons. 	
1412.1211v1	http://arxiv.org/pdf/1412.1211v1	2014	Criticality in Fiber Bundle Model	Subhadeep Roy|Purusattam Ray	  We report a novel critical behavior in the breakdown of an equal load sharing fiber bundle model at a dispersion $\delta_c$ of the breaking threshold of the fibers. For $\delta < \delta_c$, there is a finite probability $P_b$, that rupturing of the weakest fiber leads to the failure of the entire system. For $\delta \geq \delta_c$, $P_b = 0$. At $\delta_c, P_b \sim L^{-\eta}$, with $\eta \approx 1/3$, where $L$ is the size of the system. As $\delta \rightarrow \delta_c$, the relaxation time $\tau$ diverges obeying the finite size scaling law: $\tau \sim L^{\beta}(|\delta-\delta_c| L^{\alpha})$ with $\alpha, \beta = 0.33 \pm 0.05$. At $\delta_c$, the system fails, at the critical load, in avalanches (of rupturing fibers) of all sizes $s$ following the distribution $P(s) \sim s^{-\kappa}$, with $\kappa = 0.50 \pm 0.01$. We relate this critical behavior to brittle to quasi-brittle transition. 	
1412.1523v2	http://arxiv.org/pdf/1412.1523v2	2015	Information Exchange and Learning Dynamics over Weakly-Connected   Adaptive Networks	Bicheng Ying|Ali H. Sayed	  The paper examines the learning mechanism of adaptive agents over weakly-connected graphs and reveals an interesting behavior on how information flows through such topologies. The results clarify how asymmetries in the exchange of data can mask local information at certain agents and make them totally dependent on other agents. A leader-follower relationship develops with the performance of some agents being fully determined by the performance of other agents that are outside their domain of influence. This scenario can arise, for example, due to intruder attacks by malicious agents or as the result of failures by some critical links. The findings in this work help explain why strong-connectivity of the network topology, adaptation of the combination weights, and clustering of agents are important ingredients to equalize the learning abilities of all agents against such disturbances. The results also clarify how weak-connectivity can be helpful in reducing the effect of outlier data on learning performance. 	
1412.4787v1	http://arxiv.org/pdf/1412.4787v1	2014	Quenching the XXZ spin chain: quench action approach versus generalized   Gibbs ensemble	M. Mestyan|B. Pozsgay|G. Takacs|M. A. Werner	  Following our previous work [PRL 113 (2014) 09020] we present here a detailed comparison of the quench action approach and the predictions of the generalized Gibbs ensemble, with the result that while the quench action formalism correctly captures the steady state, the GGE does not give a correct description of local short-distance correlation functions. We extend our studies to include another initial state, the so-called q-dimer state. We present important details of our construction, including new results concerning exact overlaps for the dimer and q-dimer states, and we also give an exact solution of the quench-action-based overlap-TBA for the q-dimer. Furthermore, we extend our computations to include the xx spin correlations besides the zz correlations treated previously, and give a detailed discussion of the underlying reasons for the failure of the GGE, especially in the light of new developments. 	
1412.5571v1	http://arxiv.org/pdf/1412.5571v1	2014	From Co- Toward Multi-Simulation of Smart Grids based on HLA and FMI   Standards	Martin Lévesque|Christophe Béchet|Eric Suignard|Martin Maier|Anne Picault|Géza Joós	  In this article, a multi-simulation model is proposed to measure the performance of all Smart Grid perspectives as defined in the IEEE P2030 standard. As a preliminary implementation, a novel information technology (IT) and communication multi-simulator is developed following an High Level Architecture (HLA). To illustrate the usefulness of such a multi-simulator, a case study of a distribution network operation application is presented using real-world topology configurations with realistic communication traffic based on IEC 61850. The multi-simulator allows to quantify, in terms of communication delay and system reliability, the impacts of aggregating all traffic on a low-capacity wireless link based on Digital Mobile Radio (DMR) when a Long Term Evolution (LTE) network failure occurs. The case study illustrates that such a multi-simulator can be used to experiment new smart grid mechanisms and verify their impacts on all smart grid perspectives in an automated manner. Even more importantly, multi-simulation can prevent problems before modifying/upgrading a smart grid and thus potentially reduce utility costs. 	
1412.5970v1	http://arxiv.org/pdf/1412.5970v1	2014	Excitation Waves on a Minimal Small-World Model	Thomas Isele|Benedikt Hartung|Philipp Hövel|Eckehard Schöll	  We examine traveling-wave solutions on a regular ring network with one additional long-range link that spans a distance d. The nodes obey the FitzHugh-Nagumo kinetics in the excitable regime. The additional shortcut induces a plethora of spatio-temporal behavior that is not present without it. We describe the underlying mechanisms for different types of patterns: propagation failure, period decreasing, bistability, shortcut blocking and period multiplication. For this purpose, we investigate the dependence on d, the network size, the coupling range in the original ring and the global coupling strength and present a phase diagram summarizing the different scenarios. Furthermore, we discuss the scaling behavior of the critical distance by analytical means and address the connection to spatially continuous excitable media. 	
1501.00202v1	http://arxiv.org/pdf/1501.00202v1	2014	Comb models for transport along spiny dendrites	V. Méndez|A. Iomin	  This chapter is a contribution in the "Handbook of Applications of Chaos Theory" ed. by Prof. Christos H Skiadas. The chapter is organized as follows. First we study the statistical properties of combs and explain how to reduce the effect of teeth on the movement along the backbone as a waiting time distribution between consecutive jumps. Second, we justify an employment of a comb-like structure as a paradigm for further exploration of a spiny dendrite. In particular, we show how a comb-like structure can sustain the phenomenon of the anomalous diffusion, reaction-diffusion and L\'evy walks. Finally, we illustrate how the same models can be also useful to deal with the mechanism of ta translocation wave / translocation waves of CaMKII and its propagation failure. We also present a brief introduction to the fractional integro-differentiation in appendix at the end of the chapter. 	
1501.05473v2	http://arxiv.org/pdf/1501.05473v2	2015	Persistent crust-core spin lag in neutron stars	Kostas Glampedakis|Paul Lasky	  It is commonly believed that the magnetic field threading a neutron star provides the ultimate mechanism (on top of fluid viscosity) for enforcing long-term corotation between the slowly spun down solid crust and the liquid core. We show that this argument fails for axisymmetric magnetic fields with closed field lines in the core, the commonly used `twisted torus' field being the most prominent example. The failure of such magnetic fields to enforce global crust-core corotation leads to the development of a persistent spin lag between the core region occupied by the closed field lines and the rest of the crust and core. We discuss the repercussions of this spin lag for the evolution of the magnetic field, suggesting that, in order for a neutron star to settle to a stable state of crust-core corotation, the bulk of the toroidal field component should be deposited into the crust soon after the neutron star's birth. 	
1501.06024v1	http://arxiv.org/pdf/1501.06024v1	2015	Strain localization and shear banding in ductile materials	N. Bordignon|A. Piccolroaz|F. Dal Corso|D. Bigoni	  A model of a shear band as a zero-thickness nonlinear interface is proposed and tested using finite element simulations. An imperfection approach is used in this model where a shear band, that is assumed to lie in a ductile matrix material (obeying von Mises plasticity with linear hardening), is present from the beginning of loading and is considered to be a zone in which yielding occurs before the rest of the matrix. This approach is contrasted with a perturbative approach, developed for a J$_2$-deformation theory material, in which the shear band is modelled to emerge at a certain stage of a uniform deformation. Both approaches concur in showing that the shear bands (differently from cracks) propagate rectilinearly under shear loading and that a strong stress concentration should be expected to be present at the tip of the shear band, two key features in the understanding of failure mechanisms of ductile materials. 	
1502.00244v2	http://arxiv.org/pdf/1502.00244v2	2015	Multiple Tipping Points and Optimal Repairing in Interacting Networks	Antonio Majdandzic|Lidia A. Braunstein|Chester Curme|Irena Vodenska|Sary Levy-Carciente|H. Eugene Stanley|Shlomo Havlin	  Systems that comprise many interacting dynamical networks, such as the human body with its biological networks or the global economic network consisting of regional clusters, often exhibit complicated collective dynamics. To understand the collective behavior of such systems, we investigate a model of interacting networks exhibiting the fundamental processes of failure, damage spread, and recovery. We find a very rich phase diagram that becomes exponentially more complex as the number of networks is increased. In the simplest example of $n=2$ interacting networks we find two critical points, 4 triple points, 10 allowed transitions, and two "forbidden" transitions, as well as complex hysteresis loops. Remarkably, we find that triple points play the dominant role in constructing the optimal repairing strategy in damaged interacting systems. To support our model, we analyze an example of real interacting financial networks and find evidence of rapid dynamical transitions between well-defined states, in agreement with the predictions of our model. 	
1503.00254v2	http://arxiv.org/pdf/1503.00254v2	2017	Many-body critical Casimir interactions in colloidal suspensions	Hendrik Hobrecht|Alfred Hucht	  We study the fluctuation-induced Casimir interactions in colloidal suspensions, especially between colloids immersed in a binary liquid close to its critical demixing point. To simulate these systems, we present a highly efficient cluster Monte Carlo algorithm based on geometric symmetries of the Hamiltonian. Utilizing the principle of universality, the medium is represented by an Ising system while the colloids are areas of spins with fixed orientation. Our results for the Casimir interaction potential between two particles at the critical point in two dimensions perfectly agree with the exact predictions. However, we find that in finite systems the behavior strongly depends on whether the $Z_{2}$ symmetry of the system is broken by the particles. Eventually we present Monte Carlo results for the three-body Casimir interaction potential and take a close look onto the case of one particle in the vicinity of two adjacent particles, which can be calculated from the two-particle interaction by a conformal mapping. These results emphasize the failure of the common decomposition approach for many-particle critical Casimir interactions. 	
1503.01172v1	http://arxiv.org/pdf/1503.01172v1	2015	Soil cracking modelling using the mesh-free SPH method	H. H. Bui|G. D. Nguyen|J. Kodikara|M. Sanchez	  The presence of desiccation cracks in soils can significantly alter their mechanical and hydrological properties. In many circumstances, desiccation cracking in soils can cause significant damage to earthen or soil supported structures. For example, desiccation cracks can act as the preference path way for water flow, which can facilitate seepage flow causing internal erosion inside earth structures. Desiccation cracks can also trigger slope failures and landslides. Therefore, developing a computational procedure to predict desiccation cracking behaviour in soils is vital for dealing with key issues relevant to a range of applications in geotechnical and geo-environment engineering. In this paper, the smoothed particle hydrodynamics (SPH) method will be extended for the first time to simulate shrinkage-induced soil cracking. The main objective of this work is to examine the performance of the proposed numerical approach in simulating the strong discontinuity in material behaviour and to learn about the crack formation in soils, looking at the effects of soil thickness on the cracking patterns. Results show that the SPH is a promising numerical approach for simulating crack formation in soils 	
1504.02578v1	http://arxiv.org/pdf/1504.02578v1	2015	Blade: A Data Center Garbage Collector	David Terei|Amit Levy	  An increasing number of high-performance distributed systems are written in garbage collected languages. This removes a large class of harmful bugs from these systems. However, it also introduces high tail-latency do to garbage collection pause times. We address this problem through a new technique of garbage collection avoidance which we call Blade. Blade is an API between the collector and application developer that allows developers to leverage existing failure recovery mechanisms in distributed systems to coordinate collection and bound the latency impact. We describe Blade and implement it for the Go programming language. We also investigate two different systems that utilize Blade, a HTTP load-balancer and the Raft consensus algorithm. For the load-balancer, we eliminate any latency introduced by the garbage collector, for Raft, we bound the latency impact to a single network round-trip, (48 {\mu}s in our setup). In both cases, latency at the tail using Blade is up to three orders of magnitude better. 	
1504.02956v1	http://arxiv.org/pdf/1504.02956v1	2015	Liquidity crises on different time scales	Francesco Corradi|Andrea Zaccaria|Luciano Pietronero	  We present an empirical analysis of the microstructure of financial markets and, in particular, of the static and dynamic properties of liquidity. We find that on relatively large time scales (15 minutes) large price fluctuations are connected to the failure of the subtle mechanism of compensation between the flows of market and limit orders: in other words, the missed revelation of the latent order book breaks the dynamical equilibrium between the flows, triggering the large price jumps. On smaller time scales (30 seconds), instead, the static depletion of the limit order book is an indicator of an intrinsic fragility of the system, which is related to a strongly non linear enhancement of the response. In order to quantify this phenomenon we introduce a measure of the liquidity imbalance present in the book and we show that it is correlated to both the sign and the magnitude of the next price movement. These findings provide a quantitative definition of the effective liquidity, which results to be strongly dependent on the considered time scales. 	
1504.05646v2	http://arxiv.org/pdf/1504.05646v2	2015	The New South Wales iVote System: Security Failures and Verification   Flaws in a Live Online Election	J. Alex Halderman|Vanessa Teague	  In the world's largest-ever deployment of online voting, the iVote Internet voting system was trusted for the return of 280,000 ballots in the 2015 state election in New South Wales, Australia. During the election, we performed an independent security analysis of parts of the live iVote system and uncovered severe vulnerabilities that could be leveraged to manipulate votes, violate ballot privacy, and subvert the verification mechanism. These vulnerabilities do not seem to have been detected by the election authorities before we disclosed them, despite a pre-election security review and despite the system having run in a live state election for five days. One vulnerability, the result of including analytics software from an insecure external server, exposed some votes to complete compromise of privacy and integrity. At least one parliamentary seat was decided by a margin much smaller than the number of votes taken while the system was vulnerable. We also found protocol flaws, including vote verification that was itself susceptible to manipulation. This incident underscores the difficulty of conducting secure elections online and carries lessons for voters, election officials, and the e-voting research community. 	
1505.02577v1	http://arxiv.org/pdf/1505.02577v1	2015	A mathematical model for plasticity and damage: A discrete calculus   formulation	Ioannis Dassios|Andrey Jivkov|Andrew Abu-Muharib|Peter James	  In this article we propose a discrete lattice model to simulate the elastic, plastic and failure behaviour of isotropic materials. Focus is given on the mathematical derivation of the lattice elements, nodes and edges, in the presence of plastic deformations and damage, i.e. stiffness degradation. By using discrete calculus and introducing non-local potential for plasticity, a force-based approach, we provide a matrix formulation necessary for software implementation. The output is a non-linear system with allowance for elasticity, plasticity and damage in lattices. This is the key tool for explicit analysis of micro-crack generation and population growth in plastically deforming metals, leading to macroscopic degradation of their mechanical properties and fitness for service. An illustrative example, analysing a local region of a node, is given to demonstrate the system performance. 	
1505.03724v1	http://arxiv.org/pdf/1505.03724v1	2015	Generalized model of blockage in particulate flow limited by channel   carrying capacity	C. Barré|J. Talbot|P. Viot L. Angelani|A. Gabrielli	  We investigate stochastic models of particles entering a channel with a random time distribution. When the number of particles present in the channel exceeds a critical value $N$, a blockage occurs and the particle flux is definitively interrupted. By introducing an integral representation of the $n$ particle survival probabilities, we obtain exact expressions for the survival probability, the distribution of the number of particles that pass before failure, the instantaneous flux of exiting particle and their time correlation. We generalize previous results for $N=2$ to an arbitrary distribution of entry times and obtain new, exact solutions for $N=3$ for a Poisson distribution and partial results for $N\ge 4$. 	
1505.04775v1	http://arxiv.org/pdf/1505.04775v1	2015	A Microstructural View of Burrowing with RoboClam	Kerstin Nordstrom|Dan Dorsch|Wolfgang Losert|Amos Winter	  RoboClam is a burrowing technology inspired by Ensis directus, the Atlantic razor clam. Atlantic razor clams should only be strong enough to dig a few centimeters into the soil, yet they burrow to over 70 cm. The animal uses a clever trick to achieve this: by contracting its body, it agitates and locally fluidizes the soil, reducing the drag and energetic cost of burrowing. RoboClam technology, which is based on the digging mechanics of razor clams, may be valuable for subsea applications that could benefit from efficient burrowing, such as anchoring, mine detonation, and cable laying. We directly visualize the movement of soil grains during the contraction of RoboClam, using a novel index-matching technique along with particle tracking. We show that the size of the failure zone around contracting RoboClam, can be theoretically predicted from the substrate and pore fluid properties, provided that the timescale of contraction is sufficiently large. We also show that the nonaffine motions of the grains are a small fraction of the motion within the fluidized zone, affirming the relevance of a continuum model for this system, even though the grain size is comparable to the size of RoboClam. 	
1506.00439v1	http://arxiv.org/pdf/1506.00439v1	2015	Scaling of discrete element model parameters for cohesionless and   cohesive solid	Subhash C. Thakur|Jin Y. Ooi|Hossein Ahmadian	  One of the major shortcomings of discrete element modelling (DEM) is the computational cost required when the number of particles is huge, especially for fine powders and/or industry scale simulations. This study investigates the scaling of model parameters that is necessary to produce scale independent predictions for cohesionless and cohesive solid under quasi-static simulation of confined compression and unconfined compression to failure in uniaxial test. A bilinear elasto-plastic adhesive frictional contact model was used. The results show that contact stiffness (both normal and tangential) for loading and unloading scales linearly with the particle size and the adhesive force scales very well with the square of the particle size. This scaling law would allow scaled up particle DEM model to exhibit bulk mechanical loading response in uniaxial test that is similar to a material comprised of much smaller particles. This is a first step towards a mesoscopic representation of a cohesive powder that is phenomenological based to produce the key bulk characteristics of a cohesive solid and has the potential to gain considerable computational advantage for industry scale DEM simulations. 	
1506.00832v1	http://arxiv.org/pdf/1506.00832v1	2015	Nonmonotonic fracture behavior of polymer nanocomposites	Janaina G. de Castro|Rojman Zargar|Mehdi Habibi|Samet H. Varol|Sapun H. Parekh|Babak Hosseinkhani|Mokhtar Adda-Bedia|Daniel Bonn	  Polymer composite materials are widely used for their exceptional mechanical properties, notably their ability to resist large deformations. Here we examine the failure stress and strain of rubbers reinforced by varying amounts of nano-sized silica particles. We find that small amounts of silica increase the fracture stress and strain, but too much filler makes the material become brittle and consequently fracture happens at small deformations. We thus find that as a function of the amount of filler there is an optimum in the breaking resistance at intermediate filler concentrations. We use a modified Griffith theory to establish a direct relation between the material properties and the fracture behavior that agrees with the experiment. 	
1507.00481v2	http://arxiv.org/pdf/1507.00481v2	2015	Self-Elongation with Sequential Folding of a Filament of Bacterial Cells	Ryojiro Honda|Jun-ichi Wakita|Makoto Katori	  Under hard-agar and nutrient-rich conditions, a cell of $Bacillus$ $subtilis$ grows as a single filament owing to the failure of cell separation after each growth and division cycle. The self-elongating filament of cells shows sequential folding processes, and multifold structures extend over an agar plate. We report that the growth process from the exponential phase to the stationary phase is well described by the time evolution of fractal dimensions of the filament configuration. We propose a method of characterizing filament configurations using a set of lengths of multifold parts of a filament. Systems of differential equations are introduced to describe the folding processes that create multifold structures in the early stage of the growth process. We show that the fitting of experimental data to the solutions of equations is excellent, and the parameters involved in our model systems are determined., 	
1507.00672v1	http://arxiv.org/pdf/1507.00672v1	2015	The Elusive Present: Hidden Past and Future Dependency and Why We Build   Models	Pooneh M. Ara|Ryan G. James|James P. Crutchfield	  Modeling a temporal process as if it is Markovian assumes the present encodes all of the process's history. When this occurs, the present captures all of the dependency between past and future. We recently showed that if one randomly samples in the space of structured processes, this is almost never the case. So, how does the Markov failure come about? That is, how do individual measurements fail to encode the past? And, how many are needed to capture dependencies between the past and future? Here, we investigate how much information can be shared between the past and future, but not be reflected in the present. We quantify this elusive information, give explicit calculational methods, and draw out the consequences. The most important of which is that when the present hides past-future dependency we must move beyond sequence-based statistics and build state-based models. 	
1507.01939v2	http://arxiv.org/pdf/1507.01939v2	2016	Nonequilibrium many-body steady states via Keldysh formalism	Mohammad F. Maghrebi|Alexey V. Gorshkov	  Many-body systems with both coherent dynamics and dissipation constitute a rich class of models which are nevertheless much less explored than their dissipationless counterparts. The advent of numerous experimental platforms that simulate such dynamics poses an immediate challenge to systematically understand and classify these models. In particular, nontrivial many-body states emerge as steady states under non-equilibrium dynamics. While these states and their phase transitions have been studied extensively with mean field theory, the validity of the mean field approximation has not been systematically investigated. In this paper, we employ a field-theoretic approach based on the Keldysh formalism to study nonequilibrium phases and phase transitions in a variety of models. In all cases, a complete description via the Keldysh formalism indicates a partial or complete failure of the mean field analysis. Furthermore, we find that an effective temperature emerges as a result of dissipation, and the universal behavior including the dynamics near the steady state is generically described by a thermodynamic universality class. 	
1507.04679v1	http://arxiv.org/pdf/1507.04679v1	2015	Non-equilibrium first order transition marks the mechanical failure of   glasses	D. V. Denisov|M. T. Dang|B. Struth|A. Zaccone|G. H. Wegdam|P. Schall	  Glasses acquire their solid-like properties by cooling from the supercooled liquid via a continuous transition known as the glass transition. Recent research on soft glasses indicates that besides temperature, another route to liquify glasses is by application of stress that forces relaxation and flow. Here we provide experimental evidence that the stress-induced onset of flow of glasses occurs via a sharp first order-like transition. Using simultaneous x-ray scattering during the oscillatory rheology of a colloidal glass, we identify a sharp symmetry change from anisotropic solid to isotropic liquid structure at the transition from the linear to the nonlinear regime. Concomitantly, intensity fluctuations sharply acquire liquid distributions. These observations identify the yielding of glasses to increasing stress as sharp affine-to-nonaffine transition, providing a new conceptual paradigm of the yielding of this technologically important class of materials, and offering new perspectives on the glass transition. 	
1508.00699v1	http://arxiv.org/pdf/1508.00699v1	2015	The roles of jets: CF, CCSN, PN, CEE, GEE, ILOT	Noam Soker	  I review the roles of jet-inflated bubbles in determining the evolution of different astrophysical objects. I discuss astrophysical systems where jets are known to inflate bubbles (cooling flow [CF] clusters; young galaxies; intermediate luminosity optical transients [ILOTs]; bipolar planetary nebulae [PNe]), and systems that are speculated to have jet-inflated bubbles (core collapse supernovae [CCSNe]; common envelope evolution [CEE]; grazing envelope evolution [GEE]). The jets in many of these cases act through a negative jet feedback mechanism (JFM). I discuss the outcomes when the JFM fizzle, or does not work at all. According to this perspective, some very interesting and energetic events owe their existence to the failure of the JFM, including stellar black holes, gamma ray bursts, and type Ia supernovae. 	
1508.02945v1	http://arxiv.org/pdf/1508.02945v1	2015	Gene expression dynamics with stochastic bursts: exact results for a   coarse-grained model	Yen Ting Lin|Charles R. Doering	  We present a theoretical framework to analyze the dynamics of gene expression with stochastic bursts. Beginning with an individual-based model which fully accounts for the messenger RNA (mRNA) and protein populations, we propose a novel expansion of the master equation for the joint process. The resulting coarse-grained model reduces the dimensionality of the system, describing only the protein population while fully accounting for the effects of discrete and fluctuating mRNA population. Closed form expressions for the stationary distribution of the protein population and mean first-passage times of the coarse-grained model are derived and large-scale Monte Carlo simulations show that the analysis accurately describes the individual-based process accounting for mRNA population, in contrast to the failure of commonly proposed diffusion-type models. 	
1509.01566v2	http://arxiv.org/pdf/1509.01566v2	2015	Fast Compact Laser Shutter Using a Direct Current Motor and 3D Printing	Grace H. Zhang|Boris Braverman|Akio Kawasaki|Vladan Vuletić	  We present a mechanical laser shutter design that utilizes a DC electric motor to rotate a blade which blocks and unblocks a light beam. The blade and the main body of the shutter are modeled with computer aided design (CAD) and are produced by 3D printing. Rubber flaps are used to limit the blade's range of motion, reducing vibrations and preventing undesirable blade oscillations. At its nominal operating voltage, the shutter achieves a switching speed of (1.22 $\pm$ 0.02) m/s with 1 ms activation delay and 10 $\mu$s jitter in its timing performance. The shutter design is simple, easy to replicate, and highly reliable, showing no failure or degradation in performance over more than $10^8$ cycles. 	
1509.03158v1	http://arxiv.org/pdf/1509.03158v1	2015	Exploring structural inhomogeneities in glasses during cavitation	Pinaki Chaudhuri|Jürgen Horbach	  Using large-scale molecular dynamics simulations for a system of $10^6$ particles, the response of a dense amorphous solid to the continuous expansion of its volume is investigated. We find that the spatially uniform glassy state becomes unstable via the formation of cavities, which eventually leads to failure. By scanning through a wide range of densities and temperatures, we determine the state points at which the instability occurs and thereby provide estimates of the co-existence density of the resultant glass phase. Evidence for long-lived, inhomogeneous configurations with a negative pressure is found, where the frozen-in glass structure contains spherical cavities or a network of void space. Furthermore, we demonstrate the occurrence of hysteretic effects when the cavitated solid is compressed to regain the dense glassy state. As a result, a new glass state is obtained, the pressure of which is different from the initial one due to small density inhomogeneities that are generated by the dilation-compression cycle. 	
1510.04046v1	http://arxiv.org/pdf/1510.04046v1	2015	On Arthur Eddington's Theory of Everything	Helge Kragh	  From 1929 to his death in 1944, A. Eddington worked on developing a highly ambitious theory of fundamental physics that covered everything in the physical world, from the tiny electron to the universe at large. His unfinished theory included abstract mathematics and spiritual philosophy in a mix which was peculiar to Eddington but hardly intelligible to other scientists. The constants of nature, which he claimed to be able to deduce purely theoretically, were of particular significance to his project. Although highly original, Eddington's attempt to provide physics with a new foundation had to some extent parallels in the ideas of other British physicists, including P. Dirac and E. A. Milne. Eddington's project was however a grand failure in so far that it was rejected by the large majority of physicists. A major reason was his unorthodox view of quantum mechanics. 	
1511.09295v2	http://arxiv.org/pdf/1511.09295v2	2016	Exploiting Path Diversity in Datacenters using MPTCP-aware SDN	Savvas Zannettou|Michael Sirivianos|Fragkiskos Papadopoulos	  Recently, Multipath TCP (MPTCP) has been proposed as an alternative transport approach for datacenter networks. MPTCP provides the ability to split a flow into multiple paths thus providing better performance and resilience to failures. Usually, MPTCP is combined with flow-based Equal-Cost Multi-Path Routing (ECMP), which uses random hashing to split the MPTCP subflows over different paths. However, random hashing can be suboptimal as distinct subflows may end up using the same paths, while other available paths remain unutilized. In this paper, we explore an MPTCP-aware SDN controller that facilitates an alternative routing mechanism for the MPTCP subflows. The controller uses packet inspection to provide deterministic subflow assignment to paths. Using the controller, we show that MPTCP can deliver significantly improved performance when connections are not limited by the access links of hosts. To lessen the effect of throughput limitation due to access links, we also investigate the usage of multiple interfaces at the hosts. We demonstrate, using our modification of the MPTCP Linux Kernel, that using multiple subflows per pair of IP addresses can yield improved performance in multi-interface settings. 	
1512.04503v3	http://arxiv.org/pdf/1512.04503v3	2016	How Soft Gamma Repeaters May Make Fast Radio Bursts	J. I. Katz	  There are several phenomenological similarities between Soft Gamma Repeaters and Fast Radio Bursts, including duty factors, time scales and probable repetition. The sudden release of magnetic energy in a neutron star magnetosphere, as in popular models of SGR, can meet the energy requirements of FRB but requires both the presence of magnetospheric plasma in order that dissipation occur in a transparent region and a mechanism for releasing much of that energy quickly. FRB sources and SGR are distinguished by long-lived (up to thousands of years) current-carrying coronal arches remaining from formation of the young neutron star and their decay ends the phase of SGR/AXP/FRB activity even though "magnetar" fields may persist. Runaway increase in resistance when the current density exceeds a threshold releases magnetostatic energy in a sudden burst and produces high brightness GHz emission of FRB by a coherent process; SGR are produced when released energy thermalizes as an equilibrium pair plasma. Failures of some alternative FRB models and the non-detection of SGR~1806-20 at radio frequencies are discussed in appendices. 	
1512.08335v2	http://arxiv.org/pdf/1512.08335v2	2016	Hybrid Phase Transition into an Absorbing State: Percolation and   Avalanches	Deokjae Lee|S. Choi|M. Stippinger|J. Kertész|B. Kahng	  Interdependent networks are more fragile under random attacks than simplex networks, because interlayer dependencies lead to cascading failures and finally to a sudden collapse. This is a hybrid phase transition (HPT), meaning that at the transition point the order parameter has a jump but there are also critical phenomena related to it. Here we study these phenomena on the Erd\H{o}s--R\'enyi and the two dimensional interdependent networks and show that the hybrid percolation transition exhibits two kinds of critical behaviors: divergence of the fluctuations of the order parameter and power-law size distribution of finite avalanches at a transition point. At the transition point, avalanches of infinite size occur thus the avalanche statistics also has the nature of a HPT. The exponent $\beta_m$ of the order parameter is $1/2$ under general conditions, while the value of the exponent $\gamma_m$ characterizing the fluctuations of the order parameter depends on the system. The critical behavior of the finite avalanches can be described by another set of exponents, $\beta_a$ and $\gamma_a$. These two critical behaviors are coupled by a scaling law: $1-\beta_m=\gamma_a$. 	
1601.07036v1	http://arxiv.org/pdf/1601.07036v1	2016	Coded Packet Transport for Optical Packet/Burst Switched Networks	Katina Kralevska|Harald Oeverby|Danilo Gligoroski	  This paper presents the Coded Packet Transport (CPT) scheme, a novel transport mechanism for Optical Packet/Burst Switched (OPS/OBS) networks. The CPT scheme exploits the combined benefits of source coding by erasure codes and path diversity to provide efficient means for recovering from packet loss due to contentions and path failures, and to provide non-cryptographic secrecy. In the CPT scheme, erasure coding is employed at the OPS/OBS ingress node to form coded packets, which are transmitted on disjoint paths from the ingress node to an egress node in the network. The CPT scheme allows for a unified view of Quality of Service (QoS) in OPS/OBS networks by linking the interactions between survivability, performance and secrecy. We provide analytical models that illustrate how QoS aspects of CPT are affected by the number of disjoint paths, packet overhead and processing delay. 	
1602.06161v1	http://arxiv.org/pdf/1602.06161v1	2016	Abaqus/Standard-based quantification of human cardiac mechanical   properties	Martin Genet|Lik Chuan Lee|Ellen Kuhl|Julius Guccione	  Computational modeling can provide critical insight into existing and potential new surgical procedures, medical or minimally-invasive treatments for heart failure, one of the leading causes of deaths in the world that has reached epidemic proportions. In this paper, we present our Abaqus/Standard-based pipeline to create subject-specific left ventricular models. We first review our generic left ventricular model, and then the personalization process based on magnetic resonance images. Identification of subject-specific cardiac material properties is done by coupling Abaqus/Standard to the python optimization library NL-Opt. Compared to previous studies from our group, the emphasis is here on the fully implicit solving of the model, and the two-parameter optimization of the passive cardiac material properties. 	
1602.06238v2	http://arxiv.org/pdf/1602.06238v2	2016	Emergence of Robustness in Network of Networks	Kevin Roth|Flaviano Morone|Byungjoon Min|Hernán A. Makse	  A model of interdependent networks of networks (NoN) has been introduced recently in the context of brain activation to identify the neural collective influencers in the brain NoN. Here we develop a new approach to derive an exact expression for the random percolation transition in Erd\"{o}s-R\'enyi NoN. Analytical calculations are in excellent agreement with numerical simulations and highlight the robustness of the NoN against random node failures. Interestingly, the phase diagram of the model unveils particular patterns of interconnectivity for which the NoN is most vulnerable. Our results help to understand the emergence of robustness in such interdependent architectures. 	
1602.08461v2	http://arxiv.org/pdf/1602.08461v2	2017	A One-Hop Information Based Geographic Routing Protocol for Delay   Tolerant MANETs	Lei You|Jianbo Li|Changjiang We|Chenqu Dai	  Delay and Disruption Tolerant Networks (DTNs) may lack continuous network connectivity. Routing in DTNs is thus a challenge since it must handle network partitioning, long delays, and dynamic topology. Meanwhile, routing protocols of the traditional Mobile Ad hoc NETworks (MANETs) cannot work well due to the failure of its assumption that most network connections are available. In this article, a geographic routing protocol is proposed for MANETs in delay tolerant situations, by using no more than one-hop information. A utility function is designed for implementing the under-controlled replication strategy. To reduce the overheads caused by message flooding, we employ a criterion so as to evaluate the degree of message redundancy. Consequently a message redundancy coping mechanism is added to our routing protocol. Extensive simulations have been conducted and the results show that when node moving speed is relatively low, our routing protocol outperforms the other schemes such as Epidemic, Spray and Wait, FirstContact in delivery ratio and average hop count, while introducing an acceptable overhead ratio into the network. 	
1603.04099v1	http://arxiv.org/pdf/1603.04099v1	2016	Contagion and Stability in Financial Networks	Seyyed Mostafa Mousavi|Robert Mackay|Alistair Tucker	  This paper investigates two mechanisms of financial contagion that are, firstly, the correlated exposure of banks to the same source of risk, and secondly the direct exposure of banks in the interbank market. It will consider a random network of banks which are connected through the inter-bank market and will discuss the desirable level of banks exposure to the same sources of risk, that is investment in similar portfolios, for different levels of network connectivity when peering through the lens of the systemic cost incurred to the economy from the banks simultaneous failure. It demonstrates that for all levels of network connectivity, certain levels of diversifying individual banks diversifications are not optimum under any condition. So, given an acceptable level of systemic cost, the regulator could let banks decrease their capital buffers by moving away from the non-optimum area. 	
1603.05165v2	http://arxiv.org/pdf/1603.05165v2	2016	Non-Markovianity in atom-surface dispersion forces	F. Intravaia|R. O. Behunin|C. Henkel|K. Busch|D. A. R. Dalvit	  We discuss the failure of the Markov approximation in the description of atom-surface fluctuation-induced interactions, both at equilibrium (Casimir-Polder forces) and out-of-equilibrium (quantum friction). Using general theoretical arguments, we show that the Markov approximation can lead to erroneous predictions of such phenomena with regard to both strength and functional dependencies on system parameters. Our findings highlight the importance of non-Markovian effects in dispersion interactions. In particular, we show that the long-time power-law tails of temporal correlations, and the corresponding low-frequency behavior, of two-time dipole correlations, neglected in the Markovian limit, dramatically affect the prediction of the force. 	
1603.06287v3	http://arxiv.org/pdf/1603.06287v3	2016	Large deviations of radial statistics in the two-dimensional   one-component plasma	Fabio Deelan Cunden|Francesco Mezzadri|Pierpaolo Vivo	  The two-dimensional one-component plasma is an ubiquitous model for several vortex systems. For special values of the coupling constant $\beta q^2$ (where $q$ is the particles charge and $\beta$ the inverse temperature), the model also corresponds to the eigenvalues distribution of normal matrix models. Several features of the system are discussed in the limit of large number $N$ of particles for generic values of the coupling constant. We show that the statistics of a class of radial observables produces a rich phase diagram, and their asymptotic behaviour in terms of large deviation functions is calculated explicitly, including next-to-leading terms up to order 1/N. We demonstrate a split-off phenomenon associated to atypical fluctuations of the edge density profile. We also show explicitly that a failure of the fluid phase assumption of the plasma can break a genuine $1/N$-expansion of the free energy. Our findings are corroborated by numerical comparisons with exact finite-N formulae valid for $\beta q^2=2$. 	
1604.04414v2	http://arxiv.org/pdf/1604.04414v2	2016	Partial reconstruction of the rotational motion of Philae spacecraft   during its landing on comet 67P/Churyumov-Gerasimenko	Tamás Baranyai|András Balázs|Péter L. Várkonyi	  This paper presents a partial reconstruction of the rotational dynamics of the Philae spacecraft upon landing on comet 67P/Churyumov-Gerasimenko as part of ESA's Rosetta mission. We analyze the motion and the events triggered by the failure to fix the spacecraft to the comet surface at the time of the first touchdown. Dynamic trajectories obtained by numerical simulation of a 7 degree-of-freedom mechanical model of the spacecraft are fitted to directions of incoming solar radiation inferred from in-situ measurements of the electric power provided by the solar panels. The results include a lower bound of the angular velocity of the lander immediately after its first touchdown. Our study also gives insight into the effect of the programmed turn-off of the stabilizing gyroscope after touchdown; the important dynamical consequences of a small collision during Philae's journey; and the probability that a similar landing scenario harms the operability of this type of spacecraft. 	
1604.06881v1	http://arxiv.org/pdf/1604.06881v1	2016	Micro-mechanical Failure Analysis of Wet Granular Matter	Konstantin Melnikov|Falk K. Wittel|Hans J. Herrmann	  We employ a novel fluid-particle model to study the shearing behavior of granular soils under different saturation levels, ranging from the dry material via the capillary bridge regime to higher saturation levels with percolating clusters. The full complexity of possible liquid morphologies is taken into account, implying the formation of isolated arbitrary-sized liquid clusters with individual Laplace pressures that evolve by liquid exchange via films on the grain surface. Liquid clusters can grow in size, shrink, merge and split, depending on local conditions, changes of accessible liquid and the pore space morphology determined by the granular phase. This phase is represented by a discrete particle model based on Contact Dynamics, where capillary forces exerted from a liquid phase add to the motion of spherical particles. We study the macroscopic response of the system due to an external compression force at various liquid contents with the help of triaxial shear tests. Additionally, the change in liquid cluster distributions during the compression due to the deformation of the pore space is evaluated close to the critical load. 	
1604.08094v1	http://arxiv.org/pdf/1604.08094v1	2016	Optimal processes for probabilistic work extraction beyond the second   law	Vasco Cavina|Andrea Mari|Vittorio Giovannetti	  According to the second law of thermodynamics, for every transformation performed on a system which is in contact with an environment of fixed temperature, the extracted work is bounded by the decrease of the free energy of the system. However, in a single realization of a generic process, the extracted work is subject to statistical fluctuations which may allow for probabilistic violations of the previous bound. We are interested in enhancing this effect, i.e. we look for thermodynamic processes that maximize the probability of extracting work above a given arbitrary threshold. For any process obeying the Jarzynski identity, we determine an upper bound for the work extraction probability that depends also on the minimum amount of work that we are willing to extract in case of failure, or on the average work we wish to extract from the system. Then we show that this bound can be saturated within the thermodynamic formalism of quantum discrete processes composed by sequences of unitary quenches and complete thermalizations. We explicitly determine the optimal protocol which is given by two quasi-static isothermal transformations separated by a finite unitary quench. 	
1605.04069v1	http://arxiv.org/pdf/1605.04069v1	2016	Availability Aware Continuous Replica Placement Problem	Abdullah Yousafzai|Abdullah Gani|Rafidah Md Noor	  Replica placement (RP) intended at producing a set of duplicated data items across the nodes of a distributed system in order to optimize fault tolerance, availability, system performance load balancing. Typically, RP formulations employ dynamic methods to change the replica placement in the system potentially upon user request profile. Continuous Replica Placement Problem (CRPP) is an extension of replica placement problem that takes into consideration the current replication state of the distributed system along with user request profile to define a new replication scheme, subject to optimization criteria and constraints. This paper proposes an alternative technique, named Availability Aware Continuous Replica Placement Problem (AACRPP).AACRPP can be defined as: Given an already defined replica placement scheme, a user request profile, and a node failure profile define a new replication scheme, subject to optimization criteria and constraints. In this effort we use modified greedy heuristics from the CRPP and investigated the proposed mechanism using a trace driven java based simulation. 	
1605.05378v2	http://arxiv.org/pdf/1605.05378v2	2016	Frictional sliding without geometrical reflection symmetry	Michael Aldam|Yohai Bar-Sinai|Ilya Svetlizky|Efim A. Brener|Jay Fineberg|Eran Bouchbinder	  The dynamics of frictional interfaces play an important role in many physical systems spanning a broad range of scales. It is well-known that frictional interfaces separating two dissimilar materials couple interfacial slip and normal stress variations, a coupling that has major implications on their stability, failure mechanism and rupture directionality. In contrast, interfaces separating identical materials are traditionally assumed not to feature such a coupling due to symmetry considerations. We show, combining theory and experiments, that interfaces which separate bodies made of macroscopically identical materials, but lack geometrical reflection symmetry, generically feature such a coupling. We discuss two applications of this novel feature. First, we show that it accounts for a distinct, and previously unexplained, experimentally observed weakening effect in frictional cracks. Second, we demonstrate that it can destabilize frictional sliding which is otherwise stable. The emerging framework is expected to find applications in a broad range of systems. 	
1606.00315v3	http://arxiv.org/pdf/1606.00315v3	2017	Universal Finite-Size Scaling for Percolation Theory in High Dimensions	Ralph Kenna|Bertrand Berche	  We present a unifying, consistent, finite-size-scaling picture for percolation theory bringing it into the framework of a general, renormalization-group-based, scaling scheme for systems above their upper critical dimensions $d_c$. Behaviour at the critical point is non-universal in $d>d_c=6$ dimensions. Proliferation of the largest clusters, with fractal dimension $4$, is associated with the breakdown of hyperscaling there when free boundary conditions are used. But when the boundary conditions are periodic, the maximal clusters have dimension $D=2d/3$, and obey random-graph asymptotics. Universality is instead manifest at the pseudocritical point, where the failure of hyperscaling in its traditional form is universally associated with random-graph-type asymptotics for critical cluster sizes, independent of boundary conditions. 	
1606.00919v4	http://arxiv.org/pdf/1606.00919v4	2017	Global warming: Temperature estimation in annealers	Jack Raymond|Sheir Yarkoni|Evgeny Andriyash	  Sampling from a Boltzmann distribution is NP-hard and so requires heuristic approaches. Quantum annealing is one promising candidate. The failure of annealing dynamics to equilibrate on practical time scales is a well understood limitation, but does not always prevent a heuristically useful distribution from being generated. In this paper we evaluate several methods for determining a useful operational temperature range for annealers. We show that, even where distributions deviate from the Boltzmann distribution due to ergodicity breaking, these estimates can be useful. We introduce the concepts of local and global temperatures that are captured by different estimation methods. We argue that for practical application it often makes sense to analyze annealers that are subject to post-processing in order to isolate the macroscopic distribution deviations that are a practical barrier to their application. 	
1606.03390v1	http://arxiv.org/pdf/1606.03390v1	2016	Microscopic description for the emergence of collective dissipation in   extended quantum systems	Fernando Galve|Antonio Mandarino|Matteo G. A. Paris|Claudia Benedetti|Roberta Zambrini	  Practical implementations of quantum technology are limited by unavoidable effects of decoherence and dissipation. With achieved experimental control for individual atoms and photons, more complex platforms composed by several units can be assembled enabling distinctive forms of dissipation and decoherence, in independent heat baths or collectively into a common bath, with dramatic consequences for the preservation of quantum coherence. The cross-over between these two regimes has been widely attributed in the literature to the system units being farther apart than the bath's correlation length. Starting from a microscopic model of a structured environment (a crystal) sensed by two bosonic probes, here we show the failure of such conceptual relation, and identify the exact physical mechanism underlying this cross-over, displaying a sharp contrast between dephasing and dissipative baths. Depending on the frequency of the system and, crucially, on its orientation with respect to the crystal axes, collective dissipation becomes possible for very large distances between probes, opening new avenues to deal with decoherence in phononic baths. 	
1607.08725v1	http://arxiv.org/pdf/1607.08725v1	2016	Recurrent Neural Machine Translation	Biao Zhang|Deyi Xiong|Jinsong Su	  The vanilla attention-based neural machine translation has achieved promising performance because of its capability in leveraging varying-length source annotations. However, this model still suffers from failures in long sentence translation, for its incapability in capturing long-term dependencies. In this paper, we propose a novel recurrent neural machine translation (RNMT), which not only preserves the ability to model varying-length source annotations but also better captures long-term dependencies. Instead of the conventional attention mechanism, RNMT employs a recurrent neural network to extract the context vector, where the target-side previous hidden state serves as its initial state, and the source annotations serve as its inputs. We refer to this new component as contexter. As the encoder, contexter and decoder in our model are all derivable recurrent neural networks, our model can still be trained end-to-end on large-scale corpus via stochastic algorithms. Experiments on Chinese-English translation tasks demonstrate the superiority of our model to attention-based neural machine translation, especially on long sentences. Besides, further analysis of the contexter revels that our model can implicitly reflect the alignment to source sentence. 	
1608.01606v1	http://arxiv.org/pdf/1608.01606v1	2016	Particle Traces for Detecting Divergent Robot Behavior	Samuel Zapolsky|Evan Drumwright	  The motion of robots and objects in our world is often highly dependent upon contact. When contact is expected but does not occur or when contact is not expected but does occur, robot behavior diverges from plan, often disastrously. This paper describes an approach that uses simulation to detect possible such behavioral divergences on real robots. This approach, and others like it, could be applied to validation of robot behaviors, mechanism design, and even online planning.   The particle trace approach samples robot modeling parameters, sensory readings, and state estimates to evaluate a robot's behavior statistically over a range of conditions. We demonstrate that combining even coarse estimates of state and modeling parameters with fast multibody simulation can be sufficient to detect divergent robot behavior and characterize robot performance in the real world. Correspondingly, this approach could be used to assess risk and find and analyze likely failures, given the extensive data that such simulations can generate.   We assess this approach on actuated, high degree-of-freedom robot locomotion examples, a picking task with a fixed-base manipulator, and an unpowered passive dynamic walker. This research works toward understanding how multi-rigid body simulations can better characterize the behavior of robots without significantly compliant elements. 	
1608.03096v1	http://arxiv.org/pdf/1608.03096v1	2016	Protection of Accelerator Hardware: RF systems	S. -H. Kim	  The radio-frequency (RF) system is the key element that generates electric fields for beam acceleration. To keep the system reliable, a highly sophisticated protection scheme is required, which also should be designed to ensure a good balance between beam availability and machine safety. Since RF systems are complex, incorporating high-voltage and high-power equipment, a good portion of machine downtime typically comes from RF systems. Equipment and component damage in RF systems results in long and expensive repairs. Protection of RF system hardware is one of the oldest machine protection concepts, dealing with the protection of individual high-power RF equipment from breakdowns. As beam power increases in modern accelerators, the protection of accelerating structures from beam-induced faults also becomes a critical aspect of protection schemes. In this article, an overview of the RF system is given, and selected topics of failure mechanisms and examples of protection requirements are introduced. 	
1608.03398v1	http://arxiv.org/pdf/1608.03398v1	2016	Robust Relativistic Bit Commitment	Kaushik Chakraborty|André Chailloux|Anthony Leverrier	  Relativistic cryptography exploits the fact that no information can travel faster than the speed of light in order to obtain security guarantees that cannot be achieved from the laws of quantum mechanics alone. Recently, Lunghi et al [Phys. Rev. Lett. 2015] presented a bit commitment scheme where each party uses two agents that exchange classical information in a synchronized fashion, and that is both hiding and binding. A caveat is that the commitment time is intrinsically limited by the spatial configuration of the players, and increasing this time requires the agents to exchange messages during the whole duration of the protocol. While such a solution remains computationally attractive, its practicality is severely limited in realistic settings since all communication must remain perfectly synchronized at all times.   In this work, we introduce a robust protocol for relativistic bit commitment that tolerates failures of the classical communication network. This is done by adding a third agent to both parties. Our scheme provides a quadratic improvement in terms of expected sustain time compared to the original protocol, while retaining the same level of security. 	
1609.01142v1	http://arxiv.org/pdf/1609.01142v1	2016	Neurofilaments function as shock absorbers: compression response arising   from disordered proteins	Micha Kornreich|Eti Malka-Gibor|Ben Zuker|Adi Laser-Azogui|Roy Beck	  What can cells gain by using disordered, rather than folded, proteins in the architecture of their skeleton? Disordered proteins take multiple co-existing conformations, and often contain segments which act as random-walk-shaped polymers. Using X-ray scattering we measure the compression response of disordered protein hydrogels, which are the main stress-responsive component of neuron cells. We find that at high compression their mechanics are dominated by gas-like steric and ionic repulsions. At low compression, specific attractive interactions dominate. This is demonstrated by the considerable hydrogel expansion induced by the truncation of critical short protein segments. Accordingly, the floppy disordered proteins form a weakly cross-bridged hydrogel, and act as shock absorbers that sustain large deformations without failure. 	
1609.03157v1	http://arxiv.org/pdf/1609.03157v1	2016	A centralized reinforcement learning method for multi-agent job   scheduling in Grid	Milad Moradi	  One of the main challenges in Grid systems is designing an adaptive, scalable, and model-independent method for job scheduling to achieve a desirable degree of load balancing and system efficiency. Centralized job scheduling methods have some drawbacks, such as single point of failure and lack of scalability. Moreover, decentralized methods require a coordination mechanism with limited communications. In this paper, we propose a multi-agent approach to job scheduling in Grid, named Centralized Learning Distributed Scheduling (CLDS), by utilizing the reinforcement learning framework. The CLDS is a model free approach that uses the information of jobs and their completion time to estimate the efficiency of resources. In this method, there are a learner agent and several scheduler agents that perform the task of learning and job scheduling with the use of a coordination strategy that maintains the communication cost at a limited level. We evaluated the efficiency of the CLDS method by designing and performing a set of experiments on a simulated Grid system under different system scales and loads. The results show that the CLDS can effectively balance the load of system even in large scale and heavy loaded Grids, while maintains its adaptive performance and scalability. 	
1609.06587v1	http://arxiv.org/pdf/1609.06587v1	2016	Comment on "On Uniqueness of SDE Decomposition in A-type Stochastic   Integration" [arXiv:1603.07927v1]	Peijie Zhou|Tiejun Li	  The uniqueness issue of SDE decomposition theory proposed by Ao and his co-workers has recently been discussed. A comprehensive study to investigate connections among different landscape theories [J. Chem. Phys. 144, 094109 (2016)] has pointed out that the decomposition is generally not unique, while Ao et al. (arXiv:1603.07927v1) argues that such conclusions are "incorrect" because of the missing boundary conditions. In this comment, we will combine literatures research and concrete examples to show that the concrete and effective boundary conditions have not been proposed to guarantee the uniqueness, hence the arguments in [arXiv:1603.07927v1] are not sufficient. Moreover, we show that the "uniqueness" of the O-U process decomposition referred by YTA paper is unable to serve as a counterexample to ZL's result since additional assumptions have been made implicitly beyond the original SDE decomposition framework, which cannot be applied to general nonlinear cases. Some other issues such as the failure of gradient expansion method will also be discussed. Our demonstration contributes to better understanding of the relevant papers as well as the SDE decomposition theory. 	
1609.07275v1	http://arxiv.org/pdf/1609.07275v1	2016	Critical behavior of $k$-core percolation: Numerical studies	Deokjae Lee|Minjae Jo|B. Kahng	  $k$-Core percolation has served as a paradigmatic model of discontinuous percolation for a long time. Recently it was revealed that the order parameter of $k$-core percolation of random networks additionally exhibits critical behavior. Thus $k$-core percolation exhibits a hybrid phase transition. Unlike the critical behaviors of ordinary percolation that are well understood, those of hybrid percolation transitions have not been thoroughly understood yet. Here, we investigate the critical behavior of $k$-core percolation of Erd\H{o}s-R\'enyi networks. We find numerically that the fluctuations of the order parameter and the mean avalanche size diverge in different ways. Thus, we classify the critical exponents into two types: those associated with the order parameter and those with finite avalanches. The conventional scaling relations hold within each set, however, these two critical exponents are coupled. Finally we discuss some universal features of the critical behaviors of $k$-core percolation and the cascade failure model on multiplex networks. 	
1609.07850v1	http://arxiv.org/pdf/1609.07850v1	2016	A fiber-bundle model for the continuum deformation of brittle material	K. Z. Nanjo	  The deformation of brittle material is primarily accompanied by micro-cracking and faulting. However, it has often been found that continuum fluid models, usually based on a non-Newtonian viscosity, are applicable. To explain this rheology, we use a fiber-bundle model, which is a model of damage mechanics. In our analyses, yield stress was introduced. Above this stress, we hypothesize that the fibers begin to fail and a failed fiber is replaced by a new fiber. This replacement is analogous to a micro-crack or an earthquake and its iteration is analogous to stick-slip motion. Below the yield stress, we assume that no fiber failure occurs, and the material behaves elastically. We show that deformation above yield stress under a constant strain rate for a sufficient amount of time can be modeled as an equation similar to that used for non-Newtonian viscous flow. We expand our rheological model to treat viscoelasticity and consider a stress relaxation problem. The solution can be used to understand aftershock temporal decay following an earthquake. Our results provide justification for the use of a non-Newtonian viscous flow to model the continuum deformation of brittle materials. 	
1610.08550v1	http://arxiv.org/pdf/1610.08550v1	2016	First principles study of band line up at defective metal-oxide   interface: oxygen point defects at Al/SiO_2 interface	Eric Tea|Jianqiu Huang|Celine Hin	  The dielectric breakdown at metal-oxide interfaces is a critical electronic device failure mechanism. Electronic tunneling through dielectric layers is a well-accepted explanation for this phenomenon. Theoretical band alignment studies, providing information about tunneling, have already been conducted in the literature for metal-oxide interfaces. However, most of the time materials were assumed defect free. Oxygen vacancies being very common in oxides, their effect on band lineup is of prime importance in understanding electron tunneling in realistic materials and devices. This work explores the effect of oxygen vacancy and oxygen di-vacancy at the Al/SiO2 interface on the band line up within Density Functional Theory using PBE0 hybrid exchange and correlation functional. It is found that the presence of defects at the interface, and their charge state, strongly alters the band line up. 	
1611.03065v2	http://arxiv.org/pdf/1611.03065v2	2016	Toward Smart Moving Target Defense for Linux Container Resiliency	Mohamed Azab|Bassem Mokhtar|Amr S. Abed|Mohamed Eltoweissy	  This paper presents ESCAPE, an informed moving target defense mechanism for cloud containers. ESCAPE models the interaction between attackers and their target containers as a "predator searching for a prey" search game. Live migration of Linux-containers (prey) is used to avoid attacks (predator) and failures. The entire process is guided by a novel host-based behavior-monitoring system that seamlessly monitors containers for indications of intrusions and attacks. To evaluate ESCAPE effectiveness, we simulated the attack avoidance process based on a mathematical model mimicking the prey-vs-predator search game. Simulation results show high container survival probabilities with minimal added overhead. 	
1611.04269v2	http://arxiv.org/pdf/1611.04269v2	2017	Exactly solvable model for a velocity jump observed in crack propagation   in viscoelastic solids	Naoyuki Sakumichi|Ko Okumura	  Needs to impart appropriate elasticity and high toughness to viscoelastic polymer materials are ubiquitous in industries such as concerning automobiles and medical devices. One of the major problems to overcome for toughening is catastrophic failure linked to a velocity jump, i.e., a sharp transition in the velocity of crack propagation occurred in a narrow range of the applied load. However, its physical origin has remained an enigma despite previous studies over 35 years. Here, we propose an exactly solvable model that exhibits the velocity jump incorporating linear viscoelasticity with a cutoff length for a continuum description. With the exact solution, we elucidate the physical origin of the velocity jump: it emerges from a dynamic glass transition in the vicinity of the propagating crack tip. We further quantify the velocity jump together with slow- and fast-velocity regimes of crack propagation, which would stimulate the development of tough polymer materials. 	
1611.05113v2	http://arxiv.org/pdf/1611.05113v2	2017	Efficient Diffusion on Region Manifolds: Recovering Small Objects with   Compact CNN Representations	Ahmet Iscen|Giorgos Tolias|Yannis Avrithis|Teddy Furon|Ondrej Chum	  Query expansion is a popular method to improve the quality of image retrieval with both conventional and CNN representations. It has been so far limited to global image similarity. This work focuses on diffusion, a mechanism that captures the image manifold in the feature space. The diffusion is carried out on descriptors of overlapping image regions rather than on a global image descriptor like in previous approaches. An efficient off-line stage allows optional reduction in the number of stored regions. In the on-line stage, the proposed handling of unseen queries in the indexing stage removes additional computation to adjust the precomputed data. We perform diffusion through a sparse linear system solver, yielding practical query times well below one second. Experimentally, we observe a significant boost in performance of image retrieval with compact CNN descriptors on standard benchmarks, especially when the query object covers only a small part of the image. Small objects have been a common failure case of CNN-based retrieval. 	
1612.01260v1	http://arxiv.org/pdf/1612.01260v1	2016	Real-time Collision Handling in Railway Network:An Agent-based Approach	Poulami Dalapati|Abhijeet Padhy|Bhawana Mishra|Animesh Dutta|Swapan Bhattacharya	  Advancement in intelligent transportation systems with complex operations requires autonomous planning and management to avoid collisions in day-to-day traffic. As failure and/or inadequacy in traffic safety system are life-critical, such collisions must be detected and resolved in an efficient way to manage continuously rising traffic. In this paper, we address different types of collision scenarios along with their early detection and resolution techniques in a complex railway system. In order to handle collisions dynamically in distributed manner, a novel agent based solution approach is proposed using the idea of max-sum algorithm, where each agent (train agent, station agent, and junction agent) communicates and cooperates with others to generate a good feasible solution that keeps the system in a safe state, i.e., collision free. We implement the proposed mechanism in Java Agent DEvelopment Framework (JADE). The results are evaluated with exhaustive experiments and compared with different existing collision handling methods to show the efficiency of our proposed approach. 	
1612.02807v1	http://arxiv.org/pdf/1612.02807v1	2016	Random versus maximum entropy models of neural population activity	Ulisse Ferrari|Tomoyuki Obuchi|Thierry Mora	  The principle of maximum entropy provides a useful method for inferring statistical mechanics models from observations in correlated systems, and is widely used in a variety of fields where accurate data are available. While the assumptions underlying maximum entropy are intuitive and appealing, its adequacy for describing complex empirical data has been little studied in comparison to alternative approaches. Here data from the collective spiking activity of retinal neurons is reanalysed. The accuracy of the maximum entropy distribution constrained by mean firing rates and pairwise correlations is compared to a random ensemble of distributions constrained by the same observables. In general, maximum entropy approximates the true distribution better than the typical or mean distribution from that ensemble. This advantage improves with population size, with groups as small as 8 being almost always better described by maximum entropy. Failure of maximum entropy to outperform random models is found to be associated with strong correlations in the population. 	
1701.01702v1	http://arxiv.org/pdf/1701.01702v1	2017	Virtual Network Migration on the GENI Wide-Area SDN-Enabled   Infrastructure	Yimeng Zhao|Samantha Lo|Ellen Zegura|Niky Riga|Mostafa Ammar	  A virtual network (VN) contains a collection of virtual nodes and links assigned to underlying physical resources in a network substrate. VN migration is the process of remapping a VN's logical topology to a new set of physical resources to provide failure recovery, energy savings, or defense against attack. Providing VN migration that is transparent to running applications is a significant challenge. Efficient migration mechanisms are highly dependent on the technology deployed in the physical substrate. Prior work has considered migration in data centers and in the PlanetLab infrastructure. However, there has been little effort targeting an SDN-enabled wide-area networking environment - an important building block of future networking infrastructure. In this work, we are interested in the design, implementation and evaluation of VN migration in GENI as a working example of such a future network. We identify and propose techniques to address key challenges: the dynamic allocation of resources during migration, managing hosts connected to the VN, and flow table migration sequences to minimize packet loss. We find that GENI's virtualization architecture makes transparent and efficient migration challenging. We suggest alternatives that might be adopted in GENI and are worthy of adoption by virtual network providers to facilitate migration. 	
1701.03404v3	http://arxiv.org/pdf/1701.03404v3	2017	Generalized $k$-core pruning process on directed networks	Jin-Hua Zhao	  The resilience of a complex interconnected system concerns the size of the macroscopic functioning node clusters after external perturbations based on a random or designed scheme. For a representation of the interconnected systems with directional or asymmetrical interactions among constituents, the directed network is a convenient choice. Yet how the interaction directions affect the network resilience still lacks thorough exploration. Here, we study the resilience of directed networks with a generalized $k$-core pruning process as a simple failure procedure based on both the in- and out-degrees of nodes, in which any node with an in-degree $< k_{in}$ or an out-degree $< k_{ou}$ is removed iteratively. With an explicitly analytical framework, we can predict the relative sizes of residual node clusters on uncorrelated directed random graphs. We show that the discontinuous transitions rise for cases with $k_{in} \geq 2$ or $k_{ou} \geq 2$, and the unidirectional interactions among nodes drive the networks more vulnerable against perturbations based on in- and out-degrees separately. 	
1702.01318v1	http://arxiv.org/pdf/1702.01318v1	2017	Concurrent factors determine toughening in the hydraulic fracture of   poroelastic composites	Alessandro Lucantonio|Giovanni Noselli	  Brittle materials fail catastrophically. In consequence of their limited flaw-tolerance, failure occurs by localized fracture and is typically a dynamic process. Recently, experiments on epithelial cell monolayers have revealed that this scenario can be significantly modified when the material susceptible to cracking is adhered to a hydrogel substrate. Thanks to the hydraulic coupling between the brittle layer and the poroelastic substrate, such a composite can develop a toughening mechanism that relies on the simultaneous growth of multiple cracks. Here, we study this remarkable behaviour by means of a detailed model, and explore how the material and loading parameters concur in determining the macroscopic toughness of the system. By extending a previous study, our results show that rapid loading conveys material toughness by promoting distributed cracking. Moreover, our theoretical findings may suggest innovative architectures of flaw-insensitive materials with higher toughness. 	
1702.05828v1	http://arxiv.org/pdf/1702.05828v1	2017	Failure and Scaling of Graphene Nanocomposites	Cory Hage Mefford|Yao Qiao|Marco Salviato	  This work proposes an investigation on the scaling of the structural strength of polymer/graphene nanocomposites. To this end, fracture tests on geometrically scaled Single Edge Notch Bending (SENB) specimens with varying contents of graphene were conducted to study the effects of nanomodification on the scaling.   It is shown that, while the strength of the pristine polymer scales according to Linear Elastic Fracture Mechanics (LEFM), this is not the case for nanocomposites, even for very low graphene contents. In fact, small specimens exhibited a more pronounced ductility with limited scaling and a significant deviation from LEFM whereas larger specimens behaved in a more brittle way, with scaling of nominal strength closer to the one predicted by LEFM.   This behavior, due to the significant size of the Fracture Process Zone (FPZ) compared to the specimen size, needs to be taken into serious consideration. In facts, it is shown that, for the specimen sizes investigated in this work, neglecting the non-linear effects of the FPZ can lead to an underestimation of the fracture energy as high as 113%, this error decreasing for increasing specimen sizes. 	
1703.06740v1	http://arxiv.org/pdf/1703.06740v1	2017	Heterogeneous micro-structure of percolation in sparse networks	Reimer Kuehn|Tim Rogers	  We examine the heterogeneous responses of individual nodes in sparse networks to the random removal of a fraction of edges. Using the message-passing formulation of percolation, we discover considerable variation across the network in the probability of a particular node to remain part of the giant component, and in the expected size of small clusters containing that node. In the vicinity of the percolation threshold, weakly non-linear analysis reveals that node-to-node heterogeneity is captured by the recently introduced notion of non-backtracking centrality. We supplement these results for fixed finite networks by a population dynamics approach to analyse random graph models in the infinite system size limit, also providing closed-form approximations for the large mean degree limit of Erd\H{o}s-R\'enyi random graphs. Interpreted in terms of the application of percolation to real-world processes, our results shed light on the heterogeneous exposure of different nodes to cascading failures, epidemic spread, and information flow. 	
1705.01670v1	http://arxiv.org/pdf/1705.01670v1	2017	Qubit-loss-free fusion of W states employing weak cross-Kerr   nonlinearities	Meiyu Wang|Quanzhi Hao|Fengli Yan|Ting Gao	  With the assistance of weak cross-Kerr nonlinearities, we introduce an optical scheme to fuse two small-size polarization entangled W states into a large-scale W state without qubit loss, i.e.,$\mathrm{W}_{n+m}$ state can be generated from an $n$-qubit W state and a $m$-qubit W state. To complete the fusion task, two polarization entanglement processes and one spatial entanglement process are applied. The fulfillments of the above processes are contributed by a cross-Kerr nonlinear interaction between the signal photons and a coherent state via Kerr media. We analyze the resource cost and the success probability of the scheme. There is no complete failure output in our fusion mechanism, and all the garbage states are recyclable. In addition, there is no need for any controlled quantum gate and any ancillary photon, so it is simple and feasible under the current experiment technology. 	
1705.02561v1	http://arxiv.org/pdf/1705.02561v1	2017	A Reconnaissance Attack Mechanism for Fixed-Priority Real-Time Systems	Chien-Ying Chen|AmirEmad Ghassami|Sibin Mohan|Negar Kiyavash|Rakesh B. Bobba|Rodolfo Pellizzoni|Man-Ki Yoon	  In real-time embedded systems (RTS), failures due to security breaches can cause serious damage to the system, the environment and/or injury to humans. Therefore, it is very important to understand the potential threats and attacks against these systems. In this paper we present a novel reconnaissance attack that extracts the exact schedule of real-time systems designed using fixed priority scheduling algorithms. The attack is demonstrated on both a real hardware platform and a simulator, with a high success rate. Our evaluation results show that the algorithm is robust even in the presence of execution time variation. 	
1705.08884v1	http://arxiv.org/pdf/1705.08884v1	2017	Uncovering the Flop of the EU Cookie Law	Martino Trevisan|Stefano Traverso|Hassan Metwalley|Marco Mellia	  In 2002, the European Union (EU) introduced the ePrivacy Directive to regulate the usage of online tracking technologies. Its aim is to make tracking mechanisms explicit while increasing privacy awareness in users. It mandates websites to ask for explicit consent before using any kind of profiling methodology, e.g., cookies. Starting from 2013 the Directive is mandatory, and now most of European websites embed a "Cookie Bar" to explicitly ask user's consent. To the best of our knowledge, no study focused in checking whether a website respects the Directive. For this, we engineer CookieCheck, a simple tool that makes this check automatic. We use it to run a measure- ment campaign on more than 35,000 websites. Results depict a dramatic picture: 65% of websites do not respect the Directive and install tracking cookies before the user is even offered the accept button. In few words, we testify the failure of the ePrivacy Directive. Among motivations, we identify the absence of rules enabling systematic auditing procedures, the lack of tools to verify its implementation by the deputed agencies, and the technical difficulties of webmasters in implementing it. 	
1705.09292v2	http://arxiv.org/pdf/1705.09292v2	2017	Coannihilation without chemical equilibrium	Mathias Garny|Jan Heisig|Benedikt Lülf|Stefan Vogl	  Chemical equilibrium is a commonly made assumption in the freeze-out calculation of coannihilating dark matter. We explore the possible failure of this assumption and find a new conversion-driven freeze-out mechanism. Considering a representative simplified model inspired by supersymmetry with a neutralino- and sbottom-like particle we find regions in parameter space with very small couplings accommodating the measured relic density. In this region freeze-out takes place out of chemical equilibrium and dark matter self-annihilation is thoroughly inefficient. The relic density is governed primarily by the size of the conversion terms in the Boltzmann equations. Due to the small dark matter coupling the parameter region is immune to direct detection but predicts an interesting signature of disappearing tracks or displaced vertices at the LHC. Unlike freeze-in or superWIMP scenarios, conversion-driven freeze-out is not sensitive to the initial conditions at the end of reheating. 	
1706.04451v2	http://arxiv.org/pdf/1706.04451v2	2017	Correlations between thresholds and degrees: An analytic approach to   model attacks and failure cascades	Rebekka Burkholz|Frank Schweitzer	  Two node variables determine the evolution of cascades in random networks: a node's degree and threshold. Correlations between both fundamentally change the robustness of a network, yet, they are disregarded in standard analytic methods as local tree or heterogeneous mean field approximations because of the bad tractability of order statistics. We show how they become tractable in the thermodynamic limit of infinite network size. This enables the analytic description of node attacks that are characterized by threshold allocations based on node degree. Using two examples, we discuss possible implications of irregular phase transitions and different speeds of cascade evolution for the control of cascades. 	
1706.05536v1	http://arxiv.org/pdf/1706.05536v1	2017	dSDiVN: a distributed Software-Defined Networking architecture for   Infrastructure-less Vehicular Networks	Ahmed Alioua|Sidi-Mohammed Senouci|Samira Moussaoui	  In the last few years, the emerging network architecture paradigm of Software-Defined Networking (SDN), has become one of the most important technology to manage large scale networks such as Vehicular Ad-hoc Networks (VANETs). Recently, several works have shown interest in the use of SDN paradigm in VANETs. SDN brings flexibility, scalability and management facility to current VANETs. However, almost all of proposed Software-Defined VANET (SDVN) architectures are infrastructure-based. This paper will focus on how to enable SDN in infrastructure-less vehicular environments. For this aim, we propose a novel distributed SDN-based architecture for uncovered infrastructure-less vehicular scenarios. It is a scalable cluster-based architecture with distributed mobile controllers and a reliable fall back recovery mechanism based on self-organized clustering and failure anticipation. 	
1707.01098v2	http://arxiv.org/pdf/1707.01098v2	2017	Dual lattice functional renormalization group for the   Berezinskii-Kosterlitz-Thouless transition: irrelevance of amplitude and   out-of-plane fluctuations	Jan Krieg|Peter Kopietz	  We develop a new functional renormalization group (FRG) approach for the two-dimensional XY-model by combining the lattice FRG proposed by Machado and Dupuis [Phys. Rev. E 82, 041128 (2010)] with a duality transformation which explicitly introduces vortices via an integer-valued field. We show that the hierarchy of FRG flow equations for the infinite set of relevant and marginal couplings of the model can be reduced to the well-known Kosterlitz-Thouless renormalization group equations for the renormalized temperature and the vortex fugacity. Within our approach it is straightforward to include weak amplitude as well as out-of-plane fluctuations of the spins, which lead to additional interactions between the vortices that do not spoil the Berezinskii-Kosterlitz-Thouless transition. This demonstrates that previous failures to obtain a line of true fixed points within the FRG are a mathematical artifact of insufficient truncation schemes. 	
1707.02487v1	http://arxiv.org/pdf/1707.02487v1	2017	The energy spectrum of ultraheavy nuclei above 10$^{20}$ eV	Antonio Codino	  A major feature of the energy spectrum of the cosmic radiation above 10$^{19}$ eV is the increasing fraction of heavy nuclei with respect to light nuclei. This fact, along with other simple assumptions, is adopted to calculate the energy spectrum of the cosmic radiation up to 2.4$\times$10$^{21}$ eV. The predicted spectrum maintains the index of 2.67 observed at lower energies which is the basic, known, empirical well-assessed feature of the physical mechanism accelerating cosmic rays in the Galaxy. Indeed above 10$^{19}$ eV the injection of nuclei is inhibited by some filter and this inhibition causes a staircase profile of the energy spectrum. It is argued that particle injection failure versus energy commences with protons, followed by Helium and then by other heavier nuclei up to Uranium. Around 7.5$\times$10$^{20}$ the cosmic radiation consists solely of nuclei heavier than Copper and the estimated intensity is 1.8$\times$10$^{-30}$ particles/GeV s sr m$^2$. 	
1709.06632v1	http://arxiv.org/pdf/1709.06632v1	2017	Biomechanical analysis of a cranial Patient Specific Implant on the   interface with the bone using the Finite Element Method	J. Díaz|Octavio Andrés González-Estrada|C. López	  - New advance technologies based on reverse engineering , design and additive manufacturing, have expanded design capabilities for biomedical applications to include Patient Specific Implants (PSI). This change in design paradigms needs advanced tools to assess the mechanical performance of the product, and simulate the impact on the patient. In this work, we perform a structural analysis on the interface of a cranial PSI under static loading conditions. Based on those simulations, we have identified the regions with high stress and strain and checked the failure criteria both in the implant and the skull. We evaluate the quality of the design of the implant and determine their response given different materials, in order to ensure optimality of the final product to be manufactured . 	
1709.07047v1	http://arxiv.org/pdf/1709.07047v1	2017	Patterns and Thresholds of Magnetoelectric Switching in Spin Logic   Devices	Dmitri E. Nikonov|Sasikanth Manipatruni|Ian A. Young	  In the quest to develop spintronic logic, it was discovered that magnetoelectric switching results in lower energy and shorter switching time than other mechanisms. Magnetoelectric (ME) field due to exchange bias at the interface with a multi-ferroic (such as BiFeO3) is well suited for 180 degree switching of magnetization. The ME field is determined by the direction of canted magnetization in BiFeO3 which can point at an angle to the plane, to which voltage is applied. Dependence of switching time and the threshold of ME field on its angles was determined by micromagnetic simulations. Switching occurs by formation of a domain wall on the side of the nanomagnet on top of BFO and its propagation to the rest of the magnet. For in-plane magnetization, switching occurs over a wide range of angles and at all magnitudes of ME field above threshold. For out-of-plane magnetization failure occurs (with an exception of a narrow range of angles and magnitudes of ME field) due to the domain wall reflecting from the opposite end of the nanomagnet. 	
1710.03783v1	http://arxiv.org/pdf/1710.03783v1	2017	Why Black Hole Information Loss is Paradoxical	David Wallace	  I distinguish between two versions of the black hole information-loss paradox. The first arises from apparent failure of unitarity on the spacetime of a completely evaporating black hole, which appears to be non-globally-hyperbolic; this is the most commonly discussed version of the paradox in the foundational and semipopular literature, and the case for calling it `paradoxical' is less than compelling. But the second arises from a clash between a fully-statistical-mechanical interpretation of black hole evaporation and the quantum-field-theoretic description used in derivations of the Hawking effect. This version of the paradox arises long before a black hole completely evaporates, seems to be the version that has played a central role in quantum gravity, and is genuinely paradoxical. After explicating the paradox, I discuss the implications of more recent work on AdS/CFT duality and on the `Firewall paradox', and conclude that the paradox is if anything now sharper. The article is written at a (relatively) introductory level and does not assume advanced knowledge of quantum gravity. 	
1710.04914v1	http://arxiv.org/pdf/1710.04914v1	2017	Atomic force microscopy study of the tetragonal to monoclinic   transformation behaviour of silica doped yttria-stabilized zirconia	Sylvain Deville|Jérôme Chevalier|Laurent Gremillard	  The tetragonal to monoclinic phase transformation of zirconia has been the subject of extensive studies over the last 20 years [1-4]. The main features of the transformation have been identified and its martensitic nature is now widely recognised [5-8]. More specifically, the relevance of a nucleation and growth model to describe the transformation is widely accepted. Recent fracture episodes [9] of zirconia hip joint heads were reported, failures related to the t-m transformation degradation. Among the materials solutions considered for decreasing the sensitivity to t-m phase transformation, the possibility of adding silica as a dopant appears as an appealing one. Previous studies have revealed the beneficial effect of silica addition by the formation of a glassy phase at the grain boundaries and triple points. This glassy phase has been proven to reduce the residual stresses level [10], slowing down the transformation kinetics. Preliminary quantitative investigations by XRD have shown these materials are less susceptible to transformation. However, the mechanism by which the transformation propagated has still to be assessed. 	
1710.05195v1	http://arxiv.org/pdf/1710.05195v1	2017	Thermomechanical properties of zirconium tungstate/hydrogenated nitrile   butadiene rubber (HNBR) composites for low-temperature applications	Anton G. Akulichev|Ben Alcock|Avinash Tiwari|Andreas T. Echtermeyer	  Rubber compounds for pressure sealing application typically have inferior dimensional stability with temperature fluctuations compared with their steel counterparts. This effect may result in seal leakage failures when subjected to decreases in temperature. Composites of hydrogenated nitrile butadiene rubber (HNBR) and zirconium tungstate as a negative thermal expansion filler were prepared in order to control the thermal expansivity of the material. The amount of zirconium tungstate (ZrW2O8) was varied in the range of 0 to about 40 vol%. The coefficient of thermal expansion (CTE), bulk modulus, uniaxial extension and compression set properties were measured. The CTE of the ZrW2O8-filled HNBR decreases with the filler content and it is reduced by a factor of 2 at the highest filler concentration used. The filler effect on CTE is found to be stronger when HNBR is below the glass transition temperature. The experimental thermal expansion data of the composites are compared with the theoretical estimates and predictions given by FEA. The effect of ZrW2O8 on the mechanical characteristics and compression set of these materials is also discussed. 	
1710.06055v1	http://arxiv.org/pdf/1710.06055v1	2017	Evolution in Virtual Worlds	Tim Taylor	  This chapter discusses the possibility of instilling a virtual world with mechanisms for evolution and natural selection in order to generate rich ecosystems of complex organisms in a process akin to biological evolution. Some previous work in the area is described, and successes and failures are discussed. The components of a more comprehensive framework for designing such worlds are mapped out, including the design of the individual organisms, the properties and dynamics of the environmental medium in which they are evolving, and the representational relationship between organism and environment. Some of the key issues discussed include how to allow organisms to evolve new structures and functions with few restrictions, and how to create an interconnectedness between organisms in order to generate drives for continuing evolutionary activity. 	
1711.02732v1	http://arxiv.org/pdf/1711.02732v1	2017	Cohomological rigidity and the Anosov-Katok construction	Nikolaos Karaliolios	  We provide a general argument for the failure of Anosov-Katok-like constructions to produce Cohomologically Rigid diffeomorphisms in manifolds other than tori. A $C^{\infty }$ smooth diffeomorphism $f $ of a compact manifold $M$ is Cohomologically Rigid iff the equation, known as Linear Cohomological one, \begin{equation*} \psi \circ f - \psi = \varphi \end{equation*} admits a $C^{\infty }$ smooth solution $\psi$ for every $\varphi$ in a codimension $1$ closed subspace of $C^{\infty } (M, \mathbb{C} )$. As an application, we show that no Cohomologically Rigid diffeomorphisms exist in the Almost Reducibility regime for quasi-periodic cocycles in homogeneous spaces of compact type, even though the Linear Cohomological equation over a generic such system admits a solution for a dense subset of functions $\varphi$. We thus confirm a conjecture by M. Herman and A. Katok in that context and provide some insight in the mechanism obstructing the construction of counterexamples. 	
1711.02926v1	http://arxiv.org/pdf/1711.02926v1	2017	Zero-Crossing Statistics for Non-Markovian Time Series	Markus Nyberg|Ludvig Lizana|Tobias Ambjörnsson	  In applications spaning from image analysis and speech recognition, to energy dissipation in turbulence and time-to failure of fatigued materials, researchers and engineers want to calculate how often a stochastic observable crosses a specific level, such as zero. At first glance this problem looks simple, but it is in fact theoretically very challenging. And therefore, few exact results exist. One exception is the celebrated Rice formula that gives the mean number of zero-crossings in a fixed time interval of a zero-mean Gaussian stationary processes. In this study we use the so-called Independent Interval Approximation to go beyond Rice's result and derive analytic expressions for all higher-order zero-crossing cumulants and moments. Our results agrees well with simulations for the non-Markovian autoregressive model. 	
1711.06007v1	http://arxiv.org/pdf/1711.06007v1	2017	Increasing power-law range in avalanche amplitude and energy   distributions	Víctor Navas-Portella|Isabel Serra|Álvaro Corral|Eduard Vives	  Power-law type probability density functions spanning several orders of magnitude are found for different avalanche properties. We propose a methodology to overcome empirical constrains that limit the power-law range for the distributions of different avalanche observables like amplitude, energy, duration or size. By considering catalogs of events that cover different observation windows, maximum likelihood estimation of a global power-law exponent is computed. This methodology is applied to amplitude and energy distributions of acoustic emission avalanches in failure-under- compression experiments of a nanoporous silica glass, finding in some cases global exponents in an unprecedented broad range: 4.5 decades for amplitudes and 9.5 decades for energies. In the later case, however, strict statistical analysis suggests experimental limitations might alter the power-law behavior. 	
1711.08965v1	http://arxiv.org/pdf/1711.08965v1	2017	Improving Reliability of Service Function Chains with Combined VNF   Migrations and Replications	Francisco Carpio|Admela Jukan	  The Network Function Virtualization (NFV) paradigm is enabling flexibility, programmability and implementation of traditional network functions into generic hardware, in form of Virtual Network Functions (VNFs). To provide services, the VNFs are commonly concatenated in a certain ordered sequence, known as Service Function Chains (SFCs). SFCs are usually required to meeting a certain level of reliability. This creates the need to place the VNFs while optimizing reliability jointly with other objectives, such as network and server load balancing. Traditional migration and replication mechanisms, commonly used for Virtual Machines (VM) in data centers, can be used to improve SFC reliability. We study how to improve service reliability using jointly replications and migrations, considering the chaining problem inherent in NFV. While replications provide reliability, performing migrations to more reliable servers decreases the resource overhead. A Linear Programming (LP) model is presented to study the impact of active-active configurations on the network and server resources. Additionally, to provide a fast recovery from server failures, we consider N-to-N configurations in NFV networks and study its impact on server resources. The results show that replications do not only improve reliability, but can also be used to achieving a better server and network load balancing, and when used jointly with migrations can improve resource utilization without degrading reliability. 	
1711.09594v1	http://arxiv.org/pdf/1711.09594v1	2017	FCLT - A Fully-Correlational Long-Term Tracker	Alan Lukežič|Luka Čehovin Zajc|Tomáš Vojíř|Jiří Matas|Matej Kristan	  We propose FCLT - a fully-correlational long-term tracker. The two main components of FCLT are a short-term tracker which localizes the target in each frame and a detector which re-detects the target when it is lost. Both the short-term tracker and the detector are based on correlation filters. The detector exploits properties of the recent constrained filter learning and is able to re-detect the target in the whole image efficiently. A failure detection mechanism based on correlation response quality is proposed. The FCLT is tested on recent short-term and long-term benchmarks. It achieves state-of-the-art results on the short-term benchmarks and it outperforms the current best-performing tracker on the long-term benchmark by over 18%. 	
1711.10165v1	http://arxiv.org/pdf/1711.10165v1	2017	Enhanced Communication With the Assistance of Indefinite Causal Order	Daniel Ebler|Sina Salek|Giulio Chiribella	  In quantum Shannon theory, the way information is encoded and decoded takes advantage of the laws of quantum mechanics, while the way communication channels are interlinked is assumed to be classical. In this Letter we relax the assumption that quantum channels are combined classically, showing that a quantum communication network where channels are combined in an indefinite causal order can achieve tasks that are impossible in conventional quantum Shannon theory. In particular, we show that two identical copies of a completely depolarizing channel become able to transmit information when they are combined in a quantum superposition of two alternative orders. This finding runs counter to the intuition that if two communication channels are identical, using them in different orders should not make any difference. The failure of such intuition stems from the fact that a single noisy channel can be a random mixture of many elementary, non-commuting processes, whose order (or lack thereof) can affect the ability to transmit information. 	
1712.01291v1	http://arxiv.org/pdf/1712.01291v1	2017	Machine learning for predictive estimation of qubit dynamics subject to   dephasing	Riddhi Swaroop Gupta|Michael J. Biercuk	  Decoherence remains a major challenge in quantum computing hardware and a variety of physical-layer controls provide opportunities to mitigate the impact of this phenomenon through feedback and feedforward control. In this work, we compare a variety of machine learning algorithms derived from diverse fields for the task of state estimation (retrodiction) and forward prediction of future qubit state evolution for a single qubit subject to classical, non-Markovian dephasing. Our approaches involve the construction of a dynamical model capturing qubit dynamics via autoregressive or Fourier-type protocols using only a historical record of projective measurements. A detailed comparison of achievable prediction horizons, model robustness, and measurement-noise-filtering capabilities for Kalman Filters (KF) and Gaussian Process Regression (GPR) algorithms is provided. We demonstrate superior performance from the autoregressive KF relative to Fourier-based KF approaches and focus on the role of filter optimization in achieving suitable performance. Finally, we examine several realizations of GPR using different kernels and discover that these approaches are generally not suitable for forward prediction. We highlight the underlying failure mechanism in this application and identify ways in which the output of the algorithm may be misidentified numerical artefacts. 	
1712.03690v2	http://arxiv.org/pdf/1712.03690v2	2017	Cascading collapse of online social networks	János Török|János Kertész	  Online social networks have increasing influence on our society, they may play decisive roles in politics and can be crucial for the fate of companies. Such services compete with each other and some may even break down rapidly. Using social network datasets we show the main factors leading to such a dramatic collapse. At early stage mostly the loosely bound users disappear, later collective effects play the main role leading to cascading failures. We present a theory based on a generalised threshold model to explain the findings and show how the collapse time can be estimated in advance using the dynamics of the churning users. Our results shed light to possible mechanisms of instabilities in other competing social processes. 	
1712.03785v1	http://arxiv.org/pdf/1712.03785v1	2017	A primer on noise-induced transitions in applied dynamical systems	Eric Forgoston|Richard O. Moore	  Noise plays a fundamental role in a wide variety of physical and biological dynamical systems. It can arise from an external forcing or due to random dynamics internal to the system. It is well established that even weak noise can result in large behavioral changes such as transitions between or escapes from quasi-stable states. These transitions can correspond to critical events such as failures or extinctions that make them essential phenomena to understand and quantify, despite the fact that their occurrence is rare. This article will provide an overview of the theory underlying the dynamics of rare events for stochastic models along with some example applications. 	
1712.10230v1	http://arxiv.org/pdf/1712.10230v1	2017	On quality of implementation of Fortran 2008 complex intrinsic functions   on branch cuts	Anton Shterenlikht	  Branch cuts in complex functions in combination with signed zero and signed infinity have important uses in fracture mechanics, jet flow and aerofoil analysis. We present benchmarks for validating Fortran 2008 complex functions - LOG, SQRT, ASIN, ACOS, ATAN, ASINH, ACOSH and ATANH - on branch cuts with arguments of all 3 IEEE floating point binary formats: binary32, binary64 and binary128. Results are reported with 8 Fortran 2008 compilers: GCC, Flang, Cray, Oracle, PGI, Intel, NAG and IBM. Multiple test failures were revealed, e.g. wrong signs of results or unexpected overflow, underflow, or NaN. We conclude that the quality of implementation of these Fortran 2008 intrinsics in many compilers is not yet sufficient to remove the need for special code for branch cuts. The test results are complemented by conformal maps of the branch cuts and detailed derivations of the values of these functions on branch cuts, to be used as a reference. The benchmarks are freely available from cmplx.sf.net. This work will be of interest to engineers who use complex functions, as well as to compiler and maths library developers. 	
1801.00376v1	http://arxiv.org/pdf/1801.00376v1	2018	Radiative thermal runaway due to negative differential thermal emission   across a solid-solid phase transition	David M. Bierman|Andrej Lenert|Mikhail A. Kats|You Zhou|Shuyan Zhang|Matthew De La Ossa|Shriram Ramanathan|Federico Capasso|Evelyn N. Wang	  Thermal runaway occurs when a rise in system temperature results in heat generation rates exceeding dissipation rates. Here we demonstrate that thermal runaway occurs in thermal radiative systems, given a sufficient level of negative differential thermal emission. By exploiting the insulator-to-metal phase transition of vanadium dioxide, we show that a small increase in heat generation (e.g., 10 nW/mm2) can result in a large change in surface temperature (e.g., ~35 K), as the thermal emitter switches from high emissivity to low emissivity. While thermal runaway is typically associated with catastrophic failure mechanisms, detailed understanding and control of this phenomenon may give rise to new opportunities in infrared sensing, camouflage, and rectification. 	
1801.01580v1	http://arxiv.org/pdf/1801.01580v1	2018	Binder migration during drying of lithium-ion battery electrodes:   modelling and comparison to experiment	Francesc Font|Bartosz Protas|Giles Richardson|Jamie M. Foster	  The drying process is a crucial step in electrode manufacture as it can affect the component distribution within the electrode. Phenomena such as binder migration can have negative effects in the form of poor cell performance (e.g. capacity fade) or mechanical failure (e.g. electrode delamination from the current collector). We present a mathematical model that tracks the evolution of the binder concentration in the electrode during drying. Solutions to the model predict that low drying rates lead to a favourable homogeneous binder profile across the electrode film, whereas high drying rates result in an unfavourable accumulation of binder near the evaporation surface. These results show strong qualitative agreement with experimental observations and provide a cogent explanation for why fast drying conditions result in poorly performing electrodes. Finally, we provide some guidelines on how the drying process could be optimised to offer relatively short drying times whilst simultaneously maintaining a roughly homogeneous binder distribution. 	
1801.06039v1	http://arxiv.org/pdf/1801.06039v1	2017	Reason and Method in Einstein's Relativity	Hisham Ghassib	  Relativity was Einstein's main research program and scientific project. It was an open-ended program that developed throughout Einstein's scientific career, giving rise to special relativity, general relativity and unified field theory. In this paper, we want to uncover the methodological logic of the Einsteinian program, which animated the whole program and its development, and as it was revealed in SR, GR, and unified field theory. We aver that the same methodological logic animated all these theories as Einstein's work progressed. Each of these theories contributed towards constructing Einstein's ambitious program. This paper is not a paper in the history of Relativity, but, rather, it utilizes our knowledge of this history to uncover the methodological logic of the relativity program and its development. This logic is latent in the historical narrative, but is not identical to it. We hope to show that the Einsteinian relativity project is still relevant today as a theoretical scheme, despite its failures and despite quantum mechanics. 	
1801.09250v1	http://arxiv.org/pdf/1801.09250v1	2018	Virtual Breakpoints for x86/64	Gregory Michael Price	  Efficient, reliable trapping of execution in a program at the desired location is a hot area of research for security professionals. The progression of debuggers and malware is akin to a game of cat and mouse - each are constantly in a state of trying to thwart one another. At the core of most efficient debuggers today is a combination of virtual machines and traditional binary modification breakpoints (int3). In this paper, we present a design for Virtual Breakpoints, a modification to the x86 MMU which brings breakpoint management into hardware alongside page tables. We demonstrate the fundamental abstraction failures of current trapping methods, and rebuild the mechanism from the ground up. Our design delivers fast, reliable trapping without the pitfalls of binary modification. 	
1802.04040v1	http://arxiv.org/pdf/1802.04040v1	2018	Yield precursor dislocation avalanches in small crystals: the   irreversibility transition	Xiaoyue Ni|Haolu Zhang|Danilo B. Liarte|Louis W. McFaul|Karin A. Dahmen|James P. Sethna|Julia R. Greer	  The transition from elastic to plastic deformation in crystalline metals shares history dependence and scale-invariant avalanche signature with other non-equilibrium systems under external loading: dilute colloidal suspensions, plastically-deformed amorphous solids, granular materials, and dislocation-based simulations of crystals. These other systems exhibit transitions with clear analogies to work hardening and yield stress, with many typically undergoing purely elastic behavior only after 'training' through repeated cyclic loading; studies in these other systems show a power law scaling of the hysteresis loop extent and of the training time as the peak load approaches a so-called reversible-irreversible transition (RIT). We discover here that deformation of small crystals shares these key characteristics: yielding and hysteresis in uniaxial compression experiments of single-crystalline Cu nano- and micro-pillars decay under repeated cyclic loading. The amplitude and decay time of the yield precursor avalanches diverge as the peak stress approaches failure stress for each pillar, with a power law scaling virtually equivalent to RITs in other nonequilibrium systems. 	
0404593v4	http://arxiv.org/pdf/cond-mat/0404593v4	2004	The shortest path to complex networks	S. N. Dorogovtsev|J. F. F. Mendes	  1. The birth of network science. 2. What are random networks? 3. Adjacency matrix. 4. Degree distribution. 5. What are simple networks? Classical random graphs. 6. Birth of the giant component. 7. Topology of the Web. 8.Uncorrelated networks. 9. What are small worlds? 10. Real networks are mesoscopic objects. 11. What are complex networks? 12. The configuration model. 13. The absence of degree--degree correlations. 14.Networks with correlated degrees.15.Clustering. 16. What are small-world networks? 17. `Small worlds' is not the same as `small-world networks'. 18. Fat-tailed degree distributions. 19.Reasons for the fat-tailed degree distributions. 20. Preferential linking. 21. Condensation of edges. 22. Cut-offs of degree distributions. 23. Reasons for correlations in networks. 24. Classical random graphs cannot be used for comparison with real networks. 25. How to measure degree--degree correlations. 26. Assortative and disassortative mixing. 27. Disassortative mixing does not mean that vertices of high degrees rarely connect to each other. 28. Reciprocal links in directed nets. 29. Ultra-small-world effect. 30. Tree ansatz. 31.Ultraresilience against random failures. 32. When correlated nets are ultraresilient. 33. Vulnerability of complex networks. 34. The absence of an epidemic threshold. 35. Search based on local information. 36.Ultraresilience disappears in finite nets. 37.Critical behavior of cooperative models on networks. 38. Berezinskii-Kosterlitz-Thouless phase transitions in networks. 39.Cascading failures. 40.Cliques & communities. 41. Betweenness. 42.Extracting communities. 43. Optimal paths. 44.Distributions of the shortest-path length & of the loop's length are narrow. 45. Diffusion on networks. 46. What is modularity? 47.Hierarchical organization of networks. 48. Convincing modelling of real-world networks:Is it possible? 49. The small Web.. 	
0509487v1	http://arxiv.org/pdf/cond-mat/0509487v1	2005	Fluctuations of power injection in randomly driven granular gases	P. Visco|A. Puglisi|A. Barrat|E. Trizac|F. van Wijland	  We investigate the large deviation function pi(w) for the fluctuations of the power W(t)=w t, integrated over a time t, injected by a homogeneous random driving into a granular gas, in the infinite time limit. Starting from a generalized Liouville equation we obtain an equation for the generating function of the cumulants mu(lambda) which appears as a generalization of the inelastic Boltzmann equation and has a clear physical interpretation. Reasonable assumptions are used to obtain mu(lambda) in a closed analytical form. A Legendre transform is sufficient to get the large deviation function pi(w). Our main result, apart from an estimate of all the cumulants of W(t) at large times t, is that pi(w) has no negative branch. This immediately results in the failure of the Gallavotti-Cohen Fluctuation Relation (GCFR), that in previous studies had been suggested to be valid for injected power in driven granular gases. We also present numerical results, in order to discuss the finite time behavior of the fluctuations of W(t). We discover that their probability density function converges extremely slowly to its asymptotic scaling form: the third cumulant saturates after a characteristic time larger than 50 mean free times and the higher order cumulants evolve even slower. The asymptotic value is in good agreement with our theory. Remarkably, a numerical check of the GCFR is feasible only at small times, since negative events disappear at larger times. At such small times this check leads to the misleading conclusion that GCFR is satisfied for pi(w). We offer an explanation for this remarkable apparent verification. In the inelastic Maxwell model, where a better statistics can be achieved, we are able to numerically observe the failure of GCFR. 	
0505033v2	http://arxiv.org/pdf/physics/0505033v2	2005	A physical model for aftershocks triggered by dislocation on a   rectangular fault	R. Console|F. Catalli	  We find the static displacement, stress, strain and the modified Columb failure stress produced in an elastic medium by a finite size rectangular fault after its dislocation with uniform stress drop but a non uniform dislocation on the source. The time-dependent rate of triggered earthquakes is estimated by a rate-state model applied to a uniformly distributed population of faults whose equilibrium is perturbated by a stress change caused only by the first dislocation. The rate of triggered events in our simulations is exponentially proportional to the stress change, but the time at which the maximum rate begins to decrease is variable from fractions of hour for positive stress changes of the order of some MPa, up to more than a year for smaller stress changes. As a consequence, the final number of triggered events is proportional to the stress change. The model predicts that the total number of events triggered on a plane containing the fault is proportional to the 2/3 power of the seismic moment. Indeed, the total number of aftershocks produced on the fault plane scales in magnitude as 10^{M}. Including the negative contribution of the stress drop inside the source, we observe that the number of events inhibited on the fault is, at long term, nearly identical to the number of those induced outside, representing a sort of conservative natural rule. Considering its behaviour in time, our model doesn't completely match the popular Omori law; in fact it has been shown that the seismicity induced closely to the fault edges is intense but of short duration, while that expected at large distances (up to some tens times the fault dimensions) exhibits a much slower decay. 	
0508147v3	http://arxiv.org/pdf/quant-ph/0508147v3	2010	Fault Models for Quantum Mechanical Switching Networks	Jacob Biamonte|Jeff S. Allen|Marek A. Perkowski	  The difference between faults and errors is that, unlike faults, errors can be corrected using control codes. In classical test and verification one develops a test set separating a correct circuit from a circuit containing any considered fault. Classical faults are modelled at the logical level by fault models that act on classical states. The stuck fault model, thought of as a lead connected to a power rail or to a ground, is most typically considered. A classical test set complete for the stuck fault model propagates both binary basis states, 0 and 1, through all nodes in a network and is known to detect many physical faults. A classical test set complete for the stuck fault model allows all circuit nodes to be completely tested and verifies the function of many gates. It is natural to ask if one may adapt any of the known classical methods to test quantum circuits. Of course, classical fault models do not capture all the logical failures found in quantum circuits. The first obstacle faced when using methods from classical test is developing a set of realistic quantum-logical fault models. Developing fault models to abstract the test problem away from the device level motivated our study. Several results are established. First, we describe typical modes of failure present in the physical design of quantum circuits. From this we develop fault models for quantum binary circuits that enable testing at the logical level. The application of these fault models is shown by adapting the classical test set generation technique known as constructing a fault table to generate quantum test sets. A test set developed using this method is shown to detect each of the considered faults. 	
1009.3183v1	http://arxiv.org/pdf/1009.3183v1	2010	Interdependent networks with correlated degrees of mutually dependent   nodes	Sergey V. Buldyrev|Nathaniel Shere|Gabriel A. Cwilich	  We study a problem of failure of two interdependent networks in the case of correlated degrees of mutually dependent nodes. We assume that both networks (A and B) have the same number of nodes $N$ connected by the bidirectional dependency links establishing a one-to-one correspondence between the nodes of the two networks in a such a way that the mutually dependent nodes have the same number of connectivity links, i.e. their degrees coincide. This implies that both networks have the same degree distribution $P(k)$. We call such networks correspondently coupled networks (CCN). We assume that the nodes in each network are randomly connected. We define the mutually connected clusters and the mutual giant component as in earlier works on randomly coupled interdependent networks and assume that only the nodes which belong to the mutual giant component remain functional. We assume that initially a $1-p$ fraction of nodes are randomly removed due to an attack or failure and find analytically, for an arbitrary $P(k)$, the fraction of nodes $\mu(p)$ which belong to the mutual giant component. We find that the system undergoes a percolation transition at certain fraction $p=p_c$ which is always smaller than the $p_c$ for randomly coupled networks with the same $P(k)$. We also find that the system undergoes a first order transition at $p_c>0$ if $P(k)$ has a finite second moment. For the case of scale free networks with $2<\lambda \leq 3$, the transition becomes a second order transition. Moreover, if $\lambda<3$ we find $p_c=0$ as in percolation of a single network. For $\lambda=3$ we find an exact analytical expression for $p_c>0$. Finally, we find that the robustness of CCN increases with the broadness of their degree distribution. 	
1209.0959v3	http://arxiv.org/pdf/1209.0959v3	2013	How big is too big? Critical Shocks for Systemic Failure Cascades	Claudio J. Tessone|Antonios Garas|Beniamino Guerra|Frank Schweitzer	  External or internal shocks may lead to the collapse of a system consisting of many agents. If the shock hits only one agent initially and causes it to fail, this can induce a cascade of failures among neighoring agents. Several critical constellations determine whether this cascade remains finite or reaches the size of the system, i.e. leads to systemic risk. We investigate the critical parameters for such cascades in a simple model, where agents are characterized by an individual threshold \theta_i determining their capacity to handle a load \alpha\theta_i with 1-\alpha being their safety margin. If agents fail, they redistribute their load equally to K neighboring agents in a regular network. For three different threshold distributions P(\theta), we derive analytical results for the size of the cascade, X(t), which is regarded as a measure of systemic risk, and the time when it stops. We focus on two different regimes, (i) EEE, an external extreme event where the size of the shock is of the order of the total capacity of the network, and (ii) RIE, a random internal event where the size of the shock is of the order of the capacity of an agent. We find that even for large extreme events that exceed the capacity of the network finite cascades are still possible, if a power-law threshold distribution is assumed. On the other hand, even small random fluctuations may lead to full cascades if critical conditions are met. Most importantly, we demonstrate that the size of the "big" shock is not the problem, as the systemic risk only varies slightly for changes of 10 to 50 percent of the external shock. Systemic risk depends much more on ingredients such as the network topology, the safety margin and the threshold distribution, which gives hints on how to reduce systemic risk. 	
1702.08695v2	http://arxiv.org/pdf/1702.08695v2	2017	Online Robot Introspection via Wrench-based Action Grammars	Juan Rojas|Shuangqi Luo|Dingqiao Zhu|Yunlong Du|Hongbin Lin|Zhengjie Huang|Wenwei Kuang|Kensuke Harada	  Robotic failure is all too common in unstructured robot tasks. Despite well-designed controllers, robots often fail due to unexpected events. How do robots measure unexpected events? Many do not. Most robots are driven by the sense-plan act paradigm, however more recently robots are undergoing a sense-plan-act-verify paradigm. In this work, we present a principled methodology to bootstrap online robot introspection for contact tasks. In effect, we are trying to enable the robot to answer the question: what did I do? Is my behavior as expected or not? To this end, we analyze noisy wrench data and postulate that the latter inherently contains patterns that can be effectively represented by a vocabulary. The vocabulary is generated by segmenting and encoding the data. When the wrench information represents a sequence of sub-tasks, we can think of the vocabulary forming a sentence (set of words with grammar rules) for a given sub-task; allowing the latter to be uniquely represented. The grammar, which can also include unexpected events, was classified in offline and online scenarios as well as for simulated and real robot experiments. Multiclass Support Vector Machines (SVMs) were used offline, while online probabilistic SVMs were are used to give temporal confidence to the introspection result. The contribution of our work is the presentation of a generalizable online semantic scheme that enables a robot to understand its high-level state whether nominal or abnormal. It is shown to work in offline and online scenarios for a particularly challenging contact task: snap assemblies. We perform the snap assembly in one-arm simulated and real one-arm experiments and a simulated two-arm experiment. This verification mechanism can be used by high-level planners or reasoning systems to enable intelligent failure recovery or determine the next most optima manipulation skill to be used. 	
1705.09701v1	http://arxiv.org/pdf/1705.09701v1	2017	SMORE: A Cold Data Object Store for SMR Drives (Extended Version)	Peter Macko|Xiongzi Ge|John Haskins Jr.|James Kelley|David Slik|Keith A. Smith|Maxim G. Smith	  Shingled magnetic recording (SMR) increases the capacity of magnetic hard drives, but it requires that each zone of a disk be written sequentially and erased in bulk. This makes SMR a good fit for workloads dominated by large data objects with limited churn. To explore this possibility, we have developed SMORE, an object storage system designed to reliably and efficiently store large, seldom-changing data objects on an array of host-managed or host-aware SMR disks.   SMORE uses a log-structured approach to accommodate the constraint that all writes to an SMR drive must be sequential within large shingled zones. It stripes data across zones on separate disks, using erasure coding to protect against drive failure. A separate garbage collection thread reclaims space by migrating live data out of the emptiest zones so that they can be trimmed and reused. An index stored on flash and backed up to the SMR drives maps object identifiers to on-disk locations. SMORE interleaves log records with object data within SMR zones to enable index recovery after a system crash (or failure of the flash device) without any additional logging mechanism.   SMORE achieves full disk bandwidth when ingesting data---with a variety of object sizes---and when reading large objects. Read performance declines for smaller object sizes where inter- object seek time dominates. With a worst-case pattern of random deletions, SMORE has a write amplification (not counting RAID parity) of less than 2.0 at 80% occupancy. By taking an index snapshot every two hours, SMORE recovers from crashes in less than a minute. More frequent snapshots allow faster recovery. 	
1707.05063v1	http://arxiv.org/pdf/1707.05063v1	2017	Optimal Storage under Unsynchrononized Mobile Byzantine Faults	Silvia Bonomi|Antonella Del Pozzo|Maria Potop-Butucaru|Sébastien Tixeuil	  In this paper we prove lower and matching upper bounds for the number of servers required to implement a regular shared register that tolerates unsynchronized Mobile Byzantine failures. We consider the strongest model of Mobile Byzantine failures to date: agents are moved arbitrarily by an omniscient adversary from a server to another in order to deviate their computation in an unforeseen manner. When a server is infected by an Byzantine agent, it behaves arbitrarily until the adversary decides to move the agent to another server. Previous approaches considered asynchronous servers with synchronous mobile Byzantine agents (yielding impossibility results), and synchronous servers with synchronous mobile Byzantine agents (yielding optimal solutions for regular register implementation, even in the case where servers and agents periods are decoupled). We consider the remaining open case of synchronous servers with unsynchronized agents, that can move at their own pace, and change their pace during the execution of the protocol. Most of our findings relate to lower bounds, and characterizing the model parameters that make the problem solvable. It turns out that unsynchronized mobile Byzantine agent movements requires completely new proof arguments, that can be of independent interest when studying other problems in this model. Additionally, we propose a generic server-based algorithm that emulates a regular register in this model, that is tight with respect to the number of mobile Byzantine agents that can be tolerated. Our emulation spans two awareness models: servers with and without self-diagnose mechanisms. In the first case servers are aware that the mobile Byzantine agent has left and hence they can stop running the protocol until they recover a correct state while in the second case, servers are not aware of their faulty state and continue to run the protocol using an incorrect local state. 	
1708.07422v1	http://arxiv.org/pdf/1708.07422v1	2017	Resilience Design Patterns: A Structured Approach to Resilience at   Extreme Scale	Saurabh Hukerikar|Christian Engelmann	  Reliability is a serious concern for future extreme-scale high-performance computing (HPC) systems. While the HPC community has developed various resilience solutions, the solution space remains fragmented. There are no formal methods and metrics to integrate the various HPC resilience techniques into composite solutions, nor are there methods to holistically evaluate the adequacy and efficacy of such solutions in terms of their protection coverage, and their performance & power efficiency characteristics. In this paper, we develop a structured approach to the design, evaluation and optimization of HPC resilience using the concept of design patterns. A design pattern is a general repeatable solution to a commonly occurring problem. We identify the problems caused by various types of faults, errors and failures in HPC systems and the techniques used to deal with these events. Each well-known solution that addresses a specific HPC resilience challenge is described in the form of a pattern. We develop a complete catalog of such resilience design patterns, which may be used as essential building blocks when designing and deploying resilience solutions. We also develop a design framework that enhances a designer's understanding the opportunities for integrating multiple patterns across layers of the system stack and the important constraints during implementation of the individual patterns. It is also useful for defining mechanisms and interfaces to coordinate flexible fault management across hardware and software components. The overall goal of this work is to establish a systematic methodology for the design and evaluation of resilience technologies in extreme-scale HPC systems that keep scientific applications running to a correct solution in a timely and cost-efficient manner despite frequent faults, errors, and failures of various types. 	
0506717v2	http://arxiv.org/pdf/cond-mat/0506717v2	2005	A simple microscopic model for the dynamics of adhesive failure	Dominic Vella|L. Mahadevan	  We consider a microscopic model for the failure of soft adhesives in tension based on ideas of bond rupture under dynamic loading. Focusing on adhesive failure under loading at constant velocity, we demonstrate that bi-modal curves of stress against strain may occur due to effects of finite polymer chain or bond length and characterise the loading conditions under which such bi-modal behaviour is observed. The results of this analysis are in qualitative agreement with experiments performed on unconfined adhesives in which failure does not occur by cavitation. 	
0603618v1	http://arxiv.org/pdf/cond-mat/0603618v1	2006	Precursors and prediction of catastrophic avalanches	Srutarshi Pradhan|Bikas K. Chakrabarti	  In this work we review the precursors of catastrophic avalanches (global failures) in several failure models, namely (a) Fiber Bundle Model (FBM), (b) Random Fuse Model (RFM), (c) Sandpile Models and (d) Fractal Overlap Model. The precursor parameters identified here essentially reflect the growing correlations within such systems as they approach their respective failure points. As we show, often they help us to predict the global failure points in advance. 	
0702120v1	http://arxiv.org/pdf/physics/0702120v1	2007	Failure in Complex Social Networks	Damon Centola	  Tolerance against failures and errors is an important feature of many complex networked systems [1,2]. It has been shown that a class of inhomogeneously wired networks called scale-free[1,3] networks can be surprisingly robust to failures, suggesting that socially self-organized systems such as the World-Wide Web, the Internet, and other kinds of social networks [4] may have significant tolerance against failures by virtue of their scale-free degree distribution. I show that this finding only holds on the assumption that the diffusion process supported by the network is a simple one, requiring only a single contact in order for transmission to be successful. 	
1205.6561v1	http://arxiv.org/pdf/1205.6561v1	2012	Cascade Failure in a Phase Model of Power Grids	Hidetsugu Sakaguchi|Tatsuma Matsuo	  We propose a phase model to study cascade failure in power grids composed of generators and loads. If the power demand is below a critical value, the model system of power grids maintains the standard frequency by feedback control. On the other hand, if the power demand exceeds the critical value, an electric failure occurs via step out (loss of synchronization) or voltage collapse. The two failures are incorporated as two removal rules of generator nodes and load nodes. We perform direct numerical simulation of the phase model on a scale-free network and compare the results with a mean-field approximation. 	
1211.0592v1	http://arxiv.org/pdf/1211.0592v1	2012	Requirements of a Recovery Solution for Failure of Composite Web   Services	Hadi Saboohi|Sameem Abdul Kareem	  Web services are building blocks of interoperable systems. Composing Web services makes the processes capable of doing complex tasks. Composite services may fail during their execution which can be diagnosed by a mediator. The mediator adapts the structure so that the failure is recovered. Moreover, future executions should avoid the situation or organize a strategy to repair the structure with a minimum delay. In this paper the failure reasons of a composite service are reviewed. Furthermore, the requirements of a solution for recovery of a system from a failure are investigated. 	
1301.4287v1	http://arxiv.org/pdf/1301.4287v1	2013	Maximizing Reliability in WDM Networks through Lightpath Routing	Hyang-Won Lee|Kayi Lee|Eytan Modiano	  We study the reliability maximization problem in WDM networks with random link failures. Reliability in these networks is defined as the probability that the logical network is connected, and it is determined by the underlying lightpath routing, network topologies and the link failure probability. By introducing the notion of lexicographical ordering for lightpath routings, we characterize precise optimization criteria for maximum reliability in the low failure probability regime. Based on the optimization criteria, we develop lightpath routing algorithms that maximize the reliability, and logical topology augmentation algorithms for further improving reliability. We also study the reliability maximization problem in the high failure probability regime. 	
1302.1256v2	http://arxiv.org/pdf/1302.1256v2	2013	Repairing Multiple Failures in the Suh-Ramchandran Regenerating Codes	Junyu Chen|Kenneth W. Shum	  Using the idea of interference alignment, Suh and Ramchandran constructed a class of minimum-storage regenerating codes which can repair one systematic or one parity-check node with optimal repair bandwidth. With the same code structure, we show that in addition to single node failure, double node failures can be repaired collaboratively with optimal repair bandwidth as well. We give an example of how to repair double failures in the Suh-Ramchandran regenerating code with six nodes, and give the proof for the general case. 	
1302.4779v1	http://arxiv.org/pdf/1302.4779v1	2013	Failure Data Analysis of HPC Systems	Charng-Da Lu	  Continuous availability of HPC systems built from commodity components have become a primary concern as system size grows to thousands of processors. In this paper, we present the analysis of 8-24 months of real failure data collected from three HPC systems at the National Center for Supercomputing Applications (NCSA) during 2001-2004. The results show that the availability is 98.7-99.8% and most outages are due to software halts. On the other hand, the downtime are mostly contributed by hardware halts or scheduled maintenance. We also used failure clustering analysis to identify several correlated failures. 	
1309.7498v1	http://arxiv.org/pdf/1309.7498v1	2013	Most probable failure scenario in a model power grid with random power   demand	Misha Stepanov|Aditya Sundarrajan	  We consider a simple system with a local synchronous generator and a load whose power consumption is a random process. The most probable scenario of system failure (synchronization loss) is considered, and it is argued that its knowledge is virtually enough to estimate the probability of failure per unit time. We discuss two numerical methods to obtain the "optimal" evolution leading to failure. 	
1404.4225v1	http://arxiv.org/pdf/1404.4225v1	2014	Efficient rare event simulation for failure problems in random media	Jingchen Liu|Jianfeng Lu|Xiang Zhou	  In this paper we study rare events associated to solutions of elliptic partial differential equations with spatially varying random coefficients. The random coefficients follow the lognormal distribution, which is determined by a Gaussian process. This model is employed to study the failure problem of elastic materials in random media in which the failure is characterized by that the strain field exceeds a high threshold. We propose an efficient importance sampling scheme to compute small failure probabilities in the high threshold limit. The change of measure in our scheme is parametrized by two density functions. The efficiency of the importance sampling scheme is validated by numerical examples. 	
1502.01509v1	http://arxiv.org/pdf/1502.01509v1	2015	OS-level Failure Injection with SystemTap	Camille Coti|Nicolas Greneche	  Failure injection in distributed systems has been an important issue to experiment with robust, resilient distributed systems. In order to reproduce real-life conditions, parts of the application must be killed without letting the operating system close the existing network communications in a "clean" way. When a process is simply killed, the OS closes them. SystemTap is a an infrastructure that probes the Linux kernel's internal calls. If processes are killed at kernel-level, they can be destroyed without letting the OS do anything else. In this paper, we present a kernel-level failure injection system based on SystemTap. We present how it can be used to implement deterministic and probabilistic failure scenarios. 	
1503.00912v1	http://arxiv.org/pdf/1503.00912v1	2015	Exploring Beta-Like Distributions	H. R. N. van Erp|R. O. Linger|P. H. A. J. M. van Gelder	  The most well known probability distribution of probabilities is the Beta distribution. If we have observed $r$ `successes', each having a probability $\theta$, and $n-r$ `failures', each having a probability $1-\theta$. In this paper we will derive a whole family of Beta-like distributions, which take as their data not only the number of successes and failures, but also values on predictor variables and time to failure or time without failure. 	
1604.07474v1	http://arxiv.org/pdf/1604.07474v1	2016	Advancing Dynamic Fault Tree Analysis	Matthias Volk|Sebastian Junges|Joost-Pieter Katoen	  This paper presents a new state space generation approach for dynamic fault trees (DFTs) together with a technique to synthesise failures rates in DFTs. Our state space generation technique aggressively exploits the DFT structure --- detecting symmetries, spurious non-determinism, and don't cares. Benchmarks show a gain of more than two orders of magnitude in terms of state space generation and analysis time. Our approach supports DFTs with symbolic failure rates and is complemented by parameter synthesis. This enables determining the maximal tolerable failure rate of a system component while ensuring that the mean time of failure stays below a threshold. 	
1706.03664v1	http://arxiv.org/pdf/1706.03664v1	2017	Concurrent risks of dam failure due to internal degradation, strong   winds, snow and drought	Chris Collier|Alan Gadian|Ralph Burton|James Groves	  The chance (or probability) of a dam failure can change for various reasons such as structural degradation, the impacts of climate change and land-use change. Similarly the consequences of dam failure (flooding) can change for many reasons such as growth in the population in areas below a dam. Consequently both the chance that a dam might fail and the likely consequences of that failure can change over time. It is therefore crucial that reservoir safety risk analysis methods and decision-making processes are able to support (as a minimum) what-if testing (or sensitivity testing) to take into account these changes over time to gauge their effect on the estimated risk of dam failure. The consequences of a dam failure relate to the vulnerability and exposure of the receptors (for example, people, property and environment) to floodwater. Also the probability of dam failure varies with age, design and construction of the dam. Spillway failure may be caused by the dissipation of energy from water flowing down the spillway, and embankment erosion (scour) may be caused by a dam overtopping. The occurrence of these events depends upon the dam design and the likelihood of extreme rainfall, also in the case of overtopping wind-driven waves on the reservoir surface. In this study the meteorological situations of notable recent events i.e. the Boltby, North Yorkshire incident, 19 June 2005 in which the dam almost overtopped, and the spillway failure of the Ulley Dam near Rotherham at the end of June 2007, are studied. The WRF numerical model will be used to indicate how these meteorological situations might be maximized, and be coupled with the occurrence of other failure modes such as the likelihood of internal dam failure assessed from previous work by government panel engineers. 	
1102.2616v1	http://arxiv.org/pdf/1102.2616v1	2011	An Improved Multiple Faults Reassignment based Recovery in Cluster   Computing	Sanjay Bansal|Sanjeev Sharma	  In case of multiple node failures performance becomes very low as compare to single node failure. Failures of nodes in cluster computing can be tolerated by multiple fault tolerant computing. Existing recovery schemes are efficient for single fault but not with multiple faults. Recovery scheme proposed in this paper having two phases; sequentially phase, concurrent phase. In sequentially phase, loads of all working nodes are uniformly and evenly distributed by proposed dynamic rank based and load distribution algorithm. In concurrent phase, loads of all failure nodes as well as new job arrival are assigned equally to all available nodes by just finding the least loaded node among the several nodes by failure nodes job allocation algorithm. Sequential and concurrent executions of algorithms improve the performance as well better resource utilization. Dynamic rank based algorithm for load redistribution works as a sequential restoration algorithm and reassignment algorithm for distribution of failure nodes to least loaded computing nodes works as a concurrent recovery reassignment algorithm. Since load is evenly and uniformly distributed among all available working nodes with less number of iterations, low iterative time and communication overheads hence performance is improved. Dynamic ranking algorithm is low overhead, high convergence algorithm for reassignment of tasks uniformly among all available nodes. Reassignments of failure nodes are done by a low overhead efficient failure job allocation algorithm. Test results to show effectiveness of the proposed scheme are presented. 	
1103.3671v2	http://arxiv.org/pdf/1103.3671v2	2011	Easy Impossibility Proofs for k-Set Agreement in Message Passing Systems	Martin Biely|Peter Robinson|Ulrich Schmid	  Despite of being quite similar agreement problems, consensus and general k-set agreement require surprisingly different techniques for proving the impossibility in asynchronous systems with crash failures: Rather than relatively simple bivalence arguments as in the impossibility proof for consensus (= 1-set agreement) in the presence of a single crash failure, known proofs for the impossibility of k-set agreement in systems with at least k>1 crash failures use algebraic topology or a variant of Sperner's Lemma. In this paper, we present a generic theorem for proving the impossibility of k-set agreement in various message passing settings, which is based on a simple reduction to the consensus impossibility in a certain subsystem. We demonstrate the broad applicability of our result by exploring the possibility/impossibility border of k-set agreement in several message-passing system models: (i) asynchronous systems with crash failures, (ii) partially synchronous processes with (initial) crash failures, and (iii) asynchronous systems augmented with failure detectors. In (i) and (ii), the impossibility part is just an instantiation of our main theorem, whereas the possibility of achieving k-set agreement in (ii) follows by generalizing the consensus algorithm for initial crashes by Fisher, Lynch and Patterson. In (iii), applying our technique yields the exact border for the parameter k where k-set agreement is solvable with the failure detector class (Sigma_k,Omega_k), for (1<= k<= n-1), of Bonnet and Raynal. Considering that Sigma_k was shown to be necessary for solving k-set agreement, this result yields new insights on the quest for the weakest failure detector. 	
1401.0750v4	http://arxiv.org/pdf/1401.0750v4	2014	An Interaction Model for Simulation and Mitigation of Cascading Failures	Junjian Qi|Kai Sun|Shengwei Mei	  In this paper the interactions between component failures are quantified and the interaction matrix and interaction network are obtained. The quantified interactions can capture the general propagation patterns of the cascades from utilities or simulation, thus helping to better understand how cascading failures propagate and to identify key links and key components that are crucial for cascading failure propagation. By utilizing these interactions a high-level probabilistic model called interaction model is proposed to study the influence of interactions on cascading failure risk and to support online decision-making. It is much more time efficient to first quantify the interactions between component failures with fewer original cascades from a more detailed cascading failure model and then perform the interaction model simulation than it is to directly simulate a large number of cascades with a more detailed model. Interaction-based mitigation measures are suggested to mitigate cascading failure risk by weakening key links, which can be achieved in real systems by wide area protection such as blocking of some specific protective relays. The proposed interaction quantifying method and interaction model are validated with line outage data generated by the AC OPA cascading simulations on the IEEE 118-bus system. 	
1401.2282v1	http://arxiv.org/pdf/1401.2282v1	2014	How do heterogeneities in operating environments affect field failure   predictions and test planning?	Zhi-Sheng Ye|Yili Hong|Yimeng Xie	  The main objective of accelerated life tests (ALTs) is to predict fraction failings of products in the field. However, there are often discrepancies between the predicted fraction failing from the lab testing data and that from the field failure data, due to the yet unobserved heterogeneities in usage and operating conditions. Most previous research on ALT planning and data analysis ignores the discrepancies, resulting in inferior test plans and biased predictions. In this paper we model the heterogeneous environments together with their effects on the product failures as a frailty term to link the lab failure time distribution and field failure time distribution of a product. We show that in the presence of the heterogeneous operating conditions, the hazard rate function of the field failure time distribution exhibits a range of shapes. Statistical inference procedure for the frailty models is developed when both the ALT data and the field failure data are available. Based on the frailty models, optimal ALT plans aimed at predicting the field failure time distribution are obtained. The developed methods are demonstrated through a real life example. 	
1406.5282v2	http://arxiv.org/pdf/1406.5282v2	2014	STAIR Codes: A General Family of Erasure Codes for Tolerating Device and   Sector Failures	Mingqiang Li|Patrick P. C. Lee	  Practical storage systems often adopt erasure codes to tolerate device failures and sector failures, both of which are prevalent in the field. However, traditional erasure codes employ device-level redundancy to protect against sector failures, and hence incur significant space overhead. Recent sector-disk (SD) codes are available only for limited configurations. By making a relaxed but practical assumption, we construct a general family of erasure codes called \emph{STAIR codes}, which efficiently and provably tolerate both device and sector failures without any restriction on the size of a storage array and the numbers of tolerable device failures and sector failures. We propose the \emph{upstairs encoding} and \emph{downstairs encoding} methods, which provide complementary performance advantages for different configurations. We conduct extensive experiments on STAIR codes in terms of space saving, encoding/decoding speed, and update cost. We demonstrate that STAIR codes not only improve space efficiency over traditional erasure codes, but also provide better computational efficiency than SD codes based on our special code construction. Finally, we present analytical models that characterize the reliability of STAIR codes, and show that the support of a wider range of configurations by STAIR codes is critical for tolerating sector failure bursts discovered in the field. 	
1610.00997v1	http://arxiv.org/pdf/1610.00997v1	2016	Impact of embedding on predictability of failure-recovery dynamics in   networks	Lucas Böttcher|Mirko Lukovic|Jan Nagler|Shlomo Havlin|Hans J. Herrmann	  Failure, damage spread and recovery crucially underlie many spatially embedded networked systems ranging from transportation structures to the human body. Here we study the interplay between spontaneous damage, induced failure and recovery in both embedded and non-embedded networks. In our model the network's components follow three realistic processes that capture these features: (i) spontaneous failure of a component independent of the neighborhood (internal failure), (ii) failure induced by failed neighboring nodes (external failure) and (iii) spontaneous recovery of a component.We identify a metastable domain in the global network phase diagram spanned by the model's control parameters where dramatic hysteresis effects and random switching between two coexisting states are observed. The loss of predictability due to these effects depend on the characteristic link length of the embedded system. For the Euclidean lattice in particular, hysteresis and switching only occur in an extremely narrow region of the parameter space compared to random networks. We develop a unifying theory which links the dynamics of our model to contact processes. Our unifying framework may help to better understand predictability and controllability in spatially embedded and random networks where spontaneous recovery of components can mitigate spontaneous failure and damage spread in the global network. 	
1610.01364v1	http://arxiv.org/pdf/1610.01364v1	2016	FMEA Based Risk Assessment of Component Failure Modes in Industrial   Radiography	Alok Pandey|Meghraj Singh|A. U. Sonawane|Prashant S. Rawat	  Industrial radiography has its inimitable role in non-destructive examinations. Industrial radiography devices, consisting of significantly high activity of the radioisotopes, are operated manually by remotely held control unit. Malfunctioning of these devices may cause potential exposure to the operator and nearby public, and thus should be practiced under a systematic risk control. To ensure the radiation safety, proactive risk assessment should be implemented. Risk assessment in industrial radiography using the Failure Modes & Effect Analysis (FMEA) for the design and operation of industrial radiography exposure devices has been carried out in this study. Total 56 component failure modes were identified and Risk Priority Numbers (RPNs) were assigned by the FMEA expert team, based on the field experience and reported failure data of various components. Results shows all the identified failure modes have RPN in the range of 04 to 216 and most of the higher RPN are due to low detectability and high severity levels. Assessment reveals that increasing failure detectability is a practical and feasible approach to reduce the risk in most of the failure modes of industrial radiography devices. Actions for reducing RPN for each failure mode have been suggested. Feasibility of FMEA for risk assessment in industrial radiography has been established by this study 	
1701.01539v4	http://arxiv.org/pdf/1701.01539v4	2017	Algorithms for Optimal Replica Placement Under Correlated Failure in   Hierarchical Failure Domains	K. Alex Mills|R. Chandrasekaran|Neeraj Mittal	  In data centers, data replication is the primary method used to ensure availability of customer data. To avoid correlated failure, cloud storage infrastructure providers model hierarchical failure domains using a tree, and avoid placing a large number of data replicas within the same failure domain (i.e. on the same branch of the tree). Typical best practices ensure that replicas are distributed across failure domains, but relatively little is known concerning optimization algorithms for distributing data replicas. Using a hierarchical model, we answer how to distribute replicas across failure domains optimally. We formulate a novel optimization problem for replica placement in data centers. As part of our problem, we formalize and explain a new criterion for optimizing a replica placement. Our overall goal is to choose placements in which correlated failures disable as few replicas as possible. We provide two optimization algorithms for dependency models represented by trees. We first present an $O(n + \rho \log \rho)$ time dynamic programming algorithm for placing $\rho$ replicas of a single file on the leaves (representing servers) of a tree with $n$ vertices. We next consider the problem of placing replicas of $m$ blocks of data, where each block may have different replication factors. For this problem, we give an exact algorithm which runs in polynomial time when the skew, the difference in the number of replicas between the largest and smallest blocks of data, is constant. 	
1709.07302v2	http://arxiv.org/pdf/1709.07302v2	2018	Influence of Clustering on Cascading Failures in Interdependent Systems	Richard J. La	  We study the influence of clustering, more specifically triangles, on cascading failures in interdependent networks or systems, in which we model the dependence between comprising systems using a dependence graph. First, we propose a new model that captures how the presence of triangles in the dependence graph alters the manner in which failures transmit from affected systems to others. Unlike existing models, the new model allows us to approximate the failure propagation dynamics using a multi-type branching process, even with triangles. Second, making use of the model, we provide a simple condition that indicates how increasing clustering will affect the likelihood that a random failure triggers a cascade of failures, which we call the probability of cascading failures (PoCF). In particular, our condition reveals an intriguing observation that the influence of clustering on PoCF depends on the vulnerability of comprising systems to an increasing number of failed neighboring systems and the current PoCF, starting with different types of failed systems. Our numerical studies hint that increasing clustering impedes cascading failures under both (truncated) power law and Poisson degree distributions. Furthermore, our finding suggests that, as the degree distribution becomes more concentrated around the mean degree with smaller variance, increasing clustering will have greater impact on the PoCF. A numerical investigation of networks with Poisson and power law degree distributions reflects this finding and demonstrates that increasing clustering reduces the PoCF much faster under Poisson degree distributions in comparison to power law degree distributions. 	
1712.09666v1	http://arxiv.org/pdf/1712.09666v1	2017	A Fast and Accurate Failure Frequency Approximation for $k$-Terminal   Reliability Systems	Anoosheh Heidarzadeh|Alex Sprintson|Chanan Singh	  This paper considers the problem of approximating the failure frequency of large-scale composite $k$-terminal reliability systems. In such systems, the nodes ($k$ of which are terminals) are connected through components which are subject to random failure and repair processes. At any time, a system failure occurs if the surviving system fails to connect all the k terminals together. We assume that each component's up-times and down-times follow statistically independent stationary random processes, and these processes are statistically independent across the components. In this setting, the exact computation of failure frequency is known to be computationally intractable (NP-hard). In this work, we present an algorithm to approximate the failure frequency for any given multiplicative error factor that runs in polynomial time in the number of (minimal) cutsets. Moreover, for the special case of all-terminal reliability systems, i.e., where all nodes are terminals, we propose an algorithm for approximating the failure frequency within an arbitrary multiplicative error that runs in polynomial time in the number of nodes (which can be much smaller than the number of cutsets). In addition, our simulation results confirm that the proposed method is much faster and more accurate than the Monte Carlo simulation technique for approximating the failure frequency. 	
1801.10321v1	http://arxiv.org/pdf/1801.10321v1	2018	Derivative-Free Failure Avoidance Control for Manipulation using Learned   Support Constraints	Jonathan Lee|Michael Laskey|Roy Fox|Ken Goldberg	  Learning to accomplish tasks such as driving, grasping or surgery from supervisor demonstrations can be risky when the execution of the learned policy leads to col- lisions and other costly failures. Adding explicit constraints to stay within safe zones is often not possible when the state representations are complex. Furthermore, enforcing these constraints during execution of the learned policy can be difficult in environments where dynamics are not known. In this paper, we propose a two-phase method for safe control from demonstrations in robotic manipulation tasks where changes in state are limited by the magnitude of control applied. In the first phase, we use support estimation of supervisor demonstrations to infer implicit constraints on states in addition to learning a policy directly from the observed controls. We also propose a time-varying modification to the support estimation problem allowing for accurate estimation on sequential tasks. In the second phase, we present a switching policy to prevent the robot from leaving safe regions of the state space during run time using the decision function of the estimated support. The policy switches between the robot's learned policy and a novel failure avoidance policy depending on the distance to the boundary of the support. We prove that inferred constraints are guaranteed to be enforced using this failure avoidance policy if the support is well-estimated. A simulated pushing task suggests that support estimation and failure avoidance control can reduce failures by 87% while sacrificing only 40% of performance. On a line tracking task using a da Vinci Surgical Robot, failure avoidance control reduced failures by 84%. 	
0511134v1	http://arxiv.org/pdf/cond-mat/0511134v1	2005	Optimal Prediction of Time-to-Failure from Information Revealed by   Damage	D. Sornette|J. V. Andersen	  We present a general prediction scheme of failure times based on updating continuously with time the probability for failure of the global system, conditioned on the information revealed on the pre-existing idiosyncratic realization of the system by the damage that has occurred until the present time. Its implementation on a simple prototype system of interacting elements with unknown random lifetimes undergoing irreversible damage until a global rupture occurs shows that the most probable predicted failure time (mode) may evolve non-monotonically with time as information is incorporated in the prediction scheme. In addition, both the mode, its standard deviation and, in fact, the full distribution of predicted failure times exhibit sensitive dependence on the realization of the system, similarly to ``chaos'' in spinglasses, providing a multi-dimensional dynamical explanation for the broad distribution of failure times observed in many empirical situations. 	
0809.4107v1	http://arxiv.org/pdf/0809.4107v1	2008	Modelling interdependencies between the electricity and information   infrastructures	Jean-Claude Laprie|Karama Kanoun|Mohamed Kaaniche	  The aim of this paper is to provide qualitative models characterizing interdependencies related failures of two critical infrastructures: the electricity infrastructure and the associated information infrastructure. The interdependencies of these two infrastructures are increasing due to a growing connection of the power grid networks to the global information infrastructure, as a consequence of market deregulation and opening. These interdependencies increase the risk of failures. We focus on cascading, escalating and common-cause failures, which correspond to the main causes of failures due to interdependencies. We address failures in the electricity infrastructure, in combination with accidental failures in the information infrastructure, then we show briefly how malicious attacks in the information infrastructure can be addressed. 	
0902.4447v2	http://arxiv.org/pdf/0902.4447v2	2009	Percolation Processes and Wireless Network Resilience to   Degree-Dependent and Cascading Node Failures	Zhenning Kong|Edmund M. Yeh	  We study the problem of wireless network resilience to node failures from a percolation-based perspective. In practical wireless networks, it is often the case that the failure probability of a node depends on its degree (number of neighbors). We model this phenomenon as a degree-dependent site percolation process on random geometric graphs. In particular, we obtain analytical conditions for the existence of phase transitions within this model. Furthermore, in networks carrying traffic load, the failure of one node can result in redistribution of the load onto other nearby nodes. If these nodes fail due to excessive load, then this process can result in a cascading failure. Using a simple but descriptive model, we show that the cascading failure problem for large-scale wireless networks is equivalent to a degree-dependent site percolation on random geometric graphs. We obtain analytical conditions for cascades in this model. 	
0908.4290v1	http://arxiv.org/pdf/0908.4290v1	2009	Bridging the Gap between Crisis Response Operations and Systems	Khaled M. Khalil|M. Abdel-Aziz|Taymour T. Nazmy|Abdel-Badeeh M. Salem	  There exist huge problems in the current practice of crisis response operations. Response problems are projected as a combination of failure in communication, failure in technology, failure in methodology, failure of management, and finally failure of observation. In this paper we compare eight crisis response systems namely: DrillSim [2, 13], DEFACTO [12, 17], ALADDIN [1, 6], RoboCup Rescue [11, 15], FireGrid [3, 8, 18], WIPER [16], D-AESOP [4], and PLAN C [14]. Comparison results will disclose the cause of failure of current crisis response operations (the response gap). Based on comparison results; we provide recommendations for bridging this gap between response operations and systems. 	
1008.3369v1	http://arxiv.org/pdf/1008.3369v1	2010	Investigating reciprocity failure in 1.7-micron cut-off HgCdTe detectors	Michael Schubnell|Tomasz Biesiadzinski|Wolfgang Lorenzon|Robert Newman|Greg Tarle	  Flux dependent non-linearity (reciprocity failure) in HgCdTe NIR detectors with 1.7 micron cut-off was investigated. A dedicated test station was designed and built to measure reciprocity failure over the full dynamic range of near infrared detectors. For flux levels between 1 and 100,000 photons/sec a limiting sensitivity to reciprocity failure of 0.3%/decade was achieved. First measurements on several engineering grade 1.7 micron cut-off HgCdTe detectors show a wide range of reciprocity failure, from less than 0.5%/decade to about 10%/decade. For at least two of the tested detectors, significant spatial variation in the effect was observed. No indication for wavelength dependency was found. The origin of reciprocity failure is currently not well understood. In this paper we present details of our experimental set-up and show the results of measurements for several detectors. 	
1012.5961v2	http://arxiv.org/pdf/1012.5961v2	2010	Vulnerability of Networks Against Critical Link Failures	Serdar Çolak|Hilmi Luş|Ali Rana Atılgan	  Networks are known to be prone to link failures. In this paper we set out to investigate how networks of varying connectivity patterns respond to different link failure schemes in terms of connectivity, clustering coefficient and shortest path lengths. We then propose a measure, which we call the vulnerability of a network, for evaluating the extent of the damage these failures can cause. Accepting the disconnections of node pairs as a damage indicator, vulnerability simply represents how quickly the failure of the critical links cause the network to undergo a specified damage extent. Analyzing the vulnerabilities under varying damage specifications shows that scale free networks are relatively more vulnerable for small failures, but more efficient; whereas Erd\"os-R\'enyi networks are the least vulnerable despite lacking any clustered structure. 	
1109.3056v3	http://arxiv.org/pdf/1109.3056v3	2012	Wait-Freedom with Advice	Carole Delporte-Gallet|Hugues Fauconnier|Eli Gafni|Petr Kuznetsov	  We motivate and propose a new way of thinking about failure detectors which allows us to define, quite surprisingly, what it means to solve a distributed task \emph{wait-free} \emph{using a failure detector}. In our model, the system is composed of \emph{computation} processes that obtain inputs and are supposed to output in a finite number of steps and \emph{synchronization} processes that are subject to failures and can query a failure detector. We assume that, under the condition that \emph{correct} synchronization processes take sufficiently many steps, they provide the computation processes with enough \emph{advice} to solve the given task wait-free: every computation process outputs in a finite number of its own steps, regardless of the behavior of other computation processes. Every task can thus be characterized by the \emph{weakest} failure detector that allows for solving it, and we show that every such failure detector captures a form of set agreement. We then obtain a complete classification of tasks, including ones that evaded comprehensible characterization so far, such as renaming or weak symmetry breaking. 	
1212.0311v1	http://arxiv.org/pdf/1212.0311v1	2012	Enhanced Multiple Routing Configurations For Fast IP Network Recovery   From Multiple Failures	T. Anji Kumar|M. H. M. Krishna Prasad	  Now a days, Internet plays a major role in our day to day activities e.g., for online transactions, online shopping, and other network related applications. Internet suffers from slow convergence of routing protocols after a network failure which becomes a growing problem. Multiple Routing Configurations [MRC] recovers network from single node/link failures, but does not support network from multiple node/link failures. In this paper, we propose Enhanced MRC [EMRC], to support multiple node/link failures during data transmission in IP networks without frequent global re-convergence. By recovering these failures, data transmission in network will become fast. 	
1302.4984v1	http://arxiv.org/pdf/1302.4984v1	2013	Modeling Failure Priors and Persistence in Model-Based Diagnosis	Sampath Srinivas	  Probabilistic model-based diagnosis computes the posterior probabilities of failure of components from the prior probabilities of component failure and observations of system behavior. One problem with this method is that such priors are almost never directly available. One of the reasons is that the prior probability estimates include an implicit notion of a time interval over which they are specified -- for example, if the probability of failure of a component is 0.05, is this over the period of a day or is this over a week? A second problem facing probabilistic model-based diagnosis is the modeling of persistence. Say we have an observation about a system at time t_1 and then another observation at a later time t_2. To compute posterior probabilities that take into account both the observations, we need some model of how the state of the system changes from time t_1 to t_2. In this paper, we address these problems using techniques from Reliability theory. We show how to compute the failure prior of a component from an empirical measure of its reliability -- the Mean Time Between Failure (MTBF). We also develop a scheme to model persistence when handling multiple time tagged observations. 	
1306.6818v3	http://arxiv.org/pdf/1306.6818v3	2014	Tetrahedral Elliptic Curves and the local-global principle for Isogenies	Barinder Singh Banwait|John Cremona	  We study the failure of a local-global principle for the existence of $l$-isogenies for elliptic curves over number fields $K$. Sutherland has shown that over $\mathbb{Q}$ there is just one failure, which occurs for $l=7$ and a unique $j$-invariant, and has given a classification of such failures when $K$ does not contain the quadratic subfield of the $l$'th cyclotomic field. In this paper we provide a classification of failures for number fields which do contain this quadratic field, and we find a new `exceptional' source of such failures arising from the exceptional subgroups of $\mbox{PGL}_2(\mathbb{F}_l)$. By constructing models of two modular curves, $X_{\text{s}}(5)$ and $X_{S_4}(13)$, we find two new families of elliptic curves for which the principle fails, and we show that, for quadratic fields, there can be no other exceptional failures. 	
1404.6415v1	http://arxiv.org/pdf/1404.6415v1	2014	The Impact Failure Detector	Anubis G. M. Rossetto|Cláudio F. R. Geyer|Luciana Arantes|Pierre Sens	  This work proposes a new and flexible unreliable failure detector whose output is related to the trust level of a set of processes. By expressing the relevance of each process of the set by an impact factor value, our approach allows the tuning of the detector output, making possible a softer or stricter monitoring. The idea behind our proposal is that, according to an acceptable margin of failures and the impact factor assigned to processes, in some scenarios, the failure of some low impact processes may not change the user confidence in the set of processes, while the crash of a high impact factor process may seriously affect it. We outline the application scenarios and the proposed unreliable failure detector, giving a detailed account of the concept on which it is based. 	
1407.5925v1	http://arxiv.org/pdf/1407.5925v1	2014	Emergence of localized plasticity and failure through shear banding   during microcompression of a nanocrystalline alloy	Amirhossein Khalajhedayati|Timothy J. Rupert	  Microcompression testing is used to probe the uniaxial stress-strain response of a nanocrystalline alloy, with an emphasis on exploring how grain size and grain boundary relaxation state impact the complete flow curve and failure behavior. The yield strength, strain hardening, strain-to-failure, and failure mode of nanocrystalline Ni-W films with mean grain sizes of 5, 15, and 90 nm are studied using taper-free micropillars that are large enough to avoid extrinsic size effects. Strengthening is observed with grain refinement, but catastrophic failure through strain localization is found as well. Shear banding is found to cause failure, resembling the deformation of metallic glasses. Finally, we study the influence of grain boundary state by employing heat treatments that relax nonequilibrium boundary structure but leave grain size unchanged. A pronounced strengthening effect and increased strain localization is observed after relaxation in the finer grained samples. 	
1408.5951v5	http://arxiv.org/pdf/1408.5951v5	2016	Fragility of the Commons under Prospect-Theoretic Risk Attitudes	Ashish R. Hota|Siddharth Garg|Shreyas Sundaram	  We study a common-pool resource game where the resource experiences failure with a probability that grows with the aggregate investment in the resource. To capture decision making under such uncertainty, we model each player's risk preference according to the value function from prospect theory. We show the existence and uniqueness of a pure Nash equilibrium when the players have heterogeneous risk preferences and under certain assumptions on the rate of return and failure probability of the resource. Greater competition, vis-a-vis the number of players, increases the failure probability at the Nash equilibrium; we quantify this effect by obtaining bounds on the ratio of the failure probability at the Nash equilibrium to the failure probability under investment by a single user. We further show that heterogeneity in attitudes towards loss aversion leads to higher failure probability of the resource at the equilibrium. 	
1409.5401v1	http://arxiv.org/pdf/1409.5401v1	2014	Failure Detection and Isolation in Integrator Networks	Mohammad Amin Rahimian|Victor M. Preciado	  Detection and isolation of link failures under the Laplacian consensus dynamics have been the focus of our previous study. Our results relate the failure of links in the network to jump discontinuities in the derivatives of the output responses of the nodes and exploit that relation to propose failure detection and isolation (FDI) techniques, accordingly. In this work, we extend the results to general linear networked dynamics. In particular, we show that with additional niceties of the integrator networks and the enhanced proofs, we are able to incorporate both unidirectional and bidirectional link failures. At the next step, we extend the available FDI techniques to accommodate the cases of bidirectional link failures and undirected topologies. Computer experiments with large networks and both directed and undirected topologies provide interesting insights as to the role of directionality, as well as the scalability of the proposed FDI techniques with the network size. 	
1411.3197v1	http://arxiv.org/pdf/1411.3197v1	2014	Warranty Cost Estimation Using Bayesian Network	Karamjit Singh|Puneet Agarwal|Gautam Shroff	  All multi-component product manufacturing companies face the problem of warranty cost estimation. Failure rate analysis of components plays a key role in this problem. Data source used for failure rate analysis has traditionally been past failure data of components. However, failure rate analysis can be improved by means of fusion of additional information, such as symptoms observed during after-sale service of the product, geographical information (hilly or plains areas), and information from tele-diagnostic analytics. In this paper, we propose an approach, which learns dependency between part-failures and symptoms gleaned from such diverse sources of information, to predict expected number of failures with better accuracy. We also indicate how the optimum warranty period can be computed. We demonstrate, through empirical results, that our method can improve the warranty cost estimates significantly. 	
1412.3687v1	http://arxiv.org/pdf/1412.3687v1	2014	Modelling common cause failures of large digital I&C systems with   coloured Petri nets	Gilles Deleuze|Nicolae Brinzei|Nicolas Villaume	  The purpose of this study is the representation of Common Cause Failures (CCF) in large digital systems. The system under study is representative of a control system of a nuclear plant. The model for CCF is the generalized Atwood model. It can represent independent failures, CCF non-lethal for some system elements and CCF lethal to all. The Atwood model was modified to "direct" non-lethal DCC on certain parts of the system and take into account the different possible origins of DCC. Maintenance and repairs are taken into account in the model that is thus dynamic. The main evaluation results are probabilistic, the considered indicator is the probability of failure on demand (PFD). A comparison is made between the estimator of the PFD taking into account all the failures and the estimator taking into account only the detected failures. 	
1501.04227v2	http://arxiv.org/pdf/1501.04227v2	2016	Tunable failure: control of rupture through rigidity	Michelle M. Driscoll|Bryan Gin-ge Chen|Thomas H. Beuman|Stephan Ulrich|Sidney R. Nagel|Vincenzo Vitelli	  We investigate how material rigidity acts as a key control parameter for the failure of solids under stress. In both experiments and simulations, we demonstrate that material failure can be continuously tuned by varying the underlying rigidity of the material while holding the amount of disorder constant. As the rigidity transition is approached, failure due to the application of uniaxial stress evolves from brittle cracking to system-spanning diffuse breaking. This evolution in failure behavior can be parameterized by the width of the crack. As a system becomes more and more floppy, this crack width increases until it saturates at the system size. Thus, the spatial extent of the failure zone can be used as a direct probe for material rigidity. 	
1505.03469v1	http://arxiv.org/pdf/1505.03469v1	2015	The Weakest Failure Detector for Eventual Consistency	Swan Dubois|Rachid Guerraoui|Petr Kuznetsov|Franck Petit|Pierre Sens	  In its classical form, a consistent replicated service requires all replicas to witness the same evolution of the service state. Assuming a message-passing environment with a majority of correct processes, the necessary and sufficient information about failures for implementing a general state machine replication scheme ensuring consistency is captured by the {\Omega} failure detector. This paper shows that in such a message-passing environment, {\Omega} is also the weakest failure detector to implement an eventually consistent replicated service, where replicas are expected to agree on the evolution of the service state only after some (a priori unknown) time. In fact, we show that {\Omega} is the weakest to implement eventual consistency in any message-passing environment, i.e., under any assumption on when and where failures might occur. Ensuring (strong) consistency in any environment requires, in addition to {\Omega}, the quorum failure detector {\Sigma}. Our paper thus captures, for the first time, an exact computational difference be- tween building a replicated state machine that ensures consistency and one that only ensures eventual consistency. 	
1508.01122v1	http://arxiv.org/pdf/1508.01122v1	2015	On Bivariate Generalized Linear Failure Rate-Power Series Class of   Distributions	Rasool Roozegar|Ali Akbar Jafari	  Recently it has been observed that the bivariate generalized linear failure rate distribution can be used quite effectively to analyze lifetime data in two dimensions. This paper introduces a more general class of bivariate distributions. We refer to this new class of distributions as bivariate generalized linear failure rate power series model. This new class of bivariate distributions contains several lifetime models such as: generalized linear failure rate-power series, bivariate generalized linear failure rate and bivariate generalized linear failure rate geometric distributions as special cases among others. The construction and characteristics of the proposed bivariate distribution are presented along with estimation procedures for the model parameters based on maximum likelihood. The marginal and conditional laws are also studied. We present an application to the real data set where our model provides a better fit than other models. 	
1510.08380v1	http://arxiv.org/pdf/1510.08380v1	2015	Towards a Realistic Model for Failure Propagation in Interdependent   Networks	Agostino Sturaro|Simone Silvestri|Mauro Conti|Sajal K. Das	  Modern networks are becoming increasingly interdependent. As a prominent example, the smart grid is an electrical grid controlled through a communications network, which in turn is powered by the electrical grid. Such interdependencies create new vulnerabilities and make these networks more susceptible to failures. In particular, failures can easily spread across these networks due to their interdependencies, possibly causing cascade effects with a devastating impact on their functionalities.   In this paper we focus on the interdependence between the power grid and the communications network, and propose a novel realistic model, HINT (Heterogeneous Interdependent NeTworks), to study the evolution of cascading failures. Our model takes into account the heterogeneity of such networks as well as their complex interdependencies. We compare HINT with previously proposed models both on synthetic and real network topologies. Experimental results show that existing models oversimplify the failure evolution and network functionality requirements, resulting in severe underestimations of the cascading failures. 	
1604.03677v1	http://arxiv.org/pdf/1604.03677v1	2016	Robustness of Power-law Behavior in Cascading Failure Models	F. Sloothaak|S. C. Borst|A. P. Zwart	  Inspired by reliability issues in electric transmission networks, we use a probabilistic approach to study the occurrence of large failures in a stylized cascading failure model. In this model, lines have random capacities that initially meet the load demands imposed on the network. Every single line failure changes the load distribution in the surviving network, possibly causing further lines to become overloaded and trip as well. An initial single line failure can therefore potentially trigger massive cascading effects, and in this paper we measure the risk of such cascading events by the probability that the number of failed lines exceeds a certain large threshold. Under particular critical conditions, the exceedance probability follows a power-law distribution, implying a significant risk of severe failures. We examine the robustness of the power-law behavior by exploring under which assumptions this behavior prevails. 	
1606.00892v1	http://arxiv.org/pdf/1606.00892v1	2016	Developing a Methodology for Online Service Failure Prevention:   Reporting on an Action Design Research Project-in-Progress	Jacques Louis Du Preez|Mary Tate|Alireza Nili	  The increasing use of online channels for service delivery raises new challenges in service failure prevention. This work-in-progress paper reports on the first phase of an action-design research project to develop a service failure prevention methodology. In this paper we review the literature on online services, failure prevention and failure recovery and develop a theoretical framework for online service failure prevention. This provides the theoretical grounding for the artefact (the methodology) to be developed. We use this framework to develop an initial draft of our methodology. We then outline the remaining phases of the research, and offer some initial conclusions gained from the project to date. 	
1801.04523v1	http://arxiv.org/pdf/1801.04523v1	2018	Shrink or Substitute: Handling Process Failures in HPC Systems using   In-situ Recovery	Rizwan A. Ashraf|Saurabh Hukerikar|Christian Engelmann	  Efficient utilization of today's high-performance computing (HPC) systems with complex hardware and software components requires that the HPC applications are designed to tolerate process failures at runtime. With low mean time to failure (MTTF) of current and future HPC systems, long running simulations on these systems require capabilities for gracefully handling process failures by the applications themselves. In this paper, we explore the use of fault tolerance extensions to Message Passing Interface (MPI) called user-level failure mitigation (ULFM) for handling process failures without the need to discard the progress made by the application. We explore two alternative recovery strategies, which use ULFM along with application-driven in-memory checkpointing. In the first case, the application is recovered with only the surviving processes, and in the second case, spares are used to replace the failed processes, such that the original configuration of the application is restored. Our experimental results demonstrate that graceful degradation is a viable alternative for recovery in environments where spares may not be available. 	
1509.02707v1	http://arxiv.org/pdf/1509.02707v1	2015	Gravitational slopes, geomorphology, and material strengths of the   nucleus of comet 67P/Churyumov-Gerasimenko from OSIRIS observations	O. Groussin|L. Jorda|A. -T. Auger|E. Kührt|R. Gaskell|C. Capanna|F. Scholten|F. Preusker|P. Lamy|S. Hviid|J. Knollenberg|U. Keller|C. Huettig|H. Sierks|C. Barbieri|R. Rodrigo|D. Koschny|H. Rickman|M. F. A Hearn|J. Agarwal|M. A. Barucci|J. -L. Bertaux|I. Bertini|S. Boudreault|G. Cremonese|V. Da Deppo|B. Davidsson|S. Debei|M. De Cecco|M. R. El-Maarry|S. Fornasier|M. Fulle|P. J. Gutiérrez|C. Güttler|W. -H Ip|J. -R. Kramm|M. Küppers|M. Lazzarin|L. M. Lara|J. J. Lopez Moreno|S. Marchi|F. Marzari|M. Massironi|H. Michalik|G. Naletto|N. Oklay|A. Pommerol|M. Pajola|N. Thomas|I. Toth|C. Tubiana|J. -B. Vincent	  We study the link between gravitational slopes and the surface morphology on the nucleus of comet 67P/Churyumov-Gerasimenko and provide constraints on the mechanical properties of the cometary material. We computed the gravitational slopes for five regions on the nucleus that are representative of the different morphologies observed on the surface, using two shape models computed from OSIRIS images by the stereo-photoclinometry (SPC) and stereo-photogrammetry (SPG) techniques. We estimated the tensile, shear, and compressive strengths using different surface morphologies and mechanical considerations. The different regions show a similar general pattern in terms of the relation between gravitational slopes and terrain morphology: i) low-slope terrains (0-20 deg) are covered by a fine material and contain a few large ($>$10 m) and isolated boulders, ii) intermediate-slope terrains (20-45 deg) are mainly fallen consolidated materials and debris fields, with numerous intermediate-size boulders from $<$1 m to 10 m for the majority of them, and iii) high-slope terrains (45-90 deg) are cliffs that expose a consolidated material and do not show boulders or fine materials. The best range for the tensile strength of overhangs is 3-15 Pa (upper limit of 150 Pa), 4-30 Pa for the shear strength of fine surface materials and boulders, and 30-150 Pa for the compressive strength of overhangs (upper limit of 1500 Pa). The strength-to-gravity ratio is similar for 67P and weak rocks on Earth. As a result of the low compressive strength, the interior of the nucleus may have been compressed sufficiently to initiate diagenesis, which could have contributed to the formation of layers. Our value for the tensile strength is comparable to that of dust aggregates formed by gravitational instability and tends to favor a formation of comets by the accrection of pebbles at low velocities. 	
9612107v1	http://arxiv.org/pdf/astro-ph/9612107v1	1996	An Investigation of Neutrino-Driven Convection and the Core Collapse   Supernova Mechanism Using Multigroup Neutrino Transport	A. Mezzacappa|A. C. Calder|S. W. Bruenn|J. M. Blondin|M. W. Guidry|M. R. Strayer|A. S. Umar	  We investigate neutrino-driven convection in core collapse supernovae and its ramifications for the explosion mechanism. We begin with an ``optimistic'' 15 solar mass precollapse model, which is representative of the class of stars with compact iron cores. This model is evolved through core collapse and bounce in one dimension using multigroup (neutrino-energy--dependent) flux-limited diffusion (MGFLD) neutrino transport and Lagrangian hydrodynamics, providing realistic initial conditions for the postbounce convection and evolution. Our two-dimensional simulation begins at 106 ms after bounce at a time when there is a well-developed gain region, and proceeds for 400 ms. We couple two-dimensional (PPM) hydrodynamics to one-dimensional MGFLD neutrino transport. At 225 ms after bounce we see large-scale convection behind the shock, characterized by high-entropy, mushroom-like, expanding upflows and dense, low-entropy, finger-like downflows. The upflows reach the shock and distort it from sphericity. The radial convection velocities become supersonic just below the shock, reaching magnitudes in excess of 10^9 cm/sec. Eventually, however, the shock recedes to smaller radii, and at about 500 ms after bounce there is no evidence in our simulation of an explosion or of a developing explosion. Failure in our ``optimistic'' 15 solar mass Newtonian model leads us to conclude that it is unlikely, at least in our approximation, that neutrino-driven convection will lead to explosions for more massive stars with fatter iron cores or in cases in which general relativity is included. 	
0005366v5	http://arxiv.org/pdf/astro-ph/0005366v5	2000	Simulation of the Spherically Symmetric Stellar Core Collapse, Bounce,   and Postbounce Evolution of a 13 Solar Mass Star with Boltzmann Neutrino   Transport, and Its Implications for the Supernova Mechanism	Anthony Mezzacappa|Matthias Liebendoerfer|O. E. Bronson Messer|W. Raphael Hix|Friederich-Karl Thielemann|Stephen W. Bruenn	  With exact three-flavor Boltzmann neutrino transport, we simulate the stellar core collapse, bounce, and postbounce evolution of a 13 solar mass star in spherical symmetry, the Newtonian limit, without invoking convection. In the absence of convection, prior spherically symmetric models, which implemented approximations to Boltzmann transport, failed to produce explosions. We are motivated to consider exact transport to determine if these failures were due to the transport approximations made and to answer remaining fundamental questions in supernova theory. The model presented here is the first in a sequence of models beginning with different progenitors. In this model, a supernova explosion is not obtained. We discuss the ramifications of our results for the supernova mechanism. 	
9812232v1	http://arxiv.org/pdf/cond-mat/9812232v1	1998	Coupling of Length Scales and Atomistic Simulation of MEMS Resonators	Robert E. Rudd|Jeremy Q. Broughton	  We present simulations of the dynamic and temperature dependent behavior of Micro-Electro-Mechanical Systems (MEMS) by utilizing recently developed parallel codes which enable a coupling of length scales. The novel techniques used in this simulation accurately model the behavior of the mechanical components of MEMS down to the atomic scale. We study the vibrational behavior of one class of MEMS devices: micron-scale resonators made of silicon and quartz. The algorithmic and computational avenue applied here represents a significant departure from the usual finite element approach based on continuum elastic theory. The approach is to use an atomistic simulation in regions of significantly anharmonic forces and large surface area to volume ratios or where internal friction due to defects is anticipated. Peripheral regions of MEMS which are well-described by continuum elastic theory are simulated using finite elements for efficiency. Thus, in central regions of the device, the motion of millions of individual atoms is simulated, while the relatively large peripheral regions are modeled with finite elements. The two techniques run concurrently and mesh seamlessly, passing information back and forth. This coupling of length scales gives a natural domain decomposition, so that the code runs on multiprocessor workstations and supercomputers. We present novel simulations of the vibrational behavior of micron-scale silicon and quartz oscillators. Our results are contrasted with the predictions of continuum elastic theory as a function of size, and the failure of the continuum techniques is clear in the limit of small sizes. We also extract the Q value for the resonators and study the corresponding dissipative processes. 	
0610080v2	http://arxiv.org/pdf/cond-mat/0610080v2	2007	Computer simulation of fatigue under diametrical compression	H. A. Carmona|F. Kun|J. S. Andrade Jr.|H. J. Herrmann	  We study the fatigue fracture of disordered materials by means of computer simulations of a discrete element model. We extend a two-dimensional fracture model to capture the microscopic mechanisms relevant for fatigue, and we simulate the diametric compression of a disc shape specimen under a constant external force. The model allows to follow the development of the fracture process on the macro- and micro-level varying the relative influence of the mechanisms of damage accumulation over the load history and healing of microcracks. As a specific example we consider recent experimental results on the fatigue fracture of asphalt. Our numerical simulations show that for intermediate applied loads the lifetime of the specimen presents a power law behavior. Under the effect of healing, more prominent for small loads compared to the tensile strength of the material, the lifetime of the sample increases and a fatigue limit emerges below which no macroscopic failure occurs. The numerical results are in a good qualitative agreement with the experimental findings. 	
0703069v1	http://arxiv.org/pdf/cond-mat/0703069v1	2007	A physics-based life prediction methodology for thermal barrier coating   systems	Esteban Busso|L. Wright|H. E. Evans|L. N. McCartney|S. R. J Saunders|S. Osgerby|J. Nunn	  A novel mechanistic approach is proposed for the prediction of the life of thermal barrier coating (TBC) systems. The life prediction methodology is based on a criterion linked directly to the dominant failure mechanism. It relies on a statistical treatment of the TBC's morphological characteristics, non-destructive stress measurements and on a continuum mechanics framework to quantify the stresses that promote the nucleation and growth of microcracks within the TBC. The last of these accounts for the effects of TBC constituents' elasto-visco-plastic properties, the stiffening of the ceramic due to sintering and the oxidation at the interface between the thermally insulating yttria stabilized zirconia (YSZ) layer and the metallic bond coat. The mechanistic approach is used to investigate the effects on TBC life of the properties and morphology of the top YSZ coating, metallic low-pressure plasma sprayed bond coat and the thermally grown oxide. Its calibration is based on TBC damage inferred from non-destructive fluorescence measurements using piezo-spectroscopy and on the numerically predicted local TBC stresses responsible for the initiation of such damage. The potential applicability of the methodology to other types of TBC coatings and thermal loading conditions is also discussed. 	
0404014v1	http://arxiv.org/pdf/cs/0404014v1	2004	A Modular and Fault-Tolerant Data Transport Framework	Timm M. Steinbeck	  The High Level Trigger (HLT) of the future ALICE heavy-ion experiment has to reduce its input data rate of up to 25 GB/s to at most 1.25 GB/s for output before the data is written to permanent storage. To cope with these data rates a large PC cluster system is being designed to scale to several 1000 nodes, connected by a fast network. For the software that will run on these nodes a flexible data transport and distribution software framework, described in this thesis, has been developed. The framework consists of a set of separate components, that can be connected via a common interface. This allows to construct different configurations for the HLT, that are even changeable at runtime. To ensure a fault-tolerant operation of the HLT, the framework includes a basic fail-over mechanism that allows to replace whole nodes after a failure. The mechanism will be further expanded in the future, utilizing the runtime reconnection feature of the framework's component interface. To connect cluster nodes a communication class library is used that abstracts from the actual network technology and protocol used to retain flexibility in the hardware choice. It contains already two working prototype versions for the TCP protocol as well as SCI network adapters. Extensions can be added to the library without modifications to other parts of the framework. Extensive tests and measurements have been performed with the framework. Their results as well as conclusions drawn from them are also presented in this thesis. Performance tests show very promising results for the system, indicating that it can fulfill ALICE's requirements concerning the data transport. 	
0405159v2	http://arxiv.org/pdf/hep-th/0405159v2	2004	Supersymmetric Unification Without Low Energy Supersymmetry And   Signatures for Fine-Tuning at the LHC	Nima Arkani-Hamed|Savas Dimopoulos	  The cosmological constant problem is a failure of naturalness and suggests that a fine-tuning mechanism is at work, which may also address the hierarchy problem. An example -- supported by Weinberg's successful prediction of the cosmological constant -- is the potentially vast landscape of vacua in string theory, where the existence of galaxies and atoms is promoted to a vacuum selection criterion. Then, low energy SUSY becomes unnecessary, and supersymmetry -- if present in the fundamental theory -- can be broken near the unification scale. All the scalars of the supersymmetric standard model become ultraheavy, except for a single finely tuned Higgs. Yet, the fermions of the supersymmetric standard model can remain light, protected by chiral symmetry, and account for the successful unification of gauge couplings. This framework removes all the difficulties of the SSM: the absence of a light Higgs and sparticles, dimension five proton decay, SUSY flavor and CP problems, and the cosmological gravitino and moduli problems. High-scale SUSY breaking raises the mass of the light Higgs to about 120-150 GeV. The gluino is strikingly long lived, and a measurement of its lifetime can determine the ultraheavy scalar mass scale. Measuring the four Yukawa couplings of the Higgs to the gauginos and higgsinos precisely tests for high-scale SUSY. These ideas, if confirmed, will demonstrate that supersymmetry is present but irrelevant for the hierarchy problem -- just as it has been irrelevant for the cosmological constant problem -- strongly suggesting the existence of a fine-tuning mechanism in nature. 	
0710.5236v1	http://arxiv.org/pdf/0710.5236v1	2007	Saturation Throughput Analysis of IEEE 802.11 in Presence of Non Ideal   Transmission Channel and Capture Effects	F. Daneshgaran|Massimiliano Laddomada|F. Mesiti|M. Mondin	  In this paper, we provide a saturation throughput analysis of the IEEE 802.11 protocol at the data link layer by including the impact of both transmission channel and capture effects in Rayleigh fading environment. Impacts of both non-ideal channel and capture effects, specially in an environment of high interference, become important in terms of the actual observed throughput. As far as the 4-way handshaking mechanism is concerned, we extend the multi-dimensional Markovian state transition model characterizing the behavior at the MAC layer by including transmission states that account for packet transmission failures due to errors caused by propagation through the channel. This way, any channel model characterizing the physical transmission medium can be accommodated, including AWGN and fading channels. We also extend the Markov model in order to consider the behavior of the contention window when employing the basic 2-way handshaking mechanism.   Under the usual assumptions regarding the traffic generated per node and independence of packet collisions, we solve for the stationary probabilities of the Markov chain and develop expressions for the saturation throughput as a function of the number of terminals, packet sizes, raw channel error rates, capture probability, and other key system parameters. The theoretical derivations are then compared to simulation results confirming the effectiveness of the proposed models. 	
0802.1366v1	http://arxiv.org/pdf/0802.1366v1	2008	Rise and fall of the old quantum theory	Manfred Bucher	  The old quantum theory of Bohr and Sommerfeld was abandonned for the wrong reason. Its contradictions were caused not by the orbit concept but by a mental barrier--the inconceivability that an electron might collide with the atomic nucleus. Removing that barrier resolves the theory's main failures--incorrect orbital momenta, He atom, H2+ molecule ion. The inclusion of electron oscillations through the nucleus--a concept called "Coulomb oscillator"--renders the old quantum theory consistent with quantum mechanics (although devoid of wave character). The triple success of the Bohr-Sommerfeld model is its correct description of the H atom (and one-electron ions) concerning (1) the energy levels Enl, (2) the orbital angular momenta Lnl--if corrected as Lnl^2 = l(l+1) hbar^2 and with the Coulomb oscillator included--and (3) the orbits' space quantization--with (Lnl)z = ml hbar. These achievements are succinctly represented by the principal, angular and magnetic quantum numbers (n, l, ml) and visualized by orbital ellipse geometry--major axis, vertex curvature, and tilt angle, respectively. Orbit geometry also accounts for the average orbit size. Moreover, the Coulomb oscillator provides a natural explanation of (isotropic) hyperfine interaction. The shortcomings of the old quantum theory lie in its neglect of three properties of particles--their spin, their wave nature and their quantum statistics. These deficiencies notwithstanding, the visual appeal of the Bohr-Sommerfeld model remains a pedagogical asset to complement the abstract character of quantum mechanics. 	
0805.3292v2	http://arxiv.org/pdf/0805.3292v2	2009	Spontaneous dissipation of elastic energy by self-localizing thermal   runaway	S. Braeck|Y. Y. Podladchikov|S. Medvedev	  Thermal runaway instability induced by material softening due to shear heating represents a potential mechanism for mechanical failure of viscoelastic solids. In this work we present a model based on a continuum formulation of a viscoelastic material with Arrhenius dependence of viscosity on temperature, and investigate the behavior of the thermal runaway phenomenon by analytical and numerical methods. Approximate analytical descriptions of the problem reveal that onset of thermal runaway instability is controlled by only two dimensionless combinations of physical parameters. Numerical simulations of the model independently verify these analytical results and allow a quantitative examination of the complete time evolutions of the shear stress and the spatial distributions of temperature and displacement during runaway instability. Thus we find that thermal runaway processes may well develop under nonadiabatic conditions. Moreover, nonadiabaticity of the unstable runaway mode leads to continuous and extreme localization of the strain and temperature profiles in space, demonstrating that the thermal runaway process can cause shear banding. Examples of time evolutions of the spatial distribution of the shear displacement between the interior of the shear band and the essentially nondeforming material outside are presented. Finally, a simple relation between evolution of shear stress, displacement, shear-band width and temperature rise during runaway instability is given. 	
0806.0593v1	http://arxiv.org/pdf/0806.0593v1	2008	Laws of crack motion and phase-field models of fracture	Vincent Hakim|Alain Karma	  Recently proposed phase-field models offer self-consistent descriptions of brittle fracture. Here, we analyze these theories in the quasistatic regime of crack propagation. We show how to derive the laws of crack motion either by using solvability conditions in a perturbative treatment for slight departure from the Griffith threshold, or by generalizing the Eshelby tensor to phase-field models. The analysis provides a simple physical interpretation of the second component of the classic Eshelby integral in the limit of vanishing crack propagation velocity: it gives the elastic torque on the crack tip that is needed to balance the Herring torque arising from the anisotropic interface energy. This force balance condition reduces in this limit to the principle of local symmetry in isotropic media and to the principle of maximum energy release rate for smooth curvilinear cracks in anisotropic media. It can also be interpreted physically in this limit based on energetic considerations in the traditional framework of continuum fracture mechanics, in support of its general validity for real systems beyond the scope of phase-field models. Analytical predictions of crack paths in anisotropic media are validated by numerical simulations. Simulations also show that these predictions hold even if the phase-field dynamics is modified to make the failure process irreversible. In addition, the role of dissipative forces on the process zone scale as well as the extension of the results to motion of planar cracks under pure antiplane shear are discussed. 	
0904.4426v1	http://arxiv.org/pdf/0904.4426v1	2009	Word-of-mouth and dynamical inhomogeneous markets: Efficiency measure   and optimal sampling policies for the pre-launch stage	Elena Agliari|Raffaella Burioni|Davide Cassi|Franco Maria Neri	  An important assumption lying behind innovation diffusion models and word-of-mouth processes is that of homogeneous mixing: at any time, the individuals making up the market are uniformly distributed in space. When the geographical parameters of the market, such as its area extension, become important, the movement of individuals must be explicitly taken into account. The authors introduce a model for a "micro-level" process for the diffusion of an innovative product, based on a word-of-mouth mechanism, and they explicitly consider the inhomogeneity of markets and the spatial extent of the geographical region where the process takes place. This results in an unexpected behaviour of macro (aggregate) level measurable quantities. The authors study the particular case of the pre-launch stage, where a product is first presented to the market through free sample distribution. The first triers of the samples then inform the other potential customers via word-of-mouth; additional advertising is absent. The authors find an unexpected general failure of the word-of-mouth mechanism for high market densities and they obtain quantitative results for the optimal sampling policy. By introducing a threshold to discriminate between individuals who will purchase and those who will not purchase according to their individual goodwill, they calculate the length of the pre-launch campaign and the final goodwill as a function of the firm's expenditure. These results are applied to a set of major US urban areas. 	
1004.0542v3	http://arxiv.org/pdf/1004.0542v3	2011	Cognitive Interference Management in Retransmission-Based Wireless   Networks	Marco Levorato|Urbashi Mitra|Michele Zorzi	  Cognitive radio methodologies have the potential to dramatically increase the throughput of wireless systems. Herein, control strategies which enable the superposition in time and frequency of primary and secondary user transmissions are explored in contrast to more traditional sensing approaches which only allow the secondary user to transmit when the primary user is idle. In this work, the optimal transmission policy for the secondary user when the primary user adopts a retransmission based error control scheme is investigated. The policy aims to maximize the secondary users' throughput, with a constraint on the throughput loss and failure probability of the primary user. Due to the constraint, the optimal policy is randomized, and determines how often the secondary user transmits according to the retransmission state of the packet being served by the primary user. The resulting optimal strategy of the secondary user is proven to have a unique structure. In particular, the optimal throughput is achieved by the secondary user by concentrating its transmission, and thus its interference to the primary user, in the first transmissions of a primary user packet. The rather simple framework considered in this paper highlights two fundamental aspects of cognitive networks that have not been covered so far: (i) the networking mechanisms implemented by the primary users (error control by means of retransmissions in the considered model) react to secondary users' activity; (ii) if networking mechanisms are considered, then their state must be taken into account when optimizing secondary users' strategy, i.e., a strategy based on a binary active/idle perception of the primary users' state is suboptimal. 	
1006.4741v1	http://arxiv.org/pdf/1006.4741v1	2010	Compression Behavior of Single-layer Graphene	Otakar Frank|Georgia Tsoukleri|John Parthenios|Konstantinos Papagelis|Ibtsam Riaz|Rashid Jalil|Kostya S. Novoselov|Costas Galiotis	  Central to most applications involving monolayer graphene is its mechanical response under various stress states. To date most of the work reported is of theoretical nature and refers to tension and compression loading of model graphene. Most of the experimental work is indeed limited to bending of single flakes in air and the stretching of flakes up to typically ~1% using plastic substrates. Recently we have shown that by employing a cantilever beam we can subject single graphene into various degrees of axial compression. Here we extend this work much further by measuring in detail both stress uptake and compression buckling strain in single flakes of different geometries. In all cases the mechanical response is monitored by simultaneous Raman measurements through the shift of either the G or 2D phonons of graphene. In spite of the infinitely small thickness of the monolayers, the results show that graphene embedded in plastic beams exhibit remarkable compression buckling strains. For large length (l)-to-width (w) ratios (> 0.2) the buckling strain is of the order of -0.5% to -0.6%. However, for l/w <0.2 no failure is observed for strains even higher than -1%. Calculations based on classical Euler analysis show that the buckling strain enhancement provided by the polymer lateral support is more than six orders of magnitude compared to suspended graphene in air. 	
1011.1400v1	http://arxiv.org/pdf/1011.1400v1	2010	Electrical transport through a mechanically gated molecular wire	C. Toher|R. Temirov|A. Greuling|F. Pump|M. Kaczmarski|M. Rohlfing|G. Cuniberti|F. S. Tautz	  A surface-adsorbed molecule is contacted with the tip of a scanning tunneling microscope (STM) at a pre-defined atom. On tip retraction, the molecule is peeled off the surface. During this experiment, a two-dimensional differential conductance map is measured on the plane spanned by the bias voltage and the tip-surface distance. The conductance map demonstrates that tip retraction leads to mechanical gating of the molecular wire in the STM junction. The experiments are compared with a detailed ab initio simulation. We find that density functional theory (DFT) in the local density approximation (LDA) describes the tip-molecule contact formation and the geometry of the molecular junction throughout the peeling process with predictive power. However, a DFT-LDA-based transport simulation following the non-equilibrium Green's functions (NEGF) formalism fails to describe the behavior of the differential conductance as found in experiment. Further analysis reveals that this failure is due to the mean-field description of electron correlation in the local density approximation. The results presented here are expected to be of general validity and show that, for a wide range of common wire configurations, simulations which go beyond the mean-field level are required to accurately describe current conduction through molecules. Finally, the results of the present study illustrate that well-controlled experiments and concurrent ab initio transport simulations that systematically sample a large configuration space of molecule-electrode couplings allow the unambiguous identification of correlation signatures in experiment. 	
1011.2578v2	http://arxiv.org/pdf/1011.2578v2	2011	Theoretical perspective on the glass transition and amorphous materials	Ludovic Berthier|Giulio Biroli	  We provide a theoretical perspective on the glass transition in molecular liquids at thermal equilibrium, on the spatially heterogeneous and aging dynamics of disordered materials, and on the rheology of soft glassy materials. We start with a broad introduction to the field and emphasize its connections with other subjects and its relevance. The important role played by computer simulations to study and understand the dynamics of systems close to the glass transition at the molecular level is spelled out. We review the recent progress on the subject of the spatially heterogeneous dynamics that characterizes structural relaxation in materials with slow dynamics. We then present the main theoretical approaches describing the glass transition in supercooled liquids, focusing on theories that have a microscopic, statistical mechanics basis. We describe both successes and failures, and critically assess the current status of each of these approaches. The physics of aging dynamics in disordered materials and the rheology of soft glassy materials are then discussed, and recent theoretical progress is described. For each section, we give an extensive overview of the most recent advances, but we also describe in some detail the important open problems that, we believe, will occupy a central place in this field in the coming years. 	
1112.5593v1	http://arxiv.org/pdf/1112.5593v1	2011	Extending and Evaluating and Novel Course Reform of introductory   Mechanics	Marcos D. Caballero	  The research presented in this thesis was motivated by the need to improve introductory physics courses. Introductory physics courses are generally the first courses in which students learn to create models to solve complex problems. However, many students taking introductory physics courses fail to acquire a command of the concepts, methods and tools presented in these courses. The reforms proposed by this thesis focus on altering the content of introductory courses rather than content delivery methods as most reforms do.   This thesis explores how the performance on a widely used test of conceptual understanding in mechanics compares between students taking a course with updated and modified content and students taking a traditional course. Better performance by traditional students was found to stem from their additional practice on the types of items which appeared on the test. The results of this work brought into question the role of the introductory physics course for non-majors.   One aspect of this new role is the teaching of new methods such as computation (the use of a computer to solve numerically, simulate and visualize physical problems). This thesis explores the potential benefits for students who learn computation as part of physics course. After students worked through a suite of computational homework problems, many were able to model a new physical situation with which they had no experience.   The failure of some students to model this new situation might have stemmed from their unfavorable attitudes towards learning computation. In this thesis, we present the development of a new tool for characterizing students' attitudes. Preliminary measurements indicated significant differences between successful and unsuccessful students. 	
1206.6808v1	http://arxiv.org/pdf/1206.6808v1	2012	A Multi-State Power Model for Adequacy Assessment of Distributed   Generation via Universal Generating Function	Yan-Fu Li|Enrico Zio	  The current and future developments of electric power systems are pushing the boundaries of reliability assessment to consider distribution networks with renewable generators. Given the stochastic features of these elements, most modeling approaches rely on Monte Carlo simulation. The computational costs associated to the simulation approach force to treating mostly small-sized systems, i.e. with a limited number of lumped components of a given renewable technology (e.g. wind or solar, etc.) whose behavior is described by a binary state, working or failed. In this paper, we propose an analytical multi-state modeling approach for the reliability assessment of distributed generation (DG). The approach allows looking to a number of diverse energy generation technologies distributed on the system. Multiple states are used to describe the randomness in the generation units, due to the stochastic nature of the generation sources and of the mechanical degradation/failure behavior of the generation systems. The universal generating function (UGF) technique is used for the individual component multi-state modeling. A multiplication-type composition operator is introduced to combine the UGFs for the mechanical degradation and renewable generation source states into the UGF of the renewable generator power output. The overall multi-state DG system UGF is then constructed and classical reliability indices (e.g. loss of load expectation (LOLE), expected energy not supplied (EENS)) are computed from the DG system generation and load UGFs. An application of the model is shown on a DG system adapted from the IEEE 34 nodes distribution test feeder. 	
1309.7429v1	http://arxiv.org/pdf/1309.7429v1	2013	Quorum Sensing for Regenerating Codes in Distributed Storage	Mit Sheth|Krishna Gopal Benerjee|Manish K. Gupta	  Distributed storage systems with replication are well known for storing large amount of data. A large number of replication is done in order to provide reliability. This makes the system expensive. Various methods have been proposed over time to reduce the degree of replication and yet provide same level of reliability. One recently suggested scheme is of Regenerating codes, where a file is divided in to parts which are then processed by a coding mechanism and network coding to provide large number of parts. These are stored at various nodes with more than one part at each node. These codes can generate whole file and can repair a failed node by contacting some out of total existing nodes. This property ensures reliability in case of node failure and uses clever replication. This also optimizes bandwidth usage. In a practical scenario, the original file will be read and updated many times. With every update, we will have to update the data stored at many nodes. Handling multiple requests at the same time will bring a lot of complexity. Reading and writing or multiple writing on the same data at the same time should also be prevented. In this paper, we propose an algorithm that manages and executes all the requests from the users which reduces the update complexity. We also try to keep an adequate amount of availability at the same time. We use a voting based mechanism and form read, write and repair quorums. We have also done probabilistic analysis of regenerating codes. 	
1402.4180v1	http://arxiv.org/pdf/1402.4180v1	2014	Effect of Deck Deterioration on Overall System Behavior, Resilience and   Remaining Life of Composite Steel Girder Bridges	Amir Gheitasi|Devin K. Harris	  During past few decades, several studies have been conducted to characterize the performance of in-service girder-type bridge superstructures under operating conditions. Few of these efforts have focused on evaluating the actual response of the bridge systems, especially beyond the elastic limit of their behavior, and correlating the impact of damage to the overall system behavior. In practice, most of the in-service bridge superstructures behave elastically under the routine daily traffic; however, existing damage and deteriorating conditions would significantly influence different aspects of the structural performance including reserve capacity, resilience and remaining service-life. The main purpose of this study is to evaluate the response of composite steel girder bridges under the effect of subsurface delamination in the reinforced concrete deck. Commercial finite element computer software, ANSYS, was implemented to perform a nonlinear analysis on a representative single-span simply supported bridge superstructure. The system failure characteristics were captured in the numerical models by incorporating different sources of material non-linearities including cracking/crushing in the concrete and plasticity in steel components. Upon validation, non-linear behavior of the system with both intact and degraded configurations was used to evaluate the impact of integrated damage mechanism on the overall system performance. Reserve capacity of this bridge superstructure was also determined with respect to the nominal element-level design capacity. As vision to the future path, this framework can be implemented to evaluate the performance of other in-service bridges degraded under the effect of different damage scenarios, thus providing a mechanism to determine a measure of capacity, resilience and remaining service-life. 	
1406.7547v1	http://arxiv.org/pdf/1406.7547v1	2014	Influence Process Structural Learning and the Emergence of Collective   Intelligence	James Hazy|Baran Curuklu	  Recent work [Hazy 2012] has demonstrated computationally that collectives that are organized into networks which govern the flow of resources can learn to recognize newly emerging opportunities distributed in the environment. This paper argues that the system does this through a process analogous to neural network learning with relative status playing the role of synaptic weights. Hazy showed computationally that learning of this type can occur even when resource allocation decision makers have no direct visibility into the environment, have no direct understanding of the opportunity, and are not involved in their exploitation except to the extent that they evaluate the success or failure of funded projects. Effectively, the system of interactions learns which individuals have the best access to information and other resources within the ecosystem. Hazy [2012] calls this previously unidentified emergence phenomenon: Influence Process Structural Learning (IPSL). In the prior model of IPSL, a three-tiered organizational structure was predetermined in the model design [Hazy 2012]. These initial conditions delimit the extent to which the emergence of collective intelligence can be posited because the model itself assumes a defined structure. This work contributes to the field by extending the IPSL argument for collective intelligence to a holistic emergence argument. It begins by briefly reviewing previously published work. It continues the conversation by adding two additional steps: Firstly, it shows how a three-tier organizing structure might emerge through known complexity mechanisms. In this case the mechanism identified is preferential attachment [Barabasi 2002]. Secondly, the paper shows how collective intelligence can emerge within a system of agents when the influence structure among these agents is treated as a the genetic algorithm. 	
1409.1998v1	http://arxiv.org/pdf/1409.1998v1	2014	Surface tension and the mechanics of liquid inclusions in compliant   solids	Robert W. Style|John S. Wettlaufer|Eric R. Dufresne	  Eshelby's theory of inclusions has wide-reaching implications across the mechanics of materials and structures including the theories of composites, fracture, and plasticity. However, it does not include the effects of surface stress, which has recently been shown to control many processes in soft materials such as gels, elastomers and biological tissue. To extend Eshelby's theory of inclusions to soft materials, we consider liquid inclusions within an isotropic, compressible, linear-elastic solid. We solve for the displacement and stress fields around individual stretched inclusions, accounting for the bulk elasticity of the solid and the surface tension (\textit{i.e.} isotropic strain-independent surface stress) of the solid-liquid interface. Surface tension significantly alters the inclusion's shape and stiffness as well as its near- and far-field stress fields. These phenomenon depend strongly on the ratio of inclusion radius, $R$, to an elastocapillary length, $L$. Surface tension is significant whenever inclusions are smaller than $100L$. While Eshelby theory predicts that liquid inclusions generically reduce the stiffness of an elastic solid, our results show that liquid inclusions can actually stiffen a solid when $R<3L/2$. Intriguingly, surface tension cloaks the far-field signature of liquid inclusions when $R=3L/2$. These results are have far-reaching applications from measuring local stresses in biological tissue, to determining the failure strength of soft composites. 	
1412.0780v1	http://arxiv.org/pdf/1412.0780v1	2014	Emergence of Anti-Cancer Drug Resistance: Exploring the Importance of   the Microenvironmental Niche via a Spatial Model	Jana L. Gevertz|Zahra Aminzare|Kerri-Ann Norton|Judith Perez-Velazquez|Alexandria Volkening|Katarzyna A. Rejniak	  Practically, all chemotherapeutic agents lead to drug resistance. Clinically, it is a challenge to determine whether resistance arises prior to, or as a result of, cancer therapy. Further, a number of different intracellular and microenvironmental factors have been correlated with the emergence of drug resistance. With the goal of better understanding drug resistance and its connection with the tumor microenvironment, we have developed a hybrid discrete-continuous mathematical model. In this model, cancer cells described through a particle-spring approach respond to dynamically changing oxygen and DNA damaging drug concentrations described through partial differential equations. We thoroughly explored the behavior of our self-calibrated model under the following common conditions: a fixed layout of the vasculature, an identical initial configuration of cancer cells, the same mechanism of drug action, and one mechanism of cellular response to the drug. We considered one set of simulations in which drug resistance existed prior to the start of treatment, and another set in which drug resistance is acquired in response to treatment. This allows us to compare how both kinds of resistance influence the spatial and temporal dynamics of the developing tumor, and its clonal diversity. We show that both pre-existing and acquired resistance can give rise to three biologically distinct parameter regimes: successful tumor eradication, reduced effectiveness of drug during the course of treatment (resistance), and complete treatment failure. 	
1501.03194v2	http://arxiv.org/pdf/1501.03194v2	2015	The cavity method for analysis of large-scale penalized regression	Mohammad Ramezanali|Partha P. Mitra|Anirvan M. Sengupta	  Penalized regression methods aim to retrieve reliable predictors among a large set of putative ones from a limited amount of measurements. In particular, penalized regression with singular penalty functions is important for sparse reconstruction algorithms. For large-scale problems, these algorithms exhibit sharp phase transition boundaries where sparse retrieval breaks down. Large optimization problems associated with sparse reconstruction have been analyzed in the literature by setting up corresponding statistical mechanical models at a finite temperature. Using replica method for mean field approximation, and subsequently taking a zero temperature limit, this approach reproduces the algorithmic phase transition boundaries. Unfortunately, the replica trick and the non-trivial zero temperature limit obscure the underlying reasons for the failure of a sparse reconstruction algorithm, and of penalized regression methods, in general. In this paper, we employ the ``cavity method'' to give an alternative derivation of the mean field equations, working directly in the zero-temperature limit. This derivation provides insight into the origin of the different terms in the self-consistency conditions. The cavity method naturally involves a quantity, the average local susceptibility, whose behavior distinguishes different phases in this system. This susceptibility can be generalized for analysis of a broader class of sparse reconstruction algorithms. 	
1504.04169v1	http://arxiv.org/pdf/1504.04169v1	2015	Fault Tolerant BFS Structures: A Reinforcement-Backup Tradeoff	Merav Parter|David Peleg	  This paper initiates the study of fault resilient network structures that mix two orthogonal protection mechanisms: (a) {\em backup}, namely, augmenting the structure with many (redundant) low-cost but fault-prone components, and (b) {\em reinforcement}, namely, acquiring high-cost but fault-resistant components. To study the trade-off between these two mechanisms in a concrete setting, we address the problem of designing a $(b,r)$ {\em fault-tolerant} BFS (or $(b,r)$ FT-BFS for short) structure, namely, a subgraph $H$ of the network $G$ consisting of two types of edges: a set $E' \subseteq E$ of $r(n)$ fault-resistant {\em reinforcement} edges, which are assumed to never fail, and a (larger) set $E(H) \setminus E'$ of $b(n)$ fault-prone {\em backup} edges, such that subsequent to the failure of a single fault-prone backup edge $e \in E \setminus E'$, the surviving part of $H$ still contains an BFS spanning tree for (the surviving part of) $G$, satisfying $dist(s,v,H\setminus \{e\}) \leq dist(s,v,G\setminus \{e\})$ for every $v \in V$ and $e \in E \setminus E'$. We establish the following tradeoff between $b(n)$ and $r(n)$: For every real $\epsilon \in (0,1]$, if $r(n) = {\tilde\Theta}(n^{1-\epsilon})$, then $b(n) = {\tilde\Theta}(n^{1+\epsilon})$ is necessary and sufficient. 	
1505.03066v3	http://arxiv.org/pdf/1505.03066v3	2016	Globally disruptive events show predictable timing patterns	Michael Gillman|Hilary Erenler	  Globally disruptive events include asteroid/comet impacts, large igneous provinces and glaciations, all of which have been considered as contributors to mass extinctions. Understanding the overall relationship between the timings of the largest extinctions and their potential proximal causes remains one of science's great unsolved mysteries. Cycles of about 60 million years in both fossil diversity and environmental data suggest external drivers such as the passage of the Solar System through the galactic plane. While cyclic phenomena are recognised statistically, a lack of coherent mechanisms and a failure to link key events has hampered wider acceptance of multi-million year periodicity and its relevance to earth science and evolution. The generation of a robust predictive model of timings, with a clear plausible primary mechanism, would signal a paradigm shift. Here, we present a model of the timings of globally disruptive events and a possible explanation of their ultimate cause. The proposed model is a symmetrical pattern of 63 million-year sequences around a central value, interpreted as the occurrence of events along, and parallel to, the galactic midplane. The symmetry is consistent with multiple dark matter disks, aligned parallel to the midplane. One implication of the precise pattern of timings and the underlying physical model is the ability to predict future events, such as a major extinction in one to two million years. 	
1505.04500v1	http://arxiv.org/pdf/1505.04500v1	2015	Unusual plastic deformation and damage features in Titanium:   experimental tests and constitutive modeling	Benoit Revil-Baudard|Oana Cazacu|Philip Flater|Nitin Chandola|J. L. Alves	  In this paper, we present an experimental study on plastic deformation and damage of polycrystalline pure Ti, as well as modeling of the observed behavior. From the mechanical characterization data, it can be concluded that the material displays anisotropy and tension-compression asymmetry. As concerns damage, the X-ray tomography measurements conducted reveal that damage distribution and evolution in this HCP Ti material is markedly different than in a typical FCC material such as copper. Stewart and Cazacu (2011) anisotropic elastic/plastic damage model is used to describe the behavior. All material parameters involved in this model have a clear physical significance, being related to plastic properties, and are determined based on very few simple mechanical tests. It is shown that this model predicts correctly the anisotropy in plastic deformation, and its strong influence on damage distribution and damage accumulation in Ti. Specifically, for a smooth axisymmetric specimen subject to uniaxial tension, damage initiates at the center of the specimen and is diffuse; the level of damage close to failure is very low. On the other hand, for a notched specimen subject to the same loading, the model predicts that damage initiates at the outer surface of the specimen, and further grows from the outer surface to the center of the specimen, which corroborates with the in-situ tomography data. 	
1506.03049v2	http://arxiv.org/pdf/1506.03049v2	2015	Formation and growth of shear bands in glasses: existence of an   underlying directed percolation transition	Gaurav Prakash Shrivastav|Pinaki Chaudhuri|Jürgen Horbach	  The response of glasses to mechanical loading often leads to the formation of inhomogeneous flow patterns that strongly affect materials properties. Among them, shear bands are ubiquitous in a wide variety of materials, ranging from soft matter systems to metallic alloys. Shear banding is associated with strain localization, i.e. the deformation of the sheared glassy solid is localized in space in form of band-like structures. These structures are often precursors to catastrophic failure, implying that a proper understanding of the underlying mechanisms could lead to the design of smarter materials. However, despite its importance in material science, the microscopic origin of shear banding in glassy materials is only poorly understood. Here, the formation of shear banding in glassy systems is revealed by non-equilibrium molecular dynamics simulations (NEMD) of a binary Lennard-Jones mixture, subject to a constant strain rate. In its glass state, this system exhibits for all considered strain rates the formation of a percolating cluster of mobile regions at a critical strain. We show that this percolation transition belongs to the universality class of directed percolation. Only at low shear rates, where the steady-state stress is close to the yielding threshold, the percolating cluster evolves into a transient (but long-lived) shear band with a diffusive growth of its width. 	
1506.03639v4	http://arxiv.org/pdf/1506.03639v4	2017	Mean-field description of plastic flow in amorphous solids	Jie Lin|Matthieu Wyart	  Failure and flow of amorphous materials are central to various phenomena including earthquakes and landslides. There is accumulating evidence that the yielding transition between a flowing and an arrested phase is a critical phenomenon, but the associated exponents are not understood, even at a mean-field level where the validity of popular models is debated. Here we solve a mean-field model that captures the broad distribution of the mechanical noise generated by plasticity, whose behavior is related to biased L\'evy flights near an absorbing boundary. We compute the exponent $\theta$ characterising the density of shear transformation $P(x)\sim x^{\theta}$, where $x$ is the stress increment beyond which they yield. We find that after an isotropic thermal quench, $\theta=1/2$. However, $\theta$ depends continuously on the applied shear stress, this dependence is not monotonic, and its value at the yield stress is not universal. The model rationalizes previously unexplained observations, and captures reasonably well the value of exponents in three dimensions. Values of exponents in four dimensions are accurately predicted. These results support that it is the true mean-field model that applies in large dimension, and raise fundamental questions on the nature of the yielding transition. 	
1509.06778v1	http://arxiv.org/pdf/1509.06778v1	2015	On the impact of Masking and Blocking Hypotheses for measuring efficacy   of new tuberculosis vaccines	Sergio Arregui|Joaquín Sanz|Dessislava Marinova|Carlos Martín|Yamir Moreno	  Over the past 60 years, the Mycobacterium bovis bacille Calmette-Gu\'erin (BCG) has been used worldwide to prevent tuberculosis (TB). However, BCG has shown a very variable efficacy in different trials, showing a wide range of protection in adults against pulmonary TB. Previous studies indicate that this failure is related to pre-existing immune response to antigens that are common to environmental sources of mycobacterial antigens and Mycobacterium tuberculosis. Specifically, two different mechanisms have been hypothesized: the masking, (previous sensitization confers some level of protection against TB), and the blocking (previous immune response prevent vaccine taking of a new TB vaccine), effects. In this work we introduce a series of models to discriminate between masking and blocking mechanisms and address their relative likelihood. The application of our models to interpret the results coming from the BCG-REVAC clinical trials, specifically designed for the study of sources of efficacy variability yields estimates that are consistent with high levels of blocking (41% in Manaus -95% C.I. [14%-68%]- and 96% in Salvador -95% C.I. [52%-100%]-), and no support for masking to play any relevant role in modifying vaccine efficacy either alone or aside blocking. The quantification of these effects around a plausible model constitutes a relevant step towards impact evaluation of novel anti-tuberculosis vaccines, which are susceptible of being affected by similar effects if applied on individuals previously exposed to mycobacterial antigens. 	
1512.06673v2	http://arxiv.org/pdf/1512.06673v2	2017	Photodissociation of ultracold diatomic strontium molecules with quantum   state control	M. McDonald|B. H. McGuyer|F. Apfelbeck|C. -H. Lee|I. Majewska|R. Moszynski|T. Zelevinsky	  Chemical reactions at ultracold temperatures are expected to be dominated by quantum mechanical effects. Although progress towards ultracold chemistry has been made through atomic photoassociation, Feshbach resonances and bimolecular collisions, these approaches have been limited by imperfect quantum state selectivity. In particular, attaining complete control of the ground or excited continuum quantum states has remained a challenge. Here we achieve this control using photodissociation, an approach that encodes a wealth of information in the angular distribution of outgoing fragments. By photodissociating ultracold 88Sr2 molecules with full control of the low-energy continuum, we access the quantum regime of ultracold chemistry, observing resonant and nonresonant barrier tunneling, matter-wave interference of reaction products and forbidden reaction pathways. Our results illustrate the failure of the traditional quasiclassical model of photodissociation and instead are accurately described by a quantum mechanical model. The experimental ability to produce well-defined quantum continuum states at low energies will enable high-precision studies of long-range molecular potentials for which accurate quantum chemistry models are unavailable, and may serve as a source of entangled states and coherent matter waves for a wide range of experiments in quantum optics. 	
1601.03130v1	http://arxiv.org/pdf/1601.03130v1	2016	A Micrometer-sized Heat Engine Operating Between Bacterial Reservoirs	Sudeesh Krishnamurthy|Subho Ghosh|Dipankar Chatterji|Rajesh Ganapathy|A. K. Sood	  Artificial micro heat engines are prototypical models to explore and elucidate the mechanisms of energy transduction in a regime that is dominated by fluctuations [1-2]. Micro heat engines realized hitherto mimicked their macroscopic counterparts and operated between reservoirs that were effectively thermal [3-7]. For such reservoirs, temperature is a well-defined state variable and stochastic thermodynamics provides a precise framework for quantifying engine performance [8-9]. It remains unclear whether these concepts readily carry over to situations where the reservoirs are out-of-equilibrium [10], a scenario of particular importance to the functioning of synthetic [11-12] and biological [13] micro engines and motors. Here we experimentally realized a micrometer-sized active Stirling engine by periodically cycling a colloidal particle in a time-varying harmonic optical potential across bacterial baths at different activities. Unlike in equilibrium thermal reservoirs, the displacement statistics of the trapped particle becomes increasingly non-Gaussian with activity. We show that as much as $\approx$ 85\% of the total power output and $\approx$ 50\% of the overall efficiency stems from large non-Gaussian particle displacements alone. Most remarkably, at the highest activities investigated, the efficiency of our quasi-static active heat engines surpasses the equilibrium saturation limit of Stirling efficiency - the maximum efficiency of a Stirling engine with the ratio of cold and hot reservoir temperatures ${T_C\over T_H} \to 0$. Crucially, the failure of effective temperature descriptions [14-16] for active reservoirs highlights the dire need for theories that can better capture the physics of micro motors and heat engines that operate in strongly non-thermal environments. 	
1604.06567v1	http://arxiv.org/pdf/1604.06567v1	2016	Concurrent Regenerating Codes and Scalable Application in Network   Storage	Huayu Zhang|Hui Li|Hanxu Hou|K. W. Shum|ShuoYen Robert Li	  To recover simultaneous multiple failures in erasure coded storage systems, Patrick Lee et al introduce concurrent repair based minimal storage regenerating codes to reduce repair traffic. The architecture of this approach is simpler and more practical than that of the cooperative mechanism in non-fully distributed environment, hence this paper unifies such class of regenerating codes as concurrent regenerating codes and further studies its characteristics by analyzing cut-based information flow graph in the multiple-node recovery model. We present a general storage-bandwidth tradeoff and give closed-form expressions for the points on the curve, including concurrent repair mechanism based on minimal bandwidth regenerating codes. We show that the general concurrent regenerating codes can be constructed by reforming the existing single-node regenerating codes or multiplenode cooperative regenerating codes. Moreover, a connection to strong-MDS is also analyzed. On the other respect, the application of RGC is hardly limited to "repairing". It is of great significance for "scaling", a scenario where we need to increase(decrease) nodes to upgrade(degrade) redundancy and reliability. Thus, by clarifying the similarities and differences, we integrate them into a unified model to adjust to the dynamic storage network. 	
1604.08041v1	http://arxiv.org/pdf/1604.08041v1	2016	Reducing DRAM Latency at Low Cost by Exploiting Heterogeneity	Donghyuk Lee	  In modern systems, DRAM-based main memory is significantly slower than the processor. Consequently, processors spend a long time waiting to access data from main memory, making the long main memory access latency one of the most critical bottlenecks to achieving high system performance. Unfortunately, the latency of DRAM has remained almost constant in the past decade. This is mainly because DRAM has been optimized for cost-per-bit, rather than access latency. As a result, DRAM latency is not reducing with technology scaling, and continues to be an important performance bottleneck in modern and future systems.   This dissertation seeks to achieve low latency DRAM-based memory systems at low cost in three major directions. First, based on the observation that long bitlines in DRAM are one of the dominant sources of DRAM latency, we propose a new DRAM architecture, Tiered-Latency DRAM (TL-DRAM), which divides the long bitline into two shorter segments using an isolation transistor, allowing one segment to be accessed with reduced latency. Second, we propose a fine-grained DRAM latency reduction mechanism, Adaptive-Latency DRAM, which optimizes DRAM latency for the common operating conditions for individual DRAM module. Third, we propose a new technique, Architectural-Variation-Aware DRAM (AVA-DRAM), which reduces DRAM latency at low cost, by profiling and identifying only the inherently slower regions in DRAM to dynamically determine the lowest latency DRAM can operate at without causing failures.   This dissertation provides a detailed analysis of DRAM latency by using both circuit-level simulation with a detailed DRAM model and FPGA-based profiling of real DRAM modules. Our latency analysis shows that our low latency DRAM mechanisms enable significant latency reductions, leading to large improvement in both system performance and energy efficiency. 	
1605.09678v1	http://arxiv.org/pdf/1605.09678v1	2016	Level Up Your Strategy: Towards a Descriptive Framework for Meaningful   Enterprise Gamification	Umar Ruhi	  Gamification initiatives are currently top-of-mind for many organizations seeking to engage their employees in creative ways, improve their productivity, and drive positive behavioural outcomes in their workforce - ultimately leading to positive business outcomes on the whole. Despite its touted benefits, little empirical research has been done to date to investigate technological and individual personal factors that determine the success or failure of enterprise gamification initiatives. In this article, we provide a summary of our preliminary research findings from three case studies of gamification initiatives across different business contexts and present an empirically validated descriptive framework that details the key success factors for enterprise gamification. Our adaptation of the mechanics, dynamics, and aesthetics (MDA) framework for enterprise gamification aims to explicate the connections between end-user motivations, interactive gameplay elements, and technology features and functions that constitute effective gamification interventions in the enterprise. Following a discussion of the core elements in the framework and their interrelationships, the implications of our research are presented in the form of guidelines for the management and design of gamification initiatives and applications. The research findings presented in this article can potentially aid in the development of game mechanics that translate into positive user experiences and foster higher levels of employee engagement. Additionally, our research findings provide insights on key success factors for the effective adoption and institutionalization of enterprise gamification initiatives in organizations, and subsequently help them enhance the performance of their employees and drive positive business outcomes. 	
1606.08779v2	http://arxiv.org/pdf/1606.08779v2	2016	Interface propagation in fiber bundles: Local, mean-field and   intermediate range-dependent statistics	Soumyajyoti Biswas|Lucas Goehring	  The fiber bundle model is essentially an array of elements that break when sufficient load is applied on them. With a local loading mechanism, this can serve as a model for a one-dimensional interface separating the broken and unbroken parts of a solid in mode-I fracture. The interface can propagate through the system depending on the loading rate and disorder present in the failure thresholds of the fibers. In the presence of a quasi-static drive, the intermittent dynamics of the interface mimic front propagation in disordered media. Such situations appear in diverse physical systems such as mode-I crack propagation, domain wall dynamics in magnets, charge density waves, contact line in wetting etc. We study the effect of the range of interaction, i.e. the neighborhood of the interface affected following a local perturbation, on the statistics of the intermittent dynamics of the front. There exists a crossover from local to global behavior as the range of interaction grows and a continuously varying `universality' in the intermediate range. This means that the interaction range is a relevant parameter of any resulting physics. This is particularly relevant in view of the fact that there is a scatter in the experimental observations of the exponents, in even idealized experiments on fracture fronts, and also a possibility in changing the interaction range in real samples. 	
1607.05699v1	http://arxiv.org/pdf/1607.05699v1	2016	Stochastic Epidemic Networks with Strategic Link Formation	Jie Xu	  Understanding cascading failures or epidemics in networks is crucial for developing effective defensive mechanisms for many critical systems and infrastructures (e.g. biological, social and cyber networks). Most of the existing works treat the network topology as being exogenously given and study under what conditions an epidemic breaks out and/or extinguishes. However, if agents are able to strategically decide their connections according to their own self-interest, the network will instead be endogenously formed and evolving. In such systems, the epidemic, agents' strategic decisions and the network structure become complexly coupled and co-evolve. As a result, existing knowledge may no longer be applicable. Built on a continuous time Susceptible-Infected-Susceptible epidemic model with strong mixing, this paper studies stochastic epidemic networks consisting of strategic agents, who decide the number of links to form based on a careful evaluation of its current obtainable benefit and the potential future cost due to infection by forming links. A game theoretical framework is developed to analyze such networks and a number of important insights are obtained. One key result is that whereas an epidemic eventually dies out if the effective spreading rate is sufficiently low in exogenously given networks, it never dies out when agents are strategic regardless of the effective spreading rate. This property leads to reduced achievable system efficiency and considerably different optimal protection mechanisms. Without understanding the strategic behavior of agents, significant security cost may incur. 	
1610.08301v1	http://arxiv.org/pdf/1610.08301v1	2016	Characterization of Surface Deformation Behavior, Mechanical and   Physical Properties of Modified-clay Bricks	David N Githinji|Charles K Nzila|John T Githaiga|David R Tuigong|Albert O Osiemo|Peter O Ayoro	  The demand for building material is ever increasing owing to population growth. Compacted clay bricks are an important integral building material especially for low cost durable and affordable housing segment. This is a valued building material since its properties can be modified to suit various loading conditions. In this paper, the mechanical and physical properties of clay bricks modified with varying proportions of sawdust and polystyrene are determined. Increment of non-clay material proportion in the modified-clay bricks increases their porosity and water absorbency while their bulk densities, compressive and flexural strengths decreases. The use is made of Particle Image Velocimetry (PIV) method to assess the surface deformation behavior of the modified-clay bricks under uniaxial compressive loading. The distribution of surface deformation as assessed through PIV method is relatively uniform in pure-clay bricks while modified-clay bricks indicates a non-uniform deformation localized near the loading point at low strains. The strain distribution progressively spread out in the modifiedclay brick as the failure point is approached. 	
1706.01560v1	http://arxiv.org/pdf/1706.01560v1	2017	Stateless Puzzles for Real Time Online Fraud Preemption	Mizanur Rahman|Ruben Recabarren|Bogdan Carbunar|Dongwon Lee	  The profitability of fraud in online systems such as app markets and social networks marks the failure of existing defense mechanisms. In this paper, we propose FraudSys, a real-time fraud preemption approach that imposes Bitcoin-inspired computational puzzles on the devices that post online system activities, such as reviews and likes. We introduce and leverage several novel concepts that include (i) stateless, verifiable computational puzzles, that impose minimal performance overhead, but enable the efficient verification of their authenticity, (ii) a real-time, graph-based solution to assign fraud scores to user activities, and (iii) mechanisms to dynamically adjust puzzle difficulty levels based on fraud scores and the computational capabilities of devices. FraudSys does not alter the experience of users in online systems, but delays fraudulent actions and consumes significant computational resources of the fraudsters. Using real datasets from Google Play and Facebook, we demonstrate the feasibility of FraudSys by showing that the devices of honest users are minimally impacted, while fraudster controlled devices receive daily computational penalties of up to 3,079 hours. In addition, we show that with FraudSys, fraud does not pay off, as a user equipped with mining hardware (e.g., AntMiner S7) will earn less than half through fraud than from honest Bitcoin mining. 	
1709.05228v1	http://arxiv.org/pdf/1709.05228v1	2017	Spontaneous surface reserve formation in wicked membranes bestow extreme   stretchability	Paul Grandgeorge|Natacha Krins|Aurélie Hourlier-Fargette|Christel Laberty|Sébastien Neukirch|Arnaud Antkowiak	  Soft stretchable materials are key for arising technologies such as stretchable electronics or batteries, smart textiles, biomedical devices, tissue engineering and soft robotics. Recent attempts to design such materials, via e.g. micro-patterning of wavy fibres on soft substrates, polymer engineering at the molecular level or even kirigami techniques, provide appealing prospects but suffer drawbacks impacting the material viability: complexity of manufacturing, fatigue or failure upon cycling, restricted range of materials or biological incompatibility. Here, we report a universal strategy to design highly stretchable, self-assembling and fatigue-resistant synthetic fabrics. Our approach finds its inspiration in the mechanics of living animal cells that routinely encounter and cope with extreme deformations, e.g. with the engulfment of large intruders by macrophages, squeezing and stretching of immune cells in tiny capillaries or shrinking/swelling of neurons upon osmotic stimuli. All these large instant deformations are actually mediated and buffered by membrane reserves available in the form of microvilli, membrane folds or endomembrane that can be recruited on demand. We synthetically mimicked this behavior by creating nanofibrous liquid-infused tissues spontaneously forming surface reserves whose unfolding fuels any imposed shape change. Our process, relying only on geometry, elasticity and capillarity, allows to endow virtually any material with high stretchability and reversibility, making it straightforward to implement additional mechanical, electrical or chemical functions. We illustrate this with proof-of-concept activable capillary muscles, adaptable slippery liquid infused porous surfaces and stretchable basic printed electronic circuits. 	
1709.08269v1	http://arxiv.org/pdf/1709.08269v1	2017	Particle Acceleration and Fractional Transport in Turbulent Reconnection	Heinz Isliker|Theophilos Pisokas|Loukas Vlahos|Anastasios Anastasiadis	  We consider a large scale environment of turbulent reconnection that is fragmented into a number of randomly distributed Unstable Current Sheets (UCS), and we statistically analyze the acceleration of particles within this environment. We address two important cases of acceleration mechanisms when the particles interact with the UCS: (a) electric field acceleration, and (b) acceleration through reflection at contracting islands. Electrons and ions are accelerated very efficiently, attaining an energy distribution of power-law shape with an index $1-2$, depending on the acceleration mechanism. The transport coefficients in energy space are estimated from the test-particle simulation data, and we show that the classical Fokker-Planck (FP) equation fails to reproduce the simulation results when the transport coefficients are inserted into it and it is solved numerically. The cause for this failure is that the particles perform Levy flights in energy space, the distributions of energy increments exhibit power-law tails. We then use the fractional transport equation (FTE) derived by Isliker et al., 2017, whose parameters and the order of the fractional derivatives are inferred from the simulation data, and, solving the FTE numerically, we show that the FTE successfully reproduces the kinetic energy distribution of the test-particles. We discuss in detail the analysis of the simulation data and the criteria that allow judging the appropriateness of either an FTE or a classical FP equation as a transport model. 	
1710.01996v1	http://arxiv.org/pdf/1710.01996v1	2017	Network approach towards understanding the crazing in glassy amorphous   polymers	Sudarkodi Venkatesan|R. P. Vivek-Ananth|R. P. Sreejith|P. Mangalapandi|Ali A. Hassanali|Areejit Samal	  We have used molecular dynamics to simulate an amorphous glassy polymer with long chains to study deformation mechanism of crazing and associated void statistics. The Van der Waals interactions and the entanglements between chains constituting the polymer play a crucial role in crazing. Thus, we have reconstructed two underlying weighted networks, namely, the Van der Waals network and the Entanglement network from polymer configurations extracted from the molecular dynamics simulation. Subsequently, we have performed graph-theoretic analysis of the two reconstructed networks to reveal the role played by them in crazing of polymers. Our analysis captured various stages of crazing through specific trends in the network measures for Van der Waals networks and entanglement networks. To further corroborate the effectiveness of network analysis in unraveling the underlying physics of crazing in polymers, we have contrasted the trends in network measures for Van der Waals networks and entanglement networks in the light of stress-strain behaviour and voids statistics during deformation. We find that Van der Waals network plays a crucial role in craze initiation and growth. Although, the entanglement network was found to maintain its structure during craze initiation stage, it was found to progressively weaken and undergo dynamic changes during the hardening and failure stages of crazing phenomena. Our work demonstrates the utility of network theory in quantifying the underlying physics of polymer crazing and widens the scope of applications of network science to characterization of deformation mechanisms in diverse polymers. 	
1801.05378v1	http://arxiv.org/pdf/1801.05378v1	2018	Quasi-static and Dynamic Behavior of Additively Manufactured Metallic   Lattice Cylinders	Hossein Sadeghi|Dhruv Bhate|Joseph Abraham|Joseph Magallanes	  Lattice structures have tailorable mechanical properties which allows them to exhibit superior mechanical properties (per unit weight) beyond what is achievable through natural materials. In this paper, quasi-static and dynamic behavior of additively manufactured stainless steel lattice cylinders is studied. Cylindrical samples with internal lattice structure are fabricated by a laser powder bed fusion system. Equivalent hollow cylindrical samples with the same length, outer diameter, and mass (larger wall thickness) are also fabricated. Split Hopkinson bar is used to study the behavior of the specimens under high strain rate loading. It is observed that lattice cylinders reduce the transmitted wave amplitude up to about 21% compared to their equivalent hollow cylinders. However, the lower transmitted wave energy in lattice cylinders comes at the expense of a greater reduction in their stiffness, when compared to their equivalent hollow cylinder. In addition, it is observed that increasing the loading rate by five orders of magnitude leads to up to about 36% increase in the peak force that the lattice cylinder can carry, which is attributed to strain rate hardening effect in the bulk stainless steel material. Finite element simulations of the specimens under dynamic loads are performed to study the effect of strain rate hardening, thermal softening, and the failure mode on dynamic behavior of the specimens. Numerical results are compared with experimental data and good qualitative agreement is observed. 	
1003.4929v2	http://arxiv.org/pdf/1003.4929v2	2010	Dynamics of the contact between a ruthenium surface with a single   nanoasperity and a flat ruthenium surface: Molecular dynamics simulations	Alan Barros de Oliveira|Andrea Fortini|Sergey V. Buldyrev|David Srolovitz	  We study the dynamics of the contact between a pair of surfaces (with properties designed to mimic ruthenium) via molecular dynamics simulations. In particular, we study the contact between a ruthenium surface with a single nanoasperity and a flat ruthenium surface. The results of such simulations suggest that contact behavior is highly variable. The goal of this study is to investigate the source and degree of this variability. We find that during compression, the behavior of the contact force displacement curves is reproducible, while during contact separation, the behavior is highly variable. Examination of the contact surfaces suggest that two separation mechanism are in operation and give rise to this variability. One mechanism corresponds to the formation of a bridge between the two surfaces that plastically stretches as the surfaces are drawn apart and eventually separates in shear. This leads to a morphology after separation in which there are opposing asperities on the two surfaces. This plastic separation/bridge formation mechanism leads to a large work of separation. The other mechanism is a more brittle-like mode in which a crack propagates across the base of the asperity (slightly below the asperity/substrate junction) leading to most of the asperity on one surface or the other after separation and a slight depression facing this asperity on the opposing surface. This failure mode corresponds to a smaller work of separation. those in which a single mechanism operates. Furthermore, contacts made from materials that exhibit predominantly brittle-like behavior will tend to require lower work of separation than those made from ductile-like contact materials. 	
1610.09604v2	http://arxiv.org/pdf/1610.09604v2	2017	Understanding and Exploiting Design-Induced Latency Variation in Modern   DRAM Chips	Donghyuk Lee|Samira Khan|Lavanya Subramanian|Saugata Ghose|Rachata Ausavarungnirun|Gennady Pekhimenko|Vivek Seshadri|Onur Mutlu	  Variation has been shown to exist across the cells within a modern DRAM chip. We empirically demonstrate a new form of variation that exists within a real DRAM chip, induced by the design and placement of different components in the DRAM chip. Our goals are to understand design-induced variation that exists in real, state-of-the-art DRAM chips, exploit it to develop low-cost mechanisms that can dynamically find and use the lowest latency at which to operate a DRAM chip reliably, and, thus, improve overall system performance while ensuring reliable system operation.   To this end, we first experimentally demonstrate and analyze designed-induced variation in modern DRAM devices by testing and characterizing 96 DIMMs (768 DRAM chips). Our characterization identifies DRAM regions that are vulnerable to errors, if operated at lower latency, and finds consistency in their locations across a given DRAM chip generation, due to design-induced variation. Based on our extensive experimental analysis, we develop two mechanisms that reliably reduce DRAM latency. First, DIVA Profiling uses runtime profiling to dynamically identify the lowest DRAM latency that does not introduce failures. DIVA Profiling exploits design-induced variation and periodically profiles only the vulnerable regions to determine the lowest DRAM latency at low cost. Our second mechanism, DIVA Shuffling, shuffles data such that values stored in vulnerable regions are mapped to multiple error-correcting code (ECC) codewords. Combined together, our two mechanisms reduce read/write latency by 40.0%/60.5%, which translates to an overall system performance improvement of 14.7%/13.7%/13.8% (in 2-/4-/8-core systems) across a variety of workloads, while ensuring reliable operation. 	
0212104v2	http://arxiv.org/pdf/physics/0212104v2	2003	Gamma radiation measurements and dose rates in commercially-used natural   tiling rocks (granites)	M. Tzortzis|H. Tsertos|S. Christofides|G. Christodoulides	  The gamma radiation in samples of a variety of natural tiling rocks (granites) imported in Cyprus for use in the building industry was measured, employing high-resolution gamma-ray spectroscopy. The rock samples were pulverized, sealed in 1 litre plastic Marinelli beakers, and measured in the laboratory with an accumulating time between 10 and 14 hours each. From the measured gamma-ray spectra, activity concentrations were determined for Th-232 (range from 1 to 906 Bq/kg), U-238 (from 1 to 588 Bq/kg) and K-40 (from 50 to 1606 Bq/kg). The total absorbed dose rates in air calculated from the concentrations of the three radionuclides, Th-232 and U-238 series and K-40, ranged from 7 to 1209 nGy/h for full utilization of the materials, from 4 to 605 nGy/h for half utilization and from 2 to 302 nGy/h for one quarter utilization. The total effective dose rates per person indoors were determined to be between 0.02 to 2.97 mSv/y for half utilization of the materials. Applying dose criteria recently recommended by the EU for superficial materials, 25 of the samples meet the exemption dose limit of 0.3 mSv/y, two of them meet the upper dose limit of 1 mSv/y and only one exceeds clearly this limit. 	
0710.4186v1	http://arxiv.org/pdf/0710.4186v1	2007	Reflection of microwave from energy deposit by X-ray irradiation in rock   salt: Implication of an ultra high energy salt neutrino detector to act like   a radio bubble chamber	Masami Chiba|Yoko Arakawa|Toshio Kamijo|Shunsuke Nakamura|Yuji Shibasaki|Yasuhiro Takayama|Yusuke Watanabe|Fumiaki Yabuki|Osamu Yasuda|Akio Amano|Yuichi Chikashige|Keisuke Ibe|Tadashi Kon|Sosuke Ninomiya|Yutaka Shimizu|Yoshito Takeoka|Yasuyuki Taniuchi|Michiaki Utsumi|Masatoshi Fujii	  Existence of GZK neutrinos (ultra high energy neutrinos) have been justified although the flux is very low. A new method is desired to use a huge mass of a detector medium to detect them. A fundamental study of radar method was carried out to measure microwave reflection from electromagnetic energy deposit by X-ray irradiation in a small rock salt sample. The reflection rate of 1x10^-6 was found at the energy deposit of 1x10^19 eV which was proportional to square of the X-ray intensity suggesting the effect to be coherent scattering. The decay time of the reflection was several seconds. This effect implies a large scale natural rock salt formation could be utilized like a bubble chamber irradiated by radio wave instead of visible light to detect GZK neutrinos. 	
1109.3148v1	http://arxiv.org/pdf/1109.3148v1	2011	Use of groundwater lifetime expectancy for the performance assessment of   a deep geologic radioactive waste repository:2. Application to a Canadian   Shield environment	Y. -J. Park|F. J. Cornaton|S. D. Normani|J. F. Sykes|E. A. Sudicky	  Cornaton et al. [2007] introduced the concept of lifetime expectancy as a performance measure of the safety of subsurface repositories, based upon the travel time for contaminants released at a certain point in the subsurface to reach the biosphere or compliance area. The methodologies are applied to a hypothetical but realistic Canadian Shield crystalline rock environment, which is considered to be one of the most geologically stable areas on Earth. In an approximately 10\times10\times1.5 km3 hypothetical study area, up to 1000 major and intermediate fracture zones are generated from surface lineament analyses and subsurface surveys. In the study area, mean and probability density of lifetime expectancy are analyzed with realistic geologic and hydrologic shield settings in order to demonstrate the applicability of the theory and the numerical model for optimally locating a deep subsurface repository for the safe storage of spent nuclear fuel. The results demonstrate that, in general, groundwater lifetime expectancy increases with depth and it is greatest inside major matrix blocks. Various sources and aspects of uncertainty are considered, specifically geometric and hydraulic parameters of permeable fracture zones. Sensitivity analyses indicate that the existence and location of permeable fracture zones and the relationship between fracture zone permeability and depth from ground surface are the most significant factors for lifetime expectancy distribution in such a crystalline rock environment. As a consequence, it is successfully demonstrated that the concept of lifetime expectancy can be applied to siting and performance assessment studies for deep geologic repositories in crystalline fractured rock settings. 	
1204.0980v2	http://arxiv.org/pdf/1204.0980v2	2012	A New Disintegrative Capture Theory for the Origin of the Moon	Peter D. Noerdlinger	  The object that resulted in the creation of the Moon started in the same orbital path as Earth around the Sun, but at Earth's L4. This proto-Moon (PM) was 4 times less massive than the usual Giant Impact (GI) object "Theia" and was captured into Earth orbit. It had a 32% Iron-Nickel-Sulfur core supporting a dynamo, which explains magnetized lunar rocks. Following capture, it was torn apart by tidal forces and its core of iron plastered itself, with some of its rock mantle, on the surface of Earth at a very flat angle (producing the "Late Veneer"). After tidal stripping, the remaining PM rock was driven away from Earth to about 3.8 times Earth's radius and formed into what is now the Moon. The GI theory has several troubles: The violent collision melts the entire Earth, contrary to geological evidence. The Moon itself also has to condense out of the vapor cloud generated in the collision, but there is evidence that the Moon was not condensed out of vapor. In the new theory, the Moon as we know it may be only 3.8 - 3.9 billion years old, not 4.56 as usually assumed. That is the age of the PM. The minerals in the Moon would be about as old as the Earth, but would have been re-arranged in the capture and temporary disintegration process. If the Moon is as young as suggested, its origin would coincide with the beginning of life on Earth, which is unexplained in the GI theory. The manuscript asks, "Was the Moon Turned Inside-Out" and the answer is "Essentially, Yes." 	
1211.6992v1	http://arxiv.org/pdf/1211.6992v1	2012	Influence of the photonuclear effect on electron-neutrino-induced   electromagnetic cascades under the Landau-Pomeranchuk-Migdal regime in   standard rock	Mathieu Tartare|Didier Lebrun|François Montanet	  The observation of earth skimming neutrinos has been proposed as a rather sensitive method to detect ultra-high energy (UHE) cosmic neutrinos. Energetic cosmic neutrinos can interact inside the rock and produce leptons via a charged current interaction. In the case of an incoming electron neutrino undergoing a charged current interaction, the produced UHE electron will induce an underground electromagnetic shower. At high energy (above 7.7 TeV in standard rock), such showers are subject to LPM (Landau, Pomeranchuk and Migdal) suppression of the radiative processes cross sections (bremsstrahlung and pair production). The consequence of this suppression is that showers are elongated. This effect will increase the detection probability of such events allowing deeper showers to emerge with detectable energies. On the other hand, the photonuclear processes which are usually neglected in electromagnetic showers with respect to radiative processes, turn out to become dominant in the LPM regime and will reduce the shower length. In this work, we have performed a complete Monte Carlo study of an underground shower induced by UHE electrons by taking into account both the LPM suppression and the photonuclear interaction. We will discuss the effects of both of these processes on the shower length and on the detectability of such events by ground arrays or fluorescence telescopes. We show that limits on neutrino fluxes that were obtained using simulations that were obviously neglecting photonuclear processes are overoptimistic and should be corrected. 	
1303.5347v1	http://arxiv.org/pdf/1303.5347v1	2013	Evolution of seismic velocities in heavy oil sand reservoirs during   thermal recovery process	Jean-François Nauroy|Dinh Hong Doan|N. Guy|Axelle Baroni|Pierre Delage|Marc Mainguy	  In thermally enhanced recovery processes like cyclic steam stimulation (CSS) or steam assisted gravity drainage (SAGD), continuous steam injection entails changes in pore fluid, pore pressure and temperature in the rock reservoir, that are most often unconsolidated or weakly consolidated sandstones. This in turn increases or decreases the effective stresses and changes the elastic properties of the rocks. Thermally enhanced recovery processes give rise to complex couplings. Numerical simulations have been carried out on a case study so as to provide an estimation of the evolution of pressure, temperature, pore fluid saturation, stress and strain in any zone located around the injector and producer wells. The approach of Ciz and Shapiro (2007) - an extension of the poroelastic theory of Biot-Gassmann applied to rock filled elastic material - has been used to model the velocity dispersion in the oil sand mass under different conditions of temperature and stress. A good agreement has been found between these predictions and some laboratory velocity measurements carried out on samples of Canadian oil sand. Results appear to be useful to better interpret 4D seismic data in order to locate the steam chamber. 	
1404.6366v2	http://arxiv.org/pdf/1404.6366v2	2014	Pipe Poiseuille flow of viscously anisotropic, partially molten rock	Jane Allwright|Richard F Katz	  Laboratory experiments in which synthetic, partially molten rock is subjected to forced deformation provide a context for testing hypotheses about the dynamics and rheology of the mantle. Here our hypothesis is that the aggregate viscosity of partially molten mantle is anisotropic, and that this anisotropy arises from deviatoric stresses in the rock matrix. We formulate a model of pipe Poiseuille flow based on theory by Takei and Holtzman [2009a] and Takei and Katz [2013]. Pipe Poiseuille is a configuration that is accessible to laboratory experimentation but for which there are no published results. We analyse the model system through linearised analysis and numerical simulations. This analysis predicts two modes of melt segregation: migration of melt from the centre of the pipe toward the wall and localisation of melt into high-porosity bands that emerge near the wall, at a low angle to the shear plane. We compare our results to those of Takei and Katz [2013] for plane Poiseuille flow; we also describe a new approximation of radially varying anisotropy that improves the self-consistency of models over those of Takei and Katz [2013]. This study provides a set of baseline, quantitative predictions to compare with future laboratory experiments on forced pipe Poiseuille flow of partially molten mantle. 	
1407.7271v1	http://arxiv.org/pdf/1407.7271v1	2014	From stellar nebula to planetesimals	Ulysse Marboeuf|Amaury Thiabaud|Yann Alibert|Nahuel Cabral|Willy Benz	  Solar and extrasolar comets and extrasolar planets are the subject of numerous studies in order to determine their chemical composition and internal structure. In the case of planetesimals, their compositions are important as they govern in part the composition of future planets. The present works aims at determining the chemical composition of icy planetesimals, believed to be similar to present day comets, formed in stellar systems of solar chemical composition. The main objective of this work is to provide valuable theoretical data on chemical composition for models of planetesimals and comets, and models of planet formation and evolution. We have developed a model that calculates the composition of ices formed during the cooling of the stellar nebula. Coupled with a model of refractory element formation, it allows us to determine the chemical composition and mass ratio of ices to rocks in icy planetesimals throughout in the protoplanetary disc. We provide relationships for ice line positions (for different volatile species) in the disc, and chemical compositions and mass ratios of ice relative to rock for icy planetesimals in stellar systems of solar chemical composition. From an initial homogeneous composition of the nebula, a wide variety of chemical compositions of planetesimals were produced as a function of the mass of the disc and distance to the star. Ices incorporated in planetesimals are mainly composed of H2O, CO, CO2, CH3OH, and NH3. The ice/rock mass ratio is equal to 1+-0.5 in icy planetesimals following assumptions. This last value is in good agreement with observations of solar system comets, but remains lower than usual assumptions made in planet formation models, taking this ratio to be of 2-3. 	
1503.08708v1	http://arxiv.org/pdf/1503.08708v1	2015	Thulium anomalies and rare earth element patterns in meteorites and   Earth: Nebular fractionation and the nugget effect	N. Dauphas|A. Pourmand	  This study reports the bulk rare earth element (REEs, La-Lu) compositions of 41 chondrites, including 32 falls and 9 finds from carbonaceous (CI, CM, CO and CV), enstatite (EH and EL) and ordinary (H, L and LL) groups, as well as 2 enstatite achondrites (aubrite). The CI-chondrite-normalized REE patterns and Eu anomalies in ordinary and enstatite chondrites show more scatter in more metamorphosed than in unequilibrated chondrites. This is due to parent-body redistribution of the REEs in various carrier phases during metamorphism. The dispersion in REE patterns of equilibrated ordinary chondrites is explained by the nugget effect associated with concentration of REEs in minor phosphate grains.   Terrestrial rocks and samples from ordinary and enstatite chondrites display negative Tm anomalies of ~-4.5 % relative to ca chondrites. In contrast, CM, CO and CV (except Allende) show no significant Tm anomalies. Allende CV chondrite shows large excess Tm (~+10 %). These anomalies are similar to those found in group II refractory inclusions in meteorites but of much smaller magnitude. The presence of Tm anomalies in meteorites and terrestrial rocks suggests that either (i) the material in the inner part of the solar system was formed from a gas reservoir that had been depleted in refractory dust and carried positive Tm anomalies or (ii) CI chondrites are enriched in refractory dust and are not representative of solar composition for refractory elements. The observed Tm anomalies in ordinary and enstatite chondrites and terrestrial rocks, relative to carbonaceous chondrites, indicate that material akin to carbonaceous chondrites must have represented a small fraction of the constituents of the Earth. 	
1504.03389v3	http://arxiv.org/pdf/1504.03389v3	2015	Robust and efficient estimation of high dimensional scatter and location	Ricardo A. Maronna|Victor J. Yohai	  We deal with the equivariant estimation of scatter and location for p-dimensional data, giving emphasis to scatter. It it important that the estimators possess both a high efficiency for normal data and a high resistance to outliers, that is, a low bias under contamination. The most frequently employed estimators are not quite satisfactory in this respect. The Minimum Volume Ellipsoid (MVE) and Minimum Covariance Determinant (MCD) estimators are known to have a very low efficiency. S-Estimators (Davies 1987) with a monotonic weight function like the bisquare behave satisfactorily for "small" p, say p not larger than 10. Rocke (1996) showed that their efficiency tends to one with increasing p. Unfortunately, this advantage is paid with a serious loss of robustness for large p. We consider three families of estimators with controllable efficiencies: non-monotonic S-estimators (Rocke 1996), MM-estimators (Tatsuoka and Tyler 2000) and tau-estimators (Lopuhaa 1991), whose performance for large p has not been explored to date. Two types of starting estimators are employed: the MVE computed through subsampling, and a semi-deterministic procedure proposed by Pe\~na and Prieto (2007) for outlier detection. A simulation study shows that the Rocke and MM estimators starting from the Pe\~na-Prieto estimator and with an adequate tuning, can simultaneously attain high efficiency and high robustness. 	
1510.07223v1	http://arxiv.org/pdf/1510.07223v1	2015	A 1-D evolutionary model for icy satellites, applied to Enceladus	Uri Malamud|Dina Prialnik	  We develop a long-term 1-D evolution model for icy satellites that couples multiple processes: water migration and differentiation, geochemical reactions and silicate phase transitions, compaction by self-gravity, and ablation. The model further considers the following energy sources and sinks: tidal heating, radiogenic heating, geochemical energy released by serpentinization or absorbed by mineral dehydration, gravitational energy and insolation, and heat transport by conduction, convection, and advection. We apply the model to Enceladus, by guessing the initial conditions that would render a structure compatible with present-day observations, assuming the initial structure to have been homogeneous. Assuming the satellite has been losing water continually along its evolution, we postulate that it was formed as a more massive, more icy and more porous satellite, and gradually transformed into its present day state due to sustained long-term tidal heating. We consider several initial compositions and evolution scenarios and follow the evolution for the age of the Solar System, testing the present day model results against the available observational constraints. Our model shows the present configuration to be differentiated into a pure icy mantle, several tens of km thick, overlying a rocky core, composed of dehydrated rock at the center and hydrated rock in the outer part. For Enceladus, it predicts a higher rock/ice mass ratio than previously assumed and a thinner ice mantle, compatible with recent estimates based on gravity field measurements. Although, obviously, the model cannot be used to explain local phenomena, it sheds light on the internal structure invoked in explanations of localized features and activities. 	
1603.06224v2	http://arxiv.org/pdf/1603.06224v2	2016	Interpreting the Densities of the Kuiper Belt's Dwarf Planets	Amy C. Barr|Megan E. Schwamb	  Kuiper Belt objects with absolute magnitude less than 3 (radius $\gtrsim$500 km), the dwarf planets, have a range of different ice/rock ratios, and are more rock-rich than their smaller counterparts. Many of these objects have moons, which suggests that collisions may have played a role in modifying their compositions. We show that the dwarf planets fall into two categories when analysed by their mean densities and satellite-to-primary size ratio. Systems with large moons, such as Pluto/Charon and Orcus/Vanth, can form in low-velocity grazing collisions in which both bodies retain their compositions. We propose that these systems retain a primordial composition, with a density of about 1.8 g/cm$^3$. Triton, thought to be a captured KBO, could have lost enough ice during its early orbital evolution to explain its rock-enrichment relative to the primordial material. Systems with small moons, Eris, Haumea, and Quaoar, formed from a different type of collision in which icy material, perhaps a few tens of percent of the total colliding mass, is lost. The fragments would not remain in physical or dynamical proximity to the parent body. The ice loss process has not yet been demonstrated numerically, which could be due to the paucity of KBO origin simulations, or missing physical processes in the impact models. If our hypothesis is correct, we predict that large KBOs with small moons should be denser than the primordial material, and that the mean density of Orcus should be close to the primordial value. 	
1605.08080v1	http://arxiv.org/pdf/1605.08080v1	2016	Rapid yet accurate first principle based predictions of alkali halide   crystal phases using alchemical perturbation	Alisa Solovyeva|O. Anatole von Lilienfeld	  We assess the predictive power of alchemical perturbations for estimating fundamental properties in ionic crystals. Using density functional theory we have calculated formation energies, lattice constants, and bulk moduli for all sixteen iso-valence-electronic combinations of pure pristine alkali halides involving elements $A \in \{$Na, K, Rb, Cs$\}$ and $X \in \{$F, Cl, Br, I$\}$. For rock salt, zincblende and cesium chloride symmetry, alchemical Hellmann-Feynman derivatives, evaluated along lattice scans of sixteen reference crystals, have been obtained for all respective 16$\times$15 combinations of reference and predicted target crystals. Mean absolute errors (MAE) are on par with density functional theory level of accuracy for energies and bulk modulus. Predicted lattice constants are less accurate. NaCl is the best reference salt for alchemical estimates of relative energies (MAE $<$ 40 meV/atom) while alkali fluorides are the worst. By contrast, lattice constants are predicted best using NaF as a reference salt (MAE $<$ 0.5{\AA}), yielding only semi-quantitative accuracy. The best reference salt for the prediction of bulk moduli is CsCl (MAE $<$ 0.4$\times$10$^{11}$ dynes/cm$^2$). Alchemical derivatives can also be used to predict competing rock salt and cesium chloride phases in binary and ternary solid mixtures with CsCl. Alchemical predictions based on dispersion corrected density functional theory with pure RbI as a reference salt reproduce reasonably well the reversal of the rock salt/cesium chloride stability trend for binary $(AX)_{1-x}$CsCl$_x$ as well as for ternary $(AX)_{0.5-0.5x}(BY)_{0.5-0.5x}$CsCl$_x$ mixtures. 	
1607.06678v2	http://arxiv.org/pdf/1607.06678v2	2017	Random Neighborhood Graphs as Models of Fracture Networks on Rocks:   Structural and Dynamical Analysis	Ernesto Estrada|Matthew Sheerin	  We propose a new model to account for the main structural characteristics of rock fracture networks (RFNs). The model is based on a generalization of the random neighborhood graphs to consider fractures embedded into rectangular spaces. We study a series of 29 real-world RFNs and find the best fit with the random rectangular neighborhood graphs (RRNGs) proposed here. We show that this model captures most of the structural characteristics of the RFNs and allows a distinction between small and more spherical rocks and large and more elongated ones. We use a diffusion equation on the graphs in order to model diffusive processes taking place through the channels of the RFNs. We find a small set of structural parameters that highly correlates with the average diffusion time in the RFNs. We found analytically some bounds for the diameter and the algebraic connectivity of these graphs that allow to bound the diffusion time in these networks. We also show that the RRNGs can be used as a suitable model to replace the RFNs in the study of diffusion-like processes. Indeed, the diffusion time in RFNs can be predicted by using structural and dynamical parameters of the RRNGs. Finally, we also explore some potential extensions of our model to include variable fracture apertures, the possibility of long-range hops of the diffusive particles as a way to account for heterogeneities in the medium and possible superdiffusive processes, and the extension of the model to 3-dimensional space. 	
1607.08748v2	http://arxiv.org/pdf/1607.08748v2	2018	Cyclic dominance in a 2-person Rock-Scissors-Paper game	Liliana Garrido-da-Silva|Sofia B. S. D. Castro	  The Rock-Scissors-Paper game has been studied in the context of single population dynamics to account for cyclic behaviour. We use a 2-person parametrised version of this game to illustrate how cyclic behaviour is still a dominant feature of the dynamics. The cyclic behaviour is observed near a heteroclinic cycle with two nodes such that, at each node, players alternate in winning and losing. This cycle is shown to be as stable as possible for a wide range of parameter values. The parameters are related to the players' payoff when a tie occurs. This cycle is part of a heteroclinic network: there are two other cycles with two nodes and two cycles with three nodes. The cycles with two nodes exhibit some intermediate stability for a large subset of parameter space, contained in the complement of the set of stability of the first cycle. These two-node cycles represent oscillations between a tie between players and a win for only one of the players. The three-node cycles are always unstable and describe a cycle among all possible outcomes in the game, in the two possible sequences. Using some applications to price setting models, we propose what could be a starting point for the contribution of the Rock-Scissors-Paper game to the understanding of cyclic dominance in two-player games. 	
1607.08770v1	http://arxiv.org/pdf/1607.08770v1	2016	Production of $^3$He in Rocks by Reactions Induced by Particles of the   Nuclear-Active and Muon Components of Cosmic Rays: Geological and   Petrological Implications	A. V. Nesterenok|O. V. Yakubovich	  The paper presents data on the production of the $^3$He nuclide in rocks under the effect of cosmic-ray particles. The origin of the nuclide in the ground in neutron- and proton-induced spallation reactions, reactions induced by high-energy muons, and negative muon capture reactions is analyzed. The cross sections of reactions producing $^3$He and $^3$H are calculated by means of numerical simulations with the GEANT4 simulation toolkit. The production rate of the $^3$He nuclide in the ground is evaluated for the average level of solar activity at high geomagnetic latitudes and at sea level. It is proved that the production of $^3$He in near-surface ground layers by spallation reactions induced by cosmic-ray protons may be approximately 10% of the total production rate of cosmogenic $^3$He. At depths of 10-50 m.w.e., the accumulation of $^3$He is significantly contributed by reactions induced by cosmic-ray muons. Data presented in the paper make it possible to calculate the accumulation rate of $^3$He in a rock depending on depth that is necessary for the evaluation of the exposure time of the magmatic or metamorphic complex on the Earth's surface ($^3$He dating). 	
1701.01351v1	http://arxiv.org/pdf/1701.01351v1	2017	Hydrogeophysical characterization of transport processes in fractured   rock by combining push-pull tracer tests and single-hole ground penetrating   radar	A. Shakas|N. Linde|L. Baron|O. Bochet| T.|O. Bour|Le Borgne	  The in situ characterization of transport processes in fractured media is particularly challenging due to the considerable spatial uncertainty on tracer pathways and dominant controlling processes, such as dispersion, channeling, trapping, matrix diffusion, ambient and density driven flows. We attempted to reduce this uncertainty by coupling push-pull tracer experiments with single-hole ground penetrating radar (GPR) time-lapse imaging. The experiments involved different injection fractures, chaser volumes and resting times, and were performed at the fractured rock research site of Ploemeur in France (H+ network, hplus.ore.fr/en). For the GPR acquisitions, we used both fixed and moving antenna setups in a borehole that was isolated with a flexible liner. During the fixed-antenna experiment, time-varying GPR reflections allowed us to track the spatial and temporal dynamics of the tracer during the push-pull experiment. During the moving antenna experiments, we clearly imaged the dominant fractures in which tracer transport took place, fractures in which the tracer was trapped for longer time periods, and the spatial extent of the tracer distribution (up to 8 m) at different times. This demonstrated the existence of strongly channelized flow in the first few meters and radial flow at greater distances. By varying the resting time of a given experiment, we identified regions affected by density-driven and ambient flow. These experiments open up new perspectives for coupled hydrogeophysical inversion aimed at understanding transport phenomena in fractured rock formations. 	
1701.01877v1	http://arxiv.org/pdf/1701.01877v1	2017	Inferring transport characteristics in a fractured rock aquifer by   combining single-hole GPR reflection monitoring and tracer test data	C. Dorn|N. Linde|T. Le Borgne|O. Bour|M. Kleptikova	  Investigations of solute transport in fractured rock aquifers often rely on tracer test data acquired at a limited number of observation points. Such data do not, by themselves, allow detailed assessments of the spreading of the injected tracer plume. To better understand the transport behavior in a granitic aquifer, we combine tracer test data with single-hole ground-penetrating radar (GPR) reflection monitoring data. Five successful tracer tests were performed under various experimental conditions between two boreholes 6 m apart. For each experiment, saline tracer was injected into a previously identified packed-off transmissive fracture while repeatedly acquiring single-hole GPR reflection profiles together with electrical conductivity logs in the pumping borehole. By analyzing depth-migrated GPR difference images together with tracer breakthrough curves and associated simplified flow and transport modeling, we estimate (1) the number, the connectivity, and the geometry of fractures that contribute to tracer transport, (2) the velocity and the mass of tracer that was carried along each flow path, and (3) the effective transport parameters of the identified flow paths. We find a qualitative agreement when comparing the time evolution of GPR reflectivity strengths at strategic locations in the formation with those arising from simulated transport. The discrepancies are on the same order as those between observed and simulated breakthrough curves at the outflow locations. The rather subtle and repeatable GPR signals provide useful and complementary information to tracer test data acquired at the outflow locations and may help us to characterize transport phenomena in fractured rock aquifers. 	
1702.01552v1	http://arxiv.org/pdf/1702.01552v1	2017	Combining the Modified Discrete Element Method with the Virtual Element   Method for Fracturing of Porous Media	Halvor Møll Nilsen|Idar Larsen|Xavier Raynaud	  Simulation of fracturing processes in porous rocks can be divided into two main branches: (i) modeling the rock as a continuum which is enhanced with special features to account for fractures, or (ii) modeling the rock by a discrete (or discontinuous) approach that describes the material directly as a collection of separate blocks or particles, e.g., as in the discrete element method (DEM). In the modified discrete element (MDEM) method, the effective forces between virtual particles are modified in all regions, without failing elements, so that they reproduce the discretization of a first order finite element method (FEM) for linear elasticity. This provides an expression of the virtual forces in terms of general Hook's macro-parameters. Previously, MDEM has been formulated through an analogy with linear elements for FEM. We show the connection between MDEM and the virtual element method (VEM), which is a generalization of FEM to polyhedral grids. Unlike standard FEM, which computes strain-states in a reference space, MDEM and VEM compute stress-states directly in real space. This connection leads us to a new derivation of the MDEM method. Moreover, it gives the basis for coupling (M)DEM to domain with linear elasticity described by polyhedral grids, which makes it easier to apply realistic boundary conditions in hydraulic-fracturing simulations. This approach also makes it possible to combine fine-scale (M)DEM behavior near the fracturing region with linear elasticity on complex reservoir grids in the far-field region without regridding. To demonstrate the simulation of hydraulic fracturing, the coupled (M)DEM-VEM method is implemented using the Matlab Reservoir Simulation Toolbox (MRST) and linked to an industry-standard reservoir simulator. 	
1703.10997v1	http://arxiv.org/pdf/1703.10997v1	2017	Evolution of major sedimentary mounds on Mars	Edwin S. Kite|Jonathan Sneed|David P. Mayer|Kevin W. Lewis|Timothy I. Michaels|Alicia Hore|Scot C. R. Rafkin	  We present a new database of $>$300 layer-orientations from sedimentary mounds on Mars. These layer orientations, together with draped landslides, and draping of rocks over differentially-eroded paleo-domes, indicate that for the stratigraphically-uppermost $\sim$1 km, the mounds formed by the accretion of draping strata in a mound-shape. The layer-orientation data further suggest that layers lower down in the stratigraphy also formed by the accretion of draping strata in a mound-shape. The data are consistent with terrain-influenced wind erosion, but inconsistent with tilting by flexure, differential compaction over basement, or viscoelastic rebound. We use a simple landscape evolution model to show how the erosion and deposition of mound strata can be modulated by shifts in obliquity. The model is driven by multi-Gyr calculations of Mars' chaotic obliquity and a parameterization of terrain-influenced wind erosion that is derived from mesoscale modeling. Our results suggest that mound-spanning unconformities with kilometers of relief emerge as the result of chaotic obliquity shifts. Our results support the interpretation that Mars' rocks record intermittent liquid-water runoff during a $>$10$^8$-yr interval of sedimentary rock emplacement. 	
1706.10108v1	http://arxiv.org/pdf/1706.10108v1	2017	Solving petrological problems through machine learning: the study case   of tectonic discrimination using geochemical and isotopic data	Maurizio Petrelli|Diego Perugini	  Machine learning methods are evaluated to study the intriguing and debated topic of discrimination among different tectonic environments using geochemical and isotopic data. Volcanic rocks characterized by a whole geochemical signature of major elements (SiO2, TiO2, Al2O3, Fe2O3T, CaO, MgO, Na2O, K2O), selected trace elements (Sr, Ba, Rb, Zr, Nb, La, Ce, Nd, Hf, Sm, Gd, Y, Yb, Lu, Ta, Th) and isotopes (206Pb/204Pb, 207Pb/204Pb, 208Pb/204Pb, 87Sr/86Sr and 143Nd/144Nd) have been extracted from open-access and comprehensive petrological databases (i.e. PetDB and GEOROC). The obtained dataset has been analyzed using support vector machines, a set of supervised machine learning methods, which are considered particularly powerful in classification problems. Results from the application of the machine learning methods show that the combined use of major, trace elements and isotopes allow associating the geochemical composition of rocks to the relative tectonic setting with high classification scores (93%, on average). The lowest scores are recorded from volcanic rocks deriving from back-arc basins (65%). All the other tectonic settings display higher classification scores, with oceanic islands reaching values up to 99%. Results of this study could have a significant impact in other petrological studies potentially opening new perspectives for petrologists and geochemists. Other examples of applications include the development of more robust geo-thermometers and geo-barometers and the recognition of volcanic sources for tephra layers in tephro-chronological studies. 	
1710.03342v1	http://arxiv.org/pdf/1710.03342v1	2017	A Case for an Atmosphere on Super-Earth 55 Cancri e	Isabel Angelo|Renyu Hu	  One of the primary questions when characterizing Earth-sized and super-Earth-sized exoplanets is whether they have a substantial atmosphere like Earth and Venus or a bare-rock surface like Mercury. Phase curves of the planets in thermal emission provide clues to this question, because a substantial atmosphere would transport heat more efficiently than a bare-rock surface. Analyzing phase curve photometric data around secondary eclipse has previously been used to study energy transport in the atmospheres of hot Jupiters. Here we use phase curve, Spitzer time-series photometry to study the thermal emission properties of the super-Earth exoplanet 55 Cancri e. We utilize a semi-analytical framework to fit a physical model to the infrared photometric data at 4.5 micron. The model uses parameters of planetary properties including Bond albedo, heat redistribution efficiency (i.e., ratio between radiative timescale and advective timescale of the atmosphere), and atmospheric greenhouse factor. The phase curve of 55 Cancri e is dominated by thermal emission with an eastward-shifted hot spot. We determine the heat redistribution efficiency to be ~1.47, which implies that the advective timescale is on the same order as the radiative timescale. This requirement cannot be met by the bare-rock planet scenario because heat transport by currents of molten lava would be too slow. The phase curve thus favors the scenario with a substantial atmosphere. Our constraints on the heat redistribution efficiency translate to an atmospheric pressure of ~1.4 bar. The Spitzer 4.5-micron band is thus a window into the deep atmosphere of the planet 55 Cancri e. 	
1712.02553v1	http://arxiv.org/pdf/1712.02553v1	2017	Inelastic deformation during sill and laccolith emplacement: Insights   from an analytic elastoplastic model	J. Scheibert|O. Galland|A. Hafver	  Numerous geological observations evidence that inelastic deformation occurs during sills and laccoliths emplacement. However, most models of sill and laccolith emplacement neglect inelastic processes by assuming purely elastic deformation of the host rock. This assumption has never been tested, so that the role of inelastic deformation on the growth dynamics of magma intrusions remains poorly understood. In this paper, we introduce the first analytical model of shallow sill and laccolith emplacement that accounts for elasto-plastic deformation of the host rock. It considers the intrusion's overburden as a thin elastic bending plate attached to an elastic-perfectly-plastic foundation. We find that, for geologically realistic values of the model parameters, the horizontal extent of the plastic zone lp is much smaller than the radius of the intrusion a. By modeling the quasi-static growth of a sill, we find that the ratio lp/a decreases during propagation, as 1/ $\sqrt$ a 4 $\Delta$P , with $\Delta$P the magma overpressure. The model also shows that the extent of the plastic zone decreases with the intrusion's depth, while it increases if the host rock is weaker. Comparison between our elasto-plastic model and existing purely elastic models shows that plasticity can have a significant effect on intrusion propagation dynamics, with e.g. up to a doubling of the overpressure necessary for the sill to grow. Our results suggest that plasticity effects might be small for large sills, but conversely that they might be substantial for early sill propagation. 2 	
1712.02980v3	http://arxiv.org/pdf/1712.02980v3	2018	A physical model predicting instability of rock slopes with locked   segments along a potential slip surface	Chen Hongran|Qin Siqing|Xue Lei|Yang Baicun|Zhang Ke	  Predicting the occurrence of landslides is important to prevent or reduce loss of lives and property. The stability of rock slopes is often dominated by one or more locked segments along a potential slip surface; these segments have relatively high strength and accumulate strain energy. Locked segments can be preliminarily classified into three categories: "rock bridge", "retaining wall" and "sustaining arch." Coupling a one-dimensional renormalization group model with the strain-softening constitutive relation of geo-materials considering the Weibull's distribution, a physical model for predicting the instability of slopes with locked segments is established. It is found that the ratio of the strain or displacement at the peak strength point to that at the volume dilation point for a locked segment is exclusively dependent on the Weibull's shape parameter m, and is approximately constant at 1.48. A corresponding accelerating displacement increment (tertiary creep) of the slope can be observed from the onset of the volume dilation of the locked segment due to its unsteady cracking. For a slope with multiple locked segments, one can predict its critical instability displacement value according to the accelerating displacement onset corresponding to the volume dilation point of the first locked segment and the number of locked segments. The back-analysis of two typical cases, the Yanchihe rockslide in China and the wedge rockslide, Libby Dam, USA, shows that their evolutionary processes, dominated respectively by one and two locked segments, follow this model, confirming the reliability of the proposed model. 	
0309223v1	http://arxiv.org/pdf/cond-mat/0309223v1	2003	Boundary Friction on Molecular Lubricants: Rolling Mode?	V. M. Loktev|Yu. G. Pogorelov	  A theoretical model is proposed for low temperature friction between two smooth rigid solid surfaces separated by lubricant molecules, admitting their deformations and rotations. Appearance of different modes of energy dissipation (by ''rocking'' or ''rolling'' of lubricants) at slow relative displacement of the surfaces is shown to be accompanied by the stick-and-slip features and reveals a non-monotonic (mean) friction force {\it vs} external load 	
9905025v1	http://arxiv.org/pdf/hep-ex/9905025v1	1999	Neutrino Oscillations at High Energy by MACRO	F. Ronga	  We present updated results of the measurement of upward-going muons produced by neutrino interactions in the rock below the MACRO detector. These data support MACRO's previously published results. They favor a neutrino oscillation explanation of the atmospheric neutrino anomaly. 	
0403009v2	http://arxiv.org/pdf/hep-ex/0403009v2	2004	Neutron background at the Canfranc Underground Laboratory and its   contribution to the IGEX-DM dark matter experiment	J. M. Carmona|S. Cebrian|E. Garcia|I. G. Irastorza|G. Luzon|A. Morales|J. Morales|A. Ortiz de Solorzano|J. Puimedon|M. L. Sarsa|J. A. Villar	  A quantitative study of the neutron environment in the Canfranc Underground Laboratory has been performed. The analysis is based on a complete set of simulations and, particularly, it is focused on the IGEX-DM dark matter experiment. The simulations are compared to the IGEX-DM low energy data obtained with different shielding conditions. The results of the study allow us to conclude, with respect to the IGEX-DM background, that the main neutron population, coming from radioactivity from the surrounding rock, is practically eliminated after the implementation of a suitable neutron shielding. The remaining neutron background (muon-induced neutrons in the shielding and in the rock) is substantially below the present background level thanks to the muon veto system. In addition, the present analysis gives us a further insight on the effect of neutrons in other current and future experiments at the Canfranc Underground Laboratory. The comparison of simulations with the body of data available has allowed to set the flux of neutrons from radioactivity of the Canfranc rock, (3.82 +- 0.44) x 10^{-6} cm^{-2} s^{-1}, as well as the flux of muon-induced neutrons in the rock, (1.73 +- 0.22(stat) \+- 0.69(syst)) x 10^{-9} cm^{-2} s^{-1}, or the rate of neutron production by muons in the lead shielding, (4.8 +- 0.6 (stat) +- 1.9 (syst)) x 10^{-9} cm^{-3} s^{-1}. 	
0612014v1	http://arxiv.org/pdf/hep-ex/0612014v1	2006	First measurement of low intensity fast neutron background from rock at   the Boulby Underground Laboratory	E. Tziaferi|M. J. Carson|V. A. Kudryavtsev|R. Lerner|P. K. Lightfoot|S. M. Paling|M. Robinson|N. J. C. Spooner	  A technique to measure low intensity fast neutron flux has been developed. The design, calibrations, procedure for data analysis and interpretation of the results are discussed in detail. The technique has been applied to measure the neutron background from rock at the Boulby Underground Laboratory, a site used for dark matter and other experiments, requiring shielding from cosmic ray muons. The experiment was performed using a liquid scintillation detector. A 6.1 litre volume stainless steel cell was filled with an in-house made liquid scintillator loaded with Gd to enhance neutron capture. A two-pulse signature (proton recoils followed by gammas from neutron capture) was used to identify the neutron events from much larger gamma background from PMTs. Suppression of gammas from the rock was achieved by surrounding the detector with high-purity lead and copper. Calibrations of the detector were performed with various gamma and neutron sources. Special care was taken to eliminate PMT afterpulses and correlated background events from the delayed coincidences of two pulses in the Bi-Po decay chain. A four month run revealed a neutron-induced event rate of 1.84 +- 0.65 (stat.) events/day. Monte Carlo simulations based on the GEANT4 toolkit were carried out to estimate the efficiency of the detector and the energy spectra of the expected proton recoils. From comparison of the measured rate with Monte Carlo simulations the flux of fast neutrons from rock was estimated as (1.72 +- 0.61 (stat.) +- 0.38 (syst.))*10^(-6) cm^(-2) s^(-1) above 0.5 MeV. 	
0611008v1	http://arxiv.org/pdf/hep-ph/0611008v1	2006	New results on muon inelastic cross section and energy loss in rock	D. A. Timashkov|A. A. Petrukhin	  Various uncertainties in calculations of inelastic cross section and energy loss are considered. It is shown that widely used kinematic neglects and approximations result in deviations in calculations of these values. The obtained corrections increase with lepton mass, therefore possible consequences for tau-lepton are discussed, too. 	
9911057v1	http://arxiv.org/pdf/physics/9911057v1	1999	Solidification pipes: from solder pots to igneous rocks	M. Stewart Siu|Dmitry Budker	  When a substance that shrinks in volume as it solidifies (for example, lead) is melted in a container and then cooled, a deep hole is often found in the center after resolidification. We use a simple model to describe the shape of the pipe and compare it with experimental results. 	
9803061v1	http://arxiv.org/pdf/quant-ph/9803061v1	1998	Photon trains and lasing : The periodically pumped quantum dot	C. Wiele|F. Haake|C. Rocke|A. Wixforth	  We propose to pump semiconductor quantum dots with surface acoustic waves which deliver an alternating periodic sequence of electrons and holes. In combination with a good optical cavity such regular pumping could entail anti-bunching and sub-Poissonian photon statistics. In the bad-cavity limit a train of equally spaced photons would arise. 	
1102.1335v1	http://arxiv.org/pdf/1102.1335v1	2011	U and Th content in the Central Apennines continental crust: a   contribution to the determination of the geo-neutrinos flux at LNGS	M. Coltorti|R. Boraso|F. Mantovani|M. Morsilli|G. Fiorentini|A. Riva|G. Rusciadelli|R. Tassinari|C. Tomei|G. Di Carlo|V. Chubakov	  The regional contribution to the geo-neutrino signal at Gran Sasso National Laboratory (LNGS) was determined based on a detailed geological, geochemical and geophysical study of the region. U and Th abundances of more than 50 samples representative of the main lithotypes belonging to the Mesozoic and Cenozoic sedimentary cover were analyzed. Sedimentary rocks were grouped into four main "Reservoirs" based on similar paleogeographic conditions and mineralogy. Basement rocks do not outcrop in the area. Thus U and Th in the Upper and Lower Crust of Valsugana and Ivrea-Verbano areas were analyzed. Based on geological and geophysical properties, relative abundances of the various reservoirs were calculated and used to obtain the weighted U and Th abundances for each of the three geological layers (Sedimentary Cover, Upper and Lower Crust). Using the available seismic profile as well as the stratigraphic records from a number of exploration wells, a 3D modelling was developed over an area of 2^{\circ}x2^{\circ} down to the Moho depth, for a total volume of about 1.2x10^6 km^3. This model allowed us to determine the volume of the various geological layers and eventually integrate the Th and U contents of the whole crust beneath LNGS. On this base the local contribution to the geo-neutrino flux (S) was calculated and added to the contribution given by the rest of the world, yielding a Refined Reference Model prediction for the geo-neutrino signal in the Borexino detector at LNGS: S(U) = (28.7 \pm 3.9) TNU and S(Th) = (7.5 \pm 1.0) TNU. An excess over the total flux of about 4 TNU was previously obtained by Mantovani et al. (2004) who calculated, based on general worldwide assumptions, a signal of 40.5 TNU. The considerable thickness of the sedimentary rocks, almost predominantly represented by U- and Th- poor carbonatic rocks in the area near LNGS, is responsible for this difference. 	
1309.5752v2	http://arxiv.org/pdf/1309.5752v2	2013	Growth rate of the ancient human population in Australia	Ron W Nielsen aka Jan Nurzynski	  Empirical growth rates of the ancient human population in Australia have been calculated using the data for the number of rock-shelter sites. They confirm the earlier conclusion that there was no intensification of growth between 10,000 and 1000 years BP. 	
1309.6462v1	http://arxiv.org/pdf/1309.6462v1	2013	A report on the magnetic order of transition-metal $δ$-doped cubic   ZnO	Iosif Galanakis	  Preliminary results on the properties of transition-metal (Ti, V, Cr, Mn, Fe, Co, Ni) $\delta$-doped ZnO are reported. Using \emph{ab-initio} electronic structure calculations the magnetic order is studied assuming both the cubic rock-salt and zinc-blende structures for ZnO. The ground state magnetic order is found to depend strongly on the transition-metal atom. 	
1501.04608v1	http://arxiv.org/pdf/1501.04608v1	2015	Non-equilibrium thermodynamical framework for rate- and state-dependent   friction	P. Ván|N. Mitsui|T. Hatano	  Rate- and state-dependent friction law for velocity-step and healing are analysed from a thermodynamic point of view. Assuming a logarithmic deviation from steady-state a unification of the classical Dieterich and Ruina models of rock friction is proposed. 	
1502.05078v1	http://arxiv.org/pdf/1502.05078v1	2015	Paradox of Peroxy Defects and Positive Holes in Rocks - Part I: Effect   of Temperature	Friedemann T. Freund|Minoru M. Freund	  Though ubiquitous in minerals of igneous and high-grade metamorphic rocks, peroxy defects have been widely overlooked in the past. The charge carriers of interest are positive holes, chemically equivalent to O$^-$ in a matrix of O$^{2-}$, physically defect electrons in the O$^{2-}$ sublattice, highly mobile, able to propagate fast and far. O$^-$ are oxidized relative to O$^{2-}$. As such O$^-$ are not supposed to exist in minerals and rocks that come from deep within the Earth crust or upper mantle, where the environments are overwhelmingly reduced. In order to understand how peroxy defects are introduced, we look at peroxy defects in a crystallographically and compositionally well characterized model system: single crystals of nominally high-purity MgO, grown from the melt under highly reducing conditions. During crystallization the MgO crystals incorporate OH$^-$ through dissolution of traces of H$_2$O into the MgO matrix, leading to a solid solution (ss) Mg$_{1-\delta}$(OH)$_{2\delta}$O$_{1-2\delta}$, where $\delta$ <1. During cooling, ss turns into a metastable supersaturated solid solution (sss). During further cooling, OH$^-$ pairs at Mg$^{2+}$ vacancy sites rearrange their electrons, undergoing a redox conversion, which leads to peroxy anions, O$_2^{2-}$, plus molecular H$_2$. Being diffusively mobile, the H$_2$ can leave their Mg$^{2+}$ vacancy sites, leaving behind cation-deficient Mg$_{1-\delta}$O. During reheating O$_2^{2-}$ break up, releasing positive hole charge carriers, which affect the electrical conductivity behavior. In rocks, similar changes in the electrical conductivity are observed in the temperature window, where peroxy defects of the type O$_3$Si-OO-SiO$_3$ break up. They release positive holes, which control the electrical conductivity response along the geotherm. 	
1503.05372v2	http://arxiv.org/pdf/1503.05372v2	2015	A new method for analyzing collimation angle of neutron Soller   collimator	Jian-Bo Gao|Yun-Tao Liu|Dong-Feng Chen	  A new method for analyzing collimation angle of neutron Soller collimator is described. Gaussian distribution formula is used to define the angle distribution function of neutron source and neutron transmission function of Soller collimator. A relationship between FWHM of collimator rocking curve and collimation angle is derived. 	
1503.09168v1	http://arxiv.org/pdf/1503.09168v1	2015	On Convergence and Threshold Properties of Discrete Lotka-Volterra   Population Protocols	Jurek Czyzowicz|Leszek Gasieniec|Adrian Kosowski|Evangelos Kranakis|Paul G. Spirakis|Przemyslaw Uznanski	  In this work we focus on a natural class of population protocols whose dynamics are modelled by the discrete version of Lotka-Volterra equations. In such protocols, when an agent $a$ of type (species) $i$ interacts with an agent $b$ of type (species) $j$ with $a$ as the initiator, then $b$'s type becomes $i$ with probability $P\_{ij}$. In such an interaction, we think of $a$ as the predator, $b$ as the prey, and the type of the prey is either converted to that of the predator or stays as is. Such protocols capture the dynamics of some opinion spreading models and generalize the well-known Rock-Paper-Scissors discrete dynamics. We consider the pairwise interactions among agents that are scheduled uniformly at random. We start by considering the convergence time and show that any Lotka-Volterra-type protocol on an $n$-agent population converges to some absorbing state in time polynomial in $n$, w.h.p., when any pair of agents is allowed to interact. By contrast, when the interaction graph is a star, even the Rock-Paper-Scissors protocol requires exponential time to converge. We then study threshold effects exhibited by Lotka-Volterra-type protocols with 3 and more species under interactions between any pair of agents. We start by presenting a simple 4-type protocol in which the probability difference of reaching the two possible absorbing states is strongly amplified by the ratio of the initial populations of the two other types, which are transient, but "control" convergence. We then prove that the Rock-Paper-Scissors protocol reaches each of its three possible absorbing states with almost equal probability, starting from any configuration satisfying some sub-linear lower bound on the initial size of each species. That is, Rock-Paper-Scissors is a realization of a "coin-flip consensus" in a distributed system. Some of our techniques may be of independent value. 	
1604.02255v1	http://arxiv.org/pdf/1604.02255v1	2016	Sound velocity measurement methods for porous sandstone. Measurements,   finite element modelling, and diffraction correction	Mathias Sæther|Per Lunde|Geir Ersland	  Acoustic material parameters of gas hydrate bearing porous rocks are important for evaluation of methods to exploit the vast methane gas resources present in the earth's subsurface, potentially combined with CO2 injection. A solid buffer method for measuring changes of the compressional wave velocity in porous rocks with changing methane hydrate contents under high-pressure hydrate-forming conditions, is tested and evaluated with respect to effects influencing on the measurement accuracy. The limited space available in the pressure chamber represents a challenge for the measurement method. Several effects affect the measured compressional wave velocity, such as interference from sidewall reflections, diffraction effects, the amount of torque (force) used to achieve acoustic coupling, and water draining of the watersaturated rock specimen. Test measurements using the solid buffer method in the pressure chamber at atmospheric conditions are compared to independent measurements using a water-bath immersion measurement method. Compressional wave velocity measurements have been done in the steady state region at frequency 500 kHz for various specimen made of plexiglas and Bentheim sandstone. Finite element simulations of the solid buffer measurement method with plexiglas specimen have been used for comparison with the measurements, and to aid in the design, control, and evaluation of the measurement method and results. Highly favorable agreement between the two measurement methods has been obtained, also with respect to repeatability and reproducibility. The results indicate that the solid buffer method may be suitable for use in the pressure chamber with Bentheim sandstone and changing methane hydrate contents under high-pressure hydrate-forming conditions, for quantitative measurements of the compressional wave velocity in such rock core samples at these frequencies. 	
1701.02360v2	http://arxiv.org/pdf/1701.02360v2	2017	Resolving Orbital and Climate Keys of Earth and Extraterrestrial   Environments with Dynamics 1.0: A General Circulation Model for Simulating   the Climates of Rocky Planets	M. J. Way|Igor Aleinov|David. S. Amundsen|Mark Chandler|Thomas Clune|Anthony D. Del Genio|Yuka Fujii|Maxwell Kelley|Nancy Y. Kiang|Linda Sohl|Kostas Tsigaridis	  Resolving Orbital and Climate Keys of Earth and Extraterrestrial Environments with Dynamics (ROCKE-3D) is a 3-Dimensional General Circulation Model (GCM) developed at the NASA Goddard Institute for Space Studies for the modeling of atmospheres of Solar System and exoplanetary terrestrial planets. Its parent model, known as ModelE2 (Schmidt et al. 2014), is used to simulate modern and 21st Century Earth and near-term paleo-Earth climates. ROCKE-3D is an ongoing effort to expand the capabilities of ModelE2 to handle a broader range of atmospheric conditions including higher and lower atmospheric pressures, more diverse chemistries and compositions, larger and smaller planet radii and gravity, different rotation rates (slowly rotating to more rapidly rotating than modern Earth, including synchronous rotation), diverse ocean and land distributions and topographies, and potential basic biosphere functions. The first aim of ROCKE-3D is to model planetary atmospheres on terrestrial worlds within the Solar System such as paleo-Earth, modern and paleo-Mars, paleo-Venus, and Saturn's moon Titan. By validating the model for a broad range of temperatures, pressures, and atmospheric constituents we can then expand its capabilities further to those exoplanetary rocky worlds that have been discovered in the past and those to be discovered in the future. We discuss the current and near-future capabilities of ROCKE-3D as a community model for studying planetary and exoplanetary atmospheres. 	
1801.00991v1	http://arxiv.org/pdf/1801.00991v1	2018	Reconciling Magma-Ocean Crystallization Models with the present-day   Structure of the Earth's mantle	Maxim D. Ballmer|Diogo L. Lourenço|Kei Hirose|Razvan Caracas|Ryuichi Nomura	  Terrestrial planets are thought to experience episode(s) of large-scale melting early in their history. Fractionation during magma-ocean freezing leads to unstable stratification within the related cumulate layers due to progressive iron enrichment upward, but the effects of incremental cumulate overturns during MO crystallization remain to be explored. Here, we use geodynamic models with a moving-boundary approach to study convection and mixing within the growing cumulate layer, and thereafter within the fully crystallized mantle. For fractional crystallization, cumulates are efficiently stirred due to subsequent incremental overturns, except for strongly iron-enriched late-stage cumulates, which persist as a stably stratified layer at the base of the mantle for billions of years. Less extreme crystallization scenarios can lead to somewhat more subtle stratification. In any case, the long-term preservation of at least a thin layer of extremely enriched cumulates with Fe#>0.4, as predicted by all our models, is inconsistent with seismic constraints. Based on scaling relationships, however, we infer that final-stage Fe-rich magma-ocean cumulates originally formed near the surface should have overturned as small diapirs, and hence undergone melting and reaction with the host rock during sinking. The resulting moderately iron-enriched metasomatized/hybrid rock assemblages should have accumulated at the base of the mantle, potentially fed an intermittent basal magma ocean, and be preserved through the present-day. Such moderately iron-enriched rock assemblages can reconcile the physical properties of the large low shear-wave velocity provinces in the presentday lower mantle. Thus, we reveal Hadean melting and rock-reaction processes by integrating magmaocean crystallization models with the seismic-tomography snapshot. 	
0610165v1	http://arxiv.org/pdf/cs/0610165v1	2006	Decentralized Failure Diagnosis of Stochastic Discrete Event Systems	Fuchun Liu|Daowen Qiu|Hongyan Xing|Zhujun Fan	  Recently, the diagnosability of {\it stochastic discrete event systems} (SDESs) was investigated in the literature, and, the failure diagnosis considered was {\it centralized}. In this paper, we propose an approach to {\it decentralized} failure diagnosis of SDESs, where the stochastic system uses multiple local diagnosers to detect failures and each local diagnoser possesses its own information. In a way, the centralized failure diagnosis of SDESs can be viewed as a special case of the decentralized failure diagnosis presented in this paper with only one projection. The main contributions are as follows: (1) We formalize the notion of codiagnosability for stochastic automata, which means that a failure can be detected by at least one local stochastic diagnoser within a finite delay. (2) We construct a codiagnoser from a given stochastic automaton with multiple projections, and the codiagnoser associated with the local diagnosers is used to test codiagnosability condition of SDESs. (3) We deal with a number of basic properties of the codiagnoser. In particular, a necessary and sufficient condition for the codiagnosability of SDESs is presented. (4) We give a computing method in detail to check whether codiagnosability is violated. And (5) some examples are described to illustrate the applications of the codiagnosability and its computing method. 	
1206.6701v2	http://arxiv.org/pdf/1206.6701v2	2014	Evaluating the dependence of a non-leaky intervention's partial efficacy   on a categorical mark	Paul T. Edlefsen	  We address discrete-marks survival analysis, also known as categorical sieve analysis, for a setting of a randomized placebo-controlled treatment intervention to prevent infection by a pathogen to which multiple exposures are possible, with a finite number of types of "failure". In particular, we address the case of interventions that are partially efficacious due to a combination of failure-type-dependent efficacy and subject-dependent efficacy, for an intervention that is "non-leaky" (where "leaky" interventions are those for which each exposure event has a chance of resulting in a "failure" outcome, so multiple exposures to pathogens of a single type increase the chance of failure). We introduce the notion of some-or-none interventions, which are completely effective only against some of the failure types, and are completely ineffective against the others. Under conditions of no intervention-induced failures, we introduce a framework and Bayesian and frequentist methods to detect and quantify the extent to which an intervention's partial efficacy is attributable to uneven efficacy across the failure types rather than to incomplete "take" of the intervention. These new methods provide more power than existing methods to detect sieve effects when the conditions hold. We demonstrate the new framework and methods with simulation results and new analyses of genomic signatures of HIV-1 vaccine effects in the STEP and RV144 vaccine efficacy trials. 	
1310.1050v1	http://arxiv.org/pdf/1310.1050v1	2013	The failure tolerance of mechatronic software systems to random and   targeted attacks	Dharshana Kasthurirathna|Andy Dong|Mahendrarajah Piraveenan|Irem Y. Tumer	  This paper describes a complex networks approach to study the failure tolerance of mechatronic software systems under various types of hardware and/or software failures. We produce synthetic system architectures based on evidence of modular and hierarchical modular product architectures and known motifs for the interconnection of physical components to software. The system architectures are then subject to various forms of attack. The attacks simulate failure of critical hardware or software. Four types of attack are investigated: degree centrality, betweenness centrality, closeness centrality and random attack. Failure tolerance of the system is measured by a 'robustness coefficient', a topological 'size' metric of the connectedness of the attacked network. We find that the betweenness centrality attack results in the most significant reduction in the robustness coefficient, confirming betweenness centrality, rather than the number of connections (i.e. degree), as the most conservative metric of component importance. A counter-intuitive finding is that "designed" system architectures, including a bus, ring, and star architecture, are not significantly more failure-tolerant than interconnections with no prescribed architecture, that is, a random architecture. Our research provides a data-driven approach to engineer the architecture of mechatronic software systems for failure tolerance. 	
1402.2680v1	http://arxiv.org/pdf/1402.2680v1	2014	Unveiling Potential Failure Propagation Scenarios in Core Transport   Networks	Marc Manzano|Anna Manolova Fagertun|Sarah Ruepp|Eusebi Calle|Caterina Scoglio|Ali Sydney|Antonio de la Oliva|Alfonso Muñoz	  The contemporary society has become more dependent on telecommunication networks. Novel services and technologies supported by such networks, such as cloud computing or e-Health, hold a vital role in modern day living. Large-scale failures are prone to occur, thus being a constant threat to business organizations and individuals. To the best of our knowledge, there are no publicly available reports regarding failure propagation in core transport networks. Furthermore, Software Defined Networking (SDN) is becoming more prevalent in our society and we can envision more SDN-controlled Backbone Transport Networks (BTNs) in the future. For this reason, we investigate the main motivations that could lead to epidemic-like failures in BTNs and SDNTNs. To do so, we enlist the expertise of several research groups with significant background in epidemics, network resiliency, and security. In addition, we consider the experiences of three network providers. Our results illustrate that Dynamic Transport Networks (DTNs) are prone to epidemic-like failures. Moreover, we propose different situations in which a failure can propagate in SDNTNs. We believe that the key findings will aid network engineers and the scientific community to predict this type of disastrous failure scenario and plan adequate survivability strategies. 	
1402.5282v1	http://arxiv.org/pdf/1402.5282v1	2014	The Compound Class of Linear Failure Rate-Power Series Distributions:   Model, Properties and Applications	Eisa Mahmoudi|Ali Akbar Jafari	  We introduce in this paper a new class of distributions which generalizes the linear failure rate (LFR) distribution and is obtained by compounding the LFR distribution and power series (PS) class of distributions. This new class of distributions is called the linear failure rate-power series (LFRPS) distributions and contains some new distributions such as linear failure rate geometric (LFRG) distribution, linear failure rate Poisson (LFRP) distribution, linear failure rate logarithmic (LFRL) distribution, linear failure rate binomial (LFRB) distribution and Raylight-power series (RPS) class of distributions. Some former works such as exponential-power series (EPS) class of distributions, exponential geometric (EG) distribution, exponential Poisson (EP) distribution and exponential logarithmic (EL) distribution are special cases of the new proposed model.   The ability of the LFRPS class of distributions is in covering five possible hazard rate function i.e., increasing, decreasing, upside-down bathtub (unimodal), bathtub and increasing-decreasing-increasing shaped. Several properties of the LFRPS distributions such as moments, maximum likelihood estimation procedure via an EM-algorithm and inference for a large sample, are discussed in this paper. In order to show the flexibility and potentiality of the new class of distributions, the fitted results of the new class of distributions and some its submodels are compared using a real data set. 	
1408.0409v1	http://arxiv.org/pdf/1408.0409v1	2014	Vertex Fault Tolerant Additive Spanners	Merav Parter	  A {\em fault-tolerant} structure for a network is required to continue functioning following the failure of some of the network's edges or vertices. In this paper, we address the problem of designing a {\em fault-tolerant} additive spanner, namely, a subgraph $H$ of the network $G$ such that subsequent to the failure of a single vertex, the surviving part of $H$ still contains an \emph{additive} spanner for (the surviving part of) $G$, satisfying $dist(s,t,H\setminus \{v\}) \leq dist(s,t,G\setminus \{v\})+\beta$ for every $s,t,v \in V$. Recently, the problem of constructing fault-tolerant additive spanners resilient to the failure of up to $f$ \emph{edges} has been considered by Braunschvig et. al. The problem of handling \emph{vertex} failures was left open therein. In this paper we develop new techniques for constructing additive FT-spanners overcoming the failure of a single vertex in the graph. Our first result is an FT-spanner with additive stretch $2$ and $\widetilde{O}(n^{5/3})$ edges. Our second result is an FT-spanner with additive stretch $6$ and $\widetilde{O}(n^{3/2})$ edges. The construction algorithm consists of two main components: (a) constructing an FT-clustering graph and (b) applying a modified path-buying procedure suitably adopted to failure prone settings. Finally, we also describe two constructions for {\em fault-tolerant multi-source additive spanners}, aiming to guarantee a bounded additive stretch following a vertex failure, for every pair of vertices in $S \times V$ for a given subset of sources $S\subseteq V$. The additive stretch bounds of our constructions are 4 and 8 (using a different number of edges). 	
1506.03555v1	http://arxiv.org/pdf/1506.03555v1	2015	Automatic Generation of Minimal Cut Sets	Sentot Kromodimoeljo|Peter A. Lindsay	  A cut set is a collection of component failure modes that could lead to a system failure. Cut Set Analysis (CSA) is applied to critical systems to identify and rank system vulnerabilities at design time. Model checking tools have been used to automate the generation of minimal cut sets but are generally based on checking reachability of system failure states. This paper describes a new approach to CSA using a Linear Temporal Logic (LTL) model checker called BT Analyser that supports the generation of multiple counterexamples. The approach enables a broader class of system failures to be analysed, by generalising from failure state formulae to failure behaviours expressed in LTL. The traditional approach to CSA using model checking requires the model or system failure to be modified, usually by hand, to eliminate already-discovered cut sets, and the model checker to be rerun, at each step. By contrast, the new approach works incrementally and fully automatically, thereby removing the tedious and error-prone manual process and resulting in significantly reduced computation time. This in turn enables larger models to be checked. Two different strategies for using BT Analyser for CSA are presented. There is generally no single best strategy for model checking: their relative efficiency depends on the model and property being analysed. Comparative results are given for the A320 hydraulics case study in the Behavior Tree modelling language. 	
1510.02735v2	http://arxiv.org/pdf/1510.02735v2	2015	Reliability and Survivability Analysis of Data Center Network Topologies	Rodrigo de Souza Couto|Stefano Secci|Miguel Elias Mitre Campista|Luís Henrique Maciel Kosmalski Costa	  The architecture of several data centers have been proposed as alternatives to the conventional three-layer one.Most of them employ commodity equipment for cost reduction. Thus, robustness to failures becomes even more important, because commodity equipment is more failure-prone. Each architecture has a different network topology design with a specific level of redundancy. In this work, we aim at analyzing the benefits of different data center topologies taking the reliability and survivability requirements into account. We consider the topologies of three alternative data center architecture: Fat-tree, BCube, and DCell. Also, we compare these topologies with a conventional three-layer data center topology. Our analysis is independent of specific equipment, traffic patterns, or network protocols, for the sake of generality. We derive closed-form formulas for the Mean Time To Failure of each topology. The results allow us to indicate the best topology for each failure scenario. In particular, we conclude that BCube is more robust to link failures than the other topologies, whereas DCell has the most robust topology when considering switch failures. Additionally, we show that all considered alternative topologies outperform a three-layer topology for both types of failures. We also determine to which extent the robustness of BCube and DCell is influenced by the number of network interfaces per server. 	
1606.00521v1	http://arxiv.org/pdf/1606.00521v1	2016	Initial and Eventual Software Quality Relating to Continuous Integration   in GitHub	Yue Yu|Bogdan Vasilescu|Huaimin Wang|Vladimir Filkov|Premkumar Devanbu	  The constant demand for new features and bug fixes are forcing software projects to shorten cycles and deliver updates ever faster, while sustaining software quality. The availability of inexpensive, virtualized, cloud-computing has helped shorten schedules, by enabling continuous integration (CI) on demand. Platforms like GitHub support CI in-the-cloud. In projects using CI, a user submitting a pull request triggers a CI step. Besides speeding up build and test, this fortuitously creates voluminous archives of build and test successes and failures. CI is a relatively new phenomenon, and these archives allow a detailed study of CI. How many problems are exposed? Where do they occur? What factors affect CI failures? Does the "initial quality" as ascertained by CI predict how many bugs will later appear ("eventual quality") in the code? In this paper, we undertake a large-scale, fine resolution study of these records, to better understand CI processes, the nature, and predictors of CI failures, and the relationship of CI failures to the eventual quality of the code. We find that: a) CI failures appear to be concentrated in a few files, just like normal bugs; b) CI failures are not very highly correlated with eventual failures; c) The use of CI in a pull request doesn't necessarily mean the code in that request is of good quality. 	
1708.08309v2	http://arxiv.org/pdf/1708.08309v2	2017	A Dual Digraph Approach for Leaderless Atomic Broadcast (Extended   Version)	Marius Poke|Colin W. Glass	  Many distributed systems work on a common shared state; in such systems, distributed agreement is necessary for consistency. With an increasing number of servers, systems become more susceptible to single-server failures, increasing the relevance of fault-tolerance. Atomic broadcast enables fault-tolerant distributed agreement, yet it is costly to solve. Most practical algorithms entail linear work per broadcast message. AllConcur -- a leaderless approach -- reduces the work by connecting the servers via a resilient overlay network; yet, this resiliency entails redundancy, which reduces performance. In this work, we propose AllConcur+, an extension of AllConcur. During intervals with no failures, it uses an overlay network with no redundancy and automatically switches to a resilient overlay network when failures occur. Our performance estimation shows that if no failures occur, AllConcur+ achieves up to 10x higher throughput and up to 5x lower latency than AllConcur. In the presence of occasional failures, AllConcur+ still outperforms AllConcur significantly. In the worst case, AllConcur+'s performance is worse than AllConcur's, yet, this requires frequent failures at very specific intervals. Thus, for realistic use cases, leveraging redundancy-free distributed agreement during intervals with no failures, increases the expected performance. 	
0712.4258v2	http://arxiv.org/pdf/0712.4258v2	2008	Two dogmas about quantum mechanics	Jeffrey Bub|Itamar Pitowsky	  We argue that the intractable part of the measurement problem -- the 'big' measurement problem -- is a pseudo-problem that depends for its legitimacy on the acceptance of two dogmas. The first dogma is John Bell's assertion that measurement should never be introduced as a primitive process in a fundamental mechanical theory like classical or quantum mechanics, but should always be open to a complete analysis, in principle, of how the individual outcomes come about dynamically. The second dogma is the view that the quantum state has an ontological significance analogous to the significance of the classical state as the 'truthmaker' for propositions about the occurrence and non-occurrence of events, i.e., that the quantum state is a representation of physical reality. We show how both dogmas can be rejected in a realist information-theoretic interpretation of quantum mechanics as an alternative to the Everett interpretation. The Everettian, too, regards the 'big' measurement problem as a pseudo-problem, because the Everettian rejects the assumption that measurements have definite outcomes, in the sense that one particular outcome, as opposed to other possible outcomes, actually occurs in a quantum measurement process. By contrast with the Everettians, we accept that measurements have definite outcomes. By contrast with the Bohmians and the GRW 'collapse' theorists who add structure to the theory and propose dynamical solutions to the 'big' measurement problem, we take the problem to arise from the failure to see the significance of Hilbert space as a new kinematic framework for the physics of an indeterministic universe, in the sense that Hilbert space imposes kinematic (i.e., pre-dynamic) objective probabilistic constraints on correlations between events. 	
1103.1649v3	http://arxiv.org/pdf/1103.1649v3	2011	Semi-Inclusive Charged-Pion Electroproduction off Protons and Deuterons:   Cross Sections, Ratios and Access to the Quark-Parton Model at Low Energies	R. Asaturyan|R. Ent|H. Mkrtchyan|T. Navasardyan|V. Tadevosyan|G. S. Adams|A. Ahmidouch|T. Angelescu|J. Arrington|A. Asaturyan|O. K. Baker|N. Benmouna|C. Bertoncini|H. P. Blok|W. U. Boeglin|P. E. Bosted|H. Breuer|M. E. Christy|S. H. Connell|Y. Cui|M. M. Dalton|S. Danagoulian|D. Day|J. A. Dunne|D. Dutta|N. El Khayari|H. C. Fenker|V. V. Frolov|L. Gan|D. Gaskell|K. Hafidi|W. Hinton|R. J. Holt|T. Horn|G. M. Huber|E. Hungerford|X. Jiang|M. Jones|K. Joo|N. Kalantarians|J. J. Kelly|C. E. Keppel|V. Kubarovsky|Y. Li|Y. Liang|D. Mack|S. P. Malace|P. Markowitz|E. McGrath|P. McKee|D. G. Meekins|A. Mkrtchyan|B. Moziak|G. Niculescu|I. Niculescu|A. K. Opper|T. Ostapenko|P. E. Reimer|J. Reinhold|J. Roche|S. E. Rock|E. Schulte|E. Segbefia|C. Smith|G. R. Smith|P. Stoler|L. Tang|M. Ungaro|A. Uzzle|S. Vidakovic|A. Villano|W. F. Vulcan|M. Wang|G. Warren|F. R. Wesselmann|B. Wojtsekhowski|S. A. Wood|C. Xu|L. Yuan|X. Zheng	  A large set of cross sections for semi-inclusive electroproduction of charged pions ($\pi^\pm$) from both proton and deuteron targets was measured. The data are in the deep-inelastic scattering region with invariant mass squared $W^2$ > 4 GeV$^2$ and range in four-momentum transfer squared $2 < Q^2 < 4$ (GeV/c)$^2$, and cover a range in the Bjorken scaling variable 0.2 < x < 0.6. The fractional energy of the pions spans a range 0.3 < z < 1, with small transverse momenta with respect to the virtual-photon direction, $P_t^2 < 0.2$ (GeV/c)$^2$. The invariant mass that goes undetected, $M_x$ or W', is in the nucleon resonance region, W' < 2 GeV. The new data conclusively show the onset of quark-hadron duality in this process, and the relation of this phenomenon to the high-energy factorization ansatz of electron-quark scattering and subsequent quark --> pion production mechanisms. The x, z and $P_t^2$ dependences of several ratios (the ratios of favored-unfavored fragmentation functions, charged pion ratios, deuteron-hydrogen and aluminum-deuteron ratios for $\pi^+$ and $\pi^-$) have been studied. The ratios are found to be in good agreement with expectations based upon a high-energy quark-parton model description. We find the azimuthal dependences to be small, as compared to exclusive pion electroproduction, and consistent with theoretical expectations based on tree-level factorization in terms of transverse-momentum-dependent parton distribution and fragmentation functions. In the context of a simple model, the initial transverse momenta of $d$ quarks are found to be slightly smaller than for $u$ quarks, while the transverse momentum width of the favored fragmentation function is about the same as for the unfavored one, and both fragmentation widths are larger than the quark widths. 	
1609.07395v1	http://arxiv.org/pdf/1609.07395v1	2016	A study of cascading failures in real and synthetic power grid   topologies using DC power flows	Russell Spiewak|Sergey V. Buldyrev|Yakir Forman|Saleh Soltan|Gil Zussman	  Using the linearized DC power flow model, we study cascading failures and their spatial and temporal properties in the US Western Interconnect (USWI) power grid. We also introduce the preferential Degree And Distance Attachment (DADA) model, with similar degree distributions, resistances, and currents to the USWI. We investigate the behavior of both grids resulting from the failure of a single line. We find that the DADA model and the USWI model react very similarly to that failure, and that their blackout characteristics resemble each other. In many cases, the failure of a single line can cause cascading failures, which impact the entire grid. We characterize the resilience of the grid by three parameters, the most important of which is tolerance ${\alpha}$, which is the ratio of the maximal load a line can carry to its initial load. We characterize a blackout by its yield, which we define as the ratio of the final to the initial consumed currents. We find that if ${\alpha}\leq2$, the probability of a large blackout occurring is very small. By contrast, in a broad range of $1 < {\alpha} < 2$, the initial failure of a single line can result, with a high probability, in cascading failures leading to a massive blackout with final yield less than 80%. The yield has a bimodal distribution typical of a first-order transition, i.e., the failure of a randomly selected line leads either to an insignificant current reduction or to a major blackout. We find that there is a latent period in the development of major blackouts during which few lines are overloaded, and the yield remains high. The duration of this latent period is proportional to the tolerance. The existence of the latent period suggests that intervention during early time steps of a cascade can significantly reduce the risk of a major blackout. 	
0008388v1	http://arxiv.org/pdf/cond-mat/0008388v1	2000	Atomistics of Tensile Failure in Fused Silica: Weakest Link Models   Revisited	J. I. Katz	  In weakest link models the failure of a single microscopic element of a brittle material causes the failure of an entire macroscopic specimen, just as a chain fails if one link fails. Pristine samples of glass, such as optical communications fiber, approach their ideal strength, and their brittle tensile failure has been described by this model. The statistics of weakest link models are calculable in terms of the statistics of the individual links, which, unfortunately, are poorly known. Use of the skewness of the failure distribution may permit simultaneous determination of the statistics of the individual weak links and of their number density, which indicates their physical origin. However, the applicability of weakest link models to real materials remains unproven. 	
0506725v5	http://arxiv.org/pdf/cond-mat/0506725v5	2005	Entropy Optimization of Scale-Free Networks Robustness to Random   Failures	Bing Wang|Huanwen Tang|Chonghui Guo|Zhilong Xiu	  Many networks are characterized by highly heterogeneous distributions of links, which are called scale-free networks and the degree distributions follow $p(k)\sim ck^{-\alpha}$. We study the robustness of scale-free networks to random failures from the character of their heterogeneity. Entropy of the degree distribution can be an average measure of a network's heterogeneity. Optimization of scale-free network robustness to random failures with average connectivity constant is equivalent to maximize the entropy of the degree distribution. By examining the relationship of entropy of the degree distribution, scaling exponent and the minimal connectivity, we get the optimal design of scale-free network to random failures. We conclude that entropy of the degree distribution is an effective measure of network's resilience to random failures. 	
0411297v1	http://arxiv.org/pdf/math/0411297v1	2004	On Representing the Mean Residual Life in Terms of the Failure Rate	Ramesh C. Gupta|David M. Bradley	  In survival or reliability studies, the mean residual life or life expectancy is an important characteristic of the model. Whereas the failure rate can be expressed quite simply in terms of the mean residual life and its derivative, the inverse problem--namely that of expressing the mean residual life in terms of the failure rate--typically involves an integral of a complicated expression. In this paper, we obtain simple expressions for the mean residual life in terms of the failure rate for certain classes of distributions which subsume many of the standard cases. Several results in the literature can be obtained using our approach. Additionally, we develop an expansion for the mean residual life in terms of Gaussian probability functions for a broad class of ultimately increasing failure rate distributions. Some examples are provided to illustrate the procedure. 	
0509100v1	http://arxiv.org/pdf/physics/0509100v1	2005	Constitutive Laws and Failure Models for Compact Bones Subjected to   Dynamic Loading	Martine Pithioux|P. Chabrand|M. Jean	  Many biological tissues, such as bones and ligaments, are fibrous. The geometrical structure of these tissues shows that they exhibit a similar hierarchy in their ultra-structure and macro-structure. The aim of this work is to develop a model to study the failure of fibrous structures subjected to dynamic loading. The important feature of this model is that it describes failure in terms of the loss of cohesion between fibres. We have developed a model based on the lamellar structure of compact bone with fibres oriented at 0 degrees, 45 degrees and 90 degrees to the longitudinal axis of the bone, and have studied the influence of the model parameters on the failure process. Bone porosity and joint stress force at failure were found to be the most significant parameters. Using least square resolution, we deduced a phenomenological model of the lamellar structure. Finally, experimental results were found to be comparable with our numerical model. 	
0704.1952v2	http://arxiv.org/pdf/0704.1952v2	2008	Dynamic Effects Increasing Network Vulnerability to Cascading Failures	Ingve Simonsen|Lubos Buzna|Karsten Peters|Stefan Bornholdt|Dirk Helbing	  We study cascading failures in networks using a dynamical flow model based on simple conservation and distribution laws to investigate the impact of transient dynamics caused by the rebalancing of loads after an initial network failure (triggering event). It is found that considering the flow dynamics may imply reduced network robustness compared to previous static overload failure models. This is due to the transient oscillations or overshooting in the loads, when the flow dynamics adjusts to the new (remaining) network structure. We obtain {\em upper} and {\em lower} limits to network robustness, and it is shown that {\it two} time scales $\tau$ and $\tau_0$, defined by the network dynamics, are important to consider prior to accurately addressing network robustness or vulnerability. The robustness of networks showing cascading failures is generally determined by a complex interplay between the network topology and flow dynamics, where the ratio $\chi=\tau/\tau_0$ determines the relative role of the two of them. 	
0811.1301v1	http://arxiv.org/pdf/0811.1301v1	2008	Distributed Algorithms for Computing Alternate Paths Avoiding Failed   Nodes and Links	Amit M. Bhosle|Teofilo F. Gonzalez	  A recent study characterizing failures in computer networks shows that transient single element (node/link) failures are the dominant failures in large communication networks like the Internet. Thus, having the routing paths globally recomputed on a failure does not pay off since the failed element recovers fairly quickly, and the recomputed routing paths need to be discarded. In this paper, we present the first distributed algorithm that computes the alternate paths required by some "proactive recovery schemes" for handling transient failures. Our algorithm computes paths that avoid a failed node, and provides an alternate path to a particular destination from an upstream neighbor of the failed node. With minor modifications, we can have the algorithm compute alternate paths that avoid a failed link as well. To the best of our knowledge all previous algorithms proposed for computing alternate paths are centralized, and need complete information of the network graph as input to the algorithm. 	
0905.1778v1	http://arxiv.org/pdf/0905.1778v1	2009	Encoding of Network Protection Codes Against Link and Node Failures Over   Finite Fields	Salah A. Aly|Ahmed E. Kamal	  Link and node failures are common two fundamental problems that affect operational networks. Hence, protection of communication networks is essential to increase their reliability, performance, and operations. Much research work has been done to protect against link and node failures, and to provide reliable solutions based on pre-defined provision or dynamic restoration of the domain. In this paper we develop network protection strategies against multiple link failures using network coding and joint capacities. In these strategies, the source nodes apply network coding for their transmitted data to provide backup copies for recovery at the receivers' nodes. Such techniques can be applied to optical, IP, and mesh networks. The encoding operations of protection codes are defined over finite fields. Furthermore, the normalized capacity of the communication network is given by $(n-t)/n$ in case of $t$ link failures. In addition, a bound on the minimum required field size is derived. 	
1006.0671v2	http://arxiv.org/pdf/1006.0671v2	2010	Predicting Failures in Power Grids: The Case of Static Overloads	Michael Chertkov|Feng Pan|Mikhail G. Stepanov	  Here we develop an approach to predict power grid weak points, and specifically to efficiently identify the most probable failure modes in static load distribution for a given power network. This approach is applied to two examples: Guam's power system and also the IEEE RTS-96 system, both modeled within the static Direct Current power flow model. Our algorithm is a power network adaption of the worst configuration heuristics, originally developed to study low probability events in physics and failures in error-correction. One finding is that, if the normal operational mode of the grid is sufficiently healthy, the failure modes, also called instantons, are sufficiently sparse, i.e. the failures are caused by load fluctuations at only a few buses. The technique is useful for discovering weak links which are saturated at the instantons. It can also identify generators working at the capacity and generators under capacity, thus providing predictive capability for improving the reliability of any power network. 	
1006.5101v1	http://arxiv.org/pdf/1006.5101v1	2010	Probabilistic Model-Based Safety Analysis	Matthias Güdemann|Frank Ortmeier	  Model-based safety analysis approaches aim at finding critical failure combinations by analysis of models of the whole system (i.e. software, hardware, failure modes and environment). The advantage of these methods compared to traditional approaches is that the analysis of the whole system gives more precise results. Only few model-based approaches have been applied to answer quantitative questions in safety analysis, often limited to analysis of specific failure propagation models, limited types of failure modes or without system dynamics and behavior, as direct quantitative analysis is uses large amounts of computing resources. New achievements in the domain of (probabilistic) model-checking now allow for overcoming this problem.   This paper shows how functional models based on synchronous parallel semantics, which can be used for system design, implementation and qualitative safety analysis, can be directly re-used for (model-based) quantitative safety analysis. Accurate modeling of different types of probabilistic failure occurrence is shown as well as accurate interpretation of the results of the analysis. This allows for reliable and expressive assessment of the safety of a system in early design stages. 	
1106.0489v2	http://arxiv.org/pdf/1106.0489v2	2011	Recovery from Link Failures in Networks with Arbitrary Topology via   Diversity Coding	S. N. Avci|X. Hu|E. Ayanoglu	  Link failures in wide area networks are common. To recover from such failures, a number of methods such as SONET rings, protection cycles, and source rerouting have been investigated. Two important considerations in such approaches are the recovery time and the needed spare capacity to complete the recovery. Usually, these techniques attempt to achieve a recovery time less than 50 ms. In this paper we introduce an approach that provides link failure recovery in a hitless manner, or without any appreciable delay. This is achieved by means of a method called diversity coding. We present an algorithm for the design of an overlay network to achieve recovery from single link failures in arbitrary networks via diversity coding. This algorithm is designed to minimize spare capacity for recovery. We compare the recovery time and spare capacity performance of this algorithm against conventional techniques in terms of recovery time, spare capacity, and a joint metric called Quality of Recovery (QoR). QoR incorporates both the spare capacity percentages and worst case recovery times. Based on these results, we conclude that the proposed technique provides much shorter recovery times while achieving similar extra capacity, or better QoR performance overall. 	
1110.2289v1	http://arxiv.org/pdf/1110.2289v1	2011	Enhancing congestion control to address link failure loss over mobile   ad-hoc network	Mohammad Amin Kheirandish Fard|Sasan Karamizadeh|Mohammad Aflaki	  Standard congestion control cannot detect link failure losses which occur due to mobility and power scarcity in multi-hop Ad-Hoc network (MANET). Moreover, successive executions of Back-off algorithm deficiently grow Retransmission Timeout (RTO) exponentially for new route. The importance of detecting and responding link failure losses is to prevent sender from remaining idle unnecessarily and manage number of packet retransmission overhead. In contrast to Cross-layer approaches which require feedback information from lower layers, this paper operates purely in Transport layer. This paper explores an end-to-end threshold-based algorithm which enhances congestion control to address link failure loss in MANET. It consists of two phases. First, threshold-based loss classification algorithm distinguishes losses due to link failure by estimating queue usage based on Relative One-way Trip Time (ROTT). Second phase adjusts RTO for new route by comparing capabilities of new route to the broken route using available information in Transport layer such as ROTT and number of hops. 	
1203.0415v1	http://arxiv.org/pdf/1203.0415v1	2012	On Compositional Reasoning for Guaranteeing Probabilistic Properties	Jan Olaf Blech	  We present a framework to formally describe probabilistic system behavior and symbolically reason about it. In particular we aim at reasoning about possible failures and fault tolerance. We regard systems which are composed of different units: sensors, computational parts and actuators. Considering worst-case failure behavior of system components, our framework is most suited to derive reliability guarantees for composed systems. The behavior of system components is modeled using monad like constructs that serve as an abstract representation for system behavior. We introduce rules to reason about these representations and derive results like guaranteed upper bounds for system failure. Our approach is characterized by the fact that we do not just map a certain component to a failure probability, but regard distributions of error behavior and their evolvement over system runs. This serves as basis for deriving probabilities of events, in particular failure probabilities. The work presented in this paper slightly extends a complete framework and a case study which has been previously published. One focus of this report is a more detailed explanation of definitions and a more comprehensive description of examples. 	
1203.4324v3	http://arxiv.org/pdf/1203.4324v3	2012	Distributed Consensus Resilient to Both Crash Failures and Strategic   Manipulations	Xiaohui Bei|Wei Chen|Jialin Zhang	  In this paper, we study distributed consensus in synchronous systems subject to both unexpected crash failures and strategic manipulations by rational agents in the system. We adapt the concept of collusion-resistant Nash equilibrium to model protocols that are resilient to both crash failures and strategic manipulations of a group of colluding agents. For a system with $n$ distributed agents, we design a deterministic protocol that tolerates 2 colluding agents and a randomized protocol that tolerates $n - 1$ colluding agents, and both tolerate any number of failures. We also show that if colluders are allowed an extra communication round after each synchronous round, there is no protocol that can tolerate even 2 colluding agents and 1 crash failure. 	
1204.2465v1	http://arxiv.org/pdf/1204.2465v1	2012	Fast emergency paths schema to overcome transient link failures in ospf   routing	Fernando Barreto|Emilio C. G. Wille|Luiz Nacamura Jr	  A reliable network infrastructure must be able to sustain traffic flows, even when a failure occurs and changes the network topology. During the occurrence of a failure, routing protocols, like OSPF, take from hundreds of milliseconds to various seconds in order to converge. During this convergence period, packets might traverse a longer path or even a loop. An even worse transient behaviour is that packets are dropped even though destinations are reachable. In this context, this paper describes a proactive fast rerouting approach, named Fast Emergency Paths Schema (FEP-S), to overcome problems originating from transient link failures in OSPF routing. Extensive experiments were done using several network topologies with different dimensionality degrees. Results show that the recovery paths, obtained by FEPS, are shorter than those from other rerouting approaches and can improve the network reliability by reducing the packet loss rate during the routing protocols convergence caused by a failure. 	
1209.6484v1	http://arxiv.org/pdf/1209.6484v1	2012	Vulnerability Management for an Enterprise Resource Planning System	Shivani Goel|Ravi Kiran|Deepak Garg	  Enterprise resource planning (ERP) systems are commonly used in technical educational institutions(TEIs). ERP systems should continue providing services to its users irrespective of the level of failure. There could be many types of failures in the ERP systems. There are different types of measures or characteristics that can be defined for ERP systems to handle the levels of failure. Here in this paper, various types of failure levels are identified along with various characteristics which are concerned with those failures. The relation between all these is summarized. The disruptions causing vulnerabilities in TEIs are identified .A vulnerability management cycle has been suggested along with many commercial and open source vulnerability management tools. The paper also highlights the importance of resiliency in ERP systems in TEIs. 	
1211.3792v1	http://arxiv.org/pdf/1211.3792v1	2012	Modeling Repairs of Systems with a Bathtub-Shaped Failure Rate Function	Sima Varnosafaderani|Stefanka Chukova	  Most of the reliability literature on modeling the effect of repairs on systems assumes the failure rate functions are monotonically increasing. For systems with non-monotonic failure rate functions, most models deal with minimal repairs (which do not affect the working condition of the system) or replacements (which return the working condition to that of a new and identical system). We explore a new approach to model repairs of a system with a non-monotonic failure rate function; in particular, we consider systems with a bathtub-shaped failure rate function. We propose a repair model specified in terms of modifications to the virtual age function of the system, while preserving the usual definitions of the types of repair (minimal, imperfect and perfect repairs) and distinguishing between perfect repair and replacement. In addition, we provide a numerical illustration of the proposed repair model. 	
1301.2055v4	http://arxiv.org/pdf/1301.2055v4	2013	A Cascading Failure Model by Quantifying Interactions	Junjian Qi|Shengwei Mei	  Cascading failures triggered by trivial initial events are encountered in many complex systems. It is the interaction and coupling between components of the system that causes cascading failures. We propose a simple model to simulate cascading failure by using the matrix that determines how components interact with each other. A careful comparison is made between the original cascades and the simulated cascades by the proposed model. It is seen that the model can capture general features of the original cascades, suggesting that the interaction matrix can well reflect the relationship between components. An index is also defined to identify important links and the distribution follows an obvious power law. By eliminating a small number of most important links the risk of cascading failures can be significantly mitigated, which is dramatically different from getting rid of the same number of links randomly. 	
1302.3344v2	http://arxiv.org/pdf/1302.3344v2	2013	CORE: Augmenting Regenerating-Coding-Based Recovery for Single and   Concurrent Failures in Distributed Storage Systems	Runhui Li|Jian Lin|Patrick P. C. Lee	  Data availability is critical in distributed storage systems, especially when node failures are prevalent in real life. A key requirement is to minimize the amount of data transferred among nodes when recovering the lost or unavailable data of failed nodes. This paper explores recovery solutions based on regenerating codes, which are shown to provide fault-tolerant storage and minimum recovery bandwidth. Existing optimal regenerating codes are designed for single node failures. We build a system called CORE, which augments existing optimal regenerating codes to support a general number of failures including single and concurrent failures. We theoretically show that CORE achieves the minimum possible recovery bandwidth for most cases. We implement CORE and evaluate our prototype atop a Hadoop HDFS cluster testbed with up to 20 storage nodes. We demonstrate that our CORE prototype conforms to our theoretical findings and achieves recovery bandwidth saving when compared to the conventional recovery approach based on erasure codes. 	
1308.5474v2	http://arxiv.org/pdf/1308.5474v2	2013	Changes in Cascading Failure Risk with Generator Dispatch Method and   System Load Level	Pooya Rezaei|Paul D. H. Hines	  Industry reliability rules increasingly require utilities to study and mitigate cascading failure risk in their system. Motivated by this, this paper describes how cascading failure risk, in terms of expected blackout size, varies with power system load level and pre-contingency dispatch. We used Monte Carlo sampling of random branch outages to generate contingencies, and a model of cascading failure to estimate blackout sizes. The risk associated with different blackout sizes was separately estimated in order to separate small, medium, and large blackout risk. Results from $N-1$ secure models of the IEEE RTS case and a 2383 bus case indicate that blackout risk does not always increase with load level monotonically, particularly for large blackout risk. The results also show that risk is highly dependent on the method used for generator dispatch. Minimum cost methods of dispatch can result in larger long distance power transfers, which can increase cascading failure risk. 	
1312.4976v1	http://arxiv.org/pdf/1312.4976v1	2013	Analysis of Asteroid (216) Kleopatra using dynamical and structural   constraints	Masatoshi Hirabayashi|Daniel J. Scheeres	  Given the spin state by Magnusson (1990), the shape model by Ostro et al. (2000), and the mass by Descamps et al. (2011), this paper evaluates a dynamically and structurally stable size of Asteroid (216) Kleopatra. In particular, we investigate two different failure modes: material shedding from the surface and structural failure of the internal body. We construct zero-velocity curves in the vicinity of this asteroid to determine surface shedding, while we utilize a limit analysis to calculate the lower and upper bounds of structural failure under the zero-cohesion assumption. Surface shedding does not occur at the current spin period (5.385 hr) and cannot directly initiate the formation of the satellites. On the other hand, this body may be close to structural failure; in particular, the neck may be situated near a plastic state. In addition, the neck's sensitivity to structural failure changes as the body size varies. We conclude that plastic deformation has probably occurred around the neck part in the past. If the true size of this body is established through additional measurements, this method will provide strong constraints on the current friction angle for the body. 	
1405.0455v1	http://arxiv.org/pdf/1405.0455v1	2014	Epidemic and Cascading Survivability of Complex Networks	Marc Manzano|Eusebi Calle|Jordi Ripoll|Anna Manolova Fagertun|Victor Torres-Padrosa|Sakshi Pahwa|Caterina Scoglio	  Our society nowadays is governed by complex networks, examples being the power grids, telecommunication networks, biological networks, and social networks. It has become of paramount importance to understand and characterize the dynamic events (e.g. failures) that might happen in these complex networks. For this reason, in this paper, we propose two measures to evaluate the vulnerability of complex networks in two different dynamic multiple failure scenarios: epidemic-like and cascading failures. Firstly, we present \emph{epidemic survivability} ($ES$), a new network measure that describes the vulnerability of each node of a network under a specific epidemic intensity. Secondly, we propose \emph{cascading survivability} ($CS$), which characterizes how potentially injurious a node is according to a cascading failure scenario. Then, we show that by using the distribution of values obtained from $ES$ and $CS$ it is possible to describe the vulnerability of a given network. We consider a set of 17 different complex networks to illustrate the suitability of our proposals. Lastly, results reveal that distinct types of complex networks might react differently under the same multiple failure scenario. 	
1405.4342v1	http://arxiv.org/pdf/1405.4342v1	2014	The robust-yet-fragile nature of interdependent networks	Fei Tan|Yongxiang Xia	  Interdependent networks have been shown to be extremely vulnerable based on the percolation model. Parshani et. al further indicated that the more inter-similar networks are, the more robust they are to random failure. Our understanding of how coupling patterns shape and impact the cascading failures of loads in interdependent networks is limited, but is essential for the design and optimization of the real-world interdependent networked systems. This question, however, is largely unexplored. In this paper, we address this question by investigating the robustness of interdependent ER random graphs and BA scale-free networks under both random failure and intentional attack. It is found that interdependent ER random graphs are robust-yet-fragile under both random failures and intentional attack. Interdependent BA scale-free networks, however, are only robust-yet-fragile under random failure but fragile under intentional attack. These results advance our understanding of the robustness of interdependent networks significantly. 	
1406.3266v1	http://arxiv.org/pdf/1406.3266v1	2014	Event and Anomaly Detection Using Tucker3 Decomposition	Hadi Fanaee-T|Márcia D. B. Oliveira|João Gama|Simon Malinowski|Ricardo Morla	  Failure detection in telecommunication networks is a vital task. So far, several supervised and unsupervised solutions have been provided for discovering failures in such networks. Among them unsupervised approaches has attracted more attention since no label data is required. Often, network devices are not able to provide information about the type of failure. In such cases the type of failure is not known in advance and the unsupervised setting is more appropriate for diagnosis. Among unsupervised approaches, Principal Component Analysis (PCA) is a well-known solution which has been widely used in the anomaly detection literature and can be applied to matrix data (e.g. Users-Features). However, one of the important properties of network data is their temporal sequential nature. So considering the interaction of dimensions over a third dimension, such as time, may provide us better insights into the nature of network failures. In this paper we demonstrate the power of three-way analysis to detect events and anomalies in time-evolving network data. 	
1406.4261v1	http://arxiv.org/pdf/1406.4261v1	2014	Failure Inference and Optimization for Step Stress Model Based on   Bivariate Wiener Model	S. Shemehsavar|Morteza Amini	  In this paper, we consider the situation under a life test, in which the failure time of the test units are not related deterministically to an observable stochastic time varying covariate. In such a case, the joint distribution of failure time and a marker value would be useful for modeling the step stress life test. The problem of accelerating such an experiment is considered as the main aim of this paper. We present a step stress accelerated model based on a bivariate Wiener process with one component as the latent (unobservable) degradation process, which determines the failure times and the other as a marker process, the degradation values of which are recorded at times of failure. Parametric inference based on the proposed model is discussed and the optimization procedure for obtaining the optimal time for changing the stress level is presented. The optimization criterion is to minimize the approximate variance of the maximum likelihood estimator of a percentile of the products' lifetime distribution. 	
1406.7264v1	http://arxiv.org/pdf/1406.7264v1	2014	Repairable Block Failure Resilient Codes	Gokhan Calis|O. Ozan Koyluoglu	  In large scale distributed storage systems (DSS) deployed in cloud computing, correlated failures resulting in simultaneous failure (or, unavailability) of blocks of nodes are common. In such scenarios, the stored data or a content of a failed node can only be reconstructed from the available live nodes belonging to available blocks. To analyze the resilience of the system against such block failures, this work introduces the framework of Block Failure Resilient (BFR) codes, wherein the data (e.g., file in DSS) can be decoded by reading out from a same number of codeword symbols (nodes) from each available blocks of the underlying codeword. Further, repairable BFR codes are introduced, wherein any codeword symbol in a failed block can be repaired by contacting to remaining blocks in the system. Motivated from regenerating codes, file size bounds for repairable BFR codes are derived, trade-off between per node storage and repair bandwidth is analyzed, and BFR-MSR and BFR-MBR points are derived. Explicit codes achieving these two operating points for a wide set of parameters are constructed by utilizing combinatorial designs, wherein the codewords of the underlying outer codes are distributed to BFR codeword symbols according to projective planes. 	
1407.4953v1	http://arxiv.org/pdf/1407.4953v1	2014	A Topological Investigation of Phase Transitions of Cascading Failures   in Power Grids	Yakup Koç|Martijn Warnier|Piet Van Mieghem|Robert E. Kooij|Frances M. T. Brazier	  Cascading failures are one of the main reasons for blackouts in electric power transmission grids. The economic cost of such failures is in the order of tens of billion dollars annually. The loading level of power system is a key aspect to determine the amount of the damage caused by cascading failures. Existing studies show that the blackout size exhibits phase transitions as the loading level increases. This paper investigates the impact of the topology of a power grid on phase transitions in its robustness. Three spectral graph metrics are considered: spectral radius, effective graph resistance and algebraic connectivity. Experimental results from a model of cascading failures in power grids on the IEEE power systems demonstrate the applicability of these metrics to design/optimize a power grid topology for an enhanced phase transition behavior of the system. 	
1409.0025v2	http://arxiv.org/pdf/1409.0025v2	2014	Geometric control of failure behavior in perforated sheets	Michelle M. Driscoll	  Adding perforations to a continuum sheet allows new modes of deformation, and thus modifies its elastic behavior. The failure behavior of such a perforated sheet is explored, using a model experimental system: a material containing a one-dimensional array of rectangular holes. In this model system, a transition in failure mode occurs as the spacing and aspect ratio of the holes are varied: rapid failure via a running crack is completely replaced by quasi-static failure which proceeds via the breaking of struts at random positions in the array of holes. I demonstrate that this transition can be connected to the loss of stress enhancement which occurs as the material geometry is modified. 	
1410.6945v1	http://arxiv.org/pdf/1410.6945v1	2014	What The Trace Distance Security Criterion in Quantum Key Distribution   Does And Does Not Guarantee	Horace Yuen	  Cryptographic security of quantum key distribution is currently based on a trace distance criterion. The widespread misinterpretation of the criterion as failure probability and also its actual scope have been discussed previously. Recently its distinguishability advantage interpretation is re-emphasized as an operational guarantee, and the failure probability misinterpretation is maintained with a further failure probability per bit interpretation. In this paper we explain the basic perpetuating error as a confusion on the correspondence between mathematics and reality. We note that the assignment of equal a priori probability of 1/2 to the real and ideal situations for distinguishability advantage would not lead to operational guarantee. We explain why operational guarantee in terms of Eve's probabilities of getting various key bits is necessary for security, and why the failure probability interpretation misrepresents the security situation. The scope and limits of the trace distance guarantee are summarized. It is shown that there would have been no security problem to begin with if the failure probability per bit interpretation validity. 	
1508.03821v1	http://arxiv.org/pdf/1508.03821v1	2015	Vertical modeling: analysis of competing risks data with a cure   proportion	M. A. Nicolaie|J. M. G. Taylor|C. Legrand	  In this paper, we extend the vertical modeling approach for the analysis of survival data with competing risks to incorporate a cured fraction in the population, that is, a proportion of the population for which none of the competing events can occur. The proposed method has three components: the proportion of cure, the risk of failure, irrespective of the cause, and the relative risk of a certain cause of failure, given a failure occurred. Covariates may affect each of these components. An appealing aspect of the method is that it is a natural extension to competing risks of the semi-parametric mixture cure model in ordinary survival analysis; thus, causes of failure are assigned only if a failure occurs. This contrasts with the existing mixture cure model for competing risks of Larson and Dinse, which conditions at the onset on the future status presumably attained. Regression parameter estimates are obtained using an EM-algorithm. The performance of the estimators is evaluated in a simulation study. The method is illustrated using a melanoma cancer data set. 	
1509.04613v1	http://arxiv.org/pdf/1509.04613v1	2015	Gaussian process surrogates for failure detection: a Bayesian   experimental design approach	Hongqiao Wang|Guang Lin|Jinglai Li	  An important task of uncertainty quantification is to identify {the probability of} undesired events, in particular, system failures, caused by various sources of uncertainties. In this work we consider the construction of Gaussian {process} surrogates for failure detection and failure probability estimation. In particular, we consider the situation that the underlying computer models are extremely expensive, and in this setting, determining the sampling points in the state space is of essential importance. We formulate the problem as an optimal experimental design for Bayesian inferences of the limit state (i.e., the failure boundary) and propose an efficient numerical scheme to solve the resulting optimization problem. In particular, the proposed limit-state inference method is capable of determining multiple sampling points at a time, and thus it is well suited for problems where multiple computer simulations can be performed in parallel. The accuracy and performance of the proposed method is demonstrated by both academic and practical examples. 	
1603.04335v1	http://arxiv.org/pdf/1603.04335v1	2016	The landscape of software failure cause models	Lena Feinbube|Peter Tröger|Andreas Polze	  The software engineering field has a long history of classifying software failure causes. Understanding them is paramount for fault injection, focusing testing efforts or reliability prediction. Since software fails in manifold complex ways, a broad range of software failure cause models is meanwhile published in dependability literature. We present the results of a meta-study that classifies publications containing a software failure cause model in topic clusters. Our results structure the research field and can help to identify gaps. We applied the systematic mapping methodology for performing a repeatable analysis.   We identified 156 papers presenting a model of software failure causes. Their examination confirms the assumption that a large number of the publications discusses source code defects only. Models of fault-activating state conditions and error states are rare. Research seems to be driven mainly by the need for better testing methods and code-based quality improvement. Other motivations such as online error detection are less frequently given. Mostly, the IEEE definitions or orthogonal defect classification is used as base terminology. The majority of use cases comes from web, safety- and security-critical applications. 	
1606.00955v2	http://arxiv.org/pdf/1606.00955v2	2016	Message Passing for Analysis and Resilient Design of Self-Healing   Interdependent Cyber-Physical Networks	Ali Behfarnia|Ali Eslami	  Coupling cyber and physical systems gives rise to numerous engineering challenges and opportunities. An important challenge is the contagion of failure from one system to another, that can lead to large scale cascading failures. On the other hand, self-healing ability emerges as a valuable opportunity where the overlay cyber network can cure failures in the underlying physical network. To capture both self-healing and contagion, we introduce a factor graph representation of inter-dependent cyber-physical systems in which factor nodes represent various node functionalities and the edges capture the interactions between the nodes. We develop a message passing algorithm to study the dynamics of failure propagation and healing in this representation. Through applying a fixed-point analysis to this algorithm, we investigate the network reaction to initial disruptions. Our analysis provides simple yet critical guidelines for choosing network parameters to achieve resiliency against cascading failures. 	
1608.01037v1	http://arxiv.org/pdf/1608.01037v1	2016	The robustness of interdependent networks under the interplay between   cascading failures and virus propagation	Dawei Zhao|Zhen Wang|Gaoxi Xiao|Bo Gao|Lianhai Wang	  Cascading failures and epidemic dynamics, as two successful application realms of network science, are usually investigated separately. How do they affect each other is still one open, interesting problem. In this letter, we couple both processes and put them into the framework of interdependent networks, where each network only supports one dynamical process. Of particular interest, they spontaneously form a feedback loop: virus propagation triggers cascading failures of systems while cascading failures suppress virus propagation. Especially, there exists crucial threshold of virus transmissibility, above which the interdependent networks collapse completely. In addition, the interdependent networks will be more vulnerable if the network supporting virus propagation has denser connections; otherwise the interdependent systems are robust against the change of connections in other layer(s). This discovery differs from previous framework of cascading failure in interdependent networks, where better robustness usually needs denser connections. Finally, to protect interdependent networks we also propose the control measures based on the identification capability. The larger this capability, more robustness the interdependent networks will be. 	
1608.06451v1	http://arxiv.org/pdf/1608.06451v1	2016	Failure Detection for Facial Landmark Detectors	Andreas Steger|Radu Timofte|Luc Van Gool	  Most face applications depend heavily on the accuracy of the face and facial landmarks detectors employed. Prediction of attributes such as gender, age, and identity usually completely fail when the faces are badly aligned due to inaccurate facial landmark detection. Despite the impressive recent advances in face and facial landmark detection, little study is on the recovery from and detection of failures or inaccurate predictions. In this work we study two top recent facial landmark detectors and devise confidence models for their outputs. We validate our failure detection approaches on standard benchmarks (AFLW, HELEN) and correctly identify more than 40% of the failures in the outputs of the landmark detectors. Moreover, with our failure detection we can achieve a 12% error reduction on a gender estimation application at the cost of a small increase in computation. 	
1609.01580v1	http://arxiv.org/pdf/1609.01580v1	2016	Using Natural Language Processing to Screen Patients with Active Heart   Failure: An Exploration for Hospital-wide Surveillance	Shu Dong|R Kannan Mutharasan|Siddhartha Jonnalagadda	  In this paper, we proposed two different approaches, a rule-based approach and a machine-learning based approach, to identify active heart failure cases automatically by analyzing electronic health records (EHR). For the rule-based approach, we extracted cardiovascular data elements from clinical notes and matched patients to different colors according their heart failure condition by using rules provided by experts in heart failure. It achieved 69.4% accuracy and 0.729 F1-Score. For the machine learning approach, with bigram of clinical notes as features, we tried four different models while SVM with linear kernel achieved the best performance with 87.5% accuracy and 0.86 F1-Score. Also, from the classification comparison between the four different models, we believe that linear models fit better for this problem. Once we combine the machine-learning and rule-based algorithms, we will enable hospital-wide surveillance of active heart failure through increased accuracy and interpretability of the outputs. 	
1609.06187v2	http://arxiv.org/pdf/1609.06187v2	2017	Determination of Bond Wire Failure Probabilities in Microelectronic   Packages	Thorben Casper|Ulrich Römer|Sebastian Schöps	  This work deals with the computation of industry-relevant bond wire failure probabilities in microelectronic packages. Under operating conditions, a package is subject to Joule heating that can lead to electrothermally induced failures. Manufacturing tolerances result, e.g., in uncertain bond wire geometries that often induce very small failure probabilities requiring a high number of Monte Carlo (MC) samples to be computed. Therefore, a hybrid MC sampling scheme that combines the use of an expensive computer model with a cheap surrogate is used. The fraction of surrogate evaluations is maximized using an iterative procedure, yielding accurate results at reduced cost. Moreover, the scheme is non-intrusive, i.e., existing code can be reused. The algorithm is used to compute the failure probability for an example package and the computational savings are assessed by performing a surrogate efficiency study. 	
1612.07002v1	http://arxiv.org/pdf/1612.07002v1	2016	A subset multicanonical Monte Carlo method for simulating rare failure   events	Xinjuan Chen|Jinglai Li	  Estimating failure probabilities of engineering systems is an important problem in many engineering fields. In this work we consider such problems where the failure probability is extremely small (e.g $\leq10^{-10}$). In this case, standard Monte Carlo methods are not feasible due to the extraordinarily large number of samples required. To address these problems, we propose an algorithm that combines the main ideas of two very powerful failure probability estimation approaches: the subset simulation (SS) and the multicanonical Monte Carlo (MMC) methods. Unlike the standard MMC which samples in the entire domain of the input parameter in each iteration, the proposed subset MMC algorithm adaptively performs MMC simulations in a subset of the state space and thus improves the sampling efficiency. With numerical examples we demonstrate that the proposed method is significantly more efficient than both of the SS and the MMC methods. Moreover, the proposed algorithm can reconstruct the complete distribution function of the parameter of interest and thus can provide more information than just the failure probabilities of the systems. 	
1701.08787v1	http://arxiv.org/pdf/1701.08787v1	2017	Vulnerability of Clustering under Node Failure in Complex Networks	Alan Kuhnle|Nam P. Nguyen|Thang N. Dinh|My T. Thai	  Robustness in response to unexpected events is always desirable for real-world networks. To improve the robustness of any networked system, it is important to analyze vulnerability to external perturbation such as random failures or adversarial attacks occurring to elements of the network. In this paper, we study an emerging problem in assessing the robustness of complex networks: the vulnerability of the clustering of the network to the failure of network elements. Specifically, we identify vertices whose failures will critically damage the network by degrading its clustering, evaluated through the average clustering coefficient. This problem is important because any significant change made to the clustering, resulting from element-wise failures, could degrade network performance such as the ability for information to propagate in a social network. We formulate this vulnerability analysis as an optimization problem, prove its NP-completeness and non-monotonicity, and we offer two algorithms to identify the vertices most important to clustering. Finally, we conduct comprehensive experiments in synthesized social networks generated by various well-known models as well as traces of real social networks. The empirical results over other competitive strategies show the efficacy of our proposed algorithms. 	
1704.06302v1	http://arxiv.org/pdf/1704.06302v1	2017	Quality of Service of an Asynchronous Crash-Recovery Leader Election   Algorithm	Vinícius A. Reis|Gustavo M. D. Vieira	  In asynchronous distributed systems it is very hard to assess if one of the processes taking part in a computation is operating correctly or has failed. To overcome this problem, distributed algorithms are created using unreliable failure detectors that capture in an abstract way timing assumptions necessary to assess the operating status of a process. One particular type of failure detector is a leader election, that indicates a single process that has not failed. The unreliability of these failure detectors means that they can make mistakes, however if they are to be used in practice there must be limits to the eventual behavior of these detectors. These limits are defined as the quality of service (QoS) provided by the detector. Many works have tackled the problem of creating failure detectors with predictable QoS, but only for crash-stop processes and synchronous systems. This paper presents and analyzes the behavior of a new leader election algorithm named NFD-L for the asynchronous crash-recovery failure model that is efficient in terms of its use of stable memory and message exchanges. 	
1705.05776v1	http://arxiv.org/pdf/1705.05776v1	2017	Shape optimization to decrease failure probability	Matthias Bolten|Hanno Gottschalk|Camilla Hahn|Mohamed Saadi	  Ceramic is a material frequently used in industry because of its favorable properties. Common approaches in shape optimization for ceramic structures aim to minimize the tensile stress acting on the component, as it is the main driver for failure. In contrast to this, we follow a more natural approach by minimizing the component's probability of failure under a given tensile load. Since the fundamental work of Weibull, the probabilistic description of the strength of ceramics is standard and has been widely applied. Here, for the first time, the resulting failure probabilities are used as objective functions in PDE constrained shape optimization.   To minimize the probability of failure, we choose a gradient based method combined with a first discretize then optimize approach. For discretization finite elements are used. Using the Lagrangian formalism, the shape gradient via the adjoint equation is calculated at low computational cost. The implementation is verified by comparison of it with a finite difference method applied to a minimal 2d example. Furthermore, we construct shape flows towards an optimal / improved shape in the case of a simple beam and a bended joint. 	
1709.06537v1	http://arxiv.org/pdf/1709.06537v1	2017	DC-Prophet: Predicting Catastrophic Machine Failures in DataCenters	You-Luen Lee|Da-Cheng Juan|Xuan-An Tseng|Yu-Ting Chen|Shih-Chieh Chang	  When will a server fail catastrophically in an industrial datacenter? Is it possible to forecast these failures so preventive actions can be taken to increase the reliability of a datacenter? To answer these questions, we have studied what are probably the largest, publicly available datacenter traces, containing more than 104 million events from 12,500 machines. Among these samples, we observe and categorize three types of machine failures, all of which are catastrophic and may lead to information loss, or even worse, reliability degradation of a datacenter. We further propose a two-stage framework-DC-Prophet-based on One-Class Support Vector Machine and Random Forest. DC-Prophet extracts surprising patterns and accurately predicts the next failure of a machine. Experimental results show that DC-Prophet achieves an AUC of 0.93 in predicting the next machine failure, and a F3-score of 0.88 (out of 1). On average, DC-Prophet outperforms other classical machine learning methods by 39.45% in F3-score. 	
1710.08500v1	http://arxiv.org/pdf/1710.08500v1	2017	Are Multiagent Systems Resilient to Communication Failures?	Philip N. Brown|Holly P. Borowski|Jason R. Marden	  A challenge in multiagent control systems is to ensure that they are appropriately resilient to communication failures between the various agents. In many common game-theoretic formulations of these types of systems, it is implicitly assumed that all agents have access to as much information about other agents' actions as needed. This paper endeavors to augment these game-theoretic methods with policies that would allow agents to react on-the-fly to losses of this information. Unfortunately, we show that even if a single agent loses communication with one other weakly-coupled agent, this can cause arbitrarily-bad system states to emerge as various solution concepts of an associated game, regardless of how the agent accounts for the communication failure and regardless of how weakly coupled the agents are. Nonetheless, we show that the harm that communication failures can cause is limited by the structure of the problem; when agents' action spaces are richer, problems are more susceptible to these types of pathologies. Finally, we undertake an initial study into how a system designer might prevent these pathologies, and explore a few limited settings in which communication failures cannot cause harm. 	
1712.04053v1	http://arxiv.org/pdf/1712.04053v1	2017	Cascading Failures as Continuous Phase-Space Transitions	Yang Yang|Adilson E. Motter	  In network systems, a local perturbation can amplify as it propagates, potentially leading to a large-scale cascading failure. Here we derive a continuous model to advance our understanding of cascading failures in power-grid networks. The model accounts for both the failure of transmission lines and the desynchronization of power generators, and incorporates the transient dynamics between successive steps of the cascade. In this framework, we show that a cascade event is a phase-space transition from an equilibrium state with high energy to an equilibrium state with lower energy, which can be suitably described in closed form using a global Hamiltonian-like function. From this function we show that a perturbed system cannot always reach the equilibrium state predicted by quasi-steady-state cascade models, which would correspond to a reduced number of failures, and may instead undergo a larger cascade. We also show that in the presence of two or more perturbations, the outcome depends strongly on the order and timing of the individual perturbations. These results offer new insights into the current understanding of cascading dynamics, with potential implications for control interventions. 	
0303593v1	http://arxiv.org/pdf/astro-ph/0303593v1	2003	Numerical Modeling of Gamma Radiation from Galaxy Clusters	Francesco Miniati	  We investigate the spatial and spectral properties of non-thermal emission from clusters of galaxies at gamma-ray energies between 10 keV and 10 TeV due to inverse-Compton (IC) emission, pion-decay and non-thermal bremsstrahlung (NTB) from cosmic-ray(CR) ions and electrons accelerated at cosmic shock and secondary e+- from inelastic p-p collisions. We identify two main emission region, namely the core (also bright in thermal X-ray) and the outskirts region where accretion shocks occur. IC emission from shock accelerated CR electrons dominate the emission in the outer regions of galaxy clusters, provided that at least a fraction of a percent of the shock ram pressure is converted into CR electrons. A clear detection of this component and of its spatial distribution will allow us direct probing of cosmic accretion shocks. In the cluster core, gamma-ray emission above 100 MeV is dominated by pion-decay mechanism and, at lower energies, by IC emission from secondary e+-. However, IC emission from shock accelerated electrons projected onto the cluster core will not be negligible. We emphasize the importance of separating these emission components for a correct interpretation of the experimental data and outline a strategy for that purpose. Failure in addressing this issue will produce unsound estimates of the intra-cluster magnetic field strength and CR ion content. According to our estimate future space borne and ground based gamma-ray facilities should be able to measure the whole nonthermal spectrum both in the cluster core and at its outskirts. The importance of such measurements in advancing our understanding of non-thermal processes in the intra-cluster medium is discussed. 	
0407284v1	http://arxiv.org/pdf/astro-ph/0407284v1	2004	The Chandra view of NGC1800 and the X-ray scaling properties of dwarf   starbursts	Jesper Rasmussen|Ian R. Stevens|Trevor J. Ponman	  The superb spatial resolution of Chandra is utilized to study the X-ray morphology of the dwarf starburst galaxy NGC1800 embedded in a small group of galaxies. Diffuse galactic emission is detected, extending several kpc above the galactic plane, with an overall morphology similar to the galactic winds seen in nearby X-ray bright starburst galaxies. This makes NGC1800 the most distant dwarf starburst with a clear detection of diffuse X-ray emission. The diffuse X-ray luminosity of 1.3+/-0.3 *10^38 erg/s accounts for at least 60 per cent of the total soft X-ray output of the galaxy. A hot gas temperature of kT=0.25 keV and metallicity Z~0.05Z_Sun are derived, the latter in consistency with results from optical spectroscopy of the interstellar medium. Our failure to detect any hot gas associated with the embedding galaxy group translates into an upper limit to the group X-ray luminosity of L_X<10^41 erg/s. There is no convincing evidence that the outflowing wind of NGC1800 is currently interacting with any intragroup gas, and mechanical considerations indicate that the wind can escape the galaxy and its surrounding HI halo, eventually delivering energy and metals to the intragroup gas. Properties of NGC1800 are compared to those of other dwarf starburst galaxies, and a first detailed discussion of the X-ray scaling properties of this population of objects is given, set against the equivalent results obtained for normal starburst galaxies. Results indicate that dwarf starbursts to a large degree behave as down-scaled versions of normal starburst galaxies. 	
9803204v1	http://arxiv.org/pdf/cond-mat/9803204v1	1998	An Observational Test of the Critical Earthquake Concept	D. D. Bowman|G. Ouillon|C. G. Sammis|A. Sornette|D. Sornette	  We test the concept that seismicity prior to a large earthquake can be understood in terms of the statistical physics of a critical phase transition. In this model, the cumulative seismic strain release increases as a power-law time-to-failure before the final event. Furthermore, the region of correlated seismicity predicted by this model is much greater than would be predicted from simple elasto-dynamic interactions. We present a systematic procedure to test for the accelerating seismicity predicted by the critical point model and to identify the region approaching criticality, based on a comparison between the observed cumulative energy (Benioff strain) release and the power-law behavior predicted by theory. This method is used to find the critical region before all earthquakes along the San Andreas system since 1950 with M 6.5. The statistical significance of our results is assessed by performing the same procedure on a large number of randomly generated synthetic catalogs. The null hypothesis, that the observed acceleration in all these earthquakes could result from spurious patterns generated by our procedure in purely random catalogs, is rejected with 99.5% confidence. An empirical relation between the logarithm of the critical region radius (R) and the magnitude of the final event (M) is found, such that log R \mu 0.5 M, suggesting that the largest probable event in a given region scales with the size of the regional fault network. 	
9906277v2	http://arxiv.org/pdf/cond-mat/9906277v2	1999	The fraction of condensed counterions around a charged rod: Comparison   of Poisson-Boltzmann theory and computer simulations	Markus Deserno|Christian Holm|Sylvio May	  We investigate the phenomenon of counterion condensation in a solution of highly charged rigid polyelectrolytes within the cell model. A method is proposed which -- based on the charge distribution function -- identifies both the fraction of condensed ions and the radial extension of the condensed layer. Within salt-free Poisson-Boltzmann (PB) theory it reproduces the well known fraction 1-1/xi of condensed ions for a Manning parameter xi>1. Furthermore, it predicts a weak salt dependence of this fraction and a breakdown of the concept of counterion condensation in the high salt limit. We complement our theoretical investigations with molecular dynamics simulations of a cell-like model, which constantly yield a stronger condensation than predicted by PB theory. While the agreement between theory and simulation is excellent in the monovalent, weakly charged case, it deteriorates with increasing electrostatic interaction strength and, in particular, increasing valence. For instance, at a high concentration of divalent salt and large xi our computer simulations predict charge oscillations, which mean-field theory is unable to reproduce. 	
0105322v2	http://arxiv.org/pdf/cond-mat/0105322v2	2001	Velocity Distributions and Correlations in Homogeneously Heated Granular   Media	Sung Joon Moon|M. D. Shattuck|J. B. Swift	  We compare the steady state velocity distributions from our three-dimensional inelastic hard sphere molecular dynamics simulation for homogeneously heated granular media, with the predictions of a mean field-type Enskog-Boltzmann equation for inelastic hard spheres [van Noije & Ernst, Gran. Matt. {\bf 1}, 57 (1998)]. Although we find qualitative agreement for all values of density and inelasticity, the quantitative disagreement approaches $\sim 40%$ at high inelasticity or density. By contrast the predictions of the pseudo-Maxwell molecule model [Carrillo, Cercignani & Gamba, Phys. Rev. E, {\bf 62}, 7700 (2000)] are both qualitatively and quantitatively different from those of our simulation. We also measure short-range and long-range velocity correlations exhibiting non-zero correlations at contact before the collision, and being consistent with a slow algebraic decay over a decade in the unit of the diameter of the particle, proportional to $r^{-(1+\alpha)}$, where $0.2 < \alpha < 0.3$. The existence of these correlations imply the failure of the molecular chaos assumption and the mean field approximation, which is responsible for the quantitative disagreement of the inelastic hard sphere kinetic theory. 	
0306224v1	http://arxiv.org/pdf/cond-mat/0306224v1	2003	The Error and Repair Catastrophes: A Two-Dimensional Phase Diagram in   the Quasispecies Model	Emmanuel Tannenbaum|Eugene I. Shakhnovich	  This paper develops a two gene, single fitness peak model for determining the equilibrium distribution of genotypes in a unicellular population which is capable of genetic damage repair. The first gene, denoted by $ \sigma_{via} $, yields a viable organism with first order growth rate constant $ k > 1 $ if it is equal to some target ``master'' sequence $ \sigma_{via, 0} $. The second gene, denoted by $ \sigma_{rep} $, yields an organism capable of genetic repair if it is equal to some target ``master'' sequence $ \sigma_{rep, 0} $. This model is analytically solvable in the limit of infinite sequence length, and gives an equilibrium distribution which depends on $ \mu \equiv L\eps $, the product of sequence length and per base pair replication error probability, and $ \eps_r $, the probability of repair failure per base pair. The equilibrium distribution is shown to exist in one of three possible ``phases.'' In the first phase, the population is localized about the viability and repairing master sequences. As $ \eps_r $ exceeds the fraction of deleterious mutations, the population undergoes a ``repair'' catastrophe, in which the equilibrium distribution is still localized about the viability master sequence, but is spread ergodically over the sequence subspace defined by the repair gene. Below the repair catastrophe, the distribution undergoes the error catastrophe when $ \mu $ exceeds $ \ln k/\eps_r $, while above the repair catastrophe, the distribution undergoes the error catastrophe when $ \mu $ exceeds $ \ln k/f_{del} $, where $ f_{del} $ denotes the fraction of deleterious mutations. 	
0501512v1	http://arxiv.org/pdf/cond-mat/0501512v1	2005	Spatial Dynamics of Invasion: The Geometry of Introduced Species	G. Korniss|Thomas Caraco	  Many exotic species combine low probability of establishment at each introduction with rapid population growth once introduction does succeed. To analyze this phenomenon, we note that invaders often cluster spatially when rare, and consequently an introduced exotic's population dynamics should depend on locally structured interactions. Ecological theory for spatially structured invasion relies on deterministic approximations, and determinism does not address the observed uncertainty of the exotic-introduction process. We take a new approach to the population dynamics of invasion and, by extension, to the general question of invasibility in any spatial ecology. We apply the physical theory for nucleation of spatial systems to a lattice-based model of competition between plant species, a resident and an invader, and the analysis reaches conclusions that differ qualitatively from the standard ecological theories. Nucleation theory distinguishes between dynamics of single-cluster and multi-cluster invasion. Low introduction rates and small system size produce single-cluster dynamics, where success or failure of introduction is inherently stochastic. Single-cluster invasion occurs only if the cluster reaches a critical size, typically preceded by a number of failed attempts. For this case, we identify the functional form of the probability distribution of time elapsing until invasion succeeds. Although multi-cluster invasion for sufficiently large systems exhibits spatial averaging and almost-deterministic dynamics of the global densities, an analytical approximation from nucleation theory, known as Avrami's law, describes our simulation results far better than standard ecological approximations. 	
0509493v1	http://arxiv.org/pdf/cond-mat/0509493v1	2005	Dynamics of a tracer granular particle as a non-equilibrium Markov   process	Andrea Puglisi|Paolo Visco|Emmanuel Trizac|Frederic van Wijland	  The dynamics of a tracer particle in a stationary driven granular gas is investigated. We show how to transform the linear Boltzmann equation describing the dynamics of the tracer into a master equation for a continuous Markov process. The transition rates depend upon the stationary velocity distribution of the gas. When the gas has a Gaussian velocity probability distribution function (pdf), the stationary velocity pdf of the tracer is Gaussian with a lower temperature and satisfies detailed balance for any value of the restitution coefficient $\alpha$. As soon as the velocity pdf of the gas departs from the Gaussian form, detailed balance is violated. This non-equilibrium state can be characterized in terms of a Lebowitz-Spohn action functional $W(\tau)$ defined over trajectories of time duration $\tau$. We discuss the properties of this functional and of a similar functional $\bar{W}(\tau)$ which differs from the first for a term which is non-extensive in time. On the one hand we show that in numerical experiments, i.e. at finite times $\tau$, the two functionals have different fluctuations and $\bar{W}$ always satisfies an Evans-Searles-like symmetry. On the other hand we cannot observe the verification of the Lebowitz-Spohn-Gallavotti-Cohen (LS-GC) relation, which is expected for $W(\tau)$ at very large times $\tau$. We give an argument for the possible failure of the LS-GC relation in this situation. We also suggest practical recipes for measuring $W(\tau)$ and $\bar{W}(\tau)$ in experiments. 	
0509541v4	http://arxiv.org/pdf/cond-mat/0509541v4	2009	Strong-Coupling Fixed Point of the Kardar-Parisi-Zhang Equation	Léonie Canet	  {\em NOTE: This paper presented the first attempt to tackle the Kardar-Parisi-Zhang (KPZ) equation using non-perturbative renormalisation group (NPRG) methods. It exploited the most natural and frequently used approximation scheme within the NPRG framework, namely the derivative expansion (DE). However, the latter approximation turned out to yield unphysical critical exponents in dimensions $d\ge 2$ and, furthermore, hinted at very poor convergence properties of the DE. The author has since realized that in fact, this approximation may not be valid for the KPZ problem, because of the very nature of the KPZ interaction, which is not {\em potential} but {\em derivative}. The probable failure of the DE is a very unusual -- and instructive -- feature within the NPRG framework. As such, the original work, unpublished, is left available on the arXiv and can be found below.   Added note: the key to deal with the KPZ problem using NPRG lies in not truncating the momentum dependence of the correlation functions, which is investigated in a recent work {\em arXiv:0905.1025}.}   We present a new approach to the Kardar-Parisi-Zhang (KPZ) equation based on the non-perturbative renormalisation group (NPRG). The NPRG flow equations derived here, at the lowest order of the derivative expansion, provide a stable strong-coupling fixed point in all dimensions $d$, embedding in particular the exact results in $d=0$ and $d=1$. However, it yields at this order unreliable dynamical and roughness exponents $z$ and $\chi$ in higher dimensions, which suggests that a richer approximation is needed to investigate the property of the rough phase in $d \ge 2$. 	
0601087v1	http://arxiv.org/pdf/cond-mat/0601087v1	2006	Two-dimensional scaling properties of experimental fracture surfaces	Laurent Ponson|Daniel Bonamy|Elisabeth Bouchaud	  The morphology of fracture surfaces encodes the various complex damage and fracture processes occurring at the microstructure scale that have lead to the failure of a given heterogeneous material. Understanding how to decipher this morphology is therefore of fundamental interest. This has been extensively investigated over these two last decades. It has been established that 1D profiles of these fracture surfaces exhibit properties of scaling invariance. In this paper, we present deeper analysis and investigate the 2D scaling properties of these fracture surfaces. We showed that the properties of scaling invariance are anisotropic and evidenced the existence of two peculiar directions on the post-mortem fracture surface caracterized by two different scaling exponents: the direction of the crack growth and the direction of the crack front. These two exponents were found to be universal, independent of the crack growth velocity, in both silica glass and aluminum alloy, archetype of brittle and ductile material respectively. Moreover, the 2D structure function that fully characterizes the scaling properties of the fracture surface was shown to take a peculiar form similar to the one predicted by some models issued from out-of-equilibrium statistical physics. This suggest some promising analogies between dynamic phase transition models and the stability of a crack front pinned/unpinned by the heterogenities of the material. 	
0604078v2	http://arxiv.org/pdf/cond-mat/0604078v2	2006	First-order Chapman--Enskog velocity distribution function in a granular   gas	J. M. Montanero|A. Santos|V. Garzo	  A method is devised to measure the first-order Chapman-Enskog velocity distribution function associated with the heat flux in a dilute granular gas. The method is based on the application of a homogeneous, anisotropic velocity-dependent external force which produces heat flux in the absence of gradients. The form of the force is found under the condition that, in the linear response regime, the deviation of the velocity distribution function from that of the homogeneous cooling state obeys the same linear integral equation as the one derived from the conventional Chapman-Enskog expansion. The Direct Simulation Monte Carlo method is used to solve the corresponding Boltzmann equation and measure the dependence of the (modified) thermal conductivity on the coefficient of normal restitution $\alpha$. Comparison with previous simulation data obtained from the Green--Kubo relations [Brey et al., J. Phys.: Condens. Matter 17, S2489 (2005)] shows an excellent agreement, both methods consistently showing that the first Sonine approximation dramatically overestimates the thermal conductivity for high inelasticity ($\alpha\lesssim 0.7$). Since our method is tied to the Boltzmann equation, the results indicate that the failure of the first Sonine approximation is not due to velocity correlation effects absent in the Boltzmann framework. This is further confirmed by an analysis of the first-order Chapman-Enskog velocity distribution function and its three first Sonine coefficients obtained from the simulations. 	
0609448v2	http://arxiv.org/pdf/cond-mat/0609448v2	2006	A stochastic flow rule for granular materials	Ken Kamrin|Martin Z. Bazant	  There have been many attempts to derive continuum models for dense granular flow, but a general theory is still lacking. Here, we start with Mohr-Coulomb plasticity for quasi-2D granular materials to calculate (average) stresses and slip planes, but we propose a "stochastic flow rule" (SFR) to replace the principle of coaxiality in classical plasticity. The SFR takes into account two crucial features of granular materials - discreteness and randomness - via diffusing "spots" of local fluidization, which act as carriers of plasticity. We postulate that spots perform random walks biased along slip-lines with a drift direction determined by the stress imbalance upon a local switch from static to dynamic friction. In the continuum limit (based on a Fokker-Planck equation for the spot concentration), this simple model is able to predict a variety of granular flow profiles in flat-bottom silos, annular Couette cells, flowing heaps, and plate-dragging experiments -- with essentially no fitting parameters -- although it is only expected to function where material is at incipient failure and slip-lines are inadmissible. For special cases of admissible slip-lines, such as plate dragging under a heavy load or flow down an inclined plane, we postulate a transition to rate-dependent Bagnold rheology, where flow occurs by sliding shear planes. With different yield criteria, the SFR provides a general framework for multiscale modeling of plasticity in amorphous materials, cycling between continuum limit-state stress calculations, meso-scale spot random walks, and microscopic particle relaxation. 	
0611714v1	http://arxiv.org/pdf/cond-mat/0611714v1	2006	Born-Oppenheimer Breakdown in Graphene	Simone Pisana|Michele Lazzeri|Cinzia Casiraghi|Kostya S. Novoselov|Andre K. Geim|Andrea C. Ferrari|Francesco Mauri	  The Born-Oppenheimer approximation (BO) has proven effective for the accurate determination of chemical reactions, molecular dynamics and phonon frequencies in a wide range of metallic systems. Graphene, recently discovered in the free state, is a zero band-gap semiconductor, which becomes a metal if the Fermi energy is tuned applying a gate-voltage Vg. Graphene electrons near the Fermi energy have twodimensional massless dispersions, described by Dirac cones. Here we show that a change in Vg induces a stiffening of the Raman G peak (i.e. the zone-center E2g optical phonon), which cannot be described within BO. Indeed, the E2g vibrations cause rigid oscillations of the Dirac-cones in the reciprocal space. If the electrons followed adiabatically the Dirac-cone oscillations, no change in the phonon frequency would be observed. Instead, since the electron-momentum relaxation near the Fermi level is much slower than the phonon motion, the electrons do not follow the Dirac-cone displacements. This invalidates BO and results in the observed phonon stiffening. This spectacular failure of BO is quite significant since BO has been the fundamental paradigm to determine crystal vibrations from the early days of quantum mechanics. 	
0701495v3	http://arxiv.org/pdf/cond-mat/0701495v3	2009	New Consideration on Composed Nonextensive Magnetic Systems	F. A. R. Navarro|M. S. Reis|E. K Lenzi|I. S. Olivera	  In this paper a composed A+B magnetic system, with spins J_A=2 and J_B=3/2, is considered within the mean-field approximation, in the framework of Tsallis nonextensive statistics. Our motivation is twofold: (1) to approach the existing experimental data of manganese oxides (manganites), where Mn^{3+} and Mn^{4+} form two magnetic sublattices, and (2) to investigate the structure of nonextensive density matrices of composed systems. By imposing that thermodynamic quantities, such as the magnetization of sublattices A and B, must be invariant weather the calculation is taken over the total Hilbert space or over partial subspaces, we found that the expression for the nonextensive entropy must be adapted. Our argument is supported by calculation of sublattices magnetization M_A and M_B, internal energy, U_A and U_B, and magnetic specific heat, CA and CB. It is shown that only with the modified entropy the two methods of calculation agree to each other. Internal energy and magnetization are additive, but no clear relationship was found between S_A, S_B and the total entropy S_{A+B} for q \neq 1. It is shown that the reason for the failure of the standard way of calculation is the assumption of statistical independence between the two subsystems, which however does not affect the density matrix in the full Hilbert space. 	
0302022v2	http://arxiv.org/pdf/cs/0302022v2	2003	Fault-tolerant routing in peer-to-peer systems	James Aspnes|Zoe Diamadi|Gauri Shah	  We consider the problem of designing an overlay network and routing mechanism that permits finding resources efficiently in a peer-to-peer system. We argue that many existing approaches to this problem can be modeled as the construction of a random graph embedded in a metric space whose points represent resource identifiers, where the probability of a connection between two nodes depends only on the distance between them in the metric space. We study the performance of a peer-to-peer system where nodes are embedded at grid points in a simple metric space: a one-dimensional real line. We prove upper and lower bounds on the message complexity of locating particular resources in such a system, under a variety of assumptions about failures of either nodes or the connections between them. Our lower bounds in particular show that the use of inverse power-law distributions in routing, as suggested by Kleinberg (1999), is close to optimal. We also give efficient heuristics to dynamically maintain such a system as new nodes arrive and old nodes depart. Finally, we give experimental results that suggest promising directions for future work. 	
0411094v2	http://arxiv.org/pdf/cs/0411094v2	2005	On the existence of truly autonomic computing systems and the link with   quantum computing	Radhakrishnan Srinivasan|H. P. Raghunandan	  A theoretical model of truly autonomic computing systems (ACS), with infinitely many constraints, is proposed. An argument similar to Turing's for the unsolvability of the halting problem, which is permitted in classical logic, shows that such systems cannot exist. Turing's argument fails in the recently proposed non-Aristotelian finitary logic (NAFL), which permits the existence of ACS. NAFL also justifies quantum superposition and entanglement, which are essential ingredients of quantum algorithms, and resolves the Einstein-Podolsky-Rosen (EPR) paradox in favour of quantum mechanics and non-locality. NAFL requires that the autonomic manager (AM) must be conceptually and architecturally distinct from the managed element, in order for the ACS to exist as a non-self-referential entity. Such a scenario is possible if the AM uses quantum algorithms and is protected from all problems by (unbreakable) quantum encryption, while the managed element remains classical. NAFL supports such a link between autonomic and quantum computing, with the AM existing as a metamathematical entity. NAFL also allows quantum algorithms to access truly random elements and thereby supports non-standard models of quantum (hyper-) computation that permit infinite parallelism. 	
0508004v1	http://arxiv.org/pdf/cs/0508004v1	2005	A three-valued semantics for logic programmers	Lee Naish	  This paper describes a simpler way for programmers to reason about the correctness of their code. The study of semantics of logic programs has shown strong links between the model theoretic semantics (truth and falsity of atoms in the programmer's interpretation of a program), procedural semantics (for example, SLD resolution) and fixpoint semantics (which is useful for program analysis and alternative execution mechanisms). Most of this work assumes that intended interpretations are two-valued: a ground atom is true (and should succeed according to the procedural semantics) or false (and should not succeed). In reality, intended interpretations are less precise. Programmers consider that some atoms "should not occur" or are "ill-typed" or "inadmissible". Programmers don't know and don't care whether such atoms succeed. In this paper we propose a three-valued semantics for (essentially) pure Prolog programs with (ground) negation as failure which reflects this. The semantics of Fitting is similar but only associates the third truth value with non-termination. We provide tools to reason about correctness of programs without the need for unnatural precision or undue restrictions on programming style. As well as theoretical results, we provide a programmer-oriented synopsis. This work has come out of work on declarative debugging, where it has been recognised that inadmissible calls are important. This paper has been accepted to appear in Theory and Practice of Logic Programming. 	
0204131v1	http://arxiv.org/pdf/hep-th/0204131v1	2002	A tentative theory of large distance physics	Daniel Friedan	  A theoretical mechanism is devised to determine the large distance physics of spacetime. It is a two dimensional nonlinear model, the lambda model, set to govern the string worldsurface to remedy the failure of string theory. The lambda model is formulated to cancel the infrared divergent effects of handles at short distance on the worldsurface. The target manifold is the manifold of background spacetimes. The coupling strength is the spacetime coupling constant. The lambda model operates at 2d distance $\Lambda^{-1}$, very much shorter than the 2d distance $\mu^{-1}$ where the worldsurface is seen. A large characteristic spacetime distance $L$ is given by $L^2=\ln(\Lambda/\mu)$. Spacetime fields of wave number up to 1/L are the local coordinates for the manifold of spacetimes. The distribution of fluctuations at 2d distances shorter than $\Lambda^{-1}$ gives the {\it a priori} measure on the target manifold, the manifold of spacetimes. If this measure concentrates at a macroscopic spacetime, then, nearby, it is a measure on the spacetime fields. The lambda model thereby constructs a spacetime quantum field theory, cutoff at ultraviolet distance $L$, describing physics at distances larger than $L$. The lambda model also constructs an effective string theory with infrared cutoff $L$, describing physics at distances smaller than $L$. The lambda model evolves outward from zero 2d distance, $\Lambda^{-1} = 0$, building spacetime physics starting from $L=\infty$ and proceeding downward in $L$. $L$ can be taken smaller than any distance practical for experiments, so the lambda model, if right, gives all actually observable physics. The harmonic surfaces in the manifold of spacetimes are expected to have novel nonperturbative effects at large distances. 	
0405121v1	http://arxiv.org/pdf/hep-th/0405121v1	2004	Radiation reaction reexamined: bound momentum and Schott term	Dmitri V. Gal'tsov|Pavel Spirin	  We review and compare two different approaches to radiation reaction in classical electrodynamics of point charges: a local calculation of the self-force using the charge equation of motion and a global calculation consisting in integration of the electromagnetic energy-momentum flux through a hypersurface encircling the world-line. Both approaches are complementary and, being combined together, give rise to an identity relating the locally and globally computed forces. From this identity it follows that the Schott terms in the Abraham force should arise from the bound field momentum and can not be introduced by hand as an additional term in the mechanical momentum of an accelerated charge. This is in perfect agreement with the results of Dirac and Teitelboim, but disagrees with the recent calculation of the bound momentum in the retarded coordinates. We perform an independent calculation of the bound electromagnetic momentum and verify explicitly that the Schott term is the derivative of the finite part of the bound momentum indeed. The failure to obtain the same result using the method of retarded coordinates tentatively lies in an inappropriate choice of the integration surface. We also discuss the definition of the delta-function on the semi-axis involved in the local calculation of the radiation reaction force and demonstrate inconsistency of one recent proposal. 	
0412470v1	http://arxiv.org/pdf/math/0412470v1	2004	Wick rotations in 3D gravity: ML(H2)-spacetimes	Riccardo Benedetti|Francesco Bonsante	  "Ends of hyperbolic 3-manifolds should support canonical Wick Rotations, so they realize effective interactions of their ending globally hyperbolic spacetimes of constant curvature." We develop a consistent sector of WR-rescaling theory in 3D gravity, that, in particular, concretizes the above guess for many geometrically finite manifolds. ML(H2)-spacetimes are solutions of pure Lorentzian 3D gravity encoded by measured geodesic laminations of the hyperbolic plane H2, possibly invariant by any given torsion-free discrete isometry group G. The rescalings which correlate spacetimes of different curvature, as well as the conformal Wick rotations towards hyperbolic structures, are directed by the gradient of the respective canonical cosmological times, and have universal rescaling functions that only depend on their value. We get an insight into the WR-rescaling mechanism by studying rays of ML(H2)-spacetimes emanating from the static case. In particular, we determine the "derivatives" at the starting point of each ray. We point out the tamest behaviour of the cocompact G case against the different general one, even when G is of cofinite area, but non-compact. We analyze brocken T-symmetry of AdS ML(H2)-spacetimes and related earthquake failure. This helps us to figure out the main lines of development in order to achieve a complete WR rescaling theory. 	
0511079v1	http://arxiv.org/pdf/math-ph/0511079v1	2005	The Dirichlet Hopf algebra of arithmetics	Bertfried Fauser|P. D. Jarvis	  In this work, we develop systematically the ``Dirichlet Hopf algebra of arithmetics'' by dualizing addition and multiplication maps. We study the additive and multiplicative antipodal convolutions which fail to give rise to Hopf algebra structures, obeying only a weakened (multiplicative) homomorphism axiom. The consequences of the weakened structure, called a Hopf gebra, e.g. on cohomology are explored. This features multiplicativity versus complete multiplicativity of number theoretic arithmetic functions. The deficiency of not being a Hopf algebra is then cured by introducing an `unrenormalized' coproduct and an `unrenormalized' pairing. It is then argued that exactly the failure of the homomorphism property (complete multiplicativity) for non-coprime integers is a blueprint for the problems in quantum field theory (QFT) leading to the need for renormalization. Renormalization turns out to be the morphism from the algebraically sound Hopf algebra to the physical and number theoretically meaningful Hopf gebra. This can be modelled alternatively by employing Rota-Baxter operators. We stress the need for a characteristic-free development where possible, to have a sound starting point for generalizations of the algebraic structures. The last section provides three key applications: symmetric function theory, quantum (matrix) mechanics, and the combinatorics of renormalization in QFT which can be discerned as functorially inherited from the development at the number-theoretic level as outlined here. Hence the occurrence of number theoretic functions in QFT becomes natural. 	
0402025v2	http://arxiv.org/pdf/physics/0402025v2	2004	Underlying mechanism of numerical instability in large eddy simulation   of turbulent flows	Masato Ida|Nobuyuki Taniguchi	  This paper extends our recent theoretical work concerning the feasibility of stable and accurate computation of turbulence using a large eddy simulation [Ida and Taniguchi, Phys. Rev. E 68, 036705 (2003)]. In our previous paper, it was shown, based on a simple assumption regarding the instantaneous streamwise velocity, that the application of the Gaussian filter to the incompressible Navier-Stokes equations can result in the appearance of a numerically unstable term that can be decomposed into positive and negative viscosities. That result raises the question as to whether an accurate solution can be achieved by a numerically stable subgrid-scale model. In the present paper, based on assumptions regarding the statistically averaged velocity, we present similar theoretical investigations to show that in several situations, the shears appearing in the statistically averaged velocity field numerically destabilize the fluctuation components because of the derivation of a numerically unstable term that represents negative diffusion in a fixed direction. This finding can explain the problematic numerical instability that has been encountered in large eddy simulations of wall-bounded flows. The present result suggests that this numerical problem is universal in large eddy simulations, and that if there is no failure in modeling, the resulting subgrid-scale model can still have unstable characteristics; that is, the known instability problems of several existing subgrid-scale models are not something that one may remove simply by an artificial technique, but must be taken seriously so as to treat them accurately. 	
0502100v1	http://arxiv.org/pdf/physics/0502100v1	2005	Use of time-correlated single photon counting detection to measure the   speed of light in water	Pedro L. Muino|Aaron M. Thompson|Robert J. Buenker	  Traditional methods for measuring the speed of light in dispersive media have been based on the detection of interference between light waves emitted from the same source. In the present study the elapsed times for single photons to move from a laser to a photomultiplier tube are measured electronically. Time-correlated single photon counting detection produces a characteristic instrument response which has the same shape independent of both the path length the light travels and the nature of the transparent media through which it passes. This allows for an accurate calibration of the chronograph by observing shifts in the location of the instrument response for different distances traveled by the light. Measurement of the corresponding shift which occurs when light moves the same distance through air and water then enables an accurate determination of the ratio of the photon velocities in these two media. Three different wavelengths of light have been used. In two cases good agreement is found between the present measured light speeds and those which can be inferred from existing refractive index measurements in water. The shortest wavelength studied is too far in the uv to obtain a reliable estimate on the same basis, and so the ng value (1.463) measured in the present work awaits independent confirmation. A theoretical discussion of the present results is undertaken with reference to Newton's original corpuscular theory of light. It is argued that his failure to predict that light travels more slowly in water than in air arose from the inadequacy of his mechanical theory rather than his assumptions about the elementary composition of light. 	
0312022v2	http://arxiv.org/pdf/q-bio/0312022v2	2004	Failed "nonaccelerating" models of prokaryote gene regulatory networks	M. J. Gagen|J. S. Mattick	  Much current network analysis is predicated on the assumption that important biological networks will either possess scale free or exponential statistics which are independent of network size allowing unconstrained network growth over time. In this paper, we demonstrate that such network growth models are unable to explain recent comparative genomics results on the growth of prokaryote regulatory gene networks as a function of gene number. This failure largely results as prokaryote regulatory gene networks are "accelerating" and have total link numbers growing faster than linearly with network size and so can exhibit transitions from stationary to nonstationary statistics and from random to scale-free to regular statistics at particular critical network sizes. In the limit, these networks can undergo transitions so marked as to constrain network sizes to be below some critical value. This is of interest as the regulatory gene networks of single celled prokaryotes are indeed characterized by an accelerating quadratic growth with gene count and are size constrained to be less than about 10,000 genes encoded in DNA sequence of less than about 10 megabases. We develop two "nonaccelerating" network models of prokaryote regulatory gene networks in an endeavor to match observation and demonstrate that these approaches fail to reproduce observed statistics. 	
9908039v1	http://arxiv.org/pdf/quant-ph/9908039v1	1999	Quantum perfect correlations and Hardy's nonlocality theorem	Jose L. Cereceda	  In this paper the failure of Hardy's nonlocality proof for the class of maximally entangled states is considered. A detailed analysis shows that the incompatibility of the Hardy equations for this class of states physically originates from the fact that the existence of quantum perfect correlations for the three pairs of two-valued observables (D_11,D_21), (D_11,D_22) and (D_12,D_21) [in the sense of having with certainty equal (different) readings for a joint measurement of any one of the pairs (D_11,D_21), (D_11,D_22), and (D_12,D_21)], necessarily entails perfect correlation for the pair of observables (D_12,D_22) [in the sense of having with certainty equal (different) readings for a joint measurement of the pair (D_12,D_22)]. Indeed, the set of these four perfect correlations is found to satisfy the CHSH inequality, and then no violations of local realism will arise for the maximally entangled state as far as the four observables D_ij, i,j = 1,2, are concerned. The connection between this fact and the impossibility for the quantum mechanical predictions to give the maximum possible theoretical violation of the CHSH inequality is pointed out. Moreover, it is generally proved that the fulfillment of all the Hardy nonlocality conditions necessarily entails a violation of the resulting CHSH inequality. The largest violation of this latter inequality is determined. 	
0704.2379v1	http://arxiv.org/pdf/0704.2379v1	2007	Fundamental-measure density functional for the fluid of aligned hard   hexagons: New insights in fundamental measure theory	Jose A. Capitan|Jose A. Cuesta	  In this article we obtain a fundamental measure functional for the model of aligned hard hexagons in the plane. Our aim is not just to provide a functional for a new, admittedly academic, model, but to investigate the structure of fundamental measure theory. A model of aligned hard hexagons has similarities with the hard disk model. Both share "lost cases", i.e. admit configurations of three particles in which there is pairwise overlap but not triple overlap. These configurations are known to be problematic for fundamental measure functionals, which are not able to capture their contribution correctly. This failure lies in the inability of these functionals to yield a correct low density limit of the third order direct correlation function. Here we derive the functional by projecting aligned hard cubes on the plane x+y+z=0. The correct dimensional crossover behavior of these functionals permits us to follow this strategy. The functional of aligned hard cubes, however, does not have lost cases, so neither had the resulting functional for aligned hard hexagons. The latter exhibits, in fact, a peculiar structure as compared to the one for hard disks. It depends on a uniparametric family of weighted densities through a new term not appearing in the functional for hard disks. Apart from studying the freezing of this system, we discuss the implications of the functional structure for new developments of fundamental measure theory. 	
0808.1744v1	http://arxiv.org/pdf/0808.1744v1	2008	Our Brothers' Keepers: Secure Routing with High Performance	Alex Brodsky|Scott Lindenberg	  The Trinity (Brodsky et al., 2007) spam classification system is based on a distributed hash table that is implemented using a structured peer-to-peer overlay. Such an overlay must be capable of processing hundreds of messages per second, and must be able to route messages to their destination even in the presence of failures and malicious peers that misroute packets or inject fraudulent routing information into the system. Typically there is tension between the requirements to route messages securely and efficiently in the overlay.   We describe a secure and efficient routing extension that we developed within the I3 (Stoica et al. 2004) implementation of the Chord (Stoica et al. 2001) overlay. Secure routing is accomplished through several complementary approaches: First, peers in close proximity form overlapping groups that police themselves to identify and mitigate fraudulent routing information. Second, a form of random routing solves the problem of entire packet flows passing through a malicious peer. Third, a message authentication mechanism links each message to it sender, preventing spoofing. Fourth, each peer's identifier links the peer to its network address, and at the same time uniformly distributes the peers in the key-space.   Lastly, we present our initial evaluation of the system, comprising a 255 peer overlay running on a local cluster. We describe our methodology and show that the overhead of our secure implementation is quite reasonable. 	
0902.0603v2	http://arxiv.org/pdf/0902.0603v2	2009	Severe Vesico-ureteral Reflux and Urine Sequestration: Mathematical   Relations and Urodynamic Consequences	Lisieux Eyer de Jesus|Paulo de Faria Borges	  Some simple mathematical formulae to calculate the volumes of proximal pyeloureteral reflexive systems are presented, and the results are compared to bladder capacity values. Using the results of the calculi, the author discusses possible implications of severe urinary sequestration in the pyeloureteral systems. Using geometrical and topological approximations we calculate the volumes of ureters and renal pelvises, applying in vivo measurements obtained from conventional ultrasound, retrograde cystourethrograms and topographic anatomic references. Approximations use 2 decimals and assumed $\pi$ value was 3.14. Ureteral and pyelic volumes are calculated, respectively, from the mathematical formula for the cylinder and cone volumes. Dolicomegaureter are compensated using proportional calculi. Bladder volumes are estimated from conventional formulae. Proximal urinary sequestration is compared between infants and older children with VUR. Mechanisms of direct induction of bladder urodynamic failure from VUR are suggested. Sequestration of urine in the ureter and renal pelvis can be estimated from mathematical formulae in patients with VUR. The values used derive from ultrasound examinations, CUM and topographical anatomical references. Primary VUR can determine urodynamic problems. Urine sequestration in the proximal urinary system is worse in infants than in older children. 	
0903.4365v2	http://arxiv.org/pdf/0903.4365v2	2009	CliqueStream: an efficient and fault-resilient live streaming network on   a clustered peer-to-peer overlay	Shah Asaduzzaman|Ying Qiao|Gregor v. Bochmann	  Several overlay-based live multimedia streaming platforms have been proposed in the recent peer-to-peer streaming literature. In most of the cases, the overlay neighbors are chosen randomly for robustness of the overlay. However, this causes nodes that are distant in terms of proximity in the underlying physical network to become neighbors, and thus data travels unnecessary distances before reaching the destination. For efficiency of bulk data transmission like multimedia streaming, the overlay neighborhood should resemble the proximity in the underlying network. In this paper, we exploit the proximity and redundancy properties of a recently proposed clique-based clustered overlay network, named eQuus, to build efficient as well as robust overlays for multimedia stream dissemination. To combine the efficiency of content pushing over tree structured overlays and the robustness of data-driven mesh overlays, higher capacity stable nodes are organized in tree structure to carry the long haul traffic and less stable nodes with intermittent presence are organized in localized meshes. The overlay construction and fault-recovery procedures are explained in details. Simulation study demonstrates the good locality properties of the platform. The outage time and control overhead induced by the failure recovery mechanism are minimal as demonstrated by the analysis. 	
0907.0335v2	http://arxiv.org/pdf/0907.0335v2	2010	Population dynamics on random networks: simulations and analytical   models	Ganna Rozhnova|Ana Nunes	  We study the phase diagram of the standard pair approximation equations for two different models in population dynamics, the susceptible-infective-recovered-susceptible model of infection spread and a predator-prey interaction model, on a network of homogeneous degree $k$. These models have similar phase diagrams and represent two classes of systems for which noisy oscillations, still largely unexplained, are observed in nature. We show that for a certain range of the parameter $k$ both models exhibit an oscillatory phase in a region of parameter space that corresponds to weak driving. This oscillatory phase, however, disappears when $k$ is large. For $k=3, 4$, we compare the phase diagram of the standard pair approximation equations of both models with the results of simulations on regular random graphs of the same degree. We show that for parameter values in the oscillatory phase, and even for large system sizes, the simulations either die out or exhibit damped oscillations, depending on the initial conditions. We discuss this failure of the standard pair approximation model to capture even the qualitative behavior of the simulations on large regular random graphs and the relevance of the oscillatory phase in the pair approximation diagrams to explain the cycling behavior found in real populations. 	
0907.1040v1	http://arxiv.org/pdf/0907.1040v1	2009	Capacitance of graphene nanoribbons	A. A. Shylau|J. W. Klos|I. V. Zozoulenko	  We present an analytical theory for the gate electrostatics and the classical and quantum capacitance of the graphene nanoribbons (GNRs) and compare it with the exact self-consistent numerical calculations based on the tight-binding p-orbital Hamiltonian within the Hartree approximation. We demonstrate that the analytical theory is in a good qualitative (and in some aspects quantitative) agreement with the exact calculations. There are however some important discrepancies. In order to understand the origin of these discrepancies we investigate the self-consistent electronic structure and charge density distribution in the nanoribbons and relate the above discrepancy to the inability of the simple electrostatic model to capture the classical gate electrostatics of the GNRs. In turn, the failure of the classical electrostatics is traced to the quantum mechanical effects leading to the significant modification of the self-consistent charge distribution in comparison to the non-interacting electron description. The role of electron-electron interaction in the electronic structure and the capacitance of the GNRs is discussed. Our exact numerical calculations show that the density distribution and the potential profile in the GNRs are qualitatively different from those in conventional split-gate quantum wires; at the same time, the electron distribution and the potential profile in the GNRs show qualitatively similar features to those in the cleaved-edge overgrown quantum wires. Finally, we discuss an experimental extraction of the quantum capacitance from experimental data. 	
0907.3475v1	http://arxiv.org/pdf/0907.3475v1	2009	BBGKY equations, self-diffusion and 1/f noise in a slightly nonideal gas	Yuriy E. Kuzovlev	  The hypothesis of ``molecular chaos'' is shown to fail when applied to spatially inhomogeneous evolution of a low-density gas, because this hypothesis is incompatible with reduction of interactions of gas particles to ``collisions''. The failure of molecular chaos means existence of statistical correlations between colliding and closely spaced particles in configuration space. If this fact is taken into account, then in the collisional approximation (in the kinetic stage of gas evolution) in the limit of infinitely small gas parameter the Bogolyubov-Born-Green-Kirkwood-Yvon (BBGKY) hierarchy of equations yields an autonomous system of kinetic equations for the many-particle distribution functions of closely spaced particles. This system of equations can produce the Boltzmann equation only in the homogeneous case. It is used to analyze statistical properties of Brownian motion of a test gas particle. The analysis shows that there exist fluctuations with a 1/f spectrum in the diffusivity and mobility of any particle. The physical cause of these fluctuations is randomness of distribution of particles' encounters over the impact parameter values and, consequently, randomness of the rate and efficiency of collisions.   In essence, this is {\bf reprint} of the like author's paper published in Russian in [ Zh. Eksp. Teor. Fiz. {\bf 94} (12), 140-156 (Dec. 1988)] and translated into English in [ Sov. Phys. JETP {\bf 67} (12), 469-2477 (Dec. 1988)] twenty years ago but seemingly still unknown to those to whom it might be very useful. The footnotes contain presently added comments. 	
0908.0499v1	http://arxiv.org/pdf/0908.0499v1	2009	Role of the defect-core in energetics of vacancies	Vikram Gavini	  Electronic structure calculations at macroscopic scales are employed to investigate the crucial role of a defect-core in the energetics of vacancies in aluminum. We find that vacancy core-energy is significantly influenced by the state of deformation at the vacancy-core, especially volumetric strains. Insights from the core electronic structure and computed displacement fields show that this dependence on volumetric strains is closely related to the changing nature of the core-structure under volumetric deformations. These results are in sharp contrast to mechanics descriptions based on elastic interactions that often consider defect core-energies as an inconsequential constant. Calculations suggest that the variation in core-energies with changing macroscopic deformations is quantitatively more significant than the corresponding variation in relaxation energies associated with elastic fields. Upon studying the influence of various macroscopic deformations, which include volumetric, uniaxial, biaxial and shear deformations, on the formation energies of vacancies, we show that volumetric deformations play a dominant role in governing the energetics of these defects. Further, by plotting formation energies of vacancies and di-vacancies against the volumetric strain corresponding to any macroscopic deformation, we find that all variations in the formation energies collapse on to a universal curve. This suggests a universal role of volumetric strains in the energetics of vacancies. Implications of these results in the context of dynamic failure in metals due to spalling are analyzed. 	
1003.0488v2	http://arxiv.org/pdf/1003.0488v2	2010	On Secure Distributed Data Storage Under Repair Dynamics	Sameer Pawar|Salim El Rouayheb|Kannan Ramchandran	  We address the problem of securing distributed storage systems against passive eavesdroppers that can observe a limited number of storage nodes. An important aspect of these systems is node failures over time, which demand a repair mechanism aimed at maintaining a targeted high level of system reliability. If an eavesdropper observes a node that is added to the system to replace a failed node, it will have access to all the data downloaded during repair, which can potentially compromise the entire information in the system. We are interested in determining the secrecy capacity of distributed storage systems under repair dynamics, i.e., the maximum amount of data that can be securely stored and made available to a legitimate user without revealing any information to any eavesdropper. We derive a general upper bound on the secrecy capacity and show that this bound is tight for the bandwidth-limited regime which is of importance in scenarios such as peer-to-peer distributed storage systems. We also provide a simple explicit code construction that achieves the capacity for this regime. 	
1010.5176v1	http://arxiv.org/pdf/1010.5176v1	2010	A Distributed Trust Management Framework for Detecting Malicious Packet   Dropping Nodes in a Mobile Ad Hoc Network	Jaydip Sen	  In a multi-hop mobile ad hoc network (MANET) mobile nodes communicate with each other forming a cooperative radio network. Security remains a major challenge for these networks due to their features of open medium, dynamically changing topologies, reliance on cooperative algorithms, absence of centralized monitoring points, and lack of any clear lines of defense. Most of the currently existing security algorithms designed for these networks are insecure, in efficient, and have low detection accuracy for nodes' misbehaviour. In this paper, a new approach has been proposed to bring out the complementary relationship between key distribution and misbehaviour detection for developing an integrated security solution for MANETs. The redundancy of routing information in ad hoc networks is utilized to develop a highly reliable protocol that works even in presence of transient network partitioning and Byzantine failure of nodes. The proposed mechanism is fully co-operative, and thus it is more robust as the vulnerabilities of the election algorithms used for choosing the subset of nodes for cooperation are absent. Simulation results show the effectiveness of the proposed protocol. 	
1107.4898v4	http://arxiv.org/pdf/1107.4898v4	2011	Local energy: a basis for local electronegativity and local hardness	Tamas Gal	  The traditional approach to establishing a local measure of chemical hardness, by defining a local hardness concept through the derivative of the chemical potential with respect to the electron density, has been found to have limited chemical applicability, and has proved to be an unfeasible approach in principle. Here, we propose a new approach via a unique local energy concept. This local energy is shown to emerge from the Hamilton-Jacobi kind of construction of Schrodinger's quantum mechanics. It then leads to the concepts of a local chemical potential, i.e. negative of local electronegativity, and a local hardness just as the chemical potential and hardness are obtained from the energy, namely via differentiations with respect to the number of electrons. The emerging local hardness adds corrections to a recently proposed local hardness expression that has been found to be a good local measure of hardness for a series of atomic and molecular systems. These corrections become relevant for molecules with a large number of electrons. It is pointed out further that the definition of local softness that yields it as the Fukui function times the softness is not well-established, explaining recent observations of failure of this local softness concept as a proper local reactivity index for hard systems. 	
1110.5969v1	http://arxiv.org/pdf/1110.5969v1	2011	Reliable Provisioning of Spot Instances for Compute-intensive   Applications	William Voorsluys|Rajkumar Buyya	  Cloud computing providers are now offering their unused resources for leasing in the spot market, which has been considered the first step towards a full-fledged market economy for computational resources. Spot instances are virtual machines (VMs) available at lower prices than their standard on-demand counterparts. These VMs will run for as long as the current price is lower than the maximum bid price users are willing to pay per hour. Spot instances have been increasingly used for executing compute-intensive applications. In spite of an apparent economical advantage, due to an intermittent nature of biddable resources, application execution times may be prolonged or they may not finish at all. This paper proposes a resource allocation strategy that addresses the problem of running compute-intensive jobs on a pool of intermittent virtual machines, while also aiming to run applications in a fast and economical way. To mitigate potential unavailability periods, a multifaceted fault-aware resource provisioning policy is proposed. Our solution employs price and runtime estimation mechanisms, as well as three fault tolerance techniques, namely checkpointing, task duplication and migration. We evaluate our strategies using trace-driven simulations, which take as input real price variation traces, as well as an application trace from the Parallel Workload Archive. Our results demonstrate the effectiveness of executing applications on spot instances, respecting QoS constraints, despite occasional failures. 	
1111.0034v3	http://arxiv.org/pdf/1111.0034v3	2012	Diffusion Adaptation Strategies for Distributed Optimization and   Learning over Networks	Jianshu Chen|Ali H. Sayed	  We propose an adaptive diffusion mechanism to optimize a global cost function in a distributed manner over a network of nodes. The cost function is assumed to consist of a collection of individual components. Diffusion adaptation allows the nodes to cooperate and diffuse information in real-time; it also helps alleviate the effects of stochastic gradient noise and measurement noise through a continuous learning process. We analyze the mean-square-error performance of the algorithm in some detail, including its transient and steady-state behavior. We also apply the diffusion algorithm to two problems: distributed estimation with sparse parameters and distributed localization. Compared to well-studied incremental methods, diffusion methods do not require the use of a cyclic path over the nodes and are robust to node and link failure. Diffusion methods also endow networks with adaptation abilities that enable the individual nodes to continue learning even when the cost function changes with time. Examples involving such dynamic cost functions with moving targets are common in the context of biological networks. 	
1111.0385v1	http://arxiv.org/pdf/1111.0385v1	2011	A Distributed Protocol for Detection of Packet Dropping Attack in Mobile   Ad Hoc Networks	Jaydip Sen|M. Girish Chandra|P. Balamuralidhar|Harihara S. G.|Harish Reddy	  In multi-hop mobile ad hoc networks (MANETs),mobile nodes cooperate with each other without using any infrastructure such as access points or base stations. Security remains a major challenge for these networks due to their features of open medium, dynamically changing topologies, reliance on cooperative algorithms, absence of centralized monitoring points, and lack of clear lines of defense. Among the various attacks to which MANETs are vulnerable, malicious packet dropping attack is very common where a malicious node can partially degrade or completely disrupt communication in the network by consistently dropping packets. In this paper, a mechanism for detection of packet dropping attack is presented based on cooperative participation of the nodes in a MANET. The redundancy of routing information in an ad hoc network is utilized to make the scheme robust so that it works effectively even in presence of transient network partitioning and Byzantine failure of nodes. The proposed scheme is fully cooperative and thus more secure as the vulnerabilities of any election algorithm used for choosing a subset of nodes for cooperation are absent. Simulation results show the effectiveness of the protocol. 	
1201.1826v1	http://arxiv.org/pdf/1201.1826v1	2012	Hamiltonian structure of classical N-body systems of finite-size   particles subject to EM interactions	Claudio Cremaschini|Massimo Tessarotto	  An open issue in classical relativistic mechanics is the consistent treatment of the dynamics of classical $N$-body systems of mutually-interacting particles. This refers, in particular, to charged particles subject to EM interactions, including both binary and self interactions (EM-interacting $N$-body systems). In this paper it is shown that such a description can be consistently obtained in the context of classical electrodynamics, for the case of a $N$-body system of classical finite-size charged particles. A variational formulation of the problem is presented, based on the $N$-body hybrid synchronous Hamilton variational principle. Covariant Lagrangian and Hamiltonian equations of motion for the dynamics of the interacting $N$-body system are derived, which are proved to be delay-type ODEs. Then, a representation in both standard Lagrangian and Hamiltonian forms is proved to hold, the latter expressed by means of classical Poisson Brackets. The theory developed retains both the covariance with respect to the Lorentz group and the exact Hamiltonian structure of the problem, which is shown to be intrinsically non-local. Different applications of the theory are investigated. The first one concerns the development of a suitable Hamiltonian approximation of the exact equations that retains finite delay-time effects characteristic of the binary and self EM interactions. Second, basic consequences concerning the validity of Dirac generator formalism are pointed out, with particular reference to the instant-form representation of Poincar\`{e} generators. Finally, a discussion is presented both on the validity and possible extension of the Dirac generator formalism as well as the failure of the so-called Currie \textquotedblleft no-interaction\textquotedblright\ theorem for the non-local Hamiltonian system considered here. 	
1202.0885v1	http://arxiv.org/pdf/1202.0885v1	2012	The Impact of Secure OSs on Internet Security: What Cyber-Insurers Need   to Know	Ranjan Pal|Pan Hui	  In recent years, researchers have proposed \emph{cyber-insurance} as a suitable risk-management technique for enhancing security in Internet-like distributed systems. However, amongst other factors, information asymmetry between the insurer and the insured, and the inter-dependent and correlated nature of cyber risks have contributed in a big way to the failure of cyber-insurance markets. Security experts have argued in favor of operating system (OS) platform switching (ex., from Windows to Unix-based OSs) or secure OS adoption as being one of the techniques that can potentially mitigate the problems posing a challenge to successful cyber-insurance markets. In this regard we model OS platform switching dynamics using a \emph{social gossip} mechanism and study three important questions related to the nature of the dynamics, for Internet-like distributed systems: (i) which type of networks should cyber-insurers target for insuring?, (ii) what are the bounds on the asymptotic performance level of a network, where the performance parameter is an average function of the long-run individual user willingness to adopt secure OSs?, and (iii) how can cyber-insurers use the topological information of their clients to incentivize/reward them during offering contracts? Our analysis is important to a profit-minded cyber-insurer, who wants to target the right network, design optimal contracts to resolve information asymmetry problems, and at the same time promote the increase of overall network security through increasing secure OS adoption amongst users. 	
1202.5208v2	http://arxiv.org/pdf/1202.5208v2	2012	Structure of finite sphere packings via exact enumeration: Implications   for colloidal crystal nucleation	Robert S. Hoy|Jared Harwayne-Gidansky|Corey S. O'Hern	  We analyze the geometric structure and mechanical stability of a complete set of isostatic and hyperstatic sphere packings obtained via exact enumeration. The number of nonisomorphic isostatic packings grows exponentially with the number of spheres $N$, and their diversity of structure and symmetry increases with increasing $N$ and decreases with increasing hyperstaticity $H \equiv N_c - N_{ISO}$, where $N_c$ is the number of pair contacts and $N_{ISO} = 3N-6$. Maximally contacting packings are in general neither the densest nor the most symmetric. Analyses of local structure show that the fraction $f$ of nuclei with order compatible with the bulk (RHCP) crystal decreases sharply with increasing $N$ due to a high propensity for stacking faults, 5- and near-5-fold symmetric structures, and other motifs that preclude RHCP order. While $f$ increases with increasing $H$, a significant fraction of hyperstatic nuclei for $N$ as small as 11 retain non-RHCP structure. Classical theories of nucleation that consider only spherical nuclei, or only nuclei with the same ordering as the bulk crystal, cannot capture such effects. Our results provide an explanation for the failure of classical nucleation theory for hard-sphere systems of $N\lesssim 10$ particles; we argue that in this size regime, it is essential to consider nuclei of unconstrained geometry. Our results are also applicable to understanding kinetic arrest and jamming in systems that interact via hard-core-like repulsive and short-ranged attractive interactions. 	
1203.5086v1	http://arxiv.org/pdf/1203.5086v1	2012	"Selfish" algorithm for optimizing the network survivability analysis	Svetlana V. Poroseva	  In Nature, the primary goal of any network is to survive. This is less obvious for engineering networks (electric power, gas, water, transportation systems etc.) that are expected to operate under normal conditions most of time. As a result, the ability of a network to withstand massive sudden damage caused by adverse events (or survivability) has not been among traditional goals in the network design. Reality, however, calls for the adjustment of design priorities. As modern networks develop toward increasing their size, complexity, and integration, the likelihood of adverse events increases too due to technological development, climate change, and activities in the political arena among other factors. Under such circumstances, a network failure has an unprecedented effect on lives and economy. To mitigate the impact of adverse events on the network operability, the survivability analysis must be conducted at the early stage of the network design. Such analysis requires the development of new analytical and computational tools. Computational analysis of the network survivability is the exponential time problem at least. The current paper describes a new algorithm, in which the reduction of the computational complexity is achieved by mapping an initial network topology with multiple sources and sinks onto a set of simpler smaller topologies with multiple sources and a single sink. Steps for further reducing the time and space expenses of computations are also discussed. 	
1204.0320v1	http://arxiv.org/pdf/1204.0320v1	2012	Two-dimensional finite element simulation of fracture and fatigue   behaviours of alumina microstructures for hip prosthesis	Kyungmok Kim|Bernard Forest|Jean Géringer	  This paper describes a two-dimensional (2D) finite element simulation for fracture and fatigue behaviours of pure alumina microstructures such as those found at hip prostheses. Finite element models are developed using actual Al2O3 microstructures and a bilinear cohesive zone law. Simulation conditions are similar to those found at a slip zone in a dry contact between a femoral head and an acetabular cup of hip prosthesis. Contact stresses are imposed to generate cracks in the models. Magnitudes of imposed stresses are higher than those found at the microscopic scale. Effects of microstructures and contact stresses are investigated in terms of crack formation. In addition, fatigue behaviour of the microstructure is determined by performing simulations under cyclic loading conditions. It is shown that crack density observed in a microstructure increases with increasing magnitude of applied contact stress. Moreover, crack density increases linearly with respect to the number of fatigue cycles within a given contact stress range. Meanwhile, as applied contact stress increases, number of cycles to failure decreases gradually. Finally, this proposed finite element simulation offers an effective method for identifying fracture and fatigue behaviours of a microstructure provided that microstructure images are available. 	
1204.0974v2	http://arxiv.org/pdf/1204.0974v2	2012	Traditional formation scenarios fail to explain 4:3 mean motion   resonances	Hanno Rein|Matthew J. Payne|Dimitri Veras|Eric B. Ford	  At least two multi-planetary systems in a 4:3 mean motion resonance have been found by radial velocity surveys. These planets are gas giants and the systems are only stable when protected by a resonance. Additionally the Kepler mission has detected at least 4 strong candidate planetary systems with a period ratio close to 4:3.   This paper investigates traditional dynamical scenarios for the formation of these systems. We systematically study migration scenarios with both N-body and hydro-dynamic simulations. We investigate scenarios involving the in-situ formation of two planets in resonance. We look at the results from finely tuned planet-planet scattering simulations with gas disk damping. Finally, we investigate a formation scenario involving isolation-mass embryos.   Although the combined planet-planet scattering and damping scenario seems promising, none of the above scenarios is successful in forming enough systems in 4:3 resonance with planetary masses similar to the observed ones. This is a negative result but it has important implications for planet formation. Previous studies were successful in forming 2:1 and 3:2 resonances. This is generally believed to be evidence of planet migration. We highlight the main differences between those studies and our failure in forming a 4:3 resonance. We also speculate on more exotic and complicated ideas. These results will guide future investigators toward exploring the above scenarios and alternative mechanisms in a more general framework. 	
1205.3892v1	http://arxiv.org/pdf/1205.3892v1	2012	Reconsideration of the uncertainty relations and quantum measurements	Spiridon Dumitru	  Discussions on uncertainty relations (UR) and quantum measurements (QMS) persisted until nowadays in publications about quantum mechanics (QM). They originate mainly from the conventional interpretation of UR (CIUR). In the most of the QM literarure, it is underestimated the fact that, over the years, a lot of deficiencies regarding CIUR were signaled. As a rule the alluded deficiencies were remarked disparately and discussed as punctual and non-essential questions. Here we approach an investigation of the mentioned deficiencies collected in a conclusive ensemble. Subsequently we expose a reconsideration of the major problems referring to UR and QMS. We reveal that all the basic presumption of CIUR are troubled by insurmountable deficiencies which require the indubitable failure of CIUR and its necessary abandonment. Therefore the UR must be deprived of their statute of crucial pieces for physics. So, the aboriginal versions of UR appear as being in postures of either (i) thought-experimental fictions or (ii) simple QM formulae and, any other versions of them, have no connection with the QMS. Then the QMS must be viewed as an additional subject comparatively with the usual questions of QM. For a theoretical description of QMS we propose an information-transmission model, in which the quantum observables are considered as random variables. Our approach directs to natural solutions and simplifications for many problems regarding UR and QMS. 	
1206.4361v1	http://arxiv.org/pdf/1206.4361v1	2012	Tracing the Progression of Retinitis Pigmentosa via Photoreceptor   Interactions	Erika T. Camacho|Stephen Wirkus	  Retinitis pigmentosa (RP) is a group of inherited degenerative eye diseases characterized by mutations in the genetic structure of the photoreceptors that leads to the premature death of both rod and cone photoreceptors. Defects in particular genes encoding proteins that are involved in either the photoreceptor structure, phototransduction cascades, or visual cycle are expressed in the rods but ultimately affect both types of cells. RP is "typically" manifested by a steady death of rods followed by a period of stability in which cones survive initially and then inevitably die too. In some RP cases, rods and cones die off simultaneously or even cone death precedes rod death (reverse RP). The mechanisms and factors involved in the development of the different types of RP are not well understood nor have researchers been able to provide more than a limited number of short-term therapies. In this work we trace the progression of RP to complete blindness through each subtype via bifurcation theory. We show that the evolution of RP from one stage to another often requires the failure of multiple components. Our results indicate that a delicate balance between the availability of nutrients and the rates of shedding and renewal of photoreceptors is needed at every stage of RP to halt its progression. This work provides a framework for future physiological investigations potentially leading to long-term targeted multi-facet interventions and therapies dependent on the particular stage and subtype of RP under consideration. The results of this mathematical model may also give insight into the progression of many other degenerative eye diseases involving genetic mutations or secondary photoreceptor death and potential ways to circumvent these diseases. 	
1209.5256v1	http://arxiv.org/pdf/1209.5256v1	2012	Infiltration effects on a two-dimensional molecular dynamics model of   landslides	Gianluca Martelloni|Franco Bagnoli	  In this paper we propose a two-dimensional (2D) computational model, based on a molecular dynamics (MD) approach, for deep landslides triggered by rainfall. Our model is based on interacting particles or grains and describes the behavior of a fictitious granular material along a slope consisting of a vertical section, i.e. with a wide thickness. The triggering of the landslide is caused by the passing of two conditions: a threshold speed and a condition on the static friction of the particles, the latter based on the Mohr-Coulomb failure criterion (Coulomb 1776; Mohr 1914). The inter-particle interactions are through a potential that, in the absence of suitable experimental data and due to the arbitrariness of the grain dimension is modeled by means of a potential similar to the Lennard-Jones one (Lennard-Jones 1924), i.e., with an attractive and a repulsive part. For the updating of the particle positions we use a MD method which results to be very suitable to simulate this type of systems (Herrmann and Luding 1998). In order to take into account the increasing of the pore pressure due to the rainfall, a filtration model is considered. Finally we also introduce in the model the viscosity as a term in the dynamic equations of motion. The outcome of simulations, from the point of view of statistical and dynamic characterization, is quite satisfactory relative to real landslides behavior and we can claim that this types of modeling can represent a new method to simulate landslides triggered by rainfall. 	
1212.2117v1	http://arxiv.org/pdf/1212.2117v1	2012	Functional renormalization-group approach to decaying turbulence	Andrei A. Fedorenko|Pierre Le Doussal|Kay Joerg Wiese	  We reconsider the functional renormalization-group (FRG) approach to decaying Burgers turbulence, and extend it to decaying Navier-Stokes and Surface-Quasi-Geostrophic turbulence. The method is based on a renormalized small-time expansion, equivalent to a loop expansion, and naturally produces a dissipative anomaly and a cascade after a finite time. We explicitly calculate and analyze the one-loop FRG equations in the zero-viscosity limit as a function of the dimension. For Burgers they reproduce the FRG equation obtained in the context of random manifolds, extending previous results of one of us. Breakdown of energy conservation due to shocks and the appearance of a direct energy cascade corresponds to failure of dimensional reduction in the context of disordered systems. For Navier-Stokes in three dimensions, the velocity-velocity correlation function acquires a linear dependence on the distance, zeta_2=1, in the inertial range, instead of Kolmogorov's zeta_2=2/3; however the possibility remains for corrections at two- or higher-loop order. In two dimensions, we obtain a numerical solution which conserves energy and exhibits an inverse cascade, with explicit analytical results both for large and small distances, in agreement with the scaling proposed by Batchelor. In large dimensions, the one-loop FRG equation for Navier-Stokes converges to that of Burgers. 	
1302.0916v1	http://arxiv.org/pdf/1302.0916v1	2013	Quantum Discord, CHSH Inequality and Hidden Variables -- Critical   reassessment of hidden-variables models	Kazuo Fujikawa	  Hidden-variables models are critically reassessed. It is first examined if the quantum discord is classically described by the hidden-variable model of Bell in the Hilbert space with $d=2$. The criterion of vanishing quantum discord is related to the notion of reduction and, surprisingly, the hidden-variable model in $d=2$, which has been believed to be consistent so far, is in fact inconsistent and excluded by the analysis of conditional measurement and reduction. The description of the full contents of quantum discord by the deterministic hidden-variables models is not possible. We also re-examine CHSH inequality. It is shown that the well-known prediction of CHSH inequality $|B|\leq 2$ for the CHSH operator $B$ introduced by Cirel'son is not unique. This non-uniqueness arises from the failure of linearity condition in the non-contextual hidden-variables model in $d=4$ used by Bell and CHSH, in agreement with Gleason's theorem which excludes $d=4$ non-contextual hidden-variables models. If one imposes the linearity condition, their model is converted to a factored product of two $d=2$ models which describes quantum mechanical separable states. The CHSH inequality thus does not test the hidden-variables model in $d=4$. This observation is consistent with an application of the CHSH inequality to quantum cryptography by Ekert, which is based on mixed separable states without referring to dispersion-free representations. As for hidden-variables models, there exist no viable local non-contextual models in any dimensions. 	
1303.2256v2	http://arxiv.org/pdf/1303.2256v2	2013	Migraine generator network and spreading depression dynamics as   neuromodulation targets in episodic migraine	Markus A. Dahlem	  Migraine is a common disabling headache disorder characterized by recurrent episodes sometimes preceded or accompanied by focal neurological symptoms called aura. The relation between two subtypes, migraine without aura (MWoA) and migraine with aura (MWA), is explored with the aim to identify targets for neuromodulation techniques. To this end, a dynamically regulated control system is schematically reduced to a network of the trigeminal nerve, which innervates the cranial circulation, an associated descending modulatory network of brainstem nuclei, and parasympathetic vasomotor efferents. This extends the idea of a migraine generator region in the brainstem to a larger network and is still simple and explicit enough to open up possibilities for mathematical modeling in the future. In this study, it is suggested that the migraine generator network (MGN) is driven and may therefore respond differently to different spatio-temporal noxious input in the migraine subtypes MWA and MWoA. The noxious input is caused by a cortical perturbation of homeostasis, known as spreading depression (SD). The MGN might even trigger SD in the first place by a failure in vasomotor control. As a consequence, migraine is considered as an inherently dynamical disease to which a linear course from upstream to downstream events would not do justice. Minimally invasive and noninvasive neuromodulation techniques are briefly reviewed and their rational is discussed in the context of the proposed mechanism. 	
1304.1933v1	http://arxiv.org/pdf/1304.1933v1	2013	Improvements to Kramers Turnover Theory	Eli Pollak|Joachim Ankerhold	  The Kramers turnover problem, that is obtaining a uniform expression for the rate of escape of a particle over a barrier for any value of the external friction was solved in the eighties. Two formulations were given, one by Melnikov and Meshkov (MM) (J. Chem. Phys. 85, 1018 (1986)), which was based on a perturbation expansion for the motion of the particle in the presence of friction. The other, by Pollak, Grabert and Haenggi (PGH) (J. Chem. Phys. 91, 4073 (1989)), valid also for memory friction, was based on a perturbation expansion for the motion along the collective unstable normal mode of the particle. Both theories did not take into account the temperature dependence of the average energy loss to the bath. Increasing the bath temperature will reduce the average energy loss. In this paper, we analyse this effect, using a novel perturbation theory. We find that within the MM approach, the thermal energy gained from the bath diverges, the average energy gain becomes infinite, implying an essential failure of the theory. Within the PGH approach increasing the bath temperature reduces the average energy loss but only by a finite small amount, of the order of the inverse of the reduced barrier height. This then does not seriously affect the theory. Analysis and application for a cubic potential and Ohmic friction are presented. 	
1305.0727v2	http://arxiv.org/pdf/1305.0727v2	2013	Arterial stiffening provides sufficient explanation for primary   hypertension	Klas H. Pettersen|Scott M. Bugenhagen|Javaid Nauman|Daniel A. Beard|Stig W. Omholt	  Hypertension is one of the most common age-related chronic diseases and by predisposing individuals for heart failure, stroke and kidney disease, it is a major source of morbidity and mortality. Its etiology remains enigmatic despite intense research efforts over many decades. By use of empirically well-constrained computer models describing the coupled function of the baroreceptor reflex and mechanics of the circulatory system, we demonstrate quantitatively that arterial stiffening seems sufficient to explain age-related emergence of hypertension. Specifically, the empirically observed chronic changes in pulse pressure with age, and the impaired capacity of hypertensive individuals to regulate short-term changes in blood pressure, arise as emergent properties of the integrated system. Results are consistent with available experimental data from chemical and surgical manipulation of the cardio-vascular system. In contrast to widely held opinions, the results suggest that primary hypertension can be attributed to a mechanogenic etiology without challenging current conceptions of renal and sympathetic nervous system function. The results support the view that a major target for treating chronic hypertension in the elderly is the reestablishment of a proper baroreflex response. 	
1305.2190v1	http://arxiv.org/pdf/1305.2190v1	2013	Scalable Routing Easy as PIE: a Practical Isometric Embedding Protocol   (Technical Report)	Julien Herzen|Cedric Westphal|Patrick Thiran	  We present PIE, a scalable routing scheme that achieves 100% packet delivery and low path stretch. It is easy to implement in a distributed fashion and works well when costs are associated to links. Scalability is achieved by using virtual coordinates in a space of concise dimensionality, which enables greedy routing based only on local knowledge. PIE is a general routing scheme, meaning that it works on any graph. We focus however on the Internet, where routing scalability is an urgent concern. We show analytically and by using simulation that the scheme scales extremely well on Internet-like graphs. In addition, its geometric nature allows it to react efficiently to topological changes or failures by finding new paths in the network at no cost, yielding better delivery ratios than standard algorithms. The proposed routing scheme needs an amount of memory polylogarithmic in the size of the network and requires only local communication between the nodes. Although each node constructs its coordinates and routes packets locally, the path stretch remains extremely low, even lower than for centralized or less scalable state-of-the-art algorithms: PIE always finds short paths and often enough finds the shortest paths. 	
1306.1529v2	http://arxiv.org/pdf/1306.1529v2	2014	Scaling Properties of Rainfall-Induced Landslides Predicted by a   Physically Based Model	M. Alvioli|F. Guzzetti|M. Rossi	  Natural landslides exhibit scaling properties revealed by power law relationships. These relationships include the frequency of the size (e.g., area, volume) of the landslides, and the rainfall conditions responsible for slope failures in a region. Reasons for the scaling behavior of landslides are poorly known. We investigate the possibility of using the Transient Rainfall Infiltration and Grid-Based Regional Slope-Stability analysis code (TRIGRS), a consolidated, physically-based, numerical model that describes the stability/instability conditions of natural slopes forced by rainfall, to determine the frequency statistics of the area of the unstable slopes and the rainfall intensity (I) - duration (D) conditions that result in landslides in a region. We apply TRIGRS in a portion of the Upper Tiber River Basin, Central Italy. The spatially distributed model predicts the stability/instability conditions of individual grid cells, given the local terrain and rainfall conditions. We run TRIGRS using multiple, synthetic rainfall histories, and we compare the modeling results with empirical evidences of the area of landslides and of the rainfall conditions that have caused landslides in the study area. Our findings revealed that TRIGRS is capable of reproducing the frequency of the size of the patches of terrain predicted as unstable by the model, which match the frequency size statistics of landslides in the study area, and the mean rainfall D, I conditions that result in unstable slopes in the study area, which match rainfall I-D thresholds for possible landslide occurrence. Our results are a step towards understanding the mechanisms that give rise to landslide scaling properties. 	
1307.1097v3	http://arxiv.org/pdf/1307.1097v3	2013	A construction principle for ADM-type theories in maximal slicing gauge	Henrique Gomes	  The differing concepts of time in general relativity and quantum mechanics are widely accused as the main culprits in our persistent failure in finding a complete theory of quantum gravity. Here we address this issue by constructing ADM-type theories \emph{in a particular time gauge} directly from first principles. The principles are expressed as conditions on phase space constraints: we search for two sets of spatially covariant constraints, which generate symmetries (are first class) and gauge-fix each other leaving two propagating degrees of freedom.   One of the sets is the Weyl generator tr$(\pi)$, and the other is a one-parameter family containing the ADM scalar constraint $\lambda R- \beta(\pi^{ab}\pi_{ab}+(\mbox{tr}(\pi))^2/2))$. The two sets of constraints can be seen as defining ADM-type theories with a maximal slicing gauge-fixing. This work provides an independent, first principles derivation of ADM gravity.   The principles above are motivated by a heuristic argument relying in the relation between symmetry doubling and exact renormalization arguments for quantum gravity, aside from compatibility with the spatial diffeomorphisms. As a by-product, these results address one of the most popular criticisms of Shape Dynamics: its construction starts off from the ADM Hamiltonian formulation.   The present work severs this dependence: the set of constraints yield reduced phase space theories that can be naturally represented by either Shape Dynamics or ADM. More precisely, the resulting theories can be naturally "unfixed" to encompass either spatial Weyl invariance (the symmetry of Shape Dynamics) or refoliation symmetry (ADM). 	
1308.0959v4	http://arxiv.org/pdf/1308.0959v4	2017	LORD: Leader-based framework for Resource Discovery in Mobile Device   Clouds	Seyed Mohammad Asghari|Yi-Hsuan Kao|Mohammad Hassan Lotfi|Mohammad Noormohammadpour|Bhaskar Krishnamachari|Babak Hossein Khalaj|Marcos Katz	  We provide a novel solution for Resource Discovery (RD) in mobile device clouds consisting of selfish nodes. Mobile device clouds (MDCs) refer to cooperative arrangement of communication-capable devices formed with resource-sharing goal in mind. Our work is motivated by the observation that with ever-growing applications of MDCs, it is essential to quickly locate resources offered in such clouds, where the resources could be content, computing resources, or communication resources. The current approaches for RD can be categorized into two models: decentralized model, where RD is handled by each node individually; and centralized model, where RD is assisted by centralized entities like cellular network. However, we propose LORD, a Leader-based framewOrk for RD in MDCs which is not only self-organized and not prone to having a single point of failure like the centralized model, but also is able to balance the energy consumption among MDC participants better than the decentralized model. Moreover, we provide a credit-based incentive to motivate participation of selfish nodes in the leader selection process, and present the first energy-aware leader selection mechanism for credit-based models. The simulation results demonstrate that LORD balances energy consumption among nodes and prolongs overall network lifetime compared to decentralized model. 	
1308.1862v1	http://arxiv.org/pdf/1308.1862v1	2013	Percolation of Interdependent Networks with Inter-similarity	Yanqing Hu|Dong Zhou|Rui Zhang|Zhangang Han|Shlomo Havlin	  Real data show that interdependent networks usually involve inter-similarity. Intersimilarity means that a pair of interdependent nodes have neighbors in both networks that are also interdependent (Parshani et al \cite{PAR10B}). For example, the coupled world wide port network and the global airport network are intersimilar since many pairs of linked nodes (neighboring cities), by direct flights and direct shipping lines exist in both networks. Nodes in both networks in the same city are regarded as interdependent. If two neighboring nodes in one network depend on neighboring nodes in the another we call these links common links. The fraction of common links in the system is a measure of intersimilarity. Previous simulation results suggest that intersimilarity has considerable effect on reducing the cascading failures, however, a theoretical understanding on this effect on the cascading process is currently missing. Here, we map the cascading process with inter-similarity to a percolation of networks composed of components of common links and non common links. This transforms the percolation of inter-similar system to a regular percolation on a series of subnetworks, which can be solved analytically. We apply our analysis to the case where the network of common links is an Erd\H{o}s-R\'{e}nyi (ER) network with the average degree $K$, and the two networks of non-common links are also ER networks. We show for a fully coupled pair of ER networks, that for any $K\geq0$, although the cascade is reduced with increasing $K$, the phase transition is still discontinuous. Our analysis can be generalized to any kind of interdependent random networks system. 	
1310.5722v2	http://arxiv.org/pdf/1310.5722v2	2014	Architecture of the Florida Power Grid as a Complex Network	Yan Xu|Aleks Jacob Gurfinkel|Per Arne Rikvold	  We study the Florida high-voltage power grid as a technological network embedded in space. Measurements of geographical lengths of transmission lines, the mixing of generators and loads, the weighted clustering coefficient, as well as the organization of edge conductance weights show a complex architecture quite different from random-graph models usually considered. In particular, we introduce a parametrized mixing matrix to characterize the mixing pattern of generators and loads in the Florida Grid, which is intermediate between the random mixing case and the semi-bipartite case where generator-generator transmission lines are forbidden. Our observations motivate an investigation of optimization (design) principles leading to the structural organization of power grids. We thus propose two network optimization models for the Florida Grid as a case study. Our results show that the Florida Grid is optimized not only by reducing the construction cost (measured by the total length of power lines), but also through reducing the total pairwise edge resistance in the grid, which increases the robustness of power transmission between generators and loads against random line failures. We then embed our models in spatial areas of different aspect ratios and study how this geometric factor affects the network structure, as well as the box-counting fractal dimension of the grids generated by our models. 	
1310.8386v1	http://arxiv.org/pdf/1310.8386v1	2013	Intrinsic VHE Gamma-ray spectra of Blazars as a probe for Extragalactic   Background Light	K K Singh|S Sahayanathan|A K Tickoo|N Bhatt	  Very high energy (VHE) $\gamma$-rays above 10$'$s of GeV energy, emitted from distant blazars, are attenuated by photons from the extragalactic background light (EBL). Unfortunately, neither the EBL nor the intrinsic blazar spectrum is accurately known to derive one quantity from the other. In this work we use a homogeneous one zone model involving synchrotron, synchrotron self Compton (SSC) and external Compton (EC) emission mechanisms to estimate the intrinsic VHE spectra of blazars. The model is applied on three VHE blazars, namely PKS2155-304, RGB J0710+591 and 3C279, for which simultaneous multi-wavelength data are available from various observations. The predicted values of the intrinsic VHE fluxes are then compared with the observations by imaging atmospheric Cherenkov telescopes to determine the optical depth of VHE $\gamma$-rays. On comparing these optical depth values with those predicted by four different EBL models, we observe a somewhat pronounced systematic deviation for PKS2155-304 and 3C279 at higher energies, especially for the EBL model proposed by Finke et al.(2010). We attribute this deviation to be an outcome of either the failure of the extrapolation of blazar SED to VHE energies and/or due to various assumptions buried in the EBL models. 	
1311.0047v2	http://arxiv.org/pdf/1311.0047v2	2014	Bayesian inferences of galaxy formation from the K-band luminosity and   HI mass functions of galaxies: constraining star formation and feedback	Yu Lu|H. J. Mo|Zhankui Lu|Neal Katz|Martin D. Weinberg	  We infer mechanisms of galaxy formation for a broad family of semi-analytic models (SAMs) constrained by the K-band luminosity function and HI mass function of local galaxies using tools of Bayesian analysis. Even with a broad search in parameter space the whole model family fails to match to constraining data. In the best fitting models, the star formation and feedback parameters in low-mass haloes are tightly constrained by the two data sets, and the analysis reveals several generic failures of models that similarly apply to other existing SAMs. First, based on the assumption that baryon accretion follows the dark matter accretion, large mass-loading factors are required for haloes with circular velocities lower than 200 km/s, and most of the wind mass must be expelled from the haloes. Second, assuming that the feedback is powered by Type-II supernovae with a Chabrier IMF, the outflow requires more than 25% of the available SN kinetic energy. Finally, the posterior predictive distributions for the star formation history are dramatically inconsistent with observations for masses similar to or smaller than the Milky-Way mass. The inferences suggest that the current model family is still missing some key physical processes that regulate the gas accretion and star formation in galaxies with masses below that of the Milky Way. 	
1311.6546v1	http://arxiv.org/pdf/1311.6546v1	2013	The Compactness of Presupernova Stellar Cores	Tuguldur Sukhbold|Stan Woosley	  The success or failure of the neutrino-transport mechanism for producing a supernova in an evolved massive star is known to be sensitive not only to the mass of the iron core that collapses, but also to the density gradient in the silicon and oxygen shells surrounding that core. Here we study the systematics of a presupernova core's "compactness" (O'Connor & Ott 2011) as a function of the mass of the star and the physics used in its calculation. Fine-meshed surveys of presupernova evolution are calculated for stars from 15 to 65 Msun. The metallicity and the efficiency of semiconvection and overshoot mixing are both varied and bare carbon-oxygen cores are explored as well as full hydrogenic stars. Two different codes, KEPLER and MESA, are used for the study. A complex interplay of carbon and oxygen burning, especially in shells, can cause rapid variations in the compactness for stars of very nearly the same mass. On larger scales, the distribution of compactness with main sequence mass is found to be robustly non-monotonic, implying islands of "explodability", particularly around 8 to 20 Msun and 25 to 30 Msun. The carbon-oxygen (CO) core mass of a presupernova star is a better, though still ambiguous discriminant of its core structure than the main sequence mass. 	
1312.2323v1	http://arxiv.org/pdf/1312.2323v1	2013	Architectural Pattern of Health Care System Using GSM Networks	Meiappane. A|Dr. V. Prasanna Venkatesan|Selva Murugan. S|Arun. A|Ramachandran. A	  Large-scale networked environments, such as the Internet, possess the characteristics of centralised data, centralised access and centralised control; this gives the user a powerful mechanism for building and integrating large repositories of centralised information from diverse resources set. However, a centralised network system with GSM Networks development for a hospital information systems or a health care information portal is still in its infancy. The shortcomings of the currently available tools have made the use of mobile devices more appealing. In mobile computing, the issues such as low bandwidth, high latency wireless Networks, loss or degradation of wireless connections, and network errors or failures need to be dealt with. Other issues to be addressed include system adaptability, reliability, robustness, extensibility, flexibility, and maintainability. GSM approach has emerged as the most viable approach for development of intelligent software applications for wireless mobile devices in a centralized environment, which gives higher band width of 900 MHz for transmission. The e-healthcare system that we have developed provides support for physicians, nurses, pharmacists and other healthcare professionals, as well as for patients and medical devices used to monitor patients. In this paper, we present the architecture and the demonstration prototype. 	
1312.5543v1	http://arxiv.org/pdf/1312.5543v1	2013	Improvements to the Prototype Micro-Brittle Linear Elasticity Model of   Peridynamics	Georg C. Ganzenmüller|Stefan Hiermaier|Michael May	  This paper assesses the accuracy and convergence of the linear-elastic, bond-based Peridynamic model with brittle failure, known as the prototype micro-brittle (PMB) model. We investigate the discrete equations of this model, suitable for numerical implementation. It is shown that the widely used discretization approach incurs rather large errors. Motivated by this observation, a correction is proposed, which significantly increases the accuracy by cancelling errors associated with the discretization. As an additional result, we derive equations to treat the interactions between differently sized particles, i.e., a non-homogeneous discretization spacing. This presents an important step forward for the applicability of the PMB model to complex geometries, where it is desired to model interesting parts with a fine resolution (small particle spacings) and other parts with a coarse resolution in order to gain numerical efficiency. Validation of the corrected Peridynamic model is performed by comparing longitudinal sound wave propagation velocities with exact theoretical results. We find that the corrected approach correctly reproduces the sound wave velocity, while the original approach severely overestimates this quantity. Additionally, we present simulations for a crack growth problem which can be analytically solved within the framework of Linear Elastic Fracture Mechanics Theory. We find that the corrected Peridynamics model is capable of quantitatively reproducing crack initiation and propagation. 	
1402.2646v1	http://arxiv.org/pdf/1402.2646v1	2014	A Performance-Based Framework for Bridge Preservation Based on   Damage-Integrated System-Level Behavior	Amir Gheitasi|Davin K. Harris	  The safety and condition of transportation infrastructure has been at the forefront of national debates in recent times due to catastrophic bridge failures, but the issue has been a longstanding challenge for transportation agencies for many years as resources continue to diminish. The performance of this infrastructure has a direct influence on the lives of most of citizens in developed regions by providing a critical lifeline between communities and the transportation of goods and services, and as a critical component of the transportation network, bridges have received a lot of attention regarding condition assessment and maintenance practices. Despite successful implementation of advanced evaluation techniques, what is still lacking is a fundamental understanding of the system behavior in the presence of deteriorating conditions that can be used for preservation decision-making. This paper aims to present a performance-based framework that can be used to characterize the behavior of in-service bridge superstructures. In order to measure the bridge system performance with deteriorating conditions, system-level behavior of a representative composite steel girder bridge, degraded with three common damage scenarios was investigated in this study. Results obtained from validated numerical analysis demonstrated significant impact of integrated damage mechanisms on the ultimate capacity, redundancy and system ductility of the simulated bridge superstructure. It is expected that the proposed framework for evaluating system behavior will provide a first step for establishing a critical linkage between design, maintenance, and rehabilitation of highway bridges, which are uncoupled in current infrastructure decision-making processes. 	
1402.5770v1	http://arxiv.org/pdf/1402.5770v1	2014	The Case for Cloud Service Trustmarks and Assurance-as-a-Service	Theo Lynn|Philip Healy|Richard McClatchey|John Morrison|Claus Pahl|Brian Lee	  Cloud computing represents a significant economic opportunity for Europe. However, this growth is threatened by adoption barriers largely related to trust. This position paper examines trust and confidence issues in cloud computing and advances a case for addressing them through the implementation of a novel trustmark scheme for cloud service providers. The proposed trustmark would be both active and dynamic featuring multi-modal information about the performance of the underlying cloud service. The trustmarks would be informed by live performance data from the cloud service provider, or ideally an independent third-party accountability and assurance service that would communicate up-to-date information relating to service performance and dependability. By combining assurance measures with a remediation scheme, cloud service providers could both signal dependability to customers and the wider marketplace and provide customers, auditors and regulators with a mechanism for determining accountability in the event of failure or non-compliance. As a result, the trustmarks would convey to consumers of cloud services and other stakeholders that strong assurance and accountability measures are in place for the service in question and thereby address trust and confidence issues in cloud computing. 	
1402.6557v1	http://arxiv.org/pdf/1402.6557v1	2014	Mean-field dynamos: the old concept and some recent developments	K. -H. Rädler	  This article reproduces the Karl Schwarzschild lecture 2013. Some of the basic ideas of electrodynamics and magnetohydrodynamics of mean fields in turbulently moving conducting fluids are explained. It is stressed that the connection of the mean electromotive force with the mean magnetic field and its first spatial derivatives is in general neither local nor instantaneous and that quite a few claims concerning pretended failures of the mean-field concept result from ignoring this aspect. In addition to the mean-field dynamo mechanisms of $\alpha^2$ and $\alpha$ $\Omega$ type several others are considered. Much progress in mean-field electrodynamics and magnetohydrodynamics results from the test-field method for calculating the coefficients that determine the connection of the mean electromotive force with the mean magnetic field. As an important example the memory effect in homogeneous isotropic turbulence is explained. In magnetohydrodynamic turbulence there is the possibility of a mean electromotive force that is primarily independent of the mean magnetic field and labeled as Yoshizawa effect. Despite of many efforts there is so far no convincing comprehensive theory of $\alpha$ quenching, that is, the reduction of the $\alpha$ effect with growing mean magnetic field, and of the saturation of mean-field dynamos. Steps toward such a theory are explained. Finally, some remarks on laboratory experiments with dynamos are made. 	
1403.6162v1	http://arxiv.org/pdf/1403.6162v1	2014	Entropy is in Flux	Leo P. Kadanoff	  The science of thermodynamics was put together in the Nineteenth Century to describe large systems in equilibrium. One part of thermodynamics defines entropy for equilibrium systems and demands an ever-increasing entropy for non-equilibrium ones. However, starting with the work of Ludwig Boltzmann in 1872, and continuing to the present day, various models of non-equilibrium behavior have been put together with the specific aim of generalizing the concept of entropy to non-equilibrium situations. This kind of entropy has been termed {\em kinetic entropy} to distinguish it from the thermodynamic variety. Knowledge of kinetic entropy started from Boltzmann's insight about his equation for the time dependence of gaseous systems. In this paper, his result is stated as a definition of kinetic entropy in terms of a local equation for the entropy density. This definition is then applied to Landau's theory of the Fermi liquid thereby giving the kinetic entropy within that theory.   Entropy has been defined and used for a wide variety of situations in which a condensed matter system has been allowed to relax for a sufficient period so that the very most rapid fluctuations have been ironed out. One of the broadest applications of non-equilibrium analysis considers quantum degenerate systems using Martin-Schwinger Green's functions\cite{MS} as generalized of Wigner functions, $g^<$ and $g^>$. This paper describes once again these how the quantum kinetic equations for these functions give locally defined conservation laws for mass momentum and energy. In local thermodynamic equilibrium, this kinetic theory enables a reasonable local definition of entropy density. However, when the system is outside of local equilibrium, this definition fails. It is speculated that quantum entanglement is the source of this failure. 	
1404.0696v1	http://arxiv.org/pdf/1404.0696v1	2014	D-P2P-Sim+:A Novel Distributed Framework for P2P Protocols Performance   Testing	S. Sioutas|E. Sakkopoulos|A. Panaretos|D. Tsoumakos|P. Gerolymatos|G. Tzimas|Y. Manolopoulos	  In recent IoT (Internet of Things) and Web 2.0 technologies, a critical problem arises with respect to storing and processing the large amount of collected data. In this paper we develop and evaluate distributed infrastructures for storing and processing large amount of such data. We present a distributed framework that supports customized deployment of a variety of indexing engines over million-node overlays. The proposed framework provides the appropriate integrated set of tools that allows applications processing large amount of data, to evaluate and test the performance of various application protocols for very large scale deployments (multi million nodes - billions of keys). The key aim is to provide the appropriate environment that contributes in taking decisions regarding the choice of the protocol in storage P2P systems for a variety of big data applications. Using lightweight and efficient collection mechanisms, our system enables real-time registration of multiple measures, integrating support for real-life parameters such as node failure models and recovery strategies. Experiments have been performed at the PlanetLab network and at a typical research laboratory in order to verify scalability and show maximum re-usability of our setup. D-P2P-Sim+ framework is publicly available at http://code.google.com/p/d-p2p-sim/downloads/list. 	
1404.2210v2	http://arxiv.org/pdf/1404.2210v2	2014	A kinematic study of energy barriers to crack formation in graphene tilt   boundaries	Matthew Daly|Chandra Veer Singh	  Recent experimental studies have observed a surprisingly wide range of strengths in polycrystalline graphene. Previous computational investigations of graphene tilt boundaries have highlighted the role of interfacial topology in determining mechanical properties. However, a rigorous characterization of deformation energy barriers is lacking, which precludes direct comparison to the available experimental data. In the current study, molecular dynamics tensile studies are performed to quantify kinematic effects on failure initiation in a wide range of graphene tilt boundaries. Specifically, the process of crack formation is investigated to provide a conservative estimate of strength at experimental loading rates. Contrary to previous studies, significant strain rate sensitivity is observed, resulting in reductions of crack formation stresses on the order of 7 to 33%. Activation energies of crack formation are calculated in the range of 0.58 to 2.07 eV based on an Arrhenius relation that is fit to the collected simulation data. Physically, the magnitude of activation energies in graphene tilt boundaries are found to be linearly correlated to the pre-stress found at the critical bonds in graphene tilt boundaries. Predictions reported in the present study provide a possible explanation for the wide range of strengths experimentally observed in polycrystalline graphene and greatly improve upon current theoretical estimates. 	
1404.2713v1	http://arxiv.org/pdf/1404.2713v1	2014	Transaction Handling in COM, EJB and .NET	Ditmar Parmeza|Miraldi Fifo	  The technology evolution has shown a very impressive performance in the last years by introducing several technologies that are based on the concept of component. As time passes, new versions of Component- Based technologies are released in order to improve services provided by previous ones. One important issue that regards these technologies is transactional activity. Transactions are important because they consist in sending different small amounts of information collected properly in a single combined unit which makes the process simpler, less expensive and also improves the reliability of the whole system, reducing its chances to go through possible failures. Different Component-Based technologies offer different ways of handling transactions. In this paper, we will review and discuss how transactions are handled in three of them: COM, EJB and .NET. It can be expected that .NET offers more efficient mechanisms due to the fact of being released later than the other two technologies. Nevertheless, COM and EJB are still present in the market and their services are still widely used. Comparing transaction handling in these technologies will be helpful to analyze the advantages and disadvantages of each of them. This comparison and evaluation will be seen in two main perspectives: performance and security. 	
1405.3558v1	http://arxiv.org/pdf/1405.3558v1	2014	Aspects of Statistical Physics in Computational Complexity	Stefano Gogioso	  The aim of this review paper is to give a panoramic of the impact of spin glass theory and statistical physics in the study of the K-sat problem. The introduction of spin glass theory in the study of the random K-sat problem has indeed left a mark on the field, leading to some groundbreaking descriptions of the geometry of its solution space, and helping to shed light on why it seems to be so hard to solve.   Most of the geometrical intuitions have their roots in the Sherrington-Kirkpatrick model of spin glass. We'll start Chapter 2 by introducing the model from a mathematical perspective, presenting a selection of rigorous results and giving a first intuition about the cavity method. We'll then switch to a physical perspective, to explore concepts like pure states, hierarchical clustering and replica symmetry breaking.   Chapter 3 will be devoted to the spin glass formulation of K-sat, while the most important phase transitions of K-sat (clustering, condensation, freezing and SAT/UNSAT) will be extensively discussed in Chapter 4, with respect their complexity, free-entropy density and the Parisi 1RSB parameter.   The concept of algorithmic barrier will be presented in Chapter 5 and exemplified in detail on the Belief Propagation (BP) algorithm. The BP algorithm will be introduced and motivated, and numerical analysis of a BP-guided decimation algorithm will be used to show the role of the clustering, condensation and freezing phase transitions in creating an algorithmic barrier for BP.   Taking from the failure of BP in the clustered and condensed phases, Chapter 6 will finally introduce the Cavity Method to deal with the shattering of the solution space, and present its application to the development of the Survey Propagation algorithm. 	
1405.3704v1	http://arxiv.org/pdf/1405.3704v1	2014	Internal stresses and breakup of rigid isostatic aggregates in   homogeneous and isotropic turbulence	Jeremias De Bona|Alessandra S. Lanotte|Marco Vanni	  By characterising the hydrodynamic stresses generated by statistically homogeneous and isotropic turbulence in rigid aggregates, we estimate theoretically the rate of turbulent breakup of colloidal aggregates and the size distribution of the formed fragments. The adopted method combines Direct Numerical Simulation of the turbulent field with a Discrete Element Method based on Stokesian dynamics. In this way, not only the mechanics of the aggregate is modelled in detail, but the internal stresses are evaluated while the aggregate is moving in the turbulent flow. We examine doublets and cluster-cluster isostatic aggregates, where the failure of a single contact leads to the rupture of the aggregate and breakup occurs when the tensile force at a contact exceeds the cohesive strength of the bond. Due to the different role of the internal stresses, the functional relationship between breakup frequency and turbulence dissipation rate is very different in the two cases. In the limit of very small and very large values, the frequency of breakup scales exponentially with the turbulence dissipation rate for doublets, while it follows a power law for cluster-cluster aggregates. For the case of large isostatic aggregates it is confirmed that the proper scaling length for maximum stress and breakup is the radius of gyration. The cumulative fragment distribution function is nearly independent of the mean turbulence dissipation rate and can be approximated by the sum of a small erosive component and a term that is quadratic with respect to fragment size. 	
1408.0640v1	http://arxiv.org/pdf/1408.0640v1	2014	Reconstruction of disease transmission rates: applications to measles,   dengue, and influenza	Alexander Lange	  Transmission rates are key in understanding the spread of infectious diseases. Using the framework of compartmental models, we introduce a simple method that enables us to reconstruct time series of transmission rates directly from incidence or disease-related mortality data. The reconstruction exploits differential equations, which model the time evolution of infective stages and strains. Being sensitive to initial values, the method produces asymptotically correct solutions. The computations are fast, with time complexity being quadratic. We apply the reconstruction to data of measles (England and Wales, 1948-67), dengue (Thailand, 1982-99), and influenza (U.S., 1910-27). The Measles example offers comparison with earlier work. Here we re-investigate reporting corrections, include and exclude demographic information. The dengue example deals with the failure of vector-control measures in reducing dengue hemorrhagic fever (DHF) in Thailand. Two competing mechanisms have been held responsible: strain interaction and demographic transitions. Our reconstruction reveals that both explanations are possible, showing that the increase in DHF cases is consistent with decreasing transmission rates resulting from reduced vector counts. The flu example focuses on the 1918/19 pandemic, examining the transmission rate evolution for an invading strain. Our analysis indicates that the pandemic strain could have circulated in the population for many months before the pandemic was initiated by an event of highly increased transmission. 	
1409.1652v2	http://arxiv.org/pdf/1409.1652v2	2014	Elementary analysis of interferometers for wave-particle duality test   and the perspective of going beyond the complementarity principle	Zhi-Yuan Li	  Wave-particle duality and complementarity principle stand at the conceptual core of quantum theory in its orthodox Copenhagen interpretation. They imply that the wave behavior and particle behavior of quantum objects are mutually exclusive to each other in experimental observation. Here we make a systematic analysis using the elementary methodology of quantum mechanics upon   Young`s two-slit interferometer and Mach-Zehnder two-arm interferometer with the focus placed on how to measure the interference pattern (wave nature) and which-way information (particle nature) of quantum objects. We design several schemes to simultaneously acquire the which-way information for an individual quantum object and the high-contrast interference pattern for an ensemble of these quantum objects by placing two sets of measurement instrument that are well separated in space and whose perturbation on each other is negligibly small within the interferometer at the same time. Yet, improper arrangement and cooperation of these two sets of measurement instrument in the interferometer would lead to failure of simultaneous observation of wave and particle behavior.   The internal freedoms of quantum object could be harnessed to probe both the which-way information and interference pattern for the center-of-mass motion. That quantum objects can behave beyond the wave-particle duality and complementarity principle would stimulate new conceptual examination and exploration of quantum theory at a deeper level. 	
1409.3837v1	http://arxiv.org/pdf/1409.3837v1	2014	Instability and network effects in innovative markets	Paolo Sgrignoli|Elena Agliari|Raffaella Burioni|Augusto Schianchi	  We consider a network of interacting agents and we model the process of choice on the adoption of a given innovative product by means of statistical-mechanics tools. The modelization allows us to focus on the effects of direct interactions among agents in establishing the success or failure of the product itself. Mimicking real systems, the whole population is divided into two sub-communities called, respectively, Innovators and Followers, where the former are assumed to display more influence power. We study in detail and via numerical simulations on a random graph two different scenarios: no-feedback interaction, where innovators are cohesive and not sensitively affected by the remaining population, and feedback interaction, where the influence of followers on innovators is non negligible. The outcomes are markedly different: in the former case, which corresponds to the creation of a niche in the market, Innovators are able to drive and polarize the whole market. In the latter case the behavior of the market cannot be definitely predicted and become unstable. In both cases we highlight the emergence of collective phenomena and we show how the final outcome, in terms of the number of buyers, is affected by the concentration of innovators and by the interaction strengths among agents. 	
1409.6883v1	http://arxiv.org/pdf/1409.6883v1	2014	Performance Analysis of Faults Detection in Wind Turbine Generator Based   on High-Resolution Frequency Estimation Methods	Saad Chakkor|Mostafa Baghouri|Abderrahmane Hajraoui	  Electrical energy production based on wind power has become the most popular renewable resources in the recent years because it gets reliable clean energy with minimum cost. The major challenge for wind turbines is the electrical and the mechanical failures which can occur at any time causing prospective breakdowns and damages and therefore it leads to machine downtimes and to energy production loss. To circumvent this problem, several tools and techniques have been developed and used to enhance fault detection and diagnosis to be found in the stator current signature for wind turbines generators. Among these methods, parametric or super-resolution frequency estimation methods, which provides typical spectrum estimation, can be useful for this purpose. Facing on the plurality of these algorithms, a comparative performance analysis is made to evaluate robustness based on different metrics: accuracy, dispersion, computation cost, perturbations and faults severity. Finally, simulation results in MATLAB with most occurring faults indicate that ESPRIT and R-MUSIC algorithms have high capability of correctly identifying the frequencies of fault characteristic components, a performance ranking had been carried out to demonstrate the efficiency of the studied methods in faults detecting. 	
1410.0117v2	http://arxiv.org/pdf/1410.0117v2	2014	Coupling Top-down and Bottom-up Methods for 3D Human Pose and Shape   Estimation from Monocular Image Sequences	Atul Kanaujia	  Until recently Intelligence, Surveillance, and Reconnaissance (ISR) focused on acquiring behavioral information of the targets and their activities. Continuous evolution of intelligence being gathered of the human centric activities has put increased focus on the humans, especially inferring their innate characteristics - size, shapes and physiology. These bio-signatures extracted from the surveillance sensors can be used to deduce age, ethnicity, gender and actions, and further characterize human actions in unseen scenarios. However, recovery of pose and shape of humans in such monocular videos is inherently an ill-posed problem, marked by frequent depth and view based ambiguities due to self-occlusion, foreshortening and misalignment. The likelihood function often yields a highly multimodal posterior that is difficult to propagate even using the most advanced particle filtering(PF) algorithms. Motivated by the recent success of the discriminative approaches to efficiently predict 3D poses directly from the 2D images, we present several principled approaches to integrate predictive cues using learned regression models to sustain multimodality of the posterior during tracking. Additionally, these learned priors can be actively adapted to the test data using a likelihood based feedback mechanism. Estimated 3D poses are then used to fit 3D human shape model to each frame independently for inferring anthropometric bio-signatures. The proposed system is fully automated, robust to noisy test data and has ability to swiftly recover from tracking failures even after confronting with significant errors. We evaluate the system on a large number of monocular human motion sequences. 	
1410.8238v3	http://arxiv.org/pdf/1410.8238v3	2015	The Slicing Theory of Quantum Measurement: Derivation of Transient Many   Worlds Behavior	Clifford Chafin	  An emergent theory of quantum measurement arises directly by considering the particular subset of many body wavefunctions that can be associated with classical condensed matter and its interaction with delocalized wavefunctions. This transfers questions of the "strangeness" of quantum mechanics from the wavefunction to the macroscopic material itself. An effectively many-worlds picture of measurement results for long times and induces a natural arrow of time. The challenging part is then justifying why our macroscopic world is dominated by such far-from-eigenstate matter. Condensing cold mesoscopic clusters provide a pathway to a partitioning of a highly correlated many body wavefunction to long lasting islands composed of classical-like bodies widely separated in Fock space. Low mass rapidly delocalizing matter that recombines with the solids "slice" the system into a set of nearby yet very weakly interacting subsystems weighted according to the Born statistics and yields a kind of many worlds picture but with the possibility of revived phase interference on iterative particle desorption, delocalization and readsorption. A proliferation of low energy photons competes with such a possibility. Causality problems associated with correlated quantum measurement are resolved and conserved quantities are preserved for the overall many body function despite their failure in each observer's bifurcating "slice-path." The necessity of such a state for a two state logic and reliable discrete state machine suggests that later stages of the universe's evolution will destroy the physical underpinnings required for consciousness and the arrow of time even without heat-death or atomic destruction. Some exotic possibilities outside the domain of usual quantum measurement are considered such as measurement with delocalized devices and revival of information from past measurements. 	
1411.1020v1	http://arxiv.org/pdf/1411.1020v1	2014	Model of deep non-volcanic tremor part II: episodic tremor and slip	Naum I. Gershenzon|Gust Bambakidis	  Bursts of tremor accompany a moving slip pulse in Episodic Tremor and Slip (ETS) events. The sources of this non-volcanic tremor (NVT) are largely unknown. We have developed a model describing the mechanism of NTV generation. According to this model, NTV is a reflection of resonant-type oscillations excited in a fault at certain depth ranges. From a mathematical viewpoint, tremor (phonons) and slip pulses (solitons) are two different solutions of the sine-Gordon equation describing frictional processes inside a fault. In an ETS event, a moving slip pulse generates tremor due to interaction with structural heterogeneities in a fault and to failures of small asperities. Observed tremor parameters, such as central frequency and frequency attenuation curve, are associated with fault parameters and conditions, such as elastic modulus, effective normal stress, penetration hardness and friction. Model prediction of NTV frequency content is consistent with observations. In the framework of this model it is possible to explain the complicated pattern of tremor migration, including rapid tremor propagation and reverse tremor migration. Migration along the strike direction is associated with movement of the slip pulse. Rapid tremor propagation in the slip-parallel direction is associated with movement of kinks along a 2D slip pulse. A slip pulse, pinned in some places, can fragment into several pulses, causing tremor associated with some of these pulse fragments to move opposite to the main propagation direction. The model predicts that the frequency content of tremor during an ETS event is slightly different from the frequency content of ambient tremor and tremor triggered by earthquakes. 	
1411.2873v1	http://arxiv.org/pdf/1411.2873v1	2014	Improving Robustness of Next-Hop Routing	Glencora Borradaile|W. Sean Kennedy|Gordon Wilfong|Lisa Zhang	  A weakness of next-hop routing is that following a link or router failure there may be no routes between some source-destination pairs, or packets may get stuck in a routing loop as the protocol operates to establish new routes. In this article, we address these weaknesses by describing mechanisms to choose alternate next hops.   Our first contribution is to model the scenario as the following {\sc tree augmentation} problem. Consider a mixed graph where some edges are directed and some undirected. The directed edges form a spanning tree pointing towards the common destination node. Each directed edge represents the unique next hop in the routing protocol. Our goal is to direct the undirected edges so that the resulting graph remains acyclic and the number of nodes with outdegree two or more is maximized. These nodes represent those with alternative next hops in their routing paths.   We show that {\sc tree augmentation} is NP-hard in general and present a simple $\frac{1}{2}$-approximation algorithm. We also study 3 special cases. We give exact polynomial-time algorithms for when the input spanning tree consists of exactly 2 directed paths or when the input graph has bounded treewidth. For planar graphs, we present a polynomial-time approximation scheme when the input tree is a breadth-first search tree. To the best of our knowledge, {\sc tree augmentation} has not been previously studied. 	
1412.3148v2	http://arxiv.org/pdf/1412.3148v2	2016	Coarse Grained Quantum Dynamics	Cesar Agon|Vijay Balasubramanian|Skyler Kasko|Albion Lawrence	  We consider coarse graining a quantum system divided between short distance and long distance degrees of freedom, which are coupled by the Hamiltonian. Observations using purely long distance observables can be described by the reduced density matrix that arises from tracing out the short-distance observables. The dynamics of this density matrix is that of an open quantum system, and is nonlocal in time, on the order of some short time scale. We describe these dynamics in a model system with a simple hierarchy of energy gaps $\Delta E_{UV} > \Delta E_{IR}$, in which the coupling between high-and low-energy degrees of freedom is treated to second order in perturbation theory. We then describe the equations of motion under suitable time averaging, reflecting the limited time resolution of actual experiments, and find an expansion of the master equation in powers of $\Delta E_{IR}/\Delta E_{UV}$, in which the failure of the system to be Hamiltonian or even Markovian appears at higher orders in this ratio. We compute the evolution of the density matrix in two specific examples -- coupled spins, and linearly coupled simple harmonic oscillators. Finally, we discuss the evolution of the density matrix using the path integral approach, computing the Feynman-Vernon influence functional for the IR degrees of freedom in perturbation theory, and argue that this influence functional is the correct analog of the Wilsonian effective action for this problem. 	
1502.07309v2	http://arxiv.org/pdf/1502.07309v2	2015	Dimensional transitions in thermodynamic properties of ideal   Maxwell-Boltzmann gases	Alhun Aydin|Altug Sisman	  An ideal Maxwell-Boltzmann gas confined in various rectangular nano domains is considered under quantum size effects. Thermodynamic quantities are calculated from their relations with partition function which consists of triple infinite summations over momentum states in each direction. To get analytical expressions, summations are converted to integrals for macro systems by continuum approximation which fails at nanoscales. To avoid both from the numerical calculation of summations and the failure of their integral approximations at nanoscale, a method which gives an analytical expression for single particle partition function (SPPF) is proposed. It's shown that dimensional transition in momentum space occurs at certain magnitude of confinement. Therefore, to represent SPPF by lower-dimensional analytical expressions becomes possible rather than numerical calculation of summations. Considering rectangular domains with different aspect ratios, comparison of the results of derived expressions with those of summation forms of SPPF is done. It's shown that analytical expressions for SPPF give very precise results with maximum relative errors of around 1%, 2% and 3% at just the transition point for single, double and triple transitions respectively. Based on dimensional transitions, expressions for free energy, entropy, internal energy, chemical potential, heat capacity and pressure are given analytically valid for any scale. 	
1503.03944v2	http://arxiv.org/pdf/1503.03944v2	2015	A comprehensive lattice-stability limit surface for graphene	Sandeep Kumar|David Parks	  The limits of reversible deformation in graphene under various loadings are examined using lattice-dynamical stability analysis. This information is then used to construct a comprehensive lattice-stability limit surface for graphene, which provides an analytical description of incipient lattice instabilities of \textit{all kinds}, for arbitrary deformations, parametrized in terms of symmetry-invariants of strain/stress. Symmetry-invariants allow obtaining an accurate parametrization with a minimal number of coefficients. Based on this limit surface, we deduce a general continuum criterion for the onset of all kinds of lattice-stabilities in graphene: an instability appears when the magnitude of the deviatoric strain $\gamma$ reaches a critical value $\gamma^c$ which depends upon the mean hydrostatic strain $\bar {\mathcal E}$ and the directionality $\theta$ of the deviatoric stretch. We also distinguish between the distinct regions of the limit surface that correspond to fundamentally-different mechanisms of lattice instabilities in graphene, such as structural vs material instabilities, and long-wave (elastic) vs short-wave instabilities. Utility of this limit surface is demonstrated in assessment of incipient failures in defect-free graphene via its implementation in a continuum Finite Elements Analysis (FEA). The resulting scheme enables on-the-fly assessments of not only the macroscopic conditions (e.g., load; deflection) but also the microscopic conditions (e.g., local stress/strain; spatial location, temporal proximity, and nature of incipient lattice instability) at which an instability occurs in a defect-free graphene sheet subjected to an arbitrary loading condition. 	
1503.04058v1	http://arxiv.org/pdf/1503.04058v1	2015	Optimal redundancy against disjoint vulnerabilities in networks	Sebastian M. Krause|Michael M. Danziger|Vinko Zlatić	  Redundancy is commonly used to guarantee continued functionality in networked systems. However, often many nodes are vulnerable to the same failure or adversary. A "backup" path is not sufficient if both paths depend on nodes which share a vulnerability.For example, if two nodes of the Internet cannot be connected without using routers belonging to a given untrusted entity, then all of their communication-regardless of the specific paths utilized-will be intercepted by the controlling entity.In this and many other cases, the vulnerabilities affecting the network are disjoint: each node has exactly one vulnerability but the same vulnerability can affect many nodes. To discover optimal redundancy in this scenario, we describe each vulnerability as a color and develop a "color-avoiding percolation" which uncovers a hidden color-avoiding connectivity. We present algorithms for color-avoiding percolation of general networks and an analytic theory for random graphs with uniformly distributed colors including critical phenomena. We demonstrate our theory by uncovering the hidden color-avoiding connectivity of the Internet. We find that less well-connected countries are more likely able to communicate securely through optimally redundant paths than highly connected countries like the US. Our results reveal a new layer of hidden structure in complex systems and can enhance security and robustness through optimal redundancy in a wide range of systems including biological, economic and communications networks. 	
1503.04655v1	http://arxiv.org/pdf/1503.04655v1	2015	Percolation in real interdependent networks	Filippo Radicchi	  The function of a real network depends not only on the reliability of its own components, but is affected also by the simultaneous operation of other real networks coupled with it. Robustness of systems composed of interdependent network layers has been extensively studied in recent years. However, the theoretical frameworks developed so far apply only to special models in the limit of infinite sizes. These methods are therefore of little help in practical contexts, given that real interconnected networks have finite size and their structures are generally not compatible with those of graph toy models. Here, we introduce a theoretical method that takes as inputs the adjacency matrices of the layers to draw the entire phase diagram for the interconnected network, without the need of actually simulating any percolation process. We demonstrate that percolation transitions in arbitrary interdependent networks can be understood by decomposing these system into uncoupled graphs: the intersection among the layers, and the remainders of the layers. When the intersection dominates the remainders, an interconnected network undergoes a continuous percolation transition. Conversely, if the intersection is dominated by the contribution of the remainders, the transition becomes abrupt even in systems of finite size. We provide examples of real systems that have developed interdependent networks sharing a core of "high quality" edges to prevent catastrophic failures. 	
1503.07905v1	http://arxiv.org/pdf/1503.07905v1	2015	D3-Tree: A Dynamic Distributed Deterministic Load - Balancer for   decentralized tree structures	Efrosini Sourla|Spyros Sioutas|Kostas Tsichlas|Christos Zaroliagis	  In this work, we propose D3-Tree, a dynamic distributed deterministic structure for data management in decentralized networks. We present in brief the theoretical algorithmic analysis, in which our proposed structure is based on, and we describe thoroughly the key aspects of the implementation. Conducting experiments, we verify that the implemented structure outperforms other well-known hierarchical tree-based structures, since it provides better complexities regarding load-balancing operations. More specifically, the structure achieves a logarithmic amortized bound, using an efficient deterministic load-balancing mechanism, which is general enough to be applied to other hierarchical tree-based structures. Moreover, we investigate the structure's fault tolerance, which hasn't been sufficiently tackled in previous work, both theoretically and through rigorous experimentation. We prove that D3-Tree is highly fault tolerant, since, even for massive node failures, it achieves a significant success rate in element queries. Afterwards we go one step further, in order to achieve sub-logarithmic complexity and propose the ART+ structure (Autonomous Range Tree), exploiting the excellent performance of D3-Tree. ART+ is a fully dynamic and fault-tolerant structure, which achieves sub-logarithmic performance for query and update operations and performs load-balancing in sub-logarithmic amortized cost. 	
1506.06045v1	http://arxiv.org/pdf/1506.06045v1	2015	Data compression for the First G-APD Cherenkov Telescope	M. L. Ahnen|M. Balbo|M. Bergmann|A. Biland|T. Bretz|J. Buß|D. Dorner|S. Einecke|J. Freiwald|C. Hempfling|D. Hildebrand|G. Hughes|W. Lustermann|E. Lyard|K. Mannheim|K. Meier|S. Mueller|D. Neise|A. Neronov|A. -K. Overkemping|A. Paravac|F. Pauss|W. Rhode|T. Steinbring|F. Temme|J. Thaele|S. Toscano|P. Vogler|R. Walter|A. Wilbert	  The First Geiger-mode Avalanche photodiode (G-APD) Cherenkov Telescope (FACT) has been operating on the Canary island of La Palma since October 2011. Operations were automated so that the system can be operated remotely. Manual interaction is required only when the observation schedule is modified due to weather conditions or in case of unexpected events such as a mechanical failure. Automatic operations enabled high data taking efficiency, which resulted in up to two terabytes of FITS files being recorded nightly and transferred from La Palma to the FACT archive at ISDC in Switzerland. Since long term storage of hundreds of terabytes of observations data is costly, data compression is mandatory. This paper discusses the design choices that were made to increase the compression ratio and speed of writing of the data with respect to existing compression algorithms.   Following a more detailed motivation, the FACT compression algorithm along with the associated I/O layer is discussed. Eventually, the performances of the algorithm is compared to other approaches. 	
1507.04381v2	http://arxiv.org/pdf/1507.04381v2	2015	Complete Characterization of Stability of Cluster Synchronization in   Complex Dynamical Networks	Francesco Sorrentino|Louis M. Pecora|Aaron M. Hagerstrom|Thomas E. Murphy|Rajarshi Roy	  Synchronization is an important and prevalent phenomenon in natural and engineered systems. In many dynamical networks, the coupling is balanced or adjusted in order to admit global synchronization, a condition called Laplacian coupling. Many networks exhibit incomplete synchronization, where two or more clusters of synchronization persist, and computational group theory has recently proved to be valuable in discovering these cluster states based upon the topology of the network. In the important case of Laplacian coupling, additional synchronization patterns can exist that would not be predicted from the group theory analysis alone. The understanding of how and when clusters form, merge, and persist is essential for understanding collective dynamics, synchronization, and failure mechanisms of complex networks such as electric power grids, distributed control networks, and autonomous swarming vehicles. We describe here a method to find and analyze all of the possible cluster synchronization patterns in a Laplacian-coupled network, by applying methods of computational group theory to dynamically-equivalent networks. We present a general technique to evaluate the stability of each of the dynamically valid cluster synchronization patterns. Our results are validated in an electro-optic experiment on a 5 node network that confirms the synchronization patterns predicted by the theory. 	
1507.06278v3	http://arxiv.org/pdf/1507.06278v3	2015	Some Nearly Quantum Theories	Howard Barnum|Matthew A. Graydon|Alexander Wilce	  We consider possible non-signaling composites of probabilistic models based on euclidean Jordan algebras. Subject to some reasonable constraints, we show that no such composite exists having the exceptional Jordan algebra as a direct summand. We then construct several dagger compact categories of such Jordan-algebraic models. One of these neatly unifies real, complex and quaternionic mixed-state quantum mechanics, with the exception of the quaternionic "bit". Another is similar, except in that (i) it excludes the quaternionic bit, and (ii) the composite of two complex quantum systems comes with an extra classical bit. In both of these categories, states are morphisms from systems to the tensor unit, which helps give the categorical structure a clear operational interpretation. A no-go result shows that the first of these categories, at least, cannot be extended to include spin factors other than the (real, complex, and quaternionic) quantum bits, while preserving the representation of states as morphisms. The same is true for attempts to extend the second category to even-dimensional spin-factors. Interesting phenomena exhibited by some composites in these categories include failure of local tomography, supermultiplicativity of the maximal number of mutually distinguishable states, and mixed states whose marginals are pure. 	
1508.05288v3	http://arxiv.org/pdf/1508.05288v3	2015	Dynamics of Human Cooperation in Economic Games	Martin Spanknebel|Klaus Pawelzik	  Human decision behaviour is quite diverse. In many games humans on average do not achieve maximal payoff and the behaviour of individual players remains inhomogeneous even after playing many rounds. For instance, in repeated prisoner dilemma games humans do not always optimize their mean reward and frequently exhibit broad distributions of cooperativity. The reasons for these failures of maximization are not known. Here we show that the dynamics resulting from the tendency to shift choice probabilities towards previously rewarding choices in closed loop interaction with the strategy of the opponent can not only explain systematic deviations from 'rationality', but also reproduce the diversity of choice behaviours. As a representative example we investigate the dynamics of choice probabilities in prisoner dilemma games with opponents using strategies with different degrees of extortion and generosity. We find that already a simple model for human learning can account for a surprisingly wide range of human decision behaviours. It reproduces suppression of cooperation against extortionists and increasing cooperation when playing with generous opponents, explains the broad distributions of individual choices in ensembles of players, and predicts the evolution of individual subjects' cooperation rates over the course of the games. We conclude that important aspects of human decision behaviours are rooted in elementary learning mechanisms realised in the brain. 	
1509.06613v2	http://arxiv.org/pdf/1509.06613v2	2015	Stress channelling in extreme couple-stress materials Part I: Strong   ellipticity, wave propagation, ellipticity, and discontinuity relations	Panos A. Gourgiotis|Davide Bigoni	  Materials with extreme mechanical anisotropy are designed to work near a material instability threshold where they display stress channelling and strain localization, effects that can be exploited in several technologies. Extreme couple stress solids are introduced and for the first time systematically analyzed in terms of several material instability criteria: positive-definiteness of the strain energy (implying uniqueness of the mixed b.v.p.), strong ellipticity (implying uniqueness of the b.v.p. with prescribed kinematics on the whole boundary), plane wave propagation, ellipticity, and the emergence of discontinuity surfaces. Several new and unexpected features are highlighted: (i.) Ellipticity is mainly dictated by the 'Cosserat part' of the elasticity and (ii.) its failure is shown to be related to the emergence of discontinuity surfaces; (iii.) Ellipticity and wave propagation are not interdependent conditions (so that it is possible for waves not to propagate when the material is still in the elliptic range and, in very special cases, for waves to propagate when ellipticity does not hold). The proof that loss of ellipticity induces stress channelling, folding and faulting of an elastic Cosserat continuum (and the related derivation of the infinite-body Green's function under antiplane strain conditions) is deferred to Part II of this study. 	
1511.01583v2	http://arxiv.org/pdf/1511.01583v2	2016	From Brittle to Ductile: A Structure Dependent Ductility of Diamond   Nanothread	Haifei Zhan|Gang Zhang|Vincent BC Tan|Yuan Cheng|John M. Bell|Yong-Wei Zhang|Yuantong Gu	  As a potential building block for the next generation of devices or multifunctional materials that are spreading almost every technology sector, one-dimensional (1D) carbon nanomaterial has received intensive research interests. Recently, a new ultra-thin diamond nanothread (DNT) has joined this palette, which is a 1D structure with poly-benzene sections connected by Stone-Wales (SW) transformation defects. Using large-scale molecular dynamics simulations, we found that this sp3 bonded DNT can transit from a brittle to a ductile characteristic by varying the length of the poly-benzene sections, suggesting that DNT possesses entirely different mechanical responses than other 1D carbon allotropies. Analogously, the SW defects behave like a grain boundary that interrupts the consistency of the poly-benzene sections. For a DNT with a fixed length, the yield strength fluctuates in the vicinity of a certain value and is independent of the "grain size". On the other hand, both yield strength and yield strain show a clear dependence on the total length of DNT, which is due to the fact that the failure of the DNT is dominated by the SW defects. Its highly tunable ductility together with its ultra-light density and high Young's modulus makes diamond nanothread ideal for creation of extremely strong three-dimensional nano-architectures. 	
1511.04385v3	http://arxiv.org/pdf/1511.04385v3	2017	Factoring Safe Semiprimes with a Single Quantum Query	Frédéric Grosshans|Thomas Lawson|François Morain|Benjamin Smith	  Shor's factoring algorithm (SFA), by its ability to efficiently factor large numbers, has the potential to undermine contemporary encryption. At its heart is a process called order finding, which quantum mechanics lets us perform efficiently. SFA thus consists of a \emph{quantum order finding algorithm} (QOFA), bookended by classical routines which, given the order, return the factors. But, with probability up to $1/2$, these classical routines fail, and QOFA must be rerun. We modify these routines using elementary results in number theory, improving the likelihood that they return the factors.   The resulting quantum factoring algorithm is better than SFA at factoring safe semiprimes, an important class of numbers used in cryptography. With just one call to QOFA, our algorithm almost always factors safe semiprimes. As well as a speed-up, improving efficiency gives our algorithm other, practical advantages: unlike SFA, it does not need a randomly picked input, making it simpler to construct in the lab; and in the (unlikely) case of failure, the same circuit can be rerun, without modification.   We consider generalizing this result to other cases, although we do not find a simple extension, and conclude that SFA is still the best algorithm for general numbers (non safe semiprimes, in other words). Even so, we present some simple number theoretic tricks for improving SFA in this case. 	
1511.08661v3	http://arxiv.org/pdf/1511.08661v3	2016	Cascading failures in coupled networks with both inner-dependency and   inter-dependency links	Run-Ran Liu|Ming Li|Chun-Xiao Jia|Bing-Hong Wang	  We study the percolation in coupled networks with both inner-dependency and inter-dependency links, where the inner- and inter-dependency links represent the dependencies between nodes in the same or different networks, respectively. We find that when most of dependency links are inner- or inter-ones, the coupled networks system is fragile and makes a discontinuous percolation transition. However, when the numbers of two types of dependency links are close to each other, the system is robust and makes a continuous percolation transition. This indicates that the high density of dependency links could not always lead to a discontinuous percolation transition as the previous studies. More interestingly, although the robustness of the system can be optimized by adjusting the ratio of the two types of dependency links, there exists a critical average degree of the networks for coupled random networks, below which the crossover of the two types of percolation transitions disappears, and the system will always demonstrate a discontinuous percolation transition. We also develop an approach to analyze this model, which is agreement with the simulation results well. 	
1512.05180v1	http://arxiv.org/pdf/1512.05180v1	2015	Failure Mechanism of True 2D Granular Flows	Cuong T. Nguyen|Ha H. Bui|R. Fukagawa	  Most previous experimental investigations of two-dimensional (2D) granular column collapses have been conducted using three-dimensional (3D) granular materials in narrow horizontal channels (i.e., quasi-2D condition). Our recent research on 2D granular column collapses by using 2D granular materials (i.e., aluminum rods) has revealed results that differ markedly from those reported in the literature. We assume a 2D column with an initial height of h0 and initial width of d0, a defined as their ratio (a =h0/d0), a final height of h , and maximum run-out distance of d . The experimental data suggest that for the low a regime (a <0.65) the ratio of the final height to initial height is 1. However, for the high a regime (a >0.65), the ratio of a to (d-d0)/d0, h0/h , or d/d0 is expressed by power-law relations. In particular, the following power-function ratios (h0/h=1.42a^2/3 and d/d0=4.30a^0.72) are proposed for every a >0.65. In contrast, the ratio (d-d0)/d0=3.25a^0.96 only holds for 0.65< a< 1.5, whereas the ratio (d-d0)/d0=3.80a^0.73 holds for a>1.5. In addition, the influence of ground contact surfaces (hard or soft beds) on the final run-out distance and destruction zone of the granular column under true 2D conditions is investigated. 	
1512.05461v3	http://arxiv.org/pdf/1512.05461v3	2016	A Novel Material for In Situ Construction on Mars: Experiments and   Numerical Simulations	Lin Wan|Roman Wendner|Gianluca Cusatis	  A significant step in space exploration during the 21st century will be human settlement on Mars. Instead of transporting all the construction materials from Earth to the red planet with incredibly high cost, using Martian soil to construct a site on Mars is a superior choice. Knowing that Mars has long been considered a "sulfur-rich planet", a new construction material composed of simulated Martian soil and molten sulfur is developed. In addition to the raw material availability for producing sulfur concrete and a strength reaching similar or higher levels of conventional cementitious concrete, fast curing, low temperature sustainability, acid and salt environment resistance, 100% recyclability are appealing superior characteristics of the developed Martian Concrete. In this study, different percentages of sulfur are investigated to obtain the optimal mixing proportions. Three point bending, unconfined compression and splitting tests were conducted to determine strength development, strength variability, and failure mechanisms. The test results show that the strength of Martian Concrete doubles that of sulfur concrete utilizing regular sand. It is also shown that the particle size distribution plays an important role in the mixture's final strength. Furthermore, since Martian soil is metal rich, sulfates and, potentially, polysulfates are also formed during high temperature mixing, which might contribute to the high strength. The optimal mix developed as Martian Concrete has an unconfined compressive strength of above 50 MPa. The formulated Martian Concrete is simulated by the Lattice Discrete Particle Model (LDPM), which exhibits excellent ability in modeling the material response under various loading conditions. 	
1512.07343v1	http://arxiv.org/pdf/1512.07343v1	2015	Thermal stability of a free nanotube from single-layer black phosphorus	Kun Cai|Jing Wan|Ning Wei|Haifang Cai|Qing-Hua Qin	  Similar to the carbon nanotube fabricated from graphene sheet, a black phosphorus nanotube (BPNT) also can theoretically be produced by curling the rectangular single-layer black phosphorus (SLBP). In present study, the effect of thermal vibration of atoms on the failure of a BPNT is investigated using molecular dynamics simulations. Two types of double-shell BPNTs, which are obtained by curling the rectangular SLBP along its armchair/pucker direction and zigzag direction (in-plane normal) respectively, are involved in simulation. At finite temperature, a bond on the outer shell of tube is under tension due to both of curvature of tube and serious thermal vibration of atoms. As the length of a bond with such elongation approaches its critical value, i.e., 0.279 nm, or the smallest distance between two nonbonding phosphorus atoms is over 0.389nm caused by great variation of bond angle, the tube fails quickly. The critical stable states of either an armchair or a zigzag BPNT at finite temperature are calculated and compared. To achieve a stable BPNT with high robustness, the curvature of the tube should be reduced or the tube should work at a lower temperature. Only when the BPNT has structural stability, it has a potential application as a nanowire in a future nano electro-mechanical system (NEMS). 	
1603.02440v1	http://arxiv.org/pdf/1603.02440v1	2016	Simulation study of a rectifying bipolar ion channel: Detailed model   versus reduced model	Z. Ható|D. Boda|D. Gillespie|J. Vrabec|G. Rutkai|T. Kristóf	  We study a rectifying mutant of the OmpF porin ion channel using both all-atom and reduced models. The mutant was created by Miedema et al. [Nano Lett., 2007, 7, 2886] on the basis of the N-P semiconductor diode, in which an N-P junction is formed. The mutant contains a pore region with positive amino acids on the left-hand side and negative amino acids on the right-hand side. Experiments show that this mutant rectifies. Although we do not know the structure of this mutant, we can build an all-atom model for it on the basis of the structure of the wild type channel. Interestingly, molecular dynamics simulations for this all-atom model do not produce rectification. A reduced model that contains only the important degrees of freedom (the positive and negative amino acids and free ions in an implicit solvent), on the other hand, exhibits rectification. Our calculations for the reduced model (using the Nernst-Planck equation coupled to Local Equilibrium Monte Carlo simulations) reveal a rectification mechanism that is different from that seen for semiconductor diodes. The basic reason is that the ions are different in nature from electrons and holes (they do not recombine). We provide explanations for the failure of the all-atom model including the effect of all the other atoms in the system as a noise that inhibits the response of ions (that would be necessary for rectification) to the polarizing external field. 	
1604.06775v1	http://arxiv.org/pdf/1604.06775v1	2016	Spooky Action at No Distance: On the individuation of quantum mechanical   systems	David Weinbaum	  Recent experiments have perfectly verified the fact that quantum correlations between two entangled particles are stronger than any classical, local pre-quantum worldview allows. This is famously called the EPR paradox first conceived as a thought experiment and decades later realized in the lab. We discuss in depth the nature of the paradox and show that the problematics it presents is first and foremost epistemological. After briefly exploring resolutions to the paradox that after many decades of discourse still remain controversial, we argue that the paradox is rooted in the failure of our current metaphysical scheme, being the foundation of our knowledge, to accommodate and cohere our knowledge of the phenomena of entanglement. We then develop and make the case for a novel and more fundamental resolution of the paradox by changing the underlying metaphysical foundation from one based on individuals to a one based on individuation. We discuss in detail how in the light of this new scheme concepts central to the paradox such as realism, causality and locality are adjusted to the effect that the paradox is resolved without giving up these concepts so fundamental to our thinking. We conclude with a brief note about the important role of metaphysics to the progress of knowledge and our understanding of reality. 	
1605.03106v1	http://arxiv.org/pdf/1605.03106v1	2016	What is Physics: The individual and the universal, and seeing past the   noise	A. R. P. Rau	  Along with weaving together observations, experiments, and theoretical constructs into a coherent mesh of understanding of the world around us, physics over its past five centuries has continuously refined the base concepts on which the whole framework is built. In quantum physics, first in non-relativistic mechanics and later in quantum field theories, even familiar concepts of position, momentum, wave or particle, are derived constructs from the classical limit in which we live but not intrinsic to the underlying physics. Most crucially, the very idea of the individual, whether an object or an event, distinguished only in a mere label of identity from others identical to it in all the physics, exists only as an approximation, not an element of underlying reality. Failure to recognize this and seeking alternative explanations in many worlds or multiverses leads only to incoherent logic and incorrect physics.   As an example, in a physical system such as an atom in a particular state, physics deals with the universal system of all such atoms but makes no meaningful prediction of the position of an electron or the time of decay of any specific atom. Those are incidental, entirely random among all possible positions and times, even while physics makes very precise predictions for the distribution of the outcomes in measurements on atoms in that state. Physics deals with the universal, not the individual. 	
1605.08978v1	http://arxiv.org/pdf/1605.08978v1	2016	Quantile-based optimization under uncertainties using adaptive Kriging   surrogate models	M. Moustapha|B. Sudret|J. -M. Bourinet|B. Guillaume	  Uncertainties are inherent to real-world systems. Taking them into account is crucial in industrial design problems and this might be achieved through reliability-based design optimization (RBDO) techniques. In this paper, we propose a quantile-based approach to solve RBDO problems. We first transform the safety constraints usually formulated as admissible probabilities of failure into constraints on quantiles of the performance criteria. In this formulation, the quantile level controls the degree of conservatism of the design. Starting with the premise that industrial applications often involve high-fidelity and time-consuming computational models, the proposed approach makes use of Kriging surrogate models (a.k.a. Gaussian process modeling). Thanks to the Kriging variance (a measure of the local accuracy of the surrogate), we derive a procedure with two stages of enrichment of the design of computer experiments (DoE) used to construct the surrogate model. The first stage globally reduces the Kriging epistemic uncertainty and adds points in the vicinity of the limit-state surfaces describing the system performance to be attained. The second stage locally checks, and if necessary, improves the accuracy of the quantiles estimated along the optimization iterations. Applications to three analytical examples and to the optimal design of a car body subsystem (minimal mass under mechanical safety constraints) show the accuracy and the remarkable efficiency brought by the proposed procedure. 	
1606.04435v2	http://arxiv.org/pdf/1606.04435v2	2016	Adversarial Perturbations Against Deep Neural Networks for Malware   Classification	Kathrin Grosse|Nicolas Papernot|Praveen Manoharan|Michael Backes|Patrick McDaniel	  Deep neural networks, like many other machine learning models, have recently been shown to lack robustness against adversarially crafted inputs. These inputs are derived from regular inputs by minor yet carefully selected perturbations that deceive machine learning models into desired misclassifications. Existing work in this emerging field was largely specific to the domain of image classification, since the high-entropy of images can be conveniently manipulated without changing the images' overall visual appearance. Yet, it remains unclear how such attacks translate to more security-sensitive applications such as malware detection - which may pose significant challenges in sample generation and arguably grave consequences for failure.   In this paper, we show how to construct highly-effective adversarial sample crafting attacks for neural networks used as malware classifiers. The application domain of malware classification introduces additional constraints in the adversarial sample crafting problem when compared to the computer vision domain: (i) continuous, differentiable input domains are replaced by discrete, often binary inputs; and (ii) the loose condition of leaving visual appearance unchanged is replaced by requiring equivalent functional behavior. We demonstrate the feasibility of these attacks on many different instances of malware classifiers that we trained using the DREBIN Android malware data set. We furthermore evaluate to which extent potential defensive mechanisms against adversarial crafting can be leveraged to the setting of malware classification. While feature reduction did not prove to have a positive impact, distillation and re-training on adversarially crafted samples show promising results. 	
1606.05401v1	http://arxiv.org/pdf/1606.05401v1	2016	Manganite-based three level memristive devices with self-healing   capability	W. Román Acevedo|D. Rubi|J. Lecourt|U. Lüders|F. Gomez-Marlasca|P. Granell|F. Golmar|P. Levy	  We report on non-volatile memory devices based on multifunctional manganites. The electric field induced resistive switching of Ti/$La_{1/3}$$Ca_{2/3}$Mn$O_3$/n-Si devices is explored using different measurement protocols. We show that using current as the electrical stimulus (instead of standard voltage-controlled protocols) improves the electrical performance of our devices and unveils an intermediate resistance state. We observe three discrete resistance levels (low, intermediate and high), which can be set either by the application of current-voltage ramps or by means of single pulses. These states exhibit retention and endurance capabilities exceeding $10^4$ s and 70 cycles, respectively. We rationalize our experimental observations by proposing a mixed scenario were a metallic filament and a Si$O_x$ layer coexist, accounting for the observed resistive switching. Overall electrode area dependence and temperature dependent resistance measurements support our scenario. After device failure takes place, the system can be turned functional again by heating up to low temperature (120 C), a feature that could be exploited for the design of memristive devices with self-healing functionality. These results give insight into the existence of multiple resistive switching mechanisms in manganite-based memristive systems and provide strategies for controlling them. 	
1606.07193v2	http://arxiv.org/pdf/1606.07193v2	2016	A Unified Model for GRB Prompt Emission from Optical to $γ$-Rays:   Exploring GRBs as Standard Candles	S. Guiriec|C. Kouveliotou|D. H. Hartmann|J. Granot|K. Asano|P. Meszaros|R. Gill|N. Gehrels|J. McEnery	  The origin of prompt emission from gamma ray bursts remains to be an open question. Correlated prompt optical and gamma-ray emission observed in a handful of GRBs strongly suggests a common emission region, but failure to adequately fit the broadband GRB spectrum prompted the hypothesis of different emission mechanisms for the low- and high-energy radiations. We demonstrate that our multi-component model for GRB gamma-ray prompt emission provides an excellent fit to GRB 110205A from optical to gamma-ray energies. Our results show that the optical and highest gamma-ray emissions have the same spatial and spectral origin, which is different from the bulk of the X- and softest gamma-ray radiation. Finally, our accurate redshift estimate for GRB 110205A demonstrates promise for using GRBs as cosmological standard candles. 	
1606.08577v1	http://arxiv.org/pdf/1606.08577v1	2016	Reliability analysis of high-dimensional models using low-rank tensor   approximations	K. Konakli|B. Sudret	  Engineering and applied sciences use models of increasing complexity to simulate the behaviour of manufactured and physical systems. Propagation of uncertainties from the input to a response quantity of interest through such models may become intractable in cases when a single simulation is time demanding. Particularly challenging is the reliability analysis of systems represented by computationally costly models, because of the large number of model evaluations that are typically required to estimate small probabilities of failure. In this paper, we demonstrate the potential of a newly emerged meta-modelling technique known as low-rank tensor approximations to address this limitation. This technique is especially promising for high-dimensional problems because: (i) the number of unknowns in the generic functional form of the meta-model grows only linearly with the input dimension and (ii) such approximations can be constructed by relying on a series of minimization problems of small size independent of the input dimension. In example applications involving finite-element models pertinent to structural mechanics and heat conduction, low-rank tensor approximations built with polynomial bases are found to outperform the popular sparse polynomial chaos expansions in the estimation of tail probabilities when small experimental designs are used. It should be emphasized that contrary to methods particularly targeted to reliability analysis, the meta-modelling approach also provides a full probabilistic description of the model response, which can be used to estimate any statistical measure of interest. 	
1607.01510v3	http://arxiv.org/pdf/1607.01510v3	2018	Perturbation Theory for Arbitrary Coupling Strength ?	B. P. Mahapatra|N. B. Pradhan	  We present a \emph{new} formulation of perturbation theory for quantum systems, designated here as: `mean field perturbation theory'(MFPT), which is free from power-series-expansion in any physical parameter, including the coupling strength. Its application is thereby extended to deal with interactions of \textit{arbitrary} strength and to compute system-properties having non-analytic dependence on the coupling, thus overcoming the primary limitations of the `standard formulation of perturbation theory' ( SFPT). MFPT is defined by developing perturbation about a chosen input Hamiltonian, which is exactly solvable but which acquires the non-linearity and the analytic structure~(in the coupling-strength)~of the original interaction through a self-consistent, feedback mechanism. We demonstrate Borel-summability of MFPT for the case of the quartic- and sextic-anharmonic oscillators and the quartic double-well oscillator (QDWO) by obtaining uniformly accurate results for the ground state of the above systems for arbitrary physical values of the coupling strength. The results obtained for the QDWO may be of particular significance since `renormalon'-free, unambiguous results are achieved for its spectrum in contrast to the well-known failure of SFPT in this case.   \pacs{11.15.Bt,11.10.Jj,11.25.Db,12.38.Cy,03.65.Ge} 	
1608.07113v1	http://arxiv.org/pdf/1608.07113v1	2016	Effects of high-power laser irradiation on sub-superficial graphitic   layers in single crystal diamond	F. Picollo|S. Rubanov|C. Tomba|A. Battiato|E. Enrico|A. Perrat-Mabilon|C. Peaucelle|T. N. Tran Thi|L. Boarino|E. Gheeraert|P. Olivero	  We report on the structural modifications induced by a lambda = 532 nm ns-pulsed high-power laser on sub-superficial graphitic layers in single-crystal diamond realized by means of MeV ion implantation. A systematic characterization of the structures obtained under different laser irradiation conditions (power density, number of pulses) and subsequent thermal annealing was performed by different electron microscopy techniques. The main feature observed after laser irradiation is the thickening of the pre-existing graphitic layer. Cross sectional SEM imaging was performed to directly measure the thickness of the modified layers, and subsequent selective etching of the buried layers was employed to both assess their graphitic nature and enhance the SEM imaging contrast. In particular, it was found that for optimal irradiation parameters the laser processing induces a six-fold increase the thickness of sub superficial graphitic layers without inducing mechanical failures in the surrounding crystal. TEM microscopy and EELS spectroscopy allowed a detailed analysis of the internal structure of the laser irradiated layers, highlighting the presence of different nano graphitic and amorphous layers. The obtained results demonstrate the effectiveness and versatility of high-power laser irradiation for an accurate tuning of the geometrical and structural features of graphitic structures embedded in single crystal diamond, and open new opportunities in diamond fabrication. 	
1608.07499v1	http://arxiv.org/pdf/1608.07499v1	2016	Stem Cell Therapy for Alzheimer's Disease	Ankur Patel|Grishma joshi|Rupali Ugile	  The loss of neuronal cells in the central nervous system may happen in numerous neurodegenerative illnesses. Alzheimer's Disease (AD) is an intricate, irreversible, dynamic neurodegenerative sickness. It is the main source of age-related dementia, influencing roughly 5.3 million individuals in the United States alone. Promotion is a typical feeble ailment in individuals more than 65 years, bringing on disability described by decrease in memory, failure to learn and do every day exercises, intellectual weakness and influences the personal satisfaction of patients. Pathologic qualities of AD are an irregular development of specific proteins called Beta-amyloid "plaques" and Tau "Tangles" in the mind. Notwithstanding, current treatments against AD are just to calm manifestations and palliative yet are not the cure and a few promising medications competitors have fizzled in late clinical trials. There is consequently a critical need to enhance our comprehension for pathogenesis of this sickness, making new and creative prescient models with powerful treatments. As of late, stem cell treatment has been appeared to have a potential way to deal with different illnesses, including neurodegenerative disorders. In light of the far reaching nature of AD pathology, stem cell substitution procedures have been seen as an extraordinarily difficult and impossible treatment approach. Stem Cell may likewise offer an effective new way to deal with model and concentrate AD. Patient derived induced Pluripotent Stem Cells (iPSCs), for instance, may propel our comprehension of disease mechanism. In this review we will examine the capability of stem cells to help in these testing tries. 	
1608.08743v2	http://arxiv.org/pdf/1608.08743v2	2017	A Large Scale Analysis of Unreliable Stochastic Networks	Reza Aghajani|Philippe Robert|Wen Sun	  The problem of reliability of a large distributed system is analyzed via a new mathematical model. A typical framework is a system where a set of files are duplicated on several data servers. When one of these servers breaks down, all copies of files stored on it are lost. In this way, repeated failures may lead to losses of files. The efficiency of such a network is directly related to the performances of the mechanism used to duplicate files on servers. In this paper we study the evolution of the network using a natural duplication policy giving priority to the files with the least number of copies.   We investigate the asymptotic behavior of the network when the number $N$ of servers is large. The analysis is complicated by the large dimension of the state space of the empirical distribution of the state of the network. A stochastic model of the evolution of the network which has values in state space whose dimension does not depend on $N$ is introduced. Despite this description does not have the Markov property, it turns out that it is converging in distribution, when the number of nodes goes to infinity, to a nonlinear Markov process. The rate of decay of the network, which is the key characteristic of interest of these systems, can be expressed in terms of this asymptotic process. The corresponding mean-field convergence results are established. A lower bound on the exponential decay, with respect to time, of the fraction of the number of initial files with at least one copy is obtained. 	
1608.08786v2	http://arxiv.org/pdf/1608.08786v2	2016	Age-dependent Size Effect and Fracture Characteristics of Ultra High   Performance Concrete	Lin Wan|Roman Wendner|Gianluca Cusatis	  This paper presents an investigation of the age-dependent size effect and fracture characteristics of an ultra high performance concrete (UHPC). The study is based on a unique set of experimental data connecting aging tests for two curing protocols of one size and scaled size effect tests of one age. Both aging and size effect studies are performed on notched three point bending tests. Experimental data is augmented by state of the art simulations employing a recently developed discrete element based early-age computational framework. The framework is constructed by coupling a hygro-thermo-chemical (HTC) model and the Lattice Discrete Particle Model (LDPM) through a set of aging functions. The HTC component allows taking into account variable curing conditions and predicts the maturity of concrete. The mechanical component, LDPM, simulates the failure behavior of concrete at the length scale of major heterogeneities. After careful calibration and validation the mesoscale HTC-LDPM model is uniquely posed to perform predictive simulations. The ultimate flexural strengths from experiments and simulations are analyzed by the cohesive size effect curve (CSEC) method, and the classical size effect law (SEL). The fracture energies obtained by LDPM, CSEC, SEL, and cohesive crack analyses are compared and an aging formulation for fracture properties is proposed. Based on experiments, simulations, and size effect analyses, the age-dependence of size effect and the robustness of analytical size effect methods are evaluated. 	
1609.02305v2	http://arxiv.org/pdf/1609.02305v2	2018	Survey of Consistent Software-Defined Network Updates	Klaus-Tycho Foerster|Stefan Schmid|Stefano Vissicchio	  Computer networks have become a critical infrastructure. Designing dependable computer networks however is challenging, as such networks should not only meet strict requirements in terms of correctness, availability, and performance, but they should also be flexible enough to support fast updates, e.g., due to a change in the security policy, an increasing traffic demand, or a failure. The advent of Software-Defined Networks (SDNs) promises to provide such flexiblities, allowing to update networks in a fine-grained manner, also enabling a more online traffic engineering. In this paper, we present a structured survey of mechanisms and protocols to update computer networks in a fast and consistent manner. In particular, we identify and discuss the different desirable update consistency properties a network should provide, the algorithmic techniques which are needed to meet these consistency properties, their implications on the speed and costs at which updates can be performed. We also discuss the relationship of consistent network update problems to classic algorithmic optimization problems. While our survey is mainly motivated by the advent of Software-Defined Networks (SDNs), the fundamental underlying problems are not new, and we also provide a historical perspective of the subject. 	
1609.02535v1	http://arxiv.org/pdf/1609.02535v1	2016	Modeling the differentiation of A- and C-type baroreceptor firing   patterns	Jacob Sturdy|Johnny T Ottesen|Mette S Olufsen	  The baroreceptor neurons serve as the primary transducers of blood pressure for the autonomic nervous system and are thus critical in enabling the body to respond effectively to changes in blood pressure. These neurons can be separated into two types (A and C) based on the myelination of their axons and their distinct firing patterns elicited in response to specific pressure stimuli. This study has developed a comprehensive model of the afferent baroreceptor discharge built on physiological knowledge of arterial wall mechanics, firing rate responses to controlled pressure stimuli, and ion channel dynamics within the baroreceptor neurons. With this model, we were able to predict firing rates observed in previously published experiments in both A- and C-type neurons. These results were obtained by adjusting model parameters determining the maximal ion-channel conductances. The observed variation in the model parameters are hypothesized to correspond to physiological differences between A- and C-type neurons. In agreement with published experimental observations, our simulations suggest that a twofold lower potassium conductance in C-type neurons is responsible for the observed sustained basal firing, whereas a tenfold higher mechanosensitive conductance is responsible for the greater firing rate observed in A-type neurons. A better understanding of the difference between the two neuron types can potentially be used to gain more insight into the underlying pathophysiology facilitating development of targeted interventions improving baroreflex function in diseased individuals, e.g. in patients with autonomic failure, a syndrome that is difficult to diagnose in terms of its pathophysiology. 	
1609.04947v1	http://arxiv.org/pdf/1609.04947v1	2016	Robot Introspection via Wrench-based Action Grammars	Juan Rojas|Zhengjie Huang|Shuangqi Luo|Yunlong Du Wenwei Kuang|Dingqiao Zhu|Kensuke Harada	  Robotic failure is all too common in unstructured robot tasks. Despite well designed controllers, robots often fail due to unexpected events. How do robots measure unexpected events? Many do not. Most robots are driven by the senseplan- act paradigm, however more recently robots are working with a sense-plan-act-verify paradigm. In this work we present a principled methodology to bootstrap robot introspection for contact tasks. In effect, we are trying to answer the question, what did the robot do? To this end, we hypothesize that all noisy wrench data inherently contains patterns that can be effectively represented by a vocabulary. The vocabulary is generated by meaningfully segmenting the data and then encoding it. When the wrench information represents a sequence of sub-tasks, we can think of the vocabulary forming sets of words or sentences, such that each subtask is uniquely represented by a word set. Such sets can be classified using statistical or machine learning techniques. We use SVMs and Mondrian Forests to classify contacts tasks both in simulation and in real robots for one and dual arm scenarios showing the general robustness of the approach. The contribution of our work is the presentation of a simple but generalizable semantic scheme that enables a robot to understand its high level state. This verification mechanism can provide feedback for high-level planners or reasoning systems that use semantic descriptors as well. The code, data, and other supporting documentation can be found at: http://www.juanrojas.net/2017icra_wrench_introspection. 	
1610.01180v1	http://arxiv.org/pdf/1610.01180v1	2016	Counterion-Induced Swelling of Ionic Microgels	Alan R. Denton|Qiyun Tang	  Ionic microgel particles, when dispersed in a solvent, swell to equilibrium sizes that are governed by a balance between electrostatic and elastic forces. Tuning of particle size by varying external stimuli, such as $p$H, salt concentration, and temperature, has relevance for drug delivery, microfluidics, and filtration. To model swelling of ionic microgels, we derive a statistical mechanical theorem, which proves exact within the cell model, for the electrostatic contribution to the osmotic pressure inside a permeable colloidal macroion. Applying the theorem, we demonstrate how the distribution of counterions within an ionic microgel determines the internal osmotic pressure. By combining the electrostatic pressure, which we compute via both Poisson-Boltzmann theory and molecular dynamics simulation, with the elastic pressure, modeled via the Flory-Rehner theory of swollen polymer networks, we show how deswelling of ionic microgels with increasing concentration of particles can result from a redistribution of counterions that reduces electrostatic pressure. A linearized approximation for the electrostatic pressure, which proves remarkably accurate, provides physical insight and greatly eases numerical calculations for practical applications. Comparing with experiments, we explain why soft particles in deionized suspensions deswell upon increasing concentration and why this effect may be suppressed at higher ionic strength. The failure of the uniform ideal-gas approximation to adequately account for counterion-induced deswelling below close packing of microgels is attributed to neglect of spatial variation of the counterion density profile and the electrostatic pressure of incompletely neutralized macroions. 	
1611.02617v1	http://arxiv.org/pdf/1611.02617v1	2016	Color-avoiding percolation	Sebastian M. Krause|Michael M. Danziger|Vinko Zlatić	  Many real world networks have groups of similar nodes which are vulnerable to the same failure or adversary. Nodes can be colored in such a way that colors encode the shared vulnerabilities. Using multiple paths to avoid these vulnerabilities can greatly improve network robustness. Color-avoiding percolation provides a theoretical framework for analyzing this scenario, focusing on the maximal set of nodes which can be connected via multiple color-avoiding paths. In this paper we extend the basic theory of color-avoiding percolation that was published in [Krause et. al., Phys. Rev. X 6 (2016) 041022]. We explicitly account for the fact that the same particular link can be part of different paths avoiding different colors. This fact was previously accounted for with a heuristic approximation. We compare this approximation with a new, more exact theory and show that the new theory is substantially more accurate for many avoided colors. Further, we formulate our new theory with differentiated node functions, as senders/receivers or as transmitters. In both functions, nodes can be explicitly trusted or avoided. With only one avoided color we obtain standard percolation. With one by one avoiding additional colors, we can understand the critical behavior of color avoiding percolation. For heterogeneous color frequencies, we find that the colors with the largest frequencies control the critical threshold and exponent. Colors of small frequencies have only a minor influence on color avoiding connectivity, thus allowing for approximations. 	
1611.08859v2	http://arxiv.org/pdf/1611.08859v2	2017	Correlations and diagonal entropy after quantum quenches in XXZ chains	Lorenzo Piroli|Eric Vernier|Pasquale Calabrese|Marcos Rigol	  We study quantum quenches in the XXZ spin-$1/2$ Heisenberg chain from families of ferromagnetic and antiferromagnetic initial states. Using Bethe ansatz techniques, we compute short-range correlators in the complete generalized Gibbs ensemble (GGE), which takes into account all local and quasi-local conservation laws. We compare our results to exact diagonalization and numerical linked cluster expansion calculations for the diagonal ensemble finding excellent agreement and thus providing a very accurate test for the validity of the complete GGE. Furthermore, we compute the diagonal entropy in the post-quench steady state. By careful finite-size scaling analyses of the exact diagonalization results, we show that the diagonal entropy is equal to one half the Yang-Yang entropy corresponding to the complete GGE. Finally, the complete GGE is quantitatively contrasted with the GGE built using only the local conserved charges (local GGE). The predictions of the two ensembles are found to differ significantly in the case of ferromagnetic initial states. Such initial states are better suited than others considered in the literature to experimentally test the validity of the complete GGE and contrast it to the failure of the local GGE. 	
1612.00274v1	http://arxiv.org/pdf/1612.00274v1	2016	Lattice of infinite bending-resistant fibers	V. Kobelev	  This article present the double-periodical lattice made of infinite elastic fibers that withstand bending and tension. The model describes the elastic properties of flat periodic structure. With this model the behavior of a two-dimensional array of infinite fibers is simulated. The material that contains a row of broken fibers is considered. These broken fibers form the failure in the material that shapes like a long straight crack. The lattice is tensioned in the direction, which is orthogonal to the direction of straight crack. The conditions of fracture of this lattice are investigated. The closed form expression for the stress in the first unbroken fiber and the expression for fracture toughness are given. These values are the functions of mechanical parameters of lattice and tensions in both families of fibers. The closed form solution demonstrates a notable behavior of the material. Namely, the fracture behavior of two-dimensional lattice is cardinally depends upon the pre-stress in the material in the direction, parallel to crack direction. If the tension in fibers that parallel to the crack direction exists, it stabilizes the crack growth and makes the load distribution in the unbroken fibers more even. The two-dimensional lattice behaves in the presence of tension in both directions similarly to the plane elastic media. The finite length crack assumes the shape of the elongated elliptic split. Another behavior of lattice occurs if the fibers, parallel to crack direction, are unstressed. The character of stress concentration near the crack differs. The load distribution at the crack tip varies considerably. The first unbroken fiber carries higher load. The crack is lens-shaped and the crack borders form at the tip the finite angle. 	
1612.02219v1	http://arxiv.org/pdf/1612.02219v1	2016	Process Monitoring of Extrusion Based 3D Printing via Laser Scanning	Matthias Faes|Wim Abbeloos|Frederik Vogeler|Hans Valkenaers|Kurt Coppens|Toon Goedemé|Eleonora Ferraris	  Extrusion based 3D Printing (E3DP) is an Additive Manufacturing (AM) technique that extrudes thermoplastic polymer in order to build up components using a layerwise approach. Hereby, AM typically requires long production times in comparison to mass production processes such as Injection Molding. Failures during the AM process are often only noticed after build completion and frequently lead to part rejection because of dimensional inaccuracy or lack of mechanical performance, resulting in an important loss of time and material. A solution to improve the accuracy and robustness of a manufacturing technology is the integration of sensors to monitor and control process state-variables online. In this way, errors can be rapidly detected and possibly compensated at an early stage. To achieve this, we integrated a modular 2D laser triangulation scanner into an E3DP machine and analyzed feedback signals. A 2D laser triangulation scanner was selected here owing to the very compact size, achievable accuracy and the possibility of capturing geometrical 3D data. Thus, our implemented system is able to provide both quantitative and qualitative information. Also, in this work, first steps towards the development of a quality control loop for E3DP processes are presented and opportunities are discussed. 	
1701.02809v1	http://arxiv.org/pdf/1701.02809v1	2017	DyMo: Dynamic Monitoring of Large Scale LTE-Multicast Systems	Yigal Bejerano|Chandru Raman|Chun-Nam Yu|Varun Gupta|Craig Gutterman|Tomas Young|Hugo Infante|Yousef Abdelmalek|Gil Zussman	  LTE evolved Multimedia Broadcast/Multicast Service (eMBMS) is an attractive solution for video delivery to very large groups in crowded venues. However, deployment and management of eMBMS systems is challenging, due to the lack of realtime feedback from the User Equipment (UEs). Therefore, we present the Dynamic Monitoring (DyMo) system for low-overhead feedback collection. DyMo leverages eMBMS for broadcasting Stochastic Group Instructions to all UEs. These instructions indicate the reporting rates as a function of the observed Quality of Service (QoS). This simple feedback mechanism collects very limited QoS reports from the UEs. The reports are used for network optimization, thereby ensuring high QoS to the UEs. We present the design aspects of DyMo and evaluate its performance analytically and via extensive simulations. Specifically, we show that DyMo infers the optimal eMBMS settings with extremely low overhead, while meeting strict QoS requirements under different UE mobility patterns and presence of network component failures. For instance, DyMo can detect the eMBMS Signal-to-Noise Ratio (SNR) experienced by the 0.1% percentile of the UEs with Root Mean Square Error (RMSE) of 0.05% with only 5 to 10 reports per second regardless of the number of UEs. 	
1701.07561v1	http://arxiv.org/pdf/1701.07561v1	2017	Precision Pointing of Antennas in Space Using Arrays of Shape Memory   Alloy Based Linear Actuators	Nikhil S. Sonawane|Jekan Thangavelautham	  Space systems such as communication satellites, earth observation satellites and space telescopes require precise pointing to observe fixed targets over prolonged time. These systems typically use reaction-wheels to slew the spacecraft and gimballing systems containing motors to achieve precise pointing. Motor based actuators have limited life as they contain moving parts that require lubrication in space. Alternate methods have utilized piezoelectric actuators. This paper presents Shape memory alloys (SMA) actuators for control of a deployable antenna placed on a satellite. The SMAs are operated as a series of distributed linear actuators. These distributed linear actuators are not prone to single point failures and although each individual actuator is imprecise due to hysteresis and temperature variation. The system as a whole achieves reliable results. The SMAs can be programmed to perform a series of periodic motion and operate as a mechanical guidance system that is not prone to damage from radiation or space weather. Efforts are focused on developing a system that can achieve one degree pointing accuracy at first, with an ultimate goal of achieving a few arc seconds accuracy. Bench top models of the actuator system has been developed and working towards testing the system under vacuum. A demonstration flight of the technology is planned aboard a CubeSat. 	
1703.08831v1	http://arxiv.org/pdf/1703.08831v1	2017	Token-based Function Computation with Memory	Saber Salehkaleybar|S. Jamaloddin Golestani	  In distributed function computation, each node has an initial value and the goal is to compute a function of these values in a distributed manner. In this paper, we propose a novel token-based approach to compute a wide class of target functions to which we refer as "Token-based function Computation with Memory" (TCM) algorithm. In this approach, node values are attached to tokens and travel across the network. Each pair of travelling tokens would coalesce when they meet, forming a token with a new value as a function of the original token values. In contrast to the Coalescing Random Walk (CRW) algorithm, where token movement is governed by random walk, meeting of tokens in our scheme is accelerated by adopting a novel chasing mechanism. We proved that, compared to the CRW algorithm, the TCM algorithm results in a reduction of time complexity by a factor of at least $\sqrt{n/\log(n)}$ in Erd\"os-Renyi and complete graphs, and by a factor of $\log(n)/\log(\log(n))$ in torus networks. Simulation results show that there is at least a constant factor improvement in the message complexity of TCM algorithm in all considered topologies. Robustness of the CRW and TCM algorithms in the presence of node failure is analyzed. We show that their robustness can be improved by running multiple instances of the algorithms in parallel. 	
1704.00080v2	http://arxiv.org/pdf/1704.00080v2	2017	Discovering Phases, Phase Transitions and Crossovers through   Unsupervised Machine Learning: A critical examination	Wenjian Hu|Rajiv R. P. Singh|Richard T. Scalettar	  We apply unsupervised machine learning techniques, mainly principal component analysis (PCA), to compare and contrast the phase behavior and phase transitions in several classical spin models - the square and triangular-lattice Ising models, the Blume-Capel model, a highly degenerate biquadratic-exchange spin-one Ising (BSI) model, and the 2D XY model, and examine critically what machine learning is teaching us. We find that quantified principal components from PCA not only allow exploration of different phases and symmetry-breaking, but can distinguish phase transition types and locate critical points. We show that the corresponding weight vectors have a clear physical interpretation, which is particularly interesting in the frustrated models such as the triangular antiferromagnet, where they can point to incipient orders. Unlike the other well-studied models, the properties of the BSI model are less well known. Using both PCA and conventional Monte Carlo analysis, we demonstrate that the BSI model shows an absence of phase transition and macroscopic ground-state degeneracy. The failure to capture the `charge' correlations (vorticity) in the BSI model (XY model) from raw spin configurations points to some of the limitations of PCA. Finally, we employ a nonlinear unsupervised machine learning procedure, the `antoencoder method', and demonstrate that it too can be trained to capture phase transitions and critical points. 	
1704.06569v1	http://arxiv.org/pdf/1704.06569v1	2017	SFCSD: A Self-Feedback Correction System for DNS Based on Active and   Passive Measurement	Caiyun Huang|Peng Zhang|Junpeng Liu|Yong Sun|Xueqiang Zou	  Domain Name System (DNS), one of the important infrastructure in the Internet, was vulnerable to attacks, for the DNS designer didn't take security issues into consideration at the beginning. The defects of DNS may lead to users' failure of access to the websites, what's worse, users might suffer a huge economic loss.   In order to correct the DNS wrong resource records, we propose a Self-Feedback Correction System for DNS (SFCSD), which can find and track a large number of common websites' domain name and IP address correct correspondences to provide users with a real-time auto-updated correct (IP, Domain) binary tuple list. By matching specific strings with SSL, DNS and HTTP traffic passively, filtering with the CDN CNAME and non-homepage URL feature strings, verifying with webpage fingerprint algorithm, SFCSD obtains a large number of highly possibly correct IP addresses to make an active manual correction in the end. Its self-feedback mechanism can expand search range and improve performance.   Experiments show that, SFCSD can achieve 94.3% precision and 93.07% recall rate with the optimal threshold selection in the test dataset. It has 8Gbps processing speed stand-alone to find almost 1000 possibly correct (IP, Domain) per day for the each specific string and to correct almost 200. 	
1705.02477v1	http://arxiv.org/pdf/1705.02477v1	2017	Metacognitive Learning Approach for Online Tool Condition Monitoring	Mahardhika Pratama|Eric Dimla|Chow Yin Lai|Edwin Lughofer	  As manufacturing processes become increasingly automated, so should tool condition monitoring (TCM) as it is impractical to have human workers monitor the state of the tools continuously. Tool condition is crucial to ensure the good quality of products: Worn tools affect not only the surface quality but also the dimensional accuracy, which means higher reject rate of the products. Therefore, there is an urgent need to identify tool failures before it occurs on the fly. While various versions of intelligent tool condition monitoring have been proposed, most of them suffer from a cognitive nature of traditional machine learning algorithms. They focus on the how to learn process without paying attention to other two crucial issues: what to learn, and when to learn. The what to learn and the when to learn provide self regulating mechanisms to select the training samples and to determine time instants to train a model. A novel tool condition monitoring approach based on a psychologically plausible concept, namely the metacognitive scaffolding theory, is proposed and built upon a recently published algorithm, recurrent classifier (rClass). The learning process consists of three phases: what to learn, how to learn, when to learn and makes use of a generalized recurrent network structure as a cognitive component. Experimental studies with real-world manufacturing data streams were conducted where rClass demonstrated the highest accuracy while retaining the lowest complexity over its counterparts. 	
1708.04251v2	http://arxiv.org/pdf/1708.04251v2	2018	A learning framework for winner-take-all networks with stochastic   synapses	Hesham Mostafa|Gert Cauwenberghs	  Many recent generative models make use of neural networks to transform the probability distribution of a simple low-dimensional noise process into the complex distribution of the data. This raises the question of whether biological networks operate along similar principles to implement a probabilistic model of the environment through transformations of intrinsic noise processes. The intrinsic neural and synaptic noise processes in biological networks, however, are quite different from the noise processes used in current abstract generative networks. This, together with the discrete nature of spikes and local circuit interactions among the neurons, raises several difficulties when using recent generative modeling frameworks to train biologically motivated models. In this paper, we show that a biologically motivated model based on multi-layer winner-take-all (WTA) circuits and stochastic synapses admits an approximate analytical description. This allows us to use the proposed networks in a variational learning setting where stochastic backpropagation is used to optimize a lower bound on the data log likelihood, thereby learning a generative model of the data. We illustrate the generality of the proposed networks and learning technique by using them in a structured output prediction task, and in a semi-supervised learning task. Our results extend the domain of application of modern stochastic network architectures to networks where synaptic transmission failure is the principal noise mechanism. 	
1709.04075v1	http://arxiv.org/pdf/1709.04075v1	2017	Linear and nonlinear spectroscopy from quantum master equations	Jonathan H. Fetherolf|Timothy C. Berkelbach	  We investigate the accuracy of the second-order time-convolutionless (TCL2) quantum master equation for the calculation of linear and nonlinear spectroscopies of multichromophore systems. We show that, even for systems with non-adiabatic coupling, the TCL2 master equation predicts linear absorption spectra that are accurate over an extremely broad range of parameters and well beyond what would be expected based on the perturbative nature of the approach; non-equilibrium population dynamics calculated with TCL2 for identical parameters are significantly less accurate. For third-order (two-dimensional) spectroscopy, the importance of population dynamics and the violation of the so-called quantum regression theorem degrade the accuracy of TCL2 dynamics. To correct these failures, we combine the TCL2 approach with a classical ensemble sampling of slow microscopic bath degrees of freedom, leading to an efficient hybrid quantum-classical scheme that displays excellent accuracy over a wide range of parameters. In the spectroscopic setting, the success of such a hybrid scheme can be understood through its separate treatment of homogeneous and inhomogeneous broadening. Importantly, the presented approach has the computational scaling of TCL2, with the modest addition of an embarrassingly parallel prefactor associated with ensemble sampling. The presented approach can be understood as a generalized inhomogeneous cumulant expansion technique, capable of treating multilevel systems with non-adiabatic dynamics. 	
1709.06779v1	http://arxiv.org/pdf/1709.06779v1	2017	High speed self-testing quantum random number generation without   detection loophole	Yang Liu|Xiao Yuan|Ming-Han Li|Weijun Zhang|Qi Zhao|Jiaqiang Zhong|Yuan Cao|Yu-Huai Li|Luo-Kan Chen|Hao Li|Tianyi Peng|Yu-Ao Chen|Cheng-Zhi Peng|Sheng-Cai Shi|Zhen Wang|Lixing You|Xiongfeng Ma|Jingyun Fan|Qiang Zhang|Jian-Wei Pan	  Quantum mechanics provides means of generating genuine randomness that is impossible with deterministic classical processes. Remarkably, the unpredictability of randomness can be certified in a self-testing manner that is independent of implementation devices. Here, we present an experimental demonstration of self-testing quantum random number generation based on an detection-loophole free Bell test with entangled photons. In the randomness analysis, without the assumption of independent identical distribution, we consider the worst case scenario that the adversary launches the most powerful attacks against quantum adversary. After considering statistical fluctuations and applying an 80 Gb $\times$ 45.6 Mb Toeplitz matrix hashing, we achieve a final random bit rate of 114 bits/s, with a failure probability less than $10^{-5}$. Such self-testing random number generators mark a critical step towards realistic applications in cryptography and fundamental physics tests. 	
1711.04691v2	http://arxiv.org/pdf/1711.04691v2	2017	Observational Signatures of Mass-Loading in Jets Launched by Rotating   Black Holes	Michael O' Riordan|Asaf Pe'er|Jonathan C. McKinney	  It is widely believed that relativistic jets in X-ray binaries and active-galactic nuclei are powered by the rotational energy of black holes. This idea is supported by general-relativistic magnetohydrodynamic (GRMHD) simulations of accreting black holes, which demonstrate efficient energy extraction via the Blandford-Znajek mechanism. However, due to uncertainties in the physics of mass-loading, and the failure of GRMHD numerical schemes in the highly-magnetized funnel region, the matter content of the jet remains poorly constrained. We investigate the observational signatures of mass-loading in the funnel by performing general-relativistic radiative transfer calculations on a range of 3D GRMHD simulations of accreting black holes. We find significant observational differences between cases in which the funnel is empty and cases where the funnel is filled with plasma, particularly in the optical and X-ray bands. In the context of Sgr A*, current spectral data constrains the jet filling only if the black hole is rapidly rotating with $a\gtrsim0.9$. In this case, the limits on the infrared flux disfavour a strong contribution from material in the funnel. We comment on the implications of our models for interpreting future Event Horizon Telescope observations. We also scale our models to stellar-mass black holes, and discuss their applicability to the low-luminosity state in X-ray binaries. 	
1712.06205v2	http://arxiv.org/pdf/1712.06205v2	2018	No axion-like particles from core-collapse supernovae?	Ignazio Bombaci|Giorgio Galanti|Marco Roncadelli	  A strong bound on the properties of axion-like particles (ALPs) has been set by assuming that ALPs are emitted by the protoneutron star just before the core-bounce in Galactic core-collapse supernovae, and that these ALPs subsequently convert to $\gamma$-ray photons which ought to be detected by a $\gamma$-ray mission. This argument has been applied to supernova 1987A to derive the bound on the ALP-photon coupling $g_{a \gamma \gamma} \lesssim 5.3 \cdot 10^{- 12} \, {\rm GeV}^{- 1}$ for an ALP mass $m_a \lesssim 4.4 \cdot 10^{- 10} \, {\rm eV}$, and can be applied to the next Galactic supernova to derive the even stronger bound $g_{a \gamma \gamma} \lesssim 2 \cdot 10^{- 13} \, {\rm GeV}^{- 1}$ for an ALP mass $m_a \lesssim 10^{- 9} \, {\rm eV}$. We carefully analyze the considered ALP production mechanism and find that it is oversimplified to an unacceptable extent. By taking into account the minimal ingredients required by a realistic analysis, we conclude that the previous results are doomed to failure. As a consequence, all papers quoting the above bound should be properly revised. Yet, since we are unable to rule out the possibility that protoneutron stars emit ALPs, in case a core-collapse supernova explodes in the Galaxy the $\gamma$-ray satellite missions active at that time should look for photons possibly coming from the supernova. 	
1801.00062v1	http://arxiv.org/pdf/1801.00062v1	2017	Dendritic error backpropagation in deep cortical microcircuits	João Sacramento|Rui Ponte Costa|Yoshua Bengio|Walter Senn	  Animal behaviour depends on learning to associate sensory stimuli with the desired motor command. Understanding how the brain orchestrates the necessary synaptic modifications across different brain areas has remained a longstanding puzzle. Here, we introduce a multi-area neuronal network model in which synaptic plasticity continuously adapts the network towards a global desired output. In this model synaptic learning is driven by a local dendritic prediction error that arises from a failure to predict the top-down input given the bottom-up activities. Such errors occur at apical dendrites of pyramidal neurons where both long-range excitatory feedback and local inhibitory predictions are integrated. When local inhibition fails to match excitatory feedback an error occurs which triggers plasticity at bottom-up synapses at basal dendrites of the same pyramidal neurons. We demonstrate the learning capabilities of the model in a number of tasks and show that it approximates the classical error backpropagation algorithm. Finally, complementing this cortical circuit with a disinhibitory mechanism enables attention-like stimulus denoising and generation. Our framework makes several experimental predictions on the function of dendritic integration and cortical microcircuits, is consistent with recent observations of cross-area learning, and suggests a biological implementation of deep learning. 	
1801.01416v1	http://arxiv.org/pdf/1801.01416v1	2018	The dynamics of a shear band	Diana Giarola|Domenico Capuani|Davide Bigoni	  A shear band of finite length, formed inside a ductile material at a certain stage of a con- tinued homogeneous strain, provides a dynamic perturbation to an incident wave field, which strongly influences the dynamics of the material and affects its path to failure. The investigation of this perturbation is presented for a ductile metal, with reference to the incremental mechanics of a material obeying the J 2-deformation theory of plasticity (a special form of prestressed, elastic, anisotropic, and incompressible solid). The treatment originates from the derivation of integral representations relating the incremental mechan- ical fields at every point of the medium to the incremental displacement jump across the shear band faces, generated by an impinging wave. The boundary integral equations (under the plane strain assumption) are numerically approached through a collocation technique, which keeps into account the singularity at the shear band tips and permits the analysis of an incident wave impinging a shear band. It is shown that the presence of the shear band induces a resonance, visible in the incremental displacement field and in the stress intensity factor at the shear band tips, which promotes shear band growth. Moreover, the waves scattered by the shear band are shown to generate a fine texture of vibrations, par- allel to the shear band line and propagating at a long distance from it, but leaving a sort of conical shadow zone, which emanates from the tips of the shear band. 	
1801.06266v1	http://arxiv.org/pdf/1801.06266v1	2018	Generalization of BCS theory to short coherence length superconductors:   A BCS--Bose-Einstein crossover scenario	Qijin Chen	  The (mean field based) BCS theory is considered one of the most successful theories in condensed matter physics. It is justified in ordinary metal superconductors the coherence length $\xi$ is large, with two important features: the order parameter (OP) and excitation gap (EG) are identical, and the pair formation and their Bose condensation take place at the same temperature Tc. It fails to explain the underdoped cuprate superconductivity: EG is finite at Tc and thus distinct from OP. Since these superconductors belong to a large class of small $\xi$ materials, this failure has the potential for widespread impact.   Here we have extended BCS theory in a natural way to short $\xi$ superconductors, based on a BCS--BEC crossover scenario, and arrived at a simple physical picture in which incoherent, finite momentum pairs become progressively more important as the pairing interaction becomes stronger, leading to the distinction between EG and OP. The superconductivity from the fermionic perspective and BEC from the bosonic perspective are just two sides of the same coin.   Our theory is capable of making verifiable quantitative predictions. We obtain a cuprate phase diagram (with one free parameter) , in (semi-)quantitative agreement with experiment. The mutually compensating contributions from fermionic quasiparticles and bosonic pair excitations provides a natural explanation for the quasi-universal behavior of the in-plane superfluid density versus T. Our bosonic pair excitations also provide an intrinsic mechanism for the long mysterious linear T terms in the specific heat. Incoherent pair contributions lead to new low T power laws, consistent with existing experiments. Finally, we demonstrated that the onset of superconducting long range order leads to sharp features in the specific heat at Tc, consistent with experiment. 	
1801.08727v1	http://arxiv.org/pdf/1801.08727v1	2018	Impact ionization and transport properties of hexagonal boron nitride in   constant-voltage measurement	Y. Hattori|T. Taniguchi|K. Watanabe|K. Nagashio	  The electrical evaluation of the crystallinity of hexagonal boron nitride (h-BN) is still limited to the measurement of dielectric breakdown strength, in spite of its importance as the substrate for 2-dimensional van der Waals heterostructure devices. In this study, physical phenomena for degradation and failure in exfoliated single-crystal h-BN films were investigated using the constant-voltage stress test. At low electrical fields, the current gradually reduced and saturated with time, while the current increased at electrical fields higher than ~8 MV/cm and finally resulted in the catastrophic dielectric breakdown. These transient behaviors may be due to carrier trapping to the defect sites in h-BN because trapped carriers lower or enhance the electrical fields in h-BN depending on their polarities. The key finding is the current enhancement with time at the high electrical field, suggesting the accumulation of electrons generated by the impact ionization process. Therefore, a theoretical model including the electron generation rate by impact ionization process was developed. The experimental data support the expected degradation mechanism of h-BN. Moreover, the impact ionization coefficient was successfully extracted, which is comparable to that of SiO2, even though the fundamental band gap for h-BN is smaller than that for SiO2. Therefore, the dominant impact ionization in h-BN could be band-to-band excitation, not defect-assisted impact ionization. 	
1802.00951v1	http://arxiv.org/pdf/1802.00951v1	2018	Scheduling and Checkpointing optimization algorithm for Byzantine fault   tolerance in Cloud Clusters	Sathya Chinnathambi|Agilan Santhanam	  Among those faults Byzantine faults offers serious challenge to fault tolerance mechanism, because it often go undetected at the initial stage and it can easily propagate to other VMs before a detection is made. Consequently some of the mission critical application such as air traffic control, online baking etc still staying away from the cloud for such reasons. However if a Byzantine faults is not detected and tolerated at initial stage then applications such as big data analytics can go completely wrong in spite of hours of computations performed by the entire cloud. Therefore in the previous work a fool-proof Byzantine fault detection has been proposed, as a continuation this work designs a scheduling algorithm (WSSS) and checkpoint optimization algorithm (TCC) to tolerate and eliminate the Byzantine faults before it makes any impact. The WSSS algorithm keeps track of server performance which is part of Virtual Clusters to help allocate best performing server to mission critical application. WSSS therefore ranks the servers based on a counter which monitors every Virtual Nodes (VN) for time and performance failures. The TCC algorithm works to generalize the possible Byzantine error prone region through monitoring delay variation to start new VNs with previous checkpointing. Moreover it can stretch the state interval for performing and error free VNs in an effect to minimize the space, time and cost overheads caused by checkpointing. The analysis is performed with plotting state transition and CloudSim based simulation. The result shows TCC reduces fault tolerance overhead exponentially and the WSSS allots virtual resources effectively 	
1802.03898v1	http://arxiv.org/pdf/1802.03898v1	2018	Scalable Downward Routing for Wireless Sensor Networks and Internet of   Things Actuation	Xiaoyang Zhong|Yao Liang	  In this paper, we study the downward routing for network control/actuation in large-scale and heterogeneous wireless sensor networks (WSNs) and Internet of Things (IoT). We propose the Opportunistic Source Routing (OSR), a scalable and reliable downward routing protocol for WSNs/IoT. OSR introduces opportunistic routing into traditional source routing based on the parent set of a node's upward routing in data collection, significantly addressing the drastic link dynamics in low-power and lossy WSNs. We devise a novel adaptive Bloom filter mechanism to effectively and efficiently encode a downward source-route in OSR, which enables a significant reduction of the length of source-route field in packet header. OSR is scalable to very large-size WSN/IoT deployments, since each resource-constrained node in the network only stores the set of its direct children. The probabilistic nature of the Bloom filter passively explores opportunistic routing. Upon a delivery failure at any hop along the downward path, OSR actively performs opportunistic routing to bypass the obsolete/bad link. We demonstrate the desirable scalability of OSR against the standard RPL downward routing. We evaluate the performance of OSR via both simulations and real-world testbed experiments, in comparison with the standard RPL (both storing mode and non-storing mode), ORPL, and the representative dissemination protocol Drip. Our results show that OSR significantly outperforms RPL and ORPL in scalability and reliability. OSR also achieves significantly better energy efficiency compared to TinyRPL and Drip which are based on the same TinyOS platform as OSR implementation. 	
1803.01671v1	http://arxiv.org/pdf/1803.01671v1	2018	Do thermodynamically stable rigid solids exist?	Parswa Nath|Saswati Ganguly|Jürgen Horbach|Peter Sollich|Smarajit Karmakar|Surajit Sengupta	  Customarily, crystalline solids are defined to be {\em rigid} since they resist changes of shape determined by their boundaries. However, rigid solids cannot exist in the thermodynamic limit where boundaries become irrelevant. Particles in the solid may rearrange to adjust to shape changes eliminating stress without destroying crystalline order. Rigidity is therefore valid only in the {\em metastable} state that emerges because these particle rearrangements in response to a deformation, or strain, are associated with slow collective processes. Here, we show that a thermodynamic collective variable may be used to quantify particle rearrangements that occur as a solid is deformed at zero strain rate. Advanced Monte Carlo simulation techniques are then employed to obtain the equilibrium free energy as a function of this variable. Our results lead to a new view on rigidity: While at zero strain a rigid crystal coexists with one that responds to infinitesimal strain by rearranging particles and expelling stress, at finite strain the rigid crystal is metastable, associated with a free energy barrier that decreases with increasing strain. The rigid phase becomes thermodynamically stable by switching on an external field, which penalises particle rearrangements. This produces a line of first-order phase transitions in the field - strain plane that intersects the origin. Failure of a solid once strained beyond its elastic limit is associated with kinetic decay processes of the metastable rigid crystal deformed with a finite strain rate. These processes can be understood in quantitative detail using our computed phase diagram as reference. 	
0702437v1	http://arxiv.org/pdf/astro-ph/0702437v1	2007	Dirt, Gravity, and Lunar-Based Telescopes: The Value Proposition for   Astronomy	Dan Lester	  The lunar surface has historically been considered an optimal site for a broad range of astronomical telescopes. That assumption, which has come to be somewhat reflexive, is critically examined in this paper and found to be poorly substantiated. The value of the lunar surface for astronomy may be broadly compelling only in comparison to terrestrial sites. It is suggested here that the development and successful operation of the Hubble Space Telescope marked a turning point in the perception of value for free-space siting of astronomical telescopes, and for telescopes on the Moon. As the astronomical community considers the scientific potential of the Vision for Space Exploration (VSE) and the return to the Moon in particular, it should construct a value proposition that includes the tools, technology, and architecture being developed for this return, as these can well be seen as being more astronomically enabling than the lunar surface itself - a destination that offers little more than rocks and gravity. While rocks and gravity may offer astronomical opportunity in certain scientific niches, our attention should be focused on the striking potential of human and robotic dexterity across cis-lunar space. It is this command of our environs that the VSE truly offers us. 	
9910488v1	http://arxiv.org/pdf/cond-mat/9910488v1	1999	Macroscopic Dielectric Constant for Microstructures of Sedimentary Rocks	R. Hilfer|J. Widjajakusuma|B. Biswal	  An approximate method to calculate dielectric response and relaxation functions for water saturated sedimentary rocks is tested for realistic threedimensional pore space images. The test is performed by comparing the prediction from the approximate method against the exact solution. The approximate method is based on image analysis and local porosity theory. An empirical rule for the specification of the length scale in local porosity theory is advanced. The results from the exact solution are compared to those obtained using local porosity theory and various other approximate mixing laws. The calculation based on local porosity theory is found to yield improved quantitative agreement with the exact result. 	
0409429v1	http://arxiv.org/pdf/cond-mat/0409429v1	2004	Rocking motion induced charging of C60 on h-BN/Ni(111)	M. Muntwiler|W. Auwarter|A. P. Seitsonen|J. Osterwalder|T. Greber	  One monolayer of C60 on one monolayer of hexagonal boron nitride on nickel is investigated by photoemission. Between 150 and 250 K the work function decreases and the binding energy of the highest occupied molecular orbital (HOMO) increases by approx. 100 meV. In parallel, the occupancy of the, in the cold state almost empty, lowest unoccupied molecular orbital (LUMO) changes by 0.4 electrons. This charge redistribution is triggered by onset of molecular rocking motion, i.e. by orientation dependent tunneling between the LUMO of C60 and the substrate. The magnitude of the charge transfer is large and cannot be explained within a single particle picture. It is proposed to involve electron-phonon coupling where C60- polaron formation leads to electron self-trapping. 	
0510137v2	http://arxiv.org/pdf/cond-mat/0510137v2	2005	Ca_25Co_22O_56(OH)_28: a layered misfit compound	T. Klimczuk|H. W. Zandbergen|N. M. van der Pers|L. Viciu|V. L. Miller|M. -H. Lee|R. J. Cava	  The high pressure synthesis, structure and magnetic properties of Ca_25Co_22O_56(OH)_28 are reported. The compound has a misfit structure, consisting of double, square calcium oxide hydroxide rock-salt-like layers between hexagonal CoO_2 layers. The misfit compound crystallizes in the monoclinic space group C2/m, and can be characterized by the coexistence of two subsystems with common a=4.893(5)A, c=8.825(9)A and b=95.745(8) parameters, and different b parameters: b_RS=4.894(5)A, and b_HEX=2.809(3)A, for the rock-salt and hexagonal type planes respectively. The compound shows Curie-Weiss paramagnetism with an antiferromagnetic Weiss temperature of -43K and a reduced Co moment. Substantial deviations from Curie-Weiss behavior are seen below 50K with no indication of magnetic ordering. No superconductivity was observed down to a temperature of 2K. 	
9807009v1	http://arxiv.org/pdf/hep-ex/9807009v1	1998	Measurement of the energy spectrum of underground muons at Gran Sasso   with a transition radiation detector	 The MACRO Collaboration|M. Ambrosio et al	  We have measured directly the residual energy of cosmic ray muons crossing the MACRO detector at the Gran Sasso Laboratory. For this measurement we have used a transition radiation detector consisting of three identical modules, each of about 12 m^2 area, operating in the energy region from 100 GeV to 1 TeV. The results presented here were obtained with the first module collecting data for more than two years. The average single muon energy is found to be 320 +/- 4 (stat.) +/- 11 (syst.) GeV in the rock depth range 3000-6500 hg/cm^2. The results are in agreement with calculations of the energy loss of muons in the rock above the detector. 	
0207043v2	http://arxiv.org/pdf/hep-ex/0207043v2	2002	Measurement of the residual energy of muons in the Gran Sasso   underground Laboratories	 The MACRO Collaboration|M. Ambrosio	  The MACRO detector was located in the Hall B of the Gran Sasso underground Laboratories under an average rock overburden of 3700 hg/cm^2. A transition radiation detector composed of three identical modules, covering a total horizontal area of 36 m^2, was installed inside the empty upper part of the detector in order to measure the residual energy of muons. This paper presents the measurement of the residual energy of single and double muons crossing the apparatus. Our data show that double muons are more energetic than single ones. This measurement is performed over a standard rock depth range from 3000 to 6500 hg/cm^2. 	
9705408v2	http://arxiv.org/pdf/hep-ph/9705408v2	1997	A Three-Dimensional Code for Muon Propagation through the Rock: MUSIC	P. Antonioli|C. Ghetti|E. V. Korolkova|V. A. Kudryavtsev|G. Sartorelli	  We present a new three-dimensional Monte-Carlo code MUSIC (MUon SImulation Code) for muon propagation through the rock. All processes of muon interaction with matter with high energy loss (including the knock-on electron production) are treated as stochastic processes. The angular deviation and lateral displacement of muons due to multiple scattering, as well as bremsstrahlung, pair production and inelastic scattering are taken into account. The code has been applied to obtain the energy distribution and angular and lateral deviations of single muons at different depths underground. The muon multiplicity distributions obtained with MUSIC and CORSIKA (Extensive Air Shower simulation code) are also presented. We discuss the systematic uncertainties of the results due to different muon bremsstrahlung cross-sections. 	
9911493v1	http://arxiv.org/pdf/hep-ph/9911493v1	1999	Narrow muon bundles from muon pair production in rock	V. A. Kudryavtsev|E. V. Korolkova|N. J. C. Spooner	  We revise the process of muon pair production by high-energy muons in rock using the recently published cross-section. The three-dimensional Monte Carlo code MUSIC has been used to obtain the characteristics of the muon bundles initiated via this process. We have compared them with those of conventional muon bundles initiated in the atmosphere and shown that large underground detectors, capable of collecting hundreds of thousands of multiple muon events, can discriminate statistically muon induced bundles from conventional ones. However, we find that the enhancement of the measured muon decoherence function over that predicted at small distances, recently reported by the MACRO experiment, cannot be explained by the effect of muon pair production alone, unless its cross-section is underestimated by a factor of 3. 	
0601402v1	http://arxiv.org/pdf/math/0601402v1	2006	Normal generation and Clifford index	Youngook Choi|Seonja Kim|Young Rock Kim	  Let $C$ be a smooth curve of genus $g\ge 4$ and Clifford index $c$. In this paper, we prove that if $C$ is neither hyperelliptic nor bielliptic with $g\ge 2c+5$ and $\mathcal M$ computes the Clifford index of $C$, then either $\deg \mathcal M\le \frac{3c}{2}+3$ or $|\mathcal M|=|g^1_{c+2}+h^1_{c+2}|$ and $g=2c+5$. This strengthens the Coppens and Martens' theorem (\cite{CM}, Corollary 3.2.5). Furthermore, for the latter case (1) $\mathcal M$ is half-canonical unless $C$ is a $\frac{c+2}{2}$-fold covering of an elliptic curve, (2) $\mathcal M(F)$ fails to be normally generated with $\cli(\mathcal M(F))=c$, $h^1(\mathcal M(F))=2$ for $F\in g^1_{c+2}$. Such pairs $(C,\mathcal M)$ can be found on a $K3$-surface whose Picard group is generated by a hyperplane section in $\mathbb P^r$. For such a $(C, \mathcal M)$ on a K3-surface, $\mathcal M$ is normally generated while $\mathcal M(F)$ fails to be normally generated with $\cli(\mathcal M)=\cli(\mathcal M(F))=c$. 	
0202015v1	http://arxiv.org/pdf/physics/0202015v1	2002	Structure analysis of the Ga-stabilized GaAs(001)-c(8x2) surface at high   temperatures	Akihiro Ohtake|Shiro Tsukamoto|Markus Pristovsek|Nobuyuki Koguchi|Masashi Ozeki	  Structure of the Ga-stabilized GaAs(001)-c(8x2) surface has been studied using rocking-curve analysis of reflection high-energy electron diffraction (RHEED). The c(8x2) structure emerges at temperatures higher than 600C, but is unstable with respect to the change to the (2x6)/(3x6) structure at lower temperatures. Our RHEED rocking-curve analysis at high temperatures revealed that the c(8x2) surface has the structure which is basically the same as that recently proposed by Kumpf et al. [Phys. Rev. Lett. 86, 3586 (2001)]. We found that the surface atomic configurations are locally fluctuated at high temperatures without disturbing the c(8x2) periodicity. 	
0706.1110v1	http://arxiv.org/pdf/0706.1110v1	2007	Calculation of the Underground Muon Intensity Crouch Curve from a   Parameterization of the Flux at Surface	Juergen Reichenbacher	  Utilizing only the vertical muon intensity of the Gaisser parameterization of the muon flux at the surface and propagating this energy spectrum underground according to statistical ionization and radiative energy losses, it is possible to calculate the underground muon intensity Crouch curve. In addition, the primary spectral index of the Gaisser parameterization can be adjusted from $E^{-2.7}$ to $E^{-2.643}$ simply by minimizing the deviation from the Crouch curve. For chemical compositions other than standard rock, the propagation of the spectrum underground can be repeated with a different muon energy loss in the material. The resulting underground muon intensity curve represents a consistent conversion of the Crouch curve to the local rock, fully accounting for the energy dependence of the muon $dE/dx$. 	
0710.1369v1	http://arxiv.org/pdf/0710.1369v1	2007	Consequence of doping in spatiotemporal rock-paper-scissors games	Wang Zhijian	  What determines species diversity is dramatic concern in science. Here we report the effect of doping on diversity in spatiotemporal rock-paper-scissors (RPS) games, which can be observed directly in ecological, biological and social systems in nature. Doping means that there exists some buffer patches which do not involve the main procession of the conflicts but occupied the game space. Quantitative lattices simulation finds that (1) decrease of extinction possibility is exponential dependent on the increase of doping rate, (2) the possibility of the conflict is independent of doping rate at well mix evolution beginning, and is buffered by doping in long time coexistence procession. Practical meaning of doping are discussed. To demonstrate the importance of doping, we present one practical example for microbial laboratory efficient operation and one theoretical example for human-environment co-existence system better understanding. It suggests that, for diversity, doping can not be neglected. 	
0805.3639v1	http://arxiv.org/pdf/0805.3639v1	2008	Fossilization in Geopark Araripe studied through X-ray diffraction,   scanning microscopy and thermogravimetric analysis	Ricardo J. C. Lima|Paulo T. C. Freire|Zélia S. Macedo|José M. Sasaki|Antônio A. F. Saraiva	  The Geopark Araripe, located in Northeastern Brazil, is the first UNESCO Natural Park in the South hemisphere and a world-famous fossil deposit of the Early Cretaceous period (approximately 120 million years). Fossilized fish fauna in Geopark Araripe is found inside of sedimentary rocks in three-dimensional forms. In the present study sedimentary rocks and fossil fish Rhacolepis bucalis have been carefully analysed by means of X-ray powder diffraction, scanning electron microscopy and termogravimetric analysis. Mineralogical composition of the fossil fish was explained in terms of facts occurred at the initial stages of the opening of the South Atlantic and the oceanic hydrothermal phenomena (``black smoker'', ``white smoker'' and warm-water events). The occurrence of organic substance was, for the first time, evaluated in collapsed internal elements (intestinal and muscles) by termogravimetric analysis. 	
0806.2399v1	http://arxiv.org/pdf/0806.2399v1	2008	Thermal Conductivity from Core and Well log Data	Andreas Hartmann|Volker Rath|Christoph Clauser	  The relationships between thermal conductivity and other petrophysical properties have been analysed for a borehole drilled in a Tertiary Flysch sequence. We establish equations that permit us to predict rock thermal conductivity from logging data. A regression analysis of thermal conductivity, bulk density, and sonic velocity yields thermal conductivity with an average accuracy of better than 0.2 W/(m K). As a second step, logging data is used to compute a lithological depth profile, which in turn is used to calculate a thermal conductivity profile. From a comparison of the conductivity-depth profile and the laboratory data it can be concluded that thermal conductivity can be computed with an accuracy of less than 0.3 W/(m K)from conventional wireline data. The comparison of two different models shows that this approach can be practical even if old and incomplete logging data is used. The results can be used to infer thermal conductivity for boreholes without appropriate core data that are drilled in a similar geological setting. 	
0808.3312v1	http://arxiv.org/pdf/0808.3312v1	2008	Three- and four-state rock-paper-scissors games with diffusion	Matti Peltomaki|Mikko Alava	  Cyclic dominance of three species is a commonly occurring interaction dynamics, often denoted the rock-paper-scissors (RPS) game. Such type of interactions is known to promote species coexistence. Here, we generalize recent results of Reichenbach et al. (e.g. Nature 448, 1046 (2007)) of a four-state variant of RPS. We show that spiral formation takes place only without a conservation law for the total density. Nevertheless, in general fast diffusion can destroy species coexistence. We also generalize the four-state model to slightly varying reaction rates. This is shown both analytically and numerically not to change pattern formation, or the effective wave length of the spirals, and therefore does not alter the qualitative properties of the cross-over to extinction. 	
0809.1735v1	http://arxiv.org/pdf/0809.1735v1	2008	Meteorites and the physico-chemical conditions in the early solar nebula	Jerome Aleon	  Chondritic meteorites constitute the most ancient rock record available in the laboratory to study the formation of the solar system and its planets. Detailed investigations of their mineralogy, petrography, chemistry and isotopic composition and comparison with other primitive solar system samples such as cometary dust particles have allowed through the years to decipher the conditions of formation of their individual components thought to have once been free-floating pieces of dust and rocks in the early solar nebula. When put in the context of astrophysical models of young stellar objects, chondritic meteorites and cometary dust bring essential insights on the astrophysical conditions prevailing in the very first stages of the solar system. Several exemples are shown in this chapter, which include (1) high temperature processes and the formation of chondrules and refractory inclusions, (2) oxygen isotopes and their bearing on photochemistry and large scale geochemical reservoirs in the nebula, (3) organosynthesis and cold cloud chemistry recorded by organic matter and hydrogen isotopes, (4) irradiation of solids by flares from the young Sun and finally (5) large scale transport and mixing of material evidenced in chondritic interplanetary dust particles and samples returned from comet Wild2 by the Stardust mission. 	
0810.0789v1	http://arxiv.org/pdf/0810.0789v1	2008	Toward zeta functions and complex dimensions of multifractals	Michel L. Lapidus|John A. Rock	  Multifractals are inhomogeneous measures (or functions) which are typically described by a full spectrum of real dimensions, as opposed to a single real dimension. Results from the study of fractal strings in the analysis of their geometry, spectra and dynamics via certain zeta functions and their poles (the complex dimensions) are used in this text as a springboard to define similar tools fit for the study of multifractals such as the binomial measure. The goal of this work is to shine light on new ideas and perspectives rather than to summarize a coherent theory. Progress has been made which connects these new perspectives to and expands upon classical results, leading to a healthy variety of natural and interesting questions for further investigation and elaboration. 	
0810.5231v1	http://arxiv.org/pdf/0810.5231v1	2008	Tetragonal CuO: A new end member of the 3d transition metal monoxides	Wolter Siemons|Gertjan Koster|Dave H. A. Blank|Robert H. Hammond|Theodore H. Geballe|Malcolm R. Beasley	  Monoclinic CuO is anomalous both structurally as well as electronically in the 3$d$ transition metal oxide series. All the others have the cubic rock salt structure. Here we report the synthesis and electronic property determination of a tetragonal (elongated rock salt) form of CuO created using an epitaxial thin film deposition approach. In situ photoelectron spectroscopy suggests an enhanced charge transfer gap $\Delta$ with the overall bonding more ionic. As an end member of the 3d transition monoxides, its magnetic properties should be that of a high $T_N$ antiferromagnet. 	
0901.0955v2	http://arxiv.org/pdf/0901.0955v2	2009	Four-state rock-paper-scissors games on constrained Newman-Watts   networks	Guo-Yong Zhang|Yong Chen|Wei-Kai Qi|Shao-Meng Qin	  We study the cyclic dominance of three species in two-dimensional constrained Newman-Watts networks with a four-state variant of the rock-paper-scissors game. By limiting the maximal connection distance $R_{max}$ in Newman-Watts networks with the long-rang connection probability $p$, we depict more realistically the stochastic interactions among species within ecosystems. When we fix mobility and vary the value of $p$ or $R_{max}$, the Monte Carlo simulations show that the spiral waves grow in size, and the system becomes unstable and biodiversity is lost with increasing $p$ or $R_{max}$. These results are similar to recent results of Reichenbach \textit{et al.} [Nature (London) \textbf{448}, 1046 (2007)], in which they increase the mobility only without including long-range interactions. We compared extinctions with or without long-range connections and computed spatial correlation functions and correlation length. We conclude that long-range connections could improve the mobility of species, drastically changing their crossover to extinction and making the system more unstable. 	
0901.4032v1	http://arxiv.org/pdf/0901.4032v1	2009	On the upstream mobility scheme for two-phase flow in porous media	Siddhartha Mishra|Jérôme Jaffré	  When neglecting capillarity, two-phase incompressible flow in porous media is modelled as a scalar nonlinear hyperbolic conservation law. A change in the rock type results in a change of the flux function. Discretizing in one-dimensional with a finite volume method, we investigate two numerical fluxes, an extension of the Godunov flux and the upstream mobility flux, the latter being widely used in hydrogeology and petroleum engineering. Then, in the case of a changing rock type, one can give examples when the upstream mobility flux does not give the right answer. 	
0910.0811v2	http://arxiv.org/pdf/0910.0811v2	2010	Exoplanet Chemistry	Katharina Lodders	  The terrestrial and gas-giant planets in our solar system may represent some prototypes for planets around other stars; the exoplanets because most stars have similar overall elemental abundances as our sun. The solar system planets represent at least four chemical planet types, depending on the phases that make them: Terrestrial-like planets made of rock (metal plus silicates), Plutonian planets made of rock and ice, Neptunian giant planets of rocky, icy with low H and He contents, and Jovian gas-giant planets of rocky, icy planets with near-solar H and He contents. The planetary compositions are linked to the chemical fractionation in the planetary accretion disks. Chemical tracers of these fractionations are described. Many known exoplanets are gas-giant planets with up to several Jupiter-masses and their atmospheric chemistry is compared to that of brown dwarfs. Exoplanets in close orbits around their host stars may resemble hot brown dwarfs (L-dwarfs). Planets receiving less radiation form their host may compare more to the methane-rich T dwarfs. The cloud layers resulting from condensation of oxides, metal, sulfides, and salts in these hot and cool gas giant planets and their chemical tracers are described. 	
0910.1524v2	http://arxiv.org/pdf/0910.1524v2	2010	Prograde rotation of protoplanets by accretion of pebbles in a gaseous   environment	Anders Johansen|Pedro Lacerda	  We perform hydrodynamical simulations of the accretion of pebbles and rocks onto protoplanets of a few hundred kilometres in radius, including two-way drag force coupling between particles and the protoplanetary disc gas. Particle streams interacting with the gas far out within the Hill sphere of the protoplanet spiral into a prograde circumplanetary disc. Material is accreted onto the protoplanet due to stirring by the turbulent surroundings. We speculate that the trend for prograde rotation among the largest asteroids is primordial and that protoplanets accreted 10%-50% of their mass from pebbles and rocks during the gaseous solar nebula phase. Our model also offers a possible explanation for the narrow range of spin periods observed among the largest bodies in the asteroid and trans-Neptunian belts, and predicts that 1000 km-scale Kuiper belt objects that have not experienced giant impacts should preferentially spin in the prograde direction. 	
0910.2774v1	http://arxiv.org/pdf/0910.2774v1	2009	Gravity-Gradient Subtraction in 3rd Generation Underground   Gravitational-Wave Detectors in Homogeneous Media	Jan Harms|Riccardo DeSalvo|Steven Dorsher|Vuk Mandic	  In this paper, we develop a new approach to gravity-gradient noise subtraction for underground gravitational-wave detectors in homogeneous rock. The method is based on spatial harmonic expansions of seismic fields. It is shown that gravity-gradient noise produced by seismic fields from distant sources, stationary or non-stationary, can be calculated from seismic data measured locally at the test mass. Furthermore, the formula is applied to seismic fields from stationary local sources. It is found that gravity gradients from these fields can be subtracted using local seismic measurements. The results are confirmed numerically with a finite-element simulation. A new seismic-array design is proposed that provides the additional information about the seismic field required to ensure applicability of the approach to realistic scenarios even with inhomogeneous rock and non-stationary local sources. 	
1002.2715v1	http://arxiv.org/pdf/1002.2715v1	2010	Theoretical relation between water flow rate in a vertical fracture and   rock temperature in the surrounding massif	Jean-Christophe Maréchal|Pierre Perrochet	  A steady-state analytical solution is given describing the temperature distribution in a homogeneous massif perturbed by cold water flow through a discrete vertical fracture. A relation is derived to express the flow rate in the fracture as a function of the temperature measured in the surrounding rock. These mathematical results can be useful for tunnel drilling as it approaches a vertical cold water bearing structure that induces a thermal anomaly in the surrounding massif. During the tunnel drilling, by monitoring this anomaly along the tunnel axis one can quantify the flow rate in the discontinuity ahead before intersecting the fracture. The cases of the Simplon, Mont Blanc and Gotthard tunnels (Alps) are handled with this approach which shows very good agreement between observed temperatures and the theoretical trend. The flow rates before drilling of the tunnel predicted with the theoretical solution are similar in the Mont Blanc and Simplon cases, as well as the flow rates observed during the drilling. However, the absence of information on hydraulic gradients (before and during drilling) and on fracture specific storage prevents direct predictions of discharge rates in the tunnel. 	
1002.3916v1	http://arxiv.org/pdf/1002.3916v1	2010	Establishment of earth tides effect on water level fluctuations in an   unconfined hard rock aquifer using spectral analysis	Jean-Christophe Maréchal|Mp Sarma|Shakeel Ahmed|Patrick Lachassagne	  Short-interval water level measurements using automatic water level recorder in a deep well in an unconfined crystalline rock aquifer at the campus of NGRI, near Hyderabad shows a cyclic fluctuation in the water levels. The observed values clearly show the principal trend due to rainfall recharge. Spectral analysis was carried out to evaluate correlation of the cyclic fluctuation to the synthetic earth tides as well as groundwater withdrawal time series in the surrounding. It was found that these fluctuations have considerably high correlation with earth tides whereas groundwater pumping does not show any significant correlation with water table fluctuations. It is concluded that earth tides cause the fluctuation in the water table. These fluctuations were hitherto unobserved during manual observations made over larger time intervals. It indicates that the unconfined aquifer is characterised by a low porosity. 	
1003.4277v1	http://arxiv.org/pdf/1003.4277v1	2010	Pure Saddle Points and Symmetric Relative Payoff Games	Peter Duersch|Joerg Oechssler|Burkhard C. Schipper	  It is well known that the rock-paper-scissors game has no pure saddle point. We show that this holds more generally: A symmetric two-player zero-sum game has a pure saddle point if and only if it is not a generalized rock-paper-scissors game. Moreover, we show that every finite symmetric quasiconcave two-player zero-sum game has a pure saddle point. Further sufficient conditions for existence are provided. We apply our theory to a rich collection of examples by noting that the class of symmetric two-player zero-sum games coincides with the class of relative payoff games associated with symmetric two-player games. This allows us to derive results on the existence of a finite population evolutionary stable strategies. 	
1007.1467v2	http://arxiv.org/pdf/1007.1467v2	2011	Partition zeta functions, multifractal spectra, and tapestries of   complex dimensions	Kate E. Ellis|Michel L. Lapidus|Michael C. Mackenzie|John A. Rock	  For a Borel measure and a sequence of partitions on the unit interval, we define a multifractal spectrum based on coarse Holder regularity. Specifically, the coarse Holder regularity values attained by a given measure and with respect to a sequence of partitions generate a sequence of lengths (or rather, scales) which in turn define certain Dirichlet series, called the partition zeta functions. The abscissae of convergence of these functions define a multifractal spectrum whose concave envelope is the (geometric) Hausdorff multifractal spectrum which follows from a certain type of Moran construction. We discuss at some length the important special case of self-similar measures associated with weighted iterated function systems and, in particular, certain multinomial measures. Moreover, our multifractal spectrum is shown to extend to a tapestry of complex dimensions for two specific atomic measures. 	
1011.2567v1	http://arxiv.org/pdf/1011.2567v1	2010	A New Natural Gamma Radiation Measurement System for Marine Sediment and   Rock Analysis	M. A. Vasiliev|P. Blum|G. Chubarian|R. Olsen|C. Bennight|T. Cobine|D. Fackler|M. Hastedt|D. Houpt|Z. Mateo|Y. B. Vasilieva	  A new high-efficiency and low-background system for the measurement of natural gamma radioactivity in marine sediment and rock cores retrieved from beneath the seabed was designed, built, and installed on the JOIDES Resolution research vessel. The system includes eight large NaI(Tl) detectors that measure adjacent intervals of the core simultaneously, maximizing counting times and minimizing statistical error for the limited measurement times available during drilling expeditions. Effect to background ratio is maximized with passive lead shielding, including both ordinary and low-activity lead. Large-area plastic scintillator active shielding filters background associated with the high-energy part of cosmic radiation. The new system has at least an order of magnitude higher statistical reliability and significantly enhances data quality compared to other offshore natural gamma radiation (NGR) systems designed to measure geological core samples. Reliable correlations and interpretations of cored intervals are possible at rates of a few counts per second. 	
1102.0859v1	http://arxiv.org/pdf/1102.0859v1	2011	High-pressure synthesis of MnO-ZnO solid solutions with rock salt   structure: in situ X-ray diffraction studies	P. S. Sokolov|A. N. Baranov|C. Lathe|V. Z. Turkevich|V. L. Solozhenko	  X-ray diffraction with synchrotron radiation has been used for the first time to study chemical interaction in the MnO-ZnO system at 4.8 GPa and temperatures up to 1600 K. Above 750 K, the chemical reaction between MnO and ZnO has been observed that resulted in the formation of rock salt (rs) Mn1-xZnxO solid solutions (0.3 \leq x \leq 0.7). The lattice parameters of these solid solutions have been in situ measured at high pressure as a function of temperature, and corresponding thermal expansion coefficients have been calculated. 	
1110.5315v1	http://arxiv.org/pdf/1110.5315v1	2011	Self-oscillation acoustic system destined to measurement of stresses in   mass rocks	Janusz Kwasniewski|Yury Kravtsov|Ireneusz Dominik|Lech Dorobczynski	  The paper presents an electronic self-oscillation acoustic system (SAS) destined to measure of stresses variations in the elastic media. The system consists of piezoelectric detector, amplifier-limiter, pass-band filter, piezoelectric exciter and the frequency meter. The mass rock plays a role of delaying element, in which variations in stresses causing the variations of acoustic wave velocity of propagation, and successive variation in frequency of oscillations generated by system.   The laboratory test permitted to estimate variations in frequency caused by variations in stresses of elastic medium. The principles of selection of frequency and other parameters of the electronic system in application to stresses measurement in condition of the mine were presented. 	
1111.2979v1	http://arxiv.org/pdf/1111.2979v1	2011	Hydraulic transmissivity and heat exchange efficiency of open fractures:   a model based on lowpass filtered apertures	Amélie Neuville|Renaud Toussaint|Jean Schmittbuhl	  Natural open joints in rocks commonly present multi-scale self-affine apertures. This geometrical complexity affects fluid transport and heat exchange between the flow- ing fluid and the surrounding rock. In particular, long range correlations of self-affine apertures induce strong channeling of the flow which influences both mass and heat advection. A key question is to find a geometrical model of the complex aperture that describes at best the macroscopic properties (hydraulic conductivity, heat exchange) with the smallest number of parameters. Solving numerically the Stokes and heat equa- tions with a lubrication approximation, we show that a low pass filtering of the aperture geometry provides efficient estimates of the effective hydraulic and thermal properties (apertures). A detailed study of the influence of the bandwidth of the lowpass filtering on these transport properties is also performed. For instance, keeping the information of amplitude only of the largest Fourier length scales allows us to reach already an accuracy of 9% on the hydraulic and the thermal apertures. 	
1203.4763v1	http://arxiv.org/pdf/1203.4763v1	2012	Structural and electronic properties of Pb1-xCdxTe and Pb1-xMnxTe   ternary alloys	Malgorzata Bukala|Piotr Sankowski|Ryszard Buczko|Perla Kacman	  A systematic theoretical study of two PbTe-based ternary alloys, Pb1-xCdxTe and Pb1-xMnxTe, is reported. First, using ab initio methods we study the stability of the crystal structure of CdTe - PbTe solid solutions, to predict the composition for which rock-salt structure of PbTe changes into zinc-blende structure of CdTe. The dependence of the lattice parameter on Cd (Mn) content x in the mixed crystals is studied by the same methods. The obtained decrease of the lattice constant with x agrees with what is observed in both alloys. The band structures of PbTe-based ternary compounds are calculated within a tight-binding approach. To describe correctly the constituent materials new tight-binding parameterizations for PbTe and MnTe bulk crystals as well as a tight-binding description of rock-salt CdTe are proposed. For both studied ternary alloys, the calculated band gap in the L point increases with x, in qualitative agreement with photoluminescence measurements in the infrared. The results show also that in p-type Pb1-xCdxTe and Pb1-xMnxTe mixed crystals an enhancement of thermoelectrical power can be expected. 	
1203.6671v2	http://arxiv.org/pdf/1203.6671v2	2012	Von-Neumann's and related scaling laws in Rock-Paper-Scissors type   models	P. P. Avelino|D. Bazeia|L. Losano|J. Menezes	  We introduce a family of Rock-Paper-Scissors type models with $Z_N$ symmetry ($N$ is the number of species) and we show that it has a very rich structure with many completely different phases. We study realizations which lead to the formation of domains, where individuals of one or more species coexist, separated by interfaces whose (average) dynamics is curvature driven. This type of behavior, which might be relevant for the development of biological complexity, leads to an interface network evolution and pattern formation similar to the ones of several other nonlinear systems in condensed matter and cosmology. 	
1205.6078v2	http://arxiv.org/pdf/1205.6078v2	2012	Junctions and spiral patterns in Rock-Paper-Scissors type models	P. P. Avelino|D. Bazeia|L. Losano|J. Menezes|B. F. Oliveira	  We investigate the population dynamics in generalized Rock-Paper-Scissors models with an arbitrary number of species $N$. We show, for the first time, that spiral patterns with $N$-arms may develop both for odd and even $N$, in particular in models where a bidirectional predation interaction of equal strength between all species is modified to include one N-cyclic predator-prey rule. While the former case gives rise to an interface network with Y-type junctions obeying the scaling law $L \propto t^{1/2}$, where $L$ is the characteristic length of the network and $t$ is the time, the later can lead to a population network with $N$-armed spiral patterns, having a roughly constant characteristic length scale. We explicitly demonstrate the connection between interface junctions and spiral patterns in these models and compute the corresponding scaling laws. This work significantly extends the results of previous studies of population dynamics and could have profound implications for the understanding of biological complexity in systems with a large number of species. 	
1206.3010v2	http://arxiv.org/pdf/1206.3010v2	2013	Conjecture on imminent earthquake prediction --- from shaving foam to   cloud patterns	Xin Liu	  A conjecture on imminent earthquake prediction is presented. Drastic geological deformations of crustal rock strata taking place immediately (hours/days) before an earthquake may cause fast air or gas emission/absorption vertically in between ground and sky. I conjecture, inspired by an observation of strange patterns appearing on shaving foam, that this fast movement of air fluid may produce unusual cloud patterns at interfaces between atmosphere levels. This air movement is vertical and drastic, different from the horizontal and moderate meteorological air movement, hence its caused cloud patterns are expected to be different from meteorological cloud patterns. This provides a possible origin for the so-called earthquake cloud. Recognition of different earthquake cloud patterns may provide a practical way to estimate location, magnitude and strength of geological deformations of rock strata, and hence a method with support of physics for imminent earthquake prediction. In the end of this paper an experiment has been designed to test the conjecture. 	
1206.3620v2	http://arxiv.org/pdf/1206.3620v2	2013	Hopf algebras and Markov chains: Two examples and a theory	Persi Diaconis|C. Y. Amy Pang|Arun Ram	  The operation of squaring (coproduct followed by product) in a combinatorial Hopf algebra is shown to induce a Markov chain in natural bases. Chains constructed in this way include widely studied methods of card shuffling, a natural "rock-breaking" process, and Markov chains on simplicial complexes. Many of these chains can be explictly diagonalized using the primitive elements of the algebra and the combinatorics of the free Lie algebra. For card shuffling, this gives an explicit description of the eigenvectors. For rock-breaking, an explicit description of the quasi-stationary distribution and sharp rates to absorption follow. 	
1210.7019v1	http://arxiv.org/pdf/1210.7019v1	2012	Labyrinthine clustering in a spatial rock-paper-scissors ecosystem	Jeppe Juul|Kim Sneppen|Joachim Mathiesen	  The spatial rock-paper-scissors ecosystem, where three species interact cyclically, is a model example of how spatial structure can maintain biodiversity. We here consider such a system for a broad range of interaction rates. When one species grows very slowly, this species and its prey dominate the system by self-organizing into a labyrinthine configuration in which the third species propagates. The cluster size distributions of the two dominating species have heavy tails and the configuration is stabilized through a complex, spatial feedback loop. We introduce a new statistical measure that quantifies the amount of clustering in the spatial system by comparison with its mean field approximation. Hereby, we are able to quantitatively explain how the labyrinthine configuration slows down the dynamics and stabilizes the system. 	
1212.3098v1	http://arxiv.org/pdf/1212.3098v1	2012	Spatial Pattern Dynamics due to the Fitness Gradient Flux in   Evolutionary Games	Russ deForest|Andrew Belmonte	  We introduce a non-diffusive spatial coupling term into the replicator equation of evolutionary game theory. The spatial flux is based on motion due to local gradients in the relative fitness of each strategy, providing a game-dependent alternative to diffusive coupling. We study numerically the development of patterns in 1D for two-strategy games including the coordination game and the prisoner's dilemma, and in 2D for the rock-paper-scissors game. In 1D we observe modified travelling wave solutions in the presence of diffusion, and asymptotic attracting states under a frozen strategy assumption without diffusion. In 2D we observe spiral formation and breakup in the frozen strategy rock-paper-scissors game without diffusion. A change of variables appropriate to replicator dynamics is shown to correctly capture the 1D asymptotic steady state via a nonlinear diffusion equation. 	
1302.2582v1	http://arxiv.org/pdf/1302.2582v1	2013	Rapid Depresurizations: Can they lead to irreversible damage?	Pierre Berest|Hippolyte Djakeun-Djizanne|Benoît Brouard|Grégoire Hévin	  Rapid gas depressurization leads to gas cooling that is followed by slow gas warming when the cavern is kept idle. The decrease in the temperature of gas depends upon the relative withdrawal rate (in %/day), and cavern size and shape. Gas cooling may result in the onset of tensile stresses at cavern walls and roofs that may generate fractures or cracks. However, in most cases, the depth of penetration of these fractures is small, and they are perpendicular to the cavern wall. The distance between two parallel fractures becomes larger when fractures penetrate deeper into the rock mass, as some fractures stop growing. Fractures form a polygonal pattern. Salt slabs are created, with boundaries formed by the opened fractures. As long as the depth of penetration of the fracture remains small, these slabs remain strongly bonded to the rock mass, and it is believed that, in many cases, their weights are not large enough to allow them to break off the cavern wall. 	
1308.1288v1	http://arxiv.org/pdf/1308.1288v1	2013	Strain engineering of topological properties in lead-salt semiconductors	Paolo Barone|Domenico Di Sante|Silvia Picozzi	  Rock-salt chalcogenide SnTe represents the simplest realization of a topological insulator where a crystal symmetry allows for the appearence of topologically protected metallic states with an even number of Dirac cones on high-symmetry crystal surfaces. Related rock-salt lead chalcogenides have been predicted as well to undergo a phase-transition to a topological crystalline insulating phase after band inversion induced by alloying and pressure. Here we theoretically predict that strain, as realized in thin films grown on (001) substrates, may induce such topological phase-transitions. Furthermore, relevant topological properties of the surface states, such as the location of the Dirac cones on the surface Brillouin zone or the decay length of edge states, appear to be tunable with strain, with potential implications for technological devices benefiting from those additional degrees of freedom. 	
1308.5657v1	http://arxiv.org/pdf/1308.5657v1	2013	Rock-salt SnS and SnSe: Native Topological Crystalline Insulators	Yan Sun|Zhicheng Zhong|Tomonori Shirakawa|Cesare Franchini|Dianzhong Li|Yiyi Li|Seiji Yunoki|Xing-Qiu Chen	  Unlike time-reversal topological insulators, surface metallic states with Dirac cone dispersion in the recently discovered topological crystalline insulators (TCIs) are protected by crystal symmetry. To date, TCI behaviors have been observed in SnTe and the related alloys Pb$_{1-x}$Sn$_{x}$Se/Te, which incorporate heavy elements with large spin-orbit coupling (SOC). Here, by combining first-principles and {\it ab initio} tight-binding calculations, we report the formation of a TCI in the relatively lighter rock-salt SnS and SnSe. This TCI is characterized by an even number of Dirac cones at the high-symmetry (001), (110) and (111) surfaces, which are protected by the reflection symmetry with respect to the ($\bar{1}$10) mirror plane. We find that both SnS and SnSe have an intrinsically inverted band structure and the SOC is necessary only to open the bulk band gap. The bulk band gap evolution upon volume expansion reveals a topological transition from an ambient pressure TCI to a topologically trivial insulator. Our results indicate that the SOC alone is not sufficient to drive the topological transition. 	
1311.0144v1	http://arxiv.org/pdf/1311.0144v1	2013	Some Notes on the Rapanui Archaeoastronomy	Sergei Rjabchikov	  This paper is dedicated to the research of secrets of Easter Island (Rapa Nui), a remote plot of land in the Pacific; the work includes not only necessary ethnological data, but also some results on the archaeoastronomy. The analysis of several rock drawings lets us date them. The priests Hina Mango and Rahu (Rahi) were not only experts on the script, but also great astronomers. There is abundant evidence that the priests-astronomers used the astrolabe in their studies. The local astronomical terminology has been decoded. The observatory at the ceremonial platform Ahu Tongariki has been investigated carefully. The orientation of a female statue on the slope of the Rano Raraku volcano allows us to suggest that it was an image of the Moon (the moon goddess). A number of additional astronomical and calendar records in the rock art and in the writing have been deciphered. 	
1311.0770v1	http://arxiv.org/pdf/1311.0770v1	2013	Comment on "Thermodynamic properties of rock-salt ZnO" by Leitner et al.   [Thermochimica Acta 572 (2013) 1-5]	Petr S. Sokolov|Oleksandr O. Kurakevych|Andrey N. Baranov|Vladimir L. Solozhenko	  Very recently Leitner et al. [Thermochimica Acta 572 (2013) 1-5] have tried to extract the thermodynamic data of rock-salt ZnO from ab initio and experimental data available in the literature. In this Comment we show that neglecting (i) the strongly pronounced kinetic features of the pressure-induced phase transition in ZnO at room temperature and (ii) results of calorimetric measurements available in the literature [Russ. Chem. Bull. 59 (2010) 325-328] makes the proposed set of thermodynamic functions completely incorrect. 	
1311.6532v2	http://arxiv.org/pdf/1311.6532v2	2013	Epitaxial Growth and Electronic Structure of a Layered Zinc Pnictide   Semiconductor, beta-BaZn2As2	Zewen Xiao|Fan-Yong Ran|Hidenori Hiramatsu|Satoru Matsuishi|Hideo Hosono|Toshio Kamiya	  BaZn2As2 is expected for a good p-type semiconductor and has two crystalline phases of an orthorhombic alpha phase and a higher-symmetry tetragonal beta phase. Here, we report high-quality epitaxial films of the tetragonal beta-BaZn2As2 were grown on single-crystal MgO (001) substrates by a reactive solid-phase epitaxy technique. Out-of-plane and in-plane epitaxial relationships between the film and the substrate were BaZn2As2 (00l)//MgO (001) and BaZn2As2 [200]//MgO [200], respectively. The full-widths at half maximum were 0.082o for a 008 out-of-plane rocking curve and 0.342o for a 200 in-plane rocking curve. A step-and-terrace structure was observed by atomic force microscopy. The band gap of beta-BaZn2As2 was evaluated to be around 0.2 eV, which is much smaller than that of a family compound LaZnOAs (1.5 eV). Density functional theory calculation using the Heyd-Scuseria-Ernzerhof hybrid functionals supports the small band gap. 	
1311.6574v2	http://arxiv.org/pdf/1311.6574v2	2014	How river rocks round: resolving the shape-size paradox	G. Domokos|D. J. Jerolmack|A. Á. Sipos|Á. Török	  River-bed sediments display two universal downstream trends: fining, in which particle size decreases; and rounding, where pebble shapes evolve toward ellipsoids. Rounding is known to result from transport-induced abrasion; however many researchers argue that the contribution of abrasion to downstream fining is negligible. This presents a paradox: downstream shape change indicates substantial abrasion, while size change apparently rules it out. Here we use laboratory experiments and numerical modeling to show quantitatively that pebble abrasion is a curvature-driven flow problem. As a consequence, abrasion occurs in two well-separated phases: first, pebble edges rapidly round without any change in axis dimensions until the shape becomes entirely convex; and second, axis dimensions are then slowly reduced while the particle remains convex. Explicit study of pebble shape evolution helps resolve the shape-size paradox by reconciling discrepancies between laboratory and field studies, and enhances our ability to decipher the transport history of a river rock. 	
1402.1150v1	http://arxiv.org/pdf/1402.1150v1	2014	Hydrothermal formation of Clay-Carbonate alteration assemblages in the   Nili Fossae region of Mars	Adrian J. Brown|Simon J. Hook|Alice M. Baldridge|James K. Crowley|Nathan T. Bridges|Bradley J. Thomson|Giles M. Marion|Carlos R. de Souza Filho|Janice L. Bishop	  The Compact Reconnaissance Imaging Spectrometer for Mars (CRISM) has returned observations of the Nili Fossae region indicating the presence of Mg- carbonate in small (<10km sq2), relatively bright rock units that are commonly fractured (Ehlmann et al., 2008b). We have analyzed spectra from CRISM images and used co-located HiRISE images in order to further characterize these carbonate-bearing units. We applied absorption band mapping techniques to investigate a range of possible phyllosilicate and carbonate minerals that could be present in the Nili Fossae region. We also describe a clay-carbonate hydrothermal alteration mineral assemblage in the Archean Warrawoona Group of Western Australia that is a potential Earth analog to the Nili Fossae carbonate-bearing rock units. We discuss the geological and biological implications for hydrothermal processes on Noachian Mars. 	
1403.0151v1	http://arxiv.org/pdf/1403.0151v1	2014	Improper Ferroelectricity and Piezoelectric Responses in Rhombohedral   ($A$,$A^{\prime}$)$B_2$O$_6$ Perovskite Oxides	Joshua Young|James M. Rondinelli	  High-temperature electronic materials are in constant demand as the required operational range for various industries increases. Here we design $(A,A^\prime)B_2$O$_6$ perovskite oxides with [111] ``rock salt" $A$-site cation order and predict them to be potential high-temperature piezoelectric materials. By selecting bulk perovskites which have a tendency towards only out-of-phase $B$O$_6$ rotations, we avoid possible staggered ferroelectric to paraelectric phase transitions while also retaining non-centrosymmetric crystal structures necessary for ferro- and piezoelectricity. Using density functional theory calculations, we show that (La,Pr)Al$_2$O$_6$ and (Ce,Pr)Al$_2$O$_6$ display spontaneous polarizations in their polar ground state structures; we also compute the dielectric and piezoelectric constants for each phase. Additionally, we predict the critical phase transition temperatures for each material from first-principles to demonstrate that the piezoelectric responses, which are comparable to traditional lead-free piezoelectrics, should persist to high temperature. These features make the rock salt $A$-site ordered aluminates candidates for high-temperature sensors, actuators, or other electronic devices. 	
1403.6377v1	http://arxiv.org/pdf/1403.6377v1	2014	Tectonic Activity on Pluto After the Charon-Forming Impact	Amy C. Barr|Geoffrey C. Collins	  The Pluto-Charon system, likely formed from an impact, has reached the endpoint of its tidal evolution. During its evolution into the dual-synchronous state, the equilibrium tidal figures of Pluto and Charon would have also evolved as angular momentum was transferred from Pluto's spin to Charon's orbit. The rate of tidal evolution is controlled by Pluto's interior physical and thermal state. We examine three interior models for Pluto: an undifferentiated rock/ice mixture, differentiated with ice above rock, and differentiated with an ocean. For the undifferentiated case without an ocean, the Pluto-Charon binary does not evolve to its current state unless its internal temperature $T_i>200$ K, which would likely lead to strong tidal heating, melting, and differentiation. Without an ocean, Pluto's interior temperature must be higher than 240 K for Charon to evolve on a time scale less than the age of the solar system. Further tidal heating would likely create an ocean. If New Horizons finds evidence of ancient tidally-driven tectonic activity on either body, the most likely explanation is that Pluto had an internal ocean during Charon's orbital evolution. 	
1404.6450v1	http://arxiv.org/pdf/1404.6450v1	2014	A simple and robust elastoplastic constitutive model for concrete	F. Poltronieri|A. Piccolroaz|D. Bigoni	  An elasto-plastic model for concrete, based on a recently-proposed yield surface and simple hardening laws, is formulated, implemented, numerically tested and validated against available test results. The yield surface is smooth and particularly suited to represent the behaviour of rock-like materials, such as concrete, mortar, ceramic and rock. A new class of isotropic hardening laws is proposed, which can be given both an incremental and the corresponding finite form. These laws describe a smooth transition from linear elastic to plastic behaviour, incorporating linear and nonlinear hardening, and may approach the perfectly plastic limit in the latter case. The reliability of the model is demonstrated by its capability of correctly describing the results yielded by a number of well documented triaxial tests on concrete subjected to various confinement levels. Thanks to its simplicity, the model turns out to be very robust and well suited to be used in complex design situations, as those involving dynamic loads. 	
1406.3668v2	http://arxiv.org/pdf/1406.3668v2	2014	Optimal cooperation-trap strategies for the iterated Rock-Paper-Scissors   game	Zedong Bi|Hai-Jun Zhou	  In an iterated non-cooperative game, if all the players act to maximize their individual accumulated payoff, the system as a whole usually converges to a Nash equilibrium that poorly benefits any player. Here we show that such an undesirable destiny is avoidable in an iterated Rock-Paper-Scissors (RPS) game involving two players X and Y. Player X has the option of proactively adopting a cooperation-trap strategy, which enforces complete cooperation from the rational player Y and leads to a highly beneficial as well as maximally fair situation to both players. That maximal degree of cooperation is achievable in such a competitive system with cyclic dominance of actions may stimulate creative thinking on how to resolve conflicts and enhance cooperation in human societies. 	
1407.0605v4	http://arxiv.org/pdf/1407.0605v4	2016	Experimental control of transport resonances in a coherent quantum   rocking ratchet	Christopher Grossert|Martin Leder|Sergey Denisov|Peter Hänggi|Martin Weitz	  The ratchet phenomenon is a means to get directed transport without net forces. Originally conceived to rectify stochastic motion and describe operational principles of biological motors, the ratchet effect can be used to achieve controllable coherent quantum transport. This transport is an ingredient of several perspective quantum devices including atomic chips. Here we examine coherent transport of ultra-cold atoms in a rocking quantum ratchet. This is realized by loading a rubidium atomic Bose-Einstein condensate into a periodic optical potential subjected to a biharmonic temporal drive. The achieved long-time coherence allows us to resolve resonance enhancement of the atom transport induced by avoided crossings in the Floquet spectrum of the system. By tuning the strength of the temporal modulations, we observe a bifurcation of a single resonance into a doublet. Our measurements reveal the role of interactions among Floquet eigenstates for quantum ratchet transport. 	
1408.5047v1	http://arxiv.org/pdf/1408.5047v1	2014	Multi-scale approach to invasion percolation of rock fracture networks	Ali N. Ebrahimi|Falk K. Wittel|Nuno A. M. Araújo|Hans J. Herrmann	  A multi-scale scheme for the invasion percolation of rock fracture networks with heterogeneous fracture aperture fields is proposed. Inside fractures, fluid transport is calculated on the finest scale and found to be localized in channels as a consequence of the aperture field. The channel network is characterized and reduced to a vectorized artificial channel network (ACN). Different realizations of ACNs are used to systematically calculate efficient apertures for fluid transport inside differently sized fractures as well as fracture intersection and entry properties. Typical situations in fracture networks are parameterized by fracture inclination, flow path length along the fracture and intersection lengths in the entrance and outlet zones of fractures. Using these scaling relations obtained from the finer scales, we simulate the invasion process of immiscible fluids into saturated discrete fracture networks, which were studied in previous works. 	
1410.0599v1	http://arxiv.org/pdf/1410.0599v1	2014	Geomagnetic field intensity in the middle jurassic - oligocene	A. Yu. Kurazhkovskii|N. A. Kurazhkovskaya|B. I. Klain	  The present paper summarizes results of the studies on the intensity of geomagnetic field in the (167 - 23) Ma interval by sedimentary rocks of the Russian Plate and adjacent territories. The joint analysis of the data paleointensity obtained by sedimentary and thermomagnetized (from PINT12) rocks within this temporal interval is conducted. It is shown that the changes of the paleointensity were occurred chaotically. Alternating bursts and periods of quiet regime of the geomagnetic field are typical for intermittent processes and is a characteristic of the geological interval Jurassic-beginning of Paleogene. The distributions of the paleointensity corresponding to different intervals of geologic time were investigated. It is revealed that the cumulative distribution function (CDF) of the paleointensity values is best approximated by a power function. The indices of the power functions varied depending on geologic time intervals.The analysis of the paleomagnetic data suggests that the medium in which the geomagnetic field is generated is turbulent. Turbulence in the Earth's liquid core is enhanced in the Cretaceous compared with Jurassic and Paleogene. 	
1502.03370v1	http://arxiv.org/pdf/1502.03370v1	2015	Nonlinear Dynamics of the Rock-Paper-Scissors Game with Mutations	Danielle F. P. Toupo|Steven H. Strogatz	  We analyze the replicator-mutator equations for the Rock-Paper-Scissors game. Various graph-theoretic patterns of mutation are considered, ranging from a single unidirectional mutation pathway between two of the species, to global bidirectional mutation among all the species. Our main result is that the coexistence state, in which all three species exist in equilibrium, can be destabilized by arbitrarily small mutation rates. After it loses stability, the coexistence state gives birth to a stable limit cycle solution created in a supercritical Hopf bifurcation. This attracting periodic solution exists for all the mutation patterns considered, and persists arbitrarily close to the limit of zero mutation rate and a zero-sum game. 	
1503.04131v1	http://arxiv.org/pdf/1503.04131v1	2015	Lissajous rocking ratchet	S. Platonov|B. Kästner|H. W. Schumacher|S. Kohler|S. Ludwig	  Breaking time-reversal symmetry (TRS) in the absence of a net bias can give rise to directed steady-state non-equilibrium transport phenomena such as ratchet effects. Here we present, theoretically and experimentally, the concept of a Lissajous rocking ratchet as an instrument based on breaking TRS. Our system is a semiconductor quantum dot (QD) with periodically modulated dot-lead tunnel barriers. Broken TRS gives rise to single electron tunneling current. Its direction is fully controlled by exploring frequency and phase relations between the two barrier modulations. The concept of Lissajous ratchets can be realized in a large variety of different systems, including nano-electrical, nano-electromechanical or superconducting circuits. It promises applications based on a detailed on-chip comparison of radio-frequency signals. 	
1507.00471v1	http://arxiv.org/pdf/1507.00471v1	2015	Memory and limit cycles in rock-scissors-paper	James Burridge	  When playing games in groups, it is an advantage for individuals to have accurate statistical information on the strategies of their opponents. Such information may be obtained by remembering previous interactions. We consider a rock-scissors-paper game in which agents are able to recall their last $m$ interactions, used to estimate the behaviour of their opponents. At critical memory length, a Hopf bifurcation leads to the formation of stable limit cycles. In a mixed population, agents with longer memories have an advantage, provided the system has a stable fixed point, and there is some asymmetry in the payoffs of the pure strategies. However, at a critical concentration of long memory agents, the appearance of limit cycles destroys their advantage. By introducing population dynamics that favours successful agents, we show that the system evolves toward the bifurcation point. 	
1509.08321v2	http://arxiv.org/pdf/1509.08321v2	2015	Phase Stability and Properties of Manganese Oxide Polymorphs: Assessment   and Insights from Diffusion Monte Carlo	Joshua A. Schiller|Lucas K. Wagner|Elif Ertekin	  We present an analysis of the polymorphic energy ordering and properties of the rock salt and zincblende structures of manganese oxide using fixed node diffusion Monte Carlo (DMC). Manganese oxide is a correlated, antiferromagnetic material that has proven to be challenging to model from first principles across a variety of approaches. Unlike conventional density functional theory and some hybrid functionals, fixed node diffusion Monte Carlo finds the rock salt structure to be more stable than the zincblende structure, and thus recovers the correct energy ordering. Analysis of the site-resolved charge fluctuations of the wave functions according to DMC and other electronic structure descriptions give insights into elements that are missing in other theories. While the calculated band gaps within DMC are in agreement with predictions that the zincblende polymorph has a lower band gap, the gaps themselves overestimate reported experimental values. 	
1512.00758v1	http://arxiv.org/pdf/1512.00758v1	2015	Tracing the fate of carbon and the atmospheric evolution of Mars	Renyu Hu|David M. Kass|Bethany L. Ehlmann|Yuk L. Yung	  The climate of Mars likely evolved from a warmer, wetter early state to the cold, arid current state. However, no solutions for this evolution have previously been found to satisfy the observed geological features and isotopic measurements of the atmosphere. Here we show that a family of solutions exist, invoking no missing reservoirs or loss processes. Escape of carbon via CO photodissociation and sputtering enriches heavy carbon (13C) in the Martian atmosphere, partially compensated by moderate carbonate precipitation. The current atmospheric 13C/12C and rock and soil carbonate measurements indicate an early atmosphere with a surface pressure <1 bar. Only scenarios with large amounts of carbonate formation in open lakes permit higher values up to 1.8 bar. The evolutionary scenarios are fully testable with data from the MAVEN mission and further studies of the isotopic composition of carbonate in the Martian rock record through time. 	
1605.03977v1	http://arxiv.org/pdf/1605.03977v1	2016	Large-surface-area diamond (111) crystal plates for applications in   high-heat-load wavefront-preserving x-ray crystal optics	S. Stoupin|S. Antipov|J. E. Butler|A. V. Kolyadin|A. Katrusha	  We report fabrication and results of high-resolution X-ray topography characterization of diamond single crystal plates with a large surface area (10$\times$10 mm$^2$) and (111) crystal surface orientation for applications in high-heat-load X-ray crystal optics. The plates were fabricated by laser cutting of the (111) facets of diamond crystals grown using high-pressure high-temperature method. The intrinsic crystal quality of a selected 3$\times$7~mm$^2$ crystal region of one of the studied samples was found to be suitable for applications in wavefront-preserving high-heat-load crystal optics. The wavefront characterization was performed using sequential X-ray diffraction topography in the pseudo plane wave configuration and data analysis using rocking curve topography. The variation of the rocking curve width and peak position measured with a spatial resolution of 13$\times$13 $\mu m^2$ over the selected region were found to be less than one microradian. 	
1605.07841v1	http://arxiv.org/pdf/1605.07841v1	2016	Streaming potential modeling in fractured rock: Insights into the   identification of hydraulically active fractures	D. Roubinet|N. Linde|D. Jougnot|J. Irving	  Numerous field experiments suggest that the self-potential (SP) geophysical method may allow for the detection of hydraulically active fractures and provide information about fracture properties. However, a lack of suitable numerical tools for modeling streaming potentials in fractured media prevents quantitative interpretation and limits our understanding of how the SP method can be used in this regard. To address this issue, we present a highly efficient two-dimensional discrete-dual-porosity approach for solving the fluid flow and associated self-potential problems in fractured rock. Our approach is specifically designed for complex fracture networks that cannot be investigated using standard numerical methods. We then simulate SP signals associated with pumping conditions for a number of examples to show that (i) accounting for matrix fluid flow is essential for accurate SP modeling and (ii) the sensitivity of SP to hydraulically active fractures is intimately linked with fracture-matrix fluid interactions. This implies that fractures associated with strong SP amplitudes are likely to be hydraulically conductive, attracting fluid flow from the surrounding matrix. 	
1606.03065v1	http://arxiv.org/pdf/1606.03065v1	2016	Impact of Rocks and Minerals on Underground Magneto-Inductive   Communication and Localization	Traian E. Abrudan|Orfeas Kypris|Niki Trigoni|Andrew Markham	  In this paper, we analyze the effect of different underground materials on very-low and low frequency magnetic fields used in the contexts of magneto-inductive localization and communication applications, respectively. We calculate the attenuation that these magnetic fields are subject to while passing through most common rocks and minerals. Knowing the attenuation properties is crucial in the design of underground magneto-inductive communication systems. In addition, we provide means to predict the distortions in the magnetic field that impair localization systems. The proposed work offers basic design guidelines for communication and localization systems in terms of channel path-loss, operation frequencies and bandwidth. For the sake of the reproducibility of the results, we provide the raw data and processing source code to be used by the two research communities. 	
1606.05765v2	http://arxiv.org/pdf/1606.05765v2	2016	Simulation of Deformation and Flow in Fractured, Poroelastic Materials	Katja K. Hanowski|Oliver Sander	  We introduce a coupled system of PDEs for the modeling of the fluid-fluid and fluid-solid interaction in a poroelastic material with a single static fracture. The fluid flow in the fracture is modeled by a lower-dimensional Darcy equation, which interacts with the surrounding rock matrix and the fluid it contains. We explicitly allow the fracture to end within the domain, and the fracture width is an unknown of the problem. The resulting weak problem is nonlinear, elliptic and symmetric, and can be given the structure of a fixed-point problem. We show that the coupled fluid-fluid problem has a solution in a specially crafted Sobolev space, even though the fracture width cannot be bounded away from zero near the crack tip.   For numerical simulations, we combine XFEM discretizations for the rock matrix deformation and pore pressure with a standard lower-dimensional finite element method for the fracture flow problem. The resulting coupled discrete system consists of linear subdomain problems coupled by nonlinear coupling conditions. We solve the coupled system with a substructuring solver and observe very fast convergence. We also observe optimal mesh dependence of the discretization errors even in the presence of crack tips. 	
1606.08559v1	http://arxiv.org/pdf/1606.08559v1	2016	Limit of zT enhancement in rock-salt structured chalcogenides by band   convergence?	Min Hong|Zhi-Gang Chen|Yanzhong Pei|Lei Yang|Jin Zou	  Rock-salt structured chalcogenides, such as PbTe, PbSe, and SnTe, are the top candidates for mid-temperature thermoelectric applications, and their p-type thermoelectric efficiencies can be enhanced via aligning the valence bands. Here, we provided comprehensive numerical investigations on the effects of band convergence on electronic properties. We found that the extra valance band can indeed significantly enhance the power factor. Nevertheless, the extra valance band can also increase the electronic thermal conductivity, which partially offsets the enhanced power factor for the overall figure-of-merit. Finally, we predicted that the maximum figure-of-merit for PbTe, PbSe, and SnTe can reach to 2.2, 1.8, and 1.6, re-spectively, without relying on the reduction in lattice thermal conductivity. 	
1607.01679v2	http://arxiv.org/pdf/1607.01679v2	2017	On a method for Rock Classification using Textural Features and Genetic   Optimization	Manuel Blanco Valentin|Clecio Roque De Bom|Marcio Portes de Albuquerque|Marcelo Portes de Albuquerque|Elisangela Faria|Maury Duarte Correia|Rodrigo Surmas	  In this work we present a method to classify a set of rock textures based on a Spectral Analysis and the extraction of the texture Features of the resulted images. Up to 520 features were tested using 4 different filters and all 31 different combinations were verified. The classification process relies on a Naive Bayes classifier. We performed two kinds of optimizations: statistical optimization with covariance-based Principal Component Analysis (PCA) and a genetic optimization, for 10,000 randomly defined samples, achieving a final maximum classification success of 91% against the original 70% success ratio (without any optimization nor filters used). After the optimization 9 types of features emerged as most relevant. 	
1608.06230v2	http://arxiv.org/pdf/1608.06230v2	2017	On the existence, uniqueness and regularity of solutions of a   viscoelastic Stokes problem modelling salt rocks	R. A. Cipolatti|I. -S. Liu|L. A. Palermo|M. A. Rincon|R. M. S. Rosa	  A Stokes-type problem for a viscoelastic model of salt rocks is considered, and existence, uniqueness and regularity are investigated in the scale of $L^2$-based Sobolev spaces. The system is transformed into a generalized Stokes problem, and the proper conditions on the parameters of the model that guarantee that the system is uniformly elliptic are given. Under those conditions, existence, uniqueness and low-order regularity are obtained under classical regularity conditions on the data, while higher-order regularity is proved under less stringent conditions than classical ones. Explicit estimates for the solution in terms of the data are given accordingly. 	
1612.01479v1	http://arxiv.org/pdf/1612.01479v1	2016	Authoring image decompositions with generative models	Jason Rock|Theerasit Issaranon|Aditya Deshpande|David Forsyth	  We show how to extend traditional intrinsic image decompositions to incorporate further layers above albedo and shading. It is hard to obtain data to learn a multi-layer decomposition. Instead, we can learn to decompose an image into layers that are "like this" by authoring generative models for each layer using proxy examples that capture the Platonic ideal (Mondrian images for albedo; rendered 3D primitives for shading; material swatches for shading detail). Our method then generates image layers, one from each model, that explain the image. Our approach rests on innovation in generative models for images. We introduce a Convolutional Variational Auto Encoder (conv-VAE), a novel VAE architecture that can reconstruct high fidelity images. The approach is general, and does not require that layers admit a physical interpretation. 	
1701.00709v1	http://arxiv.org/pdf/1701.00709v1	2017	Single-hole GPR reflection imaging of solute transport in a granitic   aquifer	C. Dorn|N- Linde|T. Le Borgne|O. Bour|L. Baron	  Identifying transport pathways in fractured rock is extremely challenging as flow is often organized in a few fractures that occupy a very small portion of the rock volume. We demonstrate that saline tracer experiments combined with single-hole ground penetrating radar (GPR) reflection imaging can be used to monitor saline tracer movement within mm-aperture fractures. A dipole tracer test was performed in a granitic aquifer by injecting a saline solution in a known fracture, while repeatedly acquiring single-hole GPR sections in the pumping borehole located 6 m away. The final depth-migrated difference sections make it possible to identify consistent temporal changes over a 30 m depth interval at locations corresponding to fractures previously imaged in GPR sections acquired under natural flow and tracer-free conditions. The experiment allows determining the dominant flow paths of the injected tracer and the velocity (0.4-0.7 m/min) of the tracer front. 	
1705.03346v2	http://arxiv.org/pdf/1705.03346v2	2017	Quantitative vectorial magnetic imaging of multi domain rock forming   minerals using nitrogen-vacancy centers in diamond	E. Farchi|Y. Ebert|D. Farfurnik|G. Haim|R. Shaar|N. Bar-Gill	  Magnetization in rock samples is crucial for paleomagnetometry research, as it harbors valuable geological information on long term processes, such as tectonic movements and the formation of oceans and continents. Nevertheless, current techniques are limited in their ability to measure high spatial resolution and high-sensitivity quantitative vectorial magnetic signatures from such samples. As a result, our understanding of this magnetization is limited, specifically for the case of multi-domain samples. In this work we use a newly developed nitrogen-vacancy magnetic microscope, capable of quantitative vectorial magnetic imaging with optical resolution. We demonstrate direct imaging of the vectorial magnetic field of a single, multi-domain dendrite, as well as the measurement and calculation of the weak magnetic moments of individual grains on the micron scale. These results pave the way for future applications in paleomagnetometry, and for the fundamental understanding of magnetization in multi-domain samples. 	
1706.06093v1	http://arxiv.org/pdf/1706.06093v1	2017	Extraterrestrial sedimentary rocks on Earth	Yana Anfinogenova|John Anfinogenov|Larisa Budaeva|Dmitry Kuznetsov	  This concept article discusses the possibilities for identifying sedimentary-origin meteorites. The paper concerns (i) the macroscopic candidate for sedimentary meteorite in the epicenter of the 1908 Tunguska catastrophe; (ii) potential parent bodies for sedimentary meteorites; (iii) isotopic heterogeneity of unmixed silicate reservoirs on Mars; (iv) possible terrestrial loss or contamination in the noble gas signatures in new type meteorites that spent time in extreme weather conditions; (v) cosmogenic isotopes and shielding; and (vi) pseudo meteorites. We conclude that the list of candidate parent bodies for sedimentary meteorites includes, but is not limited by the Earth, Mars, Enceladus, Ganymede, Europa, and hypothetical planets that could exist between orbits of Mars and Jupiter in the past. A parent body for extraterrestrial sedimentary rocks on the Earth should be identified based on the entire body of evidence which is not limited solely by tests of oxygen and noble gas isotopes whose signatures may undergo terrestrial contamination and may exhibit significant heterogeneity within the parent bodies. Observed fall of cosmic body, evidence of hypervelocity impact complying with the criteria of impact structures, and the presence of fusion crust on the fragments should be considered as priority signs of meteoritic origin. 	
1708.08568v1	http://arxiv.org/pdf/1708.08568v1	2017	How directional mobility affects biodiversity in rock-paper-scissors   models	P. P. Avelino|D. Bazeia|L. Losano|J. Menezes|B. F. de Oliveira|M. A. Santos	  This work deals with a system of three distinct species that changes in time under the presence of mobility, reproduction and predation, as in the popular rock-paper-scissors game. The novelty of the current study is the modification of the mobility rule to the case of directional mobility in which the species move following the direction associated to a larger (averaged) number density of preys in the surrounding neighborhood. Directional mobility can be used to simulate eyes that see or a nose that smells, and we show how it may contribute to limit biodiversity. 	
1710.00189v1	http://arxiv.org/pdf/1710.00189v1	2017	Unsupervised Classification of Intrusive Igneous Rock Thin Section   Images using Edge Detection and Colour Analysis	S. Joseph|H. Ujir|I. Hipiny	  Classification of rocks is one of the fundamental tasks in a geological study. The process requires a human expert to examine sampled thin section images under a microscope. In this study, we propose a method that uses microscope automation, digital image acquisition, edge detection and colour analysis (histogram). We collected 60 digital images from 20 standard thin sections using a digital camera mounted on a conventional microscope. Each image is partitioned into a finite number of cells that form a grid structure. Edge and colour profile of pixels inside each cell determine its classification. The individual cells then determine the thin section image classification via a majority voting scheme. Our method yielded successful results as high as 90% to 100% precision. 	
1710.11249v1	http://arxiv.org/pdf/1710.11249v1	2017	Rock-Paper-Scissors, Differential Games and Biological Diversity	Tung Mai|Ioannis Panageas|Will Ratcliff|Vijay V. Vazirani|Peter Yunker	  We model a situation in which a collection of species derive their fitnesses via a rock-paper-scissors-type game; however, the precise payoffs are a function of the environment. The new aspect of our model lies in adding a feedback loop: the environment changes according to the relative fitnesses of the species; in particular, it gives a boost to the species having small populations. We cast our model in the setting of a differential game and we show that for a certain setting of parameters, this dynamics cycles. Our model is a natural one, since depletion of resources used by more frequent species will shift the payoff matrix towards favoring less frequent ones. Since the dynamics cycles, no species goes extinct and diversity is maintained. 	
1711.06956v1	http://arxiv.org/pdf/1711.06956v1	2017	On the elastic anatomy of heterogeneous fractures in rock	Fatemeh Pourahmadian|Bojan B. Guzina	  This study examines the feasibility of the full-field ultrasonic characterization of fractures in rock. To this end, a slab-like specimen of granite is subjected to in-plane, O(10$^4$Hz) excitation while monitoring the induced 2D wavefield by a Scanning Laser Doppler Vibrometer (SLDV) with sub-centimeter spatial resolution. Upon suitable filtering and interpolation, the observed wavefield is verified to conform with the plane-stress approximation and used to: (i) compute the maps of elastic modulus in the specimen (before and after fracturing) via a rudimentary application of the principle of elastography; (ii) reconstruct the fracture geometry; (iii) expose the fracture's primal (traction-displacement jump) contact behavior, and (iv) identify its profiles of shear and normal specific stiffness. Through the use of full-field ultrasonic data, the approach provides an unobscured, high-resolution insight into the fracture's contact behavior, foreshadowing in-depth laboratory exploration of interdependencies between the fracture geometry, aperture, interphase properties, and its seismic characteristics. 	
1711.07779v1	http://arxiv.org/pdf/1711.07779v1	2017	Reaction-infiltration instability in a compacting porous medium	David W. Rees Jones|Richard F. Katz	  Geological features have been interpreted as evidence of channelized magma flow in the mantle, which is a compacting porous medium. Aharonov et al. (1995) developed a simple model of reactive porous flow and numerically analysed its instability to channels in a limited parameter space. The instability relies on magma advection against a chemical solubility gradient and the porosity-dependent permeability of the porous host rock. We extend the previous analysis by systematically mapping out the parameter space. Crucially, we augment numerical solutions with asymptotic analysis to better understand the physical controls on the instability. We derive scalings for critical conditions of the instability, and also for the wavelength and growth rate of the channel structures that emerge. We obtain quantitative theories and a physical understanding of: first, how advection or diffusion over the reactive time scale set the horizontal length scale of channels; second, the role of viscous compaction of the host rock. These scalings allow us to derive estimates of the spacing of emergent channels that can be compared to the geologic record across all possible parameter regimes. 	
1801.04351v1	http://arxiv.org/pdf/1801.04351v1	2018	3D Cosmic Ray Muon Tomography from an Underground Tunnel	Elena Guardincerri|Charlotte Rowe|Emily Schultz-Fellenz|Mousumi Roy|Nicolas George|Christopher Morris|Jeffrey Bacon|Matthew Durham|Deborah Morley|Kenie Plaud-Ramos|Daniel Poulson|Alain Bonneville|Richard Kouzes	  We present an underground cosmic ray muon tomographic experiment imaging 3D density of overburden, part of a joint study with differential gravity. Muon data were acquired at four locations within a tunnel beneath Los Alamos, New Mexico, and used in a 3D tomographic inversion to recover the spatial variation in the overlying rock-air interface, and compared with a priori knowledge of the topography. Densities obtained exhibit good agreement with preliminary results of the gravity modeling, which will be presented elsewhere, and are compatible with values reported in the literature. The modeled rock-air interface matches that obtained from LIDAR within 4 m, our resolution, over much of the model volume. This experiment demonstrates the power of cosmic ray muons to image shallow geological targets using underground detectors, whose development as borehole devices will be an important new direction of passive geophysical imaging. 	
1401.0064v1	http://arxiv.org/pdf/1401.0064v1	2013	Linearity of quantum probability measure and Hardy's model	Kazuo Fujikawa|C. H. Oh|Chengjie Zhang	  We re-examine d=4 hidden-variables-models for a system of two spin-$1/2$ particles in view of the concrete model of Hardy, who analyzed the criterion of entanglement without referring to inequality. The basis of our analysis is the linearity of the probability measure related to the Born probability interpretation, which excludes non-contextual hidden-variables models in $d\geq 3$. To be specific, we note the inconsistency of the non-contextual hidden-variables model in $d=4$ with the linearity of the quantum mechanical probability measure in the sense $\langle\psi|{\bf a}\cdot {\bf \sigma}\otimes{\bf b}\cdot {\bf \sigma}|\psi\rangle+\langle\psi|{\bf a}\cdot {\bf \sigma}\otimes{\bf b}^{\prime}\cdot {\bf \sigma}|\psi\rangle=\langle\psi|{\bf a}\cdot {\bf \sigma}\otimes ({\bf b}+{\bf b}^{\prime})\cdot {\bf \sigma}|\psi\rangle$ for non-collinear ${\bf b}$ and ${\bf b}^{\prime}$. It is then shown that Hardy's model in $d=4$ does not lead to a unique mathematical expression in the demonstration of the discrepancy of local realism (hidden-variables model) with entanglement and thus his proof is incomplete. We identify the origin of this non-uniqueness with the non-uniqueness of translating quantum mechanical expressions into expressions in hidden-variables models, which results from the failure of the above linearity of the probability measure. In contrast, if the linearity of the probability measure is strictly imposed, which is tantamount to asking that the non-contextual hidden-variables model in $d=4$ gives the CHSH inequality $|\langle B\rangle|\leq 2$ uniquely, it is shown that the hidden-variables model can describe only separable quantum mechanical states; this conclusion is in perfect agreement with the so-called Gisin's theorem which states that $|\langle B\rangle|\leq 2$ implies separable states. 	
1603.05841v2	http://arxiv.org/pdf/1603.05841v2	2016	Competing damage mechanisms in a two-phase microstructure: how   microstructure and loading conditions determine the onset of fracture	T. W. J. de Geus|R. H. J. Peerlings|M. G. D. Geers	  This paper studies the competition of fracture initiation in the ductile soft phase and in the comparatively brittle hard phase in the microstructure of a two-phase material. A simple microstructural model is used to predict macroscopic fracture initiation. The simplicity of the model ensures highly efficient computations, enabling an comprehensive study: a large range of hard phase volume fractions and yield stress ratios, for wide range of applied stress states. Each combination of these parameters is analyzed using a large set of (random) microstructures. It is observed that only one of the phases dominates macroscopic fracture initiation: at low stress triaxiality the soft phase is dominant, but above a critical triaxiality the hard phase takes over resulting in a strong decrease in ductility. This transition is strongly dependent on microstructural parameters. If the hard phase volume fraction is small, the fracture initiation is dominated by the soft phase even at high phase contrast. At higher hard phase volume fraction, the hard phase dominates already at low phase contrast. This simple model thereby reconciles experimental observations from the literature for a specific combination of parameters, which may have triggered contradictory statements in the past. A microscopic analysis reveals that the average phase distribution around fracture initiation sites is nearly the same for the two failure mechanisms. Along the tensile direction, regions of the hard phase are found directly next to the fracture initiation site. This `band' of hard phase is intersected through the fracture initiation site by `bands' of the soft phase aligned with shear. Clearly, the local mechanical incompatibility is dominant for the initiation of fracture, regardless whether fracture initiates in the soft or in the hard phase. 	
0106558v1	http://arxiv.org/pdf/cond-mat/0106558v1	2001	Heterogeneous Interfacial Failure between Two Elastic Blocks	G. George Batrouni|Alex Hansen|Jean Schmittbuhl	  We investigate numerically the failure process when two elastic media, one hard and one soft that have been glued together thus forming a common interface, are pulled apart. We present three main results: (1) The area distribution of simultaneously failing glue (bursts) follows a power law consistent with the theoretically expected exponent 2.5, (2) the maximum load and displacement before catastrophic failure scale as L^2 and L^0 respectively, where L is the linear size of the system, and (3) the area distribution of failed glue regions (clusters) is a power law with exponent -1.6 when the system fails catstrophically. 	
0203476v1	http://arxiv.org/pdf/cond-mat/0203476v1	2002	Calculation of the incremental stress-strain relation of a polygonal   packing	F. Alonso-Marroquin|H. J. Herrmann	  The constitutive relation of the quasi-static deformation on two dimensional packed samples of polygons is calculated using molecular dynamic simulations. The stress values at which the system remains stable are bounded by a failure surface, that shows a power law dependence on the pressure. Below the failure surface, non linear elasticity and plastic deformation are obtained, which are evaluated in the framework of the incremental linear theory. The results shows that the stiffness tensor can be directly related to the micro-contact rearrangements. The plasticity obeys a non-associated flow rule, with a plastic limit surface that does not agree with the failure surface. 	
0405096v1	http://arxiv.org/pdf/cond-mat/0405096v1	2004	Universal Breakdown of Elasticity at the Onset of Material Failure	Craig Maloney|Anaël Lemaître	  We show that, in the athermal quasi-static deformation of amorphous materials, the onset of failure is accompanied by universal scalings associated with a \emph{divergence} of elastic constants. A normal mode analysis of the non-affine elastic displacement field allows us to clarify its relation to the zero-frequency mode at the onset of failure and to the crack-like pattern which results from the subsequent relaxation of energy. 	
0003056v1	http://arxiv.org/pdf/cs/0003056v1	2000	A note on the Declarative reading(s) of Logic Programming	Marc Denecker	  This paper analyses the declarative readings of logic programming. Logic programming - and negation as failure - has no unique declarative reading. One common view is that logic programming is a logic for default reasoning, a sub-formalism of default logic or autoepistemic logic. In this view, negation as failure is a modal operator. In an alternative view, a logic program is interpreted as a definition. In this view, negation as failure is classical objective negation. From a commonsense point of view, there is definitely a difference between these views. Surprisingly though, both types of declarative readings lead to grosso modo the same model semantics. This note investigates the causes for this. 	
0302029v1	http://arxiv.org/pdf/nlin/0302029v1	2003	Noise-induced failures of chaos stabilization: large fluctuations and   their control	I. A. Khovanov|N. A. Khovanova|P. V. E. McClintock	  Noise-induced failures in the stabilization of an unstable orbit in the one-dimensional logistic map are considered as large fluctuations from a stable state. The properties of the large fluctuations are examined by determination and analysis of the optimal path and the optimal fluctuational force corresponding to the stabilization failure. The problem of controlling noise-induced large fluctuations is discussed, and methods of control have been developed. 	
0310005v2	http://arxiv.org/pdf/q-bio/0310005v2	2003	Structured psychosocial stress and therapeutic failure	Rodrick Wallace|Deborah Wallace	  Generalized language-of-thought arguments appropriate to interacting cognitive modules permit exploration of how disease states interact with medical treatment. The interpenetrating feedback between treatment and response to it creates a kind of idiotypic hall-of-mirrors generating a synergistic pattern of efficacy, treatment failure, adverse reactions, and patient noncompliance which, from a Rate Distortion perspective, embodies a distorted image of externally-imposed structured psychosocial stress. For the US, accelerating spatial and social diffusion of such stress enmeshes both dominant and subordinate populations in a linked system which will express itself, not only in an increasingly unhealthy society, but in the diffusion of therapeutic failure, including, but not limited to, drug-based treatments. 	
0701090v1	http://arxiv.org/pdf/quant-ph/0701090v1	2007	Error propagation in loss- and failure-tolerant quantum computation   schemes	Peter P. Rohde|Timothy C. Ralph|William J. Munro	  Qubit loss and gate failure are significant obstacles for the implementation of scalable quantum computation. Recently there have been several proposals for overcoming these problems, including schemes based on parity and cluster states. While effective at dealing with loss and gate failure, these schemes typically lead to a blow-out in effective depolarizing noise rates. In this supplementary paper we present a detailed analysis of this problem and techniques for minimizing it. 	
0704.0345v1	http://arxiv.org/pdf/0704.0345v1	2007	A High Robustness and Low Cost Model for Cascading Failures	Bing Wang|Beom Jun Kim	  We study numerically the cascading failure problem by using artificially created scale-free networks and the real network structure of the power grid. The capacity for a vertex is assigned as a monotonically increasing function of the load (or the betweenness centrality). Through the use of a simple functional form with two free parameters, revealed is that it is indeed possible to make networks more robust while spending less cost. We suggest that our method to prevent cascade by protecting less vertices is particularly important for the design of more robust real-world networks to cascading failures. 	
0707.1622v1	http://arxiv.org/pdf/0707.1622v1	2007	Dynamic Failure in Amorphous Solids via a Cavitation Instability	Eran Bouchbinder|Ting-Shek Lo|Itamar Procaccia	  The understanding of dynamic failure in amorphous materials via the propagation of free boundaries like cracks and voids must go beyond elasticity theory, since plasticity intervenes in a crucial and poorly understood manner near the moving free boundary. In this Letter we focus on failure via a cavitation instability in a radially-symmetric stressed material, set up the free boundary dynamics taking both elasticity and visco-plasticity into account, using the recently proposed athermal Shear Transformation Zone theory. We demonstrate the existence (in amorphous systems) of fast cavitation modes accompanied by extensive plastic deformations and discuss the revealed physics. 	
0806.3558v2	http://arxiv.org/pdf/0806.3558v2	2009	Failure of Local Realism Revealed by Extremely Coarse-Grained   Measurements	H. Jeong|M. Paternostro|T. C. Ralph	  We show that failure of local realism can be revealed to observers for whom only extremely coarse-grained measurements are available. In our instances, Bell's inequality is violated even up to the maximum limit while both the local measurements and the initial local states under scrutiny approach the classical limit. Furthermore, we can observe failure of local realism when an inequality enforced by non-local realistic theories is satisfied. This suggests that locality alone may be violated while realism cannot be excluded for specific observables and states. Small-scale experimental demonstration of our examples may be possible in the foreseeable future. 	
0809.0279v1	http://arxiv.org/pdf/0809.0279v1	2008	Competing risks within shock models	Antonio Di Crescenzo|Maria Longobardi	  We consider a competing risks model, in which system failures are due to one out of two mutually exclusive causes, formulated within the framework of shock models driven by bivariate Poisson process. We obtain the failure densities and the survival functions as well as other related quantities under three different schemes. Namely, system failures are assumed to occur at the first instant in which a random constant threshold is reached by (a) the sum of received shocks, (b) the minimum of shocks, (c) the maximum of shocks. 	
0811.1693v1	http://arxiv.org/pdf/0811.1693v1	2008	Protection Schemes for Two Link Failures in Optical Networks	Salah A. Aly|Ahmed E. Kamal	  In this paper we develop network protection schemes against two link failures in optical networks. The motivation behind this work is the fact that the majority of all available links in an optical network suffer from single and double link failures. In the proposed network protection schemes, NPS2-I and NPS2-II, we deploy network coding and reduced capacity on the working paths to provide backup protection paths. In addition, we demonstrate the encoding and decoding aspects of the proposed schemes. 	
0902.3644v2	http://arxiv.org/pdf/0902.3644v2	2009	Failure of the Hasse principle for Chatelet surfaces in characteristic 2	Bianca Viray	  Given any global field k of characteristic 2, we construct a Chatelet surface over k which fails to satisfy the Hasse principle. This failure is due to a Brauer-Manin obstruction. This construction extends a result of Poonen to characteristic 2, thereby showing that the etale-Brauer obstruction is insufficient to explain all failures of the Hasse principle over a global field of any characteristic. 	
0903.0054v1	http://arxiv.org/pdf/0903.0054v1	2009	Considerations on Resource Usage in Exceptions and Failures in Workflows	Alexandra Fortis|Alexandru Cicortas|Victoria Iordan	  The paper presents a description of some point of view of different authors related to the failures and exceptions that appear in workflows, as a direct consequence of unavailability of resources involved in the workflow. Each of these interpretations is typical for a certain situation, depending on the authors' interpretation of failures and exceptions in workflows modeling real dynamical systems. 	
0903.3104v1	http://arxiv.org/pdf/0903.3104v1	2009	Piezonuclear neutrons from fracturing of inert solids	F. Cardone|A. Carpinteri|G. Lacidogna	  Neutron emission measurements by means of helium-3 neutron detectors were performed on solid test specimens during crushing failure. The materials used were marble and granite, selected in that they present a different behaviour in compression failure (i.e., a different brittleness index) and a different iron content. All the test specimens were of the same size and shape. Neutron emissions from the granite test specimens were found to be of about one order of magnitude higher than the natural background level at the time of failure. 	
1001.2077v2	http://arxiv.org/pdf/1001.2077v2	2010	On Random Linear Network Coding for Butterfly Network	Xuan Guang|Fang-Wei Fu	  Random linear network coding is a feasible encoding tool for network coding, specially for the non-coherent network, and its performance is important in theory and application. In this letter, we study the performance of random linear network coding for the well-known butterfly network by analyzing the failure probabilities. We determine the failure probabilities of random linear network coding for the well-known butterfly network and the butterfly network with channel failure probability p. 	
1004.5256v1	http://arxiv.org/pdf/1004.5256v1	2010	Construction auto-stabilisante d'arbre couvrant en dépit d'actions   malicieuses	Swan Dubois|Toshimitsu Masuzawa|Sébastien Tixeuil	  A self-stabilizing protocol provides by definition a tolerance to transient failures. Recently, a new class of self-stabilizing protocols appears. These protocols provides also a tolerance to a given number of permanent failures. In this article, we are interested in self-stabilizing protocols that deal with Byzantines failures. We prove that, for some problems which not allow strict stabilization (see [Nesterenko,Arora,2002]), there exist solutions that tolerates Byzantine faults if we define a new criteria of tolerance. 	
1007.5447v1	http://arxiv.org/pdf/1007.5447v1	2010	Politiques de Tests Partiels \& Systèmes de Sécurité	Florent Brissaud|Anne Barros|Christophe Bérenguer	  A set of general formulas is proposed for the probability of failure on demand (PFD) assessment of MooN architecture (i.e. k-out-of-n) systems subject to partial and full tests. Partial tests (e.g. visual inspections, imperfect testing) may detect only some failures, whereas owing to a full test, the system is restored to an as good as new condition. Following the proposed approach and according to an example, performance estimations of the system and test policies are presented, by using the feedback from partial and full tests. An optimization of the partial test distribution is also proposed, which allows reducing the average probability of system failure on demand (PFDavg). 	
1008.1369v1	http://arxiv.org/pdf/1008.1369v1	2010	Fully fault tolerant quantum computation with non-deterministic gates	Ying Li|Sean D. Barrett|Thomas M. Stace|Simon C. Benjamin	  In certain approaches to quantum computing the operations between qubits are non-deterministic and likely to fail. For example, a distributed quantum processor would achieve scalability by networking together many small components; operations between components should assumed to be failure prone. In the logical limit of this architecture each component contains only one qubit. Here we derive thresholds for fault tolerant quantum computation under such extreme paradigms. We find that computation is supported for remarkably high failure rates (exceeding 90%) providing that failures are heralded, meanwhile the rate of unknown errors should not exceed 2 in 10^4 operations. 	
1010.3828v2	http://arxiv.org/pdf/1010.3828v2	2010	Self-organized criticality as a precursor of fatigue: application to   shape memory alloys	C. Dunand-Chatellet|Z. Moumni	  Fatigue failure can be thought by studying the collective motions of defects inside materials instead of focusing on the growth of a pre-existing micro-crack. An experimental study of the statistical distribution of acoustic emissions avalanches along cycling is presented. The evolutions of critical exponents through cyclic driving are estimated to track changes in the dissipation modes and consequently identify fatigue failure precursors. We also use critical rupture models developed for earthquakes and stock market crashes predictions to forecast the time to failure with good reliability. 	
1106.0224v1	http://arxiv.org/pdf/1106.0224v1	2011	Reasoning about Minimal Belief and Negation as Failure	R. Rosati	  We investigate the problem of reasoning in the propositional fragment of MBNF, the logic of minimal belief and negation as failure introduced by Lifschitz, which can be considered as a unifying framework for several nonmonotonic formalisms, including default logic, autoepistemic logic, circumscription, epistemic queries, and logic programming. We characterize the complexity and provide algorithms for reasoning in propositional MBNF. In particular, we show that entailment in propositional MBNF lies at the third level of the polynomial hierarchy, hence it is harder than reasoning in all the above mentioned propositional formalisms for nonmonotonic reasoning. We also prove the exact correspondence between negation as failure in MBNF and negative introspection in Moore's autoepistemic logic. 	
1204.5724v1	http://arxiv.org/pdf/1204.5724v1	2012	Nonparametric survival analysis and vaccine efficacy using   Dempster-Shafer analysis	Paul T. Edlefsen|Arthur P. Dempster	  We introduce an extension of nonparametric DS inference for arbitrary univariate CDFs to the case in which some failure times are (right)-censored, and then apply this to the problem of assessing evidence regarding assertions about relative risks across two populations. The approach enables exploration of the sensitivity of survival analyses to assumed independence of the missing data process and the failure proces. We present an application to the partially efficacious RV144 (HIV-1) vaccine trial, and show that the strength of conclusions of vaccine efficacy depend on assumptions about the maximum failure rates of the subjects lost-to-followup. 	
1303.4051v1	http://arxiv.org/pdf/1303.4051v1	2013	The failure risk analysis of digital circuits	A. N. Pchelintsev	  To analyze the failure risk of asynchronous digital circuits the time-parameter is introduced into the Boolean algebra replacing the arithmetic operations by logical operations. There considered an example of construction of signals passing through the logical elements, using the described below mathematical apparatus. 	
1308.1968v1	http://arxiv.org/pdf/1308.1968v1	2013	Detection and Isolation of Link Failures under the Agreement Protocol	M. Amin Rahimian|Victor M. Preciado	  In this paper a property of the multi-agent consensus dynamics that relates the failure of links in the network to jump discontinuities in the derivatives of the output responses of the nodes is derived and verified analytically. At the next step, an algorithm for sensor placement is proposed, which would enable the designer to detect and isolate any link failures across the network based on the observed jump discontinuities in the derivatives of the responses of a subset of nodes. These results are explained through elaborative examples. 	
1309.5540v2	http://arxiv.org/pdf/1309.5540v2	2014	Detection and Isolation of Failures in Linear Multi-Agent Networks	M. Amin Rahimian|Victor M. Preciado	  In this paper the focus is on the relationship between the occurrence of failures in a (directed or undirected) network of linear single integrator agents and the presence of jump discontinuities in the derivatives of the network output. Based on this relationship, an algorithm for sensor placement is proposed, which enables the designer to detect and isolate any link failures across the network, based on the jump discontinuities observed by the sensor nodes. These results are explained through elaborative examples and computer experiments. 	
1405.5841v1	http://arxiv.org/pdf/1405.5841v1	2014	Parameter Estimates of General Failure Rate Model: A Bayesian Approach	Asok K. Nanda|Sudhansu S. Maiti|Chanchal Kundu|Amarjit Kundu	  The failure rate function plays an important role in studying the lifetime distributions in reliability theory and life testing models. A study of the general failure rate model $r(t)=a+bt^{\theta-1}$, under squared error loss function taking $a$ and $b$ independent exponential random variables has been analyzed in the literature. In this article, we consider $a$ and $b$ not necessarily independent. The estimates of the parameters $a$ and $b$ under squared error loss, linex loss and entropy loss functions are obtained here. 	
1509.00570v1	http://arxiv.org/pdf/1509.00570v1	2015	Nonlocal effects and counter measures in cascading failures	Dirk Witthaut|Marc Timme	  We study the propagation of cascading failures in complex supply networks with a focus on nonlocal effects occurring far away from the initial failure. It is shown that a high clustering and a small average path length of a network generally suppress nonlocal overloads. These properties are typical for many real-world networks, often called small-world networks, such that cascades propagate mostly locally in these networks. Furthermore, we analyze the spatial aspects of countermeasures based on the intentional removal of additional edges. Nonlocal actions are generally required in networks which have a low redundancy and are thus especially vulnerable to cascades. 	
1509.02746v1	http://arxiv.org/pdf/1509.02746v1	2015	Low-frequency failure of the Göppert-Mayer gauge transformation and   consequences for the Strong-Field Approximation	H. R. Reiss	  The G\"oppert-Mayer (GM) gauge transformation, of central importance in atomic, molecular, and optical physics since it connects the length gauge and the velocity gauge, becomes unphysical as the field frequency declines towards zero. This is not consequential for theories of transverse fields, but it is the underlying reason for the failure of gauge invariance in the dipole-approximation version of the Strong-Field Approximation (SFA). This failure of the GM gauge transformation explains why the length gauge is preferred in analytical approximation methods for fields that possess a constant electric field as a zero-frequency limit. 	
1509.06668v1	http://arxiv.org/pdf/1509.06668v1	2015	Efficient failure probability calculation through mesh refinement	Jing Li|Panos Stinis	  We present a novel way of accelerating hybrid surrogate methods for the calculation of failure probabilities. The main idea is to use mesh refinement in order to obtain improved local surrogates of low computation cost to simulate on. These improved surrogates can reduce significantly the required number of evaluations of the exact model (which is the usual bottleneck of failure probability calculations). Meanwhile the effort on evaluations of surrogates is dramatically reduced by utilizing low order local surrogates. Numerical results of the application of the proposed approach in several examples of increasing complexity show the robustness, versatility and gain in efficiency of the method. 	
1510.00990v1	http://arxiv.org/pdf/1510.00990v1	2015	On the Failure of BD-N and BD, and an Application to the Anti-Specker   Property	Robert Lubarsky	  We give the natural topological model for the failure of BD-N, and use it to show that the closure of spaces with the anti-Specker property under product does not imply BD-N. Also, the natural topological model for the failure of BD is presented. Finally, for some of the realizability models known indirectly to falsify BD-N, it is brought out in detail how BD-N fails. 	
1607.01540v1	http://arxiv.org/pdf/1607.01540v1	2016	Metallized Film Capacitor Lifetime Evaluation and Failure Mode Analysis	R. Gallay	  One of the main concerns for power electronic engineers regarding capacitors is to predict their remaining lifetime in order to anticipate costly failures or system unavailability. This may be achieved using a Weibull statistical law combined with acceleration factors for the temperature, the voltage, and the humidity. This paper discusses the different capacitor failure modes and their effects and consequences. 	
1610.02885v1	http://arxiv.org/pdf/1610.02885v1	2016	Hardening Cassandra Against Byzantine Failures	Roy Friedman|Roni Licher	  Cassandra is one of the most widely used distributed data stores these days. Cassandra supports flexible consistency guarantees over a wide-column data access model and provides almost linear scale-out performance. This enables application developers to tailor the performance and availability of Cassandra to their exact application's needs and required semantics. Yet, Cassandra is designed to withstand benign failures, and cannot cope with most forms of Byzantine attacks.   In this work, we present an analysis of Cassandra's vulnerabilities and propose protocols for hardening Cassandra against Byzantine failures. We examine several alternative design choices and compare between them both qualitatively and empirically by using the Yahoo! Cloud Serving Benchmark (YCSB) performance benchmark. We include incremental performance analysis for our algorithmic and cryptographic adjustments, supporting our design choices. 	
1704.05396v1	http://arxiv.org/pdf/1704.05396v1	2017	A Study of Deep Learning Robustness Against Computation Failures	Jean-Charles Vialatte|François Leduc-Primeau	  For many types of integrated circuits, accepting larger failure rates in computations can be used to improve energy efficiency. We study the performance of faulty implementations of certain deep neural networks based on pessimistic and optimistic models of the effect of hardware faults. After identifying the impact of hyperparameters such as the number of layers on robustness, we study the ability of the network to compensate for computational failures through an increase of the network size. We show that some networks can achieve equivalent performance under faulty implementations, and quantify the required increase in computational complexity. 	
0406005v3	http://arxiv.org/pdf/cs/0406005v3	2004	Microreboot -- A Technique for Cheap Recovery	George Candea|Shinichi Kawamoto|Yuichi Fujiki|Greg Friedman|Armando Fox	  A significant fraction of software failures in large-scale Internet systems are cured by rebooting, even when the exact failure causes are unknown. However, rebooting can be expensive, causing nontrivial service disruption or downtime even when clusters and failover are employed. In this work we separate process recovery from data recovery to enable microrebooting -- a fine-grain technique for surgically recovering faulty application components, without disturbing the rest of the application.   We evaluate microrebooting in an Internet auction system running on an application server. Microreboots recover most of the same failures as full reboots, but do so an order of magnitude faster and result in an order of magnitude savings in lost work. This cheap form of recovery engenders a new approach to high availability: microreboots can be employed at the slightest hint of failure, prior to node failover in multi-node clusters, even when mistakes in failure detection are likely; failure and recovery can be masked from end users through transparent call-level retries; and systems can be rejuvenated by parts, without ever being shut down. 	
0812.0972v2	http://arxiv.org/pdf/0812.0972v2	2008	Network Protection Codes: Providing Self-healing in Autonomic Networks   Using Network Coding	Salah A. Aly|Ahmed E. Kamal	  Agile recovery from link failures in autonomic communication networks is essential to increase robustness, accessibility, and reliability of data transmission. However, this must be done with the least amount of protection resources, while using simple management plane functionality. Recently, network coding has been proposed as a solution to provide agile and cost efficient network self-healing against link failures, in a manner that does not require data rerouting, packet retransmission, or failure localization, hence leading to simple control and management planes. To achieve this, separate paths have to be provisioned to carry encoded packets, hence requiring either the addition of extra links, or reserving some of the resources for this purpose.   In this paper we introduce autonomic self-healing strategies for autonomic networks in order to protect against link failures. The strategies are based on network coding and reduced capacity, which is a technique that we call network protection codes (NPC). In these strategies, an autonomic network is able to provide self-healing from various network failures affecting network operation. The techniques improve service and enhance reliability of autonomic communication.   Network protection codes are extended to provide self-healing from multiple link failures in autonomic networks. We provide implementation aspects of the proposed strategies. We present bounds and network protection code constructions. Finally, we study the construction of such codes over the binary field. The paper also develops an Integer Linear Program formulation to evaluate the cost of provisioning connections using the proposed strategies. 	
0905.2248v3	http://arxiv.org/pdf/0905.2248v3	2010	Protection against link errors and failures using network coding	Shizheng Li|Aditya Ramamoorthy	  We propose a network-coding based scheme to protect multiple bidirectional unicast connections against adversarial errors and failures in a network. The network consists of a set of bidirectional primary path connections that carry the uncoded traffic. The end nodes of the bidirectional connections are connected by a set of shared protection paths that provide the redundancy required for protection. Such protection strategies are employed in the domain of optical networks for recovery from failures. In this work we consider the problem of simultaneous protection against adversarial errors and failures.   Suppose that n_e paths are corrupted by the omniscient adversary. Under our proposed protocol, the errors can be corrected at all the end nodes with 4n_e protection paths. More generally, if there are n_e adversarial errors and n_f failures, 4n_e + 2n_f protection paths are sufficient. The number of protection paths only depends on the number of errors and failures being protected against and is independent of the number of unicast connections. 	
0907.3371v1	http://arxiv.org/pdf/0907.3371v1	2009	Robot Reliability Using Petri Nets and Fuzzy Lambda-Tau Methodology	Ajay Kumar|S. P. Sharma|Dinesh Kumar	  Robot reliability has become an increasingly important issue in the last few years due to increased application of robots in many industries (like automobile industry) under hazardous and unstructured environment. As the component failure behavior is dependent on configuration and environment, the available information about the constituent component of robots is most of the time imprecise, incomplete, vague and conflicting and so it is very difficult to analyze their behavior and to predict their failure pattern. The reliability analysis of any system provides an understanding about the likelihood of failures occurring in the system/component and the increased insight about its inherent weakness. The objective of this paper is to quantify the uncertainties that makes the decision more realistic, generic and extendable to application domain. In this paper various reliability parameters (such as mean time between failures, expected number of failures, reliability, availability etc.) are computed using Fuzzy Lambda-Tau methodology. Triangular fuzzy numbers are used to represent failure rates and repair times as they allow expert opinion, linguistic variables, operating conditions, uncertainty and imprecision in reliability information, to be incorporated into system model. Petri Nets are used because unlike the fault tree methodology, the use of Petri Nets allows efficient simultaneous generation of minimal cut and path sets. 	
1110.1842v3	http://arxiv.org/pdf/1110.1842v3	2011	Failure Detectors in Homonymous Distributed Systems (with an Application   to Consensus)	Sergio Arévalo|Antonio Fernández Anta|Damien Imbs|Ernesto Jiménez|Michel Raynal	  This paper addresses the consensus problem in homonymous distributed systems where processes are prone to crash failures and have no initial knowledge of the system membership ("homonymous" means that several processes may have the same identifier). New classes of failure detectors suited to these systems are first defined. Among them, the classes H\Omega\ and H\Sigma\ are introduced that are the homonymous counterparts of the classes \Omega\ and \Sigma, respectively. (Recall that the pair <\Omega,\Sigma> defines the weakest failure detector to solve consensus.) Then, the paper shows how H\Omega\ and H\Sigma\ can be implemented in homonymous systems without membership knowledge (under different synchrony requirements). Finally, two algorithms are presented that use these failure detectors to solve consensus in homonymous asynchronous systems where there is no initial knowledge of the membership. One algorithm solves consensus with <H\Omega,H\Sigma>, while the other uses only H\Omega, but needs a majority of correct processes.   Observe that the systems with unique identifiers and anonymous systems are extreme cases of homonymous systems from which follows that all these results also apply to these systems. Interestingly, the new failure detector class H\Omega\ can be implemented with partial synchrony, while the analogous class A\Omega\ defined for anonymous systems can not be implemented (even in synchronous systems). Hence, the paper provides us with the first proof showing that consensus can be solved in anonymous systems with only partial synchrony (and a majority of correct processes). 	
1206.0244v2	http://arxiv.org/pdf/1206.0244v2	2012	Detection Performance in Balanced Binary Relay Trees with Node and Link   Failures	Zhenliang Zhang|Edwin K. P. Chong|Ali Pezeshki|William Moran|Stephen D. Howard	  We study the distributed detection problem in the context of a balanced binary relay tree, where the leaves of the tree correspond to $N$ identical and independent sensors generating binary messages. The root of the tree is a fusion center making an overall decision. Every other node is a relay node that aggregates the messages received from its child nodes into a new message and sends it up toward the fusion center. We derive upper and lower bounds for the total error probability $P_N$ as explicit functions of $N$ in the case where nodes and links fail with certain probabilities. These characterize the asymptotic decay rate of the total error probability as $N$ goes to infinity. Naturally, this decay rate is not larger than that in the non-failure case, which is $\sqrt N$. However, we derive an explicit necessary and sufficient condition on the decay rate of the local failure probabilities $p_k$ (combination of node and link failure probabilities at each level) such that the decay rate of the total error probability in the failure case is the same as that of the non-failure case. More precisely, we show that $\log P_N^{-1}=\Theta(\sqrt N)$ if and only if $\log p_k^{-1}=\Omega(2^{k/2})$. 	
1401.4473v1	http://arxiv.org/pdf/1401.4473v1	2013	The Impact of the Topology on Cascading Failures in Electric Power Grids	Yakup Koç|Martijn Warnier|Piet Van Mieghem|Robert E. Kooij|Frances M. T. Brazier	  Cascading failures are one of the main reasons for blackouts in power transmission grids. The topology of a power grid, together with its operative state determine, for the most part, the robustness of the power grid against cascading failures. Secure electrical power supply requires, together with careful operation, a robust design of the electrical power grid topology. This paper investigates the impact of a power grid topology on its robustness against cascading failures. Currently, the impact of the topology on a grid robustness is mainly assessed by using purely topological approaches that fail to capture the essence of electric power flow. This paper proposes a metric, the effective graph resistance, that relates the topology of a power grid to its robustness against cascading failures by deliberate attacks, while also taking the fundamental characteristics of the electric power grid into account such as power flow allocation according to Kirchoff Laws. Experimental verification shows that the proposed metric anticipates the grid robustness accurately. The proposed metric is used to optimize a grid topology for a higher level of robustness. To demonstrate its applicability, the metric is applied on the IEEE 118 bus power system to improve its robustness against cascading failures. 	
1402.1780v1	http://arxiv.org/pdf/1402.1780v1	2014	Cascading Failures in Power Grids - Analysis and Algorithms	Saleh Soltan|Dorian Mazauric|Gil Zussman	  This paper focuses on cascading line failures in the transmission system of the power grid. Recent large-scale power outages demonstrated the limitations of percolation- and epid- emic-based tools in modeling cascades. Hence, we study cascades by using computational tools and a linearized power flow model. We first obtain results regarding the Moore-Penrose pseudo-inverse of the power grid admittance matrix. Based on these results, we study the impact of a single line failure on the flows on other lines. We also illustrate via simulation the impact of the distance and resistance distance on the flow increase following a failure, and discuss the difference from the epidemic models. We then study the cascade properties, considering metrics such as the distance between failures and the fraction of demand (load) satisfied after the cascade (yield). We use the pseudo-inverse of admittance matrix to develop an efficient algorithm to identify the cascading failure evolution, which can be a building block for cascade mitigation. Finally, we show that finding the set of lines whose removal has the most significant impact (under various metrics) is NP-Hard and introduce a simple heuristic for the minimum yield problem. Overall, the results demonstrate that using the resistance distance and the pseudo-inverse of admittance matrix provides important insights and can support the development of efficient algorithms. 	
1402.6809v1	http://arxiv.org/pdf/1402.6809v1	2014	Analyzing Cascading Failures in Smart Grids under Random and Targeted   Attacks	Sushmita Ruj|Arindam Pal	  We model smart grids as complex interdependent networks, and study targeted attacks on smart grids for the first time. A smart grid consists of two networks: the power network and the communication network, interconnected by edges. Occurrence of failures (attacks) in one network triggers failures in the other network, and propagates in cascades across the networks. Such cascading failures can result in disintegration of either (or both) of the networks. Earlier works considered only random failures. In practical situations, an attacker is more likely to compromise nodes selectively.   We study cascading failures in smart grids, where an attacker selectively compromises the nodes with probabilities proportional to their degrees; high degree nodes are compromised with higher probability. We mathematically analyze the sizes of the giant components of the networks under targeted attacks, and compare the results with the corresponding sizes under random attacks. We show that networks disintegrate faster for targeted attacks compared to random attacks. A targeted attack on a small fraction of high degree nodes disintegrates one or both of the networks, whereas both the networks contain giant components for random attack on the same fraction of nodes. 	
1410.3512v1	http://arxiv.org/pdf/1410.3512v1	2014	Cascading Failures in Finite-Size Random Geometric Networks	Ali Eslami|Chuan Huang|Junshan Zhang|Shuguang Cui	  The problem of cascading failures in cyber-physical systems is drawing much attention in lieu of different network models for a diverse range of applications. While many analytic results have been reported for the case of large networks, very few of them are readily applicable to finite-size networks. This paper studies cascading failures in finite-size geometric networks where the number of nodes is on the order of tens or hundreds as in many real-life networks. First, the impact of the tolerance parameter on network resiliency is investigated. We quantify the network reaction to initial disturbances of different sizes by measuring the damage imposed on the network. Lower and upper bounds on the number of failures are derived to characterize such damages. Such finite-size analysis reveals the decisiveness and criticality of taking action within the first few stages of failure propagation in preventing a cascade. By studying the trend of the bounds as the number of nodes increases, we observe a phase transition phenomenon in terms of the tolerance parameter. The critical value of the tolerance parameter, known as the threshold, is further derived. The findings of this paper, in particular, shed light on how to choose the tolerance parameter appropriately such that a cascade of failures could be avoided. 	
1411.5926v1	http://arxiv.org/pdf/1411.5926v1	2014	Stress and Failure Analysis of Rapidly Rotating Asteroid (29075) 1950 DA	Masatoshi Hirabayashi|Daniel J. Scheeres	  Rozitis et al. recently reported that near-Earth asteroid (29075) 1950 DA, whose bulk density ranges from 1.0 g/cm3 to 2.4 g/cm3, is a rubble pile and requires a cohesive strength of at least 44 Pa to 74 Pa to keep from failing due to its fast spin period. Since their technique for giving failure conditions required the averaged stress over the whole volume, it discarded information about the asteroid's failure mode and internal stress condition. This paper develops a finite element model and revisits the stress and failure analysis of 1950 DA. For the modeling, we do not consider material-hardening and softening. Under the assumption of an associated flow rule and uniform material distribution, we identify the deformation process of 1950 DA when its constant cohesion reaches the lowest value that keeps its current shape. The results show that to avoid structural failure the internal core requires a cohesive strength of at least 75 Pa - 85 Pa. It suggests that for the failure mode of this body, the internal core first fails structurally, followed by the surface region. This implies that if cohesion is constant over the whole volume, the equatorial ridge of 1950 DA results from a material flow going outward along the equatorial plane in the internal core, but not from a landslide as has been hypothesized. This has additional implications for the likely density of the interior of the body. 	
1604.06733v3	http://arxiv.org/pdf/1604.06733v3	2016	Cascading Failures in AC Electricity Grids	Martin Rohden|Daniel Jung|Samyak Tamrakar|Stefan Kettemann	  Sudden failure of a single transmission element in a power grid can induce a domino effect of cascading failures, which can lead to the isolation of a large number of consumers or even to the failure of the entire grid. Here we present results of the simulation of cascading failures in power grids, using an alternating current (AC) model. We first apply this model to a regular square grid topology. For a random placement of consumers and generators on the grid, the probability to find more than a certain number of unsupplied consumers decays as a power law and obeys a scaling law with respect to system size. Varying the transmitted power threshold above which a transmission line fails does not seem to change the power law exponent $q \approx 1.6$. Furthermore, we study the influence of the placement of generators and consumers on the number of affected consumers and demonstrate that large clusters of generators and consumers are especially vulnerable to cascading failures. As a real-world topology we consider the German high-voltage transmission grid. Applying the dynamic AC model and considering a random placement of consumers, we find that the probability to disconnect more than a certain number of consumers depends strongly on the threshold. For large thresholds the decay is clearly exponential, while for small ones the decay is slow, indicating a power law decay. 	
1607.06865v3	http://arxiv.org/pdf/1607.06865v3	2017	Connectivity Oracles for Graphs Subject to Vertex Failures	Ran Duan|Seth Pettie	  We introduce new data structures for answering connectivity queries in graphs subject to batched vertex failures. A deterministic structure processes a batch of $d\leq d_{\star}$ failed vertices in $\tilde{O}(d^3)$ time and thereafter answers connectivity queries in $O(d)$ time. It occupies space $O(d_{\star} m\log n)$. We develop a randomized Monte Carlo version of our data structure with update time $\tilde{O}(d^2)$, query time $O(d)$, and space $\tilde{O}(m)$ for any failure bound $d\le n$. This is the first connectivity oracle for general graphs that can efficiently deal with an unbounded number of vertex failures.   We also develop a more efficient Monte Carlo edge-failure connectivity oracle. Using space $O(n\log^2 n)$, $d$ edge failures are processed in $O(d\log d\log\log n)$ time and thereafter, connectivity queries are answered in $O(\log\log n)$ time, which are correct w.h.p.   Our data structures are based on a new decomposition theorem for an undirected graph $G=(V,E)$, which is of independent interest. It states that for any terminal set $U\subseteq V$ we can remove a set $B$ of $|U|/(s-2)$ vertices such that the remaining graph contains a Steiner forest for $U-B$ with maximum degree $s$. 	
1705.09822v2	http://arxiv.org/pdf/1705.09822v2	2017	The risk factors affecting to the software quality failures in Sri   Lankan Software industry	Namadawa Bashini Jeewanthi Gamage	  Software project failure and cancellation rates increase day by day due to technical failures, quality failures, lack of end client acceptance etc. and also the lack of proper management. There are a number of reasons affected by the software project failures. According to empirical evidence, inadequate testing resources are one of the major factors that contribute to the poor quality. The main objectives of this study are to study the risk factors that affect the software quality to provide some recommendation to minimize the risk of poor quality. There are three main factors affecting to software quality namely proper testing, test planning and QA team which are directly impacted to the software quality risks. To conduct this study, I employed an open-ended questionnaire for collecting qualitative data from responses analyzed them using thematic approach method. The participants with their experiences agreed only with requirement clarity and clearly defined acceptance criteria, not with adequate unit testing and finally and also with that not doing regression testing force to quality failures. As of data analysis, not having proper formal test planning, initial test planning not being realistic, not following quality risk management, non-proper process and contingency action planning also lead to the risk of poor project quality. According to the participants added that the following factors are also behind the reasons for the lack quality of software. The experienced and skilled employees move out from the company as there is not a proper QA process and team members as they do not have the risk management mentality. 	
1706.07575v1	http://arxiv.org/pdf/1706.07575v1	2017	A generic construction of quantum-oblivious-key-transfer-based private   query with ideal database security and zero failure	Chun-Yan Wei|Xiao-Qiu Cai|Bin Liu|Tian-Yin Wang|Fei Gao	  Higher security and lower failure probability have always been people's pursuits in quantum-oblivious-key-transfer-based private query (QOKT-PQ) protocols since Jacobi \emph{et al}. [Phys. Rev. A 83, 022301 (2011)] proposed the first protocol of this kind. However, higher database security generally has to be obtained at the cost of a higher failure probability, and vice versa. Recently, based on a round-robin differential-phase-shift quantum key distribution protocol, Liu \emph{et al}. [Sci. China-Phys. Mech. Astron.58, 100301 (2015)] presented a private query protocol (RRDPS-PQ protocol) utilizing ideal single-photon signal which realizes both ideal database security and zero failure probability. However, ideal single-photon source is not available today, and for large database the required pulse train is too long to implement. Here, we reexamine the security of RRDPS-PQ protocol under imperfect source and present an improved protocol using a special "low-shift and addition" (LSA) technique, which not only can be used to query from large database but also retains the features of "ideal database security" and "zero-failure" even under weak coherent source. Finally, we generalize the LSA technique and establish a generic QOKT-PQ model in which both "ideal database security" and "zero failure" are achieved via acceptable communications. 	
9812068v1	http://arxiv.org/pdf/cond-mat/9812068v1	1998	Incipient failure in sandpile models	O. Narayan|S. R. Nagel	  Elastoplastic and constitutive equation theories are two approaches based on very different assumptions for creating a continuum theory for the stress distributions in a static sandpile. Both models produce the same surprising prediction that in a two dimensional granular pile constructed at its angle of repose, the outside wedge will be on the verge of failure. We show how these predictions can be tested experimentally. 	
0209038v1	http://arxiv.org/pdf/cond-mat/0209038v1	2002	Real event detection and the treatment of congestive heart failure: an e   fficient technique to help cardiologists to make crucial decisions	P. Allegrini|R. Balocchi|S. Chillemi|P. Grigolini|P. Hamilton|R. Maestri|L. Palatella|G. Raffaelli	  Using a method of entropic analysis of time series we establish the correlation between heartbeat long-range memory and mortality risk in patients with congestive heart failure. 	
0403032v1	http://arxiv.org/pdf/cs/0403032v1	2004	Where Fail-Safe Default Logics Fail	Paolo Liberatore	  Reiter's original definition of default logic allows for the application of a default that contradicts a previously applied one. We call failure this condition. The possibility of generating failures has been in the past considered as a semantical problem, and variants have been proposed to solve it. We show that it is instead a computational feature that is needed to encode some domains into default logic. 	
0009078v1	http://arxiv.org/pdf/math/0009078v1	2000	On properties of theories which preclude the existence of universal   models	Mirna Džamonja|Saharon Shelah	  In this paper we investigate some properties of first order theories which prevent them from having universal models under certain cardinal arithmetic assumptions. Our results give a new syntactical condition, oak property, which is a sufficient condition for a theory not to have universal models in cardinality lambda when certain cardinal arithmetic assumptions implying the failure of GCH (and close to the failure of SCH) hold. 	
0711.3218v1	http://arxiv.org/pdf/0711.3218v1	2007	An Integral Measure of Aging/Rejuvenation for Repairable and   Non-repairable Systems	Mark Kaminskiy|Vasiliy Krivtsov	  This paper introduces a simple index that helps to assess the degree of aging or rejuvenation of a (non)repairable system. The index ranges from -1 to 1 and is negative for the class of decreasing failure rate distributions (or deteriorating point processes) and is positive for the increasing failure rate distributions (or improving point processes). The introduced index is distribution free. 	
0810.4904v1	http://arxiv.org/pdf/0810.4904v1	2008	On Finite Bases for Weak Semantics: Failures versus Impossible Futures	Taolue Chen|Wan Fokkink|Rob van Glabbeek	  We provide a finite basis for the (in)equational theory of the process algebra BCCS modulo the weak failures preorder and equivalence. We also give positive and negative results regarding the axiomatizability of BCCS modulo weak impossible futures semantics. 	
0903.0343v1	http://arxiv.org/pdf/0903.0343v1	2009	Exact three-dimensional wave function and the on-shell t-matrix for the   sharply cut off Coulomb potential: failure of the standard renormalization   factor	W. Glockle|J. Golak|R. Skibinski|H. Witala	  The 3-dimensional wave function for a sharply cut-off Coulomb potential is analytically derived. The asymptotic form of the related scattering amplitude reveals a failure of the standard renormalization factor which is believed to be generally valid for any type of screening. 	
1104.5161v1	http://arxiv.org/pdf/1104.5161v1	2011	Hubbard-Stratonovich Transformation: Successes, Failure, and Cure	H. Kleinert	  We recall the successes of the Hubbard-Stratonovich Transformation (HST) of many-body theory, point out its failure to cope with competing channels of collective phenomena and show how to overcome this by Variational Perturbation Theory. That yields exponentially fast converging results, thanks to the help of a variety of {\it collective classical fields}, rather than a fluctuating {\it collective quantum field} as suggested by the HST. 	
1106.1846v1	http://arxiv.org/pdf/1106.1846v1	2011	New Efficient Error-Free Multi-Valued Consensus with Byzantine Failures	Guanfeng Liang|Nitin Vaidya	  In this report, we investigate the multi-valued Byzantine consensus problem. We introduce two algorithms: the first one achieves traditional validity requirement for consensus, and the second one achieves a stronger "q-validity" requirement. Both algorithms are more efficient than the ones introduces in our recent PODC 2011 paper titled "Error-Free Multi-Valued Consensus with Byzantine Failures". 	
1110.3390v1	http://arxiv.org/pdf/1110.3390v1	2011	Bayesian Post-Processor and other Enhancements of Subset Simulation for   Estimating Failure Probabilities in High Dimensions	Konstantin M. Zuev|James L. Beck|Siu-Kui Au|Lambros S. Katafygiotis	  Estimation of small failure probabilities is one of the most important and challenging computational problems in reliability engineering. The failure probability is usually given by an integral over a high-dimensional uncertain parameter space that is difficult to evaluate numerically. This paper focuses on enhancements to Subset Simulation (SS), proposed by Au and Beck, which provides an efficient algorithm based on MCMC (Markov chain Monte Carlo) simulation for computing small failure probabilities for general high-dimensional reliability problems. First, we analyze the Modified Metropolis algorithm (MMA), an MCMC technique, which is used in SS for sampling from high-dimensional conditional distributions. We present some observations on the optimal scaling of MMA, and develop an optimal scaling strategy for this algorithm when it is employed within SS. Next, we provide a theoretical basis for the optimal value of the conditional failure probability $p_0$, an important parameter one has to choose when using SS. Finally, a Bayesian post-processor SS+ for the original SS method is developed where the uncertain failure probability that one is estimating is modeled as a stochastic variable whose possible values belong to the unit interval. Simulated samples from SS are viewed as informative data relevant to the system's reliability. Instead of a single real number as an estimate, SS+ produces the posterior PDF of the failure probability, which takes into account both prior information and the information in the sampled data. This PDF quantifies the uncertainty in the value of the failure probability and it may be further used in risk analyses to incorporate this uncertainty. The relationship between the original SS and SS+ is also discussed 	
1304.7710v1	http://arxiv.org/pdf/1304.7710v1	2013	Learning Geo-Temporal Non-Stationary Failure and Recovery of Power   Distribution	Yun Wei|Chuanyi Ji|Floyd Galvan|Stephen Couvillon|George Orellana|James Momoh	  Smart energy grid is an emerging area for new applications of machine learning in a non-stationary environment. Such a non-stationary environment emerges when large-scale failures occur at power distribution networks due to external disturbances such as hurricanes and severe storms. Power distribution networks lie at the edge of the grid, and are especially vulnerable to external disruptions. Quantifiable approaches are lacking and needed to learn non-stationary behaviors of large-scale failure and recovery of power distribution. This work studies such non-stationary behaviors in three aspects. First, a novel formulation is derived for an entire life cycle of large-scale failure and recovery of power distribution. Second, spatial-temporal models of failure and recovery of power distribution are developed as geo-location based multivariate non-stationary GI(t)/G(t)/Infinity queues. Third, the non-stationary spatial-temporal models identify a small number of parameters to be learned. Learning is applied to two real-life examples of large-scale disruptions. One is from Hurricane Ike, where data from an operational network is exact on failures and recoveries. The other is from Hurricane Sandy, where aggregated data is used for inferring failure and recovery processes at one of the impacted areas. Model parameters are learned using real data. Two findings emerge as results of learning: (a) Failure rates behave similarly at the two different provider networks for two different hurricanes but differently at the geographical regions. (b) Both rapid- and slow-recovery are present for Hurricane Ike but only slow recovery is shown for a regional distribution network from Hurricane Sandy. 	
1306.4943v1	http://arxiv.org/pdf/1306.4943v1	2013	Failure of Calibration is Typical	Gordon Belot	  Schervish (1985b) showed that every forecasting system is noncalibrated for uncountably many data sequences that it might see. This result is strengthened here: from a topological point of view, failure of calibration is typical and calibration rare. Meanwhile, Bayesian forecasters are certain that they are calibrated---this invites worries about the connection between Bayesianism and rationality. 	
1402.6881v1	http://arxiv.org/pdf/1402.6881v1	2014	Obstructions de Brauer-Manin entières sur les espaces homogènes à   stabilisateurs finis nilpotents	Cyril Demarche	  Let $k$ be a number field. We construct homogeneous spaces of $SL_{n,k}$ with finite nilpotent non-abelian stabilizers for which the Brauer-Manin obstruction does not explain the failure of strong approximation (resp. the failure of the integral Hasse principle). 	
1505.00692v1	http://arxiv.org/pdf/1505.00692v1	2015	Dual Failure Resilient BFS Structure	Merav Parter	  We study {\em breadth-first search (BFS)} spanning trees, and address the problem of designing a sparse {\em fault-tolerant} BFS structure, or {\em FT-BFS } for short, resilient to the failure of up to two edges in the given undirected unweighted graph $G$, i.e., a sparse subgraph $H$ of $G$ such that subsequent to the failure of up to two edges, the surviving part $H'$ of $H$ still contains a BFS spanning tree for (the surviving part of) $G$. FT-BFS structures, as well as the related notion of replacement paths, have been studied so far for the restricted case of a single failure. It has been noted widely that when concerning shortest-paths in a variety of contexts, there is a sharp qualitative difference between a single failure and two or more failures.   Our main results are as follows. We present an algorithm that for every $n$-vertex unweighted undirected graph $G$ and source node $s$ constructs a (two edge failure) FT-BFS structure rooted at $s$ with $O(n^{5/3})$ edges. To provide a useful theory of shortest paths avoiding 2 edges failures, we take a principled approach to classifying the arrangement these paths. We believe that the structural analysis provided in this paper may decrease the barrier for understanding the general case of $f\geq 2$ faults and pave the way to the future design of $f$-fault resilient structures for $f \geq 2$. We also provide a matching lower bound, which in fact holds for the general case of $f \geq 1$ and multiple sources $S \subseteq V$. It shows that for every $f\geq 1$, and integer $1 \leq \sigma \leq n$, there exist $n$-vertex graphs with a source set $S \subseteq V$ of cardinality $\sigma$ for which any FT-BFS structure rooted at each $s \in S$, resilient to up to $f$-edge faults has $\Omega(\sigma^{1/(f+1)} \cdot n^{2-1/(f+1)})$ edges. 	
1706.01226v1	http://arxiv.org/pdf/1706.01226v1	2017	Failure of 0-1 law for sparse random graph in strong logics	Saharon Shelah	  Let $\alpha\in(0,1)_\mathbb{R}$ be irrational and $G_n = G_{{n, 1/n}^\alpha}$ be the random graph with edge probability $1/n^\alpha$; we know that it satisfies the 0-1 law for first order logic. We deal with the failure of the 0-1 law for stronger logics: $\mathbb{L}_{ \infty, k}, k$ large enough and the LFP, least fix point logic. 	
1707.03173v2	http://arxiv.org/pdf/1707.03173v2	2018	Reliability of components of coherent systems: estimates in presence of   masked data	Agatha Sacramento Rodrigues|Carlos Alberto de Braganca Pereira|Adriano Polpo	  The reliability of a system of components depends on reliability of each component. Thus, the initial statistical work should be the estimation of the reliability of each component of the system. This is not an easy task because when the system fails, the failure time of a given component can not be observed, that is, censored data. Rodrigues et al. (2017) presented a solution for reliability estimation of components when it is avaliable the system failure time and the status of each component at the time of system failure (if it had failed before, after or it is responsible for system failure). However, there are situations it may be difficult to identify the status of components at the moment of system failure.   Such cases are systems with masked causes of failure. Since parallel and series systems are the simplest systems, innumerous alternative solutions for these two systems have been appeared in the literature. To the best of our knowledge, this seems to be the first work that considers the general case of coherent systems. The three-parameter Weibull distribution is considered as the component failure time model. Identically distributed failure times is not required restrictions. Furthermore, there is no restriction on the subjective choice of prior distributions but preference has been given to continuous prior distributions; these priors represent well the nuances of the environment that the system operates. The statistical work of obtaining quantities of the posterior distribution is supported by the Metropolis within Gibbs algorithm. With several simulations, the excellent performance of the model was evaluated. We also consider a computer hard-drives real dataset in order to present the practical relevance of the proposed model. 	
1709.03707v1	http://arxiv.org/pdf/1709.03707v1	2017	Failure of the local-global principle for isotropy of quadratic forms   over rational function fields	Asher Auel|V. Suresh	  We prove the failure of the local-global principle, with respect to all discrete valuations, for isotropy of quadratic forms over a rational function field of transcendence degree at least 2 over the complex numbers. Our construction involves the generalized Kummer varieties considered by Borcea and Cynk--Hulek. 	
0710.5082v1	http://arxiv.org/pdf/0710.5082v1	2007	Heating of blue compact dwarf galaxies: gas distribution and   photoionization by stars in I Zw 18	D. Pequignot	  Photoionization models so far are unable to account for the high electron temperature Te([O III]) implied by the line ratio [O III]4363A/[O III]5007A in low-metallicity blue compact dwarf galaxies, casting doubts on the assumption of photoionization by hot stars as the dominant source of heating of the gas in these objects. Combinations of runs of the 1-D photoionization code NEBU are used to explore alternative models for the giant H II region shell I Zw 18 NW. Acceptable models are obtained, which represent schematically an incomplete shell comprising radiation-bounded condensations embedded in a low-density matter-bounded diffuse medium. The thermal pressure contrast between gas components is about a factor 7. The diffuse phase can be in pressure balance with the hot superbubble fed by mechanical energy from the inner massive star cluster. The failure of previous modellings is ascribed to (1) the adoption of an inadequate small-scale gas density distribution, which proves critical when the collisional excitation of hydrogen contributes significantly to the cooling of the gas, and possibly (2) a too restrictive implementation of Wolf-Rayet stars in synthetic stellar cluster spectral energy distributions. A neutral gas component heated by soft X-rays, whose power is less than 1% of the star cluster luminosity and consistent with CHANDRA data, can explain the low-ionization fine-structure lines detected by SPITZER. [O/Fe] is slightly smaller in I Zw 18 NW than in Galactic Halo stars of similar metallicity and [C/O] is correlatively large. Extra heating by, e.g., dissipation of mechanical energy is not required to explain Te([O III]) in I Zw 18. Important astrophysical developments are at stakes in the 5% uncertainty attached to [O III] collision strengths. 	
0909.5552v1	http://arxiv.org/pdf/0909.5552v1	2009	On the physical origin of the second solar spectrum of the Sc II line at   4247 A	Luca Belluzzi	  The peculiar three-peak structure of the linear polarization profile shown in the second solar spectrum by the Ba II line at 4554 A has been interpreted as the result of the different contributions coming from the barium isotopes with and without hyperfine structure (HFS). In the same spectrum, a triple peak polarization signal is also observed in the Sc II line at 4247 A. Scandium has a single stable isotope (^{45}Sc), which shows HFS due to a nuclear spin I=7/2. We investigate the possibility of interpreting the linear polarization profile shown in the second solar spectrum by this Sc II line in terms of HFS. A two-level model atom with HFS is assumed. Adopting an optically thin slab model, the role of atomic polarization and of HFS is investigated, avoiding the complications caused by radiative transfer effects. The slab is assumed to be illuminated from below by the photospheric continuum, and the polarization of the radiation scattered at 90 degrees is investigated. The three-peak structure of the scattering polarization profile observed in this Sc II line cannot be fully explained in terms of HFS. Given the similarities between the Sc II line at 4247 A and the Ba II line at 4554 A, it is not clear why, within the same modeling assumptions, only the three-peak Q/I profile of the barium line can be fully interpreted in terms of HFS. The failure to interpret this Sc II polarization signal raises important questions, whose resolution might lead to significant improvements in our understanding of the second solar spectrum. In particular, if the three-peak structure of the Sc II signal is actually produced by a physical mechanism neglected within the approach considered here, it will be extremely interesting not only to identify this mechanism, but also to understand why it seems to be less important in the case of the barium line. 	
1003.1736v4	http://arxiv.org/pdf/1003.1736v4	2011	Modeling rf breakdown arcs	Zeke Insepov|Jim Norem|Thomas Proslier|Dazhang Huang|Sudhakar Mahalingam|Seth Veitzer	  We describe breakdown in 805 MHz rf accelerator cavities in terms of a number of self consistent mechanisms. We divide the breakdown process into three stages: 1) we model surface failure using molecular dynamics of fracture caused by electrostatic tensile stress, 2) we model the ionization of neutrals responsible for plasma initiation and plasma growth using a particle in cell code, and 3) we model surface damage by assuming a process similar to unipolar arcing. We find that the cold, dense plasma in contact with the surface produces very small Debye lengths and very high electric fields over a large area, consistent with unipolar arc behavior, although unipolar arcs are strictly defined with equipotential boundaries. These high fields produce strong erosion mechanisms, primarily self sputtering, compatible with the crater formation that we see. We use OOPIC modeling to estimate very high surface electric fields in the dense plasma and measure these field using electrohydrodynamic arguments to relate the dimensions of surface damage with the applied electric field. We also present a geometrical explanation of the large enhancement factors of field emitters.This is consistent with the apparent absence of whiskers on surfaces exposed to high fields. The enhancement factors we derive, when combined with the Fowler-Nordheim analysis produce a consistent picture of breakdown and field emission from surfaces at local fields of 7 - 10 GV/m. We show that the plasma growth rates we obtain from OOPIC are consistent with growth rates of the cavity shorting currents using x ray measurements. We believe the general picture presented here for rf breakdown arcs should be directly applicable to a larger class of vacuum arcs. Results from the plasma simulation are included as a guide to experimental verification of this model. 	
1406.3788v1	http://arxiv.org/pdf/1406.3788v1	2014	Empirical Tests of Pre-Main-Sequence Stellar Evolution Models with   Eclipsing Binaries	Keivan G. Stassun|Gregory A. Feiden|Guillermo Torres	  We examine the performance of standard PMS stellar evolution models against the accurately measured properties of a benchmark sample of 26 PMS stars in 13 EB systems. We provide a definitive compilation of all fundamental properties for the EBs. We also provide a definitive compilation of the various PMS model sets. In the H-R diagram, the masses inferred for the individual stars by the models are accurate to better than 10% above 1 Msun, but below 1 Msun they are discrepant by 50-100%. We find evidence that the failure of the models to match the data is linked to the triples in the EB sample; at least half of the EBs possess tertiary companions. Excluding the triples, the models reproduce the stellar masses to better than ~10% in the H-R diagram, down to 0.5 Msun, below which the current sample is fully contaminated by tertiaries. We consider several mechanisms by which a tertiary might cause changes in the EB properties and thus corrupt the agreement with stellar model predictions. We show that the energies of the tertiary orbits are comparable to that needed to potentially explain the scatter in the EB properties through injection of heat, perhaps involving tidal interaction. It seems from the evidence at hand that this mechanism, however it operates in detail, has more influence on the surface properties of the stars than on their internal structure, as the lithium abundances are broadly in good agreement with model predictions. The EBs that are members of young clusters appear individually coeval to within 20%, but collectively show an apparent age spread of ~50%, suggesting true age spreads in young clusters. However, this apparent spread in the EB ages may also be the result of scatter in the EB properties induced by tertiaries. [Abridged] 	
1502.04571v2	http://arxiv.org/pdf/1502.04571v2	2015	Air Entrainment in Dynamic Wetting: Knudsen Effects and the Influence of   Ambient Air Pressure	James E. Sprittles	  Recent experiments on coating flows and liquid drop impact both demonstrate that wetting failures caused by air entrainment can be suppressed by reducing the ambient gas pressure. Here, it is shown that non-equilibrium effects in the gas can account for this behaviour, with ambient pressure reductions increasing the gas' mean free path and hence the Knudsen number $Kn$. These effects first manifest themselves through Maxwell slip at the gas' boundaries so that for sufficiently small $Kn$ they can be incorporated into a continuum model for dynamic wetting flows. The resulting mathematical model contains flow structures on the nano-, micro- and milli-metre scales and is implemented into a computational platform developed specifically for such multiscale phenomena. The coating flow geometry is used to show that for a fixed gas-liquid-solid system (a) the increased Maxwell slip at reduced pressures can substantially delay air entrainment, i.e. increase the `maximum speed of wetting', (b) unbounded maximum speeds are obtained as the pressure is reduced only when slip at the gas-liquid interface is allowed for and (c) the observed behaviour can be rationalised by studying the dynamics of the gas film in front of the moving contact line. A direct comparison to experimental results obtained in the dip-coating process shows that the model recovers most trends but does not accurately predict some of the high viscosity data at reduced pressures. This discrepancy occurs because the gas flow enters the `transition regime', so that more complex descriptions of its non-equilibrium nature are required. Finally, by collapsing onto a master curve experimental data obtained for drop impact in a reduced pressure gas, it is shown that the same physical mechanisms are also likely to govern splash suppression phenomena. 	
1505.01376v3	http://arxiv.org/pdf/1505.01376v3	2017	Tissue fibrosis: a principal proof for the central role of Misrepair in   aging	Jicun Wang-Michelitsch|Thomas M. Michelitsch	  Tissue fibrosis is the phenomenon that a tissue has progressive deposition of collagen fibers with age. Tissue fibrosis is associated with aging of most of our organs, and it is the main pathology in arteriosclerosis, chronic bronchitis/emphysema, and benign prostatic hyperplasia. The causes and characteristics of fibrosis are analyzed with Misrepair mechanism, a mechanism proposed in Misrepair-accumulation aging theory. Tissue fibrosis is known to be a result of repairs of tissue by collagen fibers. A repair with collagen fibers is a manner of "Misrepair". The collagen fibers are used for replacing dead cells or disrupted extracellular matrixes (ECMs) including elastic fibers, myofibers, and basement membrane. The progressive tissue fibrosis with age manifests the essential role of Misrepair in aging, because it reveals three facts: A. a process of Misrepair exists; B. Misrepairs are unavoidable; and C. Misrepairs accumulate. As a result of accumulation of Misrepairs of tissue with collagen fibers, tissue fibrosis is focalized and self-accelerating, appearing as growing of spots of hyaline degeneration. Fibrosis results in stiffness or atrophy of an organ and progressive failure of the organ. In arteriosclerosis, the deposition of collagen fibers in arterial wall is for replacing disrupted elastic fibers or myofibers, however results in hardness of the wall. Wrinkle formation is part of skin fibrosis, and it may be a result of accumulation of collagen fibers of different lengths. Senile hair-loss and hair-whitening are probably consequence of dermal fibrosis. In conclusion, tissue fibrosis is a result of accumulation of Misrepairs of tissue with collagen fibers, and the phenomenon of fibrosis is a powerful proof for the central role of Misrepair in aging. 	
1508.02129v1	http://arxiv.org/pdf/1508.02129v1	2015	Fracture resistance of zigzag single walled carbon nanotubes	Qiang Lu|Baidurya Bhattacharya	  Brittle fracture is one of the important failure modes of Single-Walled Carbon Nanotube (SWNT) due to mechanical loading. In this paper, the fracture resistance of zigzag SWNTs with preexisting defects is calculated using fracture mechanics concepts based on atomistic simulations. The problem of unstable crack growth at finite temperature, presumably caused by lattice trapping effect, is circumvented by computing the strain energy release rate through a series of displacement-controlled tensile loading of SWNTs (applied through moving the outermost layer of atoms at one end at constant strain rate of 9.4x10-4/ps) with pre-existing crack-like defects of various lengths. The strain energy release rate, G, is computed for (17,0), (28,0) and (35,0) SWNTs (each with aspect ratio 4) with pre-existing cracks up to 29.5{\AA} long. The fracture resistance, Gc, is determined as a function of crack length for each tube at three different temperatures (1K, 300K and 500K). A significant dependence of Gc on crack length is observed reminiscent of the rising R curve behavior of metals at the macroscale: for the zigzag nanotubes Gc increases with crack length at small length, and tends to reach a constant value if the tube diameter is large enough. We suspect that the lattice trapping effect plays the role of crack tip plasticity at the atomic scale. For example, at 300 Kelvin, Gc for the (35,0) tube with aspect ratio 4 converges to 6 Joule/m2 as the crack length exceeds 20 Angstrom. This value is comparable with the fracture toughness of graphite and Silicon. The fracture resistance of the tubes is found to decrease significantly as the temperature increases. To study the length effects, the computations are repeated for zigzag nanotubes with the same three chiralities but with aspect ratio 8 at 1K. 	
1610.08036v1	http://arxiv.org/pdf/1610.08036v1	2016	Modeling pressure distribution and heat in the body tissue and extract   the relationship between them in order to improve treatment planning in HIFU	Saeed Reza Hajian|Ali Abbaspour Tehrani Fard|Majid Pouladian|Gholam Reza Hemmasi	  In high intensity focused ultrasound (HIFU) systems using non-ionizing methods in cancer treatment, if the device is applied to the body externally, the HIFU beam can damage nearby healthy tissues and burn skin due to lack of knowledge about the viscoelastic properties of patient tissue and failure to consider the physical properties of tissue in treatment planning. Addressing this problem by using various methods, such as MRI or ultrasound, elastography can effectively measure visco-elastic properties of tissue and fits within the pattern of stimulation and total treatment planning. In this paper, in a linear path of HIFU propagation, and by considering the smallest part of the path, including voxel with three mechanical elements of mass, spring and damper, which represents the properties of viscoelasticity of tissue, by creating waves of HIFU in the wire environment of MATLAB mechanics and stimulating these elements, pressure and heat transfer due to stimulation in the hypothetical voxel was obtained. Through the repeatability of these three-dimensional elements, tissue is created. The measurement was performed on three layers. The values of these elements for liver tissue and kidney of sheep in a practical example and outside the body are measured, and pressure and heat for three layers of liver and kidney tissue of an organism were obtained by applying ultrasound signals with a designed model. This action is repeated in three different directions, and the results are then compared with simulation software for ultrasound, as a reference to U.S. Food and Drug Administration (FDA) measures for HIFU, as well as comparisons of results with an operational method for an HIFU cell. 	
1704.07753v1	http://arxiv.org/pdf/1704.07753v1	2017	Fluid-structure interaction modelling and stabilisation of a   patient-specific arteriovenous access fistula	W. P. Guess|B. D. Reddy|A. McBride|B. Spottiswoode|J. Downs|T. Franz	  A patient-specific fluid-structure interaction (FSI) model of a phase-contrast magnetic resonance angiography (PC-MRA) imaged arteriovenous fistula is presented. The numerical model is developed and simulated using a commercial multiphysics simulation package where a semi-implicit FSI coupling scheme combines a finite volume method blood flow model and a finite element method vessel wall model. A pulsatile mass-flow boundary condition is prescribed at the artery inlet of the model, and a three-element Windkessel model at the artery and vein outlets. The FSI model is freely available for analysis and extension. This work shows the effectiveness of combining a number of stabilisation techniques to simultaneously overcome the added-mass effect and optimise the efficiency of the overall model. The PC-MRA data, fluid model, and FSI model results show almost identical flow features in the fistula; this applies in particular to a flow recirculation region in the vein that could potentially lead to fistula failure. 	
1706.02272v1	http://arxiv.org/pdf/1706.02272v1	2017	Handling Model and Implementation Uncertainties via an Adaptive Discrete   Sliding Mode Controller Design	Mohammad Reza Amini|Mahdi Shahbakhti|Selina Pan|J. Karl Hedrick	  Analog-to-digital conversion (ADC) and uncertainties in modeling the plant dynamics are the main sources of imprecisions in the design cycle of model-based controllers. These implementation and model uncertainties should be addressed in the early stages of the controller design, otherwise they could lead to failure in the controller performance and consequently increase the time and cost required for completing the controller verification and validation (V&V) with more iterative loops. In this paper, a new control approach is developed based on a nonlinear discrete sliding mode controller (DSMC) formulation to mitigate the ADC imprecisions and model uncertainties. To this end, a DSMC design is developed against implementation imprecisions by incorporating the knowledge of ADC uncertainties on control inputs via an online uncertainty prediction and propagation mechanism. Next, a generic online adaptive law will be derived to compensate for the impact of an unknown parameter in the controller equations that is assumed to represent the model uncertainty. The final proposed controller is an integrated adaptive DSMC with robustness to implementation and model uncertainties that includes (i) an online ADC uncertainty mechanism, and (ii) an online adaptation law. The proposed adaptive control approach is evaluated on a nonlinear automotive engine control problem in real-time using a processor-in-the-loop (PIL) setup with an actual electronic control unit (ECU). The results reveal that the proposed adaptive control technique removes the uncertainty in the model fast, and significantly improves the robustness of the controllers to ADC imprecisions. This provides up to 60% improvement in the performance of the controller under implementation and model uncertainties compared to a baseline DSMC, in which there are no incorporated ADC imprecisions. 	
1706.04087v1	http://arxiv.org/pdf/1706.04087v1	2017	MIMO First and Second Order Discrete Sliding Mode Controls of Uncertain   Linear Systems under Implementation Imprecisions	Mohammad Reza Amini|Mahdi Shahbakhti|Selina Pan	  The performance of a conventional model-based controller significantly depends on the accuracy of the modeled dynamics. The model of a plant's dynamics is subjected to errors in estimating the numerical values of the physical parameters, and variations over operating environment conditions and time. These errors and variations in the parameters of a model are the major sources of uncertainty within the controller structure. Digital implementation of controller software on an actual electronic control unit (ECU) introduces another layer of uncertainty at the controller inputs/outputs. The implementation uncertainties are mostly due to data sampling and quantization via the analog-to-digital conversion (ADC) unit. The failure to address the model and ADC uncertainties during the early stages of a controller design cycle results in a costly and time consuming verification and validation (V&V) process. In this paper, new formulations of the first and second order discrete sliding mode controllers (DSMC) are presented for a general class of uncertain linear systems. The knowledge of the ADC imprecisions is incorporated into the proposed DSMCs via an online ADC uncertainty prediction mechanism to improve the controller robustness characteristics. Moreover, the DSMCs are equipped with adaptation laws to remove two different types of modeling uncertainties (multiplicative and additive) from the parameters of the linear system model. The proposed adaptive DSMCs are evaluated on a DC motor speed control problem in real-time using a processor-in-the-loop (PIL) setup with an actual ECU. The results show that the proposed SISO and MIMO second order DSMCs improve the conventional SISO first order DSMC tracking performance by 69% and 84%, respectively. Moreover, the proposed adaptation mechanism is able to remove the uncertainties in the model by up to 90%. 	
0412128v2	http://arxiv.org/pdf/astro-ph/0412128v2	2004	Accelerator Measurements of the Askaryan effect in Rock Salt: A Roadmap   Toward Teraton Underground Neutrino Detectors	P. W. Gorham|D. Saltzberg|R. C. Field|E. Guillian|R. Milincic|D. Walz|D. Williams	  We report on further SLAC measurements of the Askaryan effect: coherent radio emission from charge asymmetry in electromagnetic cascades. We used synthetic rock salt as the dielectric medium, with cascades produced by GeV bremsstrahlung photons at the Final Focus Test Beam. We extend our prior discovery measurements to a wider range of parameter space and explore the effect in a dielectric medium of great potential interest to large scale ultra-high energy neutrino detectors: rock salt (halite), which occurs naturally in high purity formations containing in many cases hundreds of cubic km of water-equivalent mass. We observed strong coherent pulsed radio emission over a frequency band from 0.2-15 GHz. A grid of embedded dual-polarization antennas was used to confirm the high degree of linear polarization and track the change of direction of the electric-field vector with azimuth around the shower. Coherence was observed over 4 orders of magnitude of shower energy. The frequency dependence of the radiation was tested over two orders of magnitude of UHF and microwave frequencies. We have also made the first observations of coherent transition radiation from the Askaryan charge excess, and the result agrees well with theoretical predictions. Based on these results we have performed a detailed and conservative simulation of a realistic GZK neutrino telescope array within a salt-dome, and we find it capable of detecting 10 or more contained events per year from even the most conservative GZK neutrino models. 	
0607024v1	http://arxiv.org/pdf/nucl-ex/0607024v1	2006	Measurement of Neutron Background at the Pyhasalmi mine for CUPP   Project, Finland	J. N. Abdurashitov|V. N. Gavrin|V. L. Matushko|A. A. Shikhin|V. E. Yants|J. Peltoniemi|T. Keranen	  A natural neutron flux is one of significant kind of background in high-sensitive underground experiments. Therefore, when scheduling a delicate underground measurements one needs to measure neutron background. Deep underground the most significant source of neutrons are the U-Th natural radioactive chains giving a fission spectrum with the temperature of 2-3 MeV. Another source is the U-Th alpha-reactions on light nuclei of mine rock giving neutrons with different spectra in the 1-15 MeV energy region. Normal basalt mine rocks contain 1 ppm g/g of U-238 and less. Deep underground those rocks produce natural neutron fluxes of 10^{-7} - 10^{-6} cm^{-2}s^{-1} above 1 MeV. To measure such a background one needs a special techniques. In the Institute for Nuclear Research, Moscow, the neutron spectrometer was developed and built which is sensitive to such a low neutron fluxes. At the end of 2001 the collection of neutron data at the Pyhasalmi mine was started for the CUPP project. During 2002 the background and rough energy spectra of neutron at underground levels 410, 660, 990 and 1410 m were measured. The result of the measurement of the neutron background at different levels of the Pyhasalmi mine is presented and discussed. Data analysis is performed in different energy ranges from thermal neutrons up to 25 MeV and above. 	
1005.2624v2	http://arxiv.org/pdf/1005.2624v2	2010	On The Possibility of Enrichment and Differentiation in Gas Giants   During Birth by Disk Instability	Aaron C. Boley|Richard H. Durisen	  We investigate the coupling between rock-size solids and gas during the formation of gas giant planets by disk fragmentation in the outer regions of massive disks. In this study, we use three-dimensional radiative hydrodynamics simulations and model solids as a spatial distribution of particles. We assume that half of the total solid fraction is in small grains and half in large solids. The former are perfectly entrained with the gas and set the opacity in the disk, while the latter are allowed to respond to gas drag forces, with the back reaction on the gas taken into account. To explore the maximum effects of gas-solid interactions, we first consider 10cm-size particles. We then compare these results to a simulation with 1 km-size particles, which explores the low-drag regime. We show that (1) disk instability planets have the potential to form large cores due to aerodynamic capturing of rock-size solids in spiral arms before fragmentation; (2) that temporary clumps can concentrate tens of $M_{\oplus}$ of solids in very localized regions before clump disruption; (3) that the formation of permanent clumps, even in the outer disk, is dependent on the grain-size distribution, i.e., the opacity; (4) that nonaxisymmetric structure in the disk can create disk regions that have a solids-to-gas ratio greater than unity; (5) that the solid distribution may affect the fragmentation process; (6) that proto-gas giants and proto-brown dwarfs can start as differentiated objects prior to the H$_2$ collapse phase; (7) that spiral arms in a gravitationally unstable disk are able to stop the inward drift of rock-size solids, even redistributing them to larger radii; and, (8) that large solids can form spiral arms that are offset from the gaseous spiral arms. We conclude that planet embryo formation can be strongly affected by the growth of solids during the earliest stages of disk accretion. 	
1601.07608v2	http://arxiv.org/pdf/1601.07608v2	2016	Discovery and Validation of a High-Density sub-Neptune from the K2   Mission	Néstor Espinoza|Rafael Brahm|Andrés Jordán|James S. Jenkins|Felipe Rojas|Paula Jofré|Thomas Mädler|Markus Rabus|Julio Chanamé|Blake Pantoja|Maritza G. Soto|Katie M. Morzinski|Jared R. Males|Kimberly Ward-Duong|Laird M. Close	  We report the discovery of BD+20594b, a high density sub-Neptune exoplanet, made using photometry from Campaign 4 of the two-wheeled Kepler (K2) mission, ground-based radial velocity follow-up from HARPS and high resolution lucky and adaptive optics imaging obtained using AstraLux and MagAO, respectively. The host star is a bright ($V=11.04$, $K_s = 9.37$), slightly metal poor ([Fe/H]$=-0.15\pm 0.05$ dex) solar analogue located at $152.1^{+9.7}_{-7.4}$ pc from Earth, for which we find a radius of $R_*=0.928^{+0.055}_{-0.040}R_\odot$ and a mass of $M_* = 0.961^{+0.032}_{-0.029}M_\odot$. A joint analysis of the K2 photometry and HARPS radial velocities reveal that the planet is in a $\approx 42$ day orbit around its host star, has a radius of $2.23^{+0.14}_{-0.11}R_\oplus$, and a mass of $16.3^{+6.0}_{-6.1}M_\oplus$. Although the data at hand puts the planet in the region of the mass-radius diagram where we could expect planets with a pure rock (i.e. magnesium silicate) composition using two-layer models (i.e., between rock/iron and rock/ice compositions), we discuss more realistic three-layer composition models which can explain the high density of the discovered exoplanet. The fact that the planet lies in the boundary between "possibly rocky" and "non-rocky" exoplanets, makes it an interesting planet for future RV follow-up. 	
1605.00171v1	http://arxiv.org/pdf/1605.00171v1	2016	Uranus evolution models with simple thermal boundary layers	N. Nettelmann|K. Wang|J. J. Fortney|S. Hamel|S. Yellamilli|M. Bethkenhagen|R. Redmer	  The strikingly low luminosity of Uranus (Teff ~ Teq) constitutes a long-standing challenge to our understanding of Ice Giant planets. Here we present the first Uranus structure and evolution models that are constructed to agree with both the observed low luminosity and the gravity field data. Our models make use of modern ab initio equations of state at high pressures for the icy components water, methane, and ammonia. Proceeding step by step, we confirm that adiabatic models yield cooling times that are too long, even when uncertainties in the ice:rock ratio (I:R) are taken into account. We then argue that the transition between the ice/rock-rich interior and the H/He-rich outer envelope should be stably stratified. Therefore, we introduce a simple thermal boundary and adjust it to reproduce the low luminosity. Due to this thermal boundary, the deep interior of the Uranus models are up to 2--3 warmer than adiabatic models, necessitating the presence of rocks in the deep interior with a possible I:R of $1\times$ solar. Finally, we allow for an equilibrium evolution (Teff ~ Teq) that begun prior to the present day, which would therefore no longer require the current era to be a "special time" in Uranus' evolution. In this scenario, the thermal boundary leads to more rapid cooling of the outer envelope. When Teff ~ Teq is reached, a shallow, subadiabatic zone in the atmosphere begins to develop. Its depth is adjusted to meet the luminosity constraint. This work provides a simple foundation for future Ice Giant structure and evolution models, that can be improved by properly treating the heat and particle fluxes in the diffusive zones. 	
1712.05641v2	http://arxiv.org/pdf/1712.05641v2	2018	Interior Structures and Tidal Heating in the TRAPPIST-1 Planets	Amy C. Barr|Vera Dobos|László L. Kiss	  With seven planets, the TRAPPIST-1 system has the largest number of exoplanets discovered in a single system so far. The system is of astrobiological interest, because three of its planets orbit in the habitable zone of the ultracool M dwarf. Assuming the planets are composed of non-compressible iron, rock, and H$_2$O, we determine possible interior structures for each planet. To determine how much tidal heat may be dissipated within each planet, we construct a tidal heat generation model using a single uniform viscosity and rigidity for each planet based on the planet's composition. With the exception of TRAPPIST-1c, all seven of the planets have densities low enough to indicate the presence of significant H$_2$O in some form. Planets b and c experience enough heating from planetary tides to maintain magma oceans in their rock mantles; planet c may have eruptions of silicate magma on its surface, which may be detectable with next-generation instrumentation. Tidal heat fluxes on planets d, e, and f are lower, but are still twenty times higher than Earth's mean heat flow. Planets d and e are the most likely to be habitable. Planet d avoids the runaway greenhouse state if its albedo is $\gtrsim$ 0.3. Determining the planet's masses within $\sim0.1$ to 0.5 Earth masses would confirm or rule out the presence of H$_2$O and/or iron in each planet, and permit detailed models of heat production and transport in each planet. Understanding the geodynamics of ice-rich planets f, g, and h requires more sophisticated modeling that can self-consistently balance heat production and transport in both rock and ice layers. 	
9811044v1	http://arxiv.org/pdf/cond-mat/9811044v1	1998	Failure Probabilities and Tough-Brittle Crossover of Heterogeneous   Materials with Continuous Disorder	B. Q. Wu|P. L. Leath	  The failure probabilities or the strength distributions of heterogeneous 1D systems with continuous local strength distribution and local load sharing have been studied using a simple, exact, recursive method. The fracture behavior depends on the local bond-strength distribution, the system size, and the applied stress, and crossovers occur as system size or stress changes. In the brittle region, systems with continuous disorders have a failure probability of the modified-Gumbel form, similar to that for systems with percolation disorder. The modified-Gumbel form is of special significance in weak-stress situations. This new recursive method has also been generalized to calculate exactly the failure probabilities under various boundary conditions, thereby illustrating the important effect of surfaces in the fracture process. 	
0312009v1	http://arxiv.org/pdf/cs/0312009v1	2003	Failure-Free Genetic Algorithm Optimization of a System Controller Using   SAFE/LEARNING Controllers in Tandem	E. S. Sazonov|D. Del Gobbo|P. Klinkhachorn|R. L. Klein	  The paper presents a method for failure free genetic algorithm optimization of a system controller. Genetic algorithms present a powerful tool that facilitates producing near-optimal system controllers. Applied to such methods of computational intelligence as neural networks or fuzzy logic, these methods are capable of combining the non-linear mapping capabilities of the latter with learning the system behavior directly, that is, without a prior model. At the same time, genetic algorithms routinely produce solutions that lead to the failure of the controlled system. Such solutions are generally unacceptable for applications where safe operation must be guaranteed. We present here a method of design, which allows failure-free application of genetic algorithms through utilization of SAFE and LEARNING controllers in tandem, where the SAFE controller recovers the system from dangerous states while the LEARNING controller learns its behavior. The method has been validated by applying it to an inherently unstable system of inverted pendulum. 	
0506001v1	http://arxiv.org/pdf/cs/0506001v1	2005	SafeMPI - Extending MPI for Byzantine Error Detection on Parallel   Clusters	Dmitry Mogilevsky|Sean Keller	  Modern high-performance computing relies heavily on the use of commodity processors arranged together in clusters. These clusters consist of individual nodes (typically off-the-shelf single or dual processor machines) connected together with a high speed interconnect. Using cluster computation has many benefits, but also carries the liability of being failure prone due to the sheer number of components involved. Many effective solutions have been proposed to aid failure recovery in clusters, their one significant downside being the failure models they support. Most of the work in the area has focused on detecting and correcting fail-stop errors. We propose a system that will also detect more general error models, such as Byzantine errors, thus allowing existing failure recovery methods to handle them correctly. 	
0604053v1	http://arxiv.org/pdf/cs/0604053v1	2006	Survivable Routing in IP-over-WDM Networks in the Presence of Multiple   Failures	Maciej Kurant|Patrick Thiran	  Failure restoration at the IP layer in IP-over-WDM networks requires to map the IP topology on the WDM topology in such a way that a failure at the WDM layer leaves the IP topology connected. Such a mapping is called $survivable$. As finding a survivable mapping is known to be NP-complete, in practice it requires a heuristic approach. We have introduced in [1] a novel algorithm called ``SMART'', that is more effective and scalable than the heuristics known to date. Moreover, the formal analysis of SMART [2] has led to new applications: the formal verification of the existence of a survivable mapping, and a tool tracing and repairing the vulnerable areas of the network. In this paper we extend the theoretical analysis in [2] by considering $multiple failures$. 	
0605108v2	http://arxiv.org/pdf/cs/0605108v2	2006	Diagnosability of Fuzzy Discrete Event Systems	Fuchun Liu|Daowen Qiu|Hongyan Xing|Zhujun Fan	  In order to more effectively cope with the real-world problems of vagueness, {\it fuzzy discrete event systems} (FDESs) were proposed recently, and the supervisory control theory of FDESs was developed. In view of the importance of failure diagnosis, in this paper, we present an approach of the failure diagnosis in the framework of FDESs. More specifically: (1) We formalize the definition of diagnosability for FDESs, in which the observable set and failure set of events are {\it fuzzy}, that is, each event has certain degree to be observable and unobservable, and, also, each event may possess different possibility of failure occurring. (2) Through the construction of observability-based diagnosers of FDESs, we investigate its some basic properties. In particular, we present a necessary and sufficient condition for diagnosability of FDESs. (3) Some examples serving to illuminate the applications of the diagnosability of FDESs are described. To conclude, some related issues are raised for further consideration. 	
9706224v1	http://arxiv.org/pdf/math/9706224v1	1997	A polarized partition relation and failure of GCH	Saharon Shelah	  The main result is that for lambda strong limit singular failing the continuum hypothesis (i.e. 2^lambda > lambda^+), a polarized partition theorem holds. 	
0611176v1	http://arxiv.org/pdf/math/0611176v1	2006	Restricted estimation of the cumulative incidence functions   corresponding to competing risks	Hammou El Barmi|Hari Mukerjee	  In the competing risks problem, an important role is played by the cumulative incidence function (CIF), whose value at time $t$ is the probability of failure by time $t$ from a particular type of failure in the presence of other risks. In some cases there are reasons to believe that the CIFs due to various types of failure are linearly ordered. El Barmi et al. [3] studied the estimation and inference procedures under this ordering when there are only two causes of failure. In this paper we extend the results to the case of $k$ CIFs, where $k\ge3$. Although the analyses are more challenging, we show that most of the results in the 2-sample case carry over to this $k$-sample case. 	
0602040v1	http://arxiv.org/pdf/physics/0602040v1	2006	Gravitational waves from BBH-systems? A (doubly) vain quest	A. Loinger	  The theoretical reasons at the root of LIGO's experimental failure in searching gravitational waves (GW's) from binary black hole (BBH) inspirals. 	
0701047v1	http://arxiv.org/pdf/q-bio/0701047v1	2007	Many Attractors, Long Chaotic Transients, and Failure in Small-World   Networks of Excitable Neurons	Hermann Riecke|Alex Roxin|Santiago Madruga|Sara A. Solla	  We study the dynamical states that emerge in a small-world network of recurrently coupled excitable neurons through both numerical and analytical methods. These dynamics depend in large part on the fraction of long-range connections or `short-cuts' and the delay in the neuronal interactions. Persistent activity arises for a small fraction of `short-cuts', while a transition to failure occurs at a critical value of the `short-cut' density. The persistent activity consists of multi-stable periodic attractors, the number of which is at least on the order of the number of neurons in the network. For long enough delays, network activity at high `short-cut' densities is shown to exhibit exceedingly long chaotic transients whose failure-times averaged over many network configurations follow a stretched exponential. We show how this functional form arises in the ensemble-averaged activity if each network realization has a characteristic failure-time which is exponentially distributed. 	
0707.2257v1	http://arxiv.org/pdf/0707.2257v1	2007	A Bayes method for a Bathtub Failure Rate via two $\mathbf{S}$-paths	Man-Wai Ho	  A class of semi-parametric hazard/failure rates with a bathtub shape is of interest. It does not only provide a great deal of flexibility over existing parametric methods in the modeling aspect but also results in a closed and tractable Bayes estimator for the bathtub-shaped failure rate (BFR). Such an estimator is derived to be a finite sum over two $\mathbf{S}$-paths due to an explicit posterior analysis in terms of two (conditionally independent) $\mathbf{S}$-paths. These, newly discovered, explicit results can be proved to be a Rao-Blackwellization of counterpart results in terms of partitions that are readily available by a specialization of James (2005)'s work. We develop both iterative and non-iterative computational procedures based on existing efficient Monte Carlo methods for sampling one single $\mathbf{S}$-path. Nmerical simulations are given to demonstrate the practicality and the effectiveness of our methodology. Last but not least, two applications of the proposed method are discussed, of which one is about a Bayesian test for failure rates and the other is related to modeling with covariates. 	
0708.1211v1	http://arxiv.org/pdf/0708.1211v1	2007	A Deterministic Sub-linear Time Sparse Fourier Algorithm via   Non-adaptive Compressed Sensing Methods	M. A. Iwen	  We study the problem of estimating the best B term Fourier representation for a given frequency-sparse signal (i.e., vector) $\textbf{A}$ of length $N \gg B$. More explicitly, we investigate how to deterministically identify B of the largest magnitude frequencies of $\hat{\textbf{A}}$, and estimate their coefficients, in polynomial$(B,\log N)$ time. Randomized sub-linear time algorithms which have a small (controllable) probability of failure for each processed signal exist for solving this problem. However, for failure intolerant applications such as those involving mission-critical hardware designed to process many signals over a long lifetime, deterministic algorithms with no probability of failure are highly desirable. In this paper we build on the deterministic Compressed Sensing results of Cormode and Muthukrishnan (CM) \cite{CMDetCS3,CMDetCS1,CMDetCS2} in order to develop the first known deterministic sub-linear time sparse Fourier Transform algorithm suitable for failure intolerant applications. Furthermore, in the process of developing our new Fourier algorithm, we present a simplified deterministic Compressed Sensing algorithm which improves on CM's algebraic compressibility results while simultaneously maintaining their results concerning exponential decay. 	
0809.1894v1	http://arxiv.org/pdf/0809.1894v1	2008	A Generalization of the Exponential-Poisson Distribution	Wagner Barreto-Souza|Francisco Cribari-Neto	  The two-parameter distribution known as exponential-Poisson (EP) distribution, which has decreasing failure rate, was introduced by Kus (2007). In this paper we generalize the EP distribution and show that the failure rate of the new distribution can be decreasing or increasing. The failure rate can also be upside-down bathtub shaped. A comprehensive mathematical treatment of the new distribution is provided. We provide closed-form expressions for the density, cumulative distribution, survival and failure rate functions; we also obtain the density of the $i$th order statistic. We derive the $r$th raw moment of the new distribution and also the moments of order statistics. Moreover, we discuss estimation by maximum likelihood and obtain an expression for Fisher's information matrix. Furthermore, expressions for the R\'enyi and Shannon entropies are given and estimation of the stress-strength parameter is discussed. Applications using two real data sets are presented. 	
0902.0534v1	http://arxiv.org/pdf/0902.0534v1	2009	Co-fibered products of algebraic curves	Fedor Bogomolov|Yuri Tschinkel	  We give examples of failure of the existence of co-fibered products in the category of algebraic curves. 	
0912.3188v2	http://arxiv.org/pdf/0912.3188v2	2010	Robust Fault Tolerant uncapacitated facility location	Shiri Chechik|David Peleg	  In the uncapacitated facility location problem, given a graph, a set of demands and opening costs, it is required to find a set of facilities R, so as to minimize the sum of the cost of opening the facilities in R and the cost of assigning all node demands to open facilities. This paper concerns the robust fault-tolerant version of the uncapacitated facility location problem (RFTFL). In this problem, one or more facilities might fail, and each demand should be supplied by the closest open facility that did not fail. It is required to find a set of facilities R, so as to minimize the sum of the cost of opening the facilities in R and the cost of assigning all node demands to open facilities that did not fail, after the failure of up to \alpha facilities. We present a polynomial time algorithm that yields a 6.5-approximation for this problem with at most one failure and a 1.5 + 7.5\alpha-approximation for the problem with at most \alpha > 1 failures. We also show that the RFTFL problem is NP-hard even on trees, and even in the case of a single failure. 	
1002.2268v4	http://arxiv.org/pdf/1002.2268v4	2010	Do topological models provide good information about vulnerability in   electric power networks?	Paul Hines|Eduardo Cotilla-Sanchez|Seth Blumsack	  In order to identify the extent to which results from topological graph models are useful for modeling vulnerability in electricity infrastructure, we measure the susceptibility of power networks to random failures and directed attacks using three measures of vulnerability: characteristic path lengths, connectivity loss and blackout sizes. The first two are purely topological metrics. The blackout size calculation results from a model of cascading failure in power networks. Testing the response of 40 areas within the Eastern US power grid and a standard IEEE test case to a variety of attack/failure vectors indicates that directed attacks result in larger failures using all three vulnerability measures, but the attack vectors that appear to cause the most damage depend on the measure chosen. While our topological and power grid model results show some trends that are similar, there is only a mild correlation between the vulnerability measures for individual simulations. We conclude that evaluating vulnerability in power networks using purely topological metrics can be misleading. 	
1004.1706v1	http://arxiv.org/pdf/1004.1706v1	2010	Enhanced Ad-Hoc on Demand Multipath Distance Vector Routing protocol	Sujata V. Mallapur|Sujata Terdal	  Due to mobility in Ad-Hoc network the topology of the network may change randomly, rapidly and unexpectedly, because of these aspects, the routes in the network often disappear and new to arise. To avoid frequent route discovery and route failure EAOMDV was proposed based on existing routing protocol AOMDV. The EAOMDV (Enhanced Ad-Hoc on Demand Multipath Distance Vector) Routing protocol was proposed to solve the "route failure" problem in AOMDV. EAOMDV protocol reduces the route failure problem by preemptively predicting the link failure by the signal power received by the receiver (pr). This proposed protocol controls overhead, increases throughput and reduces the delay. The EAOMDV protocol was implemented on NS-2 and evaluation results show that the EAOMDV outperformed AOMDV. 	
1009.4166v1	http://arxiv.org/pdf/1009.4166v1	2010	A damage model based on failure threshold weakening	Joseph D. Gran|John B. Rundle|Donald L. Turcotte|James R. Holliday|William Klein	  A variety of studies have modeled the physics of material deformation and damage as examples of generalized phase transitions, involving either critical phenomena or spinodal nucleation. Here we study a model for frictional sliding with long range interactions and recurrent damage that is parameterized by a process of damage and partial healing during sliding. We introduce a failure threshold weakening parameter into the cellular-automaton slider-block model which allows blocks to fail at a reduced failure threshold for all subsequent failures during an event. We show that a critical point is reached beyond which the probability of a system-wide event scales with this weakening parameter. We provide a mapping to the percolation transition, and show that the values of the scaling exponents approach the values for mean-field percolation (spinodal nucleation) as lattice size $L$ is increased for fixed $R$. We also examine the effect of the weakening parameter on the frequency-magnitude scaling relationship and the ergodic behavior of the model. 	
1010.1899v1	http://arxiv.org/pdf/1010.1899v1	2010	The Failure Probability at Sink Node of Random Linear Network Coding	Xuan Guang|Fang-Wei Fu	  In practice, since many communication networks are huge in scale or complicated in structure even dynamic, the predesigned network codes based on the network topology is impossible even if the topological structure is known. Therefore, random linear network coding was proposed as an acceptable coding technique. In this paper, we further study the performance of random linear network coding by analyzing the failure probabilities at sink node for different knowledge of network topology and get some tight and asymptotically tight upper bounds of the failure probabilities. In particular, the worst cases are indicated for these bounds. Furthermore, if the more information about the network topology is utilized, the better upper bounds are obtained. These bounds improve on the known ones. Finally, we also discuss the lower bound of this failure probability and show that it is also asymptotically tight. 	
1011.3550v1	http://arxiv.org/pdf/1011.3550v1	2010	Overlay Protection Against Link Failures Using Network Coding	Ahmed E. Kamal|Aditya Ramamoorthy|Long Long|Shizheng Li	  This paper introduces a network coding-based protection scheme against single and multiple link failures. The proposed strategy ensures that in a connection, each node receives two copies of the same data unit: one copy on the working circuit, and a second copy that can be extracted from linear combinations of data units transmitted on a shared protection path. This guarantees instantaneous recovery of data units upon the failure of a working circuit. The strategy can be implemented at an overlay layer, which makes its deployment simple and scalable. While the proposed strategy is similar in spirit to the work of Kamal '07 & '10, there are significant differences. In particular, it provides protection against multiple link failures. The new scheme is simpler, less expensive, and does not require the synchronization required by the original scheme. The sharing of the protection circuit by a number of connections is the key to the reduction of the cost of protection. The paper also conducts a comparison of the cost of the proposed scheme to the 1+1 and shared backup path protection (SBPP) strategies, and establishes the benefits of our strategy. 	
1011.3638v1	http://arxiv.org/pdf/1011.3638v1	2010	Backward estimation of stochastic processes with failure events as time   origins	Kwun Chuen Gary Chan|Mei-Cheng Wang	  Stochastic processes often exhibit sudden systematic changes in pattern a short time before certain failure events. Examples include increase in medical costs before death and decrease in CD4 counts before AIDS diagnosis. To study such terminal behavior of stochastic processes, a natural and direct way is to align the processes using failure events as time origins. This paper studies backward stochastic processes counting time backward from failure events, and proposes one-sample nonparametric estimation of the mean of backward processes when follow-up is subject to left truncation and right censoring. We will discuss benefits of including prevalent cohort data to enlarge the identifiable region and large sample properties of the proposed estimator with related extensions. A SEER--Medicare linked data set is used to illustrate the proposed methodologies. 	
1011.4098v1	http://arxiv.org/pdf/1011.4098v1	2010	Understanding Cascading Failures in Power Grids	Sachin Kadloor|Nandakishore Santhi	  In the past, we have observed several large blackouts, i.e. loss of power to large areas. It has been noted by several researchers that these large blackouts are a result of a cascade of failures of various components. As a power grid is made up of several thousands or even millions of components (relays, breakers, transformers, etc.), it is quite plausible that a few of these components do not perform their function as desired. Their failure/misbehavior puts additional burden on the working components causing them to misbehave, and thus leading to a cascade of failures.   The complexity of the entire power grid makes it difficult to model each and every individual component and study the stability of the entire system. For this reason, it is often the case that abstract models of the working of the power grid are constructed and then analyzed. These models need to be computationally tractable while serving as a reasonable model for the entire system. In this work, we construct one such model for the power grid, and analyze it. 	
1106.0235v1	http://arxiv.org/pdf/1106.0235v1	2011	Robust Agent Teams via Socially-Attentive Monitoring	G. A. Kaminka|M. Tambe	  Agents in dynamic multi-agent environments must monitor their peers to execute individual and group plans. A key open question is how much monitoring of other agents' states is required to be effective: The Monitoring Selectivity Problem. We investigate this question in the context of detecting failures in teams of cooperating agents, via Socially-Attentive Monitoring, which focuses on monitoring for failures in the social relationships between the agents. We empirically and analytically explore a family of socially-attentive teamwork monitoring algorithms in two dynamic, complex, multi-agent domains, under varying conditions of task distribution and uncertainty. We show that a centralized scheme using a complex algorithm trades correctness for completeness and requires monitoring all teammates. In contrast, a simple distributed teamwork monitoring algorithm results in correct and complete detection of teamwork failures, despite relying on limited, uncertain knowledge, and monitoring only key agents in a team. In addition, we report on the design of a socially-attentive monitoring system and demonstrate its generality in monitoring several coordination relationships, diagnosing detected failures, and both on-line and off-line applications. 	
1106.3579v2	http://arxiv.org/pdf/1106.3579v2	2012	Consensus vs Broadcast in Communication Networks with Arbitrary Mobile   Omission Faults	Emmanuel Godard|Joseph Peters	  We compare the solvability of the Consensus and Broadcast problems in synchronous communication networks in which the delivery of messages is not reliable. The failure model is the mobile omission faults model. During each round, some messages can be lost and the set of possible simultaneous losses is the same for each round. We investigate these problems for the first time for arbitrary sets of possible failures. Previously, these sets were defined by bounding the numbers of failures.   In this setting, we present a new necessary condition for the solvability of Consensus that unifies previous impossibility results in this area. This condition is expressed using Broadcastability properties. As a very important application, we show that when the sets of omissions that can occur are defined by bounding the numbers of failures, counted in any way (locally, globally, etc.), then the Consensus problem is actually equivalent to the Broadcast problem. 	
1108.1426v1	http://arxiv.org/pdf/1108.1426v1	2011	Efficient Algorithms to Enhance Recovery Schema in Link State Protocols	Radwan Abujassar|Mohammed Ghanbari	  With the increasing demands for real-time applications traffic in net- works such as video and voice a high convergence time for the existing routing protocols when failure occurred is required. These applications can be very sensitive to packet loss when link/node goes down. In this paper, we propose two algorithms schemas for the link state protocol to reroute the traffic in two states; first, pre-calculated an alternative and disjoint path with the primary one from the source to the destination by re-routing traffic through it, regardless of the locations of failure and the number of failed links. Second, rerouting the traffic via an alternative path from a node whose local link is down without the need to wait until the source node knows about the failure. This is achieved by creating a new backup routing table based on the original routing table which is computed by the dijkstra algorithm. The goal of these algorithms is to reduce loss of packets, end-to-end delay time, improve throughput and avoiding local loop when nodes re-converge the topology in case of failure. 	
1109.6646v1	http://arxiv.org/pdf/1109.6646v1	2011	A Non-MDS Erasure Code Scheme For Storage Applications	Abbas Kiani|Soroush Akhlaghi	  This paper investigates the use of redundancy and self repairing against node failures in distributed storage systems, using various strategies. In replication method, access to one replication node is sufficient to reconstruct a lost node, while in MDS erasure coded systems which are optimal in terms of redundancy-reliability tradeoff, a single node failure is repaired after recovering the entire stored data. Moreover, regenerating codes yield a tradeoff curve between storage capacity and repair bandwidth. The current paper aims at investigating a new storage code. Specifically, we propose a non-MDS (2k, k) code that tolerates any three node failures and more importantly, it is shown using our code a single node failure can be repaired through access to only three nodes. 	
1202.2291v2	http://arxiv.org/pdf/1202.2291v2	2012	Onset of Localization in Heterogeneous Interfacial Failure	Arne Stormo|Knut Skogstrand Gjerden|Alex Hansen	  We study numerically the failure of an interface joining two elastic materials under load using a fiber bundle model connected to an elastic half space. We find that the breakdown process follows the equal load sharing fiber bundle model without any detectable spatial correlations between the positions of the failing fibers until localization sets in. The onset of localization is an instability, not a phase transition. Depending on the elastic constant describing the elastic half space, localization sets in before or after the critical load causing the interface to fail completely, is reached. There is a crossover between failure due to localization or failure without spatial correlations when tuning the elastic constant, not a phase transition. Contrary to earlier claims based on models different from ours, we find that a finite fraction of fibers must fail before the critical load is attained, even in the extreme localization regime, i.e.\ for very small elastic constant. We furthermore find that the critical load remains finite for all values of the elastic constant in the limit of an infinitely large system. 	
1203.0029v2	http://arxiv.org/pdf/1203.0029v2	2012	Assortativity Decreases the Robustness of Interdependent Networks	Di Zhou|Gregorio D'Agostino|Antonio Scala|H. Eugene Stanley	  It was recently recognized that interdependencies among different networks can play a crucial role in triggering cascading failures and hence system-wide disasters. A recent model shows how pairs of interdependent networks can exhibit an abrupt percolation transition as failures accumulate. We report on the effects of topology on failure propagation for a model system consisting of two interdependent networks. We find that the internal node correlations in each of the two interdependent networks significantly changes the critical density of failures that triggers the total disruption of the two-network system. Specifically, we find that the assortativity (i.e. the likelihood of nodes with similar degree to be connected) within a single network decreases the robustness of the entire system. The results of this study on the influence of assortativity may provide insights into ways of improving the robustness of network architecture, and thus enhances the level of protection of critical infrastructures. 	
1204.0285v1	http://arxiv.org/pdf/1204.0285v1	2012	Semiparametric Multivariate Accelerated Failure Time Model with   Generalized Estimating Equations	Steven Chiou|Junghi Kim|Jun Yan	  The semiparametric accelerated failure time model is not as widely used as the Cox relative risk model mainly due to computational difficulties. Recent developments in least squares estimation and induced smoothing estimating equations provide promising tools to make the accelerate failure time models more attractive in practice. For semiparametric multivariate accelerated failure time models, we propose a generalized estimating equation approach to account for the multivariate dependence through working correlation structures. The marginal error distributions can be either identical as in sequential event settings or different as in parallel event settings. Some regression coefficients can be shared across margins as needed. The initial estimator is a rank-based estimator with Gehan's weight, but obtained from an induced smoothing approach with computation ease. The resulting estimator is consistent and asymptotically normal, with a variance estimated through a multiplier resampling method. In a simulation study, our estimator was up to three times as efficient as the initial estimator, especially with stronger multivariate dependence and heavier censoring percentage. Two real examples demonstrate the utility of the proposed method. 	
1210.4973v3	http://arxiv.org/pdf/1210.4973v3	2013	Cascading Failures in Bi-partite Graphs: Model for Systemic Risk   Propagation	Xuqing Huang|Irena Vodenska|Shlomo Havlin|H. Eugene Stanley	  As economic entities become increasingly interconnected, a shock in a financial network can provoke significant cascading failures throughout the system. To study the systemic risk of financial systems, we create a bi-partite banking network model composed of banks and bank assets and propose a cascading failure model to describe the risk propagation process during crises. We empirically test the model with 2007 US commercial banks balance sheet data and compare the model prediction of the failed banks with the real failed banks after 2007. We find that our model efficiently identifies a significant portion of the actual failed banks reported by Federal Deposit Insurance Corporation. The results suggest that this model could be useful for systemic risk stress testing for financial systems. The model also identifies that commercial rather than residential real estate assets are major culprits for the failure of over 350 US commercial banks during 2008-2011. 	
1211.5499v1	http://arxiv.org/pdf/1211.5499v1	2012	Experimental Investigation of Plastic Deformations Before Granular   Avalanche	Axelle Amon|Roman Bertoni|Jérôme Crassous	  We present an experimental study of the deformation inside a granular material that is progressively tilted. We investigate the deformation before the avalanche with a spatially resolved Diffusive Wave Spectroscopy setup. At the beginning of the inclination process, we first observe localized and isolated events in the bulk, with a density which decreases with the depth. As the angle of inclination increases, series of micro-failures occur periodically in the bulk, and finally a granular avalanche takes place. The micro-failures are observed only when the tilt angles are larger than a threshold angle much smaller than the granular avalanche angle. We have characterized the density of reorganizations and the localization of micro-failures. We have also explored the effect of the nature of the grains, the relative humidity conditions and the packing fraction of the sample. We discuss those observations in the framework of the plasticity of granular matter. Micro-failures may then be viewed as the result of the accumulation of numerous plastic events. 	
1212.3295v2	http://arxiv.org/pdf/1212.3295v2	2012	Measures of Fault Tolerance in Distributed Simulated Annealing	Aaditya Prakash	  In this paper, we examine the different measures of Fault Tolerance in a Distributed Simulated Annealing process. Optimization by Simulated Annealing on a distributed system is prone to various sources of failure. We analyse simulated annealing algorithm, its architecture in distributed platform and potential sources of failures. We examine the behaviour of tolerant distributed system for optimization task. We present possible methods to overcome the failures and achieve fault tolerance for the distributed simulated annealing process. We also examine the implementation of Simulated Annealing in MapReduce system and possible ways to prevent failures in reaching the global optima. This paper will be beneficial to those who are interested in implementing a large scale distributed simulated annealing optimization problem of industrial or academic interest. We recommend hybrid tolerance technique to optimize the trade-off between efficiency and availability. 	
1212.4297v1	http://arxiv.org/pdf/1212.4297v1	2012	On Local and Global Conjugacy	Song Wang	  In this note we discuss and classify LFMO-spcial representations, for which under certain functoriality, it gives the instance of failure of multiplicity one. 	
1212.5615v1	http://arxiv.org/pdf/1212.5615v1	2012	Beta-Linear Failure Rate Distribution and its Applications	Ali Akbar Jafari|Eisa Mahmoudi	  We introduce in this paper a new four-parameter generalized version of the linear failure rate (LFR) distribution which is called Beta-linear failure rate (BLFR) distribution. The new distribution is quite flexible and can be used effectively in modeling survival data and reliability problems. It can have a constant, decreasing, increasing, upside-down bathtub (unimodal) and bathtub-shaped failure rate function depending on its parameters. It includes some well-known lifetime distributions as special submodels. We provide a comprehensive account of the mathematical properties of the new distributions. In particular, A closed-form expressions for the density, cumulative distribution and hazard rate function of the BLFR is given. Also, the $r$th order moment of this distribution is derived. We discuss maximum likelihood estimation of the unknown parameters of the new model for complete sample and obtain an expression for Fishers information matrix. In the end, to show the flexibility of this distribution and illustrative purposes, an application using a real data set is presented. 	
1302.4147v2	http://arxiv.org/pdf/1302.4147v2	2013	The Failure Probability of Random Linear Network Coding for Networks	Xuan Guang|Fang-Wei Fu	  In practice, since many communication networks are huge in scale, or complicated in structure, or even dynamic, the predesigned linear network codes based on the network topology is impossible even if the topological structure is known. Therefore, random linear network coding has been proposed as an acceptable coding technique for the case that the network topology cannot be utilized completely. Motivated by the fact that different network topological information can be obtained for different practical applications, we study the performance analysis of random linear network coding by analyzing some failure probabilities depending on these different topological information of networks. We obtain some tight or asymptotically tight upper bounds on these failure probabilities and indicate the worst cases for these bounds, i.e., the networks meeting the upper bounds with equality. In addition, if the more topological information of the network is utilized, the better upper bounds are obtained. On the other hand, we also discuss the lower bounds on the failure probabilities. 	
1304.0423v1	http://arxiv.org/pdf/1304.0423v1	2013	Reliability sensitivity analysis based on probability distribution   perturbation with application to CO2 storage	Ekaterina Sergienko|Paul Lemaître|Aurélie Arnaud|Daniel Busby|Fabrice Gamboa	  The objective of reliability sensitivity analysis is to determine input variables that mostly contribute to the variability of the failure probability. In this paper, we study a recently introduced method for the reliability sensitivity analysis based on a perturbation of the original probability distribution of the input variables. The objective is to determine the most influential input variables and to analyze their impact on the failure probability. We propose a moment independent sensitivity measure that is based on a perturbation of the original probability density independently for each input variable. The variables providing the highest variation of the original failure probability are settled to be more influential. These variables will need a proper characterization in terms of uncertainty. The method is intended to work in applications involving a computationally expensive simulation code for evaluating the failure probability such as the CO2 storage risk analysis. An application of the method to a synthetic CO2 storage case study is provided together with some analytical examples 	
1307.0412v1	http://arxiv.org/pdf/1307.0412v1	2013	Characterizing and Predicting the Robustness of Power-law Networks	Sarah LaRocca|Seth Guikema	  Power-law networks such as the Internet, terrorist cells, species relationships, and cellular metabolic interactions are susceptible to node failures, yet maintaining network connectivity is essential for network functionality. Disconnection of the network leads to fragmentation and, in some cases, collapse of the underlying system. However, the influences of the topology of networks on their ability to withstand node failures are poorly understood. Based on a study of the response of 2,000 power-law networks to node failures, we find that networks with higher nodal degree and clustering coefficient, lower betweenness centrality, and lower variability in path length and clustering coefficient maintain their cohesion better during such events. We also find that network robustness, i.e., the ability to withstand node failures, can be accurately predicted a priori for power-law networks across many fields. These results provide a basis for designing new, more robust networks, improving the robustness of existing networks such as the Internet and cellular metabolic pathways, and efficiently degrading networks such as terrorist cells. 	
1308.0174v1	http://arxiv.org/pdf/1308.0174v1	2013	MATCASC: A tool to analyse cascading line outages in power grids	Yakup Koç|Trivik Verma|Nuno A. M. Araujo|Martijn Warnier	  Blackouts in power grids typically result from cascading failures. The key importance of the electric power grid to society encourages further research into sustaining power system reliability and developing new methods to manage the risks of cascading blackouts. Adequate software tools are required to better analyze, understand, and assess the consequences of the cascading failures. This paper presents MATCASC, an open source MATLAB based tool to analyse cascading failures in power grids. Cascading effects due to line overload outages are considered. The applicability of the MATCASC tool is demonstrated by assessing the robustness of IEEE test systems and real-world power grids with respect to cascading failures. 	
1404.5406v1	http://arxiv.org/pdf/1404.5406v1	2014	Degradation Analysis of Probabilistic Parallel Choice Systems	Avinash Saxena|Shrisha Rao	  Degradation analysis is used to analyze the useful lifetimes of systems, their failure rates, and various other system parameters like mean time to failure (MTTF), mean time between failures (MTBF), and the system failure rate (SFR). In many systems, certain possible parallel paths of execution that have greater chances of success are preferred over others. Thus we introduce here the concept of probabilistic parallel choice. We use binary and $n$-ary probabilistic choice operators in describing the selections of parallel paths. These binary and $n$-ary probabilistic choice operators are considered so as to represent the complete system (described as a series-parallel system) in terms of the probabilities of selection of parallel paths and their relevant parameters. Our approach allows us to derive new and generalized formulae for system parameters like MTTF, MTBF, and SFR. We use a generalized exponential distribution, allowing distinct installation times for individual components, and use this model to derive expressions for such system parameters. 	
1405.0317v1	http://arxiv.org/pdf/1405.0317v1	2014	Robustness of Cucker-Smale flocking model	Eduardo Canale|Federico Dalmao|Ernesto Mordecki|Max Souza	  Consider a system of autonomous interacting agents moving in space, adjusting each own velocity as a weighted mean of the relative velocities of the other agents. In order to test the robustness of the model, we assume that each pair of agents, at each time step, can fail to connect with certain probability, the failure rate. This is a modification of the (deterministic) Flocking model introduced by Cucker and Smale in Emergent behavior in flocks, IEEE Trans. on Autom. Control, 2007, 52 (May) pp. 852-862. We prove that, if this random failures are independent in time and space, and have linear or sub-linear distance dependent rate of decay, the characteristic behavior of flocking exhibited by the original deterministic model, also holds true under random failures, for all failure rates. 	
1405.2866v1	http://arxiv.org/pdf/1405.2866v1	2014	Mitigating Cascading Failures in Interdependent Power Grids and   Communication Networks	Marzieh Parandehgheibi|Eytan Modiano|David Hay	  In this paper, we study the interdependency between the power grid and the communication network used to control the grid. A communication node depends on the power grid in order to receive power for operation, and a power node depends on the communication network in order to receive control signals for safe operation. We demonstrate that these dependencies can lead to cascading failures, and it is essential to consider the power flow equations for studying the behavior of such interdependent networks. We propose a two-phase control policy to mitigate the cascade of failures. In the first phase, our control policy finds the non-avoidable failures that occur due to physical disconnection. In the second phase, our algorithm redistributes the power so that all the connected communication nodes have enough power for operation and no power lines overload. We perform a sensitivity analysis to evaluate the performance of our control policy, and show that our control policy achieves close to optimal yield for many scenarios. This analysis can help design robust interdependent grids and associated control policies. 	
1408.6856v1	http://arxiv.org/pdf/1408.6856v1	2014	A multilevel Monte Carlo method for computing failure probabilities	Daniel Elfverson|Fredrik Hellman|Axel Målqvist	  We propose and analyze a method for computing failure probabilities of systems modeled as numerical deterministic models (e.g., PDEs) with uncertain input data. A failure occurs when a functional of the solution to the model is below (or above) some critical value. By combining recent results on quantile estimation and the multilevel Monte Carlo method we develop a method which reduces computational cost without loss of accuracy. We show how the computational cost of the method relates to error tolerance of the failure probability. For a wide and common class of problems, the computational cost is asymptotically proportional to solving a single accurate realization of the numerical model, i.e., independent of the number of samples. Significant reductions in computational cost are also observed in numerical experiments. 	
1501.04000v1	http://arxiv.org/pdf/1501.04000v1	2015	Large deformation and post-failure simulations of segmental retaining   walls using mesh-free method (SPH)	H. H. Bui|J. A. Kodikara|R. Pathegama|A. Bouazza|A. Haque	  Numerical methods are extremely useful in gaining insights into the behaviour of reinforced soil retaining walls. However, traditional numerical approaches such as limit equilibrium or finite element methods are unable to simulate large deformation and post-failure behaviour of soils and retaining wall blocks in the reinforced soil retaining walls system. To overcome this limitation, a novel numerical approach is developed aiming to predict accurately the large deformation and post-failure behaviour of soil and segmental wall blocks. Herein, soil is modelled using an elasto-plastic constitutive model, while segmental wall blocks are assumed rigid with full degrees of freedom. A soft contact model is proposed to simulate the interaction between soil-block and block-block. A two dimensional experiment of reinforced soil retaining walls collapse was conducted to verify the numerical results. It is shown that the proposed method can simulate satisfactory post-failure behaviour of segmental wall blocks in reinforced soil retaining wall systems. The comparison showed that the proposed method can provide satisfactory agreement with experiments. 	
1502.01496v1	http://arxiv.org/pdf/1502.01496v1	2015	Assisting V2V failure recovery using Device-to-Device Communications	Emad Abd-Elrahman|Adel Mounir Said|Thouraya Toukabri|Hossam Afifi|Michel Marot	  This paper aims to propose a new solution for failure recovery (dead-ends) in Vehicle to Vehicle (V2V) communications through LTE-assisted Device-to-Device communications (D2D). Based on the enhanced networking capabilities offered by Intelligent Transportation Systems (ITS) architecture, our solution can efficiently assist V2V communications in failure recovery situations. We also derive an analytical model to evaluate generic V2V routing recovery failures. Moreover, the proposed hybrid model is simulated and compared to the generic model under different constrains of worst and best cases of D2D discovery and communication. According to our comparison and simulation results, the hybrid model decreases the delay for alarm message propagation to the destination (typically the Traffic Control Center TCC) through the Road Side Unit (RSU) 	
1504.00856v3	http://arxiv.org/pdf/1504.00856v3	2015	Robust Control of Cascading Power Grid Failures using Stochastic   Approximation	Daniel Bienstock|Guy Grebla	  Cascading failure of a power transmission system are initiated by an exogenous event that disable a set of elements (e.g., lines) followed by a sequence of interrelated failures (or more precisely, trips) of overloaded elements caused by the combination of physics of power flows in the changed system topology, and controls. Should this sequence accelerate it can lead to a large system failure with significant loss of load. In previous work we have analyzed deterministic algorithms that in an online fashion (i.e., responding to observed data) selectively shed load so as to minimize the amount of lost load at termination of the cascade. In this work we present a rigorous methodology for incorporating noise and model errors, based on the Sample Average Approximation methodology for stochastic optimization. 	
1505.02648v1	http://arxiv.org/pdf/1505.02648v1	2015	Towards Formal Fault Tree Analysis using Theorem Proving	Waqar Ahmed|Osman Hasan	  Fault Tree Analysis (FTA) is a dependability analysis technique that has been widely used to predict reliability, availability and safety of many complex engineering systems. Traditionally, these FTA-based analyses are done using paper-and-pencil proof methods or computer simulations, which cannot ascertain absolute correctness due to their inherent limitations. As a complementary approach, we propose to use the higher-order-logic theorem prover HOL4 to conduct the FTA-based analysis of safety-critical systems where accuracy of failure analysis is a dire need. In particular, the paper presents a higher-order-logic formalization of generic Fault Tree gates, i.e., AND, OR, NAND, NOR, XOR and NOT and the formal verification of their failure probability expressions. Moreover, we have formally verified the generic probabilistic inclusion-exclusion principle, which is one of the foremost requirements for conducting the FTA-based failure analysis of any given system. For illustration purposes, we conduct the FTA-based failure analysis of a solar array that is used as the main source of power for the Dong Fang Hong-3 (DFH-3) satellite. 	
1507.07155v1	http://arxiv.org/pdf/1507.07155v1	2015	Majority Logic Decoding under Data-Dependent Logic Gate Failures	Srdan Brkic|Predrag Ivanis|Bane Vasic	  A majority logic decoder made of unreliable logic gates, whose failures are transient and datadependent, is analyzed. Based on a combinatorial representation of fault configurations a closed-form expression for the average bit error rate for an one-step majority logic decoder is derived, for a regular low-density parity-check (LDPC) code ensemble and the proposed failure model. The presented analysis framework is then used to establish bounds on the one-step majority logic decoder performance under the simplified probabilistic gate-output switching model. Based on the expander property of Tanner graphs of LDPC codes, it is proven that a version of the faulty parallel bit flipping decoder can correct a fixed fraction of channel errors in the presence of data-dependent gate failures. The results are illustrated with numerical examples of finite geometry codes. 	
1508.01691v1	http://arxiv.org/pdf/1508.01691v1	2015	Your Proof Fails? Testing Helps to Find the Reason	Guillaume Petiot|Nikolai Kosmatov|Bernard Botella|Alain Giorgetti|Jacques Julliand	  Applying deductive verification to formally prove that a program respects its formal specification is a very complex and time-consuming task due in particular to the lack of feedback in case of proof failures. Along with a non-compliance between the code and its specification (due to an error in at least one of them), possible reasons of a proof failure include a missing or too weak specification for a called function or a loop, and lack of time or simply incapacity of the prover to finish a particular proof. This work proposes a new methodology where test generation helps to identify the reason of a proof failure and to exhibit a counter-example clearly illustrating the issue. We describe how to transform an annotated C program into C code suitable for testing and illustrate the benefits of the method on comprehensive examples. The method has been implemented in STADY, a plugin of the software analysis platform FRAMA-C. Initial experiments show that detecting non-compliances and contract weaknesses allows to precisely diagnose most proof failures. 	
1508.01734v1	http://arxiv.org/pdf/1508.01734v1	2015	Can complexity decrease in Congestive Heart failure?	Sayan Mukherjee|Sanjay Kumar Palit|Santo Banerjee|M. R. K. Ariffin|Lamberto Rondoni|D. K. Bhattacharya	  The complexity of a signal can be measured by the Recurrence period density entropy (RPDE) from the reconstructed phase space. We have chosen a window based RPDE method for the classification of signals, as RPDE is an average entropic measure of the whole phase space. We have observed the changes in the complexity in cardiac signals of normal healthy person (NHP) and congestive heart failure patients (CHFP). The results show that the cardiac dynamics of a healthy subject is more complex and random compare to the same for a heart failure patient, whose dynamics is more deterministic. We have constructed a general threshold to distinguish the border line between a healthy and a congestive heart failure dynamics. The results may be useful for wide range for physiological and biomedical analysis. 	
1601.02923v1	http://arxiv.org/pdf/1601.02923v1	2016	Cascading Edge Failures: A Dynamic Network Process	June Zhang|José M. F. Moura	  This paper considers the dynamics of edges in a network. The Dynamic Bond Percolation (DBP) process models, through stochastic local rules, the dependence of an edge $(a,b)$ in a network on the states of its neighboring edges. Unlike previous models, DBP does not assume statistical independence between different edges. In applications, this means for example that failures of transmission lines in a power grid are not statistically independent, or alternatively, relationships between individuals (dyads) can lead to changes in other dyads in a social network. We consider the time evolution of the probability distribution of the network state, the collective states of all the edges (bonds), and show that it converges to a stationary distribution. We use this distribution to study the emergence of global behaviors like consensus (i.e., catastrophic failure or full recovery of the entire grid) or coexistence (i.e., some failed and some operating substructures in the grid). In particular, we show that, depending on the local dynamical rule, different network substructures, such as hub or triangle subgraphs, are more prone to failure. 	
1602.01788v2	http://arxiv.org/pdf/1602.01788v2	2016	Semiparametric Regression Analysis of Interval-Censored Competing Risks   Data	Lu Mao|D. Y. Lin|Donglin Zeng	  Interval-censored competing risks data arise when each study subject may experience an event or failure from one of several causes and the failure time is not observed exactly but rather known to lie in an interval between two successive examinations. We formulate the effects of possibly time-varying covariates on the cumulative incidence or sub-distribution function (i.e., the marginal probability of failure from a particular cause) of competing risks through a broad class of semiparametric regression models that captures both proportional and non-proportional hazards structures for the sub-distribution. We allow each subject to have an arbitrary number of examinations and accommodate missing information on the cause of failure. We consider nonparametric maximum likelihood estimation and devise a fast and stable EM-type algorithm for its computation. We then establish the consistency, asymptotic normality, and semiparametric efficiency of the resulting estimators by appealing to modern empirical process theory. In addition, we show through extensive simulation studies that the proposed methods perform well in realistic situations. Finally, we provide an application to a study on HIV-1 infection with different viral subtypes. 	
1603.06461v1	http://arxiv.org/pdf/1603.06461v1	2016	Remote Antenna Unit Selection Assisted Seamless Handover for High-Speed   Railway Communications with Distributed Antennas	Yang Lu|Ke Xiong|Zhuyan Zhao|Pingyi Fan|Zhangdui Zhong	  To attain seamless handover and reduce the han- dover failure probability for high-speed railway (HSR) com- munication systems, this paper proposes a remote antenna unit (RAU) selection assisted handover scheme where two antennas are installed on high speed train (HST) and distributed antenna system (DAS) cell architecture on ground is adopted. The RAU selection is used to provide high quality received signals for trains moving in DAS cells and the two HST antennas are employed on trains to realize seamless handover. Moreover, to efficiently evaluate the system performance, a new met- ric termed as handover occurrence probability is defined for describing the relation between handover occurrence position and handover failure probability. We then analyze the received signal strength, the handover trigger probability, the handover occurrence probability, the handover failure probability and the communication interruption probability. Numerical results are provided to compare our proposed scheme with the current existing ones. It is shown that our proposed scheme achieves better performances in terms of handover failure probability and communication interruption probability. 	
1607.07286v3	http://arxiv.org/pdf/1607.07286v3	2017	Session Types for Link Failures (Technical Report)	Manuel Adameit|Kirstin Peters|Uwe Nestmann	  We strive to use session type technology to prove behavioural properties of fault-tolerant distributed algorithms. Session types are designed to abstractly capture the structure of (even multi-party) communication protocols. The goal of session types is the analysis and verification of the protocols' behavioural properties. One important such property is progress, i.e., the absence of (unintended) deadlock. Distributed algorithms often resemble (compositions of) multi-party communication protocols. In contrast to protocols that are typically studied with session types, they are often designed to cope with system failures. An essential behavioural property is (successful) termination, despite failures, but it is often elaborate to prove for distributed algorithms.   We extend multi-party session types (and multi-party session types with nested sessions) by optional blocks that cover a limited class of link and crash failures. This allows us to automatically derive termination of distributed algorithms that come within these limits. To illustrate our approach, we prove termination for an implementation of the *rotating coordinator* Consensus algorithm. 	
1607.08322v1	http://arxiv.org/pdf/1607.08322v1	2016	Cooperative Repair of Multiple Node Failures in Distributed Storage   Systems	Kenneth W. Shum|Junyu Chen	  Cooperative regenerating codes are designed for repairing multiple node failures in distributed storage systems. In contrast to the original repair model of regenerating codes, which are for the repair of single node failure, data exchange among the new nodes is enabled. It is known that further reduction in repair bandwidth is possible with cooperative repair. Currently in the literature, we have an explicit construction of exact-repair cooperative code achieving all parameters corresponding to the minimum-bandwidth point. We give a slightly generalized and more flexible version of this cooperative regenerating code in this paper. For minimum-storage regeneration with cooperation, we present an explicit code construction which can jointly repair any number of systematic storage nodes. 	
1612.04922v2	http://arxiv.org/pdf/1612.04922v2	2017	A Continuum Description of Failure Waves	Hamid A. Said|James Glimm	  Shattering of a brittle material such as glass occurs dynamically through a propagating failure wave, which however, can not be assigned to any of the classical waves of the elasto-plastic theories of materials. Such failure waves have been a topic of research for decades. In this paper, we build a thermodynamically consistent theory based on the idea that a failure wave is analogous to a deflagration wave. Our theory admits, as special cases, the classical models of Feng and Clifton. Two fundamental thermodynamic functions (the free energy and the entropy production rate) form the basis of our theory. Such a two-function approach allows for the construction of a new variational principle and a new Lagrangian formulation that produce the equations of motion. Finally, a linearization of this theory is examined to gain insight into the coupling between the diffusive and elastic wave phenomena. 	
1701.02641v1	http://arxiv.org/pdf/1701.02641v1	2017	Distributed Algorithm for Collision Avoidance at Road Intersections in   the Presence of Communication Failures	Vladimir Savic|Elad M. Schiller|Marina Papatriantafilou	  Vehicle-to-vehicle (V2V) communication is a crucial component of the future autonomous driving systems since it enables improved awareness of the surrounding environment, even without extensive processing of sensory information. However, V2V communication is prone to failures and delays, so a distributed fault-tolerant approach is required for safe and efficient transportation. In this paper, we focus on the intersection crossing (IC) problem with autonomous vehicles that cooperate via V2V communications, and propose a novel distributed IC algorithm that can handle an unknown number of communication failures. Our analysis shows that both safety and liveness requirements are satisfied in all realistic situations. We also found, based on a real data set, that the crossing delay is only slightly increased even in the presence of highly correlated failures. 	
1701.03292v1	http://arxiv.org/pdf/1701.03292v1	2017	Structural instability of large-scale functional networks	Shogo Mizutaka|Kousuke Yakubo	  We study how large functional networks can grow stably under possible cascading overload failures and evaluated the maximum stable network size above which even a small-scale failure would cause a fatal breakdown of the network. Employing a model of cascading failures induced by temporally fluctuating loads, the maximum stable size $n_{\text{max}}$ has been calculated as a function of the load reduction parameter $r$ that characterizes how quickly the total load is reduced during the cascade. If we reduce the total load sufficiently fast ($r\ge r_{\text{c}}$), the network can grow infinitely. Otherwise, $n_{\text{max}}$ is finite and increases with $r$. For a fixed $r\,(<r_{\text{c}})$, $n_{\text{max}}$ for a scale-free network is larger than that for an exponential network with the same average degree. We also discuss how one detects and avoids the crisis of a fatal breakdown of the network from the relation between the sizes of the initial network and the largest component after an ordinarily occurring cascading failure. 	
1701.05983v1	http://arxiv.org/pdf/1701.05983v1	2017	A Minimum Reconfiguration Probability Routing Algorithm for RWA in   All-Optical Networks	Mohan Kumar S|Jagadeesha SN	  In this paper, we present a detailed study of Minimum Reconfiguration Probability Routing (MRPR) algorithm, and its performance evaluation in comparison with Adaptive unconstrained routing (AUR) and Least Loaded routing (LLR) algorithms. We have minimized the effects of failures on link and router failure in the network under changing load conditions, we assess the probability of service and number of light path failures due to link or route failure on Wavelength Interchange(WI) network. The computation complexity is reduced by using Kalman Filter(KF) techniques. The minimum reconfiguration probability routing (MRPR) algorithm selects most reliable routes and assign wavelengths to connections in a manner that utilizes the light path(LP) established efficiently considering all possible requests. 	
1703.05232v1	http://arxiv.org/pdf/1703.05232v1	2017	Modeling and Identification of Worst-Case Cascading Failures in Power   Systems	Chao Zhai|Hehong Zhang|Gaoxi Xiao|Tso-Chien Pan	  Cascading failures in power systems normally occur as a result of initial disturbance or faults on electrical elements, closely followed by errors of human operators. It remains a great challenge to systematically trace the source of cascading failures in power systems. In this paper, we develop a mathematical model to describe the cascading dynamics of transmission lines in power networks. In particular, the direct current (DC) power flow equation is employed to calculate the transmission power on the branches. By regarding the disturbances on the elements as the control inputs, we formulate the problem of determining the initial disturbances causing the cascading blackout of power grids in the framework of optimal control theory, and the magnitude of disturbances or faults on the selected branch can be obtained by solving the system of algebraic equations. Moreover, an iterative search algorithm is proposed to look for the optimal solution leading to the worst case of cascading failures. Theoretical analysis guarantees the asymptotic convergence of the iterative search algorithm. Finally, numerical simulations are carried out in IEEE 9 Bus System and IEEE 14 Bus System to validate the proposed approach. 	
1705.04453v1	http://arxiv.org/pdf/1705.04453v1	2017	The Geometry of Limit State Function Graphs and Subset Simulation	Karl Breitung	  In the last fifteen the subset sampling method has often been used in reliability problems as a tool for calculating small probabilities. This method is extrapolating from an initial Monte Carlo estimate for the probability content of a failure domain found by a suitable higher level of the original limit state function. Then iteratively conditional probabilities are estimated for failures domains decreasing to the original failure domain.   But there are assumptions not immediately obvious about the structure of the failure domains which must be fulfilled that the method works properly. Here examples are studied that show that at least in some cases if these premises are not fulfilled, inaccurate results may be obtained. For the further development of the subset sampling method it is certainly desirable to find approaches where it is possible to check that these implicit assumptions are not violated. Also it would be probably important to develop further improvements of the concept to get rid of these limitations. 	
1705.09411v1	http://arxiv.org/pdf/1705.09411v1	2017	Identifying Critical Risks of Cascading Failures in Power Systems	Hehong Zhang|Chao Zhai|Gaoxi Xiao|Tso-Chien Pan	  Potential critical risks of cascading failures in power systems can be identified by exposing those critical electrical elements on which certain initial disturbances may cause maximum disruption to power transmission networks. In this work, we investigate cascading failures in power systems described by the direct current (DC) power flow equations, while initial disturbances take the form of altering admittance of elements. The disruption is quantified with the remaining transmission power at the end of cascading process. In particular, identifying the critical elements and the corresponding initial disturbances causing the worst-case cascading blackout is formulated as a dynamic optimization problem (DOP) in the framework of optimal control theory, where the entire propagation process of cascading failures is put under consideration. An Identifying Critical Risk Algorithm (ICRA) based on the maximum principle is proposed to solve the DOP. Simulation results on the IEEE 9-Bus and the IEEE 14-Bus test systems are presented to demonstrate the effectiveness of the algorithm. 	
1707.02327v1	http://arxiv.org/pdf/1707.02327v1	2017	Why Modern Open Source Projects Fail	Jailton Coelho|Marco Tulio Valente	  Open source is experiencing a renaissance period, due to the appearance of modern platforms and workflows for developing and maintaining public code. As a result, developers are creating open source software at speeds never seen before. Consequently, these projects are also facing unprecedented mortality rates. To better understand the reasons for the failure of modern open source projects, this paper describes the results of a survey with the maintainers of 104 popular GitHub systems that have been deprecated. We provide a set of nine reasons for the failure of these open source projects. We also show that some maintenance practices -- specifically the adoption of contributing guidelines and continuous integration -- have an important association with a project failure or success. Finally, we discuss and reveal the principal strategies developers have tried to overcome the failure of the studied projects. 	
1707.09516v1	http://arxiv.org/pdf/1707.09516v1	2017	Comparing Different Models for Investigating Cascading Failures in Power   Systems	Chao Zhai|Hehong Zhang|Gaoxi Xiao|Tso-Chien Pan	  This paper centers on the comparison of three different models that describe cascading failures of power systems. Specifically, these models are different in characterizing the physical properties of power networks and computing the branch power flow. Optimal control approach is applied on these models to identify the critical disturbances that result in the worst-case cascading failures of power networks. Then we compare these models by analyzing the critical disturbances and cascading processes. Significantly, comparison results on IEEE 9 bus system demonstrate that physical and electrical properties of power networks play a crucial role in the evolution of cascading failures, and it is necessary to take into account these properties appropriately while applying the model in the analysis of cascading blackout. 	
1708.06277v1	http://arxiv.org/pdf/1708.06277v1	2017	Models of Brauer-Severi surface bundles	Andrew Kresch|Yuri Tschinkel	  We study Brauer-Severi surface bundles over smooth projective varieties via root stacks, with a view towards applications to failure of stable rationality. 	
1708.08155v1	http://arxiv.org/pdf/1708.08155v1	2017	ByRDiE: Byzantine-resilient distributed coordinate descent for   decentralized learning	Zhixiong Yang|Waheed U. Bajwa	  Distributed machine learning algorithms enable processing of datasets that are distributed over a network without gathering the data at a centralized location. While efficient distributed algorithms have been developed under the assumption of faultless networks, failures that can render these algorithms nonfunctional indeed happen in the real world. This paper focuses on the problem of Byzantine failures, which are the hardest to safeguard against in distributed algorithms. While Byzantine fault tolerance has a rich history, existing work does not translate into efficient and practical algorithms for high-dimensional distributed learning tasks. In this paper, two variants of an algorithm termed Byzantine-resilient distributed coordinate descent (ByRDiE) are developed and analyzed that solve distributed learning problems in the presence of Byzantine failures. Theoretical analysis as well as numerical experiments presented in the paper highlight the usefulness of ByRDiE for high-dimensional distributed learning in the presence of Byzantine failures. 	
1709.03096v1	http://arxiv.org/pdf/1709.03096v1	2017	Survivable Probability of SDN-enabled Cloud Networking with Random   Physical Link Failure	Zhili Zhou|Tachun Lin	  Software-driven cloud networking is a new paradigm in orchestrating physical resources (CPU, network bandwidth, energy, storage) allocated to network functions, services, and applications, which is commonly modeled as a cross-layer network. This model carries a physical network representing the physical infrastructure, a logical network showing demands, and logical-to-physical node/link mappings. In such networks, a single failure in the physical network may trigger cascading failures in the logical network and disable network services and connectivity. In this paper, we propose an evaluation metric, survivable probability, to evaluate the reliability of such networks under random physical link failure(s). We propose the concept of base protecting spanning tree and prove the necessary and sufficient conditions for its existence and relation to survivability. We then develop mathematical programming formulations for reliable cross-layer network routing design with the maximal reliable probability. Computation results demonstrate the viability of our approach. 	
1709.03439v1	http://arxiv.org/pdf/1709.03439v1	2017	Why Do Deep Neural Networks Still Not Recognize These Images?: A   Qualitative Analysis on Failure Cases of ImageNet Classification	Han S. Lee|Alex A. Agarwal|Junmo Kim	  In a recent decade, ImageNet has become the most notable and powerful benchmark database in computer vision and machine learning community. As ImageNet has emerged as a representative benchmark for evaluating the performance of novel deep learning models, its evaluation tends to include only quantitative measures such as error rate, rather than qualitative analysis. Thus, there are few studies that analyze the failure cases of deep learning models in ImageNet, though there are numerous works analyzing the networks themselves and visualizing them. In this abstract, we qualitatively analyze the failure cases of ImageNet classification results from recent deep learning model, and categorize these cases according to the certain image patterns. Through this failure analysis, we believe that it can be discovered what the final challenges are in ImageNet database, which the current deep learning model is still vulnerable to. 	
1709.06934v1	http://arxiv.org/pdf/1709.06934v1	2017	REACT to Cyber Attacks on Power Grids	Saleh Soltan|Mihalis Yannakakis|Gil Zussman	  Motivated by the recent cyber attack on the Ukrainian power grid, we study cyber attacks on power grids that affect both the physical infrastructure and the data at the control center. In particular, we assume that an adversary attacks an area by: (i) remotely disconnecting some lines within the attacked area, and (ii) modifying the information received from the attacked area to mask the line failures and hide the attacked area from the control center. For the latter, we consider two types of attacks: (i) data distortion: which distorts the data by adding powerful noise to the actual data, and (ii) data replay: which replays a locally consistent old data instead of the actual data. We use the DC power flow model and prove that the problem of finding the set of line failures given the phase angles of the nodes outside of the attacked area is strongly NP-hard, even when the attacked area is known. However, we introduce the polynomial time REcurrent Attack Containment and deTection (REACT) Algorithm to approximately detect the attacked area and line failures after a cyber attack. We numerically show that it performs very well in detecting the attacked area, and detecting single, double, and triple line failures in small and large attacked areas. 	
1709.07399v1	http://arxiv.org/pdf/1709.07399v1	2017	EXPOSE the Line Failures following a Cyber-Physical Attack on the Power   Grid	Saleh Soltan|Gil Zussman	  Recent attacks on power grids demonstrated the vulnerability of the grids to cyber and physical attacks. To analyze this vulnerability, we study cyber-physical attacks that affect both the power grid physical infrastructure and its underlying Supervisory Control And Data Acquisition (SCADA) system. We assume that an adversary attacks an area by: (i) disconnecting some lines within that area, and (ii) obstructing the information (e.g., status of the lines and voltage measurements) from within the area to reach the control center. We leverage the algebraic properties of the AC power flows to introduce the efficient EXPOSE Algorithm for detecting line failures and recovering voltages inside that attacked area after such an attack. The EXPOSE Algorithm outperforms the state-of-the-art algorithm for detecting line failures using partial information under the AC power flow model in terms of scalability and accuracy. The main advantages of the EXPOSE Algorithm are that its running time is independent of the size of the grid and number of line failures, and that it provides accurate information recovery under some conditions on the attacked area. Moreover, it approximately recovers the information and provides the confidence of the solution when these conditions do not hold. 	
1710.05640v1	http://arxiv.org/pdf/1710.05640v1	2017	Survivable Probability of Network Slicing with Random Physical Link   Failure	Zhili Zhou|Tachun Lin	  The fifth generation of communication technology (5G) revolutionizes mobile networks and the associated ecosystems through the integration of cross-domain networks. Network slicing is an enabling technology for 5G as it provides dynamic, on-demand, and reliable logical network slices (i.e., network services) over a common physical network/infrastructure. Since a network slice is subject to failures originated from disruptions, namely node or link failures, in the physical infrastructure, our utmost interest is to evaluate the reliability of a network slice before assigning it to customers. In this paper, we propose an evaluation metric, \textit{survivable probability}, to quantify the reliability of a network slice under random physical link failure(s). We prove the existence of a \textit{base protecting spanning tree set} which has the same survivable probability as that of a network slice. We propose the necessary and sufficient conditions to identify a base protecting spanning tree set and develop corresponding mathematical formulations, which can be used to generate reliable network slices in the 5G environment. In addition to proving the viability of our approaches with simulation results, we also discuss how our problems and approaches are related to the Steiner tree problems and present their computational complexity and approximability. 	
1711.02580v1	http://arxiv.org/pdf/1711.02580v1	2017	Quantifying the Influence of Component Failure Probability on Cascading   Blackout Risk	Jinpeng Guo|Feng Liu|Jianhui Wang|Ming Cao|Shengwei Mei	  The risk of cascading blackouts greatly relies on failure probabilities of individual components in power grids. To quantify how component failure probabilities (CFP) influences blackout risk (BR), this paper proposes a sample-induced semi-analytic approach to characterize the relationship between CFP and BR. To this end, we first give a generic component failure probability function (CoFPF) to describe CFP with varying parameters or forms. Then the exact relationship between BR and CoFPFs is built on the abstract Markov-sequence model of cascading outages. Leveraging a set of samples generated by blackout simulations, we further establish a sample-induced semi-analytic mapping between the unbiased estimation of BR and CoFPFs. Finally, we derive an efficient algorithm that can directly calculate the unbiased estimation of BR when the CoFPFs change. Since no additional simulations are required, the algorithm is computationally scalable and efficient. Numerical experiments well confirm the theory and the algorithm. 	
1711.06855v1	http://arxiv.org/pdf/1711.06855v1	2017	A Probabilistic Characterization of Random and Malicious Communication   Failures in Multi-Hop Networked Control	Ahmet Cetinkaya|Hideaki Ishii|Tomohisa Hayakawa	  The control problem of a linear discrete-time dynamical system over a multi-hop network is explored. The network is assumed to be subject to packet drops by malicious and nonmalicious nodes as well as random and malicious data corruption issues. We utilize asymptotic tail-probability bounds of transmission failure ratios to characterize the links and paths of a network as well as the network itself. This probabilistic characterization allows us to take into account multiple failures that depend on each other, and coordinated malicious attacks on the network. We obtain a sufficient condition for the stability of the networked control system by utilizing our probabilistic approach. We then demonstrate the efficacy of our results in different scenarios concerning transmission failures on a multi-hop network. 	
1712.05465v2	http://arxiv.org/pdf/1712.05465v2	2018	Choreographies meet Communication Failures	Fabrizio Montesi|Marco Peressotti	  Choreographies are global descriptions of communication structures, inspired by the "Alice and Bob" notation of security protocols. They have been successfully employed in the design and implementation of distributed systems. However, there is still limited evidence of the applicability of choreographies in the real-world setting of distributed programming, where communication actions may fail. In this work, we propose the first choreography model that allows for communication failures and the programming of user-defined code to deal with such failures. We validate our model by implementing common strategies for handling communication failures in a robust way, which in turn can be used as a library by choreographies that assume reliable communication. We equip our model with a typing discipline that can statically verify reliability properties, in particular at-most-once and exactly-once delivery. We demonstrate the applicability of our model by defining a semantics-preserving compilation procedure towards a process calculus equipped with unreliable I/O actions. 	
1802.00104v1	http://arxiv.org/pdf/1802.00104v1	2018	On the Achievability Region of Regenerating Codes for Multiple Erasures	Marwen Zorgui|Zhiying Wang	  We study the problem of centralized exact repair of multiple failures in distributed storage. We describe constructions that achieve a new set of interior points under exact repair. The constructions build upon the layered code construction by Tian et al., designed for exact repair of single failure. We firstly improve upon the layered construction for general system parameters. Then, we extend the improved construction to support the repair of multiple failures, with varying number of helpers. In particular, we prove the optimality of one point on the functional repair tradeoff of multiple failures for some parameters. Finally, considering minimum bandwidth cooperative repair (MBCR) codes as centralized repair codes, we determine explicitly the best achievable region obtained by space-sharing among all known points, including the MBCR point. 	
1803.01719v1	http://arxiv.org/pdf/1803.01719v1	2018	How to Start Training: The Effect of Initialization and Architecture	Boris Hanin|David Rolnick	  We investigate the effects of initialization and architecture on the start of training in deep ReLU nets. We identify two common failure modes for early training in which the mean and variance of activations are poorly behaved. For each failure mode, we give a rigorous proof of when it occurs at initialization and how to avoid it. The first failure mode, exploding/vanishing mean activation length, can be avoided by initializing weights from a symmetric distribution with variance 2/fan-in. The second failure mode, exponentially large variance of activation length, can be avoided by keeping constant the sum of the reciprocals of layer widths. We demonstrate empirically the effectiveness of our theoretical results in predicting when networks are able to start training. In particular, we note that many popular initializations fail our criteria, whereas correct initialization and architecture allows much deeper networks to be trained. 	
